{"config":{"indexing":"full","lang":["en","ru"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]"},"docs":[{"location":"","text":"Welcome to cs-notes \u00b6 This site holds my notes for different computer science courses. In the start I wanted to dedicate this to only Machine Learning part, but then I extended it to all Computer Science. Help us spread this content: \u00b6 share, like, comment, save & watch LinkedIn Twitter Github Instagram Telegram Youtube The courses and notes are as follow: \u00b6 Books Notes \u00b6 contains notes for some well known books Bootcamps Notes \u00b6 contains notes for some of the most popular bootcamps for machine learning, data structures and algorithms and many other topics in computer science Competitive Programming Adventures \u00b6 Here you will see problem solving and solutions with explaination to coding challenges from Leetcode, code forces, exercism and many other Core Computer Science \u00b6 Here you will find topics related to Computer Science core subjects, programming languages, and much more My Research \u00b6 My Research publication and conference papers for anyone interested in doing research in the fields of Machine Learning, Combinotorial Optimization Problem Blog \u00b6 section will have severl tutorials for machine learning, deep learning, AI, Data Science and any other topic which the mind. Your Contributions are highly valued \u00b6 Fort this repo and add your favorite notes Send me a pull request I will check the worth of your notes and hopefully Merge your pull requests to make the World a better Place :) Your suggestions are also highly needed and encouraged \u00b6 Write on any of the following socials Stay Connected LinkedIn Twitter Github Instagram Telegram Youtube","title":"Welcome to cs-notes \ud83d\udcd3"},{"location":"#welcome-to-cs-notes","text":"This site holds my notes for different computer science courses. In the start I wanted to dedicate this to only Machine Learning part, but then I extended it to all Computer Science.","title":"Welcome to cs-notes"},{"location":"#help-us-spread-this-content","text":"share, like, comment, save & watch LinkedIn Twitter Github Instagram Telegram Youtube","title":"Help us spread this content:"},{"location":"#the-courses-and-notes-are-as-follow","text":"","title":"The courses and notes are as follow:"},{"location":"#books-notes","text":"contains notes for some well known books","title":"Books Notes"},{"location":"#bootcamps-notes","text":"contains notes for some of the most popular bootcamps for machine learning, data structures and algorithms and many other topics in computer science","title":"Bootcamps Notes"},{"location":"#competitive-programming-adventures","text":"Here you will see problem solving and solutions with explaination to coding challenges from Leetcode, code forces, exercism and many other","title":"Competitive Programming Adventures"},{"location":"#core-computer-science","text":"Here you will find topics related to Computer Science core subjects, programming languages, and much more","title":"Core Computer Science"},{"location":"#my-research","text":"My Research publication and conference papers for anyone interested in doing research in the fields of Machine Learning, Combinotorial Optimization Problem","title":"My Research"},{"location":"#blog","text":"section will have severl tutorials for machine learning, deep learning, AI, Data Science and any other topic which the mind.","title":"Blog"},{"location":"#your-contributions-are-highly-valued","text":"Fort this repo and add your favorite notes Send me a pull request I will check the worth of your notes and hopefully Merge your pull requests to make the World a better Place :)","title":"Your Contributions are highly valued"},{"location":"#your-suggestions-are-also-highly-needed-and-encouraged","text":"Write on any of the following socials Stay Connected LinkedIn Twitter Github Instagram Telegram Youtube","title":"Your suggestions are also highly needed and encouraged"},{"location":"about/","text":"About me: \u00b6 \"The desire to know is natural to good man\" Leonardo da Vinci My name is Jawad Haider; a computer science graduate (2022); researcher; a certified tensorflow developer; a human being :) Bio ! \u00b6 Having an ethereal self I love to inspire others. while at the same time I learn a lot of new things from them. By my iota of experience from teaching, volunteering, and leading camps I had a judicious ego which is more flexible to consider all the aspects of a team task. I have the skill to make people feel their knowledge and time are worthy. I want them to love, live and laugh and spread the jubilance where ever they go.\" Throw me to the wolves and I will return leading the pack.\" Hobbies \u00b6 I play chess , run marathons , do nature photography , listen to podcasts by Lex Fridman , Read books - poetry, novel apart from cs My aims \u00b6 Actions without aims are useless as glass without water. For work details: \u00b6 LinkedIn For research details: \u00b6 Google Scholar","title":"About"},{"location":"about/#about-me","text":"\"The desire to know is natural to good man\" Leonardo da Vinci My name is Jawad Haider; a computer science graduate (2022); researcher; a certified tensorflow developer; a human being :)","title":"About me:"},{"location":"about/#bio","text":"Having an ethereal self I love to inspire others. while at the same time I learn a lot of new things from them. By my iota of experience from teaching, volunteering, and leading camps I had a judicious ego which is more flexible to consider all the aspects of a team task. I have the skill to make people feel their knowledge and time are worthy. I want them to love, live and laugh and spread the jubilance where ever they go.\" Throw me to the wolves and I will return leading the pack.\"","title":"Bio !"},{"location":"about/#hobbies","text":"I play chess , run marathons , do nature photography , listen to podcasts by Lex Fridman , Read books - poetry, novel apart from cs","title":"Hobbies"},{"location":"about/#my-aims","text":"Actions without aims are useless as glass without water.","title":"My aims"},{"location":"about/#for-work-details","text":"LinkedIn","title":"For work details:"},{"location":"about/#for-research-details","text":"Google Scholar","title":"For research details:"},{"location":"contact/","text":"Contact us: \u00b6 Check the socials to reach out for more \u00b6 Your reach out is like a call from the outerworld For official queries email Your suggestions are also highly needed and encouraged \u00b6 Write on any of the following socials Stay Connected LinkedIn Twitter Github Instagram Telegram Youtube","title":"Contact"},{"location":"contact/#contact-us","text":"","title":"Contact us:"},{"location":"contact/#check-the-socials-to-reach-out-for-more","text":"Your reach out is like a call from the outerworld For official queries email","title":"Check the socials to reach out for more"},{"location":"contact/#your-suggestions-are-also-highly-needed-and-encouraged","text":"Write on any of the following socials Stay Connected LinkedIn Twitter Github Instagram Telegram Youtube","title":"Your suggestions are also highly needed and encouraged"},{"location":"blogs/","text":"Blogs \u00b6 All will be listed here; you can search and find the one you need. Will add Soon ... \u00b6 Hope you are enjoyed the content in other tabs, subtabs. For now I am doing my best to manage everything by my own , and could not add the guides here I understand your eagerness. But right now I need time and your patience \u00b6 Thank you for the kindess and have a great study \u00b6 \"Life is like riding a bicycle. To keep your balance, you must keep moving.\" Albert Einstein you will soon find what you are looking for. regards, Jawad","title":"Blogs"},{"location":"blogs/#blogs","text":"All will be listed here; you can search and find the one you need.","title":"Blogs"},{"location":"blogs/#will-add-soon","text":"Hope you are enjoyed the content in other tabs, subtabs. For now I am doing my best to manage everything by my own , and could not add the guides here","title":"Will add Soon ..."},{"location":"blogs/#i-understand-your-eagerness-but-right-now-i-need-time-and-your-patience","text":"","title":"I understand your eagerness. But right now I need time and your patience"},{"location":"blogs/#thank-you-for-the-kindess-and-have-a-great-study","text":"\"Life is like riding a bicycle. To keep your balance, you must keep moving.\" Albert Einstein you will soon find what you are looking for. regards, Jawad","title":"Thank you for the kindess and have a great study"},{"location":"blogs/2022/How-to-contribute/","text":"Nothing to read yet. Will find something to add soon...","title":"How to contribute"},{"location":"blogs/2022/How-to-use-these-resources/","text":"How-to-use-these-resources \u00b6 Will add Soon ... \u00b6 Hope you are enjoyed the content in other tabs, subtabs. For now I am doing my best to manage everything by my own, and could not add the guides here I understand your eagerness. But right now I need time and your patience \u00b6 Thank you for the kindess and have a great study","title":"How-to-use-these-resources"},{"location":"blogs/2022/How-to-use-these-resources/#how-to-use-these-resources","text":"","title":"How-to-use-these-resources"},{"location":"blogs/2022/How-to-use-these-resources/#will-add-soon","text":"Hope you are enjoyed the content in other tabs, subtabs. For now I am doing my best to manage everything by my own, and could not add the guides here","title":"Will add Soon ..."},{"location":"blogs/2022/How-to-use-these-resources/#i-understand-your-eagerness-but-right-now-i-need-time-and-your-patience","text":"Thank you for the kindess and have a great study","title":"I understand your eagerness. But right now I need time and your patience"},{"location":"booksnotes/","text":"Books Notes \u00b6 Here you will find notes related to the most well-know books authored by the top researchers in ML The list will increase will time :: \u00b6 Feel free to send pull requests to add your favorte books :: Currently we have : \u00b6 Data Science Handbook by Jake VanderPlas The book's website a great source and the github repo for the great notebooks... go there for more fruits ! Machine Learning Handbook","title":"Books Notes"},{"location":"booksnotes/#books-notes","text":"Here you will find notes related to the most well-know books authored by the top researchers in ML","title":"Books Notes"},{"location":"booksnotes/#the-list-will-increase-will-time","text":"Feel free to send pull requests to add your favorte books ::","title":"The list will increase will time ::"},{"location":"booksnotes/#currently-we-have","text":"Data Science Handbook by Jake VanderPlas The book's website a great source and the github repo for the great notebooks... go there for more fruits ! Machine Learning Handbook","title":"Currently we have :"},{"location":"booksnotes/pythonDataScienceHandBook/","text":"Data Science Handbook \u00b6 by Jake VanderPlas This is a great book to refer when using any of the Data Science python libraries and Traditional ML algorithm implementation. The book's website a great source and the github repo for the great notebooks... go there for more fruits ! Table of content: \u00b6 Jump to any section you want Chapter 1: Introduction to Numpy Chapter 2: Data Manipulation with Pandas Chapter 3: Visualization with Matplotlib Chapter 4: Machine Learning","title":"Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/#data-science-handbook","text":"by Jake VanderPlas This is a great book to refer when using any of the Data Science python libraries and Traditional ML algorithm implementation. The book's website a great source and the github repo for the great notebooks... go there for more fruits !","title":"Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/#table-of-content","text":"Jump to any section you want Chapter 1: Introduction to Numpy Chapter 2: Data Manipulation with Pandas Chapter 3: Visualization with Matplotlib Chapter 4: Machine Learning","title":"Table of content:"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/","text":"================ by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 00 - Understanding Data Types in Python \u00b6 Understanding Data Types in Python A Python Integer Is More Than Just an Integer A Python List Is More Than Just a List Fixed typed Arrays in Python Creating Arrays from Scratch NumPy Standard Data Types Next: Basics of NumPy Arrays Understanding Data Types in Python \u00b6 Users of Python are often drawn in by its ease of use, one piece of which is dynamic typing. While a statically typed language like C or Java requires each variable to be explicitly declared, a dynamically typed language like Python skips this specification. A Python Integer Is More Than Just an Integer \u00b6 struct _longobject { long ob_refcnt; PyTypeObject *ob_type; size_t ob_size; long ob_digit[1]; }; A single integer in Python 3.4 actually contains four pieces: \u2022 ob_refcnt, a reference count that helps Python silently handle memory allocation and deallocation \u2022 ob_type, which encodes the type of the variable \u2022 ob_size, which specifies the size of the following data members \u2022 ob_digit, which contains the actual integer value that we expect the Python variable to represent A Python List Is More Than Just a List \u00b6 l1 = list ( range ( 10 )) l1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] type ( l1 [ 0 ]) int l2 = list ( str ( i ) for i in range ( 10 )) l2 ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] type ( l2 [ 0 ]) str # Becaause of the dynamically typing our list can even be heterogenious l3 = [ True , 3.146 , 7 , \"Name\" ] [ type ( i ) for i in l3 ] [bool, float, int, str] But this flexibility comes at a cost: to allow these flexible types, each item in the list must contain its own type info, reference count, and other information\u2014that is, each item is a complete Python object. In the special case that all variables are of the same type, much of this information is redundant: it can be much more efficient to store data in a fixed-type array. The difference between a dynamic-type list and a fixed-type (NumPy-style) array given below At the implementation level, the array essentially contains a single pointer to one con\u2010 tiguous block of data. The Python list, on the other hand, contains a pointer to a block of pointers, each of which in turn points to a full Python object like the Python integer we saw earlier. Again, the advantage of the list is flexibility: because each list element is a full structure containing both data and type information, the list can be filled with data of any desired type. Fixed-type NumPy-style arrays lack this flexibil\u2010 ity, but are much more efficient for storing and manipulating data Fixed typed Arrays in Python \u00b6 Much more useful, however, is the ndarray object of the NumPy package. While Python\u2019s array object provides efficient storage of array-based data, NumPy adds to this efficient operations on that data. import array l = list ( range ( 10 )) a = array . array ( 'i' , l ) #Here 'i' is a type code indicating the contents are integers a array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) type ( a ) array.array We\u2019ll start with the standard NumPy import, under the alias np: ## Creating Arrays from Python Lists import numpy as np # Integer array np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) array([1, 2, 3, 4, 5, 6]) np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = 'float32' ) # Explicitly set the data type of the resulting array array([1., 2., 3., 4., 5., 6.], dtype=float32) # Nested list results in multi-dimensional arrays check = np . array ([[[ 1 , 3 , 5 ],[ 2 , 4 , 6 ]],[[ 3 , 7 , 11 ],[ 0 , 0 , 0 ]]]) check array([[[ 1, 3, 5], [ 2, 4, 6]], [[ 3, 7, 11], [ 0, 0, 0]]]) check . shape (2, 2, 3) np . array ([ range ( i , i + 3 ) for i in [ 2 , 4 , 6 ]]) array([[2, 3, 4], [4, 5, 6], [6, 7, 8]]) Creating Arrays from Scratch \u00b6 Especially for larger arrays, it is more efficient to create arrays from scratch using rou\u2010 tines built into NumPy # Create a length-10 integer array filled with zeros np . zeros ( 10 , dtype = int ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # Create a 2,3 flaot matrix filled with zeros np . zeros (( 2 , 3 ), dtype = float ) array([[0., 0., 0.], [0., 0., 0.]]) # Create a length-5 array filled with ones np . ones ( 5 ) array([1., 1., 1., 1., 1.]) # Create a 4x3 matrix filled with ones np . ones (( 4 , 3 ), dtype = int ) array([[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]) # Create a 4x3 matrix filled with the given number np . full (( 3 , 3 ), 7 ) array([[7, 7, 7], [7, 7, 7], [7, 7, 7]]) # Create an array filled with a linear sequence # Starting at 0, ending at 20, stepping by 2 # (this is similar to the built-in range() function) np . arange ( 0 , 20 , 2 ) array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) # Create an array of five values evenly spaced between 0 and 1 np . linspace ( 0 , 1 , 5 ) array([0. , 0.25, 0.5 , 0.75, 1. ]) # Create a 3x3 array of uniformly distributed # random values between 0 and 1 np . random . random (( 3 , 3 )) array([[0.32813402, 0.33063616, 0.13412605], [0.41941497, 0.97530927, 0.04965349], [0.00563929, 0.26416415, 0.84203272]]) # Create a 3x3 array of normally distributed random values # with mean 0 and standard deviation 1 np . random . normal ( 0 , 1 ,( 3 , 3 )) array([[ 1.08133877, -1.1355389 , 1.04280763], [ 0.91102377, 0.77094415, 0.14660595], [ 2.32307703, 0.69711284, 1.26467304]]) # Create a 3x3 array of random integers in the interval [0, 10) np . random . randint ( 0 , 10 ,( 3 , 3 )) array([[2, 4, 8], [5, 8, 1], [5, 8, 3]]) # Create a 3x3 identity matrix np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) # Create an uninitialized array of three integers # The values will be whatever happens to already exist at that # memory location np . empty ( 3 ) array([1., 1., 1.]) NumPy Standard Data Types \u00b6 NumPy arrays contain values of a single type, so it is important to have detailed knowledge of those types and their limitations. Because NumPy is built in C, the types will be familiar to users of C, Fortran, and other related languages. np . zeros ( 5 , dtype = 'int16' ) array([0, 0, 0, 0, 0], dtype=int16) np . zeros ( 5 , dtype = np . int16 ) array([0, 0, 0, 0, 0], dtype=int16) Next: Basics of NumPy Arrays \u00b6 page 42","title":"00 Understanding Data Types in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#00-understanding-data-types-in-python","text":"Understanding Data Types in Python A Python Integer Is More Than Just an Integer A Python List Is More Than Just a List Fixed typed Arrays in Python Creating Arrays from Scratch NumPy Standard Data Types Next: Basics of NumPy Arrays","title":"00 - Understanding Data Types in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#understanding-data-types-in-python","text":"Users of Python are often drawn in by its ease of use, one piece of which is dynamic typing. While a statically typed language like C or Java requires each variable to be explicitly declared, a dynamically typed language like Python skips this specification.","title":"Understanding Data Types in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#a-python-integer-is-more-than-just-an-integer","text":"struct _longobject { long ob_refcnt; PyTypeObject *ob_type; size_t ob_size; long ob_digit[1]; }; A single integer in Python 3.4 actually contains four pieces: \u2022 ob_refcnt, a reference count that helps Python silently handle memory allocation and deallocation \u2022 ob_type, which encodes the type of the variable \u2022 ob_size, which specifies the size of the following data members \u2022 ob_digit, which contains the actual integer value that we expect the Python variable to represent","title":"A Python Integer Is More Than Just an Integer"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#a-python-list-is-more-than-just-a-list","text":"l1 = list ( range ( 10 )) l1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] type ( l1 [ 0 ]) int l2 = list ( str ( i ) for i in range ( 10 )) l2 ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] type ( l2 [ 0 ]) str # Becaause of the dynamically typing our list can even be heterogenious l3 = [ True , 3.146 , 7 , \"Name\" ] [ type ( i ) for i in l3 ] [bool, float, int, str] But this flexibility comes at a cost: to allow these flexible types, each item in the list must contain its own type info, reference count, and other information\u2014that is, each item is a complete Python object. In the special case that all variables are of the same type, much of this information is redundant: it can be much more efficient to store data in a fixed-type array. The difference between a dynamic-type list and a fixed-type (NumPy-style) array given below At the implementation level, the array essentially contains a single pointer to one con\u2010 tiguous block of data. The Python list, on the other hand, contains a pointer to a block of pointers, each of which in turn points to a full Python object like the Python integer we saw earlier. Again, the advantage of the list is flexibility: because each list element is a full structure containing both data and type information, the list can be filled with data of any desired type. Fixed-type NumPy-style arrays lack this flexibil\u2010 ity, but are much more efficient for storing and manipulating data","title":"A Python List Is More Than Just a List"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#fixed-typed-arrays-in-python","text":"Much more useful, however, is the ndarray object of the NumPy package. While Python\u2019s array object provides efficient storage of array-based data, NumPy adds to this efficient operations on that data. import array l = list ( range ( 10 )) a = array . array ( 'i' , l ) #Here 'i' is a type code indicating the contents are integers a array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) type ( a ) array.array We\u2019ll start with the standard NumPy import, under the alias np: ## Creating Arrays from Python Lists import numpy as np # Integer array np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) array([1, 2, 3, 4, 5, 6]) np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = 'float32' ) # Explicitly set the data type of the resulting array array([1., 2., 3., 4., 5., 6.], dtype=float32) # Nested list results in multi-dimensional arrays check = np . array ([[[ 1 , 3 , 5 ],[ 2 , 4 , 6 ]],[[ 3 , 7 , 11 ],[ 0 , 0 , 0 ]]]) check array([[[ 1, 3, 5], [ 2, 4, 6]], [[ 3, 7, 11], [ 0, 0, 0]]]) check . shape (2, 2, 3) np . array ([ range ( i , i + 3 ) for i in [ 2 , 4 , 6 ]]) array([[2, 3, 4], [4, 5, 6], [6, 7, 8]])","title":"Fixed typed Arrays in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#creating-arrays-from-scratch","text":"Especially for larger arrays, it is more efficient to create arrays from scratch using rou\u2010 tines built into NumPy # Create a length-10 integer array filled with zeros np . zeros ( 10 , dtype = int ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # Create a 2,3 flaot matrix filled with zeros np . zeros (( 2 , 3 ), dtype = float ) array([[0., 0., 0.], [0., 0., 0.]]) # Create a length-5 array filled with ones np . ones ( 5 ) array([1., 1., 1., 1., 1.]) # Create a 4x3 matrix filled with ones np . ones (( 4 , 3 ), dtype = int ) array([[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]) # Create a 4x3 matrix filled with the given number np . full (( 3 , 3 ), 7 ) array([[7, 7, 7], [7, 7, 7], [7, 7, 7]]) # Create an array filled with a linear sequence # Starting at 0, ending at 20, stepping by 2 # (this is similar to the built-in range() function) np . arange ( 0 , 20 , 2 ) array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) # Create an array of five values evenly spaced between 0 and 1 np . linspace ( 0 , 1 , 5 ) array([0. , 0.25, 0.5 , 0.75, 1. ]) # Create a 3x3 array of uniformly distributed # random values between 0 and 1 np . random . random (( 3 , 3 )) array([[0.32813402, 0.33063616, 0.13412605], [0.41941497, 0.97530927, 0.04965349], [0.00563929, 0.26416415, 0.84203272]]) # Create a 3x3 array of normally distributed random values # with mean 0 and standard deviation 1 np . random . normal ( 0 , 1 ,( 3 , 3 )) array([[ 1.08133877, -1.1355389 , 1.04280763], [ 0.91102377, 0.77094415, 0.14660595], [ 2.32307703, 0.69711284, 1.26467304]]) # Create a 3x3 array of random integers in the interval [0, 10) np . random . randint ( 0 , 10 ,( 3 , 3 )) array([[2, 4, 8], [5, 8, 1], [5, 8, 3]]) # Create a 3x3 identity matrix np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) # Create an uninitialized array of three integers # The values will be whatever happens to already exist at that # memory location np . empty ( 3 ) array([1., 1., 1.])","title":"Creating Arrays from Scratch"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#numpy-standard-data-types","text":"NumPy arrays contain values of a single type, so it is important to have detailed knowledge of those types and their limitations. Because NumPy is built in C, the types will be familiar to users of C, Fortran, and other related languages. np . zeros ( 5 , dtype = 'int16' ) array([0, 0, 0, 0, 0], dtype=int16) np . zeros ( 5 , dtype = np . int16 ) array([0, 0, 0, 0, 0], dtype=int16)","title":"NumPy Standard Data Types"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#next-basics-of-numpy-arrays","text":"page 42","title":"Next: Basics of NumPy Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/","text":"================ by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 01 - The Basics of NumPy Arrays \u00b6 The Basics of NumPy Arrays Array Attributes Array Indexing: Accessing Single Elements Array Slicing: Accessing Subarrays One-dimensional subarrays Multidimensional subarrays Accessing array rows and columns. Subarrays as no-copy views Creating copies of arrays Reshaping of Arrays Array Concatenation and Splitting Concatenation of arrays Splitting of arrays The Basics of NumPy Arrays \u00b6 This section will present several examples using NumPy array manipulation to access data and subarrays, and to split, reshape, and join the arrays. We\u2019ll cover a few categories of basic array manipulations here: Attributes of arrays Determining the size, shape, memory consumption, and data types of arrays Indexing of arrays Getting and setting the value of individual array elements Slicing of arrays Getting and setting smaller subarrays within a larger array Reshaping of arrays Changing the shape of a given array Joining and splitting of arrays Combining multiple arrays into one, and splitting one array into many Array Attributes \u00b6 import numpy as np np . random . seed ( 0 ) x1 = np . random . randint ( 10 , size = 6 ) # 1-d array x2 = np . random . randint ( 10 , size = ( 3 , 4 )) # 2-d array (matrix) x3 = np . random . randint ( 10 , size = ( 3 , 4 , 5 )) # 3-d array print ( x3 ) print ( \"x3 ndim: \" , x3 . ndim ) print ( \"x3.shape: \" , x3 . shape ) print ( \"x3.size: \" , x3 . size ) [[[8 1 5 9 8] [9 4 3 0 3] [5 0 2 3 8] [1 3 3 3 7]] [[0 1 9 9 0] [4 7 3 2 7] [2 0 0 4 5] [5 6 8 4 1]] [[4 9 8 1 1] [7 9 9 3 6] [7 2 0 3 5] [9 4 4 6 4]]] x3 ndim: 3 x3.shape: (3, 4, 5) x3.size: 60 print ( x2 ) print ( \"x2 ndim: \" , x2 . ndim ) print ( \"x2.shape: \" , x2 . shape ) print ( \"x2.size: \" , x2 . size ) [[3 5 2 4] [7 6 8 8] [1 6 7 7]] x2 ndim: 2 x2.shape: (3, 4) x2.size: 12 print ( x1 ) print ( \"x1 ndim: \" , x1 . ndim ) print ( \"x1.shape: \" , x1 . shape ) print ( \"x1.size: \" , x1 . size ) [5 0 3 3 7 9] x1 ndim: 1 x1.shape: (6,) x1.size: 6 print ( \"dtype: \" , x3 . dtype ) dtype: int64 print ( \"itemsize: \" , x3 . itemsize , \"bytes\" ) # size of each array element itemsize: 8 bytes print ( \"nbytes: \" , x3 . nbytes , \" bytes\" ) # total size of the array -> sum of bytes of each array element nbytes: 480 bytes Array Indexing: Accessing Single Elements \u00b6 If you are familiar with Python\u2019s standard list indexing, indexing in NumPy will feel quite familiar. In a one-dimensional array, you can access the ith value (counting from zero) by specifying the desired index in square brackets, just as with Python lists: x1 array([5, 0, 3, 3, 7, 9]) x1 [ - 1 ] 9 x1 [ 0 ] 5 #In a multidimensional array, you access items using a comma-separated tuple of indices: x2 array([[3, 5, 2, 4], [7, 6, 8, 8], [1, 6, 7, 7]]) x2 [ 0 , 0 ] 3 # same for modifying any element of the array x2 [ 0 , 3 ] 4 x2 [ 0 , 3 ] =- 4 x2 [ 0 , 3 ] -4 x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) Keep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated. Don\u2019t be caught unaware by this behavior! x1 [ 0 ] 5 x1 [ 0 ] =- 5.78 # this will the decimal part x1 [ 0 ] -5 Array Slicing: Accessing Subarrays \u00b6 Just as we can use square brackets to access individual array elements, we can also use them to access subarrays with the slice notation, marked by the colon (:) character. The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array x, use this: x[start:stop:step] One-dimensional subarrays \u00b6 x = np . arange ( 10 ) x array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) x [: 5 ] array([0, 1, 2, 3, 4]) x [ 1 : len ( x ) - 3 ] array([1, 2, 3, 4, 5, 6]) x [ len ( x ) // 2 :] # after mid array([5, 6, 7, 8, 9]) x [:: 2 ] # Every other element array([0, 2, 4, 6, 8]) x [ 1 :: 2 ] array([1, 3, 5, 7, 9]) A potentially confusing case is when the step value is negative. In this case, the defaults for start and stop are swapped. This becomes a convenient way to reverse an array: x [:: - 1 ] # All elements reverse array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) x [ 5 :: - 2 ] array([5, 3, 1]) Multidimensional subarrays \u00b6 Multidimensional slices work in the same way, with multiple slices separated by com\u2010 mas. x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2 [: 2 ,: 3 ] #two rows and three columns array([[3, 5, 2], [7, 6, 8]]) x2 [: 3 ,:: 2 ] # three rows and every other column array([[3, 2], [7, 8], [1, 7]]) Accessing array rows and columns. \u00b6 One commonly needed routine is accessing single rows or columns of an array. You can do this by combining indexing and slicing, using an empty slice marked by a single colon (:): x2 [:, 0 ] # first column array([3, 7, 1]) x2 [ 0 ,:] # First row array([ 3, 5, 2, -4]) #In the case of row access, the empty slice can be omitted for a more compact syntax:s x2 [ 0 ] array([ 3, 5, 2, -4]) Subarrays as no-copy views \u00b6 One important\u2014and extremely useful\u2014thing to know about array slices is that they return views rather than copies of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) # Extracting a 2x2 subarray from this x2_sub = x2 [: 2 ,: 2 ] x2_sub array([[3, 5], [7, 6]]) # Modifying this subarray will also modify the original array x2_sub [ 0 , 0 ] =- 9 x2 array([[-9, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2_sub array([[-9, 5], [ 7, 6]]) This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer. Creating copies of arrays \u00b6 Despite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the copy() method: x2_sub_copy = x2 [: 2 ,: 2 ] . copy () x2_sub_copy array([[-9, 5], [ 7, 6]]) If we now modify this subarray, the original array is not touched: \u00b6 x2_sub_copy [ 0 , 0 ] =- 999 x2 array([[-9, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2_sub_copy array([[-999, 5], [ 7, 6]]) Reshaping of Arrays \u00b6 Another useful type of operation is reshaping of arrays. The most flexible way of doing this is with the reshape() method. grid = np . arange ( 1 , 10 ) . reshape (( 3 , 3 )) grid array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Note that for this to work, the size of the initial array must match the size of the reshaped array. Where possible, the reshape method will use a no-copy view of the initial array, but with noncontiguous memory buffers this is not always the case. Another common reshaping pattern is the conversion of a one-dimensional array into a two-dimensional row or column matrix. You can do this with the reshape method, or more easily by making use of the newaxis keyword within a slice opera\u2010 tion: x = np . array ([ 1 , 2 , 3 , 4 ]) x array([1, 2, 3, 4]) x . reshape (( 1 , 4 )) # row vector reshape via reshape array([[1, 2, 3, 4]]) x . reshape (( len ( x ), 1 )) #Column vector reshape via array([[1], [2], [3], [4]]) x [ np . newaxis ,:] array([[1, 2, 3, 4]]) x [:, np . newaxis ] array([[1], [2], [3], [4]]) Array Concatenation and Splitting \u00b6 Concatenation of arrays \u00b6 Concatenation, or joining of two arrays in NumPy, is primarily accomplished through the routines np.concatenate, np.vstack, and np.hstack . np.concatenate takes a tuple or list of arrays as its first argument. * You can also concatenate more than two arrays at once * np.concatenate can also be used for two-dimensional arrays x = np . array ([ 2 , 4 , 6 , 8 ]) y = np . array ([ 1 , 3 , 5 , 7 ]) np . concatenate ([ x , y ]) array([2, 4, 6, 8, 1, 3, 5, 7]) z = np . zeros ( 4 ) z array([0., 0., 0., 0.]) np . concatenate ([ x , z , y ]) # Three arrays concatenate in given order array([2., 4., 6., 8., 0., 0., 0., 0., 1., 3., 5., 7.]) grid1 = np . random . randint ( 10 , 50 , size = ( 2 , 4 )) grid2 = np . random . randint ( 0 , 1 , size = ( 2 , 3 )) grid1 array([[39, 13, 44, 23], [49, 31, 19, 10]]) grid2 array([[0, 0, 0], [0, 0, 0]]) np . concatenate ([ grid1 , grid2 ]) ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 4 and the array at index 1 has size 3 np . concatenate ([ grid1 , grid2 ], axis = 1 ) array([[39, 13, 44, 23, 0, 0, 0], [49, 31, 19, 10, 0, 0, 0]]) grid2 = np . random . randint ( 0 , 10 , size = ( 2 , 4 )) grid2 array([[3, 0, 5, 0], [1, 2, 4, 2]]) np . concatenate ([ grid1 , grid2 ], axis = 0 ) array([[13, 22, 46, 24], [25, 30, 45, 33], [ 3, 0, 5, 0], [ 1, 2, 4, 2]]) np . concatenate ([ grid1 , grid2 ], axis = 1 ) array([[13, 22, 46, 24, 3, 0, 5, 0], [25, 30, 45, 33, 1, 2, 4, 2]]) For working with arrays of mixed dimensions, it can be clearer to use the np.vstack (vertical stack) and np.hstack (horizontal stack) functions x = np . array ([ 1 , 2 , 3 ]) grid = np . random . randint ( 1 , 10 , size = ( 2 , 3 )) x array([1, 2, 3]) grid array([[5, 7, 9], [3, 4, 1]]) np . vstack ([ x , grid ]) array([[1, 2, 3], [5, 7, 9], [3, 4, 1]]) Splitting of arrays \u00b6 The opposite of concatenation is splitting, which is implemented by the functions np.split, np.hsplit, and np.vsplit. For each of these, we can pass a list of indices giving the split points: x = [ 1 , 2 , 3 , 99 , 100 , 101 , 3 , 2 , 1 ] x1 , x2 , x3 = np . split ( x ,[ 3 , 5 ]) # split will be at index 3 & 5 x1 array([1, 2, 3]) x2 array([ 99, 100]) x3 #Notice that N split points lead to N + 1 subarrays. #The related functions np.hsplit and np.vsplit are similar: array([101, 3, 2, 1]) grid = np . arange ( 16 ) . reshape (( 4 , 4 )) upper , lower = np . vsplit ( grid ,[ 2 ]) upper array([[0, 1, 2, 3], [4, 5, 6, 7]]) lower array([[ 8, 9, 10, 11], [12, 13, 14, 15]]) grid array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) left , right = np . hsplit ( grid ,[ 2 ]) left array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13]]) right array([[ 2, 3], [ 6, 7], [10, 11], [14, 15]]) Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"01 basics of numpy arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#01-the-basics-of-numpy-arrays","text":"The Basics of NumPy Arrays Array Attributes Array Indexing: Accessing Single Elements Array Slicing: Accessing Subarrays One-dimensional subarrays Multidimensional subarrays Accessing array rows and columns. Subarrays as no-copy views Creating copies of arrays Reshaping of Arrays Array Concatenation and Splitting Concatenation of arrays Splitting of arrays","title":"01 - The Basics of NumPy Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#the-basics-of-numpy-arrays","text":"This section will present several examples using NumPy array manipulation to access data and subarrays, and to split, reshape, and join the arrays. We\u2019ll cover a few categories of basic array manipulations here: Attributes of arrays Determining the size, shape, memory consumption, and data types of arrays Indexing of arrays Getting and setting the value of individual array elements Slicing of arrays Getting and setting smaller subarrays within a larger array Reshaping of arrays Changing the shape of a given array Joining and splitting of arrays Combining multiple arrays into one, and splitting one array into many","title":"The Basics of NumPy Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#array-attributes","text":"import numpy as np np . random . seed ( 0 ) x1 = np . random . randint ( 10 , size = 6 ) # 1-d array x2 = np . random . randint ( 10 , size = ( 3 , 4 )) # 2-d array (matrix) x3 = np . random . randint ( 10 , size = ( 3 , 4 , 5 )) # 3-d array print ( x3 ) print ( \"x3 ndim: \" , x3 . ndim ) print ( \"x3.shape: \" , x3 . shape ) print ( \"x3.size: \" , x3 . size ) [[[8 1 5 9 8] [9 4 3 0 3] [5 0 2 3 8] [1 3 3 3 7]] [[0 1 9 9 0] [4 7 3 2 7] [2 0 0 4 5] [5 6 8 4 1]] [[4 9 8 1 1] [7 9 9 3 6] [7 2 0 3 5] [9 4 4 6 4]]] x3 ndim: 3 x3.shape: (3, 4, 5) x3.size: 60 print ( x2 ) print ( \"x2 ndim: \" , x2 . ndim ) print ( \"x2.shape: \" , x2 . shape ) print ( \"x2.size: \" , x2 . size ) [[3 5 2 4] [7 6 8 8] [1 6 7 7]] x2 ndim: 2 x2.shape: (3, 4) x2.size: 12 print ( x1 ) print ( \"x1 ndim: \" , x1 . ndim ) print ( \"x1.shape: \" , x1 . shape ) print ( \"x1.size: \" , x1 . size ) [5 0 3 3 7 9] x1 ndim: 1 x1.shape: (6,) x1.size: 6 print ( \"dtype: \" , x3 . dtype ) dtype: int64 print ( \"itemsize: \" , x3 . itemsize , \"bytes\" ) # size of each array element itemsize: 8 bytes print ( \"nbytes: \" , x3 . nbytes , \" bytes\" ) # total size of the array -> sum of bytes of each array element nbytes: 480 bytes","title":"Array Attributes"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#array-indexing-accessing-single-elements","text":"If you are familiar with Python\u2019s standard list indexing, indexing in NumPy will feel quite familiar. In a one-dimensional array, you can access the ith value (counting from zero) by specifying the desired index in square brackets, just as with Python lists: x1 array([5, 0, 3, 3, 7, 9]) x1 [ - 1 ] 9 x1 [ 0 ] 5 #In a multidimensional array, you access items using a comma-separated tuple of indices: x2 array([[3, 5, 2, 4], [7, 6, 8, 8], [1, 6, 7, 7]]) x2 [ 0 , 0 ] 3 # same for modifying any element of the array x2 [ 0 , 3 ] 4 x2 [ 0 , 3 ] =- 4 x2 [ 0 , 3 ] -4 x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) Keep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated. Don\u2019t be caught unaware by this behavior! x1 [ 0 ] 5 x1 [ 0 ] =- 5.78 # this will the decimal part x1 [ 0 ] -5","title":"Array Indexing: Accessing Single Elements"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#array-slicing-accessing-subarrays","text":"Just as we can use square brackets to access individual array elements, we can also use them to access subarrays with the slice notation, marked by the colon (:) character. The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array x, use this: x[start:stop:step]","title":"Array Slicing: Accessing Subarrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#one-dimensional-subarrays","text":"x = np . arange ( 10 ) x array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) x [: 5 ] array([0, 1, 2, 3, 4]) x [ 1 : len ( x ) - 3 ] array([1, 2, 3, 4, 5, 6]) x [ len ( x ) // 2 :] # after mid array([5, 6, 7, 8, 9]) x [:: 2 ] # Every other element array([0, 2, 4, 6, 8]) x [ 1 :: 2 ] array([1, 3, 5, 7, 9]) A potentially confusing case is when the step value is negative. In this case, the defaults for start and stop are swapped. This becomes a convenient way to reverse an array: x [:: - 1 ] # All elements reverse array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) x [ 5 :: - 2 ] array([5, 3, 1])","title":"One-dimensional subarrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#multidimensional-subarrays","text":"Multidimensional slices work in the same way, with multiple slices separated by com\u2010 mas. x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2 [: 2 ,: 3 ] #two rows and three columns array([[3, 5, 2], [7, 6, 8]]) x2 [: 3 ,:: 2 ] # three rows and every other column array([[3, 2], [7, 8], [1, 7]])","title":"Multidimensional subarrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#accessing-array-rows-and-columns","text":"One commonly needed routine is accessing single rows or columns of an array. You can do this by combining indexing and slicing, using an empty slice marked by a single colon (:): x2 [:, 0 ] # first column array([3, 7, 1]) x2 [ 0 ,:] # First row array([ 3, 5, 2, -4]) #In the case of row access, the empty slice can be omitted for a more compact syntax:s x2 [ 0 ] array([ 3, 5, 2, -4])","title":"Accessing array rows and columns."},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#subarrays-as-no-copy-views","text":"One important\u2014and extremely useful\u2014thing to know about array slices is that they return views rather than copies of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) # Extracting a 2x2 subarray from this x2_sub = x2 [: 2 ,: 2 ] x2_sub array([[3, 5], [7, 6]]) # Modifying this subarray will also modify the original array x2_sub [ 0 , 0 ] =- 9 x2 array([[-9, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2_sub array([[-9, 5], [ 7, 6]]) This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer.","title":"Subarrays as no-copy views"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#creating-copies-of-arrays","text":"Despite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the copy() method: x2_sub_copy = x2 [: 2 ,: 2 ] . copy () x2_sub_copy array([[-9, 5], [ 7, 6]])","title":"Creating copies of arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#if-we-now-modify-this-subarray-the-original-array-is-not-touched","text":"x2_sub_copy [ 0 , 0 ] =- 999 x2 array([[-9, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2_sub_copy array([[-999, 5], [ 7, 6]])","title":"If we now modify this subarray, the original array is not touched:"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#reshaping-of-arrays","text":"Another useful type of operation is reshaping of arrays. The most flexible way of doing this is with the reshape() method. grid = np . arange ( 1 , 10 ) . reshape (( 3 , 3 )) grid array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Note that for this to work, the size of the initial array must match the size of the reshaped array. Where possible, the reshape method will use a no-copy view of the initial array, but with noncontiguous memory buffers this is not always the case. Another common reshaping pattern is the conversion of a one-dimensional array into a two-dimensional row or column matrix. You can do this with the reshape method, or more easily by making use of the newaxis keyword within a slice opera\u2010 tion: x = np . array ([ 1 , 2 , 3 , 4 ]) x array([1, 2, 3, 4]) x . reshape (( 1 , 4 )) # row vector reshape via reshape array([[1, 2, 3, 4]]) x . reshape (( len ( x ), 1 )) #Column vector reshape via array([[1], [2], [3], [4]]) x [ np . newaxis ,:] array([[1, 2, 3, 4]]) x [:, np . newaxis ] array([[1], [2], [3], [4]])","title":"Reshaping of Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#array-concatenation-and-splitting","text":"","title":"Array Concatenation and Splitting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#concatenation-of-arrays","text":"Concatenation, or joining of two arrays in NumPy, is primarily accomplished through the routines np.concatenate, np.vstack, and np.hstack . np.concatenate takes a tuple or list of arrays as its first argument. * You can also concatenate more than two arrays at once * np.concatenate can also be used for two-dimensional arrays x = np . array ([ 2 , 4 , 6 , 8 ]) y = np . array ([ 1 , 3 , 5 , 7 ]) np . concatenate ([ x , y ]) array([2, 4, 6, 8, 1, 3, 5, 7]) z = np . zeros ( 4 ) z array([0., 0., 0., 0.]) np . concatenate ([ x , z , y ]) # Three arrays concatenate in given order array([2., 4., 6., 8., 0., 0., 0., 0., 1., 3., 5., 7.]) grid1 = np . random . randint ( 10 , 50 , size = ( 2 , 4 )) grid2 = np . random . randint ( 0 , 1 , size = ( 2 , 3 )) grid1 array([[39, 13, 44, 23], [49, 31, 19, 10]]) grid2 array([[0, 0, 0], [0, 0, 0]]) np . concatenate ([ grid1 , grid2 ]) ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 4 and the array at index 1 has size 3 np . concatenate ([ grid1 , grid2 ], axis = 1 ) array([[39, 13, 44, 23, 0, 0, 0], [49, 31, 19, 10, 0, 0, 0]]) grid2 = np . random . randint ( 0 , 10 , size = ( 2 , 4 )) grid2 array([[3, 0, 5, 0], [1, 2, 4, 2]]) np . concatenate ([ grid1 , grid2 ], axis = 0 ) array([[13, 22, 46, 24], [25, 30, 45, 33], [ 3, 0, 5, 0], [ 1, 2, 4, 2]]) np . concatenate ([ grid1 , grid2 ], axis = 1 ) array([[13, 22, 46, 24, 3, 0, 5, 0], [25, 30, 45, 33, 1, 2, 4, 2]]) For working with arrays of mixed dimensions, it can be clearer to use the np.vstack (vertical stack) and np.hstack (horizontal stack) functions x = np . array ([ 1 , 2 , 3 ]) grid = np . random . randint ( 1 , 10 , size = ( 2 , 3 )) x array([1, 2, 3]) grid array([[5, 7, 9], [3, 4, 1]]) np . vstack ([ x , grid ]) array([[1, 2, 3], [5, 7, 9], [3, 4, 1]])","title":"Concatenation of arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#splitting-of-arrays","text":"The opposite of concatenation is splitting, which is implemented by the functions np.split, np.hsplit, and np.vsplit. For each of these, we can pass a list of indices giving the split points: x = [ 1 , 2 , 3 , 99 , 100 , 101 , 3 , 2 , 1 ] x1 , x2 , x3 = np . split ( x ,[ 3 , 5 ]) # split will be at index 3 & 5 x1 array([1, 2, 3]) x2 array([ 99, 100]) x3 #Notice that N split points lead to N + 1 subarrays. #The related functions np.hsplit and np.vsplit are similar: array([101, 3, 2, 1]) grid = np . arange ( 16 ) . reshape (( 4 , 4 )) upper , lower = np . vsplit ( grid ,[ 2 ]) upper array([[0, 1, 2, 3], [4, 5, 6, 7]]) lower array([[ 8, 9, 10, 11], [12, 13, 14, 15]]) grid array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) left , right = np . hsplit ( grid ,[ 2 ]) left array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13]]) right array([[ 2, 3], [ 6, 7], [10, 11], [14, 15]])","title":"Splitting of arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/","text":"================ by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 02 - Computation on NumPy Arrays: Universal Functions \u00b6 Computation on NumPy Arrays: Universal Functions The Slowness of Loops Introducing UFuncs Exploring NumPy\u2019s UFuncs Array arithmetic Specialized ufuncs Advanced Ufunc Features Specifying output Aggregates Outer products Computation on NumPy Arrays: Universal Functions \u00b6 Computation on NumPy arrays can be very fast, or it can be very slow. The key to making it fast is to use vectorized operations, generally implemented through Num\u2010 Py\u2019s universal functions (ufuncs). The Slowness of Loops \u00b6 Python\u2019s default implementation (known as CPython) does some operations very slowly. This is in part due to the dynamic, interpreted nature of the language: the fact that types are flexible, so that sequences of operations cannot be compiled down to efficient machine code as in languages like C and Fortran. # Example of reciprocal of each item in the list import numpy as np np . random . seed ( 0 ) def compute_reciprocals ( values ): output = np . empty ( len ( values )) for i in range ( len ( values )): output [ i ] = 1.0 / values [ i ] return output values = np . random . randint ( 1 , 10 , size = 5 ) compute_reciprocals ( values ) array([0.16666667, 1. , 0.25 , 0.25 , 0.125 ]) big_array = np . random . randint ( 1 , 100 , size = 100000 ) % timeit compute_reciprocals ( big_array ) 167 ms \u00b1 5.95 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Introducing UFuncs \u00b6 For many types of operations, NumPy provides a convenient interface into just this kind of statically typed, compiled routine. This is known as a vectorized operation. You can accomplish this by simply performing an operation on the array, which will then be applied to each element. This vectorized approach is designed to push the loop into the compiled layer that underlies NumPy, leading to much faster execution. print ( compute_reciprocals ( values )) print ( 1.0 / values ) [0.16666667 1. 0.25 0.25 0.125 ] [0.16666667 1. 0.25 0.25 0.125 ] % timeit ( 1 / big_array ) 129 \u00b5s \u00b1 12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Vectorized operations in NumPy are implemented via ufuncs, whose main purpose is to quickly execute repeated operations on values in NumPy arrays. Ufuncs are extremely flexible\u2014before we saw an operation between a scalar and an array, but we can also operate between two arrays: np . arange ( 5 ) / np . arange ( 1 , 6 ) array([0. , 0.5 , 0.66666667, 0.75 , 0.8 ]) # Works for multi-d arraays x = np . arange ( 9 ) . reshape (( 3 , 3 )) 2 * x array([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]]) Exploring NumPy\u2019s UFuncs \u00b6 Array arithmetic \u00b6 print ( 'x = ' , x ) x = [[0 1 2] [3 4 5] [6 7 8]] print ( 'x-3 \\n ' , np . subtract ( x , 3 )) print ( 'x+13 \\n ' , np . add ( x , 13 )) print ( 'x*9 \\n ' , np . multiply ( x , 9 )) print ( 'x/6 \\n ' , np . subtract ( x , 6 )) print ( 'x//6 \\n ' , np . subtract ( x , 6 )) print ( 'x**4 \\n ' , np . power ( x , 4 )) print ( 'x%4 \\n ' , np . mod ( x , 4 )) x-3 [[-3 -2 -1] [ 0 1 2] [ 3 4 5]] x+13 [[13 14 15] [16 17 18] [19 20 21]] x*9 [[ 0 9 18] [27 36 45] [54 63 72]] x/6 [[-6 -5 -4] [-3 -2 -1] [ 0 1 2]] x//6 [[-6 -5 -4] [-3 -2 -1] [ 0 1 2]] x**4 [[ 0 1 16] [ 81 256 625] [1296 2401 4096]] x%4 [[0 1 2] [3 0 1] [2 3 0]] # Absolute value x = np . array ([ - 2 , 3 , - 4 , - 9 , 0 ]) abs ( x ) array([2, 3, 4, 9, 0]) np . absolute ( x ) array([2, 3, 4, 9, 0]) # Trig thet = np . linspace ( 0 , np . pi , 3 ) print ( \"Theta =\" , thet ) print ( \"Sin(theta) =\" , np . sin ( thet )) print ( \"Cos(theta) =\" , np . cos ( thet )) print ( \" \\n Inverse tri \\n \" ) print ( \"Theta =\" , thet ) print ( \"arcSin(theta) =\" , np . arcsin ( thet )) print ( \"arcCos(theta) =\" , np . arccos ( thet )) Theta = [0. 1.57079633 3.14159265] Sin(theta) = [0.0000000e+00 1.0000000e+00 1.2246468e-16] Cos(theta) = [ 1.000000e+00 6.123234e-17 -1.000000e+00] Inverse tri Theta = [0. 1.57079633 3.14159265] arcSin(theta) = [ 0. nan nan] arcCos(theta) = [1.57079633 nan nan] /tmp/ipykernel_22625/2951825817.py:9: RuntimeWarning: invalid value encountered in arcsin print(\"arcSin(theta) =\",np.arcsin(thet)) /tmp/ipykernel_22625/2951825817.py:10: RuntimeWarning: invalid value encountered in arccos print(\"arcCos(theta) =\",np.arccos(thet)) # Logarthm and Exponents x = [ 1 , 2 , 3 ] print ( \"x=\" , x ) print ( \"e^x=\" , np . exp ( x )) print ( \"2^x=\" , np . exp2 ( x )) print ( \"3^x=\" , np . power ( 3 , x )) x= [1, 2, 3] e^x= [ 2.71828183 7.3890561 20.08553692] 2^x= [2. 4. 8.] 3^x= [ 3 9 27] Specialized ufuncs \u00b6 NumPy has many more ufuncs available, including hyperbolic trig functions, bitwise arithmetic, comparison operators, conversions from radians to degrees, rounding and remainders, and much more. A look through the NumPy documentation reveals a lot of interesting functionality. Another excellent source for more specialized and obscure ufuncs is the submodule scipy.special. If you want to compute some obscure mathematical function on your data, chances are it is implemented in scipy.special. There are far too many functions to list them all, but the following snippet shows a couple that might come up in a statistics context from scipy import special #Gamma functions (generalized factorials) and related functions x = [ 1 , 5 , 10 ] print ( \"gamma(x)=\" , special . gamma ( x )) print ( \"ln|gamma(x)| =\" , special . gammaln ( x )) print ( \"beta(x, 2)=\" , special . beta ( x , 2 )) gamma(x)= [1.0000e+00 2.4000e+01 3.6288e+05] ln|gamma(x)| = [ 0. 3.17805383 12.80182748] beta(x, 2)= [0.5 0.03333333 0.00909091] # Error function (integral of Gaussian) # its complement, and its inverse x = np . array ([ 0 , 0.3 , 0.7 , 1.0 ]) print ( \"erf(x) =\" , special . erf ( x )) print ( \"erfc(x) =\" , special . erfc ( x )) print ( \"erfinv(x) =\" , special . erfinv ( x )) erf(x) = [0. 0.32862676 0.67780119 0.84270079] erfc(x) = [1. 0.67137324 0.32219881 0.15729921] erfinv(x) = [0. 0.27246271 0.73286908 inf] Advanced Ufunc Features \u00b6 Specifying output \u00b6 For large calculations, it is sometimes useful to be able to specify the array where the result of the calculation will be stored. Rather than creating a temporary array, you can use this to write computation results directly to the memory location where you\u2019d like them to be. # THis can be done using the out argument of the function x = np . arange ( 5 ) y = np . empty ( 5 ) np . multiply ( x , 10 , out = y ) print ( y ) [ 0. 10. 20. 30. 40.] # This can be done with array views y = np . zeros ( 10 ) np . power ( 2 , x , out = y [:: 2 ]) print ( y ) [ 1. 0. 2. 0. 4. 0. 8. 0. 16. 0.] np . power ( 2 , x , out = 2 ** x ) array([ 1, 2, 4, 8, 16]) Aggregates \u00b6 For binary ufuncs, there are some interesting aggregates that can be computed directly from the object. For example, if we\u2019d like to reduce an array with a particular operation, we can use the reduce method of any ufunc. A reduce repeatedly applies a given operation to the elements of an array until only a single result remains. # Calling reduce on the add ufunc x = np . arange ( 1 , 6 ) np . add . reduce ( x ) 15 x = np . arange ( 1 , 6 ) np . sum ( x ) 15 np . multiply ( x ) TypeError: multiply() takes from 2 to 3 positional arguments but 1 were given np . add ( x ) TypeError: add() takes from 2 to 3 positional arguments but 1 were given np . multiply . reduce ( x ) 120 # To store all the imtermmediate results of the computation we can use accumulate np . add . accumulate ( x ) array([ 1, 3, 6, 10, 15]) x array([1, 2, 3, 4, 5]) Note that for these particular cases, there are dedicated NumPy functions to compute the results (np.sum, np.prod, np.cumsum, np.cumprod), The ufunc.at and ufunc.reduceat methods Outer products \u00b6 Finally, any ufunc can compute the output of all pairs of two different inputs using the outer method. np . multiply . outer ( x , x ) array([[ 1, 2, 3, 4, 5], [ 2, 4, 6, 8, 10], [ 3, 6, 9, 12, 15], [ 4, 8, 12, 16, 20], [ 5, 10, 15, 20, 25]]) Another extremely useful feature of ufuncs is the ability to operate between arrays of different sizes and shapes, a set of operations known as broadcasting. Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"02 Computation on NumPy Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#02-computation-on-numpy-arrays-universal-functions","text":"Computation on NumPy Arrays: Universal Functions The Slowness of Loops Introducing UFuncs Exploring NumPy\u2019s UFuncs Array arithmetic Specialized ufuncs Advanced Ufunc Features Specifying output Aggregates Outer products","title":"02 - Computation on NumPy Arrays: Universal Functions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#computation-on-numpy-arrays-universal-functions","text":"Computation on NumPy arrays can be very fast, or it can be very slow. The key to making it fast is to use vectorized operations, generally implemented through Num\u2010 Py\u2019s universal functions (ufuncs).","title":"Computation on NumPy Arrays: Universal Functions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#the-slowness-of-loops","text":"Python\u2019s default implementation (known as CPython) does some operations very slowly. This is in part due to the dynamic, interpreted nature of the language: the fact that types are flexible, so that sequences of operations cannot be compiled down to efficient machine code as in languages like C and Fortran. # Example of reciprocal of each item in the list import numpy as np np . random . seed ( 0 ) def compute_reciprocals ( values ): output = np . empty ( len ( values )) for i in range ( len ( values )): output [ i ] = 1.0 / values [ i ] return output values = np . random . randint ( 1 , 10 , size = 5 ) compute_reciprocals ( values ) array([0.16666667, 1. , 0.25 , 0.25 , 0.125 ]) big_array = np . random . randint ( 1 , 100 , size = 100000 ) % timeit compute_reciprocals ( big_array ) 167 ms \u00b1 5.95 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)","title":"The Slowness of Loops"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#introducing-ufuncs","text":"For many types of operations, NumPy provides a convenient interface into just this kind of statically typed, compiled routine. This is known as a vectorized operation. You can accomplish this by simply performing an operation on the array, which will then be applied to each element. This vectorized approach is designed to push the loop into the compiled layer that underlies NumPy, leading to much faster execution. print ( compute_reciprocals ( values )) print ( 1.0 / values ) [0.16666667 1. 0.25 0.25 0.125 ] [0.16666667 1. 0.25 0.25 0.125 ] % timeit ( 1 / big_array ) 129 \u00b5s \u00b1 12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Vectorized operations in NumPy are implemented via ufuncs, whose main purpose is to quickly execute repeated operations on values in NumPy arrays. Ufuncs are extremely flexible\u2014before we saw an operation between a scalar and an array, but we can also operate between two arrays: np . arange ( 5 ) / np . arange ( 1 , 6 ) array([0. , 0.5 , 0.66666667, 0.75 , 0.8 ]) # Works for multi-d arraays x = np . arange ( 9 ) . reshape (( 3 , 3 )) 2 * x array([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]])","title":"Introducing UFuncs"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#exploring-numpys-ufuncs","text":"","title":"Exploring NumPy\u2019s UFuncs"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#array-arithmetic","text":"print ( 'x = ' , x ) x = [[0 1 2] [3 4 5] [6 7 8]] print ( 'x-3 \\n ' , np . subtract ( x , 3 )) print ( 'x+13 \\n ' , np . add ( x , 13 )) print ( 'x*9 \\n ' , np . multiply ( x , 9 )) print ( 'x/6 \\n ' , np . subtract ( x , 6 )) print ( 'x//6 \\n ' , np . subtract ( x , 6 )) print ( 'x**4 \\n ' , np . power ( x , 4 )) print ( 'x%4 \\n ' , np . mod ( x , 4 )) x-3 [[-3 -2 -1] [ 0 1 2] [ 3 4 5]] x+13 [[13 14 15] [16 17 18] [19 20 21]] x*9 [[ 0 9 18] [27 36 45] [54 63 72]] x/6 [[-6 -5 -4] [-3 -2 -1] [ 0 1 2]] x//6 [[-6 -5 -4] [-3 -2 -1] [ 0 1 2]] x**4 [[ 0 1 16] [ 81 256 625] [1296 2401 4096]] x%4 [[0 1 2] [3 0 1] [2 3 0]] # Absolute value x = np . array ([ - 2 , 3 , - 4 , - 9 , 0 ]) abs ( x ) array([2, 3, 4, 9, 0]) np . absolute ( x ) array([2, 3, 4, 9, 0]) # Trig thet = np . linspace ( 0 , np . pi , 3 ) print ( \"Theta =\" , thet ) print ( \"Sin(theta) =\" , np . sin ( thet )) print ( \"Cos(theta) =\" , np . cos ( thet )) print ( \" \\n Inverse tri \\n \" ) print ( \"Theta =\" , thet ) print ( \"arcSin(theta) =\" , np . arcsin ( thet )) print ( \"arcCos(theta) =\" , np . arccos ( thet )) Theta = [0. 1.57079633 3.14159265] Sin(theta) = [0.0000000e+00 1.0000000e+00 1.2246468e-16] Cos(theta) = [ 1.000000e+00 6.123234e-17 -1.000000e+00] Inverse tri Theta = [0. 1.57079633 3.14159265] arcSin(theta) = [ 0. nan nan] arcCos(theta) = [1.57079633 nan nan] /tmp/ipykernel_22625/2951825817.py:9: RuntimeWarning: invalid value encountered in arcsin print(\"arcSin(theta) =\",np.arcsin(thet)) /tmp/ipykernel_22625/2951825817.py:10: RuntimeWarning: invalid value encountered in arccos print(\"arcCos(theta) =\",np.arccos(thet)) # Logarthm and Exponents x = [ 1 , 2 , 3 ] print ( \"x=\" , x ) print ( \"e^x=\" , np . exp ( x )) print ( \"2^x=\" , np . exp2 ( x )) print ( \"3^x=\" , np . power ( 3 , x )) x= [1, 2, 3] e^x= [ 2.71828183 7.3890561 20.08553692] 2^x= [2. 4. 8.] 3^x= [ 3 9 27]","title":"Array arithmetic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#specialized-ufuncs","text":"NumPy has many more ufuncs available, including hyperbolic trig functions, bitwise arithmetic, comparison operators, conversions from radians to degrees, rounding and remainders, and much more. A look through the NumPy documentation reveals a lot of interesting functionality. Another excellent source for more specialized and obscure ufuncs is the submodule scipy.special. If you want to compute some obscure mathematical function on your data, chances are it is implemented in scipy.special. There are far too many functions to list them all, but the following snippet shows a couple that might come up in a statistics context from scipy import special #Gamma functions (generalized factorials) and related functions x = [ 1 , 5 , 10 ] print ( \"gamma(x)=\" , special . gamma ( x )) print ( \"ln|gamma(x)| =\" , special . gammaln ( x )) print ( \"beta(x, 2)=\" , special . beta ( x , 2 )) gamma(x)= [1.0000e+00 2.4000e+01 3.6288e+05] ln|gamma(x)| = [ 0. 3.17805383 12.80182748] beta(x, 2)= [0.5 0.03333333 0.00909091] # Error function (integral of Gaussian) # its complement, and its inverse x = np . array ([ 0 , 0.3 , 0.7 , 1.0 ]) print ( \"erf(x) =\" , special . erf ( x )) print ( \"erfc(x) =\" , special . erfc ( x )) print ( \"erfinv(x) =\" , special . erfinv ( x )) erf(x) = [0. 0.32862676 0.67780119 0.84270079] erfc(x) = [1. 0.67137324 0.32219881 0.15729921] erfinv(x) = [0. 0.27246271 0.73286908 inf]","title":"Specialized ufuncs"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#advanced-ufunc-features","text":"","title":"Advanced Ufunc Features"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#specifying-output","text":"For large calculations, it is sometimes useful to be able to specify the array where the result of the calculation will be stored. Rather than creating a temporary array, you can use this to write computation results directly to the memory location where you\u2019d like them to be. # THis can be done using the out argument of the function x = np . arange ( 5 ) y = np . empty ( 5 ) np . multiply ( x , 10 , out = y ) print ( y ) [ 0. 10. 20. 30. 40.] # This can be done with array views y = np . zeros ( 10 ) np . power ( 2 , x , out = y [:: 2 ]) print ( y ) [ 1. 0. 2. 0. 4. 0. 8. 0. 16. 0.] np . power ( 2 , x , out = 2 ** x ) array([ 1, 2, 4, 8, 16])","title":"Specifying output"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#aggregates","text":"For binary ufuncs, there are some interesting aggregates that can be computed directly from the object. For example, if we\u2019d like to reduce an array with a particular operation, we can use the reduce method of any ufunc. A reduce repeatedly applies a given operation to the elements of an array until only a single result remains. # Calling reduce on the add ufunc x = np . arange ( 1 , 6 ) np . add . reduce ( x ) 15 x = np . arange ( 1 , 6 ) np . sum ( x ) 15 np . multiply ( x ) TypeError: multiply() takes from 2 to 3 positional arguments but 1 were given np . add ( x ) TypeError: add() takes from 2 to 3 positional arguments but 1 were given np . multiply . reduce ( x ) 120 # To store all the imtermmediate results of the computation we can use accumulate np . add . accumulate ( x ) array([ 1, 3, 6, 10, 15]) x array([1, 2, 3, 4, 5]) Note that for these particular cases, there are dedicated NumPy functions to compute the results (np.sum, np.prod, np.cumsum, np.cumprod), The ufunc.at and ufunc.reduceat methods","title":"Aggregates"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#outer-products","text":"Finally, any ufunc can compute the output of all pairs of two different inputs using the outer method. np . multiply . outer ( x , x ) array([[ 1, 2, 3, 4, 5], [ 2, 4, 6, 8, 10], [ 3, 6, 9, 12, 15], [ 4, 8, 12, 16, 20], [ 5, 10, 15, 20, 25]]) Another extremely useful feature of ufuncs is the ability to operate between arrays of different sizes and shapes, a set of operations known as broadcasting.","title":"Outer products"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/","text":"================ by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 03 - Aggregations: Min, Max, and Everything in Between \u00b6 Aggregations: Min, Max, and Everything in Between Multidimensional aggregates Some aggregation functions Example: What Is the Average Height of US Presidents? Aggregations: Min, Max, and Everything in Between \u00b6 Often when you are faced with a large amount of data, a first step is to compute sum\u2010 mary statistics for the data in question. Perhaps the most common summary statistics are the mean and standard deviation, which allow you to summarize the \u201ctypical\u201d val\u2010 ues in a dataset, but other aggregates are useful as well (the sum, product, median, minimum and maximum, quantiles, etc.). l = np . random . random ( 100 ) sum ( l ) 47.51294159911191 np . sum ( l ) 47.5129415991119 big_array = np . random . rand ( 100000 ) % timeit sum ( big_array ) % timeit np . sum ( big_array ) 6.73 ms \u00b1 88.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 31.5 \u00b5s \u00b1 326 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Be careful, though: the sum function and the np.sum function are not identical, which can sometimes lead to confusion! In particular, their optional arguments have differ\u2010 ent meanings, and np.sum is aware of multiple array dimensions min ( big_array ), max ( big_array ) (1.4444103570987465e-06, 0.9999881721555508) np . min ( big_array ), np . max ( big_array ) (1.4444103570987465e-06, 0.9999881721555508) % timeit min ( big_array ) % timeit np . min ( big_array ) 5.44 ms \u00b1 49.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 35.5 \u00b5s \u00b1 64.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Multidimensional aggregates \u00b6 m = np . random . random (( 3 , 4 )) print ( m ) [[0.15322668 0.10058762 0.85504471 0.19779527] [0.24515716 0.61526756 0.9677193 0.0045308 ] [0.57826711 0.49512073 0.68039294 0.43271134]] m . sum () 5.325821234912031 # Aggregation functions take an additional argument specifying the axis along which the aggregate is computed. m . min ( axis = 0 ) array([0.15322668, 0.10058762, 0.68039294, 0.0045308 ]) np . max ( m ) 0.9677193034993363 m . max ( axis = 0 ) array([0.57826711, 0.61526756, 0.9677193 , 0.43271134]) m . min ( axis = 1 ) array([0.10058762, 0.0045308 , 0.43271134]) The way the axis is specified here can be confusing to users coming from other lan\u2010 guages. The axis keyword specifies the dimension of the array that will be collapsed, rather than the dimension that will be returned. So specifying axis=0 means that the The way the axis is specified here can be confusing to users coming from other lan\u2010 guages. The axis keyword specifies the dimension of the array that will be collapsed, rather than the dimension that will be returned. So specifying axis=0 means that the first axis will be collapsed: for two-dimensional arrays, this means that values within each column will be aggregated. Some aggregation functions \u00b6 Example: What Is the Average Height of US Presidents? \u00b6 Aggregates available in NumPy can be extremely useful for summarizing a set of val\u2010 ues. As a simple example, let\u2019s consider the heights of all US presidents. ! head - 4 ../ data / president_heights . csv order,name,height(cm) 1,George Washington,189 2,John Adams,170 3,Thomas Jefferson,189 import pandas as pd data = pd . read_csv ( \"../data/president_heights.csv\" ) height = np . array ( data [ 'height(cm)' ]) print ( height ) [189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173 174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183 177 185 188 188 182 185] data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order name height(cm) 0 1 George Washington 189 1 2 John Adams 170 2 3 Thomas Jefferson 189 3 4 James Madison 163 4 5 James Monroe 183 print ( f \"Mean Height = \" , np . mean ( height )) print ( f \"St.dv Height = \" , np . std ( height )) Mean Height = 179.73809523809524 St.dv Height = 6.931843442745892 print ( f \"Mean Height = \" , height . mean ()) print ( f \"St.dv Height = \" , height . std ()) Mean Height = 179.73809523809524 St.dv Height = 6.931843442745892 print ( f \"Max Height = \" , np . max ( height )) print ( f \"Min Height = \" , np . min ( height )) Max Height = 193 Min Height = 163 print ( f \"25th precentile Height = \" , np . percentile ( height , 25 )) print ( f \"Median Height = \" , np . median ( height )) print ( f \"75th Percentile = \" , np . percentile ( height , 75 )) 25th precentile Height = 174.25 Median Height = 182.0 75th Percentile = 183.0 We see that the median height of US presidents is 182 cm, or just shy of six feet % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . hist ( heights ) plt . title ( 'Height Distribution of US Presidents' ) plt . xlabel ( 'height (cm)' ) plt . ylabel ( 'number' );","title":"03 Aggregation Min Max"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#03-aggregations-min-max-and-everything-in-between","text":"Aggregations: Min, Max, and Everything in Between Multidimensional aggregates Some aggregation functions Example: What Is the Average Height of US Presidents?","title":"03 - Aggregations: Min, Max, and Everything in Between"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#aggregations-min-max-and-everything-in-between","text":"Often when you are faced with a large amount of data, a first step is to compute sum\u2010 mary statistics for the data in question. Perhaps the most common summary statistics are the mean and standard deviation, which allow you to summarize the \u201ctypical\u201d val\u2010 ues in a dataset, but other aggregates are useful as well (the sum, product, median, minimum and maximum, quantiles, etc.). l = np . random . random ( 100 ) sum ( l ) 47.51294159911191 np . sum ( l ) 47.5129415991119 big_array = np . random . rand ( 100000 ) % timeit sum ( big_array ) % timeit np . sum ( big_array ) 6.73 ms \u00b1 88.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 31.5 \u00b5s \u00b1 326 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Be careful, though: the sum function and the np.sum function are not identical, which can sometimes lead to confusion! In particular, their optional arguments have differ\u2010 ent meanings, and np.sum is aware of multiple array dimensions min ( big_array ), max ( big_array ) (1.4444103570987465e-06, 0.9999881721555508) np . min ( big_array ), np . max ( big_array ) (1.4444103570987465e-06, 0.9999881721555508) % timeit min ( big_array ) % timeit np . min ( big_array ) 5.44 ms \u00b1 49.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 35.5 \u00b5s \u00b1 64.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)","title":"Aggregations: Min, Max, and Everything in Between"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#multidimensional-aggregates","text":"m = np . random . random (( 3 , 4 )) print ( m ) [[0.15322668 0.10058762 0.85504471 0.19779527] [0.24515716 0.61526756 0.9677193 0.0045308 ] [0.57826711 0.49512073 0.68039294 0.43271134]] m . sum () 5.325821234912031 # Aggregation functions take an additional argument specifying the axis along which the aggregate is computed. m . min ( axis = 0 ) array([0.15322668, 0.10058762, 0.68039294, 0.0045308 ]) np . max ( m ) 0.9677193034993363 m . max ( axis = 0 ) array([0.57826711, 0.61526756, 0.9677193 , 0.43271134]) m . min ( axis = 1 ) array([0.10058762, 0.0045308 , 0.43271134]) The way the axis is specified here can be confusing to users coming from other lan\u2010 guages. The axis keyword specifies the dimension of the array that will be collapsed, rather than the dimension that will be returned. So specifying axis=0 means that the The way the axis is specified here can be confusing to users coming from other lan\u2010 guages. The axis keyword specifies the dimension of the array that will be collapsed, rather than the dimension that will be returned. So specifying axis=0 means that the first axis will be collapsed: for two-dimensional arrays, this means that values within each column will be aggregated.","title":"Multidimensional aggregates"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#some-aggregation-functions","text":"","title":"Some aggregation functions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#example-what-is-the-average-height-of-us-presidents","text":"Aggregates available in NumPy can be extremely useful for summarizing a set of val\u2010 ues. As a simple example, let\u2019s consider the heights of all US presidents. ! head - 4 ../ data / president_heights . csv order,name,height(cm) 1,George Washington,189 2,John Adams,170 3,Thomas Jefferson,189 import pandas as pd data = pd . read_csv ( \"../data/president_heights.csv\" ) height = np . array ( data [ 'height(cm)' ]) print ( height ) [189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173 174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183 177 185 188 188 182 185] data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order name height(cm) 0 1 George Washington 189 1 2 John Adams 170 2 3 Thomas Jefferson 189 3 4 James Madison 163 4 5 James Monroe 183 print ( f \"Mean Height = \" , np . mean ( height )) print ( f \"St.dv Height = \" , np . std ( height )) Mean Height = 179.73809523809524 St.dv Height = 6.931843442745892 print ( f \"Mean Height = \" , height . mean ()) print ( f \"St.dv Height = \" , height . std ()) Mean Height = 179.73809523809524 St.dv Height = 6.931843442745892 print ( f \"Max Height = \" , np . max ( height )) print ( f \"Min Height = \" , np . min ( height )) Max Height = 193 Min Height = 163 print ( f \"25th precentile Height = \" , np . percentile ( height , 25 )) print ( f \"Median Height = \" , np . median ( height )) print ( f \"75th Percentile = \" , np . percentile ( height , 75 )) 25th precentile Height = 174.25 Median Height = 182.0 75th Percentile = 183.0 We see that the median height of US presidents is 182 cm, or just shy of six feet % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . hist ( heights ) plt . title ( 'Height Distribution of US Presidents' ) plt . xlabel ( 'height (cm)' ) plt . ylabel ( 'number' );","title":"Example: What Is the Average Height of US Presidents?"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/","text":"================ by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 04 - Computation on Arrays: Broadcasting \u00b6 Computation on Arrays: Broadcasting Introducing Broadcasting Rules of Broadcasting Broadcasting in Practice Centering an array Plotting a two-dimensional function Computation on Arrays: Broadcasting \u00b6 We saw in the previous section how NumPy\u2019s universal functions can be used to vec\u2010 torize operations and thereby remove slow Python loops. Another means of vectoriz\u2010 ing operations is to use NumPy\u2019s broadcasting functionality. Broadcasting is simply a set of rules for applying binary ufuncs (addition, subtraction, multiplication, etc.) on arrays of different sizes. Introducing Broadcasting \u00b6 Recall that for arrays of the same size, binary operations are performed on an element-by-element basis: import numpy as np a = np . array ([ 0 , 2 , 3 , 4 , 5 ]) b = np . array ([ 7 , 8 , 9 , 9 , 6 ]) a + b array([ 7, 10, 12, 13, 11]) Broadcasting allows these types of binary operations to be performed on arrays of dif\u2010 ferent sizes\u2014for example, we can just as easily add a scalar (think of it as a zero- dimensional array) to an array a + 5 array([ 5, 7, 8, 9, 10]) We can think of this as an operation that stretches or duplicates the value 5 into the array [5, 5, 5] , and adds the results. The advantage of NumPy\u2019s broadcasting is that this duplication of values does not actually take place, but it is a useful mental model as we think about broadcasting. _ b - 9 array([-2, -1, 0, 0, -3]) b * 10 array([70, 80, 90, 90, 60]) a / 3 array([0. , 0.66666667, 1. , 1.33333333, 1.66666667]) m = np . ones (( 3 , 3 )) m array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) m + a ValueError: operands could not be broadcast together with shapes (3,3) (5,) m = np . ones (( 5 , 5 )) m array([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) m + a array([[1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.]]) Here the one-dimensional array a is stretched, or broadcast, across the second dimension in order to match the shape of M. # Broadcasting of both arrays a = np . arange ( 3 ) b = np . arange ( 3 )[:, np . newaxis ] a array([0, 1, 2]) b array([[0], [1], [2]]) a + b array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) b + a array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) Here the one-dimensional array a is stretched, or broadcast, across the second dimension in order to match the shape of M. Rules of Broadcasting \u00b6 Broadcasting in NumPy follows a strict set of rules to determine the interaction between the two arrays: * Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side. * Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape. * Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised. m = np . ones (( 2 , 3 )) a = np . arange ( 3 ) m , a (array([[1., 1., 1.], [1., 1., 1.]]), array([0, 1, 2])) m + a array([[1., 2., 3.], [1., 2., 3.]]) a = np . arange ( 3 ) . reshape (( 3 , 1 )) b = np . arange ( 3 ) a array([[0], [1], [2]]) b array([0, 1, 2]) a + b array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) Broadcasting in Practice \u00b6 Centering an array \u00b6 x = np . random . random (( 10 , 3 )) x array([[0.83823659, 0.53520822, 0.88885158], [0.89147897, 0.41948707, 0.42342134], [0.96876136, 0.46312611, 0.31711275], [0.47652192, 0.25392243, 0.25454173], [0.6239708 , 0.97644488, 0.97034953], [0.33707785, 0.24628063, 0.25686595], [0.12018414, 0.77360894, 0.19437752], [0.90201827, 0.18844719, 0.99512986], [0.84278929, 0.69803136, 0.53420956], [0.13620165, 0.90381885, 0.78541803]]) x_mean = x . mean ( 0 ) x_mean array([0.61372408, 0.54583757, 0.56202779]) # center the x arry by subtracting the mean (a boradcasting operation) x_centered = x - x_mean x_centered array([[ 0.22451251, -0.01062935, 0.3268238 ], [ 0.27775488, -0.1263505 , -0.13860644], [ 0.35503727, -0.08271145, -0.24491504], [-0.13720216, -0.29191514, -0.30748605], [ 0.01024672, 0.43060732, 0.40832174], [-0.27664623, -0.29955694, -0.30516184], [-0.49353995, 0.22777137, -0.36765026], [ 0.28829418, -0.35739038, 0.43310208], [ 0.22906521, 0.15219379, -0.02781823], [-0.47752244, 0.35798128, 0.22339024]]) Plotting a two-dimensional function \u00b6 One place that broadcasting is very useful is in displaying images based on two- dimensional functions. If we want to define a function z = f(x, y), broadcasting can be used to compute the function across the gri x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 50 )[:, np . newaxis ] x array([0. , 0.10204082, 0.20408163, 0.30612245, 0.40816327, 0.51020408, 0.6122449 , 0.71428571, 0.81632653, 0.91836735, 1.02040816, 1.12244898, 1.2244898 , 1.32653061, 1.42857143, 1.53061224, 1.63265306, 1.73469388, 1.83673469, 1.93877551, 2.04081633, 2.14285714, 2.24489796, 2.34693878, 2.44897959, 2.55102041, 2.65306122, 2.75510204, 2.85714286, 2.95918367, 3.06122449, 3.16326531, 3.26530612, 3.36734694, 3.46938776, 3.57142857, 3.67346939, 3.7755102 , 3.87755102, 3.97959184, 4.08163265, 4.18367347, 4.28571429, 4.3877551 , 4.48979592, 4.59183673, 4.69387755, 4.79591837, 4.89795918, 5. ]) y array([[0. ], [0.10204082], [0.20408163], [0.30612245], [0.40816327], [0.51020408], [0.6122449 ], [0.71428571], [0.81632653], [0.91836735], [1.02040816], [1.12244898], [1.2244898 ], [1.32653061], [1.42857143], [1.53061224], [1.63265306], [1.73469388], [1.83673469], [1.93877551], [2.04081633], [2.14285714], [2.24489796], [2.34693878], [2.44897959], [2.55102041], [2.65306122], [2.75510204], [2.85714286], [2.95918367], [3.06122449], [3.16326531], [3.26530612], [3.36734694], [3.46938776], [3.57142857], [3.67346939], [3.7755102 ], [3.87755102], [3.97959184], [4.08163265], [4.18367347], [4.28571429], [4.3877551 ], [4.48979592], [4.59183673], [4.69387755], [4.79591837], [4.89795918], [5. ]]) z = np . sin ( x ) ** 10 + np . cos ( 10 + x * y ) * np . cos ( x ) % matplotlib inline import matplotlib.pyplot as plt plt . imshow ( z , origin = 'lower' , extent = [ 0 , 5 , 0 , 5 ], cmap = 'viridis' ) plt . colorbar ();","title":"04 Computation on Arrays Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#04-computation-on-arrays-broadcasting","text":"Computation on Arrays: Broadcasting Introducing Broadcasting Rules of Broadcasting Broadcasting in Practice Centering an array Plotting a two-dimensional function","title":"04 - Computation on Arrays: Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#computation-on-arrays-broadcasting","text":"We saw in the previous section how NumPy\u2019s universal functions can be used to vec\u2010 torize operations and thereby remove slow Python loops. Another means of vectoriz\u2010 ing operations is to use NumPy\u2019s broadcasting functionality. Broadcasting is simply a set of rules for applying binary ufuncs (addition, subtraction, multiplication, etc.) on arrays of different sizes.","title":"Computation on Arrays: Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#introducing-broadcasting","text":"Recall that for arrays of the same size, binary operations are performed on an element-by-element basis: import numpy as np a = np . array ([ 0 , 2 , 3 , 4 , 5 ]) b = np . array ([ 7 , 8 , 9 , 9 , 6 ]) a + b array([ 7, 10, 12, 13, 11]) Broadcasting allows these types of binary operations to be performed on arrays of dif\u2010 ferent sizes\u2014for example, we can just as easily add a scalar (think of it as a zero- dimensional array) to an array a + 5 array([ 5, 7, 8, 9, 10]) We can think of this as an operation that stretches or duplicates the value 5 into the array [5, 5, 5] , and adds the results. The advantage of NumPy\u2019s broadcasting is that this duplication of values does not actually take place, but it is a useful mental model as we think about broadcasting. _ b - 9 array([-2, -1, 0, 0, -3]) b * 10 array([70, 80, 90, 90, 60]) a / 3 array([0. , 0.66666667, 1. , 1.33333333, 1.66666667]) m = np . ones (( 3 , 3 )) m array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) m + a ValueError: operands could not be broadcast together with shapes (3,3) (5,) m = np . ones (( 5 , 5 )) m array([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) m + a array([[1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.]]) Here the one-dimensional array a is stretched, or broadcast, across the second dimension in order to match the shape of M. # Broadcasting of both arrays a = np . arange ( 3 ) b = np . arange ( 3 )[:, np . newaxis ] a array([0, 1, 2]) b array([[0], [1], [2]]) a + b array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) b + a array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) Here the one-dimensional array a is stretched, or broadcast, across the second dimension in order to match the shape of M.","title":"Introducing Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#rules-of-broadcasting","text":"Broadcasting in NumPy follows a strict set of rules to determine the interaction between the two arrays: * Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side. * Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape. * Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised. m = np . ones (( 2 , 3 )) a = np . arange ( 3 ) m , a (array([[1., 1., 1.], [1., 1., 1.]]), array([0, 1, 2])) m + a array([[1., 2., 3.], [1., 2., 3.]]) a = np . arange ( 3 ) . reshape (( 3 , 1 )) b = np . arange ( 3 ) a array([[0], [1], [2]]) b array([0, 1, 2]) a + b array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])","title":"Rules of Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#broadcasting-in-practice","text":"","title":"Broadcasting in Practice"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#centering-an-array","text":"x = np . random . random (( 10 , 3 )) x array([[0.83823659, 0.53520822, 0.88885158], [0.89147897, 0.41948707, 0.42342134], [0.96876136, 0.46312611, 0.31711275], [0.47652192, 0.25392243, 0.25454173], [0.6239708 , 0.97644488, 0.97034953], [0.33707785, 0.24628063, 0.25686595], [0.12018414, 0.77360894, 0.19437752], [0.90201827, 0.18844719, 0.99512986], [0.84278929, 0.69803136, 0.53420956], [0.13620165, 0.90381885, 0.78541803]]) x_mean = x . mean ( 0 ) x_mean array([0.61372408, 0.54583757, 0.56202779]) # center the x arry by subtracting the mean (a boradcasting operation) x_centered = x - x_mean x_centered array([[ 0.22451251, -0.01062935, 0.3268238 ], [ 0.27775488, -0.1263505 , -0.13860644], [ 0.35503727, -0.08271145, -0.24491504], [-0.13720216, -0.29191514, -0.30748605], [ 0.01024672, 0.43060732, 0.40832174], [-0.27664623, -0.29955694, -0.30516184], [-0.49353995, 0.22777137, -0.36765026], [ 0.28829418, -0.35739038, 0.43310208], [ 0.22906521, 0.15219379, -0.02781823], [-0.47752244, 0.35798128, 0.22339024]])","title":"Centering an array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#plotting-a-two-dimensional-function","text":"One place that broadcasting is very useful is in displaying images based on two- dimensional functions. If we want to define a function z = f(x, y), broadcasting can be used to compute the function across the gri x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 50 )[:, np . newaxis ] x array([0. , 0.10204082, 0.20408163, 0.30612245, 0.40816327, 0.51020408, 0.6122449 , 0.71428571, 0.81632653, 0.91836735, 1.02040816, 1.12244898, 1.2244898 , 1.32653061, 1.42857143, 1.53061224, 1.63265306, 1.73469388, 1.83673469, 1.93877551, 2.04081633, 2.14285714, 2.24489796, 2.34693878, 2.44897959, 2.55102041, 2.65306122, 2.75510204, 2.85714286, 2.95918367, 3.06122449, 3.16326531, 3.26530612, 3.36734694, 3.46938776, 3.57142857, 3.67346939, 3.7755102 , 3.87755102, 3.97959184, 4.08163265, 4.18367347, 4.28571429, 4.3877551 , 4.48979592, 4.59183673, 4.69387755, 4.79591837, 4.89795918, 5. ]) y array([[0. ], [0.10204082], [0.20408163], [0.30612245], [0.40816327], [0.51020408], [0.6122449 ], [0.71428571], [0.81632653], [0.91836735], [1.02040816], [1.12244898], [1.2244898 ], [1.32653061], [1.42857143], [1.53061224], [1.63265306], [1.73469388], [1.83673469], [1.93877551], [2.04081633], [2.14285714], [2.24489796], [2.34693878], [2.44897959], [2.55102041], [2.65306122], [2.75510204], [2.85714286], [2.95918367], [3.06122449], [3.16326531], [3.26530612], [3.36734694], [3.46938776], [3.57142857], [3.67346939], [3.7755102 ], [3.87755102], [3.97959184], [4.08163265], [4.18367347], [4.28571429], [4.3877551 ], [4.48979592], [4.59183673], [4.69387755], [4.79591837], [4.89795918], [5. ]]) z = np . sin ( x ) ** 10 + np . cos ( 10 + x * y ) * np . cos ( x ) % matplotlib inline import matplotlib.pyplot as plt plt . imshow ( z , origin = 'lower' , extent = [ 0 , 5 , 0 , 5 ], cmap = 'viridis' ) plt . colorbar ();","title":"Plotting a two-dimensional function"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/","text":"================ by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 05 - Comparisons, Masks, and Boolean Logic \u00b6 Comparisons, Masks, and Boolean Logic Example: Counting Rainy Days Digging into the data Comparison Operators as ufuncs Working with Boolean Arrays Counting entries Boolean Operators Boolean Arrays as Masks Comparisons, Masks, and Boolean Logic \u00b6 This section covers the use of Boolean masks to examine and manipulate values within NumPy arrays. Masking comes up when you want to extract, modify, count, or otherwise manipulate values in an array based on some criterion: for example, you might wish to count all values greater than a certain value, or perhaps remove all out\u2010 liers that are above some threshold. In NumPy, Boolean masking is often the most efficient way to accomplish these types of tasks. Example: Counting Rainy Days \u00b6 import numpy as np import pandas as pd rainfall = pd . read_csv ( \"../data/Seattle2014.csv\" )[ 'PRCP' ] . values rainfall array([ 0, 41, 15, 0, 0, 3, 122, 97, 58, 43, 213, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 89, 216, 0, 23, 20, 0, 0, 0, 0, 0, 0, 51, 5, 183, 170, 46, 18, 94, 117, 264, 145, 152, 10, 30, 28, 25, 61, 130, 3, 0, 0, 0, 5, 191, 107, 165, 467, 30, 0, 323, 43, 188, 0, 0, 5, 69, 81, 277, 3, 0, 5, 0, 0, 0, 0, 0, 41, 36, 3, 221, 140, 0, 0, 0, 0, 25, 0, 46, 0, 0, 46, 0, 0, 0, 0, 0, 0, 5, 109, 185, 0, 137, 0, 51, 142, 89, 124, 0, 33, 69, 0, 0, 0, 0, 0, 333, 160, 51, 0, 0, 137, 20, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38, 0, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 64, 0, 5, 36, 13, 0, 8, 3, 0, 0, 0, 0, 0, 0, 18, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 193, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 5, 127, 216, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 13, 0, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 3, 183, 203, 43, 89, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 74, 0, 76, 71, 86, 0, 33, 150, 0, 117, 10, 320, 94, 41, 61, 15, 8, 127, 5, 254, 170, 0, 18, 109, 41, 48, 41, 0, 0, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 152, 5, 119, 13, 183, 3, 33, 343, 36, 0, 0, 0, 0, 8, 30, 74, 0, 91, 99, 130, 69, 0, 0, 0, 0, 0, 28, 130, 30, 196, 0, 0, 206, 53, 0, 0, 33, 41, 0, 0, 0]) inches = rainfall / 254 inches . shape (365,) % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . hist ( inches , 40 ); Digging into the data \u00b6 Comparison Operators as ufuncs \u00b6 x = np . array ([ 0 , 0 , 0 , 1 , 2 , 3 , 4 , 5 ]) x < 3 array([ True, True, True, True, True, False, False, False]) x > 2 array([False, False, False, False, False, True, True, True]) x >= 2 array([False, False, False, False, True, True, True, True]) x <= 3 array([ True, True, True, True, True, True, False, False]) x != 3 array([ True, True, True, True, True, False, True, True]) x == 3 array([False, False, False, False, False, True, False, False]) ( 2 * x ) == ( x ** 2 ) array([ True, True, True, False, True, False, False, False]) rng = np . random . RandomState ( 0 ) x = rng . randint ( 10 , size = ( 3 , 4 )) x array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) x < 4 array([[False, True, True, True], [False, False, True, False], [ True, False, False, False]]) Working with Boolean Arrays \u00b6 Counting entries \u00b6 print ( x ) [[5 0 3 3] [7 9 3 5] [2 4 7 6]] np . count_nonzero ( x < 4 ) 5 np . sum ( x > 3 ) 7 np . sum ( x < 3 , axis = 1 ) #checking each row array([1, 0, 1]) np . sum ( x < 3 , axis = 0 ) #checking each col array([1, 1, 0, 0]) If we\u2019re interested in quickly checking whether any or all the values are true, we can use (you guessed it) np.any() or np.all(): np . any ( x > 8 ) True np . any ( x < 5 ) True np . all ( x == 6 ) False np . all ( x < 10 ) True np . all ( x < 8 , axis = 1 ) array([ True, False, True]) Boolean Operators \u00b6 We\u2019ve already seen how we might count, say, all days with rain less than four inches, or all days with rain greater than two inches. But what if we want to know about all days with rain less than four inches and greater than one inch? This is accomplished through Python\u2019s bitwise logic operators, &, |, ^, and ~. Like with the standard arith\u2010 metic operators, NumPy overloads these as ufuncs that work element-wise on (usu\u2010 ally Boolean) arrays. np . sum (( inches > 0.5 ) & ( inches < 1 )) 29 inches > ( 0.5 & inches ) < 1 TypeError: ufunc 'bitwise_and' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'' print ( \"Number of days without rain: \" , np . sum ( inches == 0 )) print ( \"Number of days with rain: \" , np . sum ( inches != 0 )) print ( \"Days with more than 0.5 inches: \" , np . sum ( inches > 0.5 )) print ( \"Rainy days with <0.1 inches: \" , np . sum (( inches > 0 ) & ( inches < 0.2 ))) Number of days without rain: 215 Number of days with rain: 150 Days with more than 0.5 inches: 37 Rainy days with <0.1 inches: 75 Boolean Arrays as Masks \u00b6 In the preceding section, we looked at aggregates computed directly on Boolean arrays. A more powerful pattern is to use Boolean arrays as masks, to select particular subsets of the data themselves. x array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) x < 5 array([[False, True, True, True], [False, False, True, False], [ True, True, False, False]]) x [ x < 5 ] array([0, 3, 3, 3, 2, 4]) # Construct a mask of all rainy days rainy = ( inches > 0 ) # A construct of all summer days summer = ( np . arange ( 365 ) - 172 < 90 ) & ( np . arange ( 365 ) - 172 > 0 ) rainy array([False, True, True, False, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, True, False, True, True, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, False, True, False, False, False, False, False, True, True, True, True, True, False, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, False, True, True, False, False, False, False, False, True, True, True, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, True, True, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, False, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, False, True, True, True, True, False, False, False, False, False, True, True, True, True, False, False, True, True, False, False, True, True, False, False, False]) summer array([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]) print ( \"Median precip on rainy days in 2014 (inches): \" , np . median ( inches [ rainy ])) print ( \"Median precip on summber days in 2014 (inches): \" , np . median ( inches [ summer ])) print ( \"Maximum precip on summer in 2014 (inches): \" , np . max ( inches [ summer ])) print ( \"Median precip on summer in 2014 (inches) on non-summer rainy days: \" , np . median ( inches [ rainy & ~ summer ]) ) Median precip on rainy days in 2014 (inches): 0.19488188976377951 Median precip on summber days in 2014 (inches): 0.0 Maximum precip on summer in 2014 (inches): 0.8503937007874016 Median precip on summer in 2014 (inches) on non-summer rainy days: 0.20078740157480315 By combining Boolean operations, masking operations, and aggregates, we can very quickly answer these sorts of questions for our dataset. _ When you use & and | on integers, the expression operates on the bits of the element, applying the and or the or to the individual bits making up the number: When you use and or or, it\u2019s equivalent to asking Python to treat the object as a single Boolean entity. In Python, all nonzero integers will evaluate as True. Thus","title":"05 Comparisons  Masks and Boolean Logic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#05-comparisons-masks-and-boolean-logic","text":"Comparisons, Masks, and Boolean Logic Example: Counting Rainy Days Digging into the data Comparison Operators as ufuncs Working with Boolean Arrays Counting entries Boolean Operators Boolean Arrays as Masks","title":"05 - Comparisons, Masks, and Boolean Logic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#comparisons-masks-and-boolean-logic","text":"This section covers the use of Boolean masks to examine and manipulate values within NumPy arrays. Masking comes up when you want to extract, modify, count, or otherwise manipulate values in an array based on some criterion: for example, you might wish to count all values greater than a certain value, or perhaps remove all out\u2010 liers that are above some threshold. In NumPy, Boolean masking is often the most efficient way to accomplish these types of tasks.","title":"Comparisons, Masks, and Boolean Logic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#example-counting-rainy-days","text":"import numpy as np import pandas as pd rainfall = pd . read_csv ( \"../data/Seattle2014.csv\" )[ 'PRCP' ] . values rainfall array([ 0, 41, 15, 0, 0, 3, 122, 97, 58, 43, 213, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 89, 216, 0, 23, 20, 0, 0, 0, 0, 0, 0, 51, 5, 183, 170, 46, 18, 94, 117, 264, 145, 152, 10, 30, 28, 25, 61, 130, 3, 0, 0, 0, 5, 191, 107, 165, 467, 30, 0, 323, 43, 188, 0, 0, 5, 69, 81, 277, 3, 0, 5, 0, 0, 0, 0, 0, 41, 36, 3, 221, 140, 0, 0, 0, 0, 25, 0, 46, 0, 0, 46, 0, 0, 0, 0, 0, 0, 5, 109, 185, 0, 137, 0, 51, 142, 89, 124, 0, 33, 69, 0, 0, 0, 0, 0, 333, 160, 51, 0, 0, 137, 20, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38, 0, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 64, 0, 5, 36, 13, 0, 8, 3, 0, 0, 0, 0, 0, 0, 18, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 193, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 5, 127, 216, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 13, 0, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 3, 183, 203, 43, 89, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 74, 0, 76, 71, 86, 0, 33, 150, 0, 117, 10, 320, 94, 41, 61, 15, 8, 127, 5, 254, 170, 0, 18, 109, 41, 48, 41, 0, 0, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 152, 5, 119, 13, 183, 3, 33, 343, 36, 0, 0, 0, 0, 8, 30, 74, 0, 91, 99, 130, 69, 0, 0, 0, 0, 0, 28, 130, 30, 196, 0, 0, 206, 53, 0, 0, 33, 41, 0, 0, 0]) inches = rainfall / 254 inches . shape (365,) % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . hist ( inches , 40 );","title":"Example: Counting Rainy Days"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#digging-into-the-data","text":"","title":"Digging into the data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#comparison-operators-as-ufuncs","text":"x = np . array ([ 0 , 0 , 0 , 1 , 2 , 3 , 4 , 5 ]) x < 3 array([ True, True, True, True, True, False, False, False]) x > 2 array([False, False, False, False, False, True, True, True]) x >= 2 array([False, False, False, False, True, True, True, True]) x <= 3 array([ True, True, True, True, True, True, False, False]) x != 3 array([ True, True, True, True, True, False, True, True]) x == 3 array([False, False, False, False, False, True, False, False]) ( 2 * x ) == ( x ** 2 ) array([ True, True, True, False, True, False, False, False]) rng = np . random . RandomState ( 0 ) x = rng . randint ( 10 , size = ( 3 , 4 )) x array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) x < 4 array([[False, True, True, True], [False, False, True, False], [ True, False, False, False]])","title":"Comparison Operators as ufuncs"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#working-with-boolean-arrays","text":"","title":"Working with Boolean Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#counting-entries","text":"print ( x ) [[5 0 3 3] [7 9 3 5] [2 4 7 6]] np . count_nonzero ( x < 4 ) 5 np . sum ( x > 3 ) 7 np . sum ( x < 3 , axis = 1 ) #checking each row array([1, 0, 1]) np . sum ( x < 3 , axis = 0 ) #checking each col array([1, 1, 0, 0]) If we\u2019re interested in quickly checking whether any or all the values are true, we can use (you guessed it) np.any() or np.all(): np . any ( x > 8 ) True np . any ( x < 5 ) True np . all ( x == 6 ) False np . all ( x < 10 ) True np . all ( x < 8 , axis = 1 ) array([ True, False, True])","title":"Counting entries"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#boolean-operators","text":"We\u2019ve already seen how we might count, say, all days with rain less than four inches, or all days with rain greater than two inches. But what if we want to know about all days with rain less than four inches and greater than one inch? This is accomplished through Python\u2019s bitwise logic operators, &, |, ^, and ~. Like with the standard arith\u2010 metic operators, NumPy overloads these as ufuncs that work element-wise on (usu\u2010 ally Boolean) arrays. np . sum (( inches > 0.5 ) & ( inches < 1 )) 29 inches > ( 0.5 & inches ) < 1 TypeError: ufunc 'bitwise_and' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'' print ( \"Number of days without rain: \" , np . sum ( inches == 0 )) print ( \"Number of days with rain: \" , np . sum ( inches != 0 )) print ( \"Days with more than 0.5 inches: \" , np . sum ( inches > 0.5 )) print ( \"Rainy days with <0.1 inches: \" , np . sum (( inches > 0 ) & ( inches < 0.2 ))) Number of days without rain: 215 Number of days with rain: 150 Days with more than 0.5 inches: 37 Rainy days with <0.1 inches: 75","title":"Boolean Operators"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#boolean-arrays-as-masks","text":"In the preceding section, we looked at aggregates computed directly on Boolean arrays. A more powerful pattern is to use Boolean arrays as masks, to select particular subsets of the data themselves. x array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) x < 5 array([[False, True, True, True], [False, False, True, False], [ True, True, False, False]]) x [ x < 5 ] array([0, 3, 3, 3, 2, 4]) # Construct a mask of all rainy days rainy = ( inches > 0 ) # A construct of all summer days summer = ( np . arange ( 365 ) - 172 < 90 ) & ( np . arange ( 365 ) - 172 > 0 ) rainy array([False, True, True, False, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, True, False, True, True, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, False, True, False, False, False, False, False, True, True, True, True, True, False, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, False, True, True, False, False, False, False, False, True, True, True, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, True, True, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, False, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, False, True, True, True, True, False, False, False, False, False, True, True, True, True, False, False, True, True, False, False, True, True, False, False, False]) summer array([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]) print ( \"Median precip on rainy days in 2014 (inches): \" , np . median ( inches [ rainy ])) print ( \"Median precip on summber days in 2014 (inches): \" , np . median ( inches [ summer ])) print ( \"Maximum precip on summer in 2014 (inches): \" , np . max ( inches [ summer ])) print ( \"Median precip on summer in 2014 (inches) on non-summer rainy days: \" , np . median ( inches [ rainy & ~ summer ]) ) Median precip on rainy days in 2014 (inches): 0.19488188976377951 Median precip on summber days in 2014 (inches): 0.0 Maximum precip on summer in 2014 (inches): 0.8503937007874016 Median precip on summer in 2014 (inches) on non-summer rainy days: 0.20078740157480315 By combining Boolean operations, masking operations, and aggregates, we can very quickly answer these sorts of questions for our dataset. _ When you use & and | on integers, the expression operates on the bits of the element, applying the and or the or to the individual bits making up the number: When you use and or or, it\u2019s equivalent to asking Python to treat the object as a single Boolean entity. In Python, all nonzero integers will evaluate as True. Thus","title":"Boolean Arrays as Masks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/","text":"================ by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 06 - Comparisons, Masks, and Boolean Logic \u00b6 Fancy Indexing Exploring Fancy Indexing Combined Indexing Example: Selecting Random Points Modifying Values with Fancy Indexing Exmple Binning Data Fancy Indexing \u00b6 In the previous sections, we saw how to access and modify portions of arrays using simple indices (e.g., arr[0] ), slices (e.g., arr[:5] ), and Boolean masks (e.g., arr [arr> 0] ). In this section, we\u2019ll look at another style of array indexing, known as fancy indexing. Fancy indexing is like the simple indexing we\u2019ve already seen, but we pass arrays of indices in place of single scalars. This allows us to very quickly access and modify complicated subsets of an array\u2019s values. Exploring Fancy Indexing \u00b6 Fancy indexing is conceptually simple: it means passing an array of indices to access multiple array elements at once. import numpy as np rand = np . random . RandomState ( 42 ) x = rand . randint ( 100 , size = 10 ) print ( x ) [51 92 14 71 60 20 82 86 74 74] # three different elements [ x [ 3 ], x [ 7 ], x [ 2 ]] [71, 86, 14] # alternatively, we could do like this ind = [ 3 , 7 , 4 ] x [ ind ] array([71, 86, 60]) With fancy indexing, the shape of the result reflects the shape of the index arrays rather than the shape of the array being indexed: ind = np . array ([[ 3 , 7 ],[ 4 , 5 ]]) x [ ind ] array([[71, 86], [60, 20]]) # Multi-dimensional fancy indexing x = np . arange ( 12 ) . reshape (( 3 , 4 )) x array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) row = np . array ([ 0 , 1 , 2 ]) col = np . array ([ 2 , 1 , 3 ]) x [ row , col ] array([ 2, 5, 11]) x [ row [:, np . newaxis ], col ] array([[ 2, 1, 3], [ 6, 5, 7], [10, 9, 11]]) ##Here, each row value is matched with each column vector, exactly as we saw in broad\u2010 ##casting of arithmetic operations row [:, np . newaxis ] array([[0], [1], [2]]) row [:, np . newaxis ] * col array([[0, 0, 0], [2, 1, 3], [4, 2, 6]]) ## It is always important to remember with fancy indexing that the return value reflects ##the broadcasted shape of the indices, rather than the shape of the array being indexed. Combined Indexing \u00b6 For even more powerful operations, fancy indexing can be combined with the other indexing schemes x array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) # Combing fancy and simple index x [ 2 ,[ 2 , 0 , 1 ]] array([10, 8, 9]) # Combining fancy indexing with masking mask = np . array ([ 1 , 0 , 1 , 0 ], dtype = bool ) x [ row [:, np . newaxis ], mask ] array([[ 0, 2], [ 4, 6], [ 8, 10]]) Example: Selecting Random Points \u00b6 One common use of fancy indexing is the selection of subsets of rows from a matrix. For example, we might have an N by D matrix representing N points in D dimen\u2010 sions, such as the following points drawn from a two-dimensional normal distribu\u2010 tion mean = [ 0 , 0 ] conv = [[ 1 , 2 ], [ 2 , 5 ]] x = rand . multivariate_normal ( mean , conv , 100 ) x . shape (100, 2) print ( x ) [[-0.644508 -0.46220608] [ 0.7376352 1.21236921] [ 0.88151763 1.12795177] [ 2.04998983 5.97778598] [-0.1711348 -2.06258746] [ 0.67956979 0.83705124] [ 1.46860232 1.22961093] [ 0.35282131 1.49875397] [-2.51552505 -5.64629995] [ 0.0843329 -0.3543059 ] [ 0.19199272 1.48901291] [-0.02566217 -0.74987887] [ 1.00569227 2.25287315] [ 0.49514263 1.18939673] [ 0.0629872 0.57349278] [ 0.75093031 2.99487004] [-3.0236127 -6.00766046] [-0.53943081 -0.3478899 ] [ 1.53817376 1.99973464] [-0.50886808 -1.81099656] [ 1.58115602 2.86410319] [ 0.99305043 2.54294059] [-0.87753796 -1.15767204] [-1.11518048 -1.87508012] [ 0.4299908 0.36324254] [ 0.97253528 3.53815717] [ 0.32124996 0.33137032] [-0.74618649 -2.77366681] [-0.88473953 -1.81495444] [ 0.98783862 2.30280401] [-1.2033623 -2.04402725] [-1.51101746 -3.2818741 ] [-2.76337717 -7.66760648] [ 0.39158553 0.87949228] [ 0.91181024 3.32968944] [-0.84202629 -2.01226547] [ 1.06586877 0.95500019] [ 0.44457363 1.87828298] [ 0.35936721 0.40554974] [-0.90649669 -0.93486441] [-0.35790389 -0.52363012] [-1.33461668 -3.03203218] [ 0.02815138 0.79654924] [ 0.37785618 0.51409383] [-1.06505097 -2.88726779] [ 2.32083881 5.97698647] [ 0.47605744 0.83634485] [-0.35490984 -1.03657119] [ 0.57532883 -0.79997124] [ 0.33399913 2.32597923] [ 0.6575612 -0.22389518] [ 1.3707365 2.2348831 ] [ 0.07099548 -0.29685467] [ 0.6074983 1.47089233] [-0.34226126 -1.10666237] [ 0.69226246 1.21504303] [-0.31112937 -0.75912097] [-0.26888327 -1.89366817] [ 0.42044896 1.85189522] [ 0.21115245 2.00781492] [-1.83106042 -2.91352836] [ 0.7841796 1.97640753] [ 0.10259314 1.24690575] [-1.91100558 -3.66800923] [ 0.13143756 -0.07833855] [-0.1317045 -1.64159158] [-0.14547282 -1.34125678] [-0.51172373 -1.40960773] [ 0.69758045 0.72563649] [ 0.11677083 0.88385162] [-1.16586444 -2.24482237] [-2.23176235 -2.63958101] [ 0.37857234 0.69112594] [ 0.87475323 3.400675 ] [-0.86864365 -3.03568353] [-1.03637857 -1.18469125] [-0.53334959 -0.37039911] [ 0.30414557 -0.5828419 ] [-1.47656656 -2.13046298] [-0.31332021 -1.7895623 ] [ 1.12659538 1.49627535] [-1.19675798 -1.51633442] [-0.75210154 -0.79770535] [ 0.74577693 1.95834451] [ 1.56094354 2.9330816 ] [-0.72009966 -1.99780959] [-1.32319163 -2.61218347] [-2.56215914 -6.08410838] [ 1.31256297 3.13143269] [ 0.51575983 2.30284639] [ 0.01374713 -0.11539344] [-0.16863279 0.39422355] [ 0.12065651 1.13236323] [-0.83504984 -2.38632016] [ 1.05185885 1.98418223] [-0.69144553 -1.56919875] [-1.2567603 -1.125898 ] [ 0.09619333 -0.64335574] [-0.99658689 -2.35038099] [-1.21405259 -1.77693724]] % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . scatter ( x [:, 0 ], x [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f507340ffd0> Let\u2019s use fancy indexing to select 20 random points. We\u2019ll do this by first choosing 20 random indices with no repeats, and use these indices to select a portion of the origi\u2010 nal array indices = np . random . choice ( x . shape [ 0 ], 20 , replace = False ) indices array([80, 87, 70, 77, 18, 56, 41, 42, 84, 9, 85, 61, 6, 71, 24, 69, 8, 22, 23, 54]) selection = x [ indices ] selection array([[ 1.12659538, 1.49627535], [-2.56215914, -6.08410838], [-1.16586444, -2.24482237], [ 0.30414557, -0.5828419 ], [ 1.53817376, 1.99973464], [-0.31112937, -0.75912097], [-1.33461668, -3.03203218], [ 0.02815138, 0.79654924], [ 1.56094354, 2.9330816 ], [ 0.0843329 , -0.3543059 ], [-0.72009966, -1.99780959], [ 0.7841796 , 1.97640753], [ 1.46860232, 1.22961093], [-2.23176235, -2.63958101], [ 0.4299908 , 0.36324254], [ 0.11677083, 0.88385162], [-2.51552505, -5.64629995], [-0.87753796, -1.15767204], [-1.11518048, -1.87508012], [-0.34226126, -1.10666237]]) plt . scatter ( x [:, 0 ], x [:, 1 ], alpha = 0.3 ) plt . scatter ( selection [:, 0 ], selection [:, 1 ], facecolor = 'none' , s = 200 ); This sort of strategy is often used to quickly partition datasets, as is often needed in train/test splitting for validation of statistical models Modifying Values with Fancy Indexing \u00b6 Just as fancy indexing can be used to access parts of an array, it can also be used to modify parts of an array. For example, imagine we have an array of indices and we\u2019d like to set the corresponding items in an array to some value: x = np . arange ( 10 ) i = np . array ([ 2 , 1 , 8 , 4 ]) x [ i ] =- 9 x array([ 0, -9, -9, 3, -9, 5, 6, 7, -9, 9]) We can use any assignment-type operator for this Notice, though, that repeated indices with these operations can cause some poten\u2010 tially unexpected results x [ i ] -= 10 x array([ 0, -19, -19, 3, -19, 5, 6, 7, -19, 9]) x = np . zeros ( 10 ) x [[ 0 , 0 ]] = [ 4 , 6 ] x array([6., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) Where did the 4 go? The result of this operation is to first assign x[0] = 4 , followed by x[0] = 6 . The result, of course, is that x[0] contains the value 6. i = [ 2 , 3 , 3 , 4 , 4 , 4 ] x [ i ] += 1 x array([6., 0., 1., 1., 1., 0., 0., 0., 0., 0.]) You might expect that x[3] would contain the value 2, and x[4] would contain the value 3, as this is how many times each index is repeated. Why is this not the case? Conceptually, this is because x[i] += 1 is meant as a shorthand of x[i] = x[i] + 1. x[i] + 1 is evaluated, and then the result is assigned to the indices in x. With this in mind, it is not the augmentation that happens multiple times, but the assignment, which leads to the rather nonintuitive results. So what if you want the other behavior where the operation is repeated? For this, you can use the at() method of ufuncs (available since NumPy 1.8), and do the following x = np . zeros ( 10 ) np . add . at ( x , i , 1 ) x array([0., 0., 1., 2., 3., 0., 0., 0., 0., 0.]) The at() method does an in-place application of the given operator at the specified indices (here, i) with the specified value (here, 1). Another method that is similar in spirit is the reduceat() method of ufuncs, which you can read about in the NumPy documentation. Exmple Binning Data \u00b6 np . random . seed ( 42 ) x = np . random . randn ( 100 ) # compute a histogram by hand bins = np . linspace ( - 5 , 5 , 20 ) counts = np . zeros_like ( bins ) # find the approperiate bin for each x i = np . searchsorted ( bins , x ) # add 1 to each of these bins np . add . at ( counts , i , 1 ) # plot the results plt . plot ( bins , counts ) #linestyle='steps' Of course, it would be silly to have to do this each time you want to plot a histogram. This is why Matplotlib provides the plt.hist() routine, which does the same in a single line: plt . hist ( x , bins , histtype = 'step' ); print ( \"NumPy routine\" ) % timeit counts , edges = np . histogram ( x , bins ) NumPy routine 29.1 \u00b5s \u00b1 4.23 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) print ( \"custom routine\" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) custom routine 13.2 \u00b5s \u00b1 1.27 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each) Our own one-line algorithm is several times faster than the optimized algorithm in NumPy! How can this be? If you dig into the np.histogram source code (you can do this in IPython by typing np.histogram??), you\u2019ll see that it\u2019s quite a bit more involved than the simple search-and-count that we\u2019ve done; this is because NumPy\u2019s algorithm is more flexible, and particularly is designed for better performance when the number of data points becomes large: x = np . random . randn ( 1000000 ) print ( \"NumPy routine: \" ) % timeit counts , edges = np . histogram ( x , bins ) print ( \"Custom routine: \" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) NumPy routine: 67.3 ms \u00b1 5.67 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Custom routine:","title":"06 Fany Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#06-comparisons-masks-and-boolean-logic","text":"Fancy Indexing Exploring Fancy Indexing Combined Indexing Example: Selecting Random Points Modifying Values with Fancy Indexing Exmple Binning Data","title":"06 - Comparisons, Masks, and Boolean Logic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#fancy-indexing","text":"In the previous sections, we saw how to access and modify portions of arrays using simple indices (e.g., arr[0] ), slices (e.g., arr[:5] ), and Boolean masks (e.g., arr [arr> 0] ). In this section, we\u2019ll look at another style of array indexing, known as fancy indexing. Fancy indexing is like the simple indexing we\u2019ve already seen, but we pass arrays of indices in place of single scalars. This allows us to very quickly access and modify complicated subsets of an array\u2019s values.","title":"Fancy Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#exploring-fancy-indexing","text":"Fancy indexing is conceptually simple: it means passing an array of indices to access multiple array elements at once. import numpy as np rand = np . random . RandomState ( 42 ) x = rand . randint ( 100 , size = 10 ) print ( x ) [51 92 14 71 60 20 82 86 74 74] # three different elements [ x [ 3 ], x [ 7 ], x [ 2 ]] [71, 86, 14] # alternatively, we could do like this ind = [ 3 , 7 , 4 ] x [ ind ] array([71, 86, 60]) With fancy indexing, the shape of the result reflects the shape of the index arrays rather than the shape of the array being indexed: ind = np . array ([[ 3 , 7 ],[ 4 , 5 ]]) x [ ind ] array([[71, 86], [60, 20]]) # Multi-dimensional fancy indexing x = np . arange ( 12 ) . reshape (( 3 , 4 )) x array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) row = np . array ([ 0 , 1 , 2 ]) col = np . array ([ 2 , 1 , 3 ]) x [ row , col ] array([ 2, 5, 11]) x [ row [:, np . newaxis ], col ] array([[ 2, 1, 3], [ 6, 5, 7], [10, 9, 11]]) ##Here, each row value is matched with each column vector, exactly as we saw in broad\u2010 ##casting of arithmetic operations row [:, np . newaxis ] array([[0], [1], [2]]) row [:, np . newaxis ] * col array([[0, 0, 0], [2, 1, 3], [4, 2, 6]]) ## It is always important to remember with fancy indexing that the return value reflects ##the broadcasted shape of the indices, rather than the shape of the array being indexed.","title":"Exploring Fancy Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#combined-indexing","text":"For even more powerful operations, fancy indexing can be combined with the other indexing schemes x array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) # Combing fancy and simple index x [ 2 ,[ 2 , 0 , 1 ]] array([10, 8, 9]) # Combining fancy indexing with masking mask = np . array ([ 1 , 0 , 1 , 0 ], dtype = bool ) x [ row [:, np . newaxis ], mask ] array([[ 0, 2], [ 4, 6], [ 8, 10]])","title":"Combined Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#example-selecting-random-points","text":"One common use of fancy indexing is the selection of subsets of rows from a matrix. For example, we might have an N by D matrix representing N points in D dimen\u2010 sions, such as the following points drawn from a two-dimensional normal distribu\u2010 tion mean = [ 0 , 0 ] conv = [[ 1 , 2 ], [ 2 , 5 ]] x = rand . multivariate_normal ( mean , conv , 100 ) x . shape (100, 2) print ( x ) [[-0.644508 -0.46220608] [ 0.7376352 1.21236921] [ 0.88151763 1.12795177] [ 2.04998983 5.97778598] [-0.1711348 -2.06258746] [ 0.67956979 0.83705124] [ 1.46860232 1.22961093] [ 0.35282131 1.49875397] [-2.51552505 -5.64629995] [ 0.0843329 -0.3543059 ] [ 0.19199272 1.48901291] [-0.02566217 -0.74987887] [ 1.00569227 2.25287315] [ 0.49514263 1.18939673] [ 0.0629872 0.57349278] [ 0.75093031 2.99487004] [-3.0236127 -6.00766046] [-0.53943081 -0.3478899 ] [ 1.53817376 1.99973464] [-0.50886808 -1.81099656] [ 1.58115602 2.86410319] [ 0.99305043 2.54294059] [-0.87753796 -1.15767204] [-1.11518048 -1.87508012] [ 0.4299908 0.36324254] [ 0.97253528 3.53815717] [ 0.32124996 0.33137032] [-0.74618649 -2.77366681] [-0.88473953 -1.81495444] [ 0.98783862 2.30280401] [-1.2033623 -2.04402725] [-1.51101746 -3.2818741 ] [-2.76337717 -7.66760648] [ 0.39158553 0.87949228] [ 0.91181024 3.32968944] [-0.84202629 -2.01226547] [ 1.06586877 0.95500019] [ 0.44457363 1.87828298] [ 0.35936721 0.40554974] [-0.90649669 -0.93486441] [-0.35790389 -0.52363012] [-1.33461668 -3.03203218] [ 0.02815138 0.79654924] [ 0.37785618 0.51409383] [-1.06505097 -2.88726779] [ 2.32083881 5.97698647] [ 0.47605744 0.83634485] [-0.35490984 -1.03657119] [ 0.57532883 -0.79997124] [ 0.33399913 2.32597923] [ 0.6575612 -0.22389518] [ 1.3707365 2.2348831 ] [ 0.07099548 -0.29685467] [ 0.6074983 1.47089233] [-0.34226126 -1.10666237] [ 0.69226246 1.21504303] [-0.31112937 -0.75912097] [-0.26888327 -1.89366817] [ 0.42044896 1.85189522] [ 0.21115245 2.00781492] [-1.83106042 -2.91352836] [ 0.7841796 1.97640753] [ 0.10259314 1.24690575] [-1.91100558 -3.66800923] [ 0.13143756 -0.07833855] [-0.1317045 -1.64159158] [-0.14547282 -1.34125678] [-0.51172373 -1.40960773] [ 0.69758045 0.72563649] [ 0.11677083 0.88385162] [-1.16586444 -2.24482237] [-2.23176235 -2.63958101] [ 0.37857234 0.69112594] [ 0.87475323 3.400675 ] [-0.86864365 -3.03568353] [-1.03637857 -1.18469125] [-0.53334959 -0.37039911] [ 0.30414557 -0.5828419 ] [-1.47656656 -2.13046298] [-0.31332021 -1.7895623 ] [ 1.12659538 1.49627535] [-1.19675798 -1.51633442] [-0.75210154 -0.79770535] [ 0.74577693 1.95834451] [ 1.56094354 2.9330816 ] [-0.72009966 -1.99780959] [-1.32319163 -2.61218347] [-2.56215914 -6.08410838] [ 1.31256297 3.13143269] [ 0.51575983 2.30284639] [ 0.01374713 -0.11539344] [-0.16863279 0.39422355] [ 0.12065651 1.13236323] [-0.83504984 -2.38632016] [ 1.05185885 1.98418223] [-0.69144553 -1.56919875] [-1.2567603 -1.125898 ] [ 0.09619333 -0.64335574] [-0.99658689 -2.35038099] [-1.21405259 -1.77693724]] % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . scatter ( x [:, 0 ], x [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f507340ffd0> Let\u2019s use fancy indexing to select 20 random points. We\u2019ll do this by first choosing 20 random indices with no repeats, and use these indices to select a portion of the origi\u2010 nal array indices = np . random . choice ( x . shape [ 0 ], 20 , replace = False ) indices array([80, 87, 70, 77, 18, 56, 41, 42, 84, 9, 85, 61, 6, 71, 24, 69, 8, 22, 23, 54]) selection = x [ indices ] selection array([[ 1.12659538, 1.49627535], [-2.56215914, -6.08410838], [-1.16586444, -2.24482237], [ 0.30414557, -0.5828419 ], [ 1.53817376, 1.99973464], [-0.31112937, -0.75912097], [-1.33461668, -3.03203218], [ 0.02815138, 0.79654924], [ 1.56094354, 2.9330816 ], [ 0.0843329 , -0.3543059 ], [-0.72009966, -1.99780959], [ 0.7841796 , 1.97640753], [ 1.46860232, 1.22961093], [-2.23176235, -2.63958101], [ 0.4299908 , 0.36324254], [ 0.11677083, 0.88385162], [-2.51552505, -5.64629995], [-0.87753796, -1.15767204], [-1.11518048, -1.87508012], [-0.34226126, -1.10666237]]) plt . scatter ( x [:, 0 ], x [:, 1 ], alpha = 0.3 ) plt . scatter ( selection [:, 0 ], selection [:, 1 ], facecolor = 'none' , s = 200 ); This sort of strategy is often used to quickly partition datasets, as is often needed in train/test splitting for validation of statistical models","title":"Example: Selecting Random Points"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#modifying-values-with-fancy-indexing","text":"Just as fancy indexing can be used to access parts of an array, it can also be used to modify parts of an array. For example, imagine we have an array of indices and we\u2019d like to set the corresponding items in an array to some value: x = np . arange ( 10 ) i = np . array ([ 2 , 1 , 8 , 4 ]) x [ i ] =- 9 x array([ 0, -9, -9, 3, -9, 5, 6, 7, -9, 9]) We can use any assignment-type operator for this Notice, though, that repeated indices with these operations can cause some poten\u2010 tially unexpected results x [ i ] -= 10 x array([ 0, -19, -19, 3, -19, 5, 6, 7, -19, 9]) x = np . zeros ( 10 ) x [[ 0 , 0 ]] = [ 4 , 6 ] x array([6., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) Where did the 4 go? The result of this operation is to first assign x[0] = 4 , followed by x[0] = 6 . The result, of course, is that x[0] contains the value 6. i = [ 2 , 3 , 3 , 4 , 4 , 4 ] x [ i ] += 1 x array([6., 0., 1., 1., 1., 0., 0., 0., 0., 0.]) You might expect that x[3] would contain the value 2, and x[4] would contain the value 3, as this is how many times each index is repeated. Why is this not the case? Conceptually, this is because x[i] += 1 is meant as a shorthand of x[i] = x[i] + 1. x[i] + 1 is evaluated, and then the result is assigned to the indices in x. With this in mind, it is not the augmentation that happens multiple times, but the assignment, which leads to the rather nonintuitive results. So what if you want the other behavior where the operation is repeated? For this, you can use the at() method of ufuncs (available since NumPy 1.8), and do the following x = np . zeros ( 10 ) np . add . at ( x , i , 1 ) x array([0., 0., 1., 2., 3., 0., 0., 0., 0., 0.]) The at() method does an in-place application of the given operator at the specified indices (here, i) with the specified value (here, 1). Another method that is similar in spirit is the reduceat() method of ufuncs, which you can read about in the NumPy documentation.","title":"Modifying Values with Fancy Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#exmple-binning-data","text":"np . random . seed ( 42 ) x = np . random . randn ( 100 ) # compute a histogram by hand bins = np . linspace ( - 5 , 5 , 20 ) counts = np . zeros_like ( bins ) # find the approperiate bin for each x i = np . searchsorted ( bins , x ) # add 1 to each of these bins np . add . at ( counts , i , 1 ) # plot the results plt . plot ( bins , counts ) #linestyle='steps' Of course, it would be silly to have to do this each time you want to plot a histogram. This is why Matplotlib provides the plt.hist() routine, which does the same in a single line: plt . hist ( x , bins , histtype = 'step' ); print ( \"NumPy routine\" ) % timeit counts , edges = np . histogram ( x , bins ) NumPy routine 29.1 \u00b5s \u00b1 4.23 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) print ( \"custom routine\" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) custom routine 13.2 \u00b5s \u00b1 1.27 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each) Our own one-line algorithm is several times faster than the optimized algorithm in NumPy! How can this be? If you dig into the np.histogram source code (you can do this in IPython by typing np.histogram??), you\u2019ll see that it\u2019s quite a bit more involved than the simple search-and-count that we\u2019ve done; this is because NumPy\u2019s algorithm is more flexible, and particularly is designed for better performance when the number of data points becomes large: x = np . random . randn ( 1000000 ) print ( \"NumPy routine: \" ) % timeit counts , edges = np . histogram ( x , bins ) print ( \"Custom routine: \" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) NumPy routine: 67.3 ms \u00b1 5.67 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Custom routine:","title":"Exmple Binning Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/","text":"================ by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 07 - Sorting Arrays \u00b6 Sorting Arrays Fast Sorting in NumPy: np.sort and np.argsort Sorting along rows or columns Partial Sorts: Partitioning Example: k-Nearest Neighbors Sorting Arrays \u00b6 Up to this point we have been concerned mainly with tools to access and operate on array data with NumPy. This section covers algorithms related to sorting values in NumPy arrays. These algorithms are a favorite topic in introductory computer sci\u2010 ence courses: if you\u2019ve ever taken one, you probably have had dreams (or, depending on your temperament, nightmares) about insertion sorts, selection sorts, merge sorts, quick sorts, bubble sorts, and many, many more. All are means of accomplishing a similar task: sorting the values in a list or array. # Selection sort import numpy as np def selection_sort ( x ): for i in range ( len ( x )): swap = i + np . argmin ( x [ i :]) ( x [ i ], x [ swap ]) = ( x [ swap ], x [ i ]) return x x = np . array ([ 2 , 1 , 4 , 3 , 5 ]) selection_sort ( x ) array([1, 2, 3, 4, 5]) def bogosort ( x ): while np . any ( x [: - 1 ] > x [ 1 :]): np . random . shuffle ( x ) return x bogosort ( x ) array([1, 2, 3, 4, 5]) Fast Sorting in NumPy: np.sort and np.argsort \u00b6 Although Python has built-in sort and sorted functions to work with lists, we won\u2019t discuss them here because NumPy\u2019s np.sort function turns out to be much more efficient and useful for our purposes. By default np.sort uses an \ufffd N log N , quick\u2010 sort algorithm, though mergesort and heapsort are also available. For most applica\u2010 tions, the default quicksort is more than sufficient. x = np . array ([ 2 , 1 , 4 , 3 , 5 , 9 , 6 , 7 , 8 ]) np . sort ( x ) array([1, 2, 3, 4, 5, 6, 7, 8, 9]) #the sort method of array x . sort () x array([1, 2, 3, 4, 5, 6, 7, 8, 9]) A related function is argsort, which instead returns the indices of the sorted elements: x = np . array ([ 2 , 1 , 4 , 3 , 5 , 9 , 6 , 7 , 8 ]) i = np . argsort ( x ) i array([1, 0, 3, 2, 4, 6, 7, 8, 5]) # Now we can employ the fancy indexing to apply this index to sort the array x [ i ] array([1, 2, 3, 4, 5, 6, 7, 8, 9]) Sorting along rows or columns \u00b6 A useful feature of NumPy\u2019s sorting algorithms is the ability to sort along specific rows or columns of a multidimensional array using the axis argument. rand = np . random . RandomState ( 42 ) X = rand . randint ( 0 , 10 ,( 4 , 6 )) X array([[6, 3, 7, 4, 6, 9], [2, 6, 7, 4, 3, 7], [7, 2, 5, 4, 1, 7], [5, 1, 4, 0, 9, 5]]) # TO sort each column of X np . sort ( X , axis = 0 ) array([[2, 1, 4, 0, 1, 5], [5, 2, 5, 4, 3, 7], [6, 3, 7, 4, 6, 7], [7, 6, 7, 4, 9, 9]]) # TO sort each row of X np . sort ( X , axis = 1 ) array([[3, 4, 6, 6, 7, 9], [2, 3, 4, 6, 7, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 5, 9]]) Keep in mind that this treats each row or column as an independent array, and any relationships between the row or column values will be lost! Partial Sorts: Partitioning \u00b6 Sometimes we\u2019re not interested in sorting the entire array, but simply want to find the K smallest values in the array. NumPy provides this in the np.partition function. x = np . array ([ 7 , 2 , 3 , 1 , 6 , 5 , 4 ]) np . partition ( x , 3 ) array([2, 1, 3, 4, 6, 5, 7]) Similarly to sorting, we can partition along an arbitrary axis of a multidimensional array: np . partition ( X , 2 , axis = 1 ) array([[3, 4, 6, 7, 6, 9], [2, 3, 4, 7, 6, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 9, 5]]) X array([[6, 3, 7, 4, 6, 9], [2, 6, 7, 4, 3, 7], [7, 2, 5, 4, 1, 7], [5, 1, 4, 0, 9, 5]]) Example: k-Nearest Neighbors \u00b6 Let\u2019s quickly see how we might use this argsort function along multiple axes to find the nearest neighbors of each point in a set. X = rand . rand ( 10 , 2 ) X array([[0.00706631, 0.02306243], [0.52477466, 0.39986097], [0.04666566, 0.97375552], [0.23277134, 0.09060643], [0.61838601, 0.38246199], [0.98323089, 0.46676289], [0.85994041, 0.68030754], [0.45049925, 0.01326496], [0.94220176, 0.56328822], [0.3854165 , 0.01596625]]) % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ); X [:, np . newaxis ,:] . shape (10, 1, 2) X [ np . newaxis ,:,:] . shape (1, 10, 2) np . sum (( X [:, np . newaxis ,:] - X [ np . newaxis ,:,:]) ** 2 , axis = 1 ) array([[3.58879608, 2.10035624], [1.11234028, 0.97393943], [3.21004462, 4.71429169], [1.85007077, 1.68955454], [1.2368154 , 0.96342078], [3.39460409, 1.07078426], [2.36761815, 1.97878248], [1.13827476, 2.16752178], [3.01908822, 1.36825955], [1.25169759, 2.14881166]]) # Break downing the above code difference = X [:, np . newaxis ,:] - X [ np . newaxis ,:,:] difference . shape (10, 10, 2) square = difference ** 2 square . shape (10, 10, 2) dist_sqr = square . sum ( - 1 ) dist_sqr array([[0. , 0.40999909, 0.90538547, 0.05550496, 0.50287983, 1.14976739, 1.15936537, 0.19672877, 1.16632222, 0.14319923], [0.40999909, 0. , 0.55794316, 0.18090431, 0.00906581, 0.21465798, 0.19098635, 0.15497331, 0.20095384, 0.16679585], [0.90538547, 0.55794316, 0. , 0.81458763, 0.67649219, 1.13419594, 0.74752753, 1.08562368, 0.9704683 , 1.03211241], [0.05550496, 0.18090431, 0.81458763, 0. , 0.23387834, 0.70468321, 0.74108843, 0.05338715, 0.72671958, 0.0288717 ], [0.50287983, 0.00906581, 0.67649219, 0.23387834, 0. , 0.14021843, 0.1470605 , 0.16449241, 0.13755476, 0.18859392], [1.14976739, 0.21465798, 1.13419594, 0.70468321, 0.14021843, 0. , 0.06080186, 0.48946337, 0.01100053, 0.56059965], [1.15936537, 0.19098635, 0.74752753, 0.74108843, 0.1470605 , 0.06080186, 0. , 0.61258786, 0.02046045, 0.66652228], [0.19672877, 0.15497331, 1.08562368, 0.05338715, 0.16449241, 0.48946337, 0.61258786, 0. , 0.54429694, 0.00424306], [1.16632222, 0.20095384, 0.9704683 , 0.72671958, 0.13755476, 0.01100053, 0.02046045, 0.54429694, 0. , 0.60957115], [0.14319923, 0.16679585, 1.03211241, 0.0288717 , 0.18859392, 0.56059965, 0.66652228, 0.00424306, 0.60957115, 0. ]]) dist_sqr . diagonal () array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) nearest = np . argsort ( dist_sqr , axis = 1 ) nearest array([[0, 3, 9, 7, 1, 4, 2, 5, 6, 8], [1, 4, 7, 9, 3, 6, 8, 5, 0, 2], [2, 1, 4, 6, 3, 0, 8, 9, 7, 5], [3, 9, 7, 0, 1, 4, 5, 8, 6, 2], [4, 1, 8, 5, 6, 7, 9, 3, 0, 2], [5, 8, 6, 4, 1, 7, 9, 3, 2, 0], [6, 8, 5, 4, 1, 7, 9, 3, 2, 0], [7, 9, 3, 1, 4, 0, 5, 8, 6, 2], [8, 5, 6, 4, 1, 7, 9, 3, 2, 0], [9, 7, 3, 0, 1, 4, 5, 8, 6, 2]]) By using a full sort here, we\u2019ve actually done more work than we need to in this case. If we\u2019re simply interested in the nearest k neighbors, all we need is to partition each row so that the smallest k + 1 squared distances come first, with larger distances fill\u2010 ing the remaining positions of the array. We can do this with the np.argpartition function: k = 2 nearest_partition = np . argpartition ( dist_sqr , k + 1 , axis = 1 ) nearest_partition array([[3, 0, 9, 7, 1, 4, 2, 5, 8, 6], [1, 4, 7, 9, 3, 5, 6, 2, 8, 0], [2, 1, 4, 6, 3, 0, 5, 7, 8, 9], [3, 9, 7, 0, 1, 5, 6, 2, 8, 4], [1, 8, 4, 5, 7, 6, 9, 3, 2, 0], [5, 8, 6, 4, 1, 7, 9, 3, 2, 0], [6, 8, 5, 4, 1, 7, 9, 3, 2, 0], [7, 9, 3, 1, 4, 5, 6, 2, 8, 0], [8, 5, 6, 4, 1, 7, 9, 3, 2, 0], [3, 9, 7, 0, 1, 5, 6, 2, 8, 4]]) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ) <matplotlib.collections.PathCollection at 0x7f51448d7e80> # Draw the lines from each point to its two nearest neighbors k = 2 for i in range ( X . shape [ 0 ]): for j in nearest_partition [ i ,: k + 1 ]: plt . plot ( * zip ( X [ j ], X [ i ]), color = 'black' )","title":"07 Sorted Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#07-sorting-arrays","text":"Sorting Arrays Fast Sorting in NumPy: np.sort and np.argsort Sorting along rows or columns Partial Sorts: Partitioning Example: k-Nearest Neighbors","title":"07 - Sorting Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#sorting-arrays","text":"Up to this point we have been concerned mainly with tools to access and operate on array data with NumPy. This section covers algorithms related to sorting values in NumPy arrays. These algorithms are a favorite topic in introductory computer sci\u2010 ence courses: if you\u2019ve ever taken one, you probably have had dreams (or, depending on your temperament, nightmares) about insertion sorts, selection sorts, merge sorts, quick sorts, bubble sorts, and many, many more. All are means of accomplishing a similar task: sorting the values in a list or array. # Selection sort import numpy as np def selection_sort ( x ): for i in range ( len ( x )): swap = i + np . argmin ( x [ i :]) ( x [ i ], x [ swap ]) = ( x [ swap ], x [ i ]) return x x = np . array ([ 2 , 1 , 4 , 3 , 5 ]) selection_sort ( x ) array([1, 2, 3, 4, 5]) def bogosort ( x ): while np . any ( x [: - 1 ] > x [ 1 :]): np . random . shuffle ( x ) return x bogosort ( x ) array([1, 2, 3, 4, 5])","title":"Sorting Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#fast-sorting-in-numpy-npsort-and-npargsort","text":"Although Python has built-in sort and sorted functions to work with lists, we won\u2019t discuss them here because NumPy\u2019s np.sort function turns out to be much more efficient and useful for our purposes. By default np.sort uses an \ufffd N log N , quick\u2010 sort algorithm, though mergesort and heapsort are also available. For most applica\u2010 tions, the default quicksort is more than sufficient. x = np . array ([ 2 , 1 , 4 , 3 , 5 , 9 , 6 , 7 , 8 ]) np . sort ( x ) array([1, 2, 3, 4, 5, 6, 7, 8, 9]) #the sort method of array x . sort () x array([1, 2, 3, 4, 5, 6, 7, 8, 9]) A related function is argsort, which instead returns the indices of the sorted elements: x = np . array ([ 2 , 1 , 4 , 3 , 5 , 9 , 6 , 7 , 8 ]) i = np . argsort ( x ) i array([1, 0, 3, 2, 4, 6, 7, 8, 5]) # Now we can employ the fancy indexing to apply this index to sort the array x [ i ] array([1, 2, 3, 4, 5, 6, 7, 8, 9])","title":"Fast Sorting in NumPy: np.sort and np.argsort"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#sorting-along-rows-or-columns","text":"A useful feature of NumPy\u2019s sorting algorithms is the ability to sort along specific rows or columns of a multidimensional array using the axis argument. rand = np . random . RandomState ( 42 ) X = rand . randint ( 0 , 10 ,( 4 , 6 )) X array([[6, 3, 7, 4, 6, 9], [2, 6, 7, 4, 3, 7], [7, 2, 5, 4, 1, 7], [5, 1, 4, 0, 9, 5]]) # TO sort each column of X np . sort ( X , axis = 0 ) array([[2, 1, 4, 0, 1, 5], [5, 2, 5, 4, 3, 7], [6, 3, 7, 4, 6, 7], [7, 6, 7, 4, 9, 9]]) # TO sort each row of X np . sort ( X , axis = 1 ) array([[3, 4, 6, 6, 7, 9], [2, 3, 4, 6, 7, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 5, 9]]) Keep in mind that this treats each row or column as an independent array, and any relationships between the row or column values will be lost!","title":"Sorting along rows or columns"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#partial-sorts-partitioning","text":"Sometimes we\u2019re not interested in sorting the entire array, but simply want to find the K smallest values in the array. NumPy provides this in the np.partition function. x = np . array ([ 7 , 2 , 3 , 1 , 6 , 5 , 4 ]) np . partition ( x , 3 ) array([2, 1, 3, 4, 6, 5, 7]) Similarly to sorting, we can partition along an arbitrary axis of a multidimensional array: np . partition ( X , 2 , axis = 1 ) array([[3, 4, 6, 7, 6, 9], [2, 3, 4, 7, 6, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 9, 5]]) X array([[6, 3, 7, 4, 6, 9], [2, 6, 7, 4, 3, 7], [7, 2, 5, 4, 1, 7], [5, 1, 4, 0, 9, 5]])","title":"Partial Sorts: Partitioning"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#example-k-nearest-neighbors","text":"Let\u2019s quickly see how we might use this argsort function along multiple axes to find the nearest neighbors of each point in a set. X = rand . rand ( 10 , 2 ) X array([[0.00706631, 0.02306243], [0.52477466, 0.39986097], [0.04666566, 0.97375552], [0.23277134, 0.09060643], [0.61838601, 0.38246199], [0.98323089, 0.46676289], [0.85994041, 0.68030754], [0.45049925, 0.01326496], [0.94220176, 0.56328822], [0.3854165 , 0.01596625]]) % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ); X [:, np . newaxis ,:] . shape (10, 1, 2) X [ np . newaxis ,:,:] . shape (1, 10, 2) np . sum (( X [:, np . newaxis ,:] - X [ np . newaxis ,:,:]) ** 2 , axis = 1 ) array([[3.58879608, 2.10035624], [1.11234028, 0.97393943], [3.21004462, 4.71429169], [1.85007077, 1.68955454], [1.2368154 , 0.96342078], [3.39460409, 1.07078426], [2.36761815, 1.97878248], [1.13827476, 2.16752178], [3.01908822, 1.36825955], [1.25169759, 2.14881166]]) # Break downing the above code difference = X [:, np . newaxis ,:] - X [ np . newaxis ,:,:] difference . shape (10, 10, 2) square = difference ** 2 square . shape (10, 10, 2) dist_sqr = square . sum ( - 1 ) dist_sqr array([[0. , 0.40999909, 0.90538547, 0.05550496, 0.50287983, 1.14976739, 1.15936537, 0.19672877, 1.16632222, 0.14319923], [0.40999909, 0. , 0.55794316, 0.18090431, 0.00906581, 0.21465798, 0.19098635, 0.15497331, 0.20095384, 0.16679585], [0.90538547, 0.55794316, 0. , 0.81458763, 0.67649219, 1.13419594, 0.74752753, 1.08562368, 0.9704683 , 1.03211241], [0.05550496, 0.18090431, 0.81458763, 0. , 0.23387834, 0.70468321, 0.74108843, 0.05338715, 0.72671958, 0.0288717 ], [0.50287983, 0.00906581, 0.67649219, 0.23387834, 0. , 0.14021843, 0.1470605 , 0.16449241, 0.13755476, 0.18859392], [1.14976739, 0.21465798, 1.13419594, 0.70468321, 0.14021843, 0. , 0.06080186, 0.48946337, 0.01100053, 0.56059965], [1.15936537, 0.19098635, 0.74752753, 0.74108843, 0.1470605 , 0.06080186, 0. , 0.61258786, 0.02046045, 0.66652228], [0.19672877, 0.15497331, 1.08562368, 0.05338715, 0.16449241, 0.48946337, 0.61258786, 0. , 0.54429694, 0.00424306], [1.16632222, 0.20095384, 0.9704683 , 0.72671958, 0.13755476, 0.01100053, 0.02046045, 0.54429694, 0. , 0.60957115], [0.14319923, 0.16679585, 1.03211241, 0.0288717 , 0.18859392, 0.56059965, 0.66652228, 0.00424306, 0.60957115, 0. ]]) dist_sqr . diagonal () array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) nearest = np . argsort ( dist_sqr , axis = 1 ) nearest array([[0, 3, 9, 7, 1, 4, 2, 5, 6, 8], [1, 4, 7, 9, 3, 6, 8, 5, 0, 2], [2, 1, 4, 6, 3, 0, 8, 9, 7, 5], [3, 9, 7, 0, 1, 4, 5, 8, 6, 2], [4, 1, 8, 5, 6, 7, 9, 3, 0, 2], [5, 8, 6, 4, 1, 7, 9, 3, 2, 0], [6, 8, 5, 4, 1, 7, 9, 3, 2, 0], [7, 9, 3, 1, 4, 0, 5, 8, 6, 2], [8, 5, 6, 4, 1, 7, 9, 3, 2, 0], [9, 7, 3, 0, 1, 4, 5, 8, 6, 2]]) By using a full sort here, we\u2019ve actually done more work than we need to in this case. If we\u2019re simply interested in the nearest k neighbors, all we need is to partition each row so that the smallest k + 1 squared distances come first, with larger distances fill\u2010 ing the remaining positions of the array. We can do this with the np.argpartition function: k = 2 nearest_partition = np . argpartition ( dist_sqr , k + 1 , axis = 1 ) nearest_partition array([[3, 0, 9, 7, 1, 4, 2, 5, 8, 6], [1, 4, 7, 9, 3, 5, 6, 2, 8, 0], [2, 1, 4, 6, 3, 0, 5, 7, 8, 9], [3, 9, 7, 0, 1, 5, 6, 2, 8, 4], [1, 8, 4, 5, 7, 6, 9, 3, 2, 0], [5, 8, 6, 4, 1, 7, 9, 3, 2, 0], [6, 8, 5, 4, 1, 7, 9, 3, 2, 0], [7, 9, 3, 1, 4, 5, 6, 2, 8, 0], [8, 5, 6, 4, 1, 7, 9, 3, 2, 0], [3, 9, 7, 0, 1, 5, 6, 2, 8, 4]]) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ) <matplotlib.collections.PathCollection at 0x7f51448d7e80> # Draw the lines from each point to its two nearest neighbors k = 2 for i in range ( X . shape [ 0 ]): for j in nearest_partition [ i ,: k + 1 ]: plt . plot ( * zip ( X [ j ], X [ i ]), color = 'black' )","title":"Example: k-Nearest Neighbors"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/","text":"================ by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 08 - Structured Data: NumPy\u2019s Structured Arrays \u00b6 Structured Data: NumPy\u2019s Structured Arrays Structured Data: NumPy\u2019s Structured Arrays \u00b6 While often our data can be well represented by a homogeneous array of values,sometimes this is not the case. This section demonstrates the use of NumPy\u2019s struc\u2010 tured arrays and record arrays, which provide efficient storage for compound, heterogeneous data. While the patterns shown here are useful for simple operations, scenarios like this often lend themselves to the use of Pandas DataFrames. Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials Top","title":"08  Structured Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/#08-structured-data-numpys-structured-arrays","text":"Structured Data: NumPy\u2019s Structured Arrays","title":"08 - Structured Data: NumPy\u2019s Structured Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/#structured-data-numpys-structured-arrays","text":"While often our data can be well represented by a homogeneous array of values,sometimes this is not the case. This section demonstrates the use of NumPy\u2019s struc\u2010 tured arrays and record arrays, which provide efficient storage for compound, heterogeneous data. While the patterns shown here are useful for simple operations, scenarios like this often lend themselves to the use of Pandas DataFrames.","title":"Structured Data: NumPy\u2019s Structured Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials Top","title":"Great Job! Thats the end of this part."},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 01 - Introducing Pandas Objects \u00b6 Introducing Pandas Objects The Pandas Series Object Series as generalized NumPy array Series as specialized dictionary Constructing Series objects The Pandas DataFrame Object DataFrame as a generalized NumPy array DataFrame as specialized dictionary The Pandas Index Object Index as immutable array Index as ordered set Introducing Pandas Objects \u00b6 At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices. let\u2019s introduce the three fundamental Pandas data structures: the Series, DataFrame, and Index. import numpy as np import pandas as pd The Pandas Series Object \u00b6 A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array data = pd . Series ([ 0.25 , 0.5 , 3.1415 , 2.729 , 1.0 ,]) data 0 0.2500 1 0.5000 2 3.1415 3 2.7290 4 1.0000 dtype: float64 data . values array([0.25 , 0.5 , 3.1415, 2.729 , 1. ]) data . index RangeIndex(start=0, stop=5, step=1) # Like Numpy array, data can be accessed by the associated index vi the [] notation data [ 1 ] 0.5 data [ 1 : 3 ] 1 0.5000 2 3.1415 dtype: float64 Series as generalized NumPy array \u00b6 From what we\u2019ve seen so far, it may look like the Series object is basically interchangeable with a one-dimensional NumPy array. The essential difference is the presence of the index: while the NumPy array has an implicitly defined integer index usedvo access the values, the Pandas Series has an explicitly defined index associated with the values. # This explicit index definition gives the Series object additional capabilities... # Like index can be not only integer data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 data [ 'b' ] 0.5 # We can use non-contigious values for index like data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 25 , 1 , 0 , 75 ]) data 25 0.25 1 0.50 0 0.75 75 1.00 dtype: float64 data [ 1 ] 0.5 Series as specialized dictionary \u00b6 In this way, you can think of a Pandas Series a bit like a specialization of a Python dictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a Series is a structure that maps typed keys to a set of typed values. This typing is important: just as the type-specific compiled code behind a NumPy array makes it more efficient than a Python list for certain operations, the type information of a Pandas Series makes it much more efficient than Python dictionaries for certain operations. We can make the Series-as-dictionary analogy even more clear by constructing a Series object directly from a Python dictionary population_dict = { 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 } population = pd . Series ( population_dict ) population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 By default, a Series will be created where the index is drawn from the sorted keys. From here, typical dictionary-style item access can be performed: population [ 'California' ] 38332521 # Unlike a dictionary, though, the Series also supports array-style operations such as slicing population [ 'California' : 'Florida' ] California 38332521 Texas 26448193 New York 19651127 Florida 19552860 dtype: int64 Constructing Series objects \u00b6 We\u2019ve already seen a few ways of constructing a Pandas Series from scratch; all of them are some version of the following: >>> pd.Series(data, index=index) where index is an optional argument, and data can be one of many entities. data can be a list or NumPy array, in which case index defaults to an integer sequence pd . Series ([ 2 , 4 , 6 ]) 0 2 1 4 2 6 dtype: int64 data can be a scalar, which is repeated to fill the specified index: data = pd . Series ( 5 , index = [ 100 , 200 , 300 , 400 ]) data 100 5 200 5 300 5 400 5 dtype: int64 data can be a dictionary, in which index defaults to the sorted dictionary keys pd . Series ({ 2 : 'a' , 3 : 'c' , 5 : 'e' , 0 : 'i' }) 2 a 3 c 5 e 0 i dtype: object The Pandas DataFrame Object \u00b6 The next fundamental structure in Pandas is the DataFrame. Like the Series object discussed in the previous section, the DataFrame can be thought of either as a gener\u2010 alization of a NumPy array, or as a specialization of a Python dictionary. We\u2019ll now take a look at each of these perspectives. DataFrame as a generalized NumPy array \u00b6 If a Series is an analog of a one-dimensional array with flexible indices, a DataFrame is an analog of a two-dimensional array with both flexible row indices and flexible column names. Just as you might think of a two-dimensional array as an ordered sequence of aligned one-dimensional columns, you can think of a DataFrame as a sequence of aligned Series objects. Here, by \u201caligned\u201d we mean that they share the same index. area_dict = { 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 } area_dict {'California': 423967, 'Texas': 695662, 'New York': 141297, 'Florida': 170312, 'Illinois': 149995} area = pd . Series ( area_dict ) area California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 dtype: int64 population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 type ( area ) pandas.core.series.Series type ( population ) pandas.core.series.Series states = pd . DataFrame ({ 'population' : population , 'area' : area }) states .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area California 38332521 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 Like the Series object, the DataFrame has an index attribute that gives access to the index labels states . columns Index(['population', 'area'], dtype='object') states . index Index(['California', 'Texas', 'New York', 'Florida', 'Illinois'], dtype='object') DataFrame as specialized dictionary \u00b6 Similarly, we can also think of a DataFrame as a specialization of a dictionary. Where a dictionary maps a key to a value, a DataFrame maps a column name to a Series of column data. For example, asking for the \u2018area\u2019 attribute returns the Series object containing the areas we saw earlier states [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 states [ 'area' ][ 0 ] 423967 states [ 'area' ][: 3 ] California 423967 Texas 695662 New York 141297 Name: area, dtype: int64 states [ 'area' ][ 0 , 0 ] KeyError: 'key of type tuple not found and not a MultiIndex' Notice the potential point of confusion here: in a two-dimensional NumPy array, data[0] will return the first row. For a DataFrame, data['col0'] will return the first column. Because of this, it is probably better to think about DataFrames as generalized dictionaries rather than generalized arrays, though both ways of looking at the situa\u2010 tion can be useful. # Constructing dataframes from signle series object population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 pd . DataFrame ( data = population ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 pd . DataFrame ( data = population , columns = [ 'population' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 # Constructing dataframes from list of dicts data = [{ 'a' : i , 'b' : 2 * i } for i in range ( 4 )] data [{'a': 0, 'b': 0}, {'a': 1, 'b': 2}, {'a': 2, 'b': 4}, {'a': 3, 'b': 6}] pd . DataFrame ( data ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b 0 0 0 1 1 2 2 2 4 3 3 6 Even if some keys in the dictionary are missing, Pandas will fill them in with NaN (i.e., \u201cnot a number\u201d) values: pd . DataFrame ([{ 'a' : 1 , 'b' : 2 },{ 'b' : 3 , 'c' : 4 }]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c 0 1.0 2 NaN 1 NaN 3 4.0 # Constructing dataframes from a dictionary of Series Objects pd . DataFrame ({ 'population' : population , 'area' : area }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area California 38332521 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 # Constructing dataframes from two-d Numpy array pd . DataFrame ( np . random . rand ( 3 , 2 ), columns = [ 'foo' , 'bar' ], index = [ 'a' , 'b' , 'c' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } foo bar a 0.952378 0.229453 b 0.305751 0.208598 c 0.569426 0.843111 The Pandas Index Object \u00b6 We have seen here that both the Series and DataFrame objects contain an explicit index that lets you reference and modify data. This Index object is an interesting structure in itself, and it can be thought of either as an immutable array or as an ordered set (technically a multiset, as Index objects may contain repeated values). Those views have some interesting consequences in the operations available on Index objects. As a simple example, let\u2019s construct an Index from a list of integers ind = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) ind Int64Index([2, 3, 5, 7, 11], dtype='int64') type ( ind ) pandas.core.indexes.numeric.Int64Index Index as immutable array \u00b6 The Index object in many ways operates like an array. For example, we can use stan\u2010 dard Python indexing notation to retrieve values or slices ind [ 1 ] 3 ind [:: 2 ] Int64Index([2, 5, 11], dtype='int64') # Index objects also have many of the attributes familiar from NumPy arrays: print ( ind . size , ind . shape , ind . ndim , ind . dtype ) 5 (5,) 1 int64 #One difference between Index objects and NumPy arrays is that indices are immuta\u2010 #ble\u2014that is, they cannot be modified via the normal means ind [ 1 ] = 0 TypeError: Index does not support mutable operations Index as ordered set \u00b6 Pandas objects are designed to facilitate operations such as joins across datasets, which depend on many aspects of set arithmetic. the conventions used by Python\u2019s built-in set data structure, so that unions, intersec\u2010 tions, differences, and other combinations can be computed in a familiar way the conventions used by Python\u2019s built-in set data structure, so that unions, intersec\u2010 tions, differences, and other combinations can be computed in a familiar way. ind_a = pd . Index ([ 1 , 3 , 5 , 7 , 9 ]) ind_b = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) ind_a & ind_b # Intersection /tmp/ipykernel_229290/4215377278.py:1: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__. Use index.intersection(other) instead. ind_a & ind_b # Intersection Int64Index([3, 5, 7], dtype='int64') ind_a | ind_b # Union operation /tmp/ipykernel_229290/3034377863.py:1: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__. Use index.union(other) instead. ind_a | ind_b # Union operation Int64Index([1, 2, 3, 5, 7, 9, 11], dtype='int64') ind_a ^ ind_b # Symmetric difference /tmp/ipykernel_229290/3946211992.py:1: FutureWarning: Index.__xor__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__xor__. Use index.symmetric_difference(other) instead. ind_a^ind_b # Symmetric difference Int64Index([1, 2, 9, 11], dtype='int64')","title":"01 Introduction to Pandas Objects"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#01-introducing-pandas-objects","text":"Introducing Pandas Objects The Pandas Series Object Series as generalized NumPy array Series as specialized dictionary Constructing Series objects The Pandas DataFrame Object DataFrame as a generalized NumPy array DataFrame as specialized dictionary The Pandas Index Object Index as immutable array Index as ordered set","title":"01 - Introducing Pandas Objects"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#introducing-pandas-objects","text":"At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices. let\u2019s introduce the three fundamental Pandas data structures: the Series, DataFrame, and Index. import numpy as np import pandas as pd","title":"Introducing Pandas Objects"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#the-pandas-series-object","text":"A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array data = pd . Series ([ 0.25 , 0.5 , 3.1415 , 2.729 , 1.0 ,]) data 0 0.2500 1 0.5000 2 3.1415 3 2.7290 4 1.0000 dtype: float64 data . values array([0.25 , 0.5 , 3.1415, 2.729 , 1. ]) data . index RangeIndex(start=0, stop=5, step=1) # Like Numpy array, data can be accessed by the associated index vi the [] notation data [ 1 ] 0.5 data [ 1 : 3 ] 1 0.5000 2 3.1415 dtype: float64","title":"The Pandas Series Object"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#series-as-generalized-numpy-array","text":"From what we\u2019ve seen so far, it may look like the Series object is basically interchangeable with a one-dimensional NumPy array. The essential difference is the presence of the index: while the NumPy array has an implicitly defined integer index usedvo access the values, the Pandas Series has an explicitly defined index associated with the values. # This explicit index definition gives the Series object additional capabilities... # Like index can be not only integer data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 data [ 'b' ] 0.5 # We can use non-contigious values for index like data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 25 , 1 , 0 , 75 ]) data 25 0.25 1 0.50 0 0.75 75 1.00 dtype: float64 data [ 1 ] 0.5","title":"Series as generalized NumPy array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#series-as-specialized-dictionary","text":"In this way, you can think of a Pandas Series a bit like a specialization of a Python dictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a Series is a structure that maps typed keys to a set of typed values. This typing is important: just as the type-specific compiled code behind a NumPy array makes it more efficient than a Python list for certain operations, the type information of a Pandas Series makes it much more efficient than Python dictionaries for certain operations. We can make the Series-as-dictionary analogy even more clear by constructing a Series object directly from a Python dictionary population_dict = { 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 } population = pd . Series ( population_dict ) population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 By default, a Series will be created where the index is drawn from the sorted keys. From here, typical dictionary-style item access can be performed: population [ 'California' ] 38332521 # Unlike a dictionary, though, the Series also supports array-style operations such as slicing population [ 'California' : 'Florida' ] California 38332521 Texas 26448193 New York 19651127 Florida 19552860 dtype: int64","title":"Series as specialized dictionary"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#constructing-series-objects","text":"We\u2019ve already seen a few ways of constructing a Pandas Series from scratch; all of them are some version of the following: >>> pd.Series(data, index=index) where index is an optional argument, and data can be one of many entities. data can be a list or NumPy array, in which case index defaults to an integer sequence pd . Series ([ 2 , 4 , 6 ]) 0 2 1 4 2 6 dtype: int64 data can be a scalar, which is repeated to fill the specified index: data = pd . Series ( 5 , index = [ 100 , 200 , 300 , 400 ]) data 100 5 200 5 300 5 400 5 dtype: int64 data can be a dictionary, in which index defaults to the sorted dictionary keys pd . Series ({ 2 : 'a' , 3 : 'c' , 5 : 'e' , 0 : 'i' }) 2 a 3 c 5 e 0 i dtype: object","title":"Constructing Series objects"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#the-pandas-dataframe-object","text":"The next fundamental structure in Pandas is the DataFrame. Like the Series object discussed in the previous section, the DataFrame can be thought of either as a gener\u2010 alization of a NumPy array, or as a specialization of a Python dictionary. We\u2019ll now take a look at each of these perspectives.","title":"The Pandas DataFrame Object"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#dataframe-as-a-generalized-numpy-array","text":"If a Series is an analog of a one-dimensional array with flexible indices, a DataFrame is an analog of a two-dimensional array with both flexible row indices and flexible column names. Just as you might think of a two-dimensional array as an ordered sequence of aligned one-dimensional columns, you can think of a DataFrame as a sequence of aligned Series objects. Here, by \u201caligned\u201d we mean that they share the same index. area_dict = { 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 } area_dict {'California': 423967, 'Texas': 695662, 'New York': 141297, 'Florida': 170312, 'Illinois': 149995} area = pd . Series ( area_dict ) area California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 dtype: int64 population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 type ( area ) pandas.core.series.Series type ( population ) pandas.core.series.Series states = pd . DataFrame ({ 'population' : population , 'area' : area }) states .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area California 38332521 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 Like the Series object, the DataFrame has an index attribute that gives access to the index labels states . columns Index(['population', 'area'], dtype='object') states . index Index(['California', 'Texas', 'New York', 'Florida', 'Illinois'], dtype='object')","title":"DataFrame as a generalized NumPy array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#dataframe-as-specialized-dictionary","text":"Similarly, we can also think of a DataFrame as a specialization of a dictionary. Where a dictionary maps a key to a value, a DataFrame maps a column name to a Series of column data. For example, asking for the \u2018area\u2019 attribute returns the Series object containing the areas we saw earlier states [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 states [ 'area' ][ 0 ] 423967 states [ 'area' ][: 3 ] California 423967 Texas 695662 New York 141297 Name: area, dtype: int64 states [ 'area' ][ 0 , 0 ] KeyError: 'key of type tuple not found and not a MultiIndex' Notice the potential point of confusion here: in a two-dimensional NumPy array, data[0] will return the first row. For a DataFrame, data['col0'] will return the first column. Because of this, it is probably better to think about DataFrames as generalized dictionaries rather than generalized arrays, though both ways of looking at the situa\u2010 tion can be useful. # Constructing dataframes from signle series object population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 pd . DataFrame ( data = population ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 pd . DataFrame ( data = population , columns = [ 'population' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 # Constructing dataframes from list of dicts data = [{ 'a' : i , 'b' : 2 * i } for i in range ( 4 )] data [{'a': 0, 'b': 0}, {'a': 1, 'b': 2}, {'a': 2, 'b': 4}, {'a': 3, 'b': 6}] pd . DataFrame ( data ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b 0 0 0 1 1 2 2 2 4 3 3 6 Even if some keys in the dictionary are missing, Pandas will fill them in with NaN (i.e., \u201cnot a number\u201d) values: pd . DataFrame ([{ 'a' : 1 , 'b' : 2 },{ 'b' : 3 , 'c' : 4 }]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c 0 1.0 2 NaN 1 NaN 3 4.0 # Constructing dataframes from a dictionary of Series Objects pd . DataFrame ({ 'population' : population , 'area' : area }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area California 38332521 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 # Constructing dataframes from two-d Numpy array pd . DataFrame ( np . random . rand ( 3 , 2 ), columns = [ 'foo' , 'bar' ], index = [ 'a' , 'b' , 'c' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } foo bar a 0.952378 0.229453 b 0.305751 0.208598 c 0.569426 0.843111","title":"DataFrame as specialized dictionary"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#the-pandas-index-object","text":"We have seen here that both the Series and DataFrame objects contain an explicit index that lets you reference and modify data. This Index object is an interesting structure in itself, and it can be thought of either as an immutable array or as an ordered set (technically a multiset, as Index objects may contain repeated values). Those views have some interesting consequences in the operations available on Index objects. As a simple example, let\u2019s construct an Index from a list of integers ind = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) ind Int64Index([2, 3, 5, 7, 11], dtype='int64') type ( ind ) pandas.core.indexes.numeric.Int64Index","title":"The Pandas Index Object"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#index-as-immutable-array","text":"The Index object in many ways operates like an array. For example, we can use stan\u2010 dard Python indexing notation to retrieve values or slices ind [ 1 ] 3 ind [:: 2 ] Int64Index([2, 5, 11], dtype='int64') # Index objects also have many of the attributes familiar from NumPy arrays: print ( ind . size , ind . shape , ind . ndim , ind . dtype ) 5 (5,) 1 int64 #One difference between Index objects and NumPy arrays is that indices are immuta\u2010 #ble\u2014that is, they cannot be modified via the normal means ind [ 1 ] = 0 TypeError: Index does not support mutable operations","title":"Index as immutable array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#index-as-ordered-set","text":"Pandas objects are designed to facilitate operations such as joins across datasets, which depend on many aspects of set arithmetic. the conventions used by Python\u2019s built-in set data structure, so that unions, intersec\u2010 tions, differences, and other combinations can be computed in a familiar way the conventions used by Python\u2019s built-in set data structure, so that unions, intersec\u2010 tions, differences, and other combinations can be computed in a familiar way. ind_a = pd . Index ([ 1 , 3 , 5 , 7 , 9 ]) ind_b = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) ind_a & ind_b # Intersection /tmp/ipykernel_229290/4215377278.py:1: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__. Use index.intersection(other) instead. ind_a & ind_b # Intersection Int64Index([3, 5, 7], dtype='int64') ind_a | ind_b # Union operation /tmp/ipykernel_229290/3034377863.py:1: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__. Use index.union(other) instead. ind_a | ind_b # Union operation Int64Index([1, 2, 3, 5, 7, 9, 11], dtype='int64') ind_a ^ ind_b # Symmetric difference /tmp/ipykernel_229290/3946211992.py:1: FutureWarning: Index.__xor__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__xor__. Use index.symmetric_difference(other) instead. ind_a^ind_b # Symmetric difference Int64Index([1, 2, 9, 11], dtype='int64')","title":"Index as ordered set"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 02 - Data Indexing and Selection \u00b6 Data Indexing and Selection Data Selection in Series Series as one-dimensional array Indexers: loc, iloc, and ix Data Selection in DataFrame DataFrame as a Dictionary DataFrame as 2D array Data Indexing and Selection \u00b6 In Chapter 2, we looked in detail at methods and tools to access, set, and modify values in NumPy arrays. These included indexing (e.g., arr[2, 1] ), slicing (e.g., arr[:,1:5]) , masking (e.g., arr[arr > 0] ), fancy indexing (e.g., arr[0, [1, 5]] ), and combinations thereof (e.g., arr[:, [1, 5]] ). Here we\u2019ll look at similar means of accessing and modifying values in Pandas Series and DataFrame objects. If you have used the NumPy patterns, the corresponding patterns in Pandas will feel very famil\u2010 iar, though there are a few quirks to be aware of. Data Selection in Series \u00b6 As we saw in the previous section, a Series object acts in many ways like a one- dimensional NumPy array, and in many ways like a standard Python dictionary. If we keep these two overlapping analogies in mind, it will help us to understand the patterns of data indexing and selection in these arrays. ### Series as dictionary Like a dictionary, the Series object provides a mapping from a collection of keys to a collection of values: import pandas as pd data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 data [ 'b' ] 0.5 # also use dictionary-like python experssions and methods to examine the keys/indices and values 'a' in data True data . keys () Index(['a', 'b', 'c', 'd'], dtype='object') list ( data . items ()) [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)] Series objects can even be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a Series by assigning to a new index value data [ 'e' ] = 1.25 data a 0.25 b 0.50 c 0.75 d 1.00 e 1.25 dtype: float64 Series as one-dimensional array \u00b6 A Series builds on this dictionary-like interface and provides array-style item selec\u2010 tion via the same basic mechanisms as NumPy arrays\u2014that is, slices, masking, and fancy indexing. Examples of these are as follows: # slicing by explicit index data [ 'a' : 'c' ] a 0.25 b 0.50 c 0.75 dtype: float64 #slicing by implicit integer index data [ 0 : 2 ] a 0.25 b 0.50 dtype: float64 #masking data [( data > 0.3 ) & ( data < 0.8 )] b 0.50 c 0.75 dtype: float64 #fancy indexing data [[ 'a' , 'e' ]] a 0.25 e 1.25 dtype: float64 Among these, slicing may be the source of the most confusion. Notice that when you are slicing with an explicit index (i.e., data['a':'c'] ), the final index is included in the slice, while when you\u2019re slicing with an implicit index (i.e., data[0:2] ), the final index is excluded from the slice. Indexers: loc, iloc, and ix \u00b6 These slicing and indexing conventions can be a source of confusion. For example, if your Series has an explicit integer index, an indexing operation such as data[1] will use the explicit indices, while a slicing operation like data[1:3] will use the implicit Python-style index. data = pd . Series ([ 'a' , 'b' , 'c' ], index = [ 1 , 3 , 5 ]) data 1 a 3 b 5 c dtype: object # explicit index when indexing data [ 1 ] 'a' data [ 3 ] 'b' # implicit index when slicing data [ 1 : 3 ] 3 b 5 c dtype: object Because of this potential confusion in the case of integer indexes, Pandas provides some special indexer attributes that explicitly expose certain indexing schemes. These are not functional methods, but attributes that expose a particular slicing interface to the data in the Series. # First, the loc attribute allows indexing and slicing that always references the explicit index: data . loc [ 1 ] 'a' data . loc [ 1 : 3 ] 1 a 3 b dtype: object # The iloc attribute allows indexing and slicing that always references the implicit Python-style index data . iloc [ 1 ] 'b' data . iloc [ 0 ] 'a' data . loc [ 0 ] KeyError: 0 data . iloc [ 1 : 3 ] 3 b 5 c dtype: object data . loc [ 1 : 3 ] 1 a 3 b dtype: object Data Selection in DataFrame \u00b6 DataFrame as a Dictionary \u00b6 area = pd . Series ({ 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 }) pop = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 }) data = pd . DataFrame ({ 'area' : area , 'pop' : pop }) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 Florida 170312 19552860 Illinois 149995 12882135 data [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . area California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . area is data [ 'area' ] True data . pop is data [ 'pop' ] # pop method is refered instead of pop in our datafram False data [ 'density' ] = data [ 'pop' ] / data [ 'area' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop density California 423967 38332521 90.413926 Texas 695662 26448193 38.018740 New York 141297 19651127 139.076746 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763 DataFrame as 2D array \u00b6 data . values array([[4.23967000e+05, 3.83325210e+07, 9.04139261e+01], [6.95662000e+05, 2.64481930e+07, 3.80187404e+01], [1.41297000e+05, 1.96511270e+07, 1.39076746e+02], [1.70312000e+05, 1.95528600e+07, 1.14806121e+02], [1.49995000e+05, 1.28821350e+07, 8.58837628e+01]]) # Transposing the values data . T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } California Texas New York Florida Illinois area 4.239670e+05 6.956620e+05 1.412970e+05 1.703120e+05 1.499950e+05 pop 3.833252e+07 2.644819e+07 1.965113e+07 1.955286e+07 1.288214e+07 density 9.041393e+01 3.801874e+01 1.390767e+02 1.148061e+02 8.588376e+01 data . values [ 0 ] array([4.23967000e+05, 3.83325210e+07, 9.04139261e+01]) data [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . iloc [: 3 ,: 2 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 data . loc [: 'Illinois' ,: 'pop' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 Florida 170312 19552860 Illinois 149995 12882135 data . loc [ data . density > 100 ,[ 'pop' , 'density' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop density New York 19651127 139.076746 Florida 19552860 114.806121 data . iloc [ 0 , 2 ] = 90 data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop density California 423967 38332521 90.000000 Texas 695662 26448193 38.018740 New York 141297 19651127 139.076746 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763","title":"02 Data Indexing and Selection"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#02-data-indexing-and-selection","text":"Data Indexing and Selection Data Selection in Series Series as one-dimensional array Indexers: loc, iloc, and ix Data Selection in DataFrame DataFrame as a Dictionary DataFrame as 2D array","title":"02 - Data Indexing and Selection"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#data-indexing-and-selection","text":"In Chapter 2, we looked in detail at methods and tools to access, set, and modify values in NumPy arrays. These included indexing (e.g., arr[2, 1] ), slicing (e.g., arr[:,1:5]) , masking (e.g., arr[arr > 0] ), fancy indexing (e.g., arr[0, [1, 5]] ), and combinations thereof (e.g., arr[:, [1, 5]] ). Here we\u2019ll look at similar means of accessing and modifying values in Pandas Series and DataFrame objects. If you have used the NumPy patterns, the corresponding patterns in Pandas will feel very famil\u2010 iar, though there are a few quirks to be aware of.","title":"Data Indexing and Selection"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#data-selection-in-series","text":"As we saw in the previous section, a Series object acts in many ways like a one- dimensional NumPy array, and in many ways like a standard Python dictionary. If we keep these two overlapping analogies in mind, it will help us to understand the patterns of data indexing and selection in these arrays. ### Series as dictionary Like a dictionary, the Series object provides a mapping from a collection of keys to a collection of values: import pandas as pd data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 data [ 'b' ] 0.5 # also use dictionary-like python experssions and methods to examine the keys/indices and values 'a' in data True data . keys () Index(['a', 'b', 'c', 'd'], dtype='object') list ( data . items ()) [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)] Series objects can even be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a Series by assigning to a new index value data [ 'e' ] = 1.25 data a 0.25 b 0.50 c 0.75 d 1.00 e 1.25 dtype: float64","title":"Data Selection in Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#series-as-one-dimensional-array","text":"A Series builds on this dictionary-like interface and provides array-style item selec\u2010 tion via the same basic mechanisms as NumPy arrays\u2014that is, slices, masking, and fancy indexing. Examples of these are as follows: # slicing by explicit index data [ 'a' : 'c' ] a 0.25 b 0.50 c 0.75 dtype: float64 #slicing by implicit integer index data [ 0 : 2 ] a 0.25 b 0.50 dtype: float64 #masking data [( data > 0.3 ) & ( data < 0.8 )] b 0.50 c 0.75 dtype: float64 #fancy indexing data [[ 'a' , 'e' ]] a 0.25 e 1.25 dtype: float64 Among these, slicing may be the source of the most confusion. Notice that when you are slicing with an explicit index (i.e., data['a':'c'] ), the final index is included in the slice, while when you\u2019re slicing with an implicit index (i.e., data[0:2] ), the final index is excluded from the slice.","title":"Series as one-dimensional array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#indexers-loc-iloc-and-ix","text":"These slicing and indexing conventions can be a source of confusion. For example, if your Series has an explicit integer index, an indexing operation such as data[1] will use the explicit indices, while a slicing operation like data[1:3] will use the implicit Python-style index. data = pd . Series ([ 'a' , 'b' , 'c' ], index = [ 1 , 3 , 5 ]) data 1 a 3 b 5 c dtype: object # explicit index when indexing data [ 1 ] 'a' data [ 3 ] 'b' # implicit index when slicing data [ 1 : 3 ] 3 b 5 c dtype: object Because of this potential confusion in the case of integer indexes, Pandas provides some special indexer attributes that explicitly expose certain indexing schemes. These are not functional methods, but attributes that expose a particular slicing interface to the data in the Series. # First, the loc attribute allows indexing and slicing that always references the explicit index: data . loc [ 1 ] 'a' data . loc [ 1 : 3 ] 1 a 3 b dtype: object # The iloc attribute allows indexing and slicing that always references the implicit Python-style index data . iloc [ 1 ] 'b' data . iloc [ 0 ] 'a' data . loc [ 0 ] KeyError: 0 data . iloc [ 1 : 3 ] 3 b 5 c dtype: object data . loc [ 1 : 3 ] 1 a 3 b dtype: object","title":"Indexers: loc, iloc, and ix"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#data-selection-in-dataframe","text":"","title":"Data Selection in DataFrame"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#dataframe-as-a-dictionary","text":"area = pd . Series ({ 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 }) pop = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 }) data = pd . DataFrame ({ 'area' : area , 'pop' : pop }) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 Florida 170312 19552860 Illinois 149995 12882135 data [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . area California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . area is data [ 'area' ] True data . pop is data [ 'pop' ] # pop method is refered instead of pop in our datafram False data [ 'density' ] = data [ 'pop' ] / data [ 'area' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop density California 423967 38332521 90.413926 Texas 695662 26448193 38.018740 New York 141297 19651127 139.076746 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763","title":"DataFrame as a Dictionary"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#dataframe-as-2d-array","text":"data . values array([[4.23967000e+05, 3.83325210e+07, 9.04139261e+01], [6.95662000e+05, 2.64481930e+07, 3.80187404e+01], [1.41297000e+05, 1.96511270e+07, 1.39076746e+02], [1.70312000e+05, 1.95528600e+07, 1.14806121e+02], [1.49995000e+05, 1.28821350e+07, 8.58837628e+01]]) # Transposing the values data . T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } California Texas New York Florida Illinois area 4.239670e+05 6.956620e+05 1.412970e+05 1.703120e+05 1.499950e+05 pop 3.833252e+07 2.644819e+07 1.965113e+07 1.955286e+07 1.288214e+07 density 9.041393e+01 3.801874e+01 1.390767e+02 1.148061e+02 8.588376e+01 data . values [ 0 ] array([4.23967000e+05, 3.83325210e+07, 9.04139261e+01]) data [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . iloc [: 3 ,: 2 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 data . loc [: 'Illinois' ,: 'pop' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 Florida 170312 19552860 Illinois 149995 12882135 data . loc [ data . density > 100 ,[ 'pop' , 'density' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop density New York 19651127 139.076746 Florida 19552860 114.806121 data . iloc [ 0 , 2 ] = 90 data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop density California 423967 38332521 90.000000 Texas 695662 26448193 38.018740 New York 141297 19651127 139.076746 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763","title":"DataFrame as 2D array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 03 -Operations on Data in Pandas \u00b6 import numpy as np import pandas as pd rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . randint ( 0 , 10 , 4 )) ser 0 6 1 3 2 7 3 4 dtype: int64 df = pd . DataFrame ( rng . randint ( 0 , 10 ,( 3 , 4 )), columns = [ 'A' , \"B\" , 'C' , 'D' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 np . exp ( ser ) 0 403.428793 1 20.085537 2 1096.633158 3 54.598150 dtype: float64 np . sin ( df * np . pi / 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 -1.000000 7.071068e-01 1.000000 -1.000000e+00 1 -0.707107 1.224647e-16 0.707107 -7.071068e-01 2 -0.707107 1.000000e+00 -0.707107 1.224647e-16 area = pd . Series ({ 'Alaska' : 1723337 , 'Texas' : 695662 , 'California' : 423967 }, name = 'area' ) population = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 }, name = 'population' ) area Alaska 1723337 Texas 695662 California 423967 Name: area, dtype: int64 population California 38332521 Texas 26448193 New York 19651127 Name: population, dtype: int64 population / area Alaska NaN California 90.413926 New York NaN Texas 38.018740 dtype: float64 area . index | population . index /tmp/ipykernel_15930/3572280633.py:1: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__. Use index.union(other) instead. area.index | population.index Index(['Alaska', 'California', 'New York', 'Texas'], dtype='object')","title":"03 Operating on Data in Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/#03-operations-on-data-in-pandas","text":"import numpy as np import pandas as pd rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . randint ( 0 , 10 , 4 )) ser 0 6 1 3 2 7 3 4 dtype: int64 df = pd . DataFrame ( rng . randint ( 0 , 10 ,( 3 , 4 )), columns = [ 'A' , \"B\" , 'C' , 'D' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 np . exp ( ser ) 0 403.428793 1 20.085537 2 1096.633158 3 54.598150 dtype: float64 np . sin ( df * np . pi / 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 -1.000000 7.071068e-01 1.000000 -1.000000e+00 1 -0.707107 1.224647e-16 0.707107 -7.071068e-01 2 -0.707107 1.000000e+00 -0.707107 1.224647e-16 area = pd . Series ({ 'Alaska' : 1723337 , 'Texas' : 695662 , 'California' : 423967 }, name = 'area' ) population = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 }, name = 'population' ) area Alaska 1723337 Texas 695662 California 423967 Name: area, dtype: int64 population California 38332521 Texas 26448193 New York 19651127 Name: population, dtype: int64 population / area Alaska NaN California 90.413926 New York NaN Texas 38.018740 dtype: float64 area . index | population . index /tmp/ipykernel_15930/3572280633.py:1: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__. Use index.union(other) instead. area.index | population.index Index(['Alaska', 'California', 'New York', 'Texas'], dtype='object')","title":"03 -Operations on Data in Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 04 - Handling Missing Data \u00b6 None: Pythonic missing data NaN: Missing numerical data Detecting null values Filling null values None: Pythonic missing data \u00b6 import numpy as np import pandas as pd vals1 = np . array ([ 1 , None , 3 , 4 ]) vals1 array([1, None, 3, 4], dtype=object) for dtype in [ 'object' , 'int' ]: print ( \"dtype = \" , dtype ) % timeit np . arange ( 1E6 , dtype = dtype ) . sum () print () dtype = object 57.5 ms \u00b1 707 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) dtype = int 1.09 ms \u00b1 35.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) # My get error on arregate functions of numpy vals1 . sum () TypeError: unsupported operand type(s) for +: 'int' and 'NoneType' NaN: Missing numerical data \u00b6 vals2 = np . array ([ 1 , np . nan , 3 , 4 ]) vals2 . dtype vals1 . dtype 1 + np . nan 0 * np . nan vals2 . sum (), vals2 . min (), vals2 . max () NumPy does provide some special aggregations that will ignore these missing values Detecting null values \u00b6 Pandas data structures have two useful methods for detecting null data: isnull() and notnull(). Either one will return a Boolean mask over the data. data . isnull () # Deleting all null values data = pd . Series ([ 1 , np . nan , 'hello' , None ]) data df [ 3 ] = np . nan df NameError: name 'df' is not defined rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . randint ( 0 , 10 , 4 )) df = pd . DataFrame ( rng . randint ( 0 , 10 ,( 3 , 4 )), columns = [ 'A' , \"B\" , 'C' , 'D' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 df [ 3 ] = np . nan df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 3 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . columns = [ 1 , 2 , 3 , 4 , 5 ] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . dropna ( axis = 'columns' , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 df . dropna ( axis = 'rows' , thresh = 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN Filling null values \u00b6 Sometimes rather than dropping NA values, you\u2019d rather replace them with a valid value. This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values. You could do this in-place using the isnull() method as a mask, but because it is such a common operation Pandas provides the fillna() method, which returns a copy of the array with the null values replaced. data = pd . Series ([ 1 , np . nan , 2 , None , 3 ], index = list ( 'abcde' )) data a 1.0 b NaN c 2.0 d NaN e 3.0 dtype: float64 # filling na values with a single 0 sum ( data . isnull ()) 2 data . fillna ( - 1 ) a 1.0 b -1.0 c 2.0 d -1.0 e 3.0 dtype: float64 #forward-fill --> propagates the previous value forward data . fillna ( method = 'ffill' ) a 1.0 b 1.0 c 2.0 d 2.0 e 3.0 dtype: float64 # Back fill, to propgate the next value backward data . fillna ( method = 'bfill' ) a 1.0 b 2.0 c 2.0 d 3.0 e 3.0 dtype: float64 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . fillna ( method = 'ffill' , axis = 1 ) # if the previous value is not available during a forward fill, the NA value remains .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6.0 9.0 2.0 6.0 6.0 1 7.0 4.0 3.0 7.0 7.0 2 7.0 2.0 5.0 4.0 4.0","title":"04 Handling Missing Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#04-handling-missing-data","text":"None: Pythonic missing data NaN: Missing numerical data Detecting null values Filling null values","title":"04 - Handling Missing Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#none-pythonic-missing-data","text":"import numpy as np import pandas as pd vals1 = np . array ([ 1 , None , 3 , 4 ]) vals1 array([1, None, 3, 4], dtype=object) for dtype in [ 'object' , 'int' ]: print ( \"dtype = \" , dtype ) % timeit np . arange ( 1E6 , dtype = dtype ) . sum () print () dtype = object 57.5 ms \u00b1 707 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) dtype = int 1.09 ms \u00b1 35.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) # My get error on arregate functions of numpy vals1 . sum () TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'","title":"None: Pythonic missing data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#nan-missing-numerical-data","text":"vals2 = np . array ([ 1 , np . nan , 3 , 4 ]) vals2 . dtype vals1 . dtype 1 + np . nan 0 * np . nan vals2 . sum (), vals2 . min (), vals2 . max () NumPy does provide some special aggregations that will ignore these missing values","title":"NaN: Missing numerical data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#detecting-null-values","text":"Pandas data structures have two useful methods for detecting null data: isnull() and notnull(). Either one will return a Boolean mask over the data. data . isnull () # Deleting all null values data = pd . Series ([ 1 , np . nan , 'hello' , None ]) data df [ 3 ] = np . nan df NameError: name 'df' is not defined rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . randint ( 0 , 10 , 4 )) df = pd . DataFrame ( rng . randint ( 0 , 10 ,( 3 , 4 )), columns = [ 'A' , \"B\" , 'C' , 'D' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 df [ 3 ] = np . nan df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 3 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . columns = [ 1 , 2 , 3 , 4 , 5 ] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . dropna ( axis = 'columns' , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 df . dropna ( axis = 'rows' , thresh = 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN","title":"Detecting null values"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#filling-null-values","text":"Sometimes rather than dropping NA values, you\u2019d rather replace them with a valid value. This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values. You could do this in-place using the isnull() method as a mask, but because it is such a common operation Pandas provides the fillna() method, which returns a copy of the array with the null values replaced. data = pd . Series ([ 1 , np . nan , 2 , None , 3 ], index = list ( 'abcde' )) data a 1.0 b NaN c 2.0 d NaN e 3.0 dtype: float64 # filling na values with a single 0 sum ( data . isnull ()) 2 data . fillna ( - 1 ) a 1.0 b -1.0 c 2.0 d -1.0 e 3.0 dtype: float64 #forward-fill --> propagates the previous value forward data . fillna ( method = 'ffill' ) a 1.0 b 1.0 c 2.0 d 2.0 e 3.0 dtype: float64 # Back fill, to propgate the next value backward data . fillna ( method = 'bfill' ) a 1.0 b 2.0 c 2.0 d 3.0 e 3.0 dtype: float64 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . fillna ( method = 'ffill' , axis = 1 ) # if the previous value is not available during a forward fill, the NA value remains .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6.0 9.0 2.0 6.0 6.0 1 7.0 4.0 3.0 7.0 7.0 2 7.0 2.0 5.0 4.0 4.0","title":"Filling null values"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 05 - Hierarchical Indexing \u00b6 Hierarchical Indexing (Multi-indexing) A Multiply Indexed Series Methods of MultiIndex Creation Explicit MultiIndex constructors MultiIndex level names MultiIndex for columns Indexing and Slicing a MultiIndex Rearranging Multi-Indices Sorted and unsorted indices Index setting and resetting Data Aggregations on Multi-Indices Hierarchical Indexing (Multi-indexing) \u00b6 a far more common pattern in practice is to make use of hierarchical indexing (also known as multi-indexing) to incorporate multiple index levels within a single index. In this way, higher-dimensional data can be compactly represented within the familiar one-dimensional Series and two-dimensional DataFrame objects. A Multiply Indexed Series \u00b6 Let\u2019s start by considering how we might represent two-dimensional data within a one-dimensional Series. For concreteness, we will consider a series of data where each point has a character and numerical key. import numpy as np import pandas as pd # the bad way index = [( 'California' , 2000 ), ( 'California' , 2010 ), ( 'New York' , 2000 ), ( 'New York' , 2010 ), ( 'Texas' , 2000 ), ( 'Texas' , 2010 )] populations = [ 33871648 , 37253956 , 18976457 , 19378102 , 20851820 , 25145561 ] pop = pd . Series ( populations , index = index ) pop (California, 2000) 33871648 (California, 2010) 37253956 (New York, 2000) 18976457 (New York, 2010) 19378102 (Texas, 2000) 20851820 (Texas, 2010) 25145561 dtype: int64 # Indexing pop [( 'New York' , 2010 ):( 'Texas' , 2010 )] (New York, 2010) 19378102 (Texas, 2000) 20851820 (Texas, 2010) 25145561 dtype: int64 #A better way index = pd . MultiIndex . from_tuples ( index ) index MultiIndex([('California', 2000), ('California', 2010), ( 'New York', 2000), ( 'New York', 2010), ( 'Texas', 2000), ( 'Texas', 2010)], ) pop = pop . reindex ( index ) pop California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop [:, 2010 ] California 37253956 New York 19378102 Texas 25145561 dtype: int64 # Multi-index as extr dimension pop_df = pop . unstack () pop_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2000 2010 California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561 pop_df . stack () California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop_df = pd . DataFrame ({ 'total' : pop , 'under18' : [ 9267089 , 9284094 , 4687374 , 4318033 , 5906301 , 6879014 ]}) pop_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total under18 California 2000 33871648 9267089 2010 37253956 9284094 New York 2000 18976457 4687374 2010 19378102 4318033 Texas 2000 20851820 5906301 2010 25145561 6879014 Methods of MultiIndex Creation \u00b6 The most straightforward way to construct a multiply indexed Series or DataFrame is to simply pass a list of two or more index arrays to the constructor. df = pd . DataFrame ( np . random . rand ( 4 , 2 ), index = [[ 'a' , 'a' , 'b' , 'b' ],[ 1 , 2 , 1 , 2 ]], columns = [ 'data1' , 'data2' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 a 1 0.812128 0.312338 2 0.769851 0.255045 b 1 0.904529 0.364216 2 0.139294 0.501778 data = {( 'California' , 2000 ): 33871648 , ( 'California' , 2010 ): 37253956 , ( 'Texas' , 2000 ): 20851820 , ( 'Texas' , 2010 ): 25145561 , ( 'New York' , 2000 ): 18976457 , ( 'New York' , 2010 ): 19378102 } pd . Series ( data ) California 2000 33871648 2010 37253956 Texas 2000 20851820 2010 25145561 New York 2000 18976457 2010 19378102 dtype: int64 Explicit MultiIndex constructors \u00b6 For more flexibility in how the index is constructed, you can instead use the class method constructors available in the pd.MultiIndex. For example, as we did before, you can construct the MultiIndex from a simple list of arrays, giving the index values within each level: pd . MultiIndex . from_arrays ([[ 'a' , 'a' , 'b' , 'b' ],[ 1 , 2 , 1 , 2 ]]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) pd . MultiIndex . from_tuples ([( 'a' , 1 ),( 'a' , 2 ),( 'b' , 1 ),( 'b' , 2 )]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) pd . MultiIndex . from_product ([[ 'a' , 'b' ],[ 1 , 2 ]]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) MultiIndex level names \u00b6 Sometimes it is convenient to name the levels of the MultiIndex. You can accomplish this by passing the names argument to any of the above MultiIndex constructors, or by setting the names attribute of the index after the fact: pop . index MultiIndex([('California', 2000), ('California', 2010), ( 'New York', 2000), ( 'New York', 2010), ( 'Texas', 2000), ( 'Texas', 2010)], ) pop . index . names = [ 'state' , 'year' ] pop state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year 2000 2010 state California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561 MultiIndex for columns \u00b6 In a DataFrame, the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well # hierarchical indices and columns index = pd . MultiIndex . from_product ([[ 2013 , 2014 ], [ 1 , 2 ]], names = [ 'year' , 'visit' ]) columns = pd . MultiIndex . from_product ([[ 'Bob' , 'Guido' , 'Sue' ], [ 'HR' , 'Temp' ]], names = [ 'subject' , 'type' ]) # mock some data data = np . round ( np . random . randn ( 4 , 6 ), 1 ) data [:, :: 2 ] *= 10 data += 37 # create the DataFrame health_data = pd . DataFrame ( data , index = index , columns = columns ) health_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } subject Bob Guido Sue type HR Temp HR Temp HR Temp year visit 2013 1 43.0 36.1 37.0 35.9 39.0 38.6 2 13.0 35.6 36.0 37.6 30.0 35.8 2014 1 35.0 38.2 47.0 35.6 43.0 37.2 2 43.0 37.8 25.0 36.4 34.0 36.7 health_data [ 'Guido' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type HR Temp year visit 2013 1 37.0 35.9 2 36.0 37.6 2014 1 47.0 35.6 2 25.0 36.4 Indexing and Slicing a MultiIndex \u00b6 Indexing and slicing on a MultiIndex is designed to be intuitive, and it helps if you think about the indices as added dimensions. We\u2019ll first look at indexing multiply indexed Series, and then multiply indexed DataFrames. # Mutiply indexed Series pop state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop [ 'California' ] year 2000 33871648 2010 37253956 dtype: int64 pop [ 'California' ][ '2000' ] KeyError: '2000' pop [ 'California' , '2000' ] pop . loc [ 'California' : 'New York' ] pop [:, 2000 ] pop [ pop > 2200000 ] pop [[ 'California' , 'Texas' ]] Rearranging Multi-Indices \u00b6 One of the keys to working with multiply indexed data is knowing how to effectively transform the data. There are a number of operations that will preserve all the infor\u2010 mation in the dataset, but rearrange it for the purposes of various computations. We saw a brief example of this in the stack() and unstack() methods, but there are many more ways to finely control the rearrangement of data between hierarchical indices and columns, Sorted and unsorted indices \u00b6 Earlier, we briefly mentioned a caveat, but we should emphasize it more here. Many of the MultiIndex slicing operations will fail if the index is not sorted. Let\u2019s take a look at this here. We\u2019ll start by creating some simple multiply indexed data where the indices are not lexographically sorted: index = pd . MultiIndex . from_product ([[ 'a' , 'c' , 'b' ], [ 1 , 2 ]]) data = pd . Series ( np . random . rand ( 6 ), index = index ) data . index . names = [ 'char' , 'int' ] data try : data [ 'a' : 'b' ] except KeyError as e : print ( type ( e )) print ( e ) data = data . sort_index () data try : print ( data [ 'a' : 'b' ]) except KeyError as e : print ( type ( e )) print ( e ) pop pop . unstack ( level = 0 ) pop . unstack ( level = 1 ) pop . unstack () . stack () # Get the original dataset Index setting and resetting \u00b6 Another way to rearrange hierarchical data is to turn the index labels into columns; this can be accomplished with the reset_index method. Calling this on the popula\u2010 tion dictionary will result in a DataFrame with a state and year column holding the information that was formerly in the index. pop_flat = pop . reset_index ( name = 'population' ) pop_flat pop_flat . set_index ([ 'state' , 'year' ]) Data Aggregations on Multi-Indices \u00b6 We\u2019ve previously seen that Pandas has built-in data aggregation methods, such as mean(), sum(), and max(). For hierarchically indexed data, these can be passed a level parameter that controls which subset of the data the aggregate is computed on health_data data_mean = health_data . mean ( level = 'year' ) data_mean data_mean = health_data . mean ( axis = 1 , level = 'type' ) data_mean #pip install -U jupyter notebook Requirement already satisfied: jupyter in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (1.0.0) Requirement already satisfied: notebook in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (6.4.8) Collecting notebook Downloading notebook-6.4.12-py3-none-any.whl (9.9 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.9/9.9 MB 3.2 MB/s eta 0:00:00m eta 0:00:010:01:010m Requirement already satisfied: ipywidgets in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (7.6.5) Requirement already satisfied: ipykernel in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.9.1) Requirement already satisfied: qtconsole in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (5.3.0) Requirement already satisfied: nbconvert in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.4.4) Requirement already satisfied: jupyter-console in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.4.0) Requirement already satisfied: terminado>=0.8.3 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.13.1) Requirement already satisfied: argon2-cffi in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (21.3.0) Requirement already satisfied: ipython-genutils in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.2.0) Requirement already satisfied: nest-asyncio>=1.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (1.5.5) Requirement already satisfied: prometheus-client in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.14.1) Requirement already satisfied: jupyter-core>=4.6.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (4.10.0) Requirement already satisfied: traitlets>=4.2.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (5.1.1) Requirement already satisfied: jinja2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (3.1.2) Requirement already satisfied: jupyter-client>=5.3.4 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (6.1.12) Requirement already satisfied: nbformat in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (5.3.0) Requirement already satisfied: Send2Trash>=1.8.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (1.8.0) Requirement already satisfied: pyzmq>=17 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (23.2.0) Requirement already satisfied: tornado>=6.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (6.1) Requirement already satisfied: python-dateutil>=2.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter-client>=5.3.4->notebook) (2.8.2) Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.5.13) Requirement already satisfied: bleach in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (4.1.0) Requirement already satisfied: pandocfilters>=1.4.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (1.5.0) Requirement already satisfied: jupyterlab-pygments in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.1.2) Requirement already satisfied: pygments>=2.4.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (2.11.2) Requirement already satisfied: testpath in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.6.0) Requirement already satisfied: defusedxml in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.7.1) Requirement already satisfied: beautifulsoup4 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (4.11.1) Requirement already satisfied: entrypoints>=0.2.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.4) Requirement already satisfied: mistune<2,>=0.8.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.8.4) Requirement already satisfied: MarkupSafe>=2.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jinja2->notebook) (2.1.1) Requirement already satisfied: jsonschema>=2.6 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbformat->notebook) (4.4.0) Requirement already satisfied: fastjsonschema in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbformat->notebook) (2.15.1) Requirement already satisfied: ptyprocess in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from terminado>=0.8.3->notebook) (0.7.0) Requirement already satisfied: argon2-cffi-bindings in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook) (21.2.0) Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (1.5.1) Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (0.1.2) Requirement already satisfied: ipython>=7.23.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (8.2.0) Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter) (3.5.2) Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter) (1.0.0) Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter-console->jupyter) (3.0.20) Requirement already satisfied: qtpy>=2.0.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from qtconsole->jupyter) (2.0.1) Requirement already satisfied: decorator in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1) Requirement already satisfied: jedi>=0.16 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.1) Requirement already satisfied: setuptools>=18.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (63.4.1) Requirement already satisfied: pexpect>4.3 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0) Requirement already satisfied: pickleshare in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5) Requirement already satisfied: stack-data in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0) Requirement already satisfied: backcall in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0) Requirement already satisfied: attrs>=17.4.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->notebook) (21.4.0) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->notebook) (0.18.0) Requirement already satisfied: wcwidth in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.5) Requirement already satisfied: six>=1.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.1->jupyter-client>=5.3.4->notebook) (1.16.0) Requirement already satisfied: packaging in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from qtpy>=2.0.1->qtconsole->jupyter) (21.3) Requirement already satisfied: cffi>=1.0.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook) (1.15.1) Requirement already satisfied: soupsieve>1.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.3.1) Requirement already satisfied: webencodings in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->jupyter) (0.5.1) Requirement already satisfied: pycparser in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook) (2.21) Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from packaging->qtpy>=2.0.1->qtconsole->jupyter) (3.0.4) Requirement already satisfied: executing in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.8.3) Requirement already satisfied: asttokens in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.5) Requirement already satisfied: pure-eval in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2) Installing collected packages: notebook Attempting uninstall: notebook Found existing installation: notebook 6.4.8 Uninstalling notebook-6.4.8: Successfully uninstalled notebook-6.4.8 Successfully installed notebook-6.4.12 Note: you may need to restart the kernel to use updated packages.","title":"05 Hierarchical Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#05-hierarchical-indexing","text":"Hierarchical Indexing (Multi-indexing) A Multiply Indexed Series Methods of MultiIndex Creation Explicit MultiIndex constructors MultiIndex level names MultiIndex for columns Indexing and Slicing a MultiIndex Rearranging Multi-Indices Sorted and unsorted indices Index setting and resetting Data Aggregations on Multi-Indices","title":"05 - Hierarchical Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#hierarchical-indexing-multi-indexing","text":"a far more common pattern in practice is to make use of hierarchical indexing (also known as multi-indexing) to incorporate multiple index levels within a single index. In this way, higher-dimensional data can be compactly represented within the familiar one-dimensional Series and two-dimensional DataFrame objects.","title":"Hierarchical Indexing (Multi-indexing)"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#a-multiply-indexed-series","text":"Let\u2019s start by considering how we might represent two-dimensional data within a one-dimensional Series. For concreteness, we will consider a series of data where each point has a character and numerical key. import numpy as np import pandas as pd # the bad way index = [( 'California' , 2000 ), ( 'California' , 2010 ), ( 'New York' , 2000 ), ( 'New York' , 2010 ), ( 'Texas' , 2000 ), ( 'Texas' , 2010 )] populations = [ 33871648 , 37253956 , 18976457 , 19378102 , 20851820 , 25145561 ] pop = pd . Series ( populations , index = index ) pop (California, 2000) 33871648 (California, 2010) 37253956 (New York, 2000) 18976457 (New York, 2010) 19378102 (Texas, 2000) 20851820 (Texas, 2010) 25145561 dtype: int64 # Indexing pop [( 'New York' , 2010 ):( 'Texas' , 2010 )] (New York, 2010) 19378102 (Texas, 2000) 20851820 (Texas, 2010) 25145561 dtype: int64 #A better way index = pd . MultiIndex . from_tuples ( index ) index MultiIndex([('California', 2000), ('California', 2010), ( 'New York', 2000), ( 'New York', 2010), ( 'Texas', 2000), ( 'Texas', 2010)], ) pop = pop . reindex ( index ) pop California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop [:, 2010 ] California 37253956 New York 19378102 Texas 25145561 dtype: int64 # Multi-index as extr dimension pop_df = pop . unstack () pop_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2000 2010 California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561 pop_df . stack () California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop_df = pd . DataFrame ({ 'total' : pop , 'under18' : [ 9267089 , 9284094 , 4687374 , 4318033 , 5906301 , 6879014 ]}) pop_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total under18 California 2000 33871648 9267089 2010 37253956 9284094 New York 2000 18976457 4687374 2010 19378102 4318033 Texas 2000 20851820 5906301 2010 25145561 6879014","title":"A Multiply Indexed Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#methods-of-multiindex-creation","text":"The most straightforward way to construct a multiply indexed Series or DataFrame is to simply pass a list of two or more index arrays to the constructor. df = pd . DataFrame ( np . random . rand ( 4 , 2 ), index = [[ 'a' , 'a' , 'b' , 'b' ],[ 1 , 2 , 1 , 2 ]], columns = [ 'data1' , 'data2' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 a 1 0.812128 0.312338 2 0.769851 0.255045 b 1 0.904529 0.364216 2 0.139294 0.501778 data = {( 'California' , 2000 ): 33871648 , ( 'California' , 2010 ): 37253956 , ( 'Texas' , 2000 ): 20851820 , ( 'Texas' , 2010 ): 25145561 , ( 'New York' , 2000 ): 18976457 , ( 'New York' , 2010 ): 19378102 } pd . Series ( data ) California 2000 33871648 2010 37253956 Texas 2000 20851820 2010 25145561 New York 2000 18976457 2010 19378102 dtype: int64","title":"Methods of MultiIndex Creation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#explicit-multiindex-constructors","text":"For more flexibility in how the index is constructed, you can instead use the class method constructors available in the pd.MultiIndex. For example, as we did before, you can construct the MultiIndex from a simple list of arrays, giving the index values within each level: pd . MultiIndex . from_arrays ([[ 'a' , 'a' , 'b' , 'b' ],[ 1 , 2 , 1 , 2 ]]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) pd . MultiIndex . from_tuples ([( 'a' , 1 ),( 'a' , 2 ),( 'b' , 1 ),( 'b' , 2 )]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) pd . MultiIndex . from_product ([[ 'a' , 'b' ],[ 1 , 2 ]]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], )","title":"Explicit MultiIndex constructors"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#multiindex-level-names","text":"Sometimes it is convenient to name the levels of the MultiIndex. You can accomplish this by passing the names argument to any of the above MultiIndex constructors, or by setting the names attribute of the index after the fact: pop . index MultiIndex([('California', 2000), ('California', 2010), ( 'New York', 2000), ( 'New York', 2010), ( 'Texas', 2000), ( 'Texas', 2010)], ) pop . index . names = [ 'state' , 'year' ] pop state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year 2000 2010 state California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561","title":"MultiIndex level names"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#multiindex-for-columns","text":"In a DataFrame, the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well # hierarchical indices and columns index = pd . MultiIndex . from_product ([[ 2013 , 2014 ], [ 1 , 2 ]], names = [ 'year' , 'visit' ]) columns = pd . MultiIndex . from_product ([[ 'Bob' , 'Guido' , 'Sue' ], [ 'HR' , 'Temp' ]], names = [ 'subject' , 'type' ]) # mock some data data = np . round ( np . random . randn ( 4 , 6 ), 1 ) data [:, :: 2 ] *= 10 data += 37 # create the DataFrame health_data = pd . DataFrame ( data , index = index , columns = columns ) health_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } subject Bob Guido Sue type HR Temp HR Temp HR Temp year visit 2013 1 43.0 36.1 37.0 35.9 39.0 38.6 2 13.0 35.6 36.0 37.6 30.0 35.8 2014 1 35.0 38.2 47.0 35.6 43.0 37.2 2 43.0 37.8 25.0 36.4 34.0 36.7 health_data [ 'Guido' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type HR Temp year visit 2013 1 37.0 35.9 2 36.0 37.6 2014 1 47.0 35.6 2 25.0 36.4","title":"MultiIndex for columns"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#indexing-and-slicing-a-multiindex","text":"Indexing and slicing on a MultiIndex is designed to be intuitive, and it helps if you think about the indices as added dimensions. We\u2019ll first look at indexing multiply indexed Series, and then multiply indexed DataFrames. # Mutiply indexed Series pop state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop [ 'California' ] year 2000 33871648 2010 37253956 dtype: int64 pop [ 'California' ][ '2000' ] KeyError: '2000' pop [ 'California' , '2000' ] pop . loc [ 'California' : 'New York' ] pop [:, 2000 ] pop [ pop > 2200000 ] pop [[ 'California' , 'Texas' ]]","title":"Indexing and Slicing a MultiIndex"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#rearranging-multi-indices","text":"One of the keys to working with multiply indexed data is knowing how to effectively transform the data. There are a number of operations that will preserve all the infor\u2010 mation in the dataset, but rearrange it for the purposes of various computations. We saw a brief example of this in the stack() and unstack() methods, but there are many more ways to finely control the rearrangement of data between hierarchical indices and columns,","title":"Rearranging Multi-Indices"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#sorted-and-unsorted-indices","text":"Earlier, we briefly mentioned a caveat, but we should emphasize it more here. Many of the MultiIndex slicing operations will fail if the index is not sorted. Let\u2019s take a look at this here. We\u2019ll start by creating some simple multiply indexed data where the indices are not lexographically sorted: index = pd . MultiIndex . from_product ([[ 'a' , 'c' , 'b' ], [ 1 , 2 ]]) data = pd . Series ( np . random . rand ( 6 ), index = index ) data . index . names = [ 'char' , 'int' ] data try : data [ 'a' : 'b' ] except KeyError as e : print ( type ( e )) print ( e ) data = data . sort_index () data try : print ( data [ 'a' : 'b' ]) except KeyError as e : print ( type ( e )) print ( e ) pop pop . unstack ( level = 0 ) pop . unstack ( level = 1 ) pop . unstack () . stack () # Get the original dataset","title":"Sorted and unsorted indices"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#index-setting-and-resetting","text":"Another way to rearrange hierarchical data is to turn the index labels into columns; this can be accomplished with the reset_index method. Calling this on the popula\u2010 tion dictionary will result in a DataFrame with a state and year column holding the information that was formerly in the index. pop_flat = pop . reset_index ( name = 'population' ) pop_flat pop_flat . set_index ([ 'state' , 'year' ])","title":"Index setting and resetting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#data-aggregations-on-multi-indices","text":"We\u2019ve previously seen that Pandas has built-in data aggregation methods, such as mean(), sum(), and max(). For hierarchically indexed data, these can be passed a level parameter that controls which subset of the data the aggregate is computed on health_data data_mean = health_data . mean ( level = 'year' ) data_mean data_mean = health_data . mean ( axis = 1 , level = 'type' ) data_mean #pip install -U jupyter notebook Requirement already satisfied: jupyter in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (1.0.0) Requirement already satisfied: notebook in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (6.4.8) Collecting notebook Downloading notebook-6.4.12-py3-none-any.whl (9.9 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.9/9.9 MB 3.2 MB/s eta 0:00:00m eta 0:00:010:01:010m Requirement already satisfied: ipywidgets in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (7.6.5) Requirement already satisfied: ipykernel in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.9.1) Requirement already satisfied: qtconsole in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (5.3.0) Requirement already satisfied: nbconvert in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.4.4) Requirement already satisfied: jupyter-console in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.4.0) Requirement already satisfied: terminado>=0.8.3 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.13.1) Requirement already satisfied: argon2-cffi in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (21.3.0) Requirement already satisfied: ipython-genutils in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.2.0) Requirement already satisfied: nest-asyncio>=1.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (1.5.5) Requirement already satisfied: prometheus-client in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.14.1) Requirement already satisfied: jupyter-core>=4.6.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (4.10.0) Requirement already satisfied: traitlets>=4.2.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (5.1.1) Requirement already satisfied: jinja2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (3.1.2) Requirement already satisfied: jupyter-client>=5.3.4 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (6.1.12) Requirement already satisfied: nbformat in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (5.3.0) Requirement already satisfied: Send2Trash>=1.8.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (1.8.0) Requirement already satisfied: pyzmq>=17 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (23.2.0) Requirement already satisfied: tornado>=6.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (6.1) Requirement already satisfied: python-dateutil>=2.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter-client>=5.3.4->notebook) (2.8.2) Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.5.13) Requirement already satisfied: bleach in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (4.1.0) Requirement already satisfied: pandocfilters>=1.4.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (1.5.0) Requirement already satisfied: jupyterlab-pygments in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.1.2) Requirement already satisfied: pygments>=2.4.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (2.11.2) Requirement already satisfied: testpath in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.6.0) Requirement already satisfied: defusedxml in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.7.1) Requirement already satisfied: beautifulsoup4 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (4.11.1) Requirement already satisfied: entrypoints>=0.2.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.4) Requirement already satisfied: mistune<2,>=0.8.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.8.4) Requirement already satisfied: MarkupSafe>=2.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jinja2->notebook) (2.1.1) Requirement already satisfied: jsonschema>=2.6 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbformat->notebook) (4.4.0) Requirement already satisfied: fastjsonschema in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbformat->notebook) (2.15.1) Requirement already satisfied: ptyprocess in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from terminado>=0.8.3->notebook) (0.7.0) Requirement already satisfied: argon2-cffi-bindings in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook) (21.2.0) Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (1.5.1) Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (0.1.2) Requirement already satisfied: ipython>=7.23.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (8.2.0) Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter) (3.5.2) Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter) (1.0.0) Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter-console->jupyter) (3.0.20) Requirement already satisfied: qtpy>=2.0.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from qtconsole->jupyter) (2.0.1) Requirement already satisfied: decorator in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1) Requirement already satisfied: jedi>=0.16 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.1) Requirement already satisfied: setuptools>=18.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (63.4.1) Requirement already satisfied: pexpect>4.3 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0) Requirement already satisfied: pickleshare in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5) Requirement already satisfied: stack-data in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0) Requirement already satisfied: backcall in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0) Requirement already satisfied: attrs>=17.4.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->notebook) (21.4.0) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->notebook) (0.18.0) Requirement already satisfied: wcwidth in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.5) Requirement already satisfied: six>=1.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.1->jupyter-client>=5.3.4->notebook) (1.16.0) Requirement already satisfied: packaging in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from qtpy>=2.0.1->qtconsole->jupyter) (21.3) Requirement already satisfied: cffi>=1.0.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook) (1.15.1) Requirement already satisfied: soupsieve>1.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.3.1) Requirement already satisfied: webencodings in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->jupyter) (0.5.1) Requirement already satisfied: pycparser in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook) (2.21) Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from packaging->qtpy>=2.0.1->qtconsole->jupyter) (3.0.4) Requirement already satisfied: executing in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.8.3) Requirement already satisfied: asttokens in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.5) Requirement already satisfied: pure-eval in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2) Installing collected packages: notebook Attempting uninstall: notebook Found existing installation: notebook 6.4.8 Uninstalling notebook-6.4.8: Successfully uninstalled notebook-6.4.8 Successfully installed notebook-6.4.12 Note: you may need to restart the kernel to use updated packages.","title":"Data Aggregations on Multi-Indices"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 06 - Combine Dataset: Concate and Append \u00b6 Combining Datasets: Concat and Append Simple Concatenation with pd.concat Duplicate indices Catching the repeats as an error. If you\u2019d like to simply verify that the indices in the Ignoring the index. Sometimes the index itself does not matter, and you would prefer Adding MultiIndex keys. Another alternative is to use the keys option to specify a label Concatenation with joins The append() method Combining Datasets: Concat and Append \u00b6 Some of the most interesting studies of data come from combining different data sources. These operations can involve anything from very straightforward concatena\u2010 tion of two different datasets, to more complicated database-style joins and merges that correctly handle any overlaps between the datasets. Series and DataFrames are built with this type of operation in mind, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward. import numpy as np import pandas as pd def make_df ( cols , ind ): data = { c :[ str ( c ) + str ( i ) for i in ind ] for c in cols } return pd . DataFrame ( data , ind ) make_df ( 'ABC' , range ( 3 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C 0 A0 B0 C0 1 A1 B1 C1 2 A2 B2 C2 # Recall Numpy concatenation x = [ 1 , 2 , 3 ] y = [ 4 , 3 , 6 ] z = [ 9 , 8 , 0 ] np . concatenate ([ x , y , z ]) array([1, 2, 3, 4, 3, 6, 9, 8, 0]) x = [[ 2 , 4 ],[ 3 , 5 ]] x = np . concatenate ([ x , x ], axis = 1 ) x array([[2, 4, 2, 4], [3, 5, 3, 5]]) Simple Concatenation with pd.concat \u00b6 Pandas has a function, pd.concat(), which has a similar syntax to np.concatenate but contains a number of options that we\u2019ll discuss momentarily #Signature in Pandas v0.18 pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True) ser1 = pd . Series ([ 'A' , 'B' , 'C' ], index = [ 1 , 2 , 3 ]) ser2 = pd . Series ([ 'D' , 'E' , 'F' ], index = [ 4 , 5 , 6 ]) pd . concat ([ ser1 , ser2 ]) 1 A 2 B 3 C 4 D 5 E 6 F dtype: object df3 = make_df ( \"AB\" ,[ 0 , 1 ]) df4 = make_df ( \"CD\" ,[ 0 , 1 ]) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 df4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C D 0 C0 D0 1 C1 D1 pd . concat ([ df3 , df4 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 NaN NaN 1 A1 B1 NaN NaN 0 NaN NaN C0 D0 1 NaN NaN C1 D1 pd . concat ([ df3 , df4 ], axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 Duplicate indices \u00b6 One important difference between np.concatenate and pd.concat is that Pandas concatenation preserves indices, even if the result will have duplicate indices! x = make_df ( \"AB\" ,[ 0 , 1 ]) y = make_df ( \"AB\" ,[ 2 , 3 ]) x .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A2 B2 3 A3 B3 y . index Int64Index([2, 3], dtype='int64') x . index = y . index x . index Int64Index([2, 3], dtype='int64') x .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A0 B0 3 A1 B1 y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A2 B2 3 A3 B3 pd . concat ([ x , y ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A0 B0 3 A1 B1 2 A2 B2 3 A3 B3 pd . concat ([ x , y ], axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B A B 2 A0 B0 A2 B2 3 A1 B1 A3 B3 Catching the repeats as an error. If you\u2019d like to simply verify that the indices in the \u00b6 result of pd.concat() do not overlap, you can specify the verify_integrity flag. With this set to True, the concatenation will raise an exception if there are duplicate indices. Here is an example, where for clarity we\u2019ll catch and print the error message: try : pd . concat ([ x , y ], verify_integrity = True ) except ValueError as e : print ( \"ValueError:\" , e ) ValueError: Indexes have overlapping values: Int64Index([2, 3], dtype='int64') Ignoring the index. Sometimes the index itself does not matter, and you would prefer \u00b6 it to simply be ignored. You can specify this option using the ignore_index flag. With this set to True, the concatenation will create a new integer index for the resulting Series: print ( x ); print ( y ) A B 2 A0 B0 3 A1 B1 A B 2 A2 B2 3 A3 B3 pd . concat ([ x , y ], ignore_index = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 2 A2 B2 3 A3 B3 Adding MultiIndex keys. Another alternative is to use the keys option to specify a label \u00b6 for the data sources; the result will be a hierarchically indexed series containing the data: pd . concat ([ x , y ], keys = [ 'x' , 'y' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B x 2 A0 B0 3 A1 B1 y 2 A2 B2 3 A3 B3 Concatenation with joins \u00b6 In the simple examples we just looked at, we were mainly concatenating DataFrames with shared column names. In practice, data from different sources might have differ\u2010 ent sets of column names, and pd.concat offers several options in this case. Consider the concatenation of the following two DataFrames, which have some (but not all!) columns in common: df5 = make_df ( 'ABC' ,[ 1 , 2 ]) df6 = make_df ( 'BCD' ,[ 3 , 4 ]) df6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C D 3 B3 C3 D3 4 B4 C4 D4 df5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C 1 A1 B1 C1 2 A2 B2 C2 pd . concat ([ df5 , df6 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 1 A1 B1 C1 NaN 2 A2 B2 C2 NaN 3 NaN B3 C3 D3 4 NaN B4 C4 D4 By default, the entries for which no data is available are filled with NA values. To change this, we can specify one of several options for the join and join_axes param\u2010 eters of the concatenate function. pd . concat ([ df5 , df6 ], join = 'inner' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C 1 B1 C1 2 B2 C2 3 B3 C3 4 B4 C4 pd . concat ([ df5 , df6 ], join = 'outer' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 1 A1 B1 C1 NaN 2 A2 B2 C2 NaN 3 NaN B3 C3 D3 4 NaN B4 C4 D4 The append() method \u00b6 Because direct array concatenation is so common, Series and DataFrame objects have an append method that can accomplish the same thing in fewer keystrokes. For example, rather than calling pd.concat([df1, df2]), you can simply call df1.append(df2): df1 = make_df ( 'AB' ,[ 1 , 2 ]) df2 = make_df ( 'AB' ,[ 3 , 4 ]) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 1 A1 B1 2 A2 B2 df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 3 A3 B3 4 A4 B4 df1 . append ( df2 ) /tmp/ipykernel_36950/3062608662.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df1.append(df2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 1 A1 B1 2 A2 B2 3 A3 B3 4 A4 B4","title":"06 Combine dataset Concat and Append"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#06-combine-dataset-concate-and-append","text":"Combining Datasets: Concat and Append Simple Concatenation with pd.concat Duplicate indices Catching the repeats as an error. If you\u2019d like to simply verify that the indices in the Ignoring the index. Sometimes the index itself does not matter, and you would prefer Adding MultiIndex keys. Another alternative is to use the keys option to specify a label Concatenation with joins The append() method","title":"06 - Combine Dataset: Concate and Append"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#combining-datasets-concat-and-append","text":"Some of the most interesting studies of data come from combining different data sources. These operations can involve anything from very straightforward concatena\u2010 tion of two different datasets, to more complicated database-style joins and merges that correctly handle any overlaps between the datasets. Series and DataFrames are built with this type of operation in mind, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward. import numpy as np import pandas as pd def make_df ( cols , ind ): data = { c :[ str ( c ) + str ( i ) for i in ind ] for c in cols } return pd . DataFrame ( data , ind ) make_df ( 'ABC' , range ( 3 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C 0 A0 B0 C0 1 A1 B1 C1 2 A2 B2 C2 # Recall Numpy concatenation x = [ 1 , 2 , 3 ] y = [ 4 , 3 , 6 ] z = [ 9 , 8 , 0 ] np . concatenate ([ x , y , z ]) array([1, 2, 3, 4, 3, 6, 9, 8, 0]) x = [[ 2 , 4 ],[ 3 , 5 ]] x = np . concatenate ([ x , x ], axis = 1 ) x array([[2, 4, 2, 4], [3, 5, 3, 5]])","title":"Combining Datasets: Concat and Append"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#simple-concatenation-with-pdconcat","text":"Pandas has a function, pd.concat(), which has a similar syntax to np.concatenate but contains a number of options that we\u2019ll discuss momentarily #Signature in Pandas v0.18 pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True) ser1 = pd . Series ([ 'A' , 'B' , 'C' ], index = [ 1 , 2 , 3 ]) ser2 = pd . Series ([ 'D' , 'E' , 'F' ], index = [ 4 , 5 , 6 ]) pd . concat ([ ser1 , ser2 ]) 1 A 2 B 3 C 4 D 5 E 6 F dtype: object df3 = make_df ( \"AB\" ,[ 0 , 1 ]) df4 = make_df ( \"CD\" ,[ 0 , 1 ]) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 df4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C D 0 C0 D0 1 C1 D1 pd . concat ([ df3 , df4 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 NaN NaN 1 A1 B1 NaN NaN 0 NaN NaN C0 D0 1 NaN NaN C1 D1 pd . concat ([ df3 , df4 ], axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1","title":"Simple Concatenation with pd.concat"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#duplicate-indices","text":"One important difference between np.concatenate and pd.concat is that Pandas concatenation preserves indices, even if the result will have duplicate indices! x = make_df ( \"AB\" ,[ 0 , 1 ]) y = make_df ( \"AB\" ,[ 2 , 3 ]) x .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A2 B2 3 A3 B3 y . index Int64Index([2, 3], dtype='int64') x . index = y . index x . index Int64Index([2, 3], dtype='int64') x .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A0 B0 3 A1 B1 y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A2 B2 3 A3 B3 pd . concat ([ x , y ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A0 B0 3 A1 B1 2 A2 B2 3 A3 B3 pd . concat ([ x , y ], axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B A B 2 A0 B0 A2 B2 3 A1 B1 A3 B3","title":"Duplicate indices"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#catching-the-repeats-as-an-error-if-youd-like-to-simply-verify-that-the-indices-in-the","text":"result of pd.concat() do not overlap, you can specify the verify_integrity flag. With this set to True, the concatenation will raise an exception if there are duplicate indices. Here is an example, where for clarity we\u2019ll catch and print the error message: try : pd . concat ([ x , y ], verify_integrity = True ) except ValueError as e : print ( \"ValueError:\" , e ) ValueError: Indexes have overlapping values: Int64Index([2, 3], dtype='int64')","title":"Catching the repeats as an error. If you\u2019d like to simply verify that the indices in the"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#ignoring-the-index-sometimes-the-index-itself-does-not-matter-and-you-would-prefer","text":"it to simply be ignored. You can specify this option using the ignore_index flag. With this set to True, the concatenation will create a new integer index for the resulting Series: print ( x ); print ( y ) A B 2 A0 B0 3 A1 B1 A B 2 A2 B2 3 A3 B3 pd . concat ([ x , y ], ignore_index = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 2 A2 B2 3 A3 B3","title":"Ignoring the index. Sometimes the index itself does not matter, and you would prefer"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#adding-multiindex-keys-another-alternative-is-to-use-the-keys-option-to-specify-a-label","text":"for the data sources; the result will be a hierarchically indexed series containing the data: pd . concat ([ x , y ], keys = [ 'x' , 'y' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B x 2 A0 B0 3 A1 B1 y 2 A2 B2 3 A3 B3","title":"Adding MultiIndex keys. Another alternative is to use the keys option to specify a label"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#concatenation-with-joins","text":"In the simple examples we just looked at, we were mainly concatenating DataFrames with shared column names. In practice, data from different sources might have differ\u2010 ent sets of column names, and pd.concat offers several options in this case. Consider the concatenation of the following two DataFrames, which have some (but not all!) columns in common: df5 = make_df ( 'ABC' ,[ 1 , 2 ]) df6 = make_df ( 'BCD' ,[ 3 , 4 ]) df6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C D 3 B3 C3 D3 4 B4 C4 D4 df5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C 1 A1 B1 C1 2 A2 B2 C2 pd . concat ([ df5 , df6 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 1 A1 B1 C1 NaN 2 A2 B2 C2 NaN 3 NaN B3 C3 D3 4 NaN B4 C4 D4 By default, the entries for which no data is available are filled with NA values. To change this, we can specify one of several options for the join and join_axes param\u2010 eters of the concatenate function. pd . concat ([ df5 , df6 ], join = 'inner' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C 1 B1 C1 2 B2 C2 3 B3 C3 4 B4 C4 pd . concat ([ df5 , df6 ], join = 'outer' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 1 A1 B1 C1 NaN 2 A2 B2 C2 NaN 3 NaN B3 C3 D3 4 NaN B4 C4 D4","title":"Concatenation with joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#the-append-method","text":"Because direct array concatenation is so common, Series and DataFrame objects have an append method that can accomplish the same thing in fewer keystrokes. For example, rather than calling pd.concat([df1, df2]), you can simply call df1.append(df2): df1 = make_df ( 'AB' ,[ 1 , 2 ]) df2 = make_df ( 'AB' ,[ 3 , 4 ]) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 1 A1 B1 2 A2 B2 df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 3 A3 B3 4 A4 B4 df1 . append ( df2 ) /tmp/ipykernel_36950/3062608662.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df1.append(df2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 1 A1 B1 2 A2 B2 3 A3 B3 4 A4 B4","title":"The append() method"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 07 - Combining Datasets: Merge and Join \u00b6 Combining Datasets: Merge and Join Categories of Joins One-to-one joins Many-to-one joins Many-to-many joins Specification of the Merge Key The on keyword The left_on and right_on keywords Specifying Set Arithmetic for Joins Overlapping Column Names: The suffixes Keyword import numpy as np import pandas as pd Combining Datasets: Merge and Join \u00b6 One essential feature offered by Pandas is its high-performance, in-memory join and merge operations. If you have ever worked with databases, you should be familiar with this type of data interaction. The main interface for this is the pd.merge func\u2010 tion, and we\u2019ll see a few examples of how this can work in practice. ### Relational Algebra The behavior implemented in pd.merge() is a subset of what is known as relational algebra, which is a formal set of rules for manipulating relational data, and forms the conceptual foundation of operations available in most databases. The strength of the relational algebra approach is that it proposes several primitive operations, which become the building blocks of more complicated operations on any dataset. With this lexicon of fundamental operations implemented efficiently in a database or other pro\u2010 gram, a wide range of fairly complicated composite operations can be performed. Pandas implements several of these fundamental building blocks in the pd.merge() function and the related join() method of Series and DataFrames. As we will see, these let you efficiently link data from different sources. Categories of Joins \u00b6 The pd.merge() function implements a number of types of joins: the one-to-one, many-to-one, and many-to-many joins. All three types of joins are accessed via an identical call to the pd.merge() interface; the type of join performed depends on the form of the input data. One-to-one joins \u00b6 Perhaps the simplest type of merge expression is the one-to-one join, which is in many ways very similar to the column-wise concatenation df1 = pd . DataFrame ({ 'employee' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'group' : [ 'Accounting' , 'Engineering' , 'Engineering' , 'HR' ]}) df2 = pd . DataFrame ({ 'employee' : [ 'Lisa' , 'Bob' , 'Jake' , 'Sue' ], 'hire_date' : [ 2004 , 2008 , 2012 , 2014 ]}) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee hire_date 0 Lisa 2004 1 Bob 2008 2 Jake 2012 3 Sue 2014 df3 = pd . merge ( df1 , df2 ) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 Many-to-one joins \u00b6 Many-to-one joins are joins in which one of the two key columns contains duplicate entries. For the many-to-one case, the resulting DataFrame will preserve those dupli\u2010 cate entries as appropriate. df4 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Engineering' , 'HR' ], 'supervisor' : [ 'Carly' , 'Guido' , 'Steve' ]}) df4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } group supervisor 0 Accounting Carly 1 Engineering Guido 2 HR Steve df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 pd . merge ( df3 , df4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date supervisor 0 Bob Accounting 2008 Carly 1 Jake Engineering 2012 Guido 2 Lisa Engineering 2004 Guido 3 Sue HR 2014 Steve Many-to-many joins \u00b6 Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined. If the key column in both the left and right array contains duplicates, then the result is a many-to-many merge. This will be perhaps most clear with a concrete example df5 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Accounting' , 'Engineering' , 'Engineering' , 'HR' , 'HR' ], 'skills' : [ 'math' , 'spreadsheets' , 'coding' , 'linux' , 'spreadsheets' , 'organization' ]}) df5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } group skills 0 Accounting math 1 Accounting spreadsheets 2 Engineering coding 3 Engineering linux 4 HR spreadsheets 5 HR organization df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR pd . merge ( df1 , df5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group skills 0 Bob Accounting math 1 Bob Accounting spreadsheets 2 Jake Engineering coding 3 Jake Engineering linux 4 Lisa Engineering coding 5 Lisa Engineering linux 6 Sue HR spreadsheets 7 Sue HR organization Specification of the Merge Key \u00b6 We\u2019ve already seen the default behavior of pd.merge(): it looks for one or more matching column names between the two inputs, and uses this as the key. However, often the column names will not match so nicely, and pd.merge() provides a variety of options for handling this. The on keyword \u00b6 Most simply, you can explicitly specify the name of the key column using the on key\u2010 word, which takes a column name or a list of column names: df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee hire_date 0 Lisa 2004 1 Bob 2008 2 Jake 2012 3 Sue 2014 pd . merge ( df1 , df2 , on = 'employee' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 The left_on and right_on keywords \u00b6 At times you may wish to merge two datasets with different column names; for exam\u2010 ple, we may have a dataset in which the employee name is labeled as \u201cname\u201d rather than \u201cemployee\u201d. In this case, we can use the left_on and right_on keywords to specify the two column names: df3 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'salary' : [ 70000 , 80000 , 120000 , 90000 ]}) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name salary 0 Bob 70000 1 Jake 80000 2 Lisa 120000 3 Sue 90000 df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR pd . merge ( df1 , df3 , left_on = 'employee' , right_on = 'name' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group name salary 0 Bob Accounting Bob 70000 1 Jake Engineering Jake 80000 2 Lisa Engineering Lisa 120000 3 Sue HR Sue 90000 # to drop the redundant column pd . merge ( df1 , df3 , left_on = 'employee' , right_on = 'name' ) . drop ( 'name' , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group salary 0 Bob Accounting 70000 1 Jake Engineering 80000 2 Lisa Engineering 120000 3 Sue HR 90000 Specifying Set Arithmetic for Joins \u00b6 In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join. This comes up when a value appears in one key column but not the other. df6 = pd . DataFrame ({ 'name' : [ 'Peter' , 'Paul' , 'Mary' ], 'food' : [ 'fish' , 'beans' , 'bread' ]}, columns = [ 'name' , 'food' ]) df7 = pd . DataFrame ({ 'name' : [ 'Mary' , 'Joseph' ], 'drink' : [ 'wine' , 'beer' ]}, columns = [ 'name' , 'drink' ]) df6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food 0 Peter fish 1 Paul beans 2 Mary bread df7 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name drink 0 Mary wine 1 Joseph beer pd . merge ( df6 , df7 , how = 'inner' ) # Interection .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Mary bread wine pd . merge ( df6 , df7 , how = 'outer' ) # union .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Peter fish NaN 1 Paul beans NaN 2 Mary bread wine 3 Joseph NaN beer pd . merge ( df6 , df7 , how = 'left' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Peter fish NaN 1 Paul beans NaN 2 Mary bread wine pd . merge ( df6 , df7 , how = 'right' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Mary bread wine 1 Joseph NaN beer Overlapping Column Names: The suffixes Keyword \u00b6 Finally, you may end up in a case where your two input DataFrames have conflicting column names. df8 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 1 , 2 , 3 , 4 ]}) df8 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank 0 Bob 1 1 Jake 2 2 Lisa 3 3 Sue 4 df9 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 3 , 1 , 4 , 2 ]}) df9 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank 0 Bob 3 1 Jake 1 2 Lisa 4 3 Sue 2 pd . merge ( df8 , df9 , on = 'name' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank_x rank_y 0 Bob 1 3 1 Jake 2 1 2 Lisa 3 4 3 Sue 4 2 **Because the output would have two conflicting column names, the merge function automatically appends a suffix _x or _y to make the output columns unique. If these defaults are inappropriate, it is possible to specify a custom suffix using the suffixes keyword:** pd . merge ( df8 , df9 , on = 'name' , suffixes = [ '_L' , '_R' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank_L rank_R 0 Bob 1 3 1 Jake 2 1 2 Lisa 3 4 3 Sue 4 2","title":"07 Combining Dataset Merge and Join"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#07-combining-datasets-merge-and-join","text":"Combining Datasets: Merge and Join Categories of Joins One-to-one joins Many-to-one joins Many-to-many joins Specification of the Merge Key The on keyword The left_on and right_on keywords Specifying Set Arithmetic for Joins Overlapping Column Names: The suffixes Keyword import numpy as np import pandas as pd","title":"07 - Combining Datasets: Merge and Join"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#combining-datasets-merge-and-join","text":"One essential feature offered by Pandas is its high-performance, in-memory join and merge operations. If you have ever worked with databases, you should be familiar with this type of data interaction. The main interface for this is the pd.merge func\u2010 tion, and we\u2019ll see a few examples of how this can work in practice. ### Relational Algebra The behavior implemented in pd.merge() is a subset of what is known as relational algebra, which is a formal set of rules for manipulating relational data, and forms the conceptual foundation of operations available in most databases. The strength of the relational algebra approach is that it proposes several primitive operations, which become the building blocks of more complicated operations on any dataset. With this lexicon of fundamental operations implemented efficiently in a database or other pro\u2010 gram, a wide range of fairly complicated composite operations can be performed. Pandas implements several of these fundamental building blocks in the pd.merge() function and the related join() method of Series and DataFrames. As we will see, these let you efficiently link data from different sources.","title":"Combining Datasets: Merge and Join"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#categories-of-joins","text":"The pd.merge() function implements a number of types of joins: the one-to-one, many-to-one, and many-to-many joins. All three types of joins are accessed via an identical call to the pd.merge() interface; the type of join performed depends on the form of the input data.","title":"Categories of Joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#one-to-one-joins","text":"Perhaps the simplest type of merge expression is the one-to-one join, which is in many ways very similar to the column-wise concatenation df1 = pd . DataFrame ({ 'employee' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'group' : [ 'Accounting' , 'Engineering' , 'Engineering' , 'HR' ]}) df2 = pd . DataFrame ({ 'employee' : [ 'Lisa' , 'Bob' , 'Jake' , 'Sue' ], 'hire_date' : [ 2004 , 2008 , 2012 , 2014 ]}) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee hire_date 0 Lisa 2004 1 Bob 2008 2 Jake 2012 3 Sue 2014 df3 = pd . merge ( df1 , df2 ) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014","title":"One-to-one joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#many-to-one-joins","text":"Many-to-one joins are joins in which one of the two key columns contains duplicate entries. For the many-to-one case, the resulting DataFrame will preserve those dupli\u2010 cate entries as appropriate. df4 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Engineering' , 'HR' ], 'supervisor' : [ 'Carly' , 'Guido' , 'Steve' ]}) df4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } group supervisor 0 Accounting Carly 1 Engineering Guido 2 HR Steve df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 pd . merge ( df3 , df4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date supervisor 0 Bob Accounting 2008 Carly 1 Jake Engineering 2012 Guido 2 Lisa Engineering 2004 Guido 3 Sue HR 2014 Steve","title":"Many-to-one joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#many-to-many-joins","text":"Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined. If the key column in both the left and right array contains duplicates, then the result is a many-to-many merge. This will be perhaps most clear with a concrete example df5 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Accounting' , 'Engineering' , 'Engineering' , 'HR' , 'HR' ], 'skills' : [ 'math' , 'spreadsheets' , 'coding' , 'linux' , 'spreadsheets' , 'organization' ]}) df5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } group skills 0 Accounting math 1 Accounting spreadsheets 2 Engineering coding 3 Engineering linux 4 HR spreadsheets 5 HR organization df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR pd . merge ( df1 , df5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group skills 0 Bob Accounting math 1 Bob Accounting spreadsheets 2 Jake Engineering coding 3 Jake Engineering linux 4 Lisa Engineering coding 5 Lisa Engineering linux 6 Sue HR spreadsheets 7 Sue HR organization","title":"Many-to-many joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#specification-of-the-merge-key","text":"We\u2019ve already seen the default behavior of pd.merge(): it looks for one or more matching column names between the two inputs, and uses this as the key. However, often the column names will not match so nicely, and pd.merge() provides a variety of options for handling this.","title":"Specification of the Merge Key"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#the-on-keyword","text":"Most simply, you can explicitly specify the name of the key column using the on key\u2010 word, which takes a column name or a list of column names: df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee hire_date 0 Lisa 2004 1 Bob 2008 2 Jake 2012 3 Sue 2014 pd . merge ( df1 , df2 , on = 'employee' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014","title":"The on keyword"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#the-left_on-and-right_on-keywords","text":"At times you may wish to merge two datasets with different column names; for exam\u2010 ple, we may have a dataset in which the employee name is labeled as \u201cname\u201d rather than \u201cemployee\u201d. In this case, we can use the left_on and right_on keywords to specify the two column names: df3 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'salary' : [ 70000 , 80000 , 120000 , 90000 ]}) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name salary 0 Bob 70000 1 Jake 80000 2 Lisa 120000 3 Sue 90000 df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR pd . merge ( df1 , df3 , left_on = 'employee' , right_on = 'name' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group name salary 0 Bob Accounting Bob 70000 1 Jake Engineering Jake 80000 2 Lisa Engineering Lisa 120000 3 Sue HR Sue 90000 # to drop the redundant column pd . merge ( df1 , df3 , left_on = 'employee' , right_on = 'name' ) . drop ( 'name' , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group salary 0 Bob Accounting 70000 1 Jake Engineering 80000 2 Lisa Engineering 120000 3 Sue HR 90000","title":"The left_on and right_on keywords"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#specifying-set-arithmetic-for-joins","text":"In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join. This comes up when a value appears in one key column but not the other. df6 = pd . DataFrame ({ 'name' : [ 'Peter' , 'Paul' , 'Mary' ], 'food' : [ 'fish' , 'beans' , 'bread' ]}, columns = [ 'name' , 'food' ]) df7 = pd . DataFrame ({ 'name' : [ 'Mary' , 'Joseph' ], 'drink' : [ 'wine' , 'beer' ]}, columns = [ 'name' , 'drink' ]) df6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food 0 Peter fish 1 Paul beans 2 Mary bread df7 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name drink 0 Mary wine 1 Joseph beer pd . merge ( df6 , df7 , how = 'inner' ) # Interection .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Mary bread wine pd . merge ( df6 , df7 , how = 'outer' ) # union .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Peter fish NaN 1 Paul beans NaN 2 Mary bread wine 3 Joseph NaN beer pd . merge ( df6 , df7 , how = 'left' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Peter fish NaN 1 Paul beans NaN 2 Mary bread wine pd . merge ( df6 , df7 , how = 'right' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Mary bread wine 1 Joseph NaN beer","title":"Specifying Set Arithmetic for Joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#overlapping-column-names-the-suffixes-keyword","text":"Finally, you may end up in a case where your two input DataFrames have conflicting column names. df8 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 1 , 2 , 3 , 4 ]}) df8 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank 0 Bob 1 1 Jake 2 2 Lisa 3 3 Sue 4 df9 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 3 , 1 , 4 , 2 ]}) df9 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank 0 Bob 3 1 Jake 1 2 Lisa 4 3 Sue 2 pd . merge ( df8 , df9 , on = 'name' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank_x rank_y 0 Bob 1 3 1 Jake 2 1 2 Lisa 3 4 3 Sue 4 2 **Because the output would have two conflicting column names, the merge function automatically appends a suffix _x or _y to make the output columns unique. If these defaults are inappropriate, it is possible to specify a custom suffix using the suffixes keyword:** pd . merge ( df8 , df9 , on = 'name' , suffixes = [ '_L' , '_R' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank_L rank_R 0 Bob 1 3 1 Jake 2 1 2 Lisa 3 4 3 Sue 4 2","title":"Overlapping Column Names: The suffixes Keyword"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 07 - Aggregation and Grouping \u00b6 Aggregation and Grouping Aggregate, filter, transform, apply Aggregation and Grouping \u00b6 An essential piece of analysis of large data is efficient summarization: computing aggregations like sum(), mean(), median(), min(), and max(), in which a single num\u2010 ber gives insight into the nature of a potentially large dataset. In this section, we\u2019ll explore aggregations in Pandas, from simple operations akin to what we\u2019ve seen on NumPy arrays, to more sophisticated operations based on the concept of a groupby. ## Planets Data import seaborn as sns import numpy as np import pandas as pd planets = sns . load_dataset ( 'planets' ) planets . shape (1035, 6) planets . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 # simple aggregation in Pandas rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . rand ( 5 )) ser 0 0.374540 1 0.950714 2 0.731994 3 0.598658 4 0.156019 dtype: float64 ser . sum () 2.811925491708157 ser . mean () 0.5623850983416314 df = pd . DataFrame ({ 'A' : rng . rand ( 5 ), 'B' : rng . rand ( 5 )}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 0.155995 0.020584 1 0.058084 0.969910 2 0.866176 0.832443 3 0.601115 0.212339 4 0.708073 0.181825 df . mean () A 0.477888 B 0.443420 dtype: float64 df . sum () A 2.389442 B 2.217101 dtype: float64 planets . dropna () . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } number orbital_period mass distance year count 498.00000 498.000000 498.000000 498.000000 498.000000 mean 1.73494 835.778671 2.509320 52.068213 2007.377510 std 1.17572 1469.128259 3.636274 46.596041 4.167284 min 1.00000 1.328300 0.003600 1.350000 1989.000000 25% 1.00000 38.272250 0.212500 24.497500 2005.000000 50% 1.00000 357.000000 1.245000 39.940000 2009.000000 75% 2.00000 999.600000 2.867500 59.332500 2011.000000 max 6.00000 17337.500000 25.000000 354.000000 2014.000000 planets . count () method 1035 number 1035 orbital_period 992 mass 513 distance 808 year 1035 dtype: int64 These are all methods of DataFrame and Series objects. To go deeper into the data, however, simple aggregates are often not enough. The next level of data summarization is the groupby operation, which allows you to quickly and efficiently compute aggregates on subsets of data. ## GroupBy: Split, Apply, Combine Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so- called groupby operation. The name \u201cgroup by\u201d comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: split, apply, combine. ### Split, apply, combine A canonical example of this split-apply-combine operation, where the \u201capply\u201d is a summation aggregation: \u2022 The split step involves breaking up and grouping a DataFrame depending on the value of the specified key. \u2022 The apply step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups. \u2022 The combine step merges the results of these operations into an output array. While we could certainly do this manually using some combination of the masking, aggregation, and merging commands covered earlier, it\u2019s important to realize that the intermediate splits do not need to be explicitly instantiated. Rather, the GroupBy can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way. The power of the GroupBy is that it abstracts away these steps: the user need not think about how the computation is done under the hood, but rather thinks about the operation as a whole. df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data' : range ( 6 )}, columns = [ 'key' , 'data' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data 0 A 0 1 B 1 2 C 2 3 A 3 4 B 4 5 C 5 df . groupby ( 'key' ) # a groupby object is created <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f9e6c0b6a30> Notice that what is returned is not a set of DataFrames, but a DataFrameGroupBy object. This object is where the magic is: you can think of it as a special view of the DataFrame, which is poised to dig into the groups but does no actual computation until the aggregation is applied. This \u201clazy evaluation\u201d approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user. df . groupby ( 'key' ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data key A 3 B 5 C 7 Let\u2019s introduce some of the other func\u2010 tionality that can be used with the basic GroupBy operation. Column indexing. The GroupBy object supports column indexing in the same way as the DataFrame, and returns a modified GroupBy object. planets . groupby ( 'method' ) <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f9e6e0c71f0> planets . groupby ( 'method' )[ 'orbital_period' ] <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f9e6c56f9a0> Here we\u2019ve selected a particular Series group from the original DataFrame group by reference to its column name. As with the GroupBy object, no computation is done until we call some aggregate on the object: planets . groupby ( 'method' )[ 'orbital_period' ] . median () method Astrometry 631.180000 Eclipse Timing Variations 4343.500000 Imaging 27500.000000 Microlensing 3300.000000 Orbital Brightness Modulation 0.342887 Pulsar Timing 66.541900 Pulsation Timing Variations 1170.000000 Radial Velocity 360.200000 Transit 5.714932 Transit Timing Variations 57.011000 Name: orbital_period, dtype: float64 Iteration over groups . The GroupBy object supports direct iteration over the groups, returning each group as a Series or DataFrame for ( method , group ) in planets . groupby ( 'method' ): print ( \" {0:30s} shape= {1} \" . format ( method , group . shape )) Astrometry shape=(2, 6) Eclipse Timing Variations shape=(9, 6) Imaging shape=(38, 6) Microlensing shape=(23, 6) Orbital Brightness Modulation shape=(3, 6) Pulsar Timing shape=(5, 6) Pulsation Timing Variations shape=(1, 6) Radial Velocity shape=(553, 6) Transit shape=(397, 6) Transit Timing Variations shape=(4, 6) Dispatch methods. Through some Python class magic, any method not explicitly implemented by the GroupBy object will be passed through and called on the groups, whether they are DataFrame or Series objects. For example, you can use the describe() method of DataFrames to perform a set of aggregations that describe each group in the data: planets . groupby ( 'method' )[ 'year' ] . describe () . unstack () method count Astrometry 2.0 Eclipse Timing Variations 9.0 Imaging 38.0 Microlensing 23.0 Orbital Brightness Modulation 3.0 ... max Pulsar Timing 2011.0 Pulsation Timing Variations 2007.0 Radial Velocity 2014.0 Transit 2014.0 Transit Timing Variations 2014.0 Length: 80, dtype: float64 Aggregate, filter, transform, apply \u00b6 The preceding discussion focused on aggregation for the combine operation, but there are more options available. In particular, GroupBy objects have aggregate(), filter(), transform(), and apply() methods that efficiently implement a variety of useful operations before combining the grouped data. rng = np . random . RandomState ( 0 ) df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data1' : range ( 6 ), 'data2' : rng . randint ( 0 , 10 , 6 )}, columns = [ 'key' , 'data1' , 'data2' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 Aggregation. We\u2019re now familiar with GroupBy aggregations with sum(), median(), and the like, but the aggregate() method allows for even more flexibility. It can take a string, a function, or a list thereof, and compute all the aggregates at once. Here is a quick example combining all these: df . groupby ( 'key' ) . aggregate ([ 'min' , np . median , max ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } data1 data2 min median max min median max key A 0 1.5 3 3 4.0 5 B 1 2.5 4 0 3.5 7 C 2 3.5 5 3 6.0 9 df . groupby ( 'key' ) . aggregate ({ 'data1' : 'min' , 'data2' : 'max' }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 0 5 B 1 7 C 2 9 Filtering. A filtering operation allows you to drop data based on the group proper\u2010 ties. For example, we might want to keep all groups in which the standard deviation is larger than some critical value: def filter_func ( x ): return x [ 'data2' ] . std () > 4 print ( df ) key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 df . groupby ( 'key' ) . std () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 2.12132 1.414214 B 2.12132 4.949747 C 2.12132 4.242641 df . groupby ( 'key' ) . filter ( filter_func ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 1 B 1 0 2 C 2 3 4 B 4 7 5 C 5 9 Transformation. While aggregation must return a reduced version of the data, trans\u2010 formation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input. A common example is to center the data by subtracting the group-wise mean: df . groupby ( 'key' ) . transform ( lambda s : s - s . mean ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 0 -1.5 1.0 1 -1.5 -3.5 2 -1.5 -3.0 3 1.5 -1.0 4 1.5 3.5 5 1.5 3.0 The apply() method. The apply() method lets you apply an arbitrary function to the group results. The function should take a DataFrame, and return either a Pandas object (e.g., DataFrame, Series) or a scalar; the combine operation will be tailored to the type of output returned. def norm_by_data2 ( x ): x [ 'data1' ] /= x [ 'data2' ] . sum () return x df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 df . groupby ( 'key' ) . apply ( norm_by_data2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0.000000 5 1 B 0.142857 0 2 C 0.166667 3 3 A 0.375000 3 4 B 0.571429 7 5 C 0.416667 9 Specifying the split key In the simple examples presented before, we split the DataFrame on a single column name. This is just one of many options by which the groups can be defined, and we\u2019ll go through some other options for group specification here. A list, array, series, or index providing the grouping keys. The key can be any series or list with a length matching that of the DataFrame. l = [ 0 , 1 , 0 , 1 , 2 , 0 ] df . groupby ( l ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 0 7 17 1 4 3 2 4 7 df . groupby ( 'key' ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 3 8 B 5 7 C 7 12","title":"08 Aggregation and Grouping"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/#07-aggregation-and-grouping","text":"Aggregation and Grouping Aggregate, filter, transform, apply","title":"07 - Aggregation and Grouping"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/#aggregation-and-grouping","text":"An essential piece of analysis of large data is efficient summarization: computing aggregations like sum(), mean(), median(), min(), and max(), in which a single num\u2010 ber gives insight into the nature of a potentially large dataset. In this section, we\u2019ll explore aggregations in Pandas, from simple operations akin to what we\u2019ve seen on NumPy arrays, to more sophisticated operations based on the concept of a groupby. ## Planets Data import seaborn as sns import numpy as np import pandas as pd planets = sns . load_dataset ( 'planets' ) planets . shape (1035, 6) planets . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 # simple aggregation in Pandas rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . rand ( 5 )) ser 0 0.374540 1 0.950714 2 0.731994 3 0.598658 4 0.156019 dtype: float64 ser . sum () 2.811925491708157 ser . mean () 0.5623850983416314 df = pd . DataFrame ({ 'A' : rng . rand ( 5 ), 'B' : rng . rand ( 5 )}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 0.155995 0.020584 1 0.058084 0.969910 2 0.866176 0.832443 3 0.601115 0.212339 4 0.708073 0.181825 df . mean () A 0.477888 B 0.443420 dtype: float64 df . sum () A 2.389442 B 2.217101 dtype: float64 planets . dropna () . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } number orbital_period mass distance year count 498.00000 498.000000 498.000000 498.000000 498.000000 mean 1.73494 835.778671 2.509320 52.068213 2007.377510 std 1.17572 1469.128259 3.636274 46.596041 4.167284 min 1.00000 1.328300 0.003600 1.350000 1989.000000 25% 1.00000 38.272250 0.212500 24.497500 2005.000000 50% 1.00000 357.000000 1.245000 39.940000 2009.000000 75% 2.00000 999.600000 2.867500 59.332500 2011.000000 max 6.00000 17337.500000 25.000000 354.000000 2014.000000 planets . count () method 1035 number 1035 orbital_period 992 mass 513 distance 808 year 1035 dtype: int64 These are all methods of DataFrame and Series objects. To go deeper into the data, however, simple aggregates are often not enough. The next level of data summarization is the groupby operation, which allows you to quickly and efficiently compute aggregates on subsets of data. ## GroupBy: Split, Apply, Combine Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so- called groupby operation. The name \u201cgroup by\u201d comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: split, apply, combine. ### Split, apply, combine A canonical example of this split-apply-combine operation, where the \u201capply\u201d is a summation aggregation: \u2022 The split step involves breaking up and grouping a DataFrame depending on the value of the specified key. \u2022 The apply step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups. \u2022 The combine step merges the results of these operations into an output array. While we could certainly do this manually using some combination of the masking, aggregation, and merging commands covered earlier, it\u2019s important to realize that the intermediate splits do not need to be explicitly instantiated. Rather, the GroupBy can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way. The power of the GroupBy is that it abstracts away these steps: the user need not think about how the computation is done under the hood, but rather thinks about the operation as a whole. df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data' : range ( 6 )}, columns = [ 'key' , 'data' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data 0 A 0 1 B 1 2 C 2 3 A 3 4 B 4 5 C 5 df . groupby ( 'key' ) # a groupby object is created <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f9e6c0b6a30> Notice that what is returned is not a set of DataFrames, but a DataFrameGroupBy object. This object is where the magic is: you can think of it as a special view of the DataFrame, which is poised to dig into the groups but does no actual computation until the aggregation is applied. This \u201clazy evaluation\u201d approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user. df . groupby ( 'key' ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data key A 3 B 5 C 7 Let\u2019s introduce some of the other func\u2010 tionality that can be used with the basic GroupBy operation. Column indexing. The GroupBy object supports column indexing in the same way as the DataFrame, and returns a modified GroupBy object. planets . groupby ( 'method' ) <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f9e6e0c71f0> planets . groupby ( 'method' )[ 'orbital_period' ] <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f9e6c56f9a0> Here we\u2019ve selected a particular Series group from the original DataFrame group by reference to its column name. As with the GroupBy object, no computation is done until we call some aggregate on the object: planets . groupby ( 'method' )[ 'orbital_period' ] . median () method Astrometry 631.180000 Eclipse Timing Variations 4343.500000 Imaging 27500.000000 Microlensing 3300.000000 Orbital Brightness Modulation 0.342887 Pulsar Timing 66.541900 Pulsation Timing Variations 1170.000000 Radial Velocity 360.200000 Transit 5.714932 Transit Timing Variations 57.011000 Name: orbital_period, dtype: float64 Iteration over groups . The GroupBy object supports direct iteration over the groups, returning each group as a Series or DataFrame for ( method , group ) in planets . groupby ( 'method' ): print ( \" {0:30s} shape= {1} \" . format ( method , group . shape )) Astrometry shape=(2, 6) Eclipse Timing Variations shape=(9, 6) Imaging shape=(38, 6) Microlensing shape=(23, 6) Orbital Brightness Modulation shape=(3, 6) Pulsar Timing shape=(5, 6) Pulsation Timing Variations shape=(1, 6) Radial Velocity shape=(553, 6) Transit shape=(397, 6) Transit Timing Variations shape=(4, 6) Dispatch methods. Through some Python class magic, any method not explicitly implemented by the GroupBy object will be passed through and called on the groups, whether they are DataFrame or Series objects. For example, you can use the describe() method of DataFrames to perform a set of aggregations that describe each group in the data: planets . groupby ( 'method' )[ 'year' ] . describe () . unstack () method count Astrometry 2.0 Eclipse Timing Variations 9.0 Imaging 38.0 Microlensing 23.0 Orbital Brightness Modulation 3.0 ... max Pulsar Timing 2011.0 Pulsation Timing Variations 2007.0 Radial Velocity 2014.0 Transit 2014.0 Transit Timing Variations 2014.0 Length: 80, dtype: float64","title":"Aggregation and Grouping"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/#aggregate-filter-transform-apply","text":"The preceding discussion focused on aggregation for the combine operation, but there are more options available. In particular, GroupBy objects have aggregate(), filter(), transform(), and apply() methods that efficiently implement a variety of useful operations before combining the grouped data. rng = np . random . RandomState ( 0 ) df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data1' : range ( 6 ), 'data2' : rng . randint ( 0 , 10 , 6 )}, columns = [ 'key' , 'data1' , 'data2' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 Aggregation. We\u2019re now familiar with GroupBy aggregations with sum(), median(), and the like, but the aggregate() method allows for even more flexibility. It can take a string, a function, or a list thereof, and compute all the aggregates at once. Here is a quick example combining all these: df . groupby ( 'key' ) . aggregate ([ 'min' , np . median , max ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } data1 data2 min median max min median max key A 0 1.5 3 3 4.0 5 B 1 2.5 4 0 3.5 7 C 2 3.5 5 3 6.0 9 df . groupby ( 'key' ) . aggregate ({ 'data1' : 'min' , 'data2' : 'max' }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 0 5 B 1 7 C 2 9 Filtering. A filtering operation allows you to drop data based on the group proper\u2010 ties. For example, we might want to keep all groups in which the standard deviation is larger than some critical value: def filter_func ( x ): return x [ 'data2' ] . std () > 4 print ( df ) key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 df . groupby ( 'key' ) . std () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 2.12132 1.414214 B 2.12132 4.949747 C 2.12132 4.242641 df . groupby ( 'key' ) . filter ( filter_func ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 1 B 1 0 2 C 2 3 4 B 4 7 5 C 5 9 Transformation. While aggregation must return a reduced version of the data, trans\u2010 formation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input. A common example is to center the data by subtracting the group-wise mean: df . groupby ( 'key' ) . transform ( lambda s : s - s . mean ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 0 -1.5 1.0 1 -1.5 -3.5 2 -1.5 -3.0 3 1.5 -1.0 4 1.5 3.5 5 1.5 3.0 The apply() method. The apply() method lets you apply an arbitrary function to the group results. The function should take a DataFrame, and return either a Pandas object (e.g., DataFrame, Series) or a scalar; the combine operation will be tailored to the type of output returned. def norm_by_data2 ( x ): x [ 'data1' ] /= x [ 'data2' ] . sum () return x df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 df . groupby ( 'key' ) . apply ( norm_by_data2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0.000000 5 1 B 0.142857 0 2 C 0.166667 3 3 A 0.375000 3 4 B 0.571429 7 5 C 0.416667 9 Specifying the split key In the simple examples presented before, we split the DataFrame on a single column name. This is just one of many options by which the groups can be defined, and we\u2019ll go through some other options for group specification here. A list, array, series, or index providing the grouping keys. The key can be any series or list with a length matching that of the DataFrame. l = [ 0 , 1 , 0 , 1 , 2 , 0 ] df . groupby ( l ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 0 7 17 1 4 3 2 4 7 df . groupby ( 'key' ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 3 8 B 5 7 C 7 12","title":"Aggregate, filter, transform, apply"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 08 - Pivot Table \u00b6 Pivot Tables Motivating pivot tables Pivot Tables by Hand Pivot Table Syntax Multilevel pivot tables Pivot Tables \u00b6 We have seen how the GroupBy abstraction lets us explore relationships within a data\u2010 set. A pivot table is a similar operation that is commonly seen in spreadsheets and other programs that operate on tabular data. The pivot table takes simple column- wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data. The difference between pivot tables and GroupBy can sometimes cause confusion; it helps me to think of pivot tables as essentially a multidimensional version of GroupBy aggregation. That is, you split- apply-combine, but both the split and the combine happen across not a one- dimensional index, but across a two-dimensional grid. Motivating pivot tables \u00b6 import numpy as np import pandas as pd import seaborn as sns titanic = sns . load_dataset ( 'titanic' ) titanic . head () URLError: <urlopen error [Errno -3] Temporary failure in name resolution> Pivot Tables by Hand \u00b6 To start learning more about this data, we might begin by grouping it according to gender, survival status, or some combination thereof. If you have read the previous section, you might be tempted to apply a GroupBy operation\u2014for example, let\u2019s look at survival rate by gender: titanic . groupby ( 'sex' )[ '[survived]' ] . mean () NameError: name 'titanic' is not defined This immediately gives us some insight: overall, three of every four females on board survived, while only one in five males survived! This is useful, but we might like to go one step deeper and look at survival by both sex and, say, class. Using the vocabulary of GroupBy, we might proceed using something like this: we group by class and gender, select survival, apply a mean aggregate, com\u2010 bine the resulting groups, and then unstack the hierarchical index to reveal the hidden multidimensionality. In code: titanic . groupby ([ 'sex' , 'class' ])[ 'survived' ] . aggregate ( 'mean' ) . unstack () NameError: name 'titanic' is not defined Pivot Table Syntax \u00b6 Here is the equivalent to the preceding operation using the pivot_table method of DataFrames titanic . pivot_table ( 'survived' , index = 'sex' , column = 'class' ) NameError: name 'titanic' is not defined Multilevel pivot tables \u00b6 Just as in the GroupBy, the grouping in pivot tables can be specified with multiple lev\u2010 els, and via a number of options. For example, we might be interested in looking at age as a third dimension. We\u2019ll bin the age using the pd.cut function: age = pd . cut ( titanic [ 'age' ],[ 0 , 18 , 80 ]) titanic . pivot_table ( 'survived' ,[ 'sex' , ge ], 'class' ) NameError: name 'titanic' is not defined fare = pd . cut ( titanic [ 'fare' ], 2 ) titanic . pivot_table ( 'survived' ,[ 'sex' , ge ],[ fare , 'class' ]) NameError: name 'titanic' is not defined ### There are some additional options for the pivot tables # call signature as of Pandas 0.18 DataFrame . pivot_table ( data , values = None , index = None , columns = None , aggfunc = 'mean' , fill_value = None , margins = False , dropna = True , margins_name = 'All' ) NameError: name 'DataFrame' is not defined","title":"09 Pivot Tables"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#08-pivot-table","text":"Pivot Tables Motivating pivot tables Pivot Tables by Hand Pivot Table Syntax Multilevel pivot tables","title":"08 - Pivot Table"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#pivot-tables","text":"We have seen how the GroupBy abstraction lets us explore relationships within a data\u2010 set. A pivot table is a similar operation that is commonly seen in spreadsheets and other programs that operate on tabular data. The pivot table takes simple column- wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data. The difference between pivot tables and GroupBy can sometimes cause confusion; it helps me to think of pivot tables as essentially a multidimensional version of GroupBy aggregation. That is, you split- apply-combine, but both the split and the combine happen across not a one- dimensional index, but across a two-dimensional grid.","title":"Pivot Tables"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#motivating-pivot-tables","text":"import numpy as np import pandas as pd import seaborn as sns titanic = sns . load_dataset ( 'titanic' ) titanic . head () URLError: <urlopen error [Errno -3] Temporary failure in name resolution>","title":"Motivating pivot tables"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#pivot-tables-by-hand","text":"To start learning more about this data, we might begin by grouping it according to gender, survival status, or some combination thereof. If you have read the previous section, you might be tempted to apply a GroupBy operation\u2014for example, let\u2019s look at survival rate by gender: titanic . groupby ( 'sex' )[ '[survived]' ] . mean () NameError: name 'titanic' is not defined This immediately gives us some insight: overall, three of every four females on board survived, while only one in five males survived! This is useful, but we might like to go one step deeper and look at survival by both sex and, say, class. Using the vocabulary of GroupBy, we might proceed using something like this: we group by class and gender, select survival, apply a mean aggregate, com\u2010 bine the resulting groups, and then unstack the hierarchical index to reveal the hidden multidimensionality. In code: titanic . groupby ([ 'sex' , 'class' ])[ 'survived' ] . aggregate ( 'mean' ) . unstack () NameError: name 'titanic' is not defined","title":"Pivot Tables by Hand"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#pivot-table-syntax","text":"Here is the equivalent to the preceding operation using the pivot_table method of DataFrames titanic . pivot_table ( 'survived' , index = 'sex' , column = 'class' ) NameError: name 'titanic' is not defined","title":"Pivot Table Syntax"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#multilevel-pivot-tables","text":"Just as in the GroupBy, the grouping in pivot tables can be specified with multiple lev\u2010 els, and via a number of options. For example, we might be interested in looking at age as a third dimension. We\u2019ll bin the age using the pd.cut function: age = pd . cut ( titanic [ 'age' ],[ 0 , 18 , 80 ]) titanic . pivot_table ( 'survived' ,[ 'sex' , ge ], 'class' ) NameError: name 'titanic' is not defined fare = pd . cut ( titanic [ 'fare' ], 2 ) titanic . pivot_table ( 'survived' ,[ 'sex' , ge ],[ fare , 'class' ]) NameError: name 'titanic' is not defined ### There are some additional options for the pivot tables # call signature as of Pandas 0.18 DataFrame . pivot_table ( data , values = None , index = None , columns = None , aggfunc = 'mean' , fill_value = None , margins = False , dropna = True , margins_name = 'All' ) NameError: name 'DataFrame' is not defined","title":"Multilevel pivot tables"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 10 - Vectorized String Operations \u00b6 Vectorized String Operations Tables of Pandas String Methods Vectorized String Operations \u00b6 One strength of Python is its relative ease in handling and manipulating string data. Pandas builds on this and provides a comprehensive set of vectorized string operations that become an essential piece of the type of munging required when one is working with (read: cleaning up) real-world data. ## Introducing Pandas String Operations We saw in previous sections how tools like NumPy and Pandas generalize arithmetic operations so that we can easily and quickly perform the same operation on many array elements. import numpy as np x = np . array ([ 2 , 3 , 5 , 7 , 11 , 13 ]) x * 2 array([ 4, 6, 10, 14, 22, 26]) This vectorization of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done. For arrays of strings, NumPy does not provide such simple access, and thus you\u2019re stuck using a more verbose loop syntax: data = [ 'peteR' , 'khan' , 'Haider' , 'KILLY' , 'GuIDol' , ' ' ] [ s . capitalize () for s in data ] ['Peter', 'Khan', 'Haider', 'Killy', 'Guidol', ' '] data = [ 'peteR' , 'khan' , 'Haider' , None , 'KILLY' , 'GuIDol' ] [ s . capitalize () for s in data ] AttributeError: 'NoneType' object has no attribute 'capitalize' Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the str attribute of Pandas Series and Index objects containing strings. import pandas as pd names = pd . Series ( data ) names 0 peteR 1 khan 2 Haider 3 None 4 KILLY 5 GuIDol dtype: object # Now we can capitalize All without any error names . str . capitalize () 0 Peter 1 Khan 2 Haider 3 None 4 Killy 5 Guidol dtype: object Tables of Pandas String Methods \u00b6 If you have a good understanding of string manipulation in Python, most of Pandas\u2019 string syntax is intuitive enough that it\u2019s probably sufficient to just list a table of avail\u2010 able methods; we will start with that here, before diving deeper into a few of the sub\u2010 tleties. monte = pd . Series ([ 'Graham Chapman' , 'John Cleese' , 'Terry Gilliam' , 'Eric Idle' , 'Terry Jones' , 'Michael Palin' ]) monte 0 Graham Chapman 1 John Cleese 2 Terry Gilliam 3 Eric Idle 4 Terry Jones 5 Michael Palin dtype: object Methods similar to Python string methods Nearly all Python\u2019s built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas str methods that mirror Python string methods: len() lower() translate() ljust() upper() startswith() isupper() islower() rjust() find() endswith() isnumeric() center() rfind() isalnum() rsplit() rstrip() capitalize() isspace() partition() lstrip() swapcase() rpartition()`","title":"10 Vectoried String Operations"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/#10-vectorized-string-operations","text":"Vectorized String Operations Tables of Pandas String Methods","title":"10 - Vectorized String Operations"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/#vectorized-string-operations","text":"One strength of Python is its relative ease in handling and manipulating string data. Pandas builds on this and provides a comprehensive set of vectorized string operations that become an essential piece of the type of munging required when one is working with (read: cleaning up) real-world data. ## Introducing Pandas String Operations We saw in previous sections how tools like NumPy and Pandas generalize arithmetic operations so that we can easily and quickly perform the same operation on many array elements. import numpy as np x = np . array ([ 2 , 3 , 5 , 7 , 11 , 13 ]) x * 2 array([ 4, 6, 10, 14, 22, 26]) This vectorization of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done. For arrays of strings, NumPy does not provide such simple access, and thus you\u2019re stuck using a more verbose loop syntax: data = [ 'peteR' , 'khan' , 'Haider' , 'KILLY' , 'GuIDol' , ' ' ] [ s . capitalize () for s in data ] ['Peter', 'Khan', 'Haider', 'Killy', 'Guidol', ' '] data = [ 'peteR' , 'khan' , 'Haider' , None , 'KILLY' , 'GuIDol' ] [ s . capitalize () for s in data ] AttributeError: 'NoneType' object has no attribute 'capitalize' Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the str attribute of Pandas Series and Index objects containing strings. import pandas as pd names = pd . Series ( data ) names 0 peteR 1 khan 2 Haider 3 None 4 KILLY 5 GuIDol dtype: object # Now we can capitalize All without any error names . str . capitalize () 0 Peter 1 Khan 2 Haider 3 None 4 Killy 5 Guidol dtype: object","title":"Vectorized String Operations"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/#tables-of-pandas-string-methods","text":"If you have a good understanding of string manipulation in Python, most of Pandas\u2019 string syntax is intuitive enough that it\u2019s probably sufficient to just list a table of avail\u2010 able methods; we will start with that here, before diving deeper into a few of the sub\u2010 tleties. monte = pd . Series ([ 'Graham Chapman' , 'John Cleese' , 'Terry Gilliam' , 'Eric Idle' , 'Terry Jones' , 'Michael Palin' ]) monte 0 Graham Chapman 1 John Cleese 2 Terry Gilliam 3 Eric Idle 4 Terry Jones 5 Michael Palin dtype: object Methods similar to Python string methods Nearly all Python\u2019s built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas str methods that mirror Python string methods: len() lower() translate() ljust() upper() startswith() isupper() islower() rjust() find() endswith() isnumeric() center() rfind() isalnum() rsplit() rstrip() capitalize() isspace() partition() lstrip() swapcase() rpartition()`","title":"Tables of Pandas String Methods"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 11 - Working with Time Series \u00b6 Working with Time Series Dates and Times in Python Native Python dates and times: datetime and dateutil Dates and times in Pandas: Best of both worlds Pandas Time Series: Indexing by Time Pandas Time Series Data Structures Regular sequences: pd.date_range() Working with Time Series \u00b6 Pandas was developed in the context of financial modeling, so as you might expect, it contains a fairly extensive set of tools for working with dates, times, and time- indexed data. Date and time data comes in a few flavors, which we will discuss here: \u2022 Time stamps reference particular moments in time (e.g., July 4 th , 2015, at 7:00a.m.). \u2022 Time intervals and periods reference a length of time between a particular beginning and end point\u2014for example, the year 2015. Periods usually reference a special case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods constituting days). \u2022 Time deltas or durations reference an exact length of time (e.g., a duration of 22.56 seconds). Dates and Times in Python \u00b6 The Python world has a number of available representations of dates, times, deltas, and timespans. While the time series tools provided by Pandas tend to be the most useful for data science applications, it is helpful to see their relationship to other packages used in Python. Native Python dates and times: datetime and dateutil \u00b6 Python\u2019s basic objects for working with dates and times reside in the built-in date time module. Along with the third-party dateutil module, you can use it to quickly perform a host of useful functionalities on dates and times. For example, you can manually build a date using the datetime type: from datetime import datetime datetime ( year = 2022 , month = 7 , day = 30 ) datetime.datetime(2022, 7, 30, 0, 0) # Using the dateutil moduel, we can parse dates from different string formats from dateutil import parser date = parser . parse ( \"30th of August 2022\" ) date datetime.datetime(2022, 8, 30, 0, 0) # once we have a datetime object, we can do things like printng the day of the week date . strftime ( '%A' ) #In the final line, we\u2019ve used one of the standard string format codes for printing dates #(\"%A\"), 'Tuesday' A related package to be aware of is pytz, which contains tools for working with the most migraine-inducing piece of time series data: time zones. The power of datetime and dateutil lies in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in. Where they break down is when you wish to work with large arrays of dates and times: just as lists of Python numerical variables are subopti\u2010 mal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates. ### Typed arrays of times: NumPy\u2019s datetime64 The weaknesses of Python\u2019s datetime format inspired the NumPy team to add a set of native time series data type to NumPy. The datetime64 dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be represented very compactly. The date time64 requires a very specific input format: import numpy as np date = np . array ( '2022-07-30' , dtype = np . datetime64 ) date array('2022-07-30', dtype='datetime64[D]') # Now we can do vectorized operation on it date + np . arange ( 12 ) array(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06', '2022-08-07', '2022-08-08', '2022-08-09', '2022-08-10'], dtype='datetime64[D]') Because of the uniform type in NumPy datetime64 arrays, this type of operation can be accomplished much more quickly than if we were working directly with Python\u2019s datetime objects, especially as arrays get large (we introduced this type of vectoriza\u2010 tion in \u201cComputation on NumPy Arrays: Universal Functions\u201d on page 50). One detail of the datetime64 and timedelta64 objects is that they are built on a fun\u2010 damental time unit. Because the datetime64 object is limited to 64-bit precision, the range of encodable times is 264 times this fundamental unit. In other words, date time64 imposes a trade-off between time resolution and maximum time span. For example, if you want a time resolution of one nanosecond, you only have enough information to encode a range of 264 nanoseconds, or just under 600 years. NumPy will infer the desired unit from the input; for example, here is a day-based datetime: np . datetime64 ( '2022-07-30 12:00' ) numpy.datetime64('2022-07-30T12:00') Notice that the time zone is automatically set to the local time on the computer exe\u2010 cuting the code. You can force any desired fundamental unit using one of many for\u2010 mat codes; for example, here we\u2019ll force a nanosecond-based time: np . datetime64 ( '2022-7-30 12:59.50' , 'ns' ) ValueError: Error parsing datetime string \"2022-7-30 12:59.50\" at position 5 Dates and times in Pandas: Best of both worlds \u00b6 Pandas builds upon all the tools just discussed to provide a Timestamp object, which combines the ease of use of datetime and dateutil with the efficient storage and vectorized interface of numpy.datetime64. From a group of these Timestamp objects, Pandas can construct a DatetimeIndex that can be used to index data in a Series or DataFrame; we\u2019ll see many examples of this below. For example, we can use Pandas tools to repeat the demonstration from above. We can parse a flexibly formatted string date, and use format codes to output the day of the week: import pandas as pd date = pd . to_datetime ( '30th August, 2022' ) date Timestamp('2022-08-30 00:00:00') date . strftime ( '%A' ) 'Tuesday' # Additionaly we can do Numpy-style Vectorized operations directly on this object date + pd . to_timedelta ( np . arange ( 12 ), 'D' ) DatetimeIndex(['2022-08-30', '2022-08-31', '2022-09-01', '2022-09-02', '2022-09-03', '2022-09-04', '2022-09-05', '2022-09-06', '2022-09-07', '2022-09-08', '2022-09-09', '2022-09-10'], dtype='datetime64[ns]', freq=None) Pandas Time Series: Indexing by Time \u00b6 Where the Pandas time series tools really become useful is when you begin to index data by timestamps. For example, we can construct a Series object that has time- indexed data: index = pd . DatetimeIndex ([ '2022-07-04' , '2022-08-04' , '2023-07-04' , '2023-08-04' ]) data = pd . Series ([ 0 , 1 , 2 , 3 ], index = index ) data 2022-07-04 0 2022-08-04 1 2023-07-04 2 2023-08-04 3 dtype: int64 data [ '2022-07-04' : '2023-07-04' ] 2022-07-04 0 2022-08-04 1 2023-07-04 2 dtype: int64 # just passing year to obtain a slice data [ '2022' ] 2022-07-04 0 2022-08-04 1 dtype: int64 data [ '2023' ] 2023-07-04 2 2023-08-04 3 dtype: int64 Pandas Time Series Data Structures \u00b6 \u2022 For time stamps, Pandas provides the Timestamp type. As mentioned before, it is essentially a replacement for Python\u2019s native datetime, but is based on the more efficient numpy.datetime64 data type. The associated index structure is DatetimeIndex. \u2022 For time periods, Pandas provides the Period type. This encodes a fixed frequency interval based on numpy.datetime64. The associated index structure is PeriodIndex. \u2022 For time deltas or durations, Pandas provides the Timedelta type. Timedelta is a more efficient replacement for Python\u2019s native datetime.timedelta type, and is based on numpy.timedelta64. The associated index structure is TimedeltaIndex. The most fundamental of these date/time objects are the Timestamp and DatetimeIn dex objects. While these class objects can be invoked directly, it is more common to use the pd.to_datetime() function, which can parse a wide variety of formats. Passing a single date to pd.to_datetime() yields a Timestamp; passing a series of dates by default yields a DatetimeIndex: dates = pd . to_datetime ([ datetime ( 2022 , 7 , 30 ), '30th of July, 2022' , '2022-7-30' , '20220730' ]) dates DatetimeIndex(['2022-07-30', '2022-07-30', '2022-07-30', '2022-07-30'], dtype='datetime64[ns]', freq=None) Any DatetimeIndex can be converted to a PeriodIndex with the to_period() func\u2010 tion with the addition of a frequency code; here we\u2019ll use \u2018D\u2019 to indicate daily frequency: dates . to_period ( 'D' ) PeriodIndex(['2022-07-30', '2022-07-30', '2022-07-30', '2022-07-30'], dtype='period[D]') # The above is a TimedeltaIndex, we can subtract one date from another dates - dates [ 0 ] TimedeltaIndex(['0 days', '0 days', '0 days', '0 days'], dtype='timedelta64[ns]', freq=None) dates = pd . to_datetime ([ datetime ( 2022 , 7 , 28 ), '29th of July, 2022' , '2022-7-30' , '20220731' ]) dates DatetimeIndex(['2022-07-28', '2022-07-29', '2022-07-30', '2022-07-31'], dtype='datetime64[ns]', freq=None) dates . to_period ( 'D' ) PeriodIndex(['2022-07-28', '2022-07-29', '2022-07-30', '2022-07-31'], dtype='period[D]') dates - dates [ 0 ] TimedeltaIndex(['0 days', '1 days', '2 days', '3 days'], dtype='timedelta64[ns]', freq=None) dates - dates [ 1 ] TimedeltaIndex(['-1 days', '0 days', '1 days', '2 days'], dtype='timedelta64[ns]', freq=None) Regular sequences: pd.date_range() \u00b6 To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: pd.date_range() for timestamps, pd.period_range() for periods, and pd.timedelta_range() for time deltas. we have seen that Python range() and NumPy\u2019s np.arange() turn a startpoint, endpoint, and optional stepsize into a sequence. Similarly, pd.date_range() accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates. By default, the fre\u2010 quency is one day: pd . date_range ( '2022-07-30' , '2023-07-30' ) DatetimeIndex(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06', '2022-08-07', '2022-08-08', ... '2023-07-21', '2023-07-22', '2023-07-23', '2023-07-24', '2023-07-25', '2023-07-26', '2023-07-27', '2023-07-28', '2023-07-29', '2023-07-30'], dtype='datetime64[ns]', length=366, freq='D') Alternatively, the date range can be specified not with a start- and endpoint, but with a startpoint and a number of periods: pd . date_range ( '2022-07-30' , periods = 8 ) DatetimeIndex(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06'], dtype='datetime64[ns]', freq='D') You can modify the spacing by altering the freq argument, which defaults to D. For example, here we will construct a range of hourly timestamps: pd . date_range ( '2022-07-30' , periods = 8 , freq = 'H' ) DatetimeIndex(['2022-07-30 00:00:00', '2022-07-30 01:00:00', '2022-07-30 02:00:00', '2022-07-30 03:00:00', '2022-07-30 04:00:00', '2022-07-30 05:00:00', '2022-07-30 06:00:00', '2022-07-30 07:00:00'], dtype='datetime64[ns]', freq='H') To create regular sequences of period or time delta values, the very similar pd.period_range() and pd.timedelta_range() functions are useful. Here are some monthly periods: pd . period_range ( '2022-07' , periods = 8 , freq = 'M' ) PeriodIndex(['2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02'], dtype='period[M]') # and a sequence of durations increasing by an hour pd . timedelta_range ( 0 , periods = 10 , freq = 'H' ) TimedeltaIndex(['0 days 00:00:00', '0 days 01:00:00', '0 days 02:00:00', '0 days 03:00:00', '0 days 04:00:00', '0 days 05:00:00', '0 days 06:00:00', '0 days 07:00:00', '0 days 08:00:00', '0 days 09:00:00'], dtype='timedelta64[ns]', freq='H')","title":"11 Working with Time Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#11-working-with-time-series","text":"Working with Time Series Dates and Times in Python Native Python dates and times: datetime and dateutil Dates and times in Pandas: Best of both worlds Pandas Time Series: Indexing by Time Pandas Time Series Data Structures Regular sequences: pd.date_range()","title":"11 - Working with Time Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#working-with-time-series","text":"Pandas was developed in the context of financial modeling, so as you might expect, it contains a fairly extensive set of tools for working with dates, times, and time- indexed data. Date and time data comes in a few flavors, which we will discuss here: \u2022 Time stamps reference particular moments in time (e.g., July 4 th , 2015, at 7:00a.m.). \u2022 Time intervals and periods reference a length of time between a particular beginning and end point\u2014for example, the year 2015. Periods usually reference a special case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods constituting days). \u2022 Time deltas or durations reference an exact length of time (e.g., a duration of 22.56 seconds).","title":"Working with Time Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#dates-and-times-in-python","text":"The Python world has a number of available representations of dates, times, deltas, and timespans. While the time series tools provided by Pandas tend to be the most useful for data science applications, it is helpful to see their relationship to other packages used in Python.","title":"Dates and Times in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#native-python-dates-and-times-datetime-and-dateutil","text":"Python\u2019s basic objects for working with dates and times reside in the built-in date time module. Along with the third-party dateutil module, you can use it to quickly perform a host of useful functionalities on dates and times. For example, you can manually build a date using the datetime type: from datetime import datetime datetime ( year = 2022 , month = 7 , day = 30 ) datetime.datetime(2022, 7, 30, 0, 0) # Using the dateutil moduel, we can parse dates from different string formats from dateutil import parser date = parser . parse ( \"30th of August 2022\" ) date datetime.datetime(2022, 8, 30, 0, 0) # once we have a datetime object, we can do things like printng the day of the week date . strftime ( '%A' ) #In the final line, we\u2019ve used one of the standard string format codes for printing dates #(\"%A\"), 'Tuesday' A related package to be aware of is pytz, which contains tools for working with the most migraine-inducing piece of time series data: time zones. The power of datetime and dateutil lies in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in. Where they break down is when you wish to work with large arrays of dates and times: just as lists of Python numerical variables are subopti\u2010 mal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates. ### Typed arrays of times: NumPy\u2019s datetime64 The weaknesses of Python\u2019s datetime format inspired the NumPy team to add a set of native time series data type to NumPy. The datetime64 dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be represented very compactly. The date time64 requires a very specific input format: import numpy as np date = np . array ( '2022-07-30' , dtype = np . datetime64 ) date array('2022-07-30', dtype='datetime64[D]') # Now we can do vectorized operation on it date + np . arange ( 12 ) array(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06', '2022-08-07', '2022-08-08', '2022-08-09', '2022-08-10'], dtype='datetime64[D]') Because of the uniform type in NumPy datetime64 arrays, this type of operation can be accomplished much more quickly than if we were working directly with Python\u2019s datetime objects, especially as arrays get large (we introduced this type of vectoriza\u2010 tion in \u201cComputation on NumPy Arrays: Universal Functions\u201d on page 50). One detail of the datetime64 and timedelta64 objects is that they are built on a fun\u2010 damental time unit. Because the datetime64 object is limited to 64-bit precision, the range of encodable times is 264 times this fundamental unit. In other words, date time64 imposes a trade-off between time resolution and maximum time span. For example, if you want a time resolution of one nanosecond, you only have enough information to encode a range of 264 nanoseconds, or just under 600 years. NumPy will infer the desired unit from the input; for example, here is a day-based datetime: np . datetime64 ( '2022-07-30 12:00' ) numpy.datetime64('2022-07-30T12:00') Notice that the time zone is automatically set to the local time on the computer exe\u2010 cuting the code. You can force any desired fundamental unit using one of many for\u2010 mat codes; for example, here we\u2019ll force a nanosecond-based time: np . datetime64 ( '2022-7-30 12:59.50' , 'ns' ) ValueError: Error parsing datetime string \"2022-7-30 12:59.50\" at position 5","title":"Native Python dates and times: datetime and dateutil"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#dates-and-times-in-pandas-best-of-both-worlds","text":"Pandas builds upon all the tools just discussed to provide a Timestamp object, which combines the ease of use of datetime and dateutil with the efficient storage and vectorized interface of numpy.datetime64. From a group of these Timestamp objects, Pandas can construct a DatetimeIndex that can be used to index data in a Series or DataFrame; we\u2019ll see many examples of this below. For example, we can use Pandas tools to repeat the demonstration from above. We can parse a flexibly formatted string date, and use format codes to output the day of the week: import pandas as pd date = pd . to_datetime ( '30th August, 2022' ) date Timestamp('2022-08-30 00:00:00') date . strftime ( '%A' ) 'Tuesday' # Additionaly we can do Numpy-style Vectorized operations directly on this object date + pd . to_timedelta ( np . arange ( 12 ), 'D' ) DatetimeIndex(['2022-08-30', '2022-08-31', '2022-09-01', '2022-09-02', '2022-09-03', '2022-09-04', '2022-09-05', '2022-09-06', '2022-09-07', '2022-09-08', '2022-09-09', '2022-09-10'], dtype='datetime64[ns]', freq=None)","title":"Dates and times in Pandas: Best of both worlds"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#pandas-time-series-indexing-by-time","text":"Where the Pandas time series tools really become useful is when you begin to index data by timestamps. For example, we can construct a Series object that has time- indexed data: index = pd . DatetimeIndex ([ '2022-07-04' , '2022-08-04' , '2023-07-04' , '2023-08-04' ]) data = pd . Series ([ 0 , 1 , 2 , 3 ], index = index ) data 2022-07-04 0 2022-08-04 1 2023-07-04 2 2023-08-04 3 dtype: int64 data [ '2022-07-04' : '2023-07-04' ] 2022-07-04 0 2022-08-04 1 2023-07-04 2 dtype: int64 # just passing year to obtain a slice data [ '2022' ] 2022-07-04 0 2022-08-04 1 dtype: int64 data [ '2023' ] 2023-07-04 2 2023-08-04 3 dtype: int64","title":"Pandas Time Series: Indexing by Time"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#pandas-time-series-data-structures","text":"\u2022 For time stamps, Pandas provides the Timestamp type. As mentioned before, it is essentially a replacement for Python\u2019s native datetime, but is based on the more efficient numpy.datetime64 data type. The associated index structure is DatetimeIndex. \u2022 For time periods, Pandas provides the Period type. This encodes a fixed frequency interval based on numpy.datetime64. The associated index structure is PeriodIndex. \u2022 For time deltas or durations, Pandas provides the Timedelta type. Timedelta is a more efficient replacement for Python\u2019s native datetime.timedelta type, and is based on numpy.timedelta64. The associated index structure is TimedeltaIndex. The most fundamental of these date/time objects are the Timestamp and DatetimeIn dex objects. While these class objects can be invoked directly, it is more common to use the pd.to_datetime() function, which can parse a wide variety of formats. Passing a single date to pd.to_datetime() yields a Timestamp; passing a series of dates by default yields a DatetimeIndex: dates = pd . to_datetime ([ datetime ( 2022 , 7 , 30 ), '30th of July, 2022' , '2022-7-30' , '20220730' ]) dates DatetimeIndex(['2022-07-30', '2022-07-30', '2022-07-30', '2022-07-30'], dtype='datetime64[ns]', freq=None) Any DatetimeIndex can be converted to a PeriodIndex with the to_period() func\u2010 tion with the addition of a frequency code; here we\u2019ll use \u2018D\u2019 to indicate daily frequency: dates . to_period ( 'D' ) PeriodIndex(['2022-07-30', '2022-07-30', '2022-07-30', '2022-07-30'], dtype='period[D]') # The above is a TimedeltaIndex, we can subtract one date from another dates - dates [ 0 ] TimedeltaIndex(['0 days', '0 days', '0 days', '0 days'], dtype='timedelta64[ns]', freq=None) dates = pd . to_datetime ([ datetime ( 2022 , 7 , 28 ), '29th of July, 2022' , '2022-7-30' , '20220731' ]) dates DatetimeIndex(['2022-07-28', '2022-07-29', '2022-07-30', '2022-07-31'], dtype='datetime64[ns]', freq=None) dates . to_period ( 'D' ) PeriodIndex(['2022-07-28', '2022-07-29', '2022-07-30', '2022-07-31'], dtype='period[D]') dates - dates [ 0 ] TimedeltaIndex(['0 days', '1 days', '2 days', '3 days'], dtype='timedelta64[ns]', freq=None) dates - dates [ 1 ] TimedeltaIndex(['-1 days', '0 days', '1 days', '2 days'], dtype='timedelta64[ns]', freq=None)","title":"Pandas Time Series Data Structures"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#regular-sequences-pddate_range","text":"To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: pd.date_range() for timestamps, pd.period_range() for periods, and pd.timedelta_range() for time deltas. we have seen that Python range() and NumPy\u2019s np.arange() turn a startpoint, endpoint, and optional stepsize into a sequence. Similarly, pd.date_range() accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates. By default, the fre\u2010 quency is one day: pd . date_range ( '2022-07-30' , '2023-07-30' ) DatetimeIndex(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06', '2022-08-07', '2022-08-08', ... '2023-07-21', '2023-07-22', '2023-07-23', '2023-07-24', '2023-07-25', '2023-07-26', '2023-07-27', '2023-07-28', '2023-07-29', '2023-07-30'], dtype='datetime64[ns]', length=366, freq='D') Alternatively, the date range can be specified not with a start- and endpoint, but with a startpoint and a number of periods: pd . date_range ( '2022-07-30' , periods = 8 ) DatetimeIndex(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06'], dtype='datetime64[ns]', freq='D') You can modify the spacing by altering the freq argument, which defaults to D. For example, here we will construct a range of hourly timestamps: pd . date_range ( '2022-07-30' , periods = 8 , freq = 'H' ) DatetimeIndex(['2022-07-30 00:00:00', '2022-07-30 01:00:00', '2022-07-30 02:00:00', '2022-07-30 03:00:00', '2022-07-30 04:00:00', '2022-07-30 05:00:00', '2022-07-30 06:00:00', '2022-07-30 07:00:00'], dtype='datetime64[ns]', freq='H') To create regular sequences of period or time delta values, the very similar pd.period_range() and pd.timedelta_range() functions are useful. Here are some monthly periods: pd . period_range ( '2022-07' , periods = 8 , freq = 'M' ) PeriodIndex(['2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02'], dtype='period[M]') # and a sequence of durations increasing by an hour pd . timedelta_range ( 0 , periods = 10 , freq = 'H' ) TimedeltaIndex(['0 days 00:00:00', '0 days 01:00:00', '0 days 02:00:00', '0 days 03:00:00', '0 days 04:00:00', '0 days 05:00:00', '0 days 06:00:00', '0 days 07:00:00', '0 days 08:00:00', '0 days 09:00:00'], dtype='timedelta64[ns]', freq='H')","title":"Regular sequences: pd.date_range()"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 12 - Frequency Offset \u00b6 Frequencies and Offsets Frequencies and Offsets \u00b6 Fundamental to these Pandas time series tools is the concept of a frequency or date offset. Just as we saw the D (day) and H (hour) codes previously, we can use such codes to specify any desired frequency spacing. Code Description MS Month start BMS Business month start QS Quarter start BQS Business quarter start AS Year start BAS Business year start Code Description Code Description D Calendar day B Business day W Weekly M Month end BM Business month end Q Quarter end BQ Business quarter end A Year end BA Business year end H Hours BH Business hours T Minutes S Seconds L Milliseonds U Microseconds N Nanoseconds Additionally, you can change the month used to mark any quarterly or annual code by adding a three-letter month code as a suffix: \u2022 Q-JAN, BQ-FEB, QS-MAR, BQS-APR, etc. \u2022 A-JAN, BA-FEB, AS-MAR, BAS-APR, etc. In the same way, you can modify the split-point of the weekly frequency by adding a three-letter weekday code: \u2022 W-SUN, W-MON, W-TUE, W-WED, etc. On top of this, codes can be combined with numbers to specify other frequencies. For example, for a frequency of 2 hours 30 minutes, we can combine the hour (H) and minute (T) codes as follows: import pandas as pd pd . timedelta_range ( 0 , periods = 9 , freq = '2H30T' ) TimedeltaIndex(['0 days 00:00:00', '0 days 02:30:00', '0 days 05:00:00', '0 days 07:30:00', '0 days 10:00:00', '0 days 12:30:00', '0 days 15:00:00', '0 days 17:30:00', '0 days 20:00:00'], dtype='timedelta64[ns]', freq='150T') All of these short codes refer to specific instances of Pandas time series offsets, which can be found in the pd.tseries.offsets module. For example, we can create a busi\u2010 ness day offset directly as follows: from pandas.tseries.offsets import BDay pd . date_range ( '2022-07-28' , periods = 5 , freq = BDay ()) DatetimeIndex(['2022-07-28', '2022-07-29', '2022-08-01', '2022-08-02', '2022-08-03'], dtype='datetime64[ns]', freq='B')","title":"12 Frequency Offset"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/#12-frequency-offset","text":"Frequencies and Offsets","title":"12 - Frequency Offset"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/#frequencies-and-offsets","text":"Fundamental to these Pandas time series tools is the concept of a frequency or date offset. Just as we saw the D (day) and H (hour) codes previously, we can use such codes to specify any desired frequency spacing. Code Description MS Month start BMS Business month start QS Quarter start BQS Business quarter start AS Year start BAS Business year start Code Description Code Description D Calendar day B Business day W Weekly M Month end BM Business month end Q Quarter end BQ Business quarter end A Year end BA Business year end H Hours BH Business hours T Minutes S Seconds L Milliseonds U Microseconds N Nanoseconds Additionally, you can change the month used to mark any quarterly or annual code by adding a three-letter month code as a suffix: \u2022 Q-JAN, BQ-FEB, QS-MAR, BQS-APR, etc. \u2022 A-JAN, BA-FEB, AS-MAR, BAS-APR, etc. In the same way, you can modify the split-point of the weekly frequency by adding a three-letter weekday code: \u2022 W-SUN, W-MON, W-TUE, W-WED, etc. On top of this, codes can be combined with numbers to specify other frequencies. For example, for a frequency of 2 hours 30 minutes, we can combine the hour (H) and minute (T) codes as follows: import pandas as pd pd . timedelta_range ( 0 , periods = 9 , freq = '2H30T' ) TimedeltaIndex(['0 days 00:00:00', '0 days 02:30:00', '0 days 05:00:00', '0 days 07:30:00', '0 days 10:00:00', '0 days 12:30:00', '0 days 15:00:00', '0 days 17:30:00', '0 days 20:00:00'], dtype='timedelta64[ns]', freq='150T') All of these short codes refer to specific instances of Pandas time series offsets, which can be found in the pd.tseries.offsets module. For example, we can create a busi\u2010 ness day offset directly as follows: from pandas.tseries.offsets import BDay pd . date_range ( '2022-07-28' , periods = 5 , freq = BDay ()) DatetimeIndex(['2022-07-28', '2022-07-29', '2022-08-01', '2022-08-02', '2022-08-03'], dtype='datetime64[ns]', freq='B')","title":"Frequencies and Offsets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 13 - Resampling, Shifting and Windowing \u00b6 Resampling, Shifting, and Windowing Resampling and converting frequencies Time-shifts Rolling windows Resampling, Shifting, and Windowing \u00b6 The ability to use dates and times as indices to intuitively organize and access data is an important piece of the Pandas time series tools. The benefits of indexed data in general (automatic alignment during operations, intuitive data slicing and access, etc.) still apply, and Pandas provides several additional time series\u2013specific operations. ! conda install pandas - datareader - y Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /home/qalmaqihir/anaconda3 added / updated specs: - pandas-datareader The following packages will be downloaded: package | build ---------------------------|----------------- conda-4.14.0 | py39h06a4308_0 915 KB pandas-datareader-0.10.0 | pyhd3eb1b0_0 71 KB ------------------------------------------------------------ Total: 987 KB The following NEW packages will be INSTALLED: pandas-datareader pkgs/main/noarch::pandas-datareader-0.10.0-pyhd3eb1b0_0 The following packages will be UPDATED: conda 4.13.0-py39h06a4308_0 --> 4.14.0-py39h06a4308_0 Downloading and Extracting Packages pandas-datareader-0. | 71 KB | ##################################### | 100% conda-4.14.0 | 915 KB | ##################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done import pandas as pd from pandas_datareader import data google = data . DataReader ( 'GOOG' , start = '2004' , end = '2017' , data_source = 'yahoo' ) google . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } High Low Open Close Volume Adj Close Date 2004-08-19 2.591785 2.390042 2.490664 2.499133 897427216.0 2.499133 2004-08-20 2.716817 2.503118 2.515820 2.697639 458857488.0 2.697639 2004-08-23 2.826406 2.716070 2.758411 2.724787 366857939.0 2.724787 2004-08-24 2.779581 2.579581 2.770615 2.611960 306396159.0 2.611960 2004-08-25 2.689918 2.587302 2.614201 2.640104 184645512.0 2.640104 google . Close Date 2004-08-19 2.499133 2004-08-20 2.697639 2004-08-23 2.724787 2004-08-24 2.611960 2004-08-25 2.640104 ... 2016-12-23 39.495499 2016-12-27 39.577499 2016-12-28 39.252499 2016-12-29 39.139500 2016-12-30 38.591000 Name: Close, Length: 3115, dtype: float64 #google['Close'] % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () google [ 'Close' ] . plot (); Resampling and converting frequencies \u00b6 One common need for time series data is resampling at a higher or lower frequency. You can do this using the resample() method, or the much simpler asfreq() method. The primary difference between the two is that resample() is fundamentally a data aggregation, while asfreq() is fundamentally a data selection. google [ \"Close\" ] . plot ( alpha = 0.5 , style = '-' ) google [ \"Close\" ] . resample ( 'BA' ) . mean () . plot ( style = ':' ) google [ \"Close\" ] . asfreq ( 'BA' ) . plot ( style = '--' ); plt . legend ([ 'input' , 'resample' , 'asfreq' ], loc = 'upper left' ) <matplotlib.legend.Legend at 0x7fe1d96367c0> goog = google [ 'Close' ] fig , ax = plt . subplots ( 2 , sharex = True ) data = goog . iloc [: 10 ] data . asfreq ( 'D' ) . plot ( ax = ax [ 0 ], marker = 'o' ) data . asfreq ( 'D' , method = 'bfill' ) . plot ( ax = ax [ 1 ], style = '-o' ) data . asfreq ( 'D' , method = 'ffill' ) . plot ( ax = ax [ 1 ], style = '--o' ) ax [ 1 ] . legend ([ \"back-fill\" , \"forward-fill\" ]); Time-shifts \u00b6 Another common time series\u2013specific operation is shifting of data in time. Pandas has two closely related methods for computing this: shift() and tshift(). In short, the difference between them is that shift() shifts the data, while tshift() shifts the index. In both cases, the shift is specified in multiples of the frequenc fig , ax = plt . subplots ( 3 , sharey = True ) # apply a frequency to the data goog = goog . asfreq ( 'D' , method = 'pad' ) goog . plot ( ax = ax [ 0 ]) goog . shift ( 900 ) . plot ( ax = ax [ 1 ]) goog . tshift ( 900 ) . plot ( ax = ax [ 2 ]) # legends and annotations local_max = pd . to_datetime ( '2007-11-05' ) offset = pd . Timedelta ( 900 , 'D' ) ax [ 0 ] . legend ([ 'input' ], loc = 2 ) ax [ 0 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 0 ] . axvline ( local_max , alpha = 0.3 , color = 'red' ) ax [ 1 ] . legend ([ 'shift(900)' ], loc = 2 ) ax [ 1 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 1 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ) ax [ 2 ] . legend ([ 'tshift(900)' ], loc = 2 ) ax [ 2 ] . get_xticklabels ()[ 1 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 2 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ); /tmp/ipykernel_66268/1389856076.py:6: FutureWarning: tshift is deprecated and will be removed in a future version. Please use shift instead. goog.tshift(900).plot(ax=ax[2]) ROI = 100 * ( goog . tshift ( - 365 ) / goog - 1 ) ROI . plot () plt . ylabel ( '% Return on Investment' ); /tmp/ipykernel_66268/2632432407.py:1: FutureWarning: tshift is deprecated and will be removed in a future version. Please use shift instead. ROI = 100 * (goog.tshift(-365) / goog - 1) Rolling windows \u00b6 Rolling statistics are a third type of time series\u2013specific operation implemented by Pandas. These can be accomplished via the rolling() attribute of Series and Data Frame objects, which returns a view similar to what we saw with the groupby opera\u2010 tion rolling = goog . rolling ( 365 , center = True ) data = pd . DataFrame ({ 'input' : goog , 'one-year rolling_mean' : rolling . mean (), 'one-year rolling_std' : rolling . std ()}) ax = data . plot ( style = [ '-' , '--' , ':' ]) ax . lines [ 0 ] . set_alpha ( 0.3 )","title":"13 Resampling Shifting and Windowing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#13-resampling-shifting-and-windowing","text":"Resampling, Shifting, and Windowing Resampling and converting frequencies Time-shifts Rolling windows","title":"13 - Resampling, Shifting and Windowing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#resampling-shifting-and-windowing","text":"The ability to use dates and times as indices to intuitively organize and access data is an important piece of the Pandas time series tools. The benefits of indexed data in general (automatic alignment during operations, intuitive data slicing and access, etc.) still apply, and Pandas provides several additional time series\u2013specific operations. ! conda install pandas - datareader - y Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /home/qalmaqihir/anaconda3 added / updated specs: - pandas-datareader The following packages will be downloaded: package | build ---------------------------|----------------- conda-4.14.0 | py39h06a4308_0 915 KB pandas-datareader-0.10.0 | pyhd3eb1b0_0 71 KB ------------------------------------------------------------ Total: 987 KB The following NEW packages will be INSTALLED: pandas-datareader pkgs/main/noarch::pandas-datareader-0.10.0-pyhd3eb1b0_0 The following packages will be UPDATED: conda 4.13.0-py39h06a4308_0 --> 4.14.0-py39h06a4308_0 Downloading and Extracting Packages pandas-datareader-0. | 71 KB | ##################################### | 100% conda-4.14.0 | 915 KB | ##################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done import pandas as pd from pandas_datareader import data google = data . DataReader ( 'GOOG' , start = '2004' , end = '2017' , data_source = 'yahoo' ) google . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } High Low Open Close Volume Adj Close Date 2004-08-19 2.591785 2.390042 2.490664 2.499133 897427216.0 2.499133 2004-08-20 2.716817 2.503118 2.515820 2.697639 458857488.0 2.697639 2004-08-23 2.826406 2.716070 2.758411 2.724787 366857939.0 2.724787 2004-08-24 2.779581 2.579581 2.770615 2.611960 306396159.0 2.611960 2004-08-25 2.689918 2.587302 2.614201 2.640104 184645512.0 2.640104 google . Close Date 2004-08-19 2.499133 2004-08-20 2.697639 2004-08-23 2.724787 2004-08-24 2.611960 2004-08-25 2.640104 ... 2016-12-23 39.495499 2016-12-27 39.577499 2016-12-28 39.252499 2016-12-29 39.139500 2016-12-30 38.591000 Name: Close, Length: 3115, dtype: float64 #google['Close'] % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () google [ 'Close' ] . plot ();","title":"Resampling, Shifting, and Windowing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#resampling-and-converting-frequencies","text":"One common need for time series data is resampling at a higher or lower frequency. You can do this using the resample() method, or the much simpler asfreq() method. The primary difference between the two is that resample() is fundamentally a data aggregation, while asfreq() is fundamentally a data selection. google [ \"Close\" ] . plot ( alpha = 0.5 , style = '-' ) google [ \"Close\" ] . resample ( 'BA' ) . mean () . plot ( style = ':' ) google [ \"Close\" ] . asfreq ( 'BA' ) . plot ( style = '--' ); plt . legend ([ 'input' , 'resample' , 'asfreq' ], loc = 'upper left' ) <matplotlib.legend.Legend at 0x7fe1d96367c0> goog = google [ 'Close' ] fig , ax = plt . subplots ( 2 , sharex = True ) data = goog . iloc [: 10 ] data . asfreq ( 'D' ) . plot ( ax = ax [ 0 ], marker = 'o' ) data . asfreq ( 'D' , method = 'bfill' ) . plot ( ax = ax [ 1 ], style = '-o' ) data . asfreq ( 'D' , method = 'ffill' ) . plot ( ax = ax [ 1 ], style = '--o' ) ax [ 1 ] . legend ([ \"back-fill\" , \"forward-fill\" ]);","title":"Resampling and converting frequencies"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#time-shifts","text":"Another common time series\u2013specific operation is shifting of data in time. Pandas has two closely related methods for computing this: shift() and tshift(). In short, the difference between them is that shift() shifts the data, while tshift() shifts the index. In both cases, the shift is specified in multiples of the frequenc fig , ax = plt . subplots ( 3 , sharey = True ) # apply a frequency to the data goog = goog . asfreq ( 'D' , method = 'pad' ) goog . plot ( ax = ax [ 0 ]) goog . shift ( 900 ) . plot ( ax = ax [ 1 ]) goog . tshift ( 900 ) . plot ( ax = ax [ 2 ]) # legends and annotations local_max = pd . to_datetime ( '2007-11-05' ) offset = pd . Timedelta ( 900 , 'D' ) ax [ 0 ] . legend ([ 'input' ], loc = 2 ) ax [ 0 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 0 ] . axvline ( local_max , alpha = 0.3 , color = 'red' ) ax [ 1 ] . legend ([ 'shift(900)' ], loc = 2 ) ax [ 1 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 1 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ) ax [ 2 ] . legend ([ 'tshift(900)' ], loc = 2 ) ax [ 2 ] . get_xticklabels ()[ 1 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 2 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ); /tmp/ipykernel_66268/1389856076.py:6: FutureWarning: tshift is deprecated and will be removed in a future version. Please use shift instead. goog.tshift(900).plot(ax=ax[2]) ROI = 100 * ( goog . tshift ( - 365 ) / goog - 1 ) ROI . plot () plt . ylabel ( '% Return on Investment' ); /tmp/ipykernel_66268/2632432407.py:1: FutureWarning: tshift is deprecated and will be removed in a future version. Please use shift instead. ROI = 100 * (goog.tshift(-365) / goog - 1)","title":"Time-shifts"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#rolling-windows","text":"Rolling statistics are a third type of time series\u2013specific operation implemented by Pandas. These can be accomplished via the rolling() attribute of Series and Data Frame objects, which returns a view similar to what we saw with the groupby opera\u2010 tion rolling = goog . rolling ( 365 , center = True ) data = pd . DataFrame ({ 'input' : goog , 'one-year rolling_mean' : rolling . mean (), 'one-year rolling_std' : rolling . std ()}) ax = data . plot ( style = [ '-' , '--' , ':' ]) ax . lines [ 0 ] . set_alpha ( 0.3 )","title":"Rolling windows"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 Example: Visualzing Seattle Bucycle Counts \u00b6 Example: Visualizing Seattle Bicycle Counts Visualizing the Data Digging into the data Example: Visualizing Seattle Bicycle Counts \u00b6 As a more involved example of working with some time series data, let\u2019s take a look at bicycle counts on Seattle\u2019s Fremont Bridge. This data comes from an automated bicy\u2010 cle counter, installed in late 2012, which has inductive sensors on the east and west sidewalks of the bridge. ! curl - o FremontBridge . csv https : // data . seattle . gov / api / views / 65 db - xm6k / rows . csv ? accessType = DOWNLOAD % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2679k 0 2679k 0 0 170k 0 --:--:-- 0:00:15 --:--:-- 296k import pandas as pd import numpy as np data = pd . read_csv ( 'FremontBridge.csv' , index_col = 'Date' , parse_dates = True ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fremont Bridge Total Fremont Bridge East Sidewalk Fremont Bridge West Sidewalk Date 2012-10-03 00:00:00 13.0 4.0 9.0 2012-10-03 01:00:00 10.0 4.0 6.0 2012-10-03 02:00:00 2.0 1.0 1.0 2012-10-03 03:00:00 5.0 2.0 3.0 2012-10-03 04:00:00 7.0 6.0 1.0 data . columns = [ 'West' , 'East' , 'Total' ] data [ 'Total' ] = data . eval ( 'West + East' ) data . dropna () . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } West East Total count 86122.000000 86122.000000 86122.000000 mean 106.798449 47.996238 154.794687 std 134.926536 61.795993 192.517894 min 0.000000 0.000000 0.000000 25% 13.000000 6.000000 19.000000 50% 59.000000 27.000000 86.000000 75% 143.000000 66.000000 209.000000 max 1097.000000 698.000000 1569.000000 Visualizing the Data \u00b6 % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set (); data . plot () plt . ylabel ( 'Hourly Bicycle Count' ); weekly = data . resample ( 'W' ) . sum () weekly . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'Weekly bicycle count' ) Text(0, 0.5, 'Weekly bicycle count') people bicycle more in the summer than in the winter, and even within a particular season the bicy\u2010 cle use varies from week to week (likely dependent on weather daily = data . resample ( 'D' ) . sum () daily . rolling ( 30 , center = True ) . sum () . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'Mean hourly count' ) Text(0, 0.5, 'Mean hourly count') The jaggedness of the result is due to the hard cutoff of the window. We can get a smoother version of a rolling mean using a window function\u2014for example, a Gaus\u2010 sian window. daily . rolling ( 50 , center = True , win_type = 'gaussian' ) . sum ( std = 10 ) . plot ( style = [ ':' , '--' , '-' ]) <AxesSubplot:xlabel='Date'> Digging into the data \u00b6 While the smoothed data views above are useful to get an idea of the general trend in the data, they hide much of the interesting structure. For example, we might want to look at the average traffic as a function of the time of day. We can do this using the GroupBy functionality by_time = data . groupby ( data . index . time ) . mean () hourly_ticks = 4 * 60 * np . arange ( 6 ) by_time . plot ( xticks = hourly_ticks , style = [ ':' , '--' , '-' ]) <AxesSubplot:xlabel='time'> by_weekday = data . groupby ( data . index . day_of_week ) . mean () by_weekday . index = [ 'Mon' , 'Tues' , 'Wed' , 'Thr' , 'Fri' , 'Sat' , 'Sun' ] by_weekday . plot ( style = [ ':' , '--' , '-' ]) <AxesSubplot:> This shows a strong distinction between weekday and weekend totals, with around twice as many average riders crossing the bridge on Monday through Friday than on Saturday and Sunday. With this in mind, let\u2019s do a compound groupby and look at the hourly trend on weekdays versus weekends. We\u2019ll start by grouping by both a flag marking the week\u2010 end, and the time of day: weekend = np . where ( data . index . weekday < 5 , 'Weekday' , 'Weekend' ) by_time = data . groupby ([ weekend , data . index . time ]) . mean () import matplotlib.pyplot as plt fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) by_time . loc [ 'Weekday' ] . plot ( ax = ax [ 0 ], title = 'Weekdays' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]) by_time . loc [ 'Weekend' ] . plot ( ax = ax [ 1 ], title = 'Weekends' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]);","title":"Example Visualizing Seattle Bicycle Counts"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#example-visualzing-seattle-bucycle-counts","text":"Example: Visualizing Seattle Bicycle Counts Visualizing the Data Digging into the data","title":"Example: Visualzing Seattle Bucycle Counts"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#example-visualizing-seattle-bicycle-counts","text":"As a more involved example of working with some time series data, let\u2019s take a look at bicycle counts on Seattle\u2019s Fremont Bridge. This data comes from an automated bicy\u2010 cle counter, installed in late 2012, which has inductive sensors on the east and west sidewalks of the bridge. ! curl - o FremontBridge . csv https : // data . seattle . gov / api / views / 65 db - xm6k / rows . csv ? accessType = DOWNLOAD % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2679k 0 2679k 0 0 170k 0 --:--:-- 0:00:15 --:--:-- 296k import pandas as pd import numpy as np data = pd . read_csv ( 'FremontBridge.csv' , index_col = 'Date' , parse_dates = True ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fremont Bridge Total Fremont Bridge East Sidewalk Fremont Bridge West Sidewalk Date 2012-10-03 00:00:00 13.0 4.0 9.0 2012-10-03 01:00:00 10.0 4.0 6.0 2012-10-03 02:00:00 2.0 1.0 1.0 2012-10-03 03:00:00 5.0 2.0 3.0 2012-10-03 04:00:00 7.0 6.0 1.0 data . columns = [ 'West' , 'East' , 'Total' ] data [ 'Total' ] = data . eval ( 'West + East' ) data . dropna () . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } West East Total count 86122.000000 86122.000000 86122.000000 mean 106.798449 47.996238 154.794687 std 134.926536 61.795993 192.517894 min 0.000000 0.000000 0.000000 25% 13.000000 6.000000 19.000000 50% 59.000000 27.000000 86.000000 75% 143.000000 66.000000 209.000000 max 1097.000000 698.000000 1569.000000","title":"Example: Visualizing Seattle Bicycle Counts"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#visualizing-the-data","text":"% matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set (); data . plot () plt . ylabel ( 'Hourly Bicycle Count' ); weekly = data . resample ( 'W' ) . sum () weekly . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'Weekly bicycle count' ) Text(0, 0.5, 'Weekly bicycle count') people bicycle more in the summer than in the winter, and even within a particular season the bicy\u2010 cle use varies from week to week (likely dependent on weather daily = data . resample ( 'D' ) . sum () daily . rolling ( 30 , center = True ) . sum () . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'Mean hourly count' ) Text(0, 0.5, 'Mean hourly count') The jaggedness of the result is due to the hard cutoff of the window. We can get a smoother version of a rolling mean using a window function\u2014for example, a Gaus\u2010 sian window. daily . rolling ( 50 , center = True , win_type = 'gaussian' ) . sum ( std = 10 ) . plot ( style = [ ':' , '--' , '-' ]) <AxesSubplot:xlabel='Date'>","title":"Visualizing the Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#digging-into-the-data","text":"While the smoothed data views above are useful to get an idea of the general trend in the data, they hide much of the interesting structure. For example, we might want to look at the average traffic as a function of the time of day. We can do this using the GroupBy functionality by_time = data . groupby ( data . index . time ) . mean () hourly_ticks = 4 * 60 * np . arange ( 6 ) by_time . plot ( xticks = hourly_ticks , style = [ ':' , '--' , '-' ]) <AxesSubplot:xlabel='time'> by_weekday = data . groupby ( data . index . day_of_week ) . mean () by_weekday . index = [ 'Mon' , 'Tues' , 'Wed' , 'Thr' , 'Fri' , 'Sat' , 'Sun' ] by_weekday . plot ( style = [ ':' , '--' , '-' ]) <AxesSubplot:> This shows a strong distinction between weekday and weekend totals, with around twice as many average riders crossing the bridge on Monday through Friday than on Saturday and Sunday. With this in mind, let\u2019s do a compound groupby and look at the hourly trend on weekdays versus weekends. We\u2019ll start by grouping by both a flag marking the week\u2010 end, and the time of day: weekend = np . where ( data . index . weekday < 5 , 'Weekday' , 'Weekend' ) by_time = data . groupby ([ weekend , data . index . time ]) . mean () import matplotlib.pyplot as plt fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) by_time . loc [ 'Weekday' ] . plot ( ax = ax [ 0 ], title = 'Weekdays' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]) by_time . loc [ 'Weekend' ] . plot ( ax = ax [ 1 ], title = 'Weekends' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]);","title":"Digging into the data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 Example: Birthrate Data \u00b6 Example: Birthrate Data Further data exploration Example: Birthrate Data \u00b6 As a more interesting example, let\u2019s take a look at the freely available data on births in the United States, provided by the Centers for Disease Control (CDC). This data can be found at link births.csv (this dataset has been analyzed rather extensively by Andrew Gelman and his group; see, for example, this blog post) # shell command to download the data: # !curl -O https://raw.githubusercontent.com/jakevdp/data-CDCbirths/ # master/births.csv import numpy as np import pandas as pd births = pd . read_csv ( \"../data/births.csv\" ) births . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births 0 1969 1 1.0 F 4046 1 1969 1 1.0 M 4440 2 1969 1 2.0 F 4454 3 1969 1 2.0 M 4548 4 1969 1 3.0 F 4548 We can start to understand this data a bit more by using a pivot table. Let\u2019s add a dec\u2010 ade column, and take a look at male and female births as a function of decade: births [ 'decade' ] = 10 * ( births [ 'year' ] // 10 ) births . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1.0 F 4046 1960 1 1969 1 1.0 M 4440 1960 2 1969 1 2.0 F 4454 1960 3 1969 1 2.0 M 4548 1960 4 1969 1 3.0 F 4548 1960 births . pivot_table ( 'births' , index = 'decade' , columns = 'gender' , aggfunc = 'sum' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gender F M decade 1960 1753634 1846572 1970 16263075 17121550 1980 18310351 19243452 1990 19479454 20420553 2000 18229309 19106428 We immediately see that male births outnumber female births in every decade. To see this trend a bit more clearly, we can use the built-in plotting tools in Pandas to visual\u2010 ize the total number of births by year % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns sns . set () births . pivot_table ( 'births' , index = 'year' , columns = 'gender' , aggfunc = 'sum' ) . plot () plt . ylabel ( \"total birth per year\" ) Text(0, 0.5, 'total birth per year') Further data exploration \u00b6 Though this doesn\u2019t necessarily relate to the pivot table, there are a few more interest\u2010 ing features we can pull out of this dataset using the Pandas tools covered up to this point. We must start by cleaning the data a bit, removing outliers caused by mistyped dates (e.g., June 31 st ) or missing values (e.g., June 99 th ). One easy way to remove these all at once is to cut outliers; we\u2019ll do this via a robust sigma-clipping operation quartiles = np . percentile ( births [ 'births' ],[ 25 , 50 , 75 ]) mu = quartiles [ 1 ] sig = 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) mu 4814.0 sig 689.31 This final line is a robust estimate of the sample mean, where the 0.74 comes from the interquartile range of a Gaussian distribution. With this we can use the query() method to filter out rows with births outside these values: births = births . query ( '(births > @mu -5 * @sig) & (births<@mu + 5*@sig)' ) births .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1.0 F 4046 1960 1 1969 1 1.0 M 4440 1960 2 1969 1 2.0 F 4454 1960 3 1969 1 2.0 M 4548 1960 4 1969 1 3.0 F 4548 1960 ... ... ... ... ... ... ... 15062 1988 12 29.0 M 5944 1980 15063 1988 12 30.0 F 5742 1980 15064 1988 12 30.0 M 6095 1980 15065 1988 12 31.0 F 4435 1980 15066 1988 12 31.0 M 4698 1980 14610 rows \u00d7 6 columns Next we set the day column to integers; previously it had been a string because some columns in the dataset contained the value \u2018null\u2019: births [ 'day' ] = births [ 'day' ] . astype ( int ) /tmp/ipykernel_18639/3805690895.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy births['day']=births['day'].astype(int) births .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1 F 4046 1960 1 1969 1 1 M 4440 1960 2 1969 1 2 F 4454 1960 3 1969 1 2 M 4548 1960 4 1969 1 3 F 4548 1960 ... ... ... ... ... ... ... 15062 1988 12 29 M 5944 1980 15063 1988 12 30 F 5742 1980 15064 1988 12 30 M 6095 1980 15065 1988 12 31 F 4435 1980 15066 1988 12 31 M 4698 1980 14610 rows \u00d7 6 columns Finally, we can combine the day, month, and year to create a Date index This allows us to quickly compute the weekday corresponding to each row: # create a datetime index from the year, month, day births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = '%Y%m %d ' ) births [ 'dayofweek' ] = births . index . day_of_week # Using this we can plot the births by weekday for several decades import matplotlib.pyplot as plt import matplotlib as mpl births . pivot_table ( 'births' , index = 'dayofweek' , columns = 'decade' , aggfunc = 'mean' ) . plot () plt . gca () . set_xticklabels ([ 'Mon' , 'Tues' , 'Wed' , 'Thurs' , 'Fri' , 'Sat' , 'Sun' ]) plt . ylabel ( 'mean births by day' ); /tmp/ipykernel_18639/3967923407.py:6: UserWarning: FixedFormatter should only be used together with FixedLocator plt.gca().set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']) births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } births 1 1 4009.225 2 4247.400 3 4500.900 4 4571.350 5 4603.625 ... ... ... 12 27 4850.150 28 5044.200 29 5120.150 30 5172.350 31 4859.200 366 rows \u00d7 1 columns births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] /tmp/ipykernel_18639/1749910599.py:1: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead. births_by_date.index=[pd.datetime(2012,month,day) for (month,day)in births_by_date.index] births_by_date .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } births 2012-01-01 4009.225 2012-01-02 4247.400 2012-01-03 4500.900 2012-01-04 4571.350 2012-01-05 4603.625 ... ... 2012-12-27 4850.150 2012-12-28 5044.200 2012-12-29 5120.150 2012-12-30 5172.350 2012-12-31 4859.200 366 rows \u00d7 1 columns # Focusing on the month and day only, we now have a time series reflecting the average #number of births by date of the year. fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) <AxesSubplot:>","title":"Example  Birthrate Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/#example-birthrate-data","text":"Example: Birthrate Data Further data exploration","title":"Example: Birthrate Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/#example-birthrate-data_1","text":"As a more interesting example, let\u2019s take a look at the freely available data on births in the United States, provided by the Centers for Disease Control (CDC). This data can be found at link births.csv (this dataset has been analyzed rather extensively by Andrew Gelman and his group; see, for example, this blog post) # shell command to download the data: # !curl -O https://raw.githubusercontent.com/jakevdp/data-CDCbirths/ # master/births.csv import numpy as np import pandas as pd births = pd . read_csv ( \"../data/births.csv\" ) births . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births 0 1969 1 1.0 F 4046 1 1969 1 1.0 M 4440 2 1969 1 2.0 F 4454 3 1969 1 2.0 M 4548 4 1969 1 3.0 F 4548 We can start to understand this data a bit more by using a pivot table. Let\u2019s add a dec\u2010 ade column, and take a look at male and female births as a function of decade: births [ 'decade' ] = 10 * ( births [ 'year' ] // 10 ) births . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1.0 F 4046 1960 1 1969 1 1.0 M 4440 1960 2 1969 1 2.0 F 4454 1960 3 1969 1 2.0 M 4548 1960 4 1969 1 3.0 F 4548 1960 births . pivot_table ( 'births' , index = 'decade' , columns = 'gender' , aggfunc = 'sum' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gender F M decade 1960 1753634 1846572 1970 16263075 17121550 1980 18310351 19243452 1990 19479454 20420553 2000 18229309 19106428 We immediately see that male births outnumber female births in every decade. To see this trend a bit more clearly, we can use the built-in plotting tools in Pandas to visual\u2010 ize the total number of births by year % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns sns . set () births . pivot_table ( 'births' , index = 'year' , columns = 'gender' , aggfunc = 'sum' ) . plot () plt . ylabel ( \"total birth per year\" ) Text(0, 0.5, 'total birth per year')","title":"Example: Birthrate Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/#further-data-exploration","text":"Though this doesn\u2019t necessarily relate to the pivot table, there are a few more interest\u2010 ing features we can pull out of this dataset using the Pandas tools covered up to this point. We must start by cleaning the data a bit, removing outliers caused by mistyped dates (e.g., June 31 st ) or missing values (e.g., June 99 th ). One easy way to remove these all at once is to cut outliers; we\u2019ll do this via a robust sigma-clipping operation quartiles = np . percentile ( births [ 'births' ],[ 25 , 50 , 75 ]) mu = quartiles [ 1 ] sig = 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) mu 4814.0 sig 689.31 This final line is a robust estimate of the sample mean, where the 0.74 comes from the interquartile range of a Gaussian distribution. With this we can use the query() method to filter out rows with births outside these values: births = births . query ( '(births > @mu -5 * @sig) & (births<@mu + 5*@sig)' ) births .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1.0 F 4046 1960 1 1969 1 1.0 M 4440 1960 2 1969 1 2.0 F 4454 1960 3 1969 1 2.0 M 4548 1960 4 1969 1 3.0 F 4548 1960 ... ... ... ... ... ... ... 15062 1988 12 29.0 M 5944 1980 15063 1988 12 30.0 F 5742 1980 15064 1988 12 30.0 M 6095 1980 15065 1988 12 31.0 F 4435 1980 15066 1988 12 31.0 M 4698 1980 14610 rows \u00d7 6 columns Next we set the day column to integers; previously it had been a string because some columns in the dataset contained the value \u2018null\u2019: births [ 'day' ] = births [ 'day' ] . astype ( int ) /tmp/ipykernel_18639/3805690895.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy births['day']=births['day'].astype(int) births .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1 F 4046 1960 1 1969 1 1 M 4440 1960 2 1969 1 2 F 4454 1960 3 1969 1 2 M 4548 1960 4 1969 1 3 F 4548 1960 ... ... ... ... ... ... ... 15062 1988 12 29 M 5944 1980 15063 1988 12 30 F 5742 1980 15064 1988 12 30 M 6095 1980 15065 1988 12 31 F 4435 1980 15066 1988 12 31 M 4698 1980 14610 rows \u00d7 6 columns Finally, we can combine the day, month, and year to create a Date index This allows us to quickly compute the weekday corresponding to each row: # create a datetime index from the year, month, day births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = '%Y%m %d ' ) births [ 'dayofweek' ] = births . index . day_of_week # Using this we can plot the births by weekday for several decades import matplotlib.pyplot as plt import matplotlib as mpl births . pivot_table ( 'births' , index = 'dayofweek' , columns = 'decade' , aggfunc = 'mean' ) . plot () plt . gca () . set_xticklabels ([ 'Mon' , 'Tues' , 'Wed' , 'Thurs' , 'Fri' , 'Sat' , 'Sun' ]) plt . ylabel ( 'mean births by day' ); /tmp/ipykernel_18639/3967923407.py:6: UserWarning: FixedFormatter should only be used together with FixedLocator plt.gca().set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']) births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } births 1 1 4009.225 2 4247.400 3 4500.900 4 4571.350 5 4603.625 ... ... ... 12 27 4850.150 28 5044.200 29 5120.150 30 5172.350 31 4859.200 366 rows \u00d7 1 columns births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] /tmp/ipykernel_18639/1749910599.py:1: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead. births_by_date.index=[pd.datetime(2012,month,day) for (month,day)in births_by_date.index] births_by_date .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } births 2012-01-01 4009.225 2012-01-02 4247.400 2012-01-03 4500.900 2012-01-04 4571.350 2012-01-05 4603.625 ... ... 2012-12-27 4850.150 2012-12-28 5044.200 2012-12-29 5120.150 2012-12-30 5172.350 2012-12-31 4859.200 366 rows \u00d7 1 columns # Focusing on the month and day only, we now have a time series reflecting the average #number of births by date of the year. fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) <AxesSubplot:>","title":"Further data exploration"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 Example: US States Data \u00b6 Example: US States Data Example: US States Data \u00b6 Merge and join operations come up most often when one is combining data from dif\u2010 ferent sources. Here we will consider an example of some data about US states and their populations. import numpy as np import pandas as pd pop = pd . read_csv ( \"../data/state-population.csv\" ) areas = pd . read_csv ( \"../data/state-areas.csv\" ) abbrevs = pd . read_csv ( \"../data/state-abbrevs.csv\" ) pop . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population 0 AL under18 2012 1117489.0 1 AL total 2012 4817528.0 2 AL under18 2010 1130966.0 3 AL total 2010 4785570.0 4 AL under18 2011 1125763.0 areas . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state area (sq. mi) 0 Alabama 52423 1 Alaska 656425 2 Arizona 114006 3 Arkansas 53182 4 California 163707 abbrevs . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state abbreviation 0 Alabama AL 1 Alaska AK 2 Arizona AZ 3 Arkansas AR 4 California CA Given this information, say we want to compute a relatively straightforward result:rank US states and territories by their 2010 population density merged = pd . merge ( pop , abbrevs , how = 'outer' , left_on = 'state/region' , right_on = 'abbreviation' ) merged .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state abbreviation 0 AL under18 2012 1117489.0 Alabama AL 1 AL total 2012 4817528.0 Alabama AL 2 AL under18 2010 1130966.0 Alabama AL 3 AL total 2010 4785570.0 Alabama AL 4 AL under18 2011 1125763.0 Alabama AL ... ... ... ... ... ... ... 2539 USA total 2010 309326295.0 NaN NaN 2540 USA under18 2011 73902222.0 NaN NaN 2541 USA total 2011 311582564.0 NaN NaN 2542 USA under18 2012 73708179.0 NaN NaN 2543 USA total 2012 313873685.0 NaN NaN 2544 rows \u00d7 6 columns merged = merged . drop ( 'abbreviation' , 1 ) /tmp/ipykernel_88168/2168094788.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only. merged=merged.drop('abbreviation',1) merged . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 0 AL under18 2012 1117489.0 Alabama 1 AL total 2012 4817528.0 Alabama 2 AL under18 2010 1130966.0 Alabama 3 AL total 2010 4785570.0 Alabama 4 AL under18 2011 1125763.0 Alabama # CHeck if there is any mismatch, merged . isnull () . any () state/region False ages False year False population True state True dtype: bool ## Some of the population and state info is null, lets check which one merged [ merged [ 'population' ] . isnull ()] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 2448 PR under18 1990 NaN NaN 2449 PR total 1990 NaN NaN 2450 PR total 1991 NaN NaN 2451 PR under18 1991 NaN NaN 2452 PR total 1993 NaN NaN merged [ merged [ 'state' ] . isnull ()] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 2448 PR under18 1990 NaN NaN 2449 PR total 1990 NaN NaN 2450 PR total 1991 NaN NaN 2451 PR under18 1991 NaN NaN 2452 PR total 1993 NaN NaN merged . loc [ merged [ 'state' ] . isnull (), 'state/region' ] . unique () array(['PR', 'USA'], dtype=object) # To fix the missing values of PR, USA merged . loc [ merged [ 'state/region' ] == 'PR' , 'state' ] = 'Puerto Rico' merged . loc [ merged [ 'state/region' ] == 'USA' , 'state' ] = 'United State' merged . isnull () . any () state/region False ages False year False population True state False dtype: bool #Now lets merged the result wiht the area dataset final = pd . merge ( merged , areas , on = 'state' , how = 'left' ) final . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 final . isnull () . any () state/region False ages False year False population True state False area (sq. mi) True dtype: bool # Lets check the regions which areas is null final [ 'state' ][ final [ 'area (sq. mi)' ] . isnull ()] . unique () array(['United State'], dtype=object) # No area value for USA; we can either insert it by suming all the areas or just drop it final . dropna ( inplace = True ) final . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 Now we have all the data we need. To answer the question of interest, let\u2019s first select the portion of the data corresponding with the year 2000, and the total population. We\u2019ll use the query() function to do this quickly (this requires the numexpr package to be installed; data2000 = final . query ( \"year==2000 & ages=='total'\" ) data2000 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 28 AL total 2000 4452173.0 Alabama 52423.0 68 AK total 2000 627963.0 Alaska 656425.0 124 AZ total 2000 5160586.0 Arizona 114006.0 162 AR total 2000 2678588.0 Arkansas 53182.0 220 CA total 2000 33987977.0 California 163707.0 Now let\u2019s compute the population density and display it in order. We\u2019ll start by rein\u2010 dexing our data on the state, and then compute the result: data2000 . set_index ( 'state' , inplace = True ) density = data2000 [ 'population' ] / data2000 [ 'area (sq. mi)' ] density . sort_values ( ascending = False , inplace = True ) density . head () state District of Columbia 8412.441176 Puerto Rico 1084.098151 New Jersey 966.592639 Rhode Island 679.785113 Connecticut 615.399892 dtype: float64 density . tail () state South Dakota 9.800755 North Dakota 9.080434 Montana 6.146192 Wyoming 5.053262 Alaska 0.956641 dtype: float64","title":"Example  US States Data (Merge and Join)"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/#example-us-states-data","text":"Example: US States Data","title":"Example: US States Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/#example-us-states-data_1","text":"Merge and join operations come up most often when one is combining data from dif\u2010 ferent sources. Here we will consider an example of some data about US states and their populations. import numpy as np import pandas as pd pop = pd . read_csv ( \"../data/state-population.csv\" ) areas = pd . read_csv ( \"../data/state-areas.csv\" ) abbrevs = pd . read_csv ( \"../data/state-abbrevs.csv\" ) pop . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population 0 AL under18 2012 1117489.0 1 AL total 2012 4817528.0 2 AL under18 2010 1130966.0 3 AL total 2010 4785570.0 4 AL under18 2011 1125763.0 areas . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state area (sq. mi) 0 Alabama 52423 1 Alaska 656425 2 Arizona 114006 3 Arkansas 53182 4 California 163707 abbrevs . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state abbreviation 0 Alabama AL 1 Alaska AK 2 Arizona AZ 3 Arkansas AR 4 California CA Given this information, say we want to compute a relatively straightforward result:rank US states and territories by their 2010 population density merged = pd . merge ( pop , abbrevs , how = 'outer' , left_on = 'state/region' , right_on = 'abbreviation' ) merged .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state abbreviation 0 AL under18 2012 1117489.0 Alabama AL 1 AL total 2012 4817528.0 Alabama AL 2 AL under18 2010 1130966.0 Alabama AL 3 AL total 2010 4785570.0 Alabama AL 4 AL under18 2011 1125763.0 Alabama AL ... ... ... ... ... ... ... 2539 USA total 2010 309326295.0 NaN NaN 2540 USA under18 2011 73902222.0 NaN NaN 2541 USA total 2011 311582564.0 NaN NaN 2542 USA under18 2012 73708179.0 NaN NaN 2543 USA total 2012 313873685.0 NaN NaN 2544 rows \u00d7 6 columns merged = merged . drop ( 'abbreviation' , 1 ) /tmp/ipykernel_88168/2168094788.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only. merged=merged.drop('abbreviation',1) merged . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 0 AL under18 2012 1117489.0 Alabama 1 AL total 2012 4817528.0 Alabama 2 AL under18 2010 1130966.0 Alabama 3 AL total 2010 4785570.0 Alabama 4 AL under18 2011 1125763.0 Alabama # CHeck if there is any mismatch, merged . isnull () . any () state/region False ages False year False population True state True dtype: bool ## Some of the population and state info is null, lets check which one merged [ merged [ 'population' ] . isnull ()] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 2448 PR under18 1990 NaN NaN 2449 PR total 1990 NaN NaN 2450 PR total 1991 NaN NaN 2451 PR under18 1991 NaN NaN 2452 PR total 1993 NaN NaN merged [ merged [ 'state' ] . isnull ()] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 2448 PR under18 1990 NaN NaN 2449 PR total 1990 NaN NaN 2450 PR total 1991 NaN NaN 2451 PR under18 1991 NaN NaN 2452 PR total 1993 NaN NaN merged . loc [ merged [ 'state' ] . isnull (), 'state/region' ] . unique () array(['PR', 'USA'], dtype=object) # To fix the missing values of PR, USA merged . loc [ merged [ 'state/region' ] == 'PR' , 'state' ] = 'Puerto Rico' merged . loc [ merged [ 'state/region' ] == 'USA' , 'state' ] = 'United State' merged . isnull () . any () state/region False ages False year False population True state False dtype: bool #Now lets merged the result wiht the area dataset final = pd . merge ( merged , areas , on = 'state' , how = 'left' ) final . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 final . isnull () . any () state/region False ages False year False population True state False area (sq. mi) True dtype: bool # Lets check the regions which areas is null final [ 'state' ][ final [ 'area (sq. mi)' ] . isnull ()] . unique () array(['United State'], dtype=object) # No area value for USA; we can either insert it by suming all the areas or just drop it final . dropna ( inplace = True ) final . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 Now we have all the data we need. To answer the question of interest, let\u2019s first select the portion of the data corresponding with the year 2000, and the total population. We\u2019ll use the query() function to do this quickly (this requires the numexpr package to be installed; data2000 = final . query ( \"year==2000 & ages=='total'\" ) data2000 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 28 AL total 2000 4452173.0 Alabama 52423.0 68 AK total 2000 627963.0 Alaska 656425.0 124 AZ total 2000 5160586.0 Arizona 114006.0 162 AR total 2000 2678588.0 Arkansas 53182.0 220 CA total 2000 33987977.0 California 163707.0 Now let\u2019s compute the population density and display it in order. We\u2019ll start by rein\u2010 dexing our data on the state, and then compute the result: data2000 . set_index ( 'state' , inplace = True ) density = data2000 [ 'population' ] / data2000 [ 'area (sq. mi)' ] density . sort_values ( ascending = False , inplace = True ) density . head () state District of Columbia 8412.441176 Puerto Rico 1084.098151 New Jersey 966.592639 Rhode Island 679.785113 Connecticut 615.399892 dtype: float64 density . tail () state South Dakota 9.800755 North Dakota 9.080434 Montana 6.146192 Wyoming 5.053262 Alaska 0.956641 dtype: float64","title":"Example: US States Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/","text":"================ by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 Example: Recipe Database \u00b6 Example: Recipe Database Unfortunately the dataset is not present to do all the other operations :( import numpy as np import pandas as pd Example: Recipe Database \u00b6 These vectorized string operations become most useful in the process of cleaning up messy, real-world data. Here I\u2019ll walk through an example of that, using an open recipe database compiled from various sources on the Web. Our goal will be to parse the recipe data into ingredient lists, so we can quickly find a recipe based on some ingredients we have on hand try : recipes = pd . read_json ( '../data/recipeitems-latest.json' ) except ValueError as e : print ( \"Value Error: \" , e ) Value Error: Expected object or value with open ( '../data/recipeitems-latest.json' ) as f : line = f . readline () pd . read_json ( line ) . shape ValueError: Expected object or value Unfortunately the dataset is not present to do all the other operations :( \u00b6","title":"Example Recipe Database"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/#example-recipe-database","text":"Example: Recipe Database Unfortunately the dataset is not present to do all the other operations :( import numpy as np import pandas as pd","title":"Example: Recipe Database"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/#example-recipe-database_1","text":"These vectorized string operations become most useful in the process of cleaning up messy, real-world data. Here I\u2019ll walk through an example of that, using an open recipe database compiled from various sources on the Web. Our goal will be to parse the recipe data into ingredient lists, so we can quickly find a recipe based on some ingredients we have on hand try : recipes = pd . read_json ( '../data/recipeitems-latest.json' ) except ValueError as e : print ( \"Value Error: \" , e ) Value Error: Expected object or value with open ( '../data/recipeitems-latest.json' ) as f : line = f . readline () pd . read_json ( line ) . shape ValueError: Expected object or value","title":"Example: Recipe Database"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/#unfortunately-the-dataset-is-not-present-to-do-all-the-other-operations","text":"","title":"Unfortunately the dataset is not present to do all the other operations :("},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 01 - General Matplotlib Tips \u00b6 General Matplotlib Tips Importing matplotlib Setting Styles show() or No show()? How to Display Your Plots Saving Figures to File Two Interfaces for the Price of One MATLAB-style interface Object-oriented interface Mtplotlib is a multiplatform data visualization library built on NumPy arrays, and dsigned to work with the broader SciPy stack. One of Matplotlib\u2019s most important features is its ability to play well with many operating systems and graphics backends. Matplotlib supports dozens of backends and output types, which means you can count on it to work regardless of which operating system you are using or which output format you wish. This cross-platform, everything-to-everyone approach has been one of the great strengths of Matplotlib. It has led to a large userbase, which in turn has led to an active developer base and Matplotlib\u2019s powerful tools and ubiquity within the scientific Python world. General Matplotlib Tips \u00b6 Before we dive into the details of creating visualizations with Matplotlib, there are a few useful things you should know about using the package. Importing matplotlib \u00b6 Just as we use the np shorthand for NumPy and the pd shorthand for Pandas, we will use some standard shorthands for Matplotlib imports: import matplotlib as mpl import matplotlib.pyplot as plt Setting Styles \u00b6 We will use the plt.style directive to choose appropriate aesthetic styles for our fig\u2010 ures. Here we will set the classic style, which ensures that the plots we create use the classic Matplotlib style: plt . style . use ( 'classic' ) show() or No show()? How to Display Your Plots \u00b6 A visualization you can\u2019t see won\u2019t be of much use, but just how you view your Mat\u2010 plotlib plots depends on the context. The best use of Matplotlib differs depending on how you are using it; roughly, the three applicable contexts are using Matplotlib in a script, in an IPython terminal, or in an IPython notebook. #### Plotting from a script If you are using Matplotlib from within a script, the function plt.show() is your friend. plt.show() starts an event loop, looks for all currently active figure objects, and opens one or more interactive windows that display your figure or figures import numpy as np x = np . linspace ( 0 , 10 , 100 ) plt . plot ( x , np . cos ( x )) plt . plot ( x , np . sin ( x )) x = np . linspace ( 0 , 10 , 100 ) plt . plot ( x , np . cos ( x )) plt . plot ( x , np . sin ( x )) plt . show () You can then run this script from the command-line prompt, which will result in a window opening with your figure displayed: $ python myplot.py The plt.show() command does a lot under the hood, as it must interact with your system\u2019s interactive graphical backend. The details of this operation can vary greatly from system to system and even installation to installation, but Matplotlib does its best to hide all these details from you. \u2022 %matplotlib notebook will lead to interactive plots embedded within the notebook \u2022 %matplotlib inline will lead to static images of your plot embedded in the notebook fig = plt . figure () plt . plot ( x , np . sin ( x ), '--' ) plt . plot ( x , np . cos ( x ), '-' ); Saving Figures to File \u00b6 One nice feature of Matplotlib is the ability to save figures in a wide variety of for\u2010 mats. You can save a figure using the savefig() command. For example, to save the previous figure as a PNG file, you can run this: fig . savefig ( 'cos_sinplots.png' ) ! ls - lh total 168K -rw-rw-r-- 1 qalmaqihir qalmaqihir 128K \u0441\u0435\u043d 5 09:09 01_Intro.ipynb -rw-rw-r-- 1 qalmaqihir qalmaqihir 40K \u0441\u0435\u043d 5 09:10 cos_sinplots.png #lets display the image of our plots back from IPython.display import Image Image ( 'cos_sinplots.png' ) In savefig(), the file format is inferred from the extension of the given filename. Depending on what backends you have installed, many different file formats are available. You can find the list of supported file types for your system by using the following method of the figure canvas object: fig . canvas . get_supported_filetypes () {'eps': 'Encapsulated Postscript', 'jpg': 'Joint Photographic Experts Group', 'jpeg': 'Joint Photographic Experts Group', 'pdf': 'Portable Document Format', 'pgf': 'PGF code for LaTeX', 'png': 'Portable Network Graphics', 'ps': 'Postscript', 'raw': 'Raw RGBA bitmap', 'rgba': 'Raw RGBA bitmap', 'svg': 'Scalable Vector Graphics', 'svgz': 'Scalable Vector Graphics', 'tif': 'Tagged Image File Format', 'tiff': 'Tagged Image File Format'} Two Interfaces for the Price of One \u00b6 A potentially confusing feature of Matplotlib is its dual interfaces: a convenient MATLAB-style state-based interface, and a more powerful object-oriented interface. We\u2019ll quickly highlight the differences between the two here. MATLAB-style interface \u00b6 Matplotlib was originally written as a Python alternative for MATLAB users, and much of its syntax reflects that fact. The MATLAB-style tools are contained in the pyplot (plt) interface. plt . figure () # create a plot figure #create the first of two panels and set current axis plt . subplot ( 2 , 1 , 1 ) plt . plot ( x , np . sin ( x )) #create the 2nd of two panels and set current axis plt . subplot ( 2 , 1 , 2 ) plt . plot ( x , np . cos ( x )) plt . gcf () <Figure size 640x480 with 0 Axes> <Figure size 640x480 with 0 Axes> plt . gca () <AxesSubplot:> It\u2019s important to note that this interface is stateful: it keeps track of the \u201ccurrent\u201d figure and axes, which are where all plt commands are applied. You can get a reference to these using the plt.gcf() (get current figure) and plt.gca() (get current axes) routines. While this stateful interface is fast and convenient for simple plots, it is easy to run into problems. For example, once the second panel is created, how can we go back and add something to the first? This is possible within the MATLAB-style interface, but a bit clunky. Fortunately, there is a better way. Object-oriented interface \u00b6 The object-oriented interface is available for these more complicated situations, and for when you want more control over your figure. Rather than depending on some notion of an \u201cactive\u201d figure or axes, in the object-oriented interface the plotting func\u2010 tions are methods of explicit Figure and Axes objects. To re-create the previous plot using this style of plotting, you might do the following # first create a grid of plots # ax will be an array of two Axes Objects fig , ax = plt . subplots ( 2 ) #call the plot() method on the appropriate object ax [ 0 ] . plot ( x , np . cos ( x )) ax [ 0 ] . plot ( x , np . sin ( x )) ax [ 1 ] . plot ( x , np . cos ( x )) For more simple plots, the choice of which style to use is largely a matter of prefer\u2010 ence, but the object-oriented approach can become a necessity as plots become more complicated.","title":"01general Matplotlib tips"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#01-general-matplotlib-tips","text":"General Matplotlib Tips Importing matplotlib Setting Styles show() or No show()? How to Display Your Plots Saving Figures to File Two Interfaces for the Price of One MATLAB-style interface Object-oriented interface Mtplotlib is a multiplatform data visualization library built on NumPy arrays, and dsigned to work with the broader SciPy stack. One of Matplotlib\u2019s most important features is its ability to play well with many operating systems and graphics backends. Matplotlib supports dozens of backends and output types, which means you can count on it to work regardless of which operating system you are using or which output format you wish. This cross-platform, everything-to-everyone approach has been one of the great strengths of Matplotlib. It has led to a large userbase, which in turn has led to an active developer base and Matplotlib\u2019s powerful tools and ubiquity within the scientific Python world.","title":"01 - General Matplotlib Tips"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#general-matplotlib-tips","text":"Before we dive into the details of creating visualizations with Matplotlib, there are a few useful things you should know about using the package.","title":"General Matplotlib Tips"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#importing-matplotlib","text":"Just as we use the np shorthand for NumPy and the pd shorthand for Pandas, we will use some standard shorthands for Matplotlib imports: import matplotlib as mpl import matplotlib.pyplot as plt","title":"Importing matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#setting-styles","text":"We will use the plt.style directive to choose appropriate aesthetic styles for our fig\u2010 ures. Here we will set the classic style, which ensures that the plots we create use the classic Matplotlib style: plt . style . use ( 'classic' )","title":"Setting Styles"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#show-or-no-show-how-to-display-your-plots","text":"A visualization you can\u2019t see won\u2019t be of much use, but just how you view your Mat\u2010 plotlib plots depends on the context. The best use of Matplotlib differs depending on how you are using it; roughly, the three applicable contexts are using Matplotlib in a script, in an IPython terminal, or in an IPython notebook. #### Plotting from a script If you are using Matplotlib from within a script, the function plt.show() is your friend. plt.show() starts an event loop, looks for all currently active figure objects, and opens one or more interactive windows that display your figure or figures import numpy as np x = np . linspace ( 0 , 10 , 100 ) plt . plot ( x , np . cos ( x )) plt . plot ( x , np . sin ( x )) x = np . linspace ( 0 , 10 , 100 ) plt . plot ( x , np . cos ( x )) plt . plot ( x , np . sin ( x )) plt . show () You can then run this script from the command-line prompt, which will result in a window opening with your figure displayed: $ python myplot.py The plt.show() command does a lot under the hood, as it must interact with your system\u2019s interactive graphical backend. The details of this operation can vary greatly from system to system and even installation to installation, but Matplotlib does its best to hide all these details from you. \u2022 %matplotlib notebook will lead to interactive plots embedded within the notebook \u2022 %matplotlib inline will lead to static images of your plot embedded in the notebook fig = plt . figure () plt . plot ( x , np . sin ( x ), '--' ) plt . plot ( x , np . cos ( x ), '-' );","title":"show() or No show()? How to Display Your Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#saving-figures-to-file","text":"One nice feature of Matplotlib is the ability to save figures in a wide variety of for\u2010 mats. You can save a figure using the savefig() command. For example, to save the previous figure as a PNG file, you can run this: fig . savefig ( 'cos_sinplots.png' ) ! ls - lh total 168K -rw-rw-r-- 1 qalmaqihir qalmaqihir 128K \u0441\u0435\u043d 5 09:09 01_Intro.ipynb -rw-rw-r-- 1 qalmaqihir qalmaqihir 40K \u0441\u0435\u043d 5 09:10 cos_sinplots.png #lets display the image of our plots back from IPython.display import Image Image ( 'cos_sinplots.png' ) In savefig(), the file format is inferred from the extension of the given filename. Depending on what backends you have installed, many different file formats are available. You can find the list of supported file types for your system by using the following method of the figure canvas object: fig . canvas . get_supported_filetypes () {'eps': 'Encapsulated Postscript', 'jpg': 'Joint Photographic Experts Group', 'jpeg': 'Joint Photographic Experts Group', 'pdf': 'Portable Document Format', 'pgf': 'PGF code for LaTeX', 'png': 'Portable Network Graphics', 'ps': 'Postscript', 'raw': 'Raw RGBA bitmap', 'rgba': 'Raw RGBA bitmap', 'svg': 'Scalable Vector Graphics', 'svgz': 'Scalable Vector Graphics', 'tif': 'Tagged Image File Format', 'tiff': 'Tagged Image File Format'}","title":"Saving Figures to File"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#two-interfaces-for-the-price-of-one","text":"A potentially confusing feature of Matplotlib is its dual interfaces: a convenient MATLAB-style state-based interface, and a more powerful object-oriented interface. We\u2019ll quickly highlight the differences between the two here.","title":"Two Interfaces for the Price of One"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#matlab-style-interface","text":"Matplotlib was originally written as a Python alternative for MATLAB users, and much of its syntax reflects that fact. The MATLAB-style tools are contained in the pyplot (plt) interface. plt . figure () # create a plot figure #create the first of two panels and set current axis plt . subplot ( 2 , 1 , 1 ) plt . plot ( x , np . sin ( x )) #create the 2nd of two panels and set current axis plt . subplot ( 2 , 1 , 2 ) plt . plot ( x , np . cos ( x )) plt . gcf () <Figure size 640x480 with 0 Axes> <Figure size 640x480 with 0 Axes> plt . gca () <AxesSubplot:> It\u2019s important to note that this interface is stateful: it keeps track of the \u201ccurrent\u201d figure and axes, which are where all plt commands are applied. You can get a reference to these using the plt.gcf() (get current figure) and plt.gca() (get current axes) routines. While this stateful interface is fast and convenient for simple plots, it is easy to run into problems. For example, once the second panel is created, how can we go back and add something to the first? This is possible within the MATLAB-style interface, but a bit clunky. Fortunately, there is a better way.","title":"MATLAB-style interface"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#object-oriented-interface","text":"The object-oriented interface is available for these more complicated situations, and for when you want more control over your figure. Rather than depending on some notion of an \u201cactive\u201d figure or axes, in the object-oriented interface the plotting func\u2010 tions are methods of explicit Figure and Axes objects. To re-create the previous plot using this style of plotting, you might do the following # first create a grid of plots # ax will be an array of two Axes Objects fig , ax = plt . subplots ( 2 ) #call the plot() method on the appropriate object ax [ 0 ] . plot ( x , np . cos ( x )) ax [ 0 ] . plot ( x , np . sin ( x )) ax [ 1 ] . plot ( x , np . cos ( x )) For more simple plots, the choice of which style to use is largely a matter of prefer\u2010 ence, but the object-oriented approach can become a necessity as plots become more complicated.","title":"Object-oriented interface"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 02 - Simple Line Plots \u00b6 Simple Line Plots Adjusting the Plot: Line Colors and Styles Adjusting the Plot: Axes Limits Labeling Plots Simple Line Plots \u00b6 Perhaps the simplest of all plots is the visualization of a single function y = f x . Here we will take a first look at creating a simple plot of this type. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( \"seaborn-whitegrid\" ) import numpy as np # all Matplotlib plots can be started by creating a figure and an axes # their simplist form fig = plt . figure () ax = plt . axes () In Matplotlib, the figure (an instance of the class plt.Figure) can be thought of as a single container that contains all the objects representing axes, graphics, text, and labels. The axes (an instance of the class plt.Axes) is what we see above: a bounding box with ticks and labels, which will eventually contain the plot elements that make up our visualization. #Once we have created an axes, we can use the ax.plot function to plot some data. fig = plt . figure () ax = plt . axes () x = np . linspace ( 0 , 10 , 100 ) ax . plot ( x , np . sin ( x )) Alternatively, we can use the pylab interface and let the figure and axes be created for us in the background fig = plt . figure () ax = plt . axes () plt . plot ( x , np . sin ( x )) If we want to create a single figure with multiple lines, we can simply call the plot function multiple times fig = plt . figure () ax = plt . axes () plt . plot ( x , np . sin ( x )) plt . plot ( x , np . cos ( x )) # plt.plot(x,np.tan(x)) plt . plot ( x , np . log ( x )) /tmp/ipykernel_58842/3301612854.py:6: RuntimeWarning: divide by zero encountered in log plt.plot(x,np.log(x)) Adjusting the Plot: Line Colors and Styles \u00b6 The first adjustment you might wish to make to a plot is to control the line colors and styles. The plt.plot() function takes additional arguments that can be used to spec\u2010 ify these. To adjust the color, you can use the color keyword, which accepts a string argument representing virtually any imaginable color. The color can be specified in a variety of ways plt . plot ( x , np . sin ( x - 0 ), color = 'blue' ) # specify color by name plt . plot ( x , np . sin ( x - 1 ), color = 'g' ) # short color code (rgbcmyk) plt . plot ( x , np . sin ( x - 2 ), color = '0.75' ) # Grayscale between 0 and 1 plt . plot ( x , np . sin ( x - 3 ), color = '#FFDD44' ) # Hex code (RRGGBB from 00 to FF) plt . plot ( x , np . sin ( x - 4 ), color = ( 1.0 , 0.2 , 0.3 )) # RGB tuple, values 0 and 1 plt . plot ( x , np . sin ( x - 5 ), color = 'chartreuse' ); # all HTML color names supported *If no color is specified, Matplotlib will automatically cycle through a set of default colors for multiple lines. similarly, you can adjust the line style using linestyle keyword* plt . plot ( x , x + 0 , linestyle = 'solid' ) plt . plot ( x , x + 1 , linestyle = 'dashed' ) plt . plot ( x , x + 2 , linestyle = 'dashdot' ) plt . plot ( x , x + 3 , linestyle = 'dotted' ); # For short, you can use the following codes: plt . plot ( x , x + 4 , linestyle = '-' ) # solid plt . plot ( x , x + 5 , linestyle = '--' ) # dashed plt . plot ( x , x + 6 , linestyle = '-.' ) # dashdot plt . plot ( x , x + 7 , linestyle = ':' ); # dotted If you would like to be extremely terse, these linestyle and color codes can be com\u2010 bined into a single nonkeyword argument to the plt.plot() function Adjusting the Plot: Axes Limits \u00b6 Matplotlib does a decent job of choosing default axes limits for your plot, but some\u2010 times it\u2019s nice to have finer control. The most basic way to adjust axis limits is to use the plt.xlim() and plt.ylim() methods plt . plot ( x , np . sin ( x )) plt . xlim ( - 1 , 11 ) plt . ylim ( - 1.5 , 1.5 ) (-1.5, 1.5) if for some reason you\u2019d like either axis to be displayed in reverse, you can simply reverse the order of the arguments plt . plot ( x , np . sin ( x )) plt . xlim ( 10 , 0 ) plt . ylim ( 1.2 , - 1.2 ) (1.2, -1.2) A useful related method is plt.axis() (note here the potential confusion between axes with an e, and axis with an i). The plt.axis() method allows you to set the x and y limits with a single call, by passing a list that specifies [xmin, xmax, ymin,ymax] plt . plot ( x , np . sin ( x )) plt . axis ([ - 1 , 11 , - 1.5 , 1.5 ]); The plt.axis() method goes even beyond this, allowing you to do things like auto\u2010 matically tighten the bounds around the current plot plt . plot ( x , np . sin ( x )) plt . axis ( 'tight' ); It allows even higher-level specifications, such as ensuring an equal aspect ratio so that on your screen, one unit in x is equal to one unit in y plt . plot ( x , np . sin ( x )) plt . axis ( 'equal' ); Labeling Plots \u00b6 As the last piece of this section, we\u2019ll briefly look at the labeling of plots: titles, axis labels, and simple legends. Titles and axis labels are the simplest such labels\u2014there are methods that can be used to quickly set them plt . plot ( x , np . sin ( x )) plt . title ( 'A sine curve' ) plt . xlabel ( 'x' ) plt . ylabel ( 'sin(x)' ) Text(0, 0.5, 'sin(x)') When multiple lines are being shown within a single axes, it can be useful to create a plot legend that labels each line type. Again, Matplotlib has a built-in way of quickly creating such a legend. It is done via the (you guessed it) plt.legend() method. Though there are several valid ways of using this, I find it easiest to specify the label of each line using the label keyword of the plot function plt . plot ( x , np . sin ( x ), '-g' , label = 'sin(x)' ) plt . plot ( x , np . cos ( x ), ':b' , label = 'cos(x)' ) plt . axis ( 'equal' ) plt . legend ();","title":"02simple lineplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#02-simple-line-plots","text":"Simple Line Plots Adjusting the Plot: Line Colors and Styles Adjusting the Plot: Axes Limits Labeling Plots","title":"02 - Simple Line Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#simple-line-plots","text":"Perhaps the simplest of all plots is the visualization of a single function y = f x . Here we will take a first look at creating a simple plot of this type. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( \"seaborn-whitegrid\" ) import numpy as np # all Matplotlib plots can be started by creating a figure and an axes # their simplist form fig = plt . figure () ax = plt . axes () In Matplotlib, the figure (an instance of the class plt.Figure) can be thought of as a single container that contains all the objects representing axes, graphics, text, and labels. The axes (an instance of the class plt.Axes) is what we see above: a bounding box with ticks and labels, which will eventually contain the plot elements that make up our visualization. #Once we have created an axes, we can use the ax.plot function to plot some data. fig = plt . figure () ax = plt . axes () x = np . linspace ( 0 , 10 , 100 ) ax . plot ( x , np . sin ( x )) Alternatively, we can use the pylab interface and let the figure and axes be created for us in the background fig = plt . figure () ax = plt . axes () plt . plot ( x , np . sin ( x )) If we want to create a single figure with multiple lines, we can simply call the plot function multiple times fig = plt . figure () ax = plt . axes () plt . plot ( x , np . sin ( x )) plt . plot ( x , np . cos ( x )) # plt.plot(x,np.tan(x)) plt . plot ( x , np . log ( x )) /tmp/ipykernel_58842/3301612854.py:6: RuntimeWarning: divide by zero encountered in log plt.plot(x,np.log(x))","title":"Simple Line Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#adjusting-the-plot-line-colors-and-styles","text":"The first adjustment you might wish to make to a plot is to control the line colors and styles. The plt.plot() function takes additional arguments that can be used to spec\u2010 ify these. To adjust the color, you can use the color keyword, which accepts a string argument representing virtually any imaginable color. The color can be specified in a variety of ways plt . plot ( x , np . sin ( x - 0 ), color = 'blue' ) # specify color by name plt . plot ( x , np . sin ( x - 1 ), color = 'g' ) # short color code (rgbcmyk) plt . plot ( x , np . sin ( x - 2 ), color = '0.75' ) # Grayscale between 0 and 1 plt . plot ( x , np . sin ( x - 3 ), color = '#FFDD44' ) # Hex code (RRGGBB from 00 to FF) plt . plot ( x , np . sin ( x - 4 ), color = ( 1.0 , 0.2 , 0.3 )) # RGB tuple, values 0 and 1 plt . plot ( x , np . sin ( x - 5 ), color = 'chartreuse' ); # all HTML color names supported *If no color is specified, Matplotlib will automatically cycle through a set of default colors for multiple lines. similarly, you can adjust the line style using linestyle keyword* plt . plot ( x , x + 0 , linestyle = 'solid' ) plt . plot ( x , x + 1 , linestyle = 'dashed' ) plt . plot ( x , x + 2 , linestyle = 'dashdot' ) plt . plot ( x , x + 3 , linestyle = 'dotted' ); # For short, you can use the following codes: plt . plot ( x , x + 4 , linestyle = '-' ) # solid plt . plot ( x , x + 5 , linestyle = '--' ) # dashed plt . plot ( x , x + 6 , linestyle = '-.' ) # dashdot plt . plot ( x , x + 7 , linestyle = ':' ); # dotted If you would like to be extremely terse, these linestyle and color codes can be com\u2010 bined into a single nonkeyword argument to the plt.plot() function","title":"Adjusting the Plot: Line Colors and Styles"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#adjusting-the-plot-axes-limits","text":"Matplotlib does a decent job of choosing default axes limits for your plot, but some\u2010 times it\u2019s nice to have finer control. The most basic way to adjust axis limits is to use the plt.xlim() and plt.ylim() methods plt . plot ( x , np . sin ( x )) plt . xlim ( - 1 , 11 ) plt . ylim ( - 1.5 , 1.5 ) (-1.5, 1.5) if for some reason you\u2019d like either axis to be displayed in reverse, you can simply reverse the order of the arguments plt . plot ( x , np . sin ( x )) plt . xlim ( 10 , 0 ) plt . ylim ( 1.2 , - 1.2 ) (1.2, -1.2) A useful related method is plt.axis() (note here the potential confusion between axes with an e, and axis with an i). The plt.axis() method allows you to set the x and y limits with a single call, by passing a list that specifies [xmin, xmax, ymin,ymax] plt . plot ( x , np . sin ( x )) plt . axis ([ - 1 , 11 , - 1.5 , 1.5 ]); The plt.axis() method goes even beyond this, allowing you to do things like auto\u2010 matically tighten the bounds around the current plot plt . plot ( x , np . sin ( x )) plt . axis ( 'tight' ); It allows even higher-level specifications, such as ensuring an equal aspect ratio so that on your screen, one unit in x is equal to one unit in y plt . plot ( x , np . sin ( x )) plt . axis ( 'equal' );","title":"Adjusting the Plot: Axes Limits"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#labeling-plots","text":"As the last piece of this section, we\u2019ll briefly look at the labeling of plots: titles, axis labels, and simple legends. Titles and axis labels are the simplest such labels\u2014there are methods that can be used to quickly set them plt . plot ( x , np . sin ( x )) plt . title ( 'A sine curve' ) plt . xlabel ( 'x' ) plt . ylabel ( 'sin(x)' ) Text(0, 0.5, 'sin(x)') When multiple lines are being shown within a single axes, it can be useful to create a plot legend that labels each line type. Again, Matplotlib has a built-in way of quickly creating such a legend. It is done via the (you guessed it) plt.legend() method. Though there are several valid ways of using this, I find it easiest to specify the label of each line using the label keyword of the plot function plt . plot ( x , np . sin ( x ), '-g' , label = 'sin(x)' ) plt . plot ( x , np . cos ( x ), ':b' , label = 'cos(x)' ) plt . axis ( 'equal' ) plt . legend ();","title":"Labeling Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 03 - Simple Scatter Plots \u00b6 Simple Scatter Plots Scatter Plots with plt.plot Scatter Plots with plt.scatter Simple Scatter Plots \u00b6 Another commonly used plot type is the simple scatter plot, a close cousin of the line plot. Instead of points being joined by line segments, here the points are represented individually with a dot, circle, or other shape. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np Scatter Plots with plt.plot \u00b6 In the previous section, we looked at plt.plot/ax.plot to produce line plots. It turns out that this same function can produce scatter plots as well x = np . linspace ( 0 , 10 , 30 ) y = np . sin ( x ) plt . plot ( x , y , 'o' , color = 'black' ); # possible markers are rng = np . random . RandomState ( 0 ) for marker in [ 'o' , '.' , ',' , 'x' , '+' , 'v' , '^' , '<' , '>' , 's' , 'd' ]: plt . plot ( rng . rand ( 5 ), rng . rand ( 5 ), marker , label = \"marker=' {0} '\" . format ( marker )) plt . legend ( numpoints = 1 ) plt . xlim ( 0 , 1.8 ); For even more possibilities, these character codes can be used together with line and color codes to plot points along with a line connecting them plt . plot ( x , y , '-*r' ); Additional keyword arguments to plt.plot specify a wide range of properties of the lines and markers plt . plot ( x , y , '-p' , color = 'gray' , markersize = 15 , linewidth = 4 , markerfacecolor = 'white' , markeredgecolor = 'gray' , markeredgewidth = 2 ) plt . ylim ( - 1.2 , 1.2 ) (-1.2, 1.2) Scatter Plots with plt.scatter \u00b6 A second, more powerful method of creating scatter plots is the plt.scatter func\u2010 tion, which can be used very similarly to the plt.plot function plt . scatter ( x , y , marker = 'o' ) <matplotlib.collections.PathCollection at 0x7fe4dcc12040> The primary difference of plt.scatter from plt.plot is that it can be used to create scatter plots where the properties of each individual point (size, face color, edge color, etc.) can be individually controlled or mapped to data. rng = np . random . RandomState ( 0 ) x = rng . randn ( 100 ) y = rng . randn ( 100 ) colors = rng . rand ( 100 ) sizes = 1000 * rng . rand ( 100 ) plt . scatter ( x , y , c = colors , s = sizes , alpha = 0.3 , cmap = 'viridis' ) plt . colorbar (); # show color scale /tmp/ipykernel_61416/429789784.py:8: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first. plt.colorbar(); # show color scale Notice that the color argument is automatically mapped to a color scale (shown here by the colorbar() command), and the size argument is given in pixels. In this way, the color and size of points can be used to convey information in the visualization, in order to illustrate multidimensional data. For example, we might use the Iris data from Scikit-Learn, where each sample is one of three types of flowers that has had the size of its petals and sepals carefully meas\u2010 ured from sklearn.datasets import load_iris iris = load_iris () features = iris . data . T plt . scatter ( features [ 0 ], feaatures [ 1 ], alpha = 0.2 , s = 100 * features [ 3 ], c = iris . target , cmap = 'viridis' ) plt . xlabel ( iris . feature_names [ 0 ]) plt . ylabel ( iris . feature_names [ 1 ]);","title":"03simple scatter plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#03-simple-scatter-plots","text":"Simple Scatter Plots Scatter Plots with plt.plot Scatter Plots with plt.scatter","title":"03 - Simple Scatter Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#simple-scatter-plots","text":"Another commonly used plot type is the simple scatter plot, a close cousin of the line plot. Instead of points being joined by line segments, here the points are represented individually with a dot, circle, or other shape. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np","title":"Simple Scatter Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#scatter-plots-with-pltplot","text":"In the previous section, we looked at plt.plot/ax.plot to produce line plots. It turns out that this same function can produce scatter plots as well x = np . linspace ( 0 , 10 , 30 ) y = np . sin ( x ) plt . plot ( x , y , 'o' , color = 'black' ); # possible markers are rng = np . random . RandomState ( 0 ) for marker in [ 'o' , '.' , ',' , 'x' , '+' , 'v' , '^' , '<' , '>' , 's' , 'd' ]: plt . plot ( rng . rand ( 5 ), rng . rand ( 5 ), marker , label = \"marker=' {0} '\" . format ( marker )) plt . legend ( numpoints = 1 ) plt . xlim ( 0 , 1.8 ); For even more possibilities, these character codes can be used together with line and color codes to plot points along with a line connecting them plt . plot ( x , y , '-*r' ); Additional keyword arguments to plt.plot specify a wide range of properties of the lines and markers plt . plot ( x , y , '-p' , color = 'gray' , markersize = 15 , linewidth = 4 , markerfacecolor = 'white' , markeredgecolor = 'gray' , markeredgewidth = 2 ) plt . ylim ( - 1.2 , 1.2 ) (-1.2, 1.2)","title":"Scatter Plots with plt.plot"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#scatter-plots-with-pltscatter","text":"A second, more powerful method of creating scatter plots is the plt.scatter func\u2010 tion, which can be used very similarly to the plt.plot function plt . scatter ( x , y , marker = 'o' ) <matplotlib.collections.PathCollection at 0x7fe4dcc12040> The primary difference of plt.scatter from plt.plot is that it can be used to create scatter plots where the properties of each individual point (size, face color, edge color, etc.) can be individually controlled or mapped to data. rng = np . random . RandomState ( 0 ) x = rng . randn ( 100 ) y = rng . randn ( 100 ) colors = rng . rand ( 100 ) sizes = 1000 * rng . rand ( 100 ) plt . scatter ( x , y , c = colors , s = sizes , alpha = 0.3 , cmap = 'viridis' ) plt . colorbar (); # show color scale /tmp/ipykernel_61416/429789784.py:8: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first. plt.colorbar(); # show color scale Notice that the color argument is automatically mapped to a color scale (shown here by the colorbar() command), and the size argument is given in pixels. In this way, the color and size of points can be used to convey information in the visualization, in order to illustrate multidimensional data. For example, we might use the Iris data from Scikit-Learn, where each sample is one of three types of flowers that has had the size of its petals and sepals carefully meas\u2010 ured from sklearn.datasets import load_iris iris = load_iris () features = iris . data . T plt . scatter ( features [ 0 ], feaatures [ 1 ], alpha = 0.2 , s = 100 * features [ 3 ], c = iris . target , cmap = 'viridis' ) plt . xlabel ( iris . feature_names [ 0 ]) plt . ylabel ( iris . feature_names [ 1 ]);","title":"Scatter Plots with plt.scatter"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 04 - Basic Errorbars \u00b6 Visualizing Errors Basic Errorbars Visualizing Errors \u00b6 For any scientific measurement, accurate accounting for errors is nearly as important, if not more important, than accurate reporting of the number itself. Basic Errorbars \u00b6 A basic errorbar can be created with a single Matplotlib function call % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np x = np . linspace ( 0 , 10 , 50 ) dy = 0.8 y = np . sin ( x ) + dy * np . random . randn ( 50 ) plt . errorbar ( x , y , yerr = dy , fmt = 'k' ); Here the fmt is a format code controlling the appearance of lines plt . errorbar ( x , y , yerr = dy , fmt = 'o' , color = 'black' , ecolor = 'lightgray' , elinewidth = 3 , capsize = 0 ); In addition to these options, you can also specify horizontal errorbars (xerr), one- sided errorbars, and many other variants. For more information on the options avail\u2010 able, refer to the docstring of plt.errorbar. ## Continuous Errors In some situations it is desirable to show errorbars on continuous quantities. Though Matplotlib does not have a built-in convenience routine for this type of application, it\u2019s relatively easy to combine primitives like plt.plot and plt.fill_between for a useful result. from sklearn.gaussian_process import GaussianProcessRegressor #define the model and draw some dataa model = lambda x : x * np . sin ( x ) xdata = np . array ([ 1 , 3 , 5 , 6 , 8 ]) ydata = model ( xdata ) # computer the Gaussain process fit gr = GaussianProcessRegressor () gr . fit ( xdata [:, np . newaxis ], ydata ) #sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;} GaussianProcessRegressor() In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. GaussianProcessRegressor GaussianProcessRegressor() xfit = np . linspace ( 0 , 10 , 1000 ) yfit = gr . predict ( xfit [:, np . newaxis ])","title":"04visualizing errors"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/#04-basic-errorbars","text":"Visualizing Errors Basic Errorbars","title":"04 - Basic Errorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/#visualizing-errors","text":"For any scientific measurement, accurate accounting for errors is nearly as important, if not more important, than accurate reporting of the number itself.","title":"Visualizing Errors"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/#basic-errorbars","text":"A basic errorbar can be created with a single Matplotlib function call % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np x = np . linspace ( 0 , 10 , 50 ) dy = 0.8 y = np . sin ( x ) + dy * np . random . randn ( 50 ) plt . errorbar ( x , y , yerr = dy , fmt = 'k' ); Here the fmt is a format code controlling the appearance of lines plt . errorbar ( x , y , yerr = dy , fmt = 'o' , color = 'black' , ecolor = 'lightgray' , elinewidth = 3 , capsize = 0 ); In addition to these options, you can also specify horizontal errorbars (xerr), one- sided errorbars, and many other variants. For more information on the options avail\u2010 able, refer to the docstring of plt.errorbar. ## Continuous Errors In some situations it is desirable to show errorbars on continuous quantities. Though Matplotlib does not have a built-in convenience routine for this type of application, it\u2019s relatively easy to combine primitives like plt.plot and plt.fill_between for a useful result. from sklearn.gaussian_process import GaussianProcessRegressor #define the model and draw some dataa model = lambda x : x * np . sin ( x ) xdata = np . array ([ 1 , 3 , 5 , 6 , 8 ]) ydata = model ( xdata ) # computer the Gaussain process fit gr = GaussianProcessRegressor () gr . fit ( xdata [:, np . newaxis ], ydata ) #sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;} GaussianProcessRegressor() In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. GaussianProcessRegressor GaussianProcessRegressor() xfit = np . linspace ( 0 , 10 , 1000 ) yfit = gr . predict ( xfit [:, np . newaxis ])","title":"Basic Errorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 05 - Density and Contour Plots \u00b6 Density and Contour Plots Visualizing a Three-Dimensional Function Density and Contour Plots \u00b6 Sometimes it is useful to display three-dimensional data in two dimensions using contours or color-coded regions. There are three Matplotlib functions that can be helpful for this task: plt.contour for contour plots, plt.contourf for filled contour plots, and plt.imshow for showing images. This section looks at several examples of using these. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np Visualizing a Three-Dimensional Function \u00b6 We\u2019ll start by demonstrating a contour plot using a function z = f x, y , using the fol\u2010 lowing particular choice for f def f ( x , y ): return np . sin ( x ) ** 10 + np . cos ( 10 + y * x ) * np . cos ( x ) A contour plot can be created with the plt.contour function. It takes three argu\u2010 ments: a grid of x values, a grid of y values, and a grid of z values. The x and y values represent positions on the plot, and the z values will be represented by the contour levels. Perhaps the most straightforward way to prepare such data is to use the np.meshgrid function, which builds two-dimensional grids from one-dimensional arrays: x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 40 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) plt . contour ( X , Y , Z , colors = 'black' ) <matplotlib.contour.QuadContourSet at 0x7f5d78321eb0> Notice that by default when a single color is used, negative values are represented by dashed lines, and positive values by solid lines. Alternatively, you can color-code the lines by specifying a colormap with the cmap argument. Here, we\u2019ll also specify that we want more lines to be drawn\u201420 equally spaced intervals within the data range plt . contour ( X , Y , Z , 20 , cmap = 'RdGy' ) <matplotlib.contour.QuadContourSet at 0x7f5d78a0adc0> Here we chose the RdGy (short for Red-Gray) colormap, which is a good choice for centered data. Matplotlib has a wide range of colormaps available. Our plot is looking nicer, but the spaces between the lines may be a bit distracting. We can change this by switching to a filled contour plot using the plt.contourf() function (notice the f at the end), which uses largely the same syntax as plt.contour() .Additionally, we\u2019ll add a plt.colorbar() command, which automatically creates an additional axis with labeled color information for the plot plt . contourf ( X , Y , Z , 20 , cmap = 'RdGy' ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f5d7847d190> The colorbar makes it clear that the black regions are \u201cpeaks,\u201d while the red regions are \u201cvalleys.\u201d One potential issue with this plot is that it is a bit \u201csplotchy.\u201d That is, the color steps are discrete rather than continuous, which is not always what is desired. You could remedy this by setting the number of contours to a very high number, but this results in a rather inefficient plot: Matplotlib must render a new polygon for each step in the level. A better way to handle this is to use the plt.imshow() function, which inter\u2010 prets a two-dimensional grid of data as an image. plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' ) plt . colorbar () plt . axis ( aspect = 'image' ) TypeError: axis() got an unexpected keyword argument 'aspect' There are a few potential gotchas with imshow(), however: \u2022 plt.imshow() doesn\u2019t accept an x and y grid, so you must manually specify the extent [xmin, xmax, ymin, ymax] of the image on the plot. \u2022 plt.imshow() by default follows the standard image array definition where the origin is in the upper left, not in the lower left as in most contour plots. This must be changed when showing gridded data. \u2022 plt.imshow() will automatically adjust the axis aspect ratio to match the input data; you can change this by setting, for example, plt.axis(aspect=\u2018image\u2019) to make x and y units match. # A combined contours plots and image plots contours = plt . contour ( X , Y , Z , 3 , colors = 'black' ) plt . clabel ( contours , inline = True , fontsize = 8 ) plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' , alpha = 0.5 ) plt . colorbar ();","title":"05density and contour plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/#05-density-and-contour-plots","text":"Density and Contour Plots Visualizing a Three-Dimensional Function","title":"05 - Density and Contour Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/#density-and-contour-plots","text":"Sometimes it is useful to display three-dimensional data in two dimensions using contours or color-coded regions. There are three Matplotlib functions that can be helpful for this task: plt.contour for contour plots, plt.contourf for filled contour plots, and plt.imshow for showing images. This section looks at several examples of using these. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np","title":"Density and Contour Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/#visualizing-a-three-dimensional-function","text":"We\u2019ll start by demonstrating a contour plot using a function z = f x, y , using the fol\u2010 lowing particular choice for f def f ( x , y ): return np . sin ( x ) ** 10 + np . cos ( 10 + y * x ) * np . cos ( x ) A contour plot can be created with the plt.contour function. It takes three argu\u2010 ments: a grid of x values, a grid of y values, and a grid of z values. The x and y values represent positions on the plot, and the z values will be represented by the contour levels. Perhaps the most straightforward way to prepare such data is to use the np.meshgrid function, which builds two-dimensional grids from one-dimensional arrays: x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 40 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) plt . contour ( X , Y , Z , colors = 'black' ) <matplotlib.contour.QuadContourSet at 0x7f5d78321eb0> Notice that by default when a single color is used, negative values are represented by dashed lines, and positive values by solid lines. Alternatively, you can color-code the lines by specifying a colormap with the cmap argument. Here, we\u2019ll also specify that we want more lines to be drawn\u201420 equally spaced intervals within the data range plt . contour ( X , Y , Z , 20 , cmap = 'RdGy' ) <matplotlib.contour.QuadContourSet at 0x7f5d78a0adc0> Here we chose the RdGy (short for Red-Gray) colormap, which is a good choice for centered data. Matplotlib has a wide range of colormaps available. Our plot is looking nicer, but the spaces between the lines may be a bit distracting. We can change this by switching to a filled contour plot using the plt.contourf() function (notice the f at the end), which uses largely the same syntax as plt.contour() .Additionally, we\u2019ll add a plt.colorbar() command, which automatically creates an additional axis with labeled color information for the plot plt . contourf ( X , Y , Z , 20 , cmap = 'RdGy' ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f5d7847d190> The colorbar makes it clear that the black regions are \u201cpeaks,\u201d while the red regions are \u201cvalleys.\u201d One potential issue with this plot is that it is a bit \u201csplotchy.\u201d That is, the color steps are discrete rather than continuous, which is not always what is desired. You could remedy this by setting the number of contours to a very high number, but this results in a rather inefficient plot: Matplotlib must render a new polygon for each step in the level. A better way to handle this is to use the plt.imshow() function, which inter\u2010 prets a two-dimensional grid of data as an image. plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' ) plt . colorbar () plt . axis ( aspect = 'image' ) TypeError: axis() got an unexpected keyword argument 'aspect' There are a few potential gotchas with imshow(), however: \u2022 plt.imshow() doesn\u2019t accept an x and y grid, so you must manually specify the extent [xmin, xmax, ymin, ymax] of the image on the plot. \u2022 plt.imshow() by default follows the standard image array definition where the origin is in the upper left, not in the lower left as in most contour plots. This must be changed when showing gridded data. \u2022 plt.imshow() will automatically adjust the axis aspect ratio to match the input data; you can change this by setting, for example, plt.axis(aspect=\u2018image\u2019) to make x and y units match. # A combined contours plots and image plots contours = plt . contour ( X , Y , Z , 3 , colors = 'black' ) plt . clabel ( contours , inline = True , fontsize = 8 ) plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' , alpha = 0.5 ) plt . colorbar ();","title":"Visualizing a Three-Dimensional Function"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 06 - Histograms, Binnings, and Density \u00b6 Histograms, Binnings, and Density Two-Dimensional Histograms and Binnings plt.hexbin: Hexagonal binnings Kernel density estimation Histograms, Binnings, and Density \u00b6 A simple histogram can be a great first step in understanding a dataset. Earlier, we saw a preview of Matplotlib\u2019s histogram function % matplotlib inline import numpy as np import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) data = np . random . randn ( 1000 ) plt . hist ( data ); # A more customized histogram plt . hist ( data , bins = 30 , alpha = 0.5 , histtype = 'stepfilled' , color = 'steelblue' , edgecolor = 'none' ); The plt.hist docstring has more information on other customization options avail\u2010 able. I find this combination of histtype=\u2018stepfilled\u2019 along with some transpar\u2010 ency alpha to be very useful when comparing histograms of several distributions x1 = np . random . normal ( 0 , 0.8 , 1000 ) x2 = np . random . normal ( - 2 , 1 , 1000 ) x3 = np . random . normal ( 2 , 3 , 1000 ) kwargs = dict ( histtype = 'stepfilled' , alpha = 0.3 , bins = 40 ) plt . hist ( x1 , ** kwargs ); plt . hist ( x2 , ** kwargs ); plt . hist ( x3 , ** kwargs ); If you would like to simply compute the histogram (that is, count the number of points in a given bin) and not display it, the np.histogram() function is available: counts , bin_edges = np . histogram ( data , bins = 5 ) counts array([ 17, 292, 555, 132, 4]) Two-Dimensional Histograms and Binnings \u00b6 Just as we create histograms in one dimension by dividing the number line into bins, we can also create histograms in two dimensions by dividing points among two- dimensional bins. We\u2019ll take a brief look at several ways to do this here. We\u2019ll start by defining some data\u2014an x and y array drawn from a multivariate Gaussian distribution: mean = [ 0 , 0 ] con = [[ 1 , 1 ],[ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , con , 10000 ) . T plt.hist2d: Two-dimensional histogram \u00b6 One straightforward way to plot a two-dimensional histogram is to use Matplotlib\u2019s plt.hist2d function plt . hist2d ( x , y , bins = 30 , cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( 'Counts in Bin' ) Just as with plt.hist, plt.hist2d has a number of extra options to fine-tune the plot and the binning, which are nicely outlined in the function docstring. Further, just as plt.hist has a counterpart in np.histogram, plt.hist2d has a counterpart in np.histogram2d, which can be used as follows: counts , xedges , yedges = np . histogram2d ( x , y , bins = 30 ) xedges array([-4.42186998, -4.13921105, -3.85655212, -3.57389318, -3.29123425, -3.00857531, -2.72591638, -2.44325744, -2.16059851, -1.87793957, -1.59528064, -1.31262171, -1.02996277, -0.74730384, -0.4646449 , -0.18198597, 0.10067297, 0.3833319 , 0.66599084, 0.94864977, 1.2313087 , 1.51396764, 1.79662657, 2.07928551, 2.36194444, 2.64460338, 2.92726231, 3.20992125, 3.49258018, 3.77523911, 4.05789805]) plt.hexbin: Hexagonal binnings \u00b6 The two-dimensional histogram creates a tessellation of squares across the axes. Another natural shape for such a tessellation is the regular hexagon. For this purpose, Matplotlib provides the plt.hexbin routine, which represents a two-dimensional dataset binned within a grid of hexagons plt . hexbin ( x , y , gridsize = 30 , cmap = 'Blues' ) cb = plt . colorbar ( label = 'Count in Bins' ) Kernel density estimation \u00b6 Another common method of evaluating densities in multiple dimensions is kernel density estimation (KDE). KDE can be thought of as a way to \u201csmear out\u201d the points in space and add up the result to obtain a smooth function. One extremely quick and simple KDE implementation exists in the scipy.stats package. Here is a quick example of using the KDE on this data from scipy.stats import gaussian_kde # fit an arry of size nxn data = np . vstack ([ x , y ]) kde = gaussian_kde ( data ) # evaluate on a regular grid xgrid = np . linspace ( - 3.5 , 3.5 , 40 ) ygrid = np . linspace ( - 6 , 6 , 40 ) Xgrid , Ygrid = np . meshgrid ( xgrid , ygrid ) Z = kde . evaluate ( np . vstack ([ Xgrid . ravel (), Ygrid . ravel ()])) # plot the result as an image plt . imshow ( Z . reshape ( Xgrid . shape ), origin = 'lower' , aspect = 'auto' , extent = [ - 3.5 , 3.5 , - 5 , 5 ], cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( 'Density' ) KDE has a smoothing length that effectively slides the knob between detail and smoothness (one example of the ubiquitous bias\u2013variance trade-off). The literature on choosing an appropriate smoothing length is vast: gaussian_kde uses a rule of thumb to attempt to find a nearly optimal smoothing length for the input data.","title":"06Histograms Binnings and Density"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#06-histograms-binnings-and-density","text":"Histograms, Binnings, and Density Two-Dimensional Histograms and Binnings plt.hexbin: Hexagonal binnings Kernel density estimation","title":"06 - Histograms, Binnings, and Density"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#histograms-binnings-and-density","text":"A simple histogram can be a great first step in understanding a dataset. Earlier, we saw a preview of Matplotlib\u2019s histogram function % matplotlib inline import numpy as np import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) data = np . random . randn ( 1000 ) plt . hist ( data ); # A more customized histogram plt . hist ( data , bins = 30 , alpha = 0.5 , histtype = 'stepfilled' , color = 'steelblue' , edgecolor = 'none' ); The plt.hist docstring has more information on other customization options avail\u2010 able. I find this combination of histtype=\u2018stepfilled\u2019 along with some transpar\u2010 ency alpha to be very useful when comparing histograms of several distributions x1 = np . random . normal ( 0 , 0.8 , 1000 ) x2 = np . random . normal ( - 2 , 1 , 1000 ) x3 = np . random . normal ( 2 , 3 , 1000 ) kwargs = dict ( histtype = 'stepfilled' , alpha = 0.3 , bins = 40 ) plt . hist ( x1 , ** kwargs ); plt . hist ( x2 , ** kwargs ); plt . hist ( x3 , ** kwargs ); If you would like to simply compute the histogram (that is, count the number of points in a given bin) and not display it, the np.histogram() function is available: counts , bin_edges = np . histogram ( data , bins = 5 ) counts array([ 17, 292, 555, 132, 4])","title":"Histograms, Binnings, and Density"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#two-dimensional-histograms-and-binnings","text":"Just as we create histograms in one dimension by dividing the number line into bins, we can also create histograms in two dimensions by dividing points among two- dimensional bins. We\u2019ll take a brief look at several ways to do this here. We\u2019ll start by defining some data\u2014an x and y array drawn from a multivariate Gaussian distribution: mean = [ 0 , 0 ] con = [[ 1 , 1 ],[ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , con , 10000 ) . T","title":"Two-Dimensional Histograms and Binnings"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#plthist2d-two-dimensional-histogram","text":"One straightforward way to plot a two-dimensional histogram is to use Matplotlib\u2019s plt.hist2d function plt . hist2d ( x , y , bins = 30 , cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( 'Counts in Bin' ) Just as with plt.hist, plt.hist2d has a number of extra options to fine-tune the plot and the binning, which are nicely outlined in the function docstring. Further, just as plt.hist has a counterpart in np.histogram, plt.hist2d has a counterpart in np.histogram2d, which can be used as follows: counts , xedges , yedges = np . histogram2d ( x , y , bins = 30 ) xedges array([-4.42186998, -4.13921105, -3.85655212, -3.57389318, -3.29123425, -3.00857531, -2.72591638, -2.44325744, -2.16059851, -1.87793957, -1.59528064, -1.31262171, -1.02996277, -0.74730384, -0.4646449 , -0.18198597, 0.10067297, 0.3833319 , 0.66599084, 0.94864977, 1.2313087 , 1.51396764, 1.79662657, 2.07928551, 2.36194444, 2.64460338, 2.92726231, 3.20992125, 3.49258018, 3.77523911, 4.05789805])","title":"plt.hist2d: Two-dimensional histogram"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#plthexbin-hexagonal-binnings","text":"The two-dimensional histogram creates a tessellation of squares across the axes. Another natural shape for such a tessellation is the regular hexagon. For this purpose, Matplotlib provides the plt.hexbin routine, which represents a two-dimensional dataset binned within a grid of hexagons plt . hexbin ( x , y , gridsize = 30 , cmap = 'Blues' ) cb = plt . colorbar ( label = 'Count in Bins' )","title":"plt.hexbin: Hexagonal binnings"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#kernel-density-estimation","text":"Another common method of evaluating densities in multiple dimensions is kernel density estimation (KDE). KDE can be thought of as a way to \u201csmear out\u201d the points in space and add up the result to obtain a smooth function. One extremely quick and simple KDE implementation exists in the scipy.stats package. Here is a quick example of using the KDE on this data from scipy.stats import gaussian_kde # fit an arry of size nxn data = np . vstack ([ x , y ]) kde = gaussian_kde ( data ) # evaluate on a regular grid xgrid = np . linspace ( - 3.5 , 3.5 , 40 ) ygrid = np . linspace ( - 6 , 6 , 40 ) Xgrid , Ygrid = np . meshgrid ( xgrid , ygrid ) Z = kde . evaluate ( np . vstack ([ Xgrid . ravel (), Ygrid . ravel ()])) # plot the result as an image plt . imshow ( Z . reshape ( Xgrid . shape ), origin = 'lower' , aspect = 'auto' , extent = [ - 3.5 , 3.5 , - 5 , 5 ], cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( 'Density' ) KDE has a smoothing length that effectively slides the knob between detail and smoothness (one example of the ubiquitous bias\u2013variance trade-off). The literature on choosing an appropriate smoothing length is vast: gaussian_kde uses a rule of thumb to attempt to find a nearly optimal smoothing length for the input data.","title":"Kernel density estimation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 07 - Customizing Plot Legends \u00b6 Customizing Plot Legends Choosing Elements for the Legend Legend for Size of Points Multiple Legends Customizing Plot Legends \u00b6 Plot legends give meaning to a visualization, assigning labels to the various plot ele\u2010 ments. We previously saw how to create a simple legend; here we\u2019ll take a look at cus\u2010 tomizing the placement and aesthetics of the legend in Matplotlib. The simplest legend can be created with the plt.legend() command, which auto\u2010 matically creates a legend for any labeled plot elements import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np x = np . linspace ( 0 , 10 , 10000 ) fig , ax = plt . subplots () ax . plot ( x , np . sin ( x ), '-b' , label = 'Sine' ) ax . plot ( x , np . cos ( x ), '--r' , label = 'Cosine' ) ax . axis ( 'equal' ) leg = ax . legend (); #many wasy we can customize the legend, # we can specify the location and turn off the frame ax . legend ( loc = 'upper left' , frameon = False ) fig #We can use the ncol command to specify the number of columns in the legend ax . legend ( frameon = False , loc = 'lower center' , ncol = 2 ) fig # We can use a rounded box (fancybox) or add a shadow, change the transparency #(alpha value) of the frame, or change the padding around the text ax . legend ( fancybox = True , framealpha = 0.8 , shadow = True , borderpad = 0.5 ) fig Choosing Elements for the Legend \u00b6 As we\u2019ve already seen, the legend includes all labeled elements by default. If this is not what is desired, we can fine-tune which elements and labels appear in the legend by using the objects returned by plot commands. The plt.plot() command is able to create multiple lines at once, and returns a list of created line instances. Passing any of these to plt.legend() will tell it which to identify, along with the labels we\u2019d like to specify y = np . sin ( x [:, np . newaxis ] + np . pi * np . arange ( 0 , 2 , 0.5 )) lines = plt . plot ( x , y ) # line is a list of plt.line2D instance plt . legend ( lines [: 2 ],[ 'first' , 'second' ]) <matplotlib.legend.Legend at 0x7fba9d7a2c10> I generally find in practice that it is clearer to use the first method, applying labels to the plot elements you\u2019d like to show on the legend plt . plot ( x , y [:, 0 ], label = 'first' ) plt . plot ( x , y [:, 1 ], label = 'second' ) plt . plot ( x , y [:, 2 :]) plt . legend ( framealpha = 1 , frameon = True ); Legend for Size of Points \u00b6 Sometimes the legend defaults are not sufficient for the given visualization. For exam\u2010 ple, perhaps you\u2019re using the size of points to mark certain features of the data, and want to create a legend reflecting this. Here is an example where we\u2019ll use the size of points to indicate populations of California cities. We\u2019d like a legend that specifies the scale of the sizes of the points, and we\u2019ll accomplish this by plotting some labeled data with no entries import pandas as pd cities = pd . read_csv ( '../data/california_cities.csv' ) # extract the data we'er interestd in lat , lon = cities [ 'latd' ], cities [ 'longd' ] population , area = cities [ 'population_total' ], cities [ 'area_total_km2' ] # scatter the points, using size and color but no label plt . scatter ( lon , lat , label = None , c = np . log10 ( population ), cmap = 'viridis' , s = area , linewidth = 0 , alpha = 0.5 ) #plt.axis(aspect='equal') plt . xlabel ( 'Longitude' ) plt . ylabel ( 'Latitude' ) plt . colorbar ( label = 'log$_ {10} $(population)' ) plt . clim ( 3 , 7 ) # scatter the points, using size and color but no label plt . scatter ( lon , lat , label = None , c = np . log10 ( population ), cmap = 'viridis' , s = area , linewidth = 0 , alpha = 0.5 ) #plt.axis(aspect='equal') plt . xlabel ( 'Longitude' ) plt . ylabel ( 'Latitude' ) plt . colorbar ( label = 'log$_ {10} $(population)' ) plt . clim ( 3 , 7 ) # here we created a legend: # we'll plot empty lists with the desired size and label for area in [ 100 , 300 , 500 ]: plt . scatter ([],[], c = 'k' , alpha = 0.3 , s = area , label = str ( area ) + ' km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , title = 'City Area' ) plt . title ( 'California cities: Area and Population' ) Text(0.5, 1.0, 'California cities: Area and Population') Multiple Legends \u00b6 Sometimes when designing a plot you\u2019d like to add multiple legends to the same axes. Unfortunately, Matplotlib does not make this easy: via the standard legend interface, it is only possible to create a single legend for the entire plot. If you try to create a second legend using plt.legend() or ax.legend(), it will simply override the first one. We can work around this by creating a new legend artist from scratch, and then using the lower-level ax.add_artist() method to manually add the second artist to the plot fig , ax = plt . subplots () lines = [] styles = [ '-' , '--' , '-.' , ':' ] x = np . linspace ( 0 , 10 , 1000 ) for i in range ( 4 ): lines += ax . plot ( x , np . sin ( x - i * np . pi / 2 ), styles [ i ], color = 'black' ) ax . axis ( 'equal' ) # specify the lines and labels of the first legend ax . legend ( lines [: 2 ], [ 'line A' , 'line B' ], loc = 'upper right' , frameon = False ) # Create the second legend and add the artist manually. from matplotlib.legend import Legend leg = Legend ( ax , lines [ 2 :], [ 'line C' , 'line D' ], loc = 'lower right' , frameon = False ) ax . add_artist ( leg );","title":"07customized plot legends"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#07-customizing-plot-legends","text":"Customizing Plot Legends Choosing Elements for the Legend Legend for Size of Points Multiple Legends","title":"07 - Customizing Plot Legends"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#customizing-plot-legends","text":"Plot legends give meaning to a visualization, assigning labels to the various plot ele\u2010 ments. We previously saw how to create a simple legend; here we\u2019ll take a look at cus\u2010 tomizing the placement and aesthetics of the legend in Matplotlib. The simplest legend can be created with the plt.legend() command, which auto\u2010 matically creates a legend for any labeled plot elements import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np x = np . linspace ( 0 , 10 , 10000 ) fig , ax = plt . subplots () ax . plot ( x , np . sin ( x ), '-b' , label = 'Sine' ) ax . plot ( x , np . cos ( x ), '--r' , label = 'Cosine' ) ax . axis ( 'equal' ) leg = ax . legend (); #many wasy we can customize the legend, # we can specify the location and turn off the frame ax . legend ( loc = 'upper left' , frameon = False ) fig #We can use the ncol command to specify the number of columns in the legend ax . legend ( frameon = False , loc = 'lower center' , ncol = 2 ) fig # We can use a rounded box (fancybox) or add a shadow, change the transparency #(alpha value) of the frame, or change the padding around the text ax . legend ( fancybox = True , framealpha = 0.8 , shadow = True , borderpad = 0.5 ) fig","title":"Customizing Plot Legends"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#choosing-elements-for-the-legend","text":"As we\u2019ve already seen, the legend includes all labeled elements by default. If this is not what is desired, we can fine-tune which elements and labels appear in the legend by using the objects returned by plot commands. The plt.plot() command is able to create multiple lines at once, and returns a list of created line instances. Passing any of these to plt.legend() will tell it which to identify, along with the labels we\u2019d like to specify y = np . sin ( x [:, np . newaxis ] + np . pi * np . arange ( 0 , 2 , 0.5 )) lines = plt . plot ( x , y ) # line is a list of plt.line2D instance plt . legend ( lines [: 2 ],[ 'first' , 'second' ]) <matplotlib.legend.Legend at 0x7fba9d7a2c10> I generally find in practice that it is clearer to use the first method, applying labels to the plot elements you\u2019d like to show on the legend plt . plot ( x , y [:, 0 ], label = 'first' ) plt . plot ( x , y [:, 1 ], label = 'second' ) plt . plot ( x , y [:, 2 :]) plt . legend ( framealpha = 1 , frameon = True );","title":"Choosing Elements for the Legend"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#legend-for-size-of-points","text":"Sometimes the legend defaults are not sufficient for the given visualization. For exam\u2010 ple, perhaps you\u2019re using the size of points to mark certain features of the data, and want to create a legend reflecting this. Here is an example where we\u2019ll use the size of points to indicate populations of California cities. We\u2019d like a legend that specifies the scale of the sizes of the points, and we\u2019ll accomplish this by plotting some labeled data with no entries import pandas as pd cities = pd . read_csv ( '../data/california_cities.csv' ) # extract the data we'er interestd in lat , lon = cities [ 'latd' ], cities [ 'longd' ] population , area = cities [ 'population_total' ], cities [ 'area_total_km2' ] # scatter the points, using size and color but no label plt . scatter ( lon , lat , label = None , c = np . log10 ( population ), cmap = 'viridis' , s = area , linewidth = 0 , alpha = 0.5 ) #plt.axis(aspect='equal') plt . xlabel ( 'Longitude' ) plt . ylabel ( 'Latitude' ) plt . colorbar ( label = 'log$_ {10} $(population)' ) plt . clim ( 3 , 7 ) # scatter the points, using size and color but no label plt . scatter ( lon , lat , label = None , c = np . log10 ( population ), cmap = 'viridis' , s = area , linewidth = 0 , alpha = 0.5 ) #plt.axis(aspect='equal') plt . xlabel ( 'Longitude' ) plt . ylabel ( 'Latitude' ) plt . colorbar ( label = 'log$_ {10} $(population)' ) plt . clim ( 3 , 7 ) # here we created a legend: # we'll plot empty lists with the desired size and label for area in [ 100 , 300 , 500 ]: plt . scatter ([],[], c = 'k' , alpha = 0.3 , s = area , label = str ( area ) + ' km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , title = 'City Area' ) plt . title ( 'California cities: Area and Population' ) Text(0.5, 1.0, 'California cities: Area and Population')","title":"Legend for Size of Points"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#multiple-legends","text":"Sometimes when designing a plot you\u2019d like to add multiple legends to the same axes. Unfortunately, Matplotlib does not make this easy: via the standard legend interface, it is only possible to create a single legend for the entire plot. If you try to create a second legend using plt.legend() or ax.legend(), it will simply override the first one. We can work around this by creating a new legend artist from scratch, and then using the lower-level ax.add_artist() method to manually add the second artist to the plot fig , ax = plt . subplots () lines = [] styles = [ '-' , '--' , '-.' , ':' ] x = np . linspace ( 0 , 10 , 1000 ) for i in range ( 4 ): lines += ax . plot ( x , np . sin ( x - i * np . pi / 2 ), styles [ i ], color = 'black' ) ax . axis ( 'equal' ) # specify the lines and labels of the first legend ax . legend ( lines [: 2 ], [ 'line A' , 'line B' ], loc = 'upper right' , frameon = False ) # Create the second legend and add the artist manually. from matplotlib.legend import Legend leg = Legend ( ax , lines [ 2 :], [ 'line C' , 'line D' ], loc = 'lower right' , frameon = False ) ax . add_artist ( leg );","title":"Multiple Legends"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 08 - Customizing Colorbars \u00b6 Customizing Colorbars Customizing Colorbars Choosing the colormap Color limits and extensions Discrete colorbars Customizing Colorbars \u00b6 Plot legends identify discrete labels of discrete points. For continuous labels based on the color of points, lines, or regions, a labeled colorbar can be a great tool. In Mat\u2010 plotlib, a colorbar is a separate axes that can provide a key for the meaning of colors in a plot. import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np x = np . linspace ( 0 , 10 , 1000 ) I = np . sin ( x ) * np . cos ( x [:, np . newaxis ]) np . cos ( x [:, np . newaxis ]) array([[ 1.00000000e+00], [ 9.99949900e-01], [ 9.99799606e-01], [ 9.99549133e-01], [ 9.99198505e-01], [ 9.98747758e-01], [ 9.98196937e-01], [ 9.97546097e-01], [ 9.96795304e-01], [ 9.95944632e-01], [ 9.94994167e-01], [ 9.93944004e-01], [ 9.92794249e-01], [ 9.91545016e-01], [ 9.90196431e-01], [ 9.88748629e-01], [ 9.87201754e-01], [ 9.85555963e-01], [ 9.83811419e-01], [ 9.81968298e-01], [ 9.80026785e-01], [ 9.77987073e-01], [ 9.75849367e-01], [ 9.73613882e-01], [ 9.71280841e-01], [ 9.68850478e-01], [ 9.66323038e-01], [ 9.63698772e-01], [ 9.60977944e-01], [ 9.58160826e-01], [ 9.55247701e-01], [ 9.52238861e-01], [ 9.49134607e-01], [ 9.45935251e-01], [ 9.42641112e-01], [ 9.39252521e-01], [ 9.35769817e-01], [ 9.32193350e-01], [ 9.28523478e-01], [ 9.24760567e-01], [ 9.20904997e-01], [ 9.16957152e-01], [ 9.12917429e-01], [ 9.08786232e-01], [ 9.04563975e-01], [ 9.00251081e-01], [ 8.95847982e-01], [ 8.91355120e-01], [ 8.86772945e-01], [ 8.82101915e-01], [ 8.77342499e-01], [ 8.72495174e-01], [ 8.67560426e-01], [ 8.62538748e-01], [ 8.57430645e-01], [ 8.52236627e-01], [ 8.46957216e-01], [ 8.41592940e-01], [ 8.36144337e-01], [ 8.30611953e-01], [ 8.24996341e-01], [ 8.19298066e-01], [ 8.13517698e-01], [ 8.07655815e-01], [ 8.01713006e-01], [ 7.95689865e-01], [ 7.89586997e-01], [ 7.83405013e-01], [ 7.77144531e-01], [ 7.70806181e-01], [ 7.64390596e-01], [ 7.57898419e-01], [ 7.51330302e-01], [ 7.44686901e-01], [ 7.37968884e-01], [ 7.31176922e-01], [ 7.24311697e-01], [ 7.17373896e-01], [ 7.10364214e-01], [ 7.03283355e-01], [ 6.96132027e-01], [ 6.88910947e-01], [ 6.81620838e-01], [ 6.74262431e-01], [ 6.66836464e-01], [ 6.59343679e-01], [ 6.51784829e-01], [ 6.44160671e-01], [ 6.36471968e-01], [ 6.28719491e-01], [ 6.20904016e-01], [ 6.13026327e-01], [ 6.05087214e-01], [ 5.97087471e-01], [ 5.89027900e-01], [ 5.80909308e-01], [ 5.72732510e-01], [ 5.64498325e-01], [ 5.56207577e-01], [ 5.47861097e-01], [ 5.39459722e-01], [ 5.31004293e-01], [ 5.22495658e-01], [ 5.13934670e-01], [ 5.05322185e-01], [ 4.96659067e-01], [ 4.87946184e-01], [ 4.79184410e-01], [ 4.70374621e-01], [ 4.61517701e-01], [ 4.52614537e-01], [ 4.43666022e-01], [ 4.34673051e-01], [ 4.25636526e-01], [ 4.16557353e-01], [ 4.07436441e-01], [ 3.98274704e-01], [ 3.89073060e-01], [ 3.79832432e-01], [ 3.70553744e-01], [ 3.61237927e-01], [ 3.51885914e-01], [ 3.42498642e-01], [ 3.33077052e-01], [ 3.23622088e-01], [ 3.14134698e-01], [ 3.04615831e-01], [ 2.95066442e-01], [ 2.85487487e-01], [ 2.75879926e-01], [ 2.66244723e-01], [ 2.56582842e-01], [ 2.46895251e-01], [ 2.37182922e-01], [ 2.27446827e-01], [ 2.17687942e-01], [ 2.07907245e-01], [ 1.98105716e-01], [ 1.88284337e-01], [ 1.78444091e-01], [ 1.68585966e-01], [ 1.58710948e-01], [ 1.48820028e-01], [ 1.38914196e-01], [ 1.28994445e-01], [ 1.19061768e-01], [ 1.09117162e-01], [ 9.91616222e-02], [ 8.91961465e-02], [ 7.92217334e-02], [ 6.92393823e-02], [ 5.92500934e-02], [ 4.92548678e-02], [ 3.92547068e-02], [ 2.92506125e-02], [ 1.92435873e-02], [ 9.23463398e-03], [-7.75244699e-04], [-1.07850457e-02], [-2.07937660e-02], [-3.08004029e-02], [-4.08039535e-02], [-5.08034156e-02], [-6.07977872e-02], [-7.07860669e-02], [-8.07672539e-02], [-9.07403481e-02], [-1.00704350e-01], [-1.10658262e-01], [-1.20601085e-01], [-1.30531825e-01], [-1.40449485e-01], [-1.50353072e-01], [-1.60241594e-01], [-1.70114060e-01], [-1.79969480e-01], [-1.89806868e-01], [-1.99625237e-01], [-2.09423604e-01], [-2.19200987e-01], [-2.28956405e-01], [-2.38688883e-01], [-2.48397444e-01], [-2.58081116e-01], [-2.67738928e-01], [-2.77369913e-01], [-2.86973105e-01], [-2.96547543e-01], [-3.06092267e-01], [-3.15606321e-01], [-3.25088751e-01], [-3.34538607e-01], [-3.43954943e-01], [-3.53336815e-01], [-3.62683283e-01], [-3.71993410e-01], [-3.81266263e-01], [-3.90500914e-01], [-3.99696437e-01], [-4.08851910e-01], [-4.17966417e-01], [-4.27039043e-01], [-4.36068881e-01], [-4.45055025e-01], [-4.53996574e-01], [-4.62892633e-01], [-4.71742311e-01], [-4.80544720e-01], [-4.89298979e-01], [-4.98004210e-01], [-5.06659542e-01], [-5.15264107e-01], [-5.23817042e-01], [-5.32317492e-01], [-5.40764603e-01], [-5.49157530e-01], [-5.57495432e-01], [-5.65777473e-01], [-5.74002823e-01], [-5.82170659e-01], [-5.90280161e-01], [-5.98330518e-01], [-6.06320922e-01], [-6.14250574e-01], [-6.22118677e-01], [-6.29924445e-01], [-6.37667095e-01], [-6.45345851e-01], [-6.52959943e-01], [-6.60508609e-01], [-6.67991093e-01], [-6.75406644e-01], [-6.82754520e-01], [-6.90033984e-01], [-6.97244307e-01], [-7.04384767e-01], [-7.11454648e-01], [-7.18453241e-01], [-7.25379846e-01], [-7.32233768e-01], [-7.39014320e-01], [-7.45720824e-01], [-7.52352607e-01], [-7.58909005e-01], [-7.65389360e-01], [-7.71793024e-01], [-7.78119354e-01], [-7.84367718e-01], [-7.90537488e-01], [-7.96628046e-01], [-8.02638783e-01], [-8.08569096e-01], [-8.14418391e-01], [-8.20186082e-01], [-8.25871590e-01], [-8.31474347e-01], [-8.36993790e-01], [-8.42429367e-01], [-8.47780532e-01], [-8.53046751e-01], [-8.58227495e-01], [-8.63322245e-01], [-8.68330490e-01], [-8.73251730e-01], [-8.78085470e-01], [-8.82831226e-01], [-8.87488523e-01], [-8.92056894e-01], [-8.96535881e-01], [-9.00925037e-01], [-9.05223919e-01], [-9.09432099e-01], [-9.13549155e-01], [-9.17574673e-01], [-9.21508251e-01], [-9.25349494e-01], [-9.29098017e-01], [-9.32753446e-01], [-9.36315413e-01], [-9.39783561e-01], [-9.43157544e-01], [-9.46437023e-01], [-9.49621670e-01], [-9.52711165e-01], [-9.55705199e-01], [-9.58603471e-01], [-9.61405692e-01], [-9.64111581e-01], [-9.66720867e-01], [-9.69233287e-01], [-9.71648591e-01], [-9.73966536e-01], [-9.76186890e-01], [-9.78309431e-01], [-9.80333945e-01], [-9.82260231e-01], [-9.84088095e-01], [-9.85817354e-01], [-9.87447834e-01], [-9.88979373e-01], [-9.90411816e-01], [-9.91745021e-01], [-9.92978853e-01], [-9.94113189e-01], [-9.95147916e-01], [-9.96082929e-01], [-9.96918136e-01], [-9.97653452e-01], [-9.98288803e-01], [-9.98824127e-01], [-9.99259369e-01], [-9.99594485e-01], [-9.99829443e-01], [-9.99964218e-01], [-9.99998798e-01], [-9.99933178e-01], [-9.99767366e-01], [-9.99501377e-01], [-9.99135239e-01], [-9.98668988e-01], [-9.98102670e-01], [-9.97436344e-01], [-9.96670075e-01], [-9.95803940e-01], [-9.94838026e-01], [-9.93772430e-01], [-9.92607258e-01], [-9.91342628e-01], [-9.89978665e-01], [-9.88515508e-01], [-9.86953301e-01], [-9.85292203e-01], [-9.83532378e-01], [-9.81674005e-01], [-9.79717268e-01], [-9.77662364e-01], [-9.75509498e-01], [-9.73258887e-01], [-9.70910757e-01], [-9.68465341e-01], [-9.65922886e-01], [-9.63283645e-01], [-9.60547884e-01], [-9.57715877e-01], [-9.54787907e-01], [-9.51764268e-01], [-9.48645263e-01], [-9.45431204e-01], [-9.42122413e-01], [-9.38719222e-01], [-9.35221972e-01], [-9.31631013e-01], [-9.27946706e-01], [-9.24169418e-01], [-9.20299529e-01], [-9.16337427e-01], [-9.12283508e-01], [-9.08138179e-01], [-9.03901855e-01], [-8.99574960e-01], [-8.95157929e-01], [-8.90651203e-01], [-8.86055234e-01], [-8.81370484e-01], [-8.76597420e-01], [-8.71736521e-01], [-8.66788276e-01], [-8.61753178e-01], [-8.56631733e-01], [-8.51424454e-01], [-8.46131863e-01], [-8.40754490e-01], [-8.35292874e-01], [-8.29747562e-01], [-8.24119109e-01], [-8.18408081e-01], [-8.12615048e-01], [-8.06740592e-01], [-8.00785301e-01], [-7.94749771e-01], [-7.88634608e-01], [-7.82440424e-01], [-7.76167840e-01], [-7.69817485e-01], [-7.63389994e-01], [-7.56886012e-01], [-7.50306190e-01], [-7.43651188e-01], [-7.36921673e-01], [-7.30118318e-01], [-7.23241806e-01], [-7.16292826e-01], [-7.09272073e-01], [-7.02180252e-01], [-6.95018073e-01], [-6.87786253e-01], [-6.80485517e-01], [-6.73116597e-01], [-6.65680231e-01], [-6.58177165e-01], [-6.50608149e-01], [-6.42973943e-01], [-6.35275311e-01], [-6.27513025e-01], [-6.19687862e-01], [-6.11800607e-01], [-6.03852050e-01], [-5.95842988e-01], [-5.87774222e-01], [-5.79646561e-01], [-5.71460820e-01], [-5.63217820e-01], [-5.54918385e-01], [-5.46563347e-01], [-5.38153544e-01], [-5.29689819e-01], [-5.21173018e-01], [-5.12603997e-01], [-5.03983613e-01], [-4.95312730e-01], [-4.86592217e-01], [-4.77822948e-01], [-4.69005801e-01], [-4.60141660e-01], [-4.51231412e-01], [-4.42275952e-01], [-4.33276176e-01], [-4.24232986e-01], [-4.15147288e-01], [-4.06019993e-01], [-3.96852014e-01], [-3.87644272e-01], [-3.78397687e-01], [-3.69113187e-01], [-3.59791703e-01], [-3.50434167e-01], [-3.41041518e-01], [-3.31614697e-01], [-3.22154648e-01], [-3.12662319e-01], [-3.03138662e-01], [-2.93584631e-01], [-2.84001182e-01], [-2.74389277e-01], [-2.64749878e-01], [-2.55083951e-01], [-2.45392466e-01], [-2.35676391e-01], [-2.25936703e-01], [-2.16174375e-01], [-2.06390387e-01], [-1.96585719e-01], [-1.86761353e-01], [-1.76918273e-01], [-1.67057466e-01], [-1.57179921e-01], [-1.47286626e-01], [-1.37378573e-01], [-1.27456754e-01], [-1.17522165e-01], [-1.07575800e-01], [-9.76186559e-02], [-8.76517305e-02], [-7.76760224e-02], [-6.76925312e-02], [-5.77022572e-02], [-4.77062016e-02], [-3.77053657e-02], [-2.77007519e-02], [-1.76933624e-02], [-7.68420006e-03], [ 2.32573223e-03], [ 1.23354315e-02], [ 2.23438947e-02], [ 3.23501191e-02], [ 4.23531021e-02], [ 5.23518413e-02], [ 6.23453348e-02], [ 7.23325814e-02], [ 8.23125803e-02], [ 9.22843315e-02], [ 1.02246836e-01], [ 1.12199095e-01], [ 1.22140112e-01], [ 1.32068891e-01], [ 1.41984436e-01], [ 1.51885755e-01], [ 1.61771855e-01], [ 1.71641745e-01], [ 1.81494437e-01], [ 1.91328943e-01], [ 2.01144278e-01], [ 2.10939459e-01], [ 2.20713504e-01], [ 2.30465433e-01], [ 2.40194270e-01], [ 2.49899039e-01], [ 2.59578769e-01], [ 2.69232489e-01], [ 2.78859232e-01], [ 2.88458033e-01], [ 2.98027932e-01], [ 3.07567967e-01], [ 3.17077185e-01], [ 3.26554632e-01], [ 3.35999358e-01], [ 3.45410418e-01], [ 3.54786867e-01], [ 3.64127767e-01], [ 3.73432181e-01], [ 3.82699178e-01], [ 3.91927829e-01], [ 4.01117208e-01], [ 4.10266396e-01], [ 4.19374475e-01], [ 4.28440534e-01], [ 4.37463662e-01], [ 4.46442957e-01], [ 4.55377519e-01], [ 4.64266452e-01], [ 4.73108866e-01], [ 4.81903875e-01], [ 4.90650597e-01], [ 4.99348156e-01], [ 5.07995681e-01], [ 5.16592305e-01], [ 5.25137167e-01], [ 5.33629410e-01], [ 5.42068184e-01], [ 5.50452643e-01], [ 5.58781947e-01], [ 5.67055261e-01], [ 5.75271756e-01], [ 5.83430610e-01], [ 5.91531004e-01], [ 5.99572127e-01], [ 6.07553173e-01], [ 6.15473343e-01], [ 6.23331843e-01], [ 6.31127885e-01], [ 6.38860689e-01], [ 6.46529479e-01], [ 6.54133487e-01], [ 6.61671951e-01], [ 6.69144116e-01], [ 6.76549233e-01], [ 6.83886561e-01], [ 6.91155363e-01], [ 6.98354912e-01], [ 7.05484486e-01], [ 7.12543371e-01], [ 7.19530859e-01], [ 7.26446251e-01], [ 7.33288853e-01], [ 7.40057981e-01], [ 7.46752954e-01], [ 7.53373104e-01], [ 7.59917766e-01], [ 7.66386284e-01], [ 7.72778011e-01], [ 7.79092307e-01], [ 7.85328537e-01], [ 7.91486078e-01], [ 7.97564313e-01], [ 8.03562632e-01], [ 8.09480434e-01], [ 8.15317127e-01], [ 8.21072126e-01], [ 8.26744853e-01], [ 8.32334742e-01], [ 8.37841230e-01], [ 8.43263768e-01], [ 8.48601811e-01], [ 8.53854824e-01], [ 8.59022282e-01], [ 8.64103666e-01], [ 8.69098468e-01], [ 8.74006186e-01], [ 8.78826329e-01], [ 8.83558414e-01], [ 8.88201968e-01], [ 8.92756523e-01], [ 8.97221626e-01], [ 9.01596827e-01], [ 9.05881688e-01], [ 9.10075781e-01], [ 9.14178684e-01], [ 9.18189988e-01], [ 9.22109289e-01], [ 9.25936195e-01], [ 9.29670323e-01], [ 9.33311299e-01], [ 9.36858757e-01], [ 9.40312343e-01], [ 9.43671709e-01], [ 9.46936521e-01], [ 9.50106449e-01], [ 9.53181178e-01], [ 9.56160398e-01], [ 9.59043812e-01], [ 9.61831130e-01], [ 9.64522073e-01], [ 9.67116371e-01], [ 9.69613765e-01], [ 9.72014004e-01], [ 9.74316848e-01], [ 9.76522066e-01], [ 9.78629437e-01], [ 9.80638749e-01], [ 9.82549803e-01], [ 9.84362405e-01], [ 9.86076374e-01], [ 9.87691540e-01], [ 9.89207739e-01], [ 9.90624820e-01], [ 9.91942641e-01], [ 9.93161070e-01], [ 9.94279984e-01], [ 9.95299273e-01], [ 9.96218833e-01], [ 9.97038572e-01], [ 9.97758408e-01], [ 9.98378270e-01], [ 9.98898095e-01], [ 9.99317830e-01], [ 9.99637435e-01], [ 9.99856876e-01], [ 9.99976133e-01], [ 9.99995192e-01], [ 9.99914052e-01], [ 9.99732722e-01], [ 9.99451218e-01], [ 9.99069571e-01], [ 9.98587817e-01], [ 9.98006005e-01], [ 9.97324193e-01], [ 9.96542450e-01], [ 9.95660854e-01], [ 9.94679493e-01], [ 9.93598466e-01], [ 9.92417881e-01], [ 9.91137856e-01], [ 9.89758520e-01], [ 9.88280011e-01], [ 9.86702476e-01], [ 9.85026074e-01], [ 9.83250973e-01], [ 9.81377351e-01], [ 9.79405396e-01], [ 9.77335304e-01], [ 9.75167284e-01], [ 9.72901553e-01], [ 9.70538338e-01], [ 9.68077875e-01], [ 9.65520411e-01], [ 9.62866203e-01], [ 9.60115516e-01], [ 9.57268626e-01], [ 9.54325818e-01], [ 9.51287388e-01], [ 9.48153638e-01], [ 9.44924885e-01], [ 9.41601450e-01], [ 9.38183667e-01], [ 9.34671879e-01], [ 9.31066437e-01], [ 9.27367703e-01], [ 9.23576047e-01], [ 9.19691849e-01], [ 9.15715499e-01], [ 9.11647395e-01], [ 9.07487943e-01], [ 9.03237562e-01], [ 8.98896678e-01], [ 8.94465724e-01], [ 8.89945145e-01], [ 8.85335394e-01], [ 8.80636933e-01], [ 8.75850233e-01], [ 8.70975773e-01], [ 8.66014042e-01], [ 8.60965536e-01], [ 8.55830762e-01], [ 8.50610235e-01], [ 8.45304477e-01], [ 8.39914019e-01], [ 8.34439403e-01], [ 8.28881176e-01], [ 8.23239896e-01], [ 8.17516128e-01], [ 8.11710445e-01], [ 8.05823429e-01], [ 7.99855670e-01], [ 7.93807766e-01], [ 7.87680323e-01], [ 7.81473955e-01], [ 7.75189283e-01], [ 7.68826938e-01], [ 7.62387557e-01], [ 7.55871785e-01], [ 7.49280275e-01], [ 7.42613687e-01], [ 7.35872690e-01], [ 7.29057959e-01], [ 7.22170177e-01], [ 7.15210034e-01], [ 7.08178227e-01], [ 7.01075461e-01], [ 6.93902448e-01], [ 6.86659906e-01], [ 6.79348561e-01], [ 6.71969145e-01], [ 6.64522399e-01], [ 6.57009068e-01], [ 6.49429905e-01], [ 6.41785669e-01], [ 6.34077127e-01], [ 6.26305051e-01], [ 6.18470219e-01], [ 6.10573417e-01], [ 6.02615435e-01], [ 5.94597072e-01], [ 5.86519131e-01], [ 5.78382421e-01], [ 5.70187757e-01], [ 5.61935960e-01], [ 5.53627858e-01], [ 5.45264283e-01], [ 5.36846073e-01], [ 5.28374071e-01], [ 5.19849126e-01], [ 5.11272092e-01], [ 5.02643829e-01], [ 4.93965202e-01], [ 4.85237080e-01], [ 4.76460337e-01], [ 4.67635853e-01], [ 4.58764512e-01], [ 4.49847203e-01], [ 4.40884820e-01], [ 4.31878260e-01], [ 4.22828426e-01], [ 4.13736226e-01], [ 4.04602569e-01], [ 3.95428371e-01], [ 3.86214551e-01], [ 3.76962033e-01], [ 3.67671743e-01], [ 3.58344613e-01], [ 3.48981577e-01], [ 3.39583574e-01], [ 3.30151544e-01], [ 3.20686433e-01], [ 3.11189189e-01], [ 3.01660765e-01], [ 2.92102114e-01], [ 2.82514195e-01], [ 2.72897968e-01], [ 2.63254397e-01], [ 2.53584448e-01], [ 2.43889090e-01], [ 2.34169294e-01], [ 2.24426035e-01], [ 2.14660288e-01], [ 2.04873032e-01], [ 1.95065249e-01], [ 1.85237919e-01], [ 1.75392030e-01], [ 1.65528565e-01], [ 1.55648516e-01], [ 1.45752870e-01], [ 1.35842619e-01], [ 1.25918758e-01], [ 1.15982279e-01], [ 1.06034179e-01], [ 9.60754549e-02], [ 8.61071037e-02], [ 7.61301246e-02], [ 6.61455173e-02], [ 5.61542823e-02], [ 4.61574206e-02], [ 3.61559340e-02], [ 2.61508246e-02], [ 1.61430949e-02], [ 6.13374766e-03], [-3.87621418e-03], [-1.38857876e-02], [-2.38939697e-02], [-3.38997577e-02], [-4.39021489e-02], [-5.39001411e-02], [-6.38927325e-02], [-7.38789220e-02], [-8.38577088e-02], [-9.38280931e-02], [-1.03789076e-01], [-1.13739659e-01], [-1.23678846e-01], [-1.33605640e-01], [-1.43519046e-01], [-1.53418073e-01], [-1.63301726e-01], [-1.73169017e-01], [-1.83018957e-01], [-1.92850558e-01], [-2.02662836e-01], [-2.12454807e-01], [-2.22225490e-01], [-2.31973906e-01], [-2.41699079e-01], [-2.51400034e-01], [-2.61075798e-01], [-2.70725403e-01], [-2.80347881e-01], [-2.89942268e-01], [-2.99507604e-01], [-3.09042928e-01], [-3.18547287e-01], [-3.28019728e-01], [-3.37459301e-01], [-3.46865061e-01], [-3.56236066e-01], [-3.65571375e-01], [-3.74870055e-01], [-3.84131173e-01], [-3.93353801e-01], [-4.02537015e-01], [-4.11679896e-01], [-4.20781526e-01], [-4.29840994e-01], [-4.38857392e-01], [-4.47829817e-01], [-4.56757370e-01], [-4.65639155e-01], [-4.74474285e-01], [-4.83261871e-01], [-4.92001036e-01], [-5.00690902e-01], [-5.09330599e-01], [-5.17919262e-01], [-5.26456029e-01], [-5.34940046e-01], [-5.43370462e-01], [-5.51746432e-01], [-5.60067118e-01], [-5.68331685e-01], [-5.76539306e-01], [-5.84689158e-01], [-5.92780425e-01], [-6.00812295e-01], [-6.08783964e-01], [-6.16694633e-01], [-6.24543510e-01], [-6.32329808e-01], [-6.40052747e-01], [-6.47711552e-01], [-6.55305458e-01], [-6.62833702e-01], [-6.70295531e-01], [-6.77690196e-01], [-6.85016957e-01], [-6.92275080e-01], [-6.99463837e-01], [-7.06582509e-01], [-7.13630381e-01], [-7.20606748e-01], [-7.27510910e-01], [-7.34342176e-01], [-7.41099862e-01], [-7.47783289e-01], [-7.54391789e-01], [-7.60924700e-01], [-7.67381366e-01], [-7.73761141e-01], [-7.80063386e-01], [-7.86287469e-01], [-7.92432766e-01], [-7.98498661e-01], [-8.04484548e-01], [-8.10389826e-01], [-8.16213903e-01], [-8.21956196e-01], [-8.27616129e-01], [-8.33193136e-01], [-8.38686657e-01], [-8.44096142e-01], [-8.49421049e-01], [-8.54660845e-01], [-8.59815004e-01], [-8.64883010e-01], [-8.69864355e-01], [-8.74758541e-01], [-8.79565076e-01], [-8.84283479e-01], [-8.88913277e-01], [-8.93454007e-01], [-8.97905213e-01], [-9.02266449e-01], [-9.06537279e-01], [-9.10717274e-01], [-9.14806016e-01], [-9.18803095e-01], [-9.22708110e-01], [-9.26520671e-01], [-9.30240394e-01], [-9.33866908e-01], [-9.37399849e-01], [-9.40838863e-01], [-9.44183606e-01], [-9.47433742e-01], [-9.50588945e-01], [-9.53648900e-01], [-9.56613300e-01], [-9.59481847e-01], [-9.62254255e-01], [-9.64930246e-01], [-9.67509551e-01], [-9.69991913e-01], [-9.72377081e-01], [-9.74664818e-01], [-9.76854895e-01], [-9.78947090e-01], [-9.80941196e-01], [-9.82837012e-01], [-9.84634348e-01], [-9.86333025e-01], [-9.87932871e-01], [-9.89433727e-01], [-9.90835442e-01], [-9.92137877e-01], [-9.93340899e-01], [-9.94444389e-01], [-9.95448237e-01], [-9.96352341e-01], [-9.97156611e-01], [-9.97860966e-01], [-9.98465337e-01], [-9.98969661e-01], [-9.99373890e-01], [-9.99677982e-01], [-9.99881906e-01], [-9.99985643e-01], [-9.99989182e-01], [-9.99892522e-01], [-9.99695674e-01], [-9.99398657e-01], [-9.99001501e-01], [-9.98504245e-01], [-9.97906940e-01], [-9.97209644e-01], [-9.96412429e-01], [-9.95515375e-01], [-9.94518569e-01], [-9.93422114e-01], [-9.92226118e-01], [-9.90930702e-01], [-9.89535995e-01], [-9.88042137e-01], [-9.86449278e-01], [-9.84757577e-01], [-9.82967204e-01], [-9.81078338e-01], [-9.79091169e-01], [-9.77005895e-01], [-9.74822726e-01], [-9.72541880e-01], [-9.70163586e-01], [-9.67688082e-01], [-9.65115616e-01], [-9.62446446e-01], [-9.59680840e-01], [-9.56819074e-01], [-9.53861435e-01], [-9.50808220e-01], [-9.47659734e-01], [-9.44416293e-01], [-9.41078223e-01], [-9.37645857e-01], [-9.34119539e-01], [-9.30499623e-01], [-9.26786471e-01], [-9.22980456e-01], [-9.19081959e-01], [-9.15091370e-01], [-9.11009089e-01], [-9.06835526e-01], [-9.02571099e-01], [-8.98216234e-01], [-8.93771368e-01], [-8.89236948e-01], [-8.84613426e-01], [-8.79901266e-01], [-8.75100941e-01], [-8.70212930e-01], [-8.65237726e-01], [-8.60175824e-01], [-8.55027734e-01], [-8.49793970e-01], [-8.44475058e-01], [-8.39071529e-01]]) plt . imshow ( I ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f5d4c0728b0> Customizing Colorbars \u00b6 We can specify the colormap using the cmap argument to the plotting function that is creating the visualization plt . imshow ( I , cmap = 'gray' ) <matplotlib.image.AxesImage at 0x7f5d3a3e2490> All the available colormaps are in the plt.cm namespace; using IPython\u2019s tab- completion feature will give you a full list of built-in possibilities: #plt.imshow(I, cmap='winter_r') But being able to choose a colormap is just the first step: more important is how to decide among the possibilities! The choice turns out to be much more subtle than you might initially expect. Choosing the colormap \u00b6 A full treatment of color choice within visualization is beyond the scope of this book, but for entertaining reading on this subject and others, Broadly, you should be aware of three different categories of colormaps: Sequential colormaps These consist of one continuous sequence of colors (e.g., binary or viridis). Divergent colormaps These usually contain two distinct colors, which show positive and negative deviations from a mean (e.g., RdBu or PuOr). Qualitative colormaps These mix colors with no particular sequence (e.g., rainbow or jet). The jet colormap, which was the default in Matplotlib prior to version 2.0, is an example of a qualitative colormap. Its status as the default was quite unfortunate, because qualitative maps are often a poor choice for representing quantitative data. Among the problems is the fact that qualitative maps usually do not display any uni\u2010 form progression in brightness as the scale increases. from matplotlib.colors import LinearSegmentedColormap def grayscale_cmap ( cmap ): \"\"\"Return a grayscale version of the given colormap\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) # convert RGBA to perceived grayscale luminance # cf. http://alienryderflex.com/hsp.html RGB_weight = [ 0.299 , 0.587 , 0.114 ] luminance = np . sqrt ( np . dot ( colors [:, : 3 ] ** 2 , RGB_weight )) colors [:, : 3 ] = luminance [:, np . newaxis ] return LinearSegmentedColormap . from_list ( cmap . name + \"_gray\" , colors , cmap . N ) def view_colormap ( cmap ): \"\"\"Plot a colormap with its grayscale equivalent\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) cmap = grayscale_cmap ( cmap ) grayscale = cmap ( np . arange ( cmap . N )) fig , ax = plt . subplots ( 2 , figsize = ( 6 , 2 ), subplot_kw = dict ( xticks = [], yticks = [])) ax [ 0 ] . imshow ([ colors ], extent = [ 0 , 10 , 0 , 1 ]) ax [ 1 ] . imshow ([ grayscale ], extent = [ 0 , 10 , 0 , 1 ]) view_colormap ( 'jet' ) Notice the bright stripes in the grayscale image. Even in full color, this uneven bright\u2010 ness means that the eye will be drawn to certain portions of the color range, which will potentially emphasize unimportant parts of the dataset. It\u2019s better to use a color\u2010 map such as viridis (the default as of Matplotlib 2.0), which is specifically construc\u2010 ted to have an even brightness variation across the range. Thus, it not only plays well with our color perception, but also will translate well to grayscale printing view_colormap ( 'viridis' ) view_colormap ( 'cubehelix' ) view_colormap ( 'RdBu' ) Color limits and extensions \u00b6 Matplotlib allows for a large range of colorbar customization. The colorbar itself is simply an instance of plt.Axes, so all of the axes and tick formatting tricks we\u2019ve learned are applicable. The colorbar has some interesting flexibility; for example, we can narrow the color limits and indicate the out-of-bounds values with a triangular arrow at the top and bottom by setting the extend property. This might come in handy, for example, if you\u2019re displaying an image that is subject to noise # make noise in 1% of the image pixel speckles = ( np . random . random ( I . shape ) < 0.01 ) I [ speckles ] = np . random . normal ( 0 , 3 , np . count_nonzero ( speckles )) plt . figure ( figsize = ( 10 , 3.5 )) plt . subplot ( 1 , 2 , 1 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar () plt . subplot ( 1 , 2 , 2 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar ( extend = 'both' ) plt . clim ( - 1 , 1 ) Discrete colorbars \u00b6 Colormaps are by default continuous, but sometimes you\u2019d like to represent discrete values. The easiest way to do this is to use the plt.cm.get_cmap() function, and pass the name of a suitable colormap along with the number of desired bins plt . imshow ( I , cmap = plt . cm . get_cmap ( 'Blues' , 6 )) plt . colorbar () plt . clim ( - 1 , 1 );","title":"08customizing colorbar"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#08-customizing-colorbars","text":"Customizing Colorbars Customizing Colorbars Choosing the colormap Color limits and extensions Discrete colorbars","title":"08 - Customizing Colorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#customizing-colorbars","text":"Plot legends identify discrete labels of discrete points. For continuous labels based on the color of points, lines, or regions, a labeled colorbar can be a great tool. In Mat\u2010 plotlib, a colorbar is a separate axes that can provide a key for the meaning of colors in a plot. import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np x = np . linspace ( 0 , 10 , 1000 ) I = np . sin ( x ) * np . cos ( x [:, np . newaxis ]) np . cos ( x [:, np . newaxis ]) array([[ 1.00000000e+00], [ 9.99949900e-01], [ 9.99799606e-01], [ 9.99549133e-01], [ 9.99198505e-01], [ 9.98747758e-01], [ 9.98196937e-01], [ 9.97546097e-01], [ 9.96795304e-01], [ 9.95944632e-01], [ 9.94994167e-01], [ 9.93944004e-01], [ 9.92794249e-01], [ 9.91545016e-01], [ 9.90196431e-01], [ 9.88748629e-01], [ 9.87201754e-01], [ 9.85555963e-01], [ 9.83811419e-01], [ 9.81968298e-01], [ 9.80026785e-01], [ 9.77987073e-01], [ 9.75849367e-01], [ 9.73613882e-01], [ 9.71280841e-01], [ 9.68850478e-01], [ 9.66323038e-01], [ 9.63698772e-01], [ 9.60977944e-01], [ 9.58160826e-01], [ 9.55247701e-01], [ 9.52238861e-01], [ 9.49134607e-01], [ 9.45935251e-01], [ 9.42641112e-01], [ 9.39252521e-01], [ 9.35769817e-01], [ 9.32193350e-01], [ 9.28523478e-01], [ 9.24760567e-01], [ 9.20904997e-01], [ 9.16957152e-01], [ 9.12917429e-01], [ 9.08786232e-01], [ 9.04563975e-01], [ 9.00251081e-01], [ 8.95847982e-01], [ 8.91355120e-01], [ 8.86772945e-01], [ 8.82101915e-01], [ 8.77342499e-01], [ 8.72495174e-01], [ 8.67560426e-01], [ 8.62538748e-01], [ 8.57430645e-01], [ 8.52236627e-01], [ 8.46957216e-01], [ 8.41592940e-01], [ 8.36144337e-01], [ 8.30611953e-01], [ 8.24996341e-01], [ 8.19298066e-01], [ 8.13517698e-01], [ 8.07655815e-01], [ 8.01713006e-01], [ 7.95689865e-01], [ 7.89586997e-01], [ 7.83405013e-01], [ 7.77144531e-01], [ 7.70806181e-01], [ 7.64390596e-01], [ 7.57898419e-01], [ 7.51330302e-01], [ 7.44686901e-01], [ 7.37968884e-01], [ 7.31176922e-01], [ 7.24311697e-01], [ 7.17373896e-01], [ 7.10364214e-01], [ 7.03283355e-01], [ 6.96132027e-01], [ 6.88910947e-01], [ 6.81620838e-01], [ 6.74262431e-01], [ 6.66836464e-01], [ 6.59343679e-01], [ 6.51784829e-01], [ 6.44160671e-01], [ 6.36471968e-01], [ 6.28719491e-01], [ 6.20904016e-01], [ 6.13026327e-01], [ 6.05087214e-01], [ 5.97087471e-01], [ 5.89027900e-01], [ 5.80909308e-01], [ 5.72732510e-01], [ 5.64498325e-01], [ 5.56207577e-01], [ 5.47861097e-01], [ 5.39459722e-01], [ 5.31004293e-01], [ 5.22495658e-01], [ 5.13934670e-01], [ 5.05322185e-01], [ 4.96659067e-01], [ 4.87946184e-01], [ 4.79184410e-01], [ 4.70374621e-01], [ 4.61517701e-01], [ 4.52614537e-01], [ 4.43666022e-01], [ 4.34673051e-01], [ 4.25636526e-01], [ 4.16557353e-01], [ 4.07436441e-01], [ 3.98274704e-01], [ 3.89073060e-01], [ 3.79832432e-01], [ 3.70553744e-01], [ 3.61237927e-01], [ 3.51885914e-01], [ 3.42498642e-01], [ 3.33077052e-01], [ 3.23622088e-01], [ 3.14134698e-01], [ 3.04615831e-01], [ 2.95066442e-01], [ 2.85487487e-01], [ 2.75879926e-01], [ 2.66244723e-01], [ 2.56582842e-01], [ 2.46895251e-01], [ 2.37182922e-01], [ 2.27446827e-01], [ 2.17687942e-01], [ 2.07907245e-01], [ 1.98105716e-01], [ 1.88284337e-01], [ 1.78444091e-01], [ 1.68585966e-01], [ 1.58710948e-01], [ 1.48820028e-01], [ 1.38914196e-01], [ 1.28994445e-01], [ 1.19061768e-01], [ 1.09117162e-01], [ 9.91616222e-02], [ 8.91961465e-02], [ 7.92217334e-02], [ 6.92393823e-02], [ 5.92500934e-02], [ 4.92548678e-02], [ 3.92547068e-02], [ 2.92506125e-02], [ 1.92435873e-02], [ 9.23463398e-03], [-7.75244699e-04], [-1.07850457e-02], [-2.07937660e-02], [-3.08004029e-02], [-4.08039535e-02], [-5.08034156e-02], [-6.07977872e-02], [-7.07860669e-02], [-8.07672539e-02], [-9.07403481e-02], [-1.00704350e-01], [-1.10658262e-01], [-1.20601085e-01], [-1.30531825e-01], [-1.40449485e-01], [-1.50353072e-01], [-1.60241594e-01], [-1.70114060e-01], [-1.79969480e-01], [-1.89806868e-01], [-1.99625237e-01], [-2.09423604e-01], [-2.19200987e-01], [-2.28956405e-01], [-2.38688883e-01], [-2.48397444e-01], [-2.58081116e-01], [-2.67738928e-01], [-2.77369913e-01], [-2.86973105e-01], [-2.96547543e-01], [-3.06092267e-01], [-3.15606321e-01], [-3.25088751e-01], [-3.34538607e-01], [-3.43954943e-01], [-3.53336815e-01], [-3.62683283e-01], [-3.71993410e-01], [-3.81266263e-01], [-3.90500914e-01], [-3.99696437e-01], [-4.08851910e-01], [-4.17966417e-01], [-4.27039043e-01], [-4.36068881e-01], [-4.45055025e-01], [-4.53996574e-01], [-4.62892633e-01], [-4.71742311e-01], [-4.80544720e-01], [-4.89298979e-01], [-4.98004210e-01], [-5.06659542e-01], [-5.15264107e-01], [-5.23817042e-01], [-5.32317492e-01], [-5.40764603e-01], [-5.49157530e-01], [-5.57495432e-01], [-5.65777473e-01], [-5.74002823e-01], [-5.82170659e-01], [-5.90280161e-01], [-5.98330518e-01], [-6.06320922e-01], [-6.14250574e-01], [-6.22118677e-01], [-6.29924445e-01], [-6.37667095e-01], [-6.45345851e-01], [-6.52959943e-01], [-6.60508609e-01], [-6.67991093e-01], [-6.75406644e-01], [-6.82754520e-01], [-6.90033984e-01], [-6.97244307e-01], [-7.04384767e-01], [-7.11454648e-01], [-7.18453241e-01], [-7.25379846e-01], [-7.32233768e-01], [-7.39014320e-01], [-7.45720824e-01], [-7.52352607e-01], [-7.58909005e-01], [-7.65389360e-01], [-7.71793024e-01], [-7.78119354e-01], [-7.84367718e-01], [-7.90537488e-01], [-7.96628046e-01], [-8.02638783e-01], [-8.08569096e-01], [-8.14418391e-01], [-8.20186082e-01], [-8.25871590e-01], [-8.31474347e-01], [-8.36993790e-01], [-8.42429367e-01], [-8.47780532e-01], [-8.53046751e-01], [-8.58227495e-01], [-8.63322245e-01], [-8.68330490e-01], [-8.73251730e-01], [-8.78085470e-01], [-8.82831226e-01], [-8.87488523e-01], [-8.92056894e-01], [-8.96535881e-01], [-9.00925037e-01], [-9.05223919e-01], [-9.09432099e-01], [-9.13549155e-01], [-9.17574673e-01], [-9.21508251e-01], [-9.25349494e-01], [-9.29098017e-01], [-9.32753446e-01], [-9.36315413e-01], [-9.39783561e-01], [-9.43157544e-01], [-9.46437023e-01], [-9.49621670e-01], [-9.52711165e-01], [-9.55705199e-01], [-9.58603471e-01], [-9.61405692e-01], [-9.64111581e-01], [-9.66720867e-01], [-9.69233287e-01], [-9.71648591e-01], [-9.73966536e-01], [-9.76186890e-01], [-9.78309431e-01], [-9.80333945e-01], [-9.82260231e-01], [-9.84088095e-01], [-9.85817354e-01], [-9.87447834e-01], [-9.88979373e-01], [-9.90411816e-01], [-9.91745021e-01], [-9.92978853e-01], [-9.94113189e-01], [-9.95147916e-01], [-9.96082929e-01], [-9.96918136e-01], [-9.97653452e-01], [-9.98288803e-01], [-9.98824127e-01], [-9.99259369e-01], [-9.99594485e-01], [-9.99829443e-01], [-9.99964218e-01], [-9.99998798e-01], [-9.99933178e-01], [-9.99767366e-01], [-9.99501377e-01], [-9.99135239e-01], [-9.98668988e-01], [-9.98102670e-01], [-9.97436344e-01], [-9.96670075e-01], [-9.95803940e-01], [-9.94838026e-01], [-9.93772430e-01], [-9.92607258e-01], [-9.91342628e-01], [-9.89978665e-01], [-9.88515508e-01], [-9.86953301e-01], [-9.85292203e-01], [-9.83532378e-01], [-9.81674005e-01], [-9.79717268e-01], [-9.77662364e-01], [-9.75509498e-01], [-9.73258887e-01], [-9.70910757e-01], [-9.68465341e-01], [-9.65922886e-01], [-9.63283645e-01], [-9.60547884e-01], [-9.57715877e-01], [-9.54787907e-01], [-9.51764268e-01], [-9.48645263e-01], [-9.45431204e-01], [-9.42122413e-01], [-9.38719222e-01], [-9.35221972e-01], [-9.31631013e-01], [-9.27946706e-01], [-9.24169418e-01], [-9.20299529e-01], [-9.16337427e-01], [-9.12283508e-01], [-9.08138179e-01], [-9.03901855e-01], [-8.99574960e-01], [-8.95157929e-01], [-8.90651203e-01], [-8.86055234e-01], [-8.81370484e-01], [-8.76597420e-01], [-8.71736521e-01], [-8.66788276e-01], [-8.61753178e-01], [-8.56631733e-01], [-8.51424454e-01], [-8.46131863e-01], [-8.40754490e-01], [-8.35292874e-01], [-8.29747562e-01], [-8.24119109e-01], [-8.18408081e-01], [-8.12615048e-01], [-8.06740592e-01], [-8.00785301e-01], [-7.94749771e-01], [-7.88634608e-01], [-7.82440424e-01], [-7.76167840e-01], [-7.69817485e-01], [-7.63389994e-01], [-7.56886012e-01], [-7.50306190e-01], [-7.43651188e-01], [-7.36921673e-01], [-7.30118318e-01], [-7.23241806e-01], [-7.16292826e-01], [-7.09272073e-01], [-7.02180252e-01], [-6.95018073e-01], [-6.87786253e-01], [-6.80485517e-01], [-6.73116597e-01], [-6.65680231e-01], [-6.58177165e-01], [-6.50608149e-01], [-6.42973943e-01], [-6.35275311e-01], [-6.27513025e-01], [-6.19687862e-01], [-6.11800607e-01], [-6.03852050e-01], [-5.95842988e-01], [-5.87774222e-01], [-5.79646561e-01], [-5.71460820e-01], [-5.63217820e-01], [-5.54918385e-01], [-5.46563347e-01], [-5.38153544e-01], [-5.29689819e-01], [-5.21173018e-01], [-5.12603997e-01], [-5.03983613e-01], [-4.95312730e-01], [-4.86592217e-01], [-4.77822948e-01], [-4.69005801e-01], [-4.60141660e-01], [-4.51231412e-01], [-4.42275952e-01], [-4.33276176e-01], [-4.24232986e-01], [-4.15147288e-01], [-4.06019993e-01], [-3.96852014e-01], [-3.87644272e-01], [-3.78397687e-01], [-3.69113187e-01], [-3.59791703e-01], [-3.50434167e-01], [-3.41041518e-01], [-3.31614697e-01], [-3.22154648e-01], [-3.12662319e-01], [-3.03138662e-01], [-2.93584631e-01], [-2.84001182e-01], [-2.74389277e-01], [-2.64749878e-01], [-2.55083951e-01], [-2.45392466e-01], [-2.35676391e-01], [-2.25936703e-01], [-2.16174375e-01], [-2.06390387e-01], [-1.96585719e-01], [-1.86761353e-01], [-1.76918273e-01], [-1.67057466e-01], [-1.57179921e-01], [-1.47286626e-01], [-1.37378573e-01], [-1.27456754e-01], [-1.17522165e-01], [-1.07575800e-01], [-9.76186559e-02], [-8.76517305e-02], [-7.76760224e-02], [-6.76925312e-02], [-5.77022572e-02], [-4.77062016e-02], [-3.77053657e-02], [-2.77007519e-02], [-1.76933624e-02], [-7.68420006e-03], [ 2.32573223e-03], [ 1.23354315e-02], [ 2.23438947e-02], [ 3.23501191e-02], [ 4.23531021e-02], [ 5.23518413e-02], [ 6.23453348e-02], [ 7.23325814e-02], [ 8.23125803e-02], [ 9.22843315e-02], [ 1.02246836e-01], [ 1.12199095e-01], [ 1.22140112e-01], [ 1.32068891e-01], [ 1.41984436e-01], [ 1.51885755e-01], [ 1.61771855e-01], [ 1.71641745e-01], [ 1.81494437e-01], [ 1.91328943e-01], [ 2.01144278e-01], [ 2.10939459e-01], [ 2.20713504e-01], [ 2.30465433e-01], [ 2.40194270e-01], [ 2.49899039e-01], [ 2.59578769e-01], [ 2.69232489e-01], [ 2.78859232e-01], [ 2.88458033e-01], [ 2.98027932e-01], [ 3.07567967e-01], [ 3.17077185e-01], [ 3.26554632e-01], [ 3.35999358e-01], [ 3.45410418e-01], [ 3.54786867e-01], [ 3.64127767e-01], [ 3.73432181e-01], [ 3.82699178e-01], [ 3.91927829e-01], [ 4.01117208e-01], [ 4.10266396e-01], [ 4.19374475e-01], [ 4.28440534e-01], [ 4.37463662e-01], [ 4.46442957e-01], [ 4.55377519e-01], [ 4.64266452e-01], [ 4.73108866e-01], [ 4.81903875e-01], [ 4.90650597e-01], [ 4.99348156e-01], [ 5.07995681e-01], [ 5.16592305e-01], [ 5.25137167e-01], [ 5.33629410e-01], [ 5.42068184e-01], [ 5.50452643e-01], [ 5.58781947e-01], [ 5.67055261e-01], [ 5.75271756e-01], [ 5.83430610e-01], [ 5.91531004e-01], [ 5.99572127e-01], [ 6.07553173e-01], [ 6.15473343e-01], [ 6.23331843e-01], [ 6.31127885e-01], [ 6.38860689e-01], [ 6.46529479e-01], [ 6.54133487e-01], [ 6.61671951e-01], [ 6.69144116e-01], [ 6.76549233e-01], [ 6.83886561e-01], [ 6.91155363e-01], [ 6.98354912e-01], [ 7.05484486e-01], [ 7.12543371e-01], [ 7.19530859e-01], [ 7.26446251e-01], [ 7.33288853e-01], [ 7.40057981e-01], [ 7.46752954e-01], [ 7.53373104e-01], [ 7.59917766e-01], [ 7.66386284e-01], [ 7.72778011e-01], [ 7.79092307e-01], [ 7.85328537e-01], [ 7.91486078e-01], [ 7.97564313e-01], [ 8.03562632e-01], [ 8.09480434e-01], [ 8.15317127e-01], [ 8.21072126e-01], [ 8.26744853e-01], [ 8.32334742e-01], [ 8.37841230e-01], [ 8.43263768e-01], [ 8.48601811e-01], [ 8.53854824e-01], [ 8.59022282e-01], [ 8.64103666e-01], [ 8.69098468e-01], [ 8.74006186e-01], [ 8.78826329e-01], [ 8.83558414e-01], [ 8.88201968e-01], [ 8.92756523e-01], [ 8.97221626e-01], [ 9.01596827e-01], [ 9.05881688e-01], [ 9.10075781e-01], [ 9.14178684e-01], [ 9.18189988e-01], [ 9.22109289e-01], [ 9.25936195e-01], [ 9.29670323e-01], [ 9.33311299e-01], [ 9.36858757e-01], [ 9.40312343e-01], [ 9.43671709e-01], [ 9.46936521e-01], [ 9.50106449e-01], [ 9.53181178e-01], [ 9.56160398e-01], [ 9.59043812e-01], [ 9.61831130e-01], [ 9.64522073e-01], [ 9.67116371e-01], [ 9.69613765e-01], [ 9.72014004e-01], [ 9.74316848e-01], [ 9.76522066e-01], [ 9.78629437e-01], [ 9.80638749e-01], [ 9.82549803e-01], [ 9.84362405e-01], [ 9.86076374e-01], [ 9.87691540e-01], [ 9.89207739e-01], [ 9.90624820e-01], [ 9.91942641e-01], [ 9.93161070e-01], [ 9.94279984e-01], [ 9.95299273e-01], [ 9.96218833e-01], [ 9.97038572e-01], [ 9.97758408e-01], [ 9.98378270e-01], [ 9.98898095e-01], [ 9.99317830e-01], [ 9.99637435e-01], [ 9.99856876e-01], [ 9.99976133e-01], [ 9.99995192e-01], [ 9.99914052e-01], [ 9.99732722e-01], [ 9.99451218e-01], [ 9.99069571e-01], [ 9.98587817e-01], [ 9.98006005e-01], [ 9.97324193e-01], [ 9.96542450e-01], [ 9.95660854e-01], [ 9.94679493e-01], [ 9.93598466e-01], [ 9.92417881e-01], [ 9.91137856e-01], [ 9.89758520e-01], [ 9.88280011e-01], [ 9.86702476e-01], [ 9.85026074e-01], [ 9.83250973e-01], [ 9.81377351e-01], [ 9.79405396e-01], [ 9.77335304e-01], [ 9.75167284e-01], [ 9.72901553e-01], [ 9.70538338e-01], [ 9.68077875e-01], [ 9.65520411e-01], [ 9.62866203e-01], [ 9.60115516e-01], [ 9.57268626e-01], [ 9.54325818e-01], [ 9.51287388e-01], [ 9.48153638e-01], [ 9.44924885e-01], [ 9.41601450e-01], [ 9.38183667e-01], [ 9.34671879e-01], [ 9.31066437e-01], [ 9.27367703e-01], [ 9.23576047e-01], [ 9.19691849e-01], [ 9.15715499e-01], [ 9.11647395e-01], [ 9.07487943e-01], [ 9.03237562e-01], [ 8.98896678e-01], [ 8.94465724e-01], [ 8.89945145e-01], [ 8.85335394e-01], [ 8.80636933e-01], [ 8.75850233e-01], [ 8.70975773e-01], [ 8.66014042e-01], [ 8.60965536e-01], [ 8.55830762e-01], [ 8.50610235e-01], [ 8.45304477e-01], [ 8.39914019e-01], [ 8.34439403e-01], [ 8.28881176e-01], [ 8.23239896e-01], [ 8.17516128e-01], [ 8.11710445e-01], [ 8.05823429e-01], [ 7.99855670e-01], [ 7.93807766e-01], [ 7.87680323e-01], [ 7.81473955e-01], [ 7.75189283e-01], [ 7.68826938e-01], [ 7.62387557e-01], [ 7.55871785e-01], [ 7.49280275e-01], [ 7.42613687e-01], [ 7.35872690e-01], [ 7.29057959e-01], [ 7.22170177e-01], [ 7.15210034e-01], [ 7.08178227e-01], [ 7.01075461e-01], [ 6.93902448e-01], [ 6.86659906e-01], [ 6.79348561e-01], [ 6.71969145e-01], [ 6.64522399e-01], [ 6.57009068e-01], [ 6.49429905e-01], [ 6.41785669e-01], [ 6.34077127e-01], [ 6.26305051e-01], [ 6.18470219e-01], [ 6.10573417e-01], [ 6.02615435e-01], [ 5.94597072e-01], [ 5.86519131e-01], [ 5.78382421e-01], [ 5.70187757e-01], [ 5.61935960e-01], [ 5.53627858e-01], [ 5.45264283e-01], [ 5.36846073e-01], [ 5.28374071e-01], [ 5.19849126e-01], [ 5.11272092e-01], [ 5.02643829e-01], [ 4.93965202e-01], [ 4.85237080e-01], [ 4.76460337e-01], [ 4.67635853e-01], [ 4.58764512e-01], [ 4.49847203e-01], [ 4.40884820e-01], [ 4.31878260e-01], [ 4.22828426e-01], [ 4.13736226e-01], [ 4.04602569e-01], [ 3.95428371e-01], [ 3.86214551e-01], [ 3.76962033e-01], [ 3.67671743e-01], [ 3.58344613e-01], [ 3.48981577e-01], [ 3.39583574e-01], [ 3.30151544e-01], [ 3.20686433e-01], [ 3.11189189e-01], [ 3.01660765e-01], [ 2.92102114e-01], [ 2.82514195e-01], [ 2.72897968e-01], [ 2.63254397e-01], [ 2.53584448e-01], [ 2.43889090e-01], [ 2.34169294e-01], [ 2.24426035e-01], [ 2.14660288e-01], [ 2.04873032e-01], [ 1.95065249e-01], [ 1.85237919e-01], [ 1.75392030e-01], [ 1.65528565e-01], [ 1.55648516e-01], [ 1.45752870e-01], [ 1.35842619e-01], [ 1.25918758e-01], [ 1.15982279e-01], [ 1.06034179e-01], [ 9.60754549e-02], [ 8.61071037e-02], [ 7.61301246e-02], [ 6.61455173e-02], [ 5.61542823e-02], [ 4.61574206e-02], [ 3.61559340e-02], [ 2.61508246e-02], [ 1.61430949e-02], [ 6.13374766e-03], [-3.87621418e-03], [-1.38857876e-02], [-2.38939697e-02], [-3.38997577e-02], [-4.39021489e-02], [-5.39001411e-02], [-6.38927325e-02], [-7.38789220e-02], [-8.38577088e-02], [-9.38280931e-02], [-1.03789076e-01], [-1.13739659e-01], [-1.23678846e-01], [-1.33605640e-01], [-1.43519046e-01], [-1.53418073e-01], [-1.63301726e-01], [-1.73169017e-01], [-1.83018957e-01], [-1.92850558e-01], [-2.02662836e-01], [-2.12454807e-01], [-2.22225490e-01], [-2.31973906e-01], [-2.41699079e-01], [-2.51400034e-01], [-2.61075798e-01], [-2.70725403e-01], [-2.80347881e-01], [-2.89942268e-01], [-2.99507604e-01], [-3.09042928e-01], [-3.18547287e-01], [-3.28019728e-01], [-3.37459301e-01], [-3.46865061e-01], [-3.56236066e-01], [-3.65571375e-01], [-3.74870055e-01], [-3.84131173e-01], [-3.93353801e-01], [-4.02537015e-01], [-4.11679896e-01], [-4.20781526e-01], [-4.29840994e-01], [-4.38857392e-01], [-4.47829817e-01], [-4.56757370e-01], [-4.65639155e-01], [-4.74474285e-01], [-4.83261871e-01], [-4.92001036e-01], [-5.00690902e-01], [-5.09330599e-01], [-5.17919262e-01], [-5.26456029e-01], [-5.34940046e-01], [-5.43370462e-01], [-5.51746432e-01], [-5.60067118e-01], [-5.68331685e-01], [-5.76539306e-01], [-5.84689158e-01], [-5.92780425e-01], [-6.00812295e-01], [-6.08783964e-01], [-6.16694633e-01], [-6.24543510e-01], [-6.32329808e-01], [-6.40052747e-01], [-6.47711552e-01], [-6.55305458e-01], [-6.62833702e-01], [-6.70295531e-01], [-6.77690196e-01], [-6.85016957e-01], [-6.92275080e-01], [-6.99463837e-01], [-7.06582509e-01], [-7.13630381e-01], [-7.20606748e-01], [-7.27510910e-01], [-7.34342176e-01], [-7.41099862e-01], [-7.47783289e-01], [-7.54391789e-01], [-7.60924700e-01], [-7.67381366e-01], [-7.73761141e-01], [-7.80063386e-01], [-7.86287469e-01], [-7.92432766e-01], [-7.98498661e-01], [-8.04484548e-01], [-8.10389826e-01], [-8.16213903e-01], [-8.21956196e-01], [-8.27616129e-01], [-8.33193136e-01], [-8.38686657e-01], [-8.44096142e-01], [-8.49421049e-01], [-8.54660845e-01], [-8.59815004e-01], [-8.64883010e-01], [-8.69864355e-01], [-8.74758541e-01], [-8.79565076e-01], [-8.84283479e-01], [-8.88913277e-01], [-8.93454007e-01], [-8.97905213e-01], [-9.02266449e-01], [-9.06537279e-01], [-9.10717274e-01], [-9.14806016e-01], [-9.18803095e-01], [-9.22708110e-01], [-9.26520671e-01], [-9.30240394e-01], [-9.33866908e-01], [-9.37399849e-01], [-9.40838863e-01], [-9.44183606e-01], [-9.47433742e-01], [-9.50588945e-01], [-9.53648900e-01], [-9.56613300e-01], [-9.59481847e-01], [-9.62254255e-01], [-9.64930246e-01], [-9.67509551e-01], [-9.69991913e-01], [-9.72377081e-01], [-9.74664818e-01], [-9.76854895e-01], [-9.78947090e-01], [-9.80941196e-01], [-9.82837012e-01], [-9.84634348e-01], [-9.86333025e-01], [-9.87932871e-01], [-9.89433727e-01], [-9.90835442e-01], [-9.92137877e-01], [-9.93340899e-01], [-9.94444389e-01], [-9.95448237e-01], [-9.96352341e-01], [-9.97156611e-01], [-9.97860966e-01], [-9.98465337e-01], [-9.98969661e-01], [-9.99373890e-01], [-9.99677982e-01], [-9.99881906e-01], [-9.99985643e-01], [-9.99989182e-01], [-9.99892522e-01], [-9.99695674e-01], [-9.99398657e-01], [-9.99001501e-01], [-9.98504245e-01], [-9.97906940e-01], [-9.97209644e-01], [-9.96412429e-01], [-9.95515375e-01], [-9.94518569e-01], [-9.93422114e-01], [-9.92226118e-01], [-9.90930702e-01], [-9.89535995e-01], [-9.88042137e-01], [-9.86449278e-01], [-9.84757577e-01], [-9.82967204e-01], [-9.81078338e-01], [-9.79091169e-01], [-9.77005895e-01], [-9.74822726e-01], [-9.72541880e-01], [-9.70163586e-01], [-9.67688082e-01], [-9.65115616e-01], [-9.62446446e-01], [-9.59680840e-01], [-9.56819074e-01], [-9.53861435e-01], [-9.50808220e-01], [-9.47659734e-01], [-9.44416293e-01], [-9.41078223e-01], [-9.37645857e-01], [-9.34119539e-01], [-9.30499623e-01], [-9.26786471e-01], [-9.22980456e-01], [-9.19081959e-01], [-9.15091370e-01], [-9.11009089e-01], [-9.06835526e-01], [-9.02571099e-01], [-8.98216234e-01], [-8.93771368e-01], [-8.89236948e-01], [-8.84613426e-01], [-8.79901266e-01], [-8.75100941e-01], [-8.70212930e-01], [-8.65237726e-01], [-8.60175824e-01], [-8.55027734e-01], [-8.49793970e-01], [-8.44475058e-01], [-8.39071529e-01]]) plt . imshow ( I ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f5d4c0728b0>","title":"Customizing Colorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#customizing-colorbars_1","text":"We can specify the colormap using the cmap argument to the plotting function that is creating the visualization plt . imshow ( I , cmap = 'gray' ) <matplotlib.image.AxesImage at 0x7f5d3a3e2490> All the available colormaps are in the plt.cm namespace; using IPython\u2019s tab- completion feature will give you a full list of built-in possibilities: #plt.imshow(I, cmap='winter_r') But being able to choose a colormap is just the first step: more important is how to decide among the possibilities! The choice turns out to be much more subtle than you might initially expect.","title":"Customizing Colorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#choosing-the-colormap","text":"A full treatment of color choice within visualization is beyond the scope of this book, but for entertaining reading on this subject and others, Broadly, you should be aware of three different categories of colormaps: Sequential colormaps These consist of one continuous sequence of colors (e.g., binary or viridis). Divergent colormaps These usually contain two distinct colors, which show positive and negative deviations from a mean (e.g., RdBu or PuOr). Qualitative colormaps These mix colors with no particular sequence (e.g., rainbow or jet). The jet colormap, which was the default in Matplotlib prior to version 2.0, is an example of a qualitative colormap. Its status as the default was quite unfortunate, because qualitative maps are often a poor choice for representing quantitative data. Among the problems is the fact that qualitative maps usually do not display any uni\u2010 form progression in brightness as the scale increases. from matplotlib.colors import LinearSegmentedColormap def grayscale_cmap ( cmap ): \"\"\"Return a grayscale version of the given colormap\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) # convert RGBA to perceived grayscale luminance # cf. http://alienryderflex.com/hsp.html RGB_weight = [ 0.299 , 0.587 , 0.114 ] luminance = np . sqrt ( np . dot ( colors [:, : 3 ] ** 2 , RGB_weight )) colors [:, : 3 ] = luminance [:, np . newaxis ] return LinearSegmentedColormap . from_list ( cmap . name + \"_gray\" , colors , cmap . N ) def view_colormap ( cmap ): \"\"\"Plot a colormap with its grayscale equivalent\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) cmap = grayscale_cmap ( cmap ) grayscale = cmap ( np . arange ( cmap . N )) fig , ax = plt . subplots ( 2 , figsize = ( 6 , 2 ), subplot_kw = dict ( xticks = [], yticks = [])) ax [ 0 ] . imshow ([ colors ], extent = [ 0 , 10 , 0 , 1 ]) ax [ 1 ] . imshow ([ grayscale ], extent = [ 0 , 10 , 0 , 1 ]) view_colormap ( 'jet' ) Notice the bright stripes in the grayscale image. Even in full color, this uneven bright\u2010 ness means that the eye will be drawn to certain portions of the color range, which will potentially emphasize unimportant parts of the dataset. It\u2019s better to use a color\u2010 map such as viridis (the default as of Matplotlib 2.0), which is specifically construc\u2010 ted to have an even brightness variation across the range. Thus, it not only plays well with our color perception, but also will translate well to grayscale printing view_colormap ( 'viridis' ) view_colormap ( 'cubehelix' ) view_colormap ( 'RdBu' )","title":"Choosing the colormap"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#color-limits-and-extensions","text":"Matplotlib allows for a large range of colorbar customization. The colorbar itself is simply an instance of plt.Axes, so all of the axes and tick formatting tricks we\u2019ve learned are applicable. The colorbar has some interesting flexibility; for example, we can narrow the color limits and indicate the out-of-bounds values with a triangular arrow at the top and bottom by setting the extend property. This might come in handy, for example, if you\u2019re displaying an image that is subject to noise # make noise in 1% of the image pixel speckles = ( np . random . random ( I . shape ) < 0.01 ) I [ speckles ] = np . random . normal ( 0 , 3 , np . count_nonzero ( speckles )) plt . figure ( figsize = ( 10 , 3.5 )) plt . subplot ( 1 , 2 , 1 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar () plt . subplot ( 1 , 2 , 2 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar ( extend = 'both' ) plt . clim ( - 1 , 1 )","title":"Color limits and extensions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#discrete-colorbars","text":"Colormaps are by default continuous, but sometimes you\u2019d like to represent discrete values. The easiest way to do this is to use the plt.cm.get_cmap() function, and pass the name of a suitable colormap along with the number of desired bins plt . imshow ( I , cmap = plt . cm . get_cmap ( 'Blues' , 6 )) plt . colorbar () plt . clim ( - 1 , 1 );","title":"Discrete colorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 09 - Multiple Subplots \u00b6 Multiple Subplots plt.axes: Subplots by Hand plt.subplot: Simple Grids of Subplots plt.subplots: The Whole Grid in One Go plt.GridSpec: More Complicated Arrangements Multiple Subplots \u00b6 Sometimes it is helpful to compare different views of data side by side. To this end, Matplotlib has the concept of subplots: groups of smaller axes that can exist together within a single figure. These subplots might be insets, grids of plots, or other more complicated layouts. In this section, we\u2019ll explore four routines for creating subplots in Matplotlib. We\u2019ll start by setting up the notebook for plotting and importing the functions we will use: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np plt.axes: Subplots by Hand \u00b6 The most basic method of creating an axes is to use the plt.axes function. As we\u2019ve seen previously, by default this creates a standard axes object that fills the entire fig\u2010 ure. plt.axes also takes an optional argument that is a list of four numbers in the figure coordinate system. These numbers represent [bottom, left, width, height] in the figure coordinate system, which ranges from 0 at the bottom left of the figure to 1 at the top right of the figure. ax1 = plt . axes () ax2 = plt . axes ([ 0.65 , 0.65 , 0.2 , 0.2 ]) The equivalent of this command within the object-oriented interface is fig.add_axes(). Let\u2019s use this to create two vertically stacked axes fig = plt . figure () ax1 = fig . add_axes ([ 0.1 , 0.5 , 0.8 , 0.4 ], xticklabels = [], ylim = ( - 1.2 , 1.2 )) ax2 = fig . add_axes ([ 0.1 , 0.1 , 0.8 , 0.4 ], xticklabels = [], ylim = ( - 1.2 , 1.2 )) x = np . linspace ( - 0 , 10 ) ax1 . plot ( np . sin ( x )) ax2 . plot ( np . cos ( x )); plt.subplot: Simple Grids of Subplots \u00b6 Aligned columns or rows of subplots are a common enough need that Matplotlib has several convenience routines that make them easy to create. The lowest level of these is plt.subplot(), which creates a single subplot within a grid. As you can see, this command takes three integer arguments\u2014the number of rows, the number of col\u2010 umns, and the index of the plot to be created in this scheme, which runs from the upper left to the bottom right for i in range ( 1 , 7 ): plt . subplot ( 2 , 3 , i ) plt . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) The command plt.subplots_adjust can be used to adjust the spacing between these plots. object-oriented command, fig.add_subplot(): fig = plt . figure () fig . subplots_adjust ( hspace = 0.4 , wspace = 0.4 ) for i in range ( 1 , 7 ): ax = fig . add_subplot ( 2 , 3 , i ) ax . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) plt.subplots: The Whole Grid in One Go \u00b6 The approach just described can become quite tedious when you\u2019re creating a large grid of subplots, especially if you\u2019d like to hide the x- and y-axis labels on the inner plots. For this purpose, plt.subplots() is the easier tool to use (note the s at the end of subplots). Rather than creating a single subplot, this function creates a full grid of subplots in a single line, returning them in a NumPy array. The arguments are the number of rows and number of columns, along with optional keywords sharex and sharey, which allow you to specify the relationships between different axes. fig , ax = plt . subplots ( 2 , 3 , sharex = 'col' , sharey = 'row' ) for i in range ( 2 ): for j in range ( 3 ): ax [ i , j ] . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) fig plt.GridSpec: More Complicated Arrangements \u00b6 To go beyond a regular grid to subplots that span multiple rows and columns, plt.GridSpec() is the best tool. The plt.GridSpec() object does not create a plot by itself; it is simply a convenient interface that is recognized by the plt.subplot() command. grid = plt . GridSpec ( 2 , 3 , wspace = 0.4 , hspace = 0.3 ) plt . subplot ( grid [ 0 , 0 ]) plt . subplot ( grid [ 0 , 1 :]) plt . subplot ( grid [ 1 ,: 2 ]) plt . subplot ( grid [ 1 , 2 ]); # Create some normally distributed data mean = [ 0 , 0 ] cov = [[ 1 , 1 ],[ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , cov , 3000 ) . T # Set up the axes with gridspec fig = plt . figure ( figsize = ( 6 , 6 )) grid = plt . GridSpec ( 4 , 4 , wspace = 0.2 , hspace = 0.2 ) main_ax = fig . add_subplot ( grid [: - 1 , 1 :]) y_hist = fig . add_subplot ( grid [: - 1 , 0 ], xticklabels = [], sharey = main_ax ) x_hist = fig . add_subplot ( grid [ - 1 , 1 :], yticklabels = [], sharex = main_ax ) # scatter points on the main axes main_ax . plot ( x , y , 'ok' , markersize = 3 , alpha = 0.2 ) # historgram on the attached axes x_hist . hist ( x , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'gray' ) x_hist . invert_yaxis () y_hist . hist ( y , 40 , histtype = 'stepfilled' , orientation = 'horizontal' , color = 'gray' ) y_hist . invert_xaxis ()","title":"09multiple subplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#09-multiple-subplots","text":"Multiple Subplots plt.axes: Subplots by Hand plt.subplot: Simple Grids of Subplots plt.subplots: The Whole Grid in One Go plt.GridSpec: More Complicated Arrangements","title":"09 - Multiple Subplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#multiple-subplots","text":"Sometimes it is helpful to compare different views of data side by side. To this end, Matplotlib has the concept of subplots: groups of smaller axes that can exist together within a single figure. These subplots might be insets, grids of plots, or other more complicated layouts. In this section, we\u2019ll explore four routines for creating subplots in Matplotlib. We\u2019ll start by setting up the notebook for plotting and importing the functions we will use: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np","title":"Multiple Subplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#pltaxes-subplots-by-hand","text":"The most basic method of creating an axes is to use the plt.axes function. As we\u2019ve seen previously, by default this creates a standard axes object that fills the entire fig\u2010 ure. plt.axes also takes an optional argument that is a list of four numbers in the figure coordinate system. These numbers represent [bottom, left, width, height] in the figure coordinate system, which ranges from 0 at the bottom left of the figure to 1 at the top right of the figure. ax1 = plt . axes () ax2 = plt . axes ([ 0.65 , 0.65 , 0.2 , 0.2 ]) The equivalent of this command within the object-oriented interface is fig.add_axes(). Let\u2019s use this to create two vertically stacked axes fig = plt . figure () ax1 = fig . add_axes ([ 0.1 , 0.5 , 0.8 , 0.4 ], xticklabels = [], ylim = ( - 1.2 , 1.2 )) ax2 = fig . add_axes ([ 0.1 , 0.1 , 0.8 , 0.4 ], xticklabels = [], ylim = ( - 1.2 , 1.2 )) x = np . linspace ( - 0 , 10 ) ax1 . plot ( np . sin ( x )) ax2 . plot ( np . cos ( x ));","title":"plt.axes: Subplots by Hand"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#pltsubplot-simple-grids-of-subplots","text":"Aligned columns or rows of subplots are a common enough need that Matplotlib has several convenience routines that make them easy to create. The lowest level of these is plt.subplot(), which creates a single subplot within a grid. As you can see, this command takes three integer arguments\u2014the number of rows, the number of col\u2010 umns, and the index of the plot to be created in this scheme, which runs from the upper left to the bottom right for i in range ( 1 , 7 ): plt . subplot ( 2 , 3 , i ) plt . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) The command plt.subplots_adjust can be used to adjust the spacing between these plots. object-oriented command, fig.add_subplot(): fig = plt . figure () fig . subplots_adjust ( hspace = 0.4 , wspace = 0.4 ) for i in range ( 1 , 7 ): ax = fig . add_subplot ( 2 , 3 , i ) ax . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' )","title":"plt.subplot: Simple Grids of Subplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#pltsubplots-the-whole-grid-in-one-go","text":"The approach just described can become quite tedious when you\u2019re creating a large grid of subplots, especially if you\u2019d like to hide the x- and y-axis labels on the inner plots. For this purpose, plt.subplots() is the easier tool to use (note the s at the end of subplots). Rather than creating a single subplot, this function creates a full grid of subplots in a single line, returning them in a NumPy array. The arguments are the number of rows and number of columns, along with optional keywords sharex and sharey, which allow you to specify the relationships between different axes. fig , ax = plt . subplots ( 2 , 3 , sharex = 'col' , sharey = 'row' ) for i in range ( 2 ): for j in range ( 3 ): ax [ i , j ] . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) fig","title":"plt.subplots: The Whole Grid in One Go"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#pltgridspec-more-complicated-arrangements","text":"To go beyond a regular grid to subplots that span multiple rows and columns, plt.GridSpec() is the best tool. The plt.GridSpec() object does not create a plot by itself; it is simply a convenient interface that is recognized by the plt.subplot() command. grid = plt . GridSpec ( 2 , 3 , wspace = 0.4 , hspace = 0.3 ) plt . subplot ( grid [ 0 , 0 ]) plt . subplot ( grid [ 0 , 1 :]) plt . subplot ( grid [ 1 ,: 2 ]) plt . subplot ( grid [ 1 , 2 ]); # Create some normally distributed data mean = [ 0 , 0 ] cov = [[ 1 , 1 ],[ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , cov , 3000 ) . T # Set up the axes with gridspec fig = plt . figure ( figsize = ( 6 , 6 )) grid = plt . GridSpec ( 4 , 4 , wspace = 0.2 , hspace = 0.2 ) main_ax = fig . add_subplot ( grid [: - 1 , 1 :]) y_hist = fig . add_subplot ( grid [: - 1 , 0 ], xticklabels = [], sharey = main_ax ) x_hist = fig . add_subplot ( grid [ - 1 , 1 :], yticklabels = [], sharex = main_ax ) # scatter points on the main axes main_ax . plot ( x , y , 'ok' , markersize = 3 , alpha = 0.2 ) # historgram on the attached axes x_hist . hist ( x , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'gray' ) x_hist . invert_yaxis () y_hist . hist ( y , 40 , histtype = 'stepfilled' , orientation = 'horizontal' , color = 'gray' ) y_hist . invert_xaxis ()","title":"plt.GridSpec: More Complicated Arrangements"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 10 - Text and Annotation \u00b6 Text and Annotation Transforms and Text Position Arrows and Annotation Text and Annotation \u00b6 Creating a good visualization involves guiding the reader so that the figure tells a story. In some cases, this story can be told in an entirely visual manner, without the need for added text, but in others, small textual cues and labels are necessary. Perhaps the most basic types of annotations you will use are axes labels and titles, but the options go beyond this. Let\u2019s take a look at some data and how we might visualize and annotate it to help convey interesting information. % matplotlib inline import matplotlib.pyplot as plt import matplotlib as mpl plt . style . use ( 'seaborn-whitegrid' ) import numpy as np import pandas as pd births = pd . read_csv ( '../data/births.csv' ) quartiles = np . percentile ( births [ 'births' ],[ 25 , 50 , 75 ]) mu , sig = quartiles [ 1 ], 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) births = births . query ( '(births > @mu - 5*@sig)& (births<@mu + 5* @sig)' ) births [ 'day' ] = births [ 'day' ] . astype ( int ) births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = \"%Y%m %d \" ) births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] /tmp/ipykernel_132909/914953373.py:15: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead. births_by_date.index=[pd.datetime(2012, month, day) for (month, day) in births_by_date.index] fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) <AxesSubplot:> fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot style = dict ( size = 10 , color = 'gray' ) ax . text ( '2012-1-1' , 3950 , \"New Year's Day\" , ** style ) ax . text ( '2012-7-4' , 4250 , \"Independence Day\" , ha = 'center' , ** style ) ax . text ( '2012-9-4' , 4850 , \"Labor Day\" , ha = 'center' , ** style ) ax . text ( '2012-10-31' , 4600 , \"Halloween\" , ha = 'right' , ** style ) ax . text ( '2012-11-25' , 4450 , \"Thanksgiving\" , ha = 'center' , ** style ) ax . text ( '2012-12-25' , 3850 , \"Christmas \" , ha = 'right' , ** style ) # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' )); Transforms and Text Position \u00b6 In the previous example, we anchored our text annotations to data locations. Some\u2010 times it\u2019s preferable to anchor the text to a position on the axes or figure, independent of the data. In Matplotlib, we do this by modifying the transform. Any graphics display framework needs some scheme for translating between coordinate systems. For example, a data point at x, y = 1, 1 needs to somehow be represented at a certain location on the figure, which in turn needs to be represented in pixels on the screen. Mathematically, such coordinate transformations are relatively straightforward, and Matplotlib has a well-developed set of tools that it uses internally to perform them (the tools can be explored in the Matplotlib.transforms submodule). The average user rarely needs to worry about the details of these transforms, but it is helpful knowledge to have when considering the placement of text on a figure. There are three predefined transforms that can be useful in this situation: ax.transData Transform associated with data coordinates ax.transAxes Transform associated with the axes (in units of axes dimensions) fig.transFigure Transform associated with the figure (in units of figure dimensions) # # %matplotlib inline % matplotlib notebook # import matplotlib.pyplot as plt # import matplotlib as mpl # plt.style.use('seaborn-whitegrid') # import numpy as np # import pandas as pd fig , ax = plt . subplots ( facecolor = 'lightgray' ) ax . axis ([ 0 , 10 , 0 , 10 ]) # transform=ax.transData is the default, ax . text ( 1 , 5 , \". Data: (1,5)\" , transform = ax . transData ) ax . text ( 5 , 3 , \". Data: (2,1.5)\" , transform = ax . transData ) ax . text ( 0.2 , 0.2 , \". Data: (0.2,0.2)\" , transform = ax . transData ) <IPython.core.display.Javascript object> Text(0.2, 0.2, '. Data: (0.2,0.2)') ax . set_xlim ( 0 , 2 ) ax . set_ylim ( - 6 , 6 ) fig <IPython.core.display.Javascript object> Arrows and Annotation \u00b6 Along with tick marks and text, another useful annotation mark is the simple arrow. Drawing arrows in Matplotlib is often much harder than you might hope. While there is a plt.arrow() function available, I wouldn\u2019t suggest using it; the arrows it creates are SVG objects that will be subject to the varying aspect ratio of your plots, and the result is rarely what the user intended. Instead, I\u2019d suggest using the plt.anno tate() function. This function creates some text and an arrow, and the arrows can be very flexibly specified. % matplotlib inline fig , ax = plt . subplots () x = np . linspace ( 0 , 20 , 1000 ) ax . plot ( x , np . cos ( x )) ax . axis ( 'equal' ) ax . annotate ( 'local maximum' , xy = ( 6.28 , 1 ), xytext = ( 10 , 4 ), arrowprops = dict ( facecolor = 'black' , shrink = 0.05 )) ax . annotate ( 'local minimum' , xy = ( 5 * np . pi , - 1 ), xytext = ( 2 , - 6 ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )); The arrow style is controlled through the arrowprops dictionary, which has numer\u2010 ous options available. These options are fairly well documented in Matplotlib\u2019s online documentation, fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot ax . annotate ( \"New Year's Day\" , xy = ( '2012-1-1' , 4100 ), xycoords = 'data' , xytext = ( 50 , - 30 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"arc3,rad=-0.2\" )) ax . annotate ( \"Independence Day\" , xy = ( '2012-7-4' , 4250 ), xycoords = 'data' , bbox = dict ( boxstyle = \"round\" , fc = \"none\" , ec = \"gray\" ), xytext = ( 10 , - 40 ), textcoords = 'offset points' , ha = 'center' , arrowprops = dict ( arrowstyle = \"->\" )) ax . annotate ( 'Labor Day' , xy = ( '2012-9-4' , 4850 ), xycoords = 'data' , ha = 'center' , xytext = ( 0 , - 20 ), textcoords = 'offset points' ) ax . annotate ( '' , xy = ( '2012-9-1' , 4850 ), xytext = ( '2012-9-7' , 4850 ), xycoords = 'data' , textcoords = 'data' , arrowprops = { 'arrowstyle' : '|-|,widthA=0.2,widthB=0.2' , }) ax . annotate ( 'Halloween' , xy = ( '2012-10-31' , 4600 ), xycoords = 'data' , xytext = ( - 80 , - 40 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"fancy\" , fc = \"0.6\" , ec = \"none\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )) ax . annotate ( 'Thanksgiving' , xy = ( '2012-11-25' , 4500 ), xycoords = 'data' , xytext = ( - 120 , - 60 ), textcoords = 'offset points' , bbox = dict ( boxstyle = \"round4,pad=.5\" , fc = \"0.9\" ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle,angleA=0,angleB=80,rad=20\" )) ax . annotate ( 'Christmas' , xy = ( '2012-12-25' , 3850 ), xycoords = 'data' , xytext = ( - 30 , 0 ), textcoords = 'offset points' , size = 13 , ha = 'right' , va = \"center\" , bbox = dict ( boxstyle = \"round\" , alpha = 0.1 ), arrowprops = dict ( arrowstyle = \"wedge,tail_width=0.5\" , alpha = 0.1 )); # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' )); ax . set_ylim ( 3600 , 5400 );","title":"10text and annotation Example"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#10-text-and-annotation","text":"Text and Annotation Transforms and Text Position Arrows and Annotation","title":"10 - Text and Annotation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#text-and-annotation","text":"Creating a good visualization involves guiding the reader so that the figure tells a story. In some cases, this story can be told in an entirely visual manner, without the need for added text, but in others, small textual cues and labels are necessary. Perhaps the most basic types of annotations you will use are axes labels and titles, but the options go beyond this. Let\u2019s take a look at some data and how we might visualize and annotate it to help convey interesting information. % matplotlib inline import matplotlib.pyplot as plt import matplotlib as mpl plt . style . use ( 'seaborn-whitegrid' ) import numpy as np import pandas as pd births = pd . read_csv ( '../data/births.csv' ) quartiles = np . percentile ( births [ 'births' ],[ 25 , 50 , 75 ]) mu , sig = quartiles [ 1 ], 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) births = births . query ( '(births > @mu - 5*@sig)& (births<@mu + 5* @sig)' ) births [ 'day' ] = births [ 'day' ] . astype ( int ) births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = \"%Y%m %d \" ) births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] /tmp/ipykernel_132909/914953373.py:15: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead. births_by_date.index=[pd.datetime(2012, month, day) for (month, day) in births_by_date.index] fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) <AxesSubplot:> fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot style = dict ( size = 10 , color = 'gray' ) ax . text ( '2012-1-1' , 3950 , \"New Year's Day\" , ** style ) ax . text ( '2012-7-4' , 4250 , \"Independence Day\" , ha = 'center' , ** style ) ax . text ( '2012-9-4' , 4850 , \"Labor Day\" , ha = 'center' , ** style ) ax . text ( '2012-10-31' , 4600 , \"Halloween\" , ha = 'right' , ** style ) ax . text ( '2012-11-25' , 4450 , \"Thanksgiving\" , ha = 'center' , ** style ) ax . text ( '2012-12-25' , 3850 , \"Christmas \" , ha = 'right' , ** style ) # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' ));","title":"Text and Annotation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#transforms-and-text-position","text":"In the previous example, we anchored our text annotations to data locations. Some\u2010 times it\u2019s preferable to anchor the text to a position on the axes or figure, independent of the data. In Matplotlib, we do this by modifying the transform. Any graphics display framework needs some scheme for translating between coordinate systems. For example, a data point at x, y = 1, 1 needs to somehow be represented at a certain location on the figure, which in turn needs to be represented in pixels on the screen. Mathematically, such coordinate transformations are relatively straightforward, and Matplotlib has a well-developed set of tools that it uses internally to perform them (the tools can be explored in the Matplotlib.transforms submodule). The average user rarely needs to worry about the details of these transforms, but it is helpful knowledge to have when considering the placement of text on a figure. There are three predefined transforms that can be useful in this situation: ax.transData Transform associated with data coordinates ax.transAxes Transform associated with the axes (in units of axes dimensions) fig.transFigure Transform associated with the figure (in units of figure dimensions) # # %matplotlib inline % matplotlib notebook # import matplotlib.pyplot as plt # import matplotlib as mpl # plt.style.use('seaborn-whitegrid') # import numpy as np # import pandas as pd fig , ax = plt . subplots ( facecolor = 'lightgray' ) ax . axis ([ 0 , 10 , 0 , 10 ]) # transform=ax.transData is the default, ax . text ( 1 , 5 , \". Data: (1,5)\" , transform = ax . transData ) ax . text ( 5 , 3 , \". Data: (2,1.5)\" , transform = ax . transData ) ax . text ( 0.2 , 0.2 , \". Data: (0.2,0.2)\" , transform = ax . transData ) <IPython.core.display.Javascript object> Text(0.2, 0.2, '. Data: (0.2,0.2)') ax . set_xlim ( 0 , 2 ) ax . set_ylim ( - 6 , 6 ) fig <IPython.core.display.Javascript object>","title":"Transforms and Text Position"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#arrows-and-annotation","text":"Along with tick marks and text, another useful annotation mark is the simple arrow. Drawing arrows in Matplotlib is often much harder than you might hope. While there is a plt.arrow() function available, I wouldn\u2019t suggest using it; the arrows it creates are SVG objects that will be subject to the varying aspect ratio of your plots, and the result is rarely what the user intended. Instead, I\u2019d suggest using the plt.anno tate() function. This function creates some text and an arrow, and the arrows can be very flexibly specified. % matplotlib inline fig , ax = plt . subplots () x = np . linspace ( 0 , 20 , 1000 ) ax . plot ( x , np . cos ( x )) ax . axis ( 'equal' ) ax . annotate ( 'local maximum' , xy = ( 6.28 , 1 ), xytext = ( 10 , 4 ), arrowprops = dict ( facecolor = 'black' , shrink = 0.05 )) ax . annotate ( 'local minimum' , xy = ( 5 * np . pi , - 1 ), xytext = ( 2 , - 6 ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )); The arrow style is controlled through the arrowprops dictionary, which has numer\u2010 ous options available. These options are fairly well documented in Matplotlib\u2019s online documentation, fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot ax . annotate ( \"New Year's Day\" , xy = ( '2012-1-1' , 4100 ), xycoords = 'data' , xytext = ( 50 , - 30 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"arc3,rad=-0.2\" )) ax . annotate ( \"Independence Day\" , xy = ( '2012-7-4' , 4250 ), xycoords = 'data' , bbox = dict ( boxstyle = \"round\" , fc = \"none\" , ec = \"gray\" ), xytext = ( 10 , - 40 ), textcoords = 'offset points' , ha = 'center' , arrowprops = dict ( arrowstyle = \"->\" )) ax . annotate ( 'Labor Day' , xy = ( '2012-9-4' , 4850 ), xycoords = 'data' , ha = 'center' , xytext = ( 0 , - 20 ), textcoords = 'offset points' ) ax . annotate ( '' , xy = ( '2012-9-1' , 4850 ), xytext = ( '2012-9-7' , 4850 ), xycoords = 'data' , textcoords = 'data' , arrowprops = { 'arrowstyle' : '|-|,widthA=0.2,widthB=0.2' , }) ax . annotate ( 'Halloween' , xy = ( '2012-10-31' , 4600 ), xycoords = 'data' , xytext = ( - 80 , - 40 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"fancy\" , fc = \"0.6\" , ec = \"none\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )) ax . annotate ( 'Thanksgiving' , xy = ( '2012-11-25' , 4500 ), xycoords = 'data' , xytext = ( - 120 , - 60 ), textcoords = 'offset points' , bbox = dict ( boxstyle = \"round4,pad=.5\" , fc = \"0.9\" ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle,angleA=0,angleB=80,rad=20\" )) ax . annotate ( 'Christmas' , xy = ( '2012-12-25' , 3850 ), xycoords = 'data' , xytext = ( - 30 , 0 ), textcoords = 'offset points' , size = 13 , ha = 'right' , va = \"center\" , bbox = dict ( boxstyle = \"round\" , alpha = 0.1 ), arrowprops = dict ( arrowstyle = \"wedge,tail_width=0.5\" , alpha = 0.1 )); # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' )); ax . set_ylim ( 3600 , 5400 );","title":"Arrows and Annotation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 11 - Customizing Ticks \u00b6 Customizing Ticks Major and Minor Ticks Hiding Ticks or Labels Reducing or Increasing the Number of Ticks Fancy Tick Formats Customizing Ticks \u00b6 Matplotlib\u2019s default tick locators and formatters are designed to be generally sufficient in many common situations, but are in no way optimal for every plot. Major and Minor Ticks \u00b6 Within each axis, there is the concept of a major tick mark and a minor tick mark. As the names would imply, major ticks are usually bigger or more pronounced, while minor ticks are usually smaller. By default, Matplotlib rarely makes use of minor ticks, but one place you can see them is within logarithmic plots % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np ax = plt . axes ( xscale = 'log' , yscale = 'log' ) print ( ax . xaxis . get_major_locator ()) print ( ax . xaxis . get_minor_locator ()) <matplotlib.ticker.LogLocator object at 0x7faa7bb8f640> <matplotlib.ticker.LogLocator object at 0x7faa7be3baf0> print ( ax . xaxis . get_major_formatter ()) print ( ax . xaxis . get_minor_formatter ()) <matplotlib.ticker.LogFormatterSciNotation object at 0x7faa7bb7ca60> <matplotlib.ticker.LogFormatterSciNotation object at 0x7faab054b6d0> Hiding Ticks or Labels \u00b6 Perhaps the most common tick/label formatting operation is the act of hiding ticks or labels. We can do this using plt.NullLocator() and plt.NullFormatter(), as shown here ax = plt . axes () ax . plot ( np . random . rand ( 50 )) ax . yaxis . set_major_locator ( plt . NullLocator ()) ax . yaxis . set_major_formatter ( plt . NullFormatter ()) ax = plt . axes () ax . plot ( np . random . rand ( 50 )) fig , ax = plt . subplots ( 5 , 5 , figsize = ( 5 , 5 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) #Get some face images from scikit-learn from sklearn.datasets import fetch_olivetti_faces faces = fetch_olivetti_faces () . images for i in range ( 5 ): for j in range ( 5 ): ax [ i , j ] . xaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . yaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . imshow ( faces [ 10 * i + j ], cmap = 'bone' ) Reducing or Increasing the Number of Ticks \u00b6 One common problem with the default settings is that smaller subplots can end up with crowded labels. fig , ac = plt . subplots ( 4 , 4 , sharex = True , sharey = True ) Particularly for the x ticks, the numbers nearly overlap, making them quite difficult to decipher. We can fix this with the plt.MaxNLocator(), which allows us to specify the maximum number of ticks that will be displayed. Given this maximum number, Mat\u2010 plotlib will use internal logic to choose the particular tick locations for axi in ax . flat : axi . xaxis . set_major_locator ( plt . MaxNLocator ( 3 )) axi . yaxis . set_major_locator ( plt . MaxNLocator ( 3 )) fig Fancy Tick Formats \u00b6 Matplotlib\u2019s default tick formatting can leave a lot to be desired; it works well as a broad default, but sometimes you\u2019d like to do something more # Plot a sine and cosine curve fig , ax = plt . subplots () x = np . linspace ( 0 , 3 * np . pi , 1000 ) ax . plot ( x , np . sin ( x ), lw = 3 , label = 'Sine' ) ax . plot ( x , np . cos ( x ), lw = 3 , label = 'Cosine' ) # Set up grid, legend, and limits ax . grid ( True ) ax . legend ( frameon = False ) ax . axis ( 'equal' ) ax . set_xlim ( 0 , 3 * np . pi ); ax . xaxis . set_major_locator ( plt . MultipleLocator ( np . pi / 2 )) ax . xaxis . set_minor_locator ( plt . MultipleLocator ( np . pi / 4 )) fig def format_func ( value , tick_number ): # find number of multiples of pi/2 N = int ( np . round ( 2 * value / np . pi )) if N == 0 : return \"0\" elif N == 1 : return r \"$\\pi/2$\" elif N == 2 : return r \"$\\pi$\" elif N % 2 > 0 : return r \"$ {0} \\pi/2$\" . format ( N ) else : return r \"$ {0} \\pi$\" . format ( N // 2 ) ax . xaxis . set_major_formatter ( plt . FuncFormatter ( format_func )) fig This is much better! Notice that we\u2019ve made use of Matplotlib\u2019s LaTeX support, speci\u2010 fied by enclosing the string within dollar signs. This is very convenient for display of mathematical symbols and formulae; in this case, \u201c \\(\\pi\\) \u201d is rendered as the Greek character \u03c0. The plt.FuncFormatter() offers extremely fine-grained control over the appearance of your plot ticks, and comes in very handy when you\u2019re preparing plots for presenta\u2010 tion or publication.","title":"11customizing ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#11-customizing-ticks","text":"Customizing Ticks Major and Minor Ticks Hiding Ticks or Labels Reducing or Increasing the Number of Ticks Fancy Tick Formats","title":"11 - Customizing Ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#customizing-ticks","text":"Matplotlib\u2019s default tick locators and formatters are designed to be generally sufficient in many common situations, but are in no way optimal for every plot.","title":"Customizing Ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#major-and-minor-ticks","text":"Within each axis, there is the concept of a major tick mark and a minor tick mark. As the names would imply, major ticks are usually bigger or more pronounced, while minor ticks are usually smaller. By default, Matplotlib rarely makes use of minor ticks, but one place you can see them is within logarithmic plots % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np ax = plt . axes ( xscale = 'log' , yscale = 'log' ) print ( ax . xaxis . get_major_locator ()) print ( ax . xaxis . get_minor_locator ()) <matplotlib.ticker.LogLocator object at 0x7faa7bb8f640> <matplotlib.ticker.LogLocator object at 0x7faa7be3baf0> print ( ax . xaxis . get_major_formatter ()) print ( ax . xaxis . get_minor_formatter ()) <matplotlib.ticker.LogFormatterSciNotation object at 0x7faa7bb7ca60> <matplotlib.ticker.LogFormatterSciNotation object at 0x7faab054b6d0>","title":"Major and Minor Ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#hiding-ticks-or-labels","text":"Perhaps the most common tick/label formatting operation is the act of hiding ticks or labels. We can do this using plt.NullLocator() and plt.NullFormatter(), as shown here ax = plt . axes () ax . plot ( np . random . rand ( 50 )) ax . yaxis . set_major_locator ( plt . NullLocator ()) ax . yaxis . set_major_formatter ( plt . NullFormatter ()) ax = plt . axes () ax . plot ( np . random . rand ( 50 )) fig , ax = plt . subplots ( 5 , 5 , figsize = ( 5 , 5 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) #Get some face images from scikit-learn from sklearn.datasets import fetch_olivetti_faces faces = fetch_olivetti_faces () . images for i in range ( 5 ): for j in range ( 5 ): ax [ i , j ] . xaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . yaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . imshow ( faces [ 10 * i + j ], cmap = 'bone' )","title":"Hiding Ticks or Labels"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#reducing-or-increasing-the-number-of-ticks","text":"One common problem with the default settings is that smaller subplots can end up with crowded labels. fig , ac = plt . subplots ( 4 , 4 , sharex = True , sharey = True ) Particularly for the x ticks, the numbers nearly overlap, making them quite difficult to decipher. We can fix this with the plt.MaxNLocator(), which allows us to specify the maximum number of ticks that will be displayed. Given this maximum number, Mat\u2010 plotlib will use internal logic to choose the particular tick locations for axi in ax . flat : axi . xaxis . set_major_locator ( plt . MaxNLocator ( 3 )) axi . yaxis . set_major_locator ( plt . MaxNLocator ( 3 )) fig","title":"Reducing or Increasing the Number of Ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#fancy-tick-formats","text":"Matplotlib\u2019s default tick formatting can leave a lot to be desired; it works well as a broad default, but sometimes you\u2019d like to do something more # Plot a sine and cosine curve fig , ax = plt . subplots () x = np . linspace ( 0 , 3 * np . pi , 1000 ) ax . plot ( x , np . sin ( x ), lw = 3 , label = 'Sine' ) ax . plot ( x , np . cos ( x ), lw = 3 , label = 'Cosine' ) # Set up grid, legend, and limits ax . grid ( True ) ax . legend ( frameon = False ) ax . axis ( 'equal' ) ax . set_xlim ( 0 , 3 * np . pi ); ax . xaxis . set_major_locator ( plt . MultipleLocator ( np . pi / 2 )) ax . xaxis . set_minor_locator ( plt . MultipleLocator ( np . pi / 4 )) fig def format_func ( value , tick_number ): # find number of multiples of pi/2 N = int ( np . round ( 2 * value / np . pi )) if N == 0 : return \"0\" elif N == 1 : return r \"$\\pi/2$\" elif N == 2 : return r \"$\\pi$\" elif N % 2 > 0 : return r \"$ {0} \\pi/2$\" . format ( N ) else : return r \"$ {0} \\pi$\" . format ( N // 2 ) ax . xaxis . set_major_formatter ( plt . FuncFormatter ( format_func )) fig This is much better! Notice that we\u2019ve made use of Matplotlib\u2019s LaTeX support, speci\u2010 fied by enclosing the string within dollar signs. This is very convenient for display of mathematical symbols and formulae; in this case, \u201c \\(\\pi\\) \u201d is rendered as the Greek character \u03c0. The plt.FuncFormatter() offers extremely fine-grained control over the appearance of your plot ticks, and comes in very handy when you\u2019re preparing plots for presenta\u2010 tion or publication.","title":"Fancy Tick Formats"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 12 - Customizing Matplotlib: Configurations and Stylesheets \u00b6 Customizing Matplotlib: Configurations and Stylesheets Plot Customization by Hand Changing the Defaults: rcParams Stylesheets Default style Customizing Matplotlib: Configurations and Stylesheets \u00b6 Matplotlib\u2019s default plot settings are often the subject of complaint among its users. While much is slated to change in the 2.0 Matplotlib release, the ability to customize default settings helps bring the package in line with your own aesthetic preferences. Here we\u2019ll walk through some of Matplotlib\u2019s runtime configuration (rc) options, and take a look at the newer stylesheets feature, which contains some nice sets of default configurations. Plot Customization by Hand \u00b6 Throughout this chapter, we\u2019ve seen how it is possible to tweak individual plot set\u2010 tings to end up with something that looks a little bit nicer than the default. It\u2019s possi\u2010 ble to do these customizations for each individual plot. For example, here is a fairly drab default histogram import matplotlib.pyplot as plt plt . style . use ( 'classic' ) import numpy as np % matplotlib inline x = np . random . randn ( 1000 ) plt . hist ( x ); # We can adjust this by hand to make it a much more visually pleasing plot # use a gray backgoung ax = plt . axes () ax . set_axisbelow ( True ) # draw solid white grid line plt . grid ( color = 'w' , linestyle = 'solid' ) # hide axis spines for spine in ax . spines . values (): spine . set_visible ( False ) #hide top and right ticks ax . xaxis . tick_bottom () ax . yaxis . tick_left () #lighten ticks and labels ax . tick_params ( color = 'gray' , direction = 'out' ) for tick in ax . get_xticklabels (): tick . set_color ( 'gray' ) for tick in ax . get_yticklabels (): tick . set_color ( 'gray' ) # control face and edge color of histogram ax . hist ( x , edgecolor = '#E6E6E6' , color = '#EE6666' ) (array([ 12., 37., 98., 173., 257., 213., 124., 59., 23., 4.]), array([-2.90788595, -2.30201003, -1.69613411, -1.09025818, -0.48438226, 0.12149367, 0.72736959, 1.33324551, 1.93912144, 2.54499736, 3.15087329]), <BarContainer object of 10 artists>) This looks better, and you may recognize the look as inspired by the look of the R language\u2019s ggplot visualization package. But this took a whole lot of effort! We defi\u2010 nitely do not want to have to do all that tweaking each time we create a plot. Fortu\u2010 nately, there is a way to adjust these defaults once in a way that will work for all plots. Changing the Defaults: rcParams \u00b6 Each time Matplotlib loads, it defines a runtime configuration (rc) containing the default styles for every plot element you create. You can adjust this configuration at any time using the plt.rc convenience routine. Let\u2019s see what it looks like to modify the rc parameters so that our default plot will look similar to what we did before. We\u2019ll start by saving a copy of the current rcParams dictionary, so we can easily reset these changes in the current session: Ipython_default = plt . rcParams . copy () from matplotlib import cycler colors = cycler ( 'color' , [ '#EE6666' , '#3388BB' , '#9988DD' , '#EECC55' , '#88BB44' , '#FFBBBB' ]) plt . rc ( 'axes' , facecolor = '#E6E6E6' , edgecolor = 'none' , axisbelow = True , grid = True , prop_cycle = colors ) plt . rc ( 'grid' , color = 'w' , linestyle = 'solid' ) plt . rc ( 'xtick' , direction = 'out' , color = 'gray' ) plt . rc ( 'ytick' , direction = 'out' , color = 'gray' ) plt . rc ( 'patch' , edgecolor = '#E6E6E6' ) plt . rc ( 'lines' , linewidth = 2 ) # With these settings defined, we can now create a plot and see our settings in action plt . hist ( x ); # Let\u2019s see what simple line plots look like with these rc parameters for i in range ( 4 ): plt . plot ( np . random . rand ( 10 )) I find this much more aesthetically pleasing than the default styling. If you disagree with my aesthetic sense, the good news is that you can adjust the rc parameters to suit your own tastes! These settings can be saved in a .matplotlibrc file, which you can read about in the Matplotlib documentation. That said, I prefer to customize Mat\u2010 plotlib using its stylesheets instead. Stylesheets \u00b6 Even if you don\u2019t create your own style, the stylesheets included by default are extremely useful. The available styles are listed in plt.style.available\u2014here I\u2019ll list only the first five for brevity: plt . style . available [: 5 ] ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh'] plt . style . use ( 'Solarize_Light2' ) with plt . style . context ( 'Solarize_Light2' ): make_a_plot () NameError: name 'make_a_plot' is not defined def hist_and_lines (): np . random . seed ( 0 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 11 , 4 )) ax [ 0 ] . hist ( np . random . randn ( 1000 )) for i in range ( 3 ): ax [ 1 ] . plot ( np . random . rand ( 100 )) ax [ 1 ] . legend ([ 'a' , 'b' , 'c' ], loc = 'lower left' ) Default style \u00b6 The default style is what we\u2019ve been seeing so far throughout the book; we\u2019ll start with that. First, let\u2019s reset our runtime configuration to the notebook default: # reset rcParams plt . rcParams . update ( Ipython_default ); # let's see how it looks hist_and_lines () # let use one of the styles avialable plt . style . available [:] ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10'] # Using fivethrityeight with plt . style . context ( 'fivethirtyeight' ): hist_and_lines () # Using ggplot with plt . style . context ( 'ggplot' ): hist_and_lines () Bayesian Methods for Hackers style There is a very nice short online book called Probabilistic Programming and Bayesian Methods for Hackers; it features figures created with Matplotlib, and uses a nice set of rc parameters to create a consistent and visually appealing style throughout the book. This style is reproduced in the bmh stylesheet # Using bmh with plt . style . context ( 'bmh' ): hist_and_lines () Dark background For figures used within presentations, it is often useful to have a dark rather than light background. The dark_background style provides this with plt . style . context ( 'dark_background' ): hist_and_lines () Grayscale Sometimes you might find yourself preparing figures for a print publication that does not accept color figures. with plt . style . context ( 'grayscale' ): hist_and_lines () Seaborn style Matplotlib also has stylesheets inspired by the Seaborn library (discussed more fully in \u201cVisualization with Seaborn\u201d on page 311). As we will see, these styles are loaded automatically when Seaborn is imported into a notebook. import seaborn hist_and_lines () with plt . style . context ( 'seaborn' ): hist_and_lines ()","title":"12customizing matplotlib configuration and stylesheets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#12-customizing-matplotlib-configurations-and-stylesheets","text":"Customizing Matplotlib: Configurations and Stylesheets Plot Customization by Hand Changing the Defaults: rcParams Stylesheets Default style","title":"12 - Customizing Matplotlib: Configurations and Stylesheets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#customizing-matplotlib-configurations-and-stylesheets","text":"Matplotlib\u2019s default plot settings are often the subject of complaint among its users. While much is slated to change in the 2.0 Matplotlib release, the ability to customize default settings helps bring the package in line with your own aesthetic preferences. Here we\u2019ll walk through some of Matplotlib\u2019s runtime configuration (rc) options, and take a look at the newer stylesheets feature, which contains some nice sets of default configurations.","title":"Customizing Matplotlib: Configurations and Stylesheets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#plot-customization-by-hand","text":"Throughout this chapter, we\u2019ve seen how it is possible to tweak individual plot set\u2010 tings to end up with something that looks a little bit nicer than the default. It\u2019s possi\u2010 ble to do these customizations for each individual plot. For example, here is a fairly drab default histogram import matplotlib.pyplot as plt plt . style . use ( 'classic' ) import numpy as np % matplotlib inline x = np . random . randn ( 1000 ) plt . hist ( x ); # We can adjust this by hand to make it a much more visually pleasing plot # use a gray backgoung ax = plt . axes () ax . set_axisbelow ( True ) # draw solid white grid line plt . grid ( color = 'w' , linestyle = 'solid' ) # hide axis spines for spine in ax . spines . values (): spine . set_visible ( False ) #hide top and right ticks ax . xaxis . tick_bottom () ax . yaxis . tick_left () #lighten ticks and labels ax . tick_params ( color = 'gray' , direction = 'out' ) for tick in ax . get_xticklabels (): tick . set_color ( 'gray' ) for tick in ax . get_yticklabels (): tick . set_color ( 'gray' ) # control face and edge color of histogram ax . hist ( x , edgecolor = '#E6E6E6' , color = '#EE6666' ) (array([ 12., 37., 98., 173., 257., 213., 124., 59., 23., 4.]), array([-2.90788595, -2.30201003, -1.69613411, -1.09025818, -0.48438226, 0.12149367, 0.72736959, 1.33324551, 1.93912144, 2.54499736, 3.15087329]), <BarContainer object of 10 artists>) This looks better, and you may recognize the look as inspired by the look of the R language\u2019s ggplot visualization package. But this took a whole lot of effort! We defi\u2010 nitely do not want to have to do all that tweaking each time we create a plot. Fortu\u2010 nately, there is a way to adjust these defaults once in a way that will work for all plots.","title":"Plot Customization by Hand"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#changing-the-defaults-rcparams","text":"Each time Matplotlib loads, it defines a runtime configuration (rc) containing the default styles for every plot element you create. You can adjust this configuration at any time using the plt.rc convenience routine. Let\u2019s see what it looks like to modify the rc parameters so that our default plot will look similar to what we did before. We\u2019ll start by saving a copy of the current rcParams dictionary, so we can easily reset these changes in the current session: Ipython_default = plt . rcParams . copy () from matplotlib import cycler colors = cycler ( 'color' , [ '#EE6666' , '#3388BB' , '#9988DD' , '#EECC55' , '#88BB44' , '#FFBBBB' ]) plt . rc ( 'axes' , facecolor = '#E6E6E6' , edgecolor = 'none' , axisbelow = True , grid = True , prop_cycle = colors ) plt . rc ( 'grid' , color = 'w' , linestyle = 'solid' ) plt . rc ( 'xtick' , direction = 'out' , color = 'gray' ) plt . rc ( 'ytick' , direction = 'out' , color = 'gray' ) plt . rc ( 'patch' , edgecolor = '#E6E6E6' ) plt . rc ( 'lines' , linewidth = 2 ) # With these settings defined, we can now create a plot and see our settings in action plt . hist ( x ); # Let\u2019s see what simple line plots look like with these rc parameters for i in range ( 4 ): plt . plot ( np . random . rand ( 10 )) I find this much more aesthetically pleasing than the default styling. If you disagree with my aesthetic sense, the good news is that you can adjust the rc parameters to suit your own tastes! These settings can be saved in a .matplotlibrc file, which you can read about in the Matplotlib documentation. That said, I prefer to customize Mat\u2010 plotlib using its stylesheets instead.","title":"Changing the Defaults: rcParams"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#stylesheets","text":"Even if you don\u2019t create your own style, the stylesheets included by default are extremely useful. The available styles are listed in plt.style.available\u2014here I\u2019ll list only the first five for brevity: plt . style . available [: 5 ] ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh'] plt . style . use ( 'Solarize_Light2' ) with plt . style . context ( 'Solarize_Light2' ): make_a_plot () NameError: name 'make_a_plot' is not defined def hist_and_lines (): np . random . seed ( 0 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 11 , 4 )) ax [ 0 ] . hist ( np . random . randn ( 1000 )) for i in range ( 3 ): ax [ 1 ] . plot ( np . random . rand ( 100 )) ax [ 1 ] . legend ([ 'a' , 'b' , 'c' ], loc = 'lower left' )","title":"Stylesheets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#default-style","text":"The default style is what we\u2019ve been seeing so far throughout the book; we\u2019ll start with that. First, let\u2019s reset our runtime configuration to the notebook default: # reset rcParams plt . rcParams . update ( Ipython_default ); # let's see how it looks hist_and_lines () # let use one of the styles avialable plt . style . available [:] ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10'] # Using fivethrityeight with plt . style . context ( 'fivethirtyeight' ): hist_and_lines () # Using ggplot with plt . style . context ( 'ggplot' ): hist_and_lines () Bayesian Methods for Hackers style There is a very nice short online book called Probabilistic Programming and Bayesian Methods for Hackers; it features figures created with Matplotlib, and uses a nice set of rc parameters to create a consistent and visually appealing style throughout the book. This style is reproduced in the bmh stylesheet # Using bmh with plt . style . context ( 'bmh' ): hist_and_lines () Dark background For figures used within presentations, it is often useful to have a dark rather than light background. The dark_background style provides this with plt . style . context ( 'dark_background' ): hist_and_lines () Grayscale Sometimes you might find yourself preparing figures for a print publication that does not accept color figures. with plt . style . context ( 'grayscale' ): hist_and_lines () Seaborn style Matplotlib also has stylesheets inspired by the Seaborn library (discussed more fully in \u201cVisualization with Seaborn\u201d on page 311). As we will see, these styles are loaded automatically when Seaborn is imported into a notebook. import seaborn hist_and_lines () with plt . style . context ( 'seaborn' ): hist_and_lines ()","title":"Default style"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 13 - Three-Dimensional Plotting in Matplotlib \u00b6 Three-Dimensional Plotting in Matplotlib Three-Dimensional Points and Lines Three-Dimensional Contour Plots Wireframes and Surface Plots Surface Triangulations Three-Dimensional Plotting in Matplotlib \u00b6 Matplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib\u2019s two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. We enable three-dimensional plots by importing the mplot3d toolkit, included with the main Matplotlib installation from mpl_toolkits import mplot3d % matplotlib inline import numpy as np import matplotlib.pyplot as plt fig = plt . figure () ax = plt . axes ( projection = '3d' ) With this 3D axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code. % matplotlib notebook Three-Dimensional Points and Lines \u00b6 The most basic three-dimensional plot is a line or scatter plot created from sets of (x,y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, we can create these using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to \u201cSimple Line Plots\u201d and \u201cSimple Scatter Plots\u201d for more information on controlling the output. ax = plt . axes ( projection = '3d' ) # data for 3d line zline = np . linspace ( 0 , 15 , 1000 ) xline = np . sin ( zline ) yline = np . cos ( zline ) ax . plot3D ( xline , yline , zline , 'gray' ) # data for 3d scatter points zdata = 15 * np . random . random ( 100 ) xdata = np . sin ( zdata ) + 0.1 * np . random . randn ( 100 ) ydata = np . cos ( zdata ) + 0.1 * np . random . randn ( 100 ) ax . scatter3D ( xdata , ydata , zdata , c = zdata , cmap = 'Greens' ); <IPython.core.display.Javascript object> Three-Dimensional Contour Plots \u00b6 Analogous to the contour plots we explored in \u201cDensity and Contour Plots\u201d, mplot3d contains tools to create three-dimensional relief plots using the same inputs. Like two-dimensional ax.contour plots, ax.contour3D requires all the input data to be in the form of two-dimensional regular grids, with the Z data evaluated at each point. Here we\u2019ll show a three-dimensional contour diagram of a three-dimensional sinusoidal function def f ( x , y ): return np . sin ( np . sqrt ( x ** 2 + y ** 2 )) x = np . linspace ( - 6 , 6 , 30 ) y = np . linspace ( - 6 , 6 , 30 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . contour3D ( X , Y , Z , 50 , cmap = 'binary' ) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 'y' ) ax . set_zlabel ( 'z' ); <IPython.core.display.Javascript object> Sometimes the default viewing angle is not optimal, in which case we can use the view_init method to set the elevation and azimuthal angles. In this example (the result of which is shown below, we\u2019ll use an elevation of 60 degrees (that is, 60 degrees above the x-y plane) and an azimuth of 35 degrees (that is, rotated 35 degrees counter-clockwise about the z-axis): ax . view_init ( 60 , 35 ) fig <IPython.core.display.Javascript object> Wireframes and Surface Plots \u00b6 Two other types of three-dimensional plots that work on gridded data are wireframes and surface plots. These take a grid of values and project it onto the specified three- dimensional surface, and can make the resulting three-dimensional forms quite easy to visualize. Here\u2019s an example using a wireframe fit = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_wireframe ( X , Y , Z , color = 'black' ) ax . set_title ( 'WireFrame' ); <IPython.core.display.Javascript object> A surface plot is like a wireframe plot, but each face of the wireframe is a filled poly\u2010 gon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ) ax . set_title ( 'Surface' ); <IPython.core.display.Javascript object> Note that though the grid of values for a surface plot needs to be two-dimensional, it need not be rectilinear. Here is an example of creating a partial polar grid, which when used with the surface3D plot can give us a slice into the function we\u2019re visualiz\u2010 ing fig = plt . figure () r = np . linspace ( 0 , 6 , 20 ) theta = np . linspace ( - 0.9 * np . pi , 0.8 * np . pi , 40 ) r , theta = np . meshgrid ( r , theta ) X = r * np . sin ( theta ) Y = r * np . cos ( theta ) Z = f ( X , Y ) ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ); <IPython.core.display.Javascript object> Surface Triangulations \u00b6 For some applications, the evenly sampled grids required by the preceding routines are overly restrictive and inconvenient. In these situations, the triangulation-based plots can be very useful. What if rather than an even draw from a Cartesian or a polar grid, we instead have a set of random draws? theta = 2 * np . pi * np . random . random ( 1000 ) r = 6 * np . random . random ( 1000 ) x = np . ravel ( r * np . sin ( theta )) y = np . ravel ( r * np . cos ( theta )) z = f ( x , y ) fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . scatter ( x , y , z , c = z , cmap = 'viridis' , linewidth = 0.5 ) <IPython.core.display.Javascript object> <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f86f7843820> This leaves a lot to be desired. The function that will help us in this case is ax.plot_trisurf, which creates a surface by first finding a set of triangles formed between adjacent points fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , cmap = 'viridis' , edgecolor = 'none' ) <IPython.core.display.Javascript object> <mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x7f86f7206460> The result is certainly not as clean as when it is plotted with a grid, but the flexibility of such a triangulation allows for some really interesting three-dimensional plots. For example, it is actually possible to plot a three-dimensional M\u00f6bius strip using this, as we\u2019ll see next.","title":"13threedimensional plotting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#13-three-dimensional-plotting-in-matplotlib","text":"Three-Dimensional Plotting in Matplotlib Three-Dimensional Points and Lines Three-Dimensional Contour Plots Wireframes and Surface Plots Surface Triangulations","title":"13 - Three-Dimensional Plotting in Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#three-dimensional-plotting-in-matplotlib","text":"Matplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib\u2019s two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. We enable three-dimensional plots by importing the mplot3d toolkit, included with the main Matplotlib installation from mpl_toolkits import mplot3d % matplotlib inline import numpy as np import matplotlib.pyplot as plt fig = plt . figure () ax = plt . axes ( projection = '3d' ) With this 3D axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code. % matplotlib notebook","title":"Three-Dimensional Plotting in Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#three-dimensional-points-and-lines","text":"The most basic three-dimensional plot is a line or scatter plot created from sets of (x,y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, we can create these using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to \u201cSimple Line Plots\u201d and \u201cSimple Scatter Plots\u201d for more information on controlling the output. ax = plt . axes ( projection = '3d' ) # data for 3d line zline = np . linspace ( 0 , 15 , 1000 ) xline = np . sin ( zline ) yline = np . cos ( zline ) ax . plot3D ( xline , yline , zline , 'gray' ) # data for 3d scatter points zdata = 15 * np . random . random ( 100 ) xdata = np . sin ( zdata ) + 0.1 * np . random . randn ( 100 ) ydata = np . cos ( zdata ) + 0.1 * np . random . randn ( 100 ) ax . scatter3D ( xdata , ydata , zdata , c = zdata , cmap = 'Greens' ); <IPython.core.display.Javascript object>","title":"Three-Dimensional Points and Lines"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#three-dimensional-contour-plots","text":"Analogous to the contour plots we explored in \u201cDensity and Contour Plots\u201d, mplot3d contains tools to create three-dimensional relief plots using the same inputs. Like two-dimensional ax.contour plots, ax.contour3D requires all the input data to be in the form of two-dimensional regular grids, with the Z data evaluated at each point. Here we\u2019ll show a three-dimensional contour diagram of a three-dimensional sinusoidal function def f ( x , y ): return np . sin ( np . sqrt ( x ** 2 + y ** 2 )) x = np . linspace ( - 6 , 6 , 30 ) y = np . linspace ( - 6 , 6 , 30 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . contour3D ( X , Y , Z , 50 , cmap = 'binary' ) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 'y' ) ax . set_zlabel ( 'z' ); <IPython.core.display.Javascript object> Sometimes the default viewing angle is not optimal, in which case we can use the view_init method to set the elevation and azimuthal angles. In this example (the result of which is shown below, we\u2019ll use an elevation of 60 degrees (that is, 60 degrees above the x-y plane) and an azimuth of 35 degrees (that is, rotated 35 degrees counter-clockwise about the z-axis): ax . view_init ( 60 , 35 ) fig <IPython.core.display.Javascript object>","title":"Three-Dimensional Contour Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#wireframes-and-surface-plots","text":"Two other types of three-dimensional plots that work on gridded data are wireframes and surface plots. These take a grid of values and project it onto the specified three- dimensional surface, and can make the resulting three-dimensional forms quite easy to visualize. Here\u2019s an example using a wireframe fit = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_wireframe ( X , Y , Z , color = 'black' ) ax . set_title ( 'WireFrame' ); <IPython.core.display.Javascript object> A surface plot is like a wireframe plot, but each face of the wireframe is a filled poly\u2010 gon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ) ax . set_title ( 'Surface' ); <IPython.core.display.Javascript object> Note that though the grid of values for a surface plot needs to be two-dimensional, it need not be rectilinear. Here is an example of creating a partial polar grid, which when used with the surface3D plot can give us a slice into the function we\u2019re visualiz\u2010 ing fig = plt . figure () r = np . linspace ( 0 , 6 , 20 ) theta = np . linspace ( - 0.9 * np . pi , 0.8 * np . pi , 40 ) r , theta = np . meshgrid ( r , theta ) X = r * np . sin ( theta ) Y = r * np . cos ( theta ) Z = f ( X , Y ) ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ); <IPython.core.display.Javascript object>","title":"Wireframes and Surface Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#surface-triangulations","text":"For some applications, the evenly sampled grids required by the preceding routines are overly restrictive and inconvenient. In these situations, the triangulation-based plots can be very useful. What if rather than an even draw from a Cartesian or a polar grid, we instead have a set of random draws? theta = 2 * np . pi * np . random . random ( 1000 ) r = 6 * np . random . random ( 1000 ) x = np . ravel ( r * np . sin ( theta )) y = np . ravel ( r * np . cos ( theta )) z = f ( x , y ) fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . scatter ( x , y , z , c = z , cmap = 'viridis' , linewidth = 0.5 ) <IPython.core.display.Javascript object> <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f86f7843820> This leaves a lot to be desired. The function that will help us in this case is ax.plot_trisurf, which creates a surface by first finding a set of triangles formed between adjacent points fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , cmap = 'viridis' , edgecolor = 'none' ) <IPython.core.display.Javascript object> <mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x7f86f7206460> The result is certainly not as clean as when it is plotted with a grid, but the flexibility of such a triangulation allows for some really interesting three-dimensional plots. For example, it is actually possible to plot a three-dimensional M\u00f6bius strip using this, as we\u2019ll see next.","title":"Surface Triangulations"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 14 - Geographic Data with Basemap \u00b6 Geographic Data with Basemap Map Projections Let\u2019s start by a convenience routine to draw our world map along with the longitude and latitude lines: Cylindrical projections Pseudo-cylindrical projections Perspective projections Conic projections Other projections Drawing a Map Background Plotting Data on Maps Geographic Data with Basemap \u00b6 One common type of visualization in data science is that of geographic data. Matplot\u2010 lib\u2019s main tool for this type of visualization is the Basemap toolkit, which is one of several Matplotlib toolkits that live under the mpl_toolkits namespace. Admittedly, Basemap feels a bit clunky to use, and often even simple visualizations take much longer to render than you might hope. More modern solutions, such as leaflet or the Google Maps API, may be a better choice for more intensive map visualizations. Still, Basemap is a useful tool for Python users to have in their virtual toolbelts. In this sec\u2010 tion, we\u2019ll show several examples of the type of map visualization that is possible with this toolkit. Installation of Basemap is straightforward; if you\u2019re using conda you can type this and the package will be downloaded: $ conda install basemap # !conda install basemap # !python -m pip install basemap % matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap from PIL import Image plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = 'i' , lat_0 = 50 , lon_0 =- 100 ) # m.bluemarble(scale=0.9); # plt.show(m) m . bluemarble () m . drawcoastlines () plt . show () Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). from mpl_toolkits.basemap import Basemap import matplotlib.pyplot as plt ma = Basemap ( llcrnrlon =- 10.5 , llcrnrlat = 33 , urcrnrlon = 10. , urcrnrlat = 46. , resolution = 'i' , projection = 'cass' , lat_0 = 39.5 , lon_0 = 0. ) ma . bluemarble () ma . drawcoastlines () plt . show () The useful thing is that the globe shown here is not a mere image; it is a fully func\u2010 tioning Matplotlib axes that understands spherical coordinates and allows us to easily over-plot data on the map! For example, we can use a different map projection, zoom in to North America, and plot the location of Seattle. We\u2019ll use an etopo image (which shows topographical features both on land and under the ocean) as the map back\u2010 ground fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . etopo ( scale = 0.5 , alpha = 0.5 ) # Map (long, lat) to (x,y) for plotting x , y = m ( - 122.3 , 47.6 ) plt . plot ( x , y , 'ok' , markersize = 5 ) plt . text ( x , y , 'Seattle' , fontsize = 12 ); plt . show () warning: width and height keywords ignored for Orthographic projection Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Map Projections \u00b6 The first thing to decide when you are using maps is which projection to use. You\u2019re probably familiar with the fact that it is impossible to project a spherical map, such as that of the Earth, onto a flat surface without somehow distorting it or breaking its continuity. These projections have been developed over the course of human history, and there are a lot of choices! Depending on the intended use of the map projection, there are certain map features (e.g., direction, area, distance, shape, or other consider\u2010 ations) that are useful to maintain. Let\u2019s start by a convenience routine to draw our world map along with the longitude and latitude lines: \u00b6 # from itertools import chain # def draw_map(m, scale=0.2): # #draw a shadded-relief image # m.shadedrelief(scale=scale) # # lats and longs are returned as a dictionary # lats = m.drawparallels(np.linspace(-90,90,13)) # lons = m.drawparallels(np.linspace(-180,180,13)) # # keys contain the plt.Line2D instances # lat_lines=chain(*(tup[1][0] for tup in lats.items())) # lon_lines = chain(*(tup[1][0] for tup in lons.items())) # all_lines = chain(lat_lines, lon_lines) # # cycle throught these lines and set the desird style # for line in all_lines: # line.set(linestyle='-', alpha=0.3, color='w') from itertools import chain def draw_map ( m , scale = 0.2 ): # draw a shaded-relief image m . shadedrelief ( scale = scale ) # lats and longs are returned as a dictionary lats = m . drawparallels ( np . linspace ( - 90 , 90 , 13 )) lons = m . drawmeridians ( np . linspace ( - 180 , 180 , 13 )) # keys contain the plt.Line2D instances lat_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lats . items ())) lon_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lons . items ())) all_lines = chain ( lat_lines , lon_lines ) # cycle through these lines and set the desired style for line in all_lines : line . set ( linestyle = '-' , alpha = 0.3 , color = 'w' ) Cylindrical projections \u00b6 The simplest of map projections are cylindrical projections, in which lines of constant latitude and longitude are mapped to horizontal and vertical lines, respectively. This type of mapping represents equatorial regions quite well, but results in extreme dis\u2010 tortions near the poles. The spacing of latitude lines varies between different cylindri\u2010 cal projections, leading to different conservation properties, and different distortion near the poles. we show an example of the equidistant cylindrical pro\u2010 jection, which chooses a latitude scaling that preserves distances along meridians. Other cylindrical projections are the Mercator (projection=\u2018merc\u2019) and the cylin\u2010 drical equal-area (projection=\u2018cea\u2019) projections. # An equidistant cylinderical projection fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'cyl' , resolution = None , llcrnrlat =- 90 , urcrnrlat = 90 , llcrnrlon =- 180 , urcrnrlon = 180 , ) draw_map ( m ) # The additional arguments to Basemap for this view specify the latitude (lat) and lon\u2010 # gitude (lon) of the lower-left corner (llcrnr) and upper-right corner (urcrnr) for the # desired map, in units of degrees # # An equidistant cylinderical projection # fig = plt.figure(figsize=(8, 6), edgecolor='w') # m = Basemap(projection='merc', resolution=None, # llcrnrlat=-90, urcrnrlat=90, # llcrnrlon=-180, urcrnrlon=180, ) # draw_map(m) # The additional arguments to Basemap for this view specify the latitude (lat) and lon\u2010 # gitude (lon) of the lower-left corner (llcrnr) and upper-right corner (urcrnr) for the # desired map, in units of degrees Pseudo-cylindrical projections \u00b6 Pseudo-cylindrical projections relax the requirement that meridians (lines of constant longitude) remain vertical; this can give better properties near the poles of the projec\u2010 tion. The Mollweide projection (projection=\u2018moll\u2019) is one common example of this, in which all meridians are elliptical arcs. It is constructed so as to preserve area across the map: though there are distortions near the poles, the area of small patches reflects the true area. Other pseudo-cylindrical projections are the sinusoidal (projection=\u2018sinu\u2019) and Robinson (projection=\u2018robin\u2019) projections. fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'moll' , resolution = None , lat_0 = 0 , lon_0 = 0 ) draw_map ( m ) # The extra arguments to Basemap here refer to the central latitude (lat_0) and longi\u2010 # tude (lon_0) for the desired map. Perspective projections \u00b6 Perspective projections are constructed using a particular choice of perspective point, similar to if you photographed the Earth from a particular point in space (a point which, for some projections, technically lies within the Earth!). One common exam\u2010 ple is the orthographic projection (projection=\u2018ortho\u2019), which shows one side of the globe as seen from a viewer at a very long distance. Thus, it can show only half the globe at a time. Other perspective-based projections include the gnomonic projection (projection=\u2018gnom\u2019) and stereographic projection (projection=\u2018stere\u2019). These are often the most useful for showing small portions of the map. fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , lat_0 = 50 , lon_0 = 0 ) draw_map ( m ) Conic projections \u00b6 A conic projection projects the map onto a single cone, which is then unrolled. This can lead to very good local properties, but regions far from the focus point of the cone may become very distorted. One example of this is the Lambert conformal conic projection (projection=\u2018lcc\u2019), which we saw earlier in the map of North America. It projects the map onto a cone arranged in such a way that two standard parallels (specified in Basemap by lat_1 and lat_2) have well-represented distances, with scale decreasing between them and increasing outside of them. Other useful conic projec\u2010 tions are the equidistant conic (projection=\u2018eqdc\u2019) and the Albers equal-area (pro jection=\u2018aea\u2019) projection. Conic projections, like perspective projections, tend to be good choices for representing small to medium patches of the globe fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = None , lon_0 = 0 , lat_0 = 50 , lat_1 = 45 , lat_2 = 55 , width = 1.6E7 , height = 1.2E7 ) draw_map ( m ) Other projections \u00b6 If you\u2019re going to do much with map-based visualizations, I encourage you to read up on other available projections, along with their properties, advantages, and disadvan\u2010 tages. Most likely, they are available in the Basemap package. If you dig deep enough into this topic, you\u2019ll find an incredible subculture of geo-viz geeks who will be ready to argue fervently in support of their favorite projection for any given application! Drawing a Map Background \u00b6 Earlier we saw the bluemarble() and shadedrelief() methods for projecting global images on the map, as well as the drawparallels() and drawmeridians() methods for drawing lines of constant latitude and longitude. The Basemap package contains a range of useful functions for drawing borders of physical features like continents, oceans, lakes, and rivers, as well as political boundaries such as countries and US states and counties. The following are some of the available drawing functions that you may wish to explore using IPython\u2019s help features: \u2022 Physical boundaries and bodies of water drawcoastlines() Draw continental coast lines drawlsmask() Draw a mask between the land and sea, for sea with projecting images on one or the other drawmapboundary() Draw the map boundary, including the fill color for oceans drawrivers() Draw rivers on the map fillcontinents() Fill the continents with a given color; optionally fill lakes with another color \u2022 Political boundaries drawcountries() Draw country boundaries drawstates() Draw US state boundaries drawcounties() Draw US county boundaries \u2022 Map features drawgreatcircle() Draw a great circle between two points drawparallels() Draw lines of constant latitude drawmeridians() Draw lines of constant longitude drawmapscale() Draw a linear scale on the map \u2022 Whole-globe images bluemarble() Project NASA\u2019s blue marble image onto the map shadedrelief() Project a shaded relief image onto the map etopo() Draw an etopo relief image onto the map warpimage() Project a user-provided image onto the map For the boundary-based features, you must set the desired resolution when creating a Basemap image. The resolution argument of the Basemap class sets the level of detail in boundaries, either \u2018c\u2019 (crude), \u2018l\u2019 (low), \u2018i\u2019 (intermediate), \u2018h\u2019 (high), \u2018f\u2019 (full), or None if no boundaries will be used. This choice is important: setting high- resolution boundaries on a global map, for example, can be very slow. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 8 )) for i , res in enumerate ([ 'l' , 'h' ]): m = Basemap ( projection = 'gnom' , lat_0 = 57.3 , lon_0 =- 6.2 , width = 90000 , height = 120000 , resolution = res , ax = ax [ i ]) m . fillcontinents ( color = '#FFDDCC' , lake_color = '#DDEEFF' ) m . drawmapboundary ( fill_color = '#DDEEFF' ) m . drawcoastlines () ax [ i ] . set_title ( \"resolution = ' {0} ''\" . format ( res )); Notice that the low-resolution coastlines are not suitable for this level of zoom, while high-resolution works just fine. The low level would work just fine for a global view, however, and would be much faster than loading the high-resolution border data for the entire globe! It might require some experimentation to find the correct resolution parameter for a given view; the best route is to start with a fast, low-resolution plot and increase the resolution as needed. Plotting Data on Maps \u00b6 Perhaps the most useful piece of the Basemap toolkit is the ability to over-plot a variety of data onto a map background. For simple plotting and text, any plt function works on the map; you can use the Basemap instance to project latitude and longitude coordinates to (x, y) coordinates for plotting with plt, as we saw earlier in the Seattle example. In addition to this, there are many map-specific functions available as methods of the Basemap instance. These work very similarly to their standard Matplotlib counterparts, but have an additional Boolean argument latlon, which if set to True allows you to pass raw latitudes and longitudes to the method, rather than projected (x, y) coordinates. Some of these map-specific methods are: contour()/contourf() Draw contour lines or filled contours imshow() Draw an image pcolor()/pcolormesh() Draw a pseudocolor plot for irregular/regular meshes plot() Draw lines and/or markers scatter() Draw points with markers quiver() Draw vectors barbs() Draw wind barbs drawgreatcircle() Draw a great circle","title":"14 geographic data with basemap"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#14-geographic-data-with-basemap","text":"Geographic Data with Basemap Map Projections Let\u2019s start by a convenience routine to draw our world map along with the longitude and latitude lines: Cylindrical projections Pseudo-cylindrical projections Perspective projections Conic projections Other projections Drawing a Map Background Plotting Data on Maps","title":"14 - Geographic Data with Basemap"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#geographic-data-with-basemap","text":"One common type of visualization in data science is that of geographic data. Matplot\u2010 lib\u2019s main tool for this type of visualization is the Basemap toolkit, which is one of several Matplotlib toolkits that live under the mpl_toolkits namespace. Admittedly, Basemap feels a bit clunky to use, and often even simple visualizations take much longer to render than you might hope. More modern solutions, such as leaflet or the Google Maps API, may be a better choice for more intensive map visualizations. Still, Basemap is a useful tool for Python users to have in their virtual toolbelts. In this sec\u2010 tion, we\u2019ll show several examples of the type of map visualization that is possible with this toolkit. Installation of Basemap is straightforward; if you\u2019re using conda you can type this and the package will be downloaded: $ conda install basemap # !conda install basemap # !python -m pip install basemap % matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap from PIL import Image plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = 'i' , lat_0 = 50 , lon_0 =- 100 ) # m.bluemarble(scale=0.9); # plt.show(m) m . bluemarble () m . drawcoastlines () plt . show () Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). from mpl_toolkits.basemap import Basemap import matplotlib.pyplot as plt ma = Basemap ( llcrnrlon =- 10.5 , llcrnrlat = 33 , urcrnrlon = 10. , urcrnrlat = 46. , resolution = 'i' , projection = 'cass' , lat_0 = 39.5 , lon_0 = 0. ) ma . bluemarble () ma . drawcoastlines () plt . show () The useful thing is that the globe shown here is not a mere image; it is a fully func\u2010 tioning Matplotlib axes that understands spherical coordinates and allows us to easily over-plot data on the map! For example, we can use a different map projection, zoom in to North America, and plot the location of Seattle. We\u2019ll use an etopo image (which shows topographical features both on land and under the ocean) as the map back\u2010 ground fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . etopo ( scale = 0.5 , alpha = 0.5 ) # Map (long, lat) to (x,y) for plotting x , y = m ( - 122.3 , 47.6 ) plt . plot ( x , y , 'ok' , markersize = 5 ) plt . text ( x , y , 'Seattle' , fontsize = 12 ); plt . show () warning: width and height keywords ignored for Orthographic projection Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).","title":"Geographic Data with Basemap"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#map-projections","text":"The first thing to decide when you are using maps is which projection to use. You\u2019re probably familiar with the fact that it is impossible to project a spherical map, such as that of the Earth, onto a flat surface without somehow distorting it or breaking its continuity. These projections have been developed over the course of human history, and there are a lot of choices! Depending on the intended use of the map projection, there are certain map features (e.g., direction, area, distance, shape, or other consider\u2010 ations) that are useful to maintain.","title":"Map Projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#lets-start-by-a-convenience-routine-to-draw-our-world-map-along-with-the-longitude-and-latitude-lines","text":"# from itertools import chain # def draw_map(m, scale=0.2): # #draw a shadded-relief image # m.shadedrelief(scale=scale) # # lats and longs are returned as a dictionary # lats = m.drawparallels(np.linspace(-90,90,13)) # lons = m.drawparallels(np.linspace(-180,180,13)) # # keys contain the plt.Line2D instances # lat_lines=chain(*(tup[1][0] for tup in lats.items())) # lon_lines = chain(*(tup[1][0] for tup in lons.items())) # all_lines = chain(lat_lines, lon_lines) # # cycle throught these lines and set the desird style # for line in all_lines: # line.set(linestyle='-', alpha=0.3, color='w') from itertools import chain def draw_map ( m , scale = 0.2 ): # draw a shaded-relief image m . shadedrelief ( scale = scale ) # lats and longs are returned as a dictionary lats = m . drawparallels ( np . linspace ( - 90 , 90 , 13 )) lons = m . drawmeridians ( np . linspace ( - 180 , 180 , 13 )) # keys contain the plt.Line2D instances lat_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lats . items ())) lon_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lons . items ())) all_lines = chain ( lat_lines , lon_lines ) # cycle through these lines and set the desired style for line in all_lines : line . set ( linestyle = '-' , alpha = 0.3 , color = 'w' )","title":"Let\u2019s start by a convenience routine to draw our world map along with the longitude and latitude lines:"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#cylindrical-projections","text":"The simplest of map projections are cylindrical projections, in which lines of constant latitude and longitude are mapped to horizontal and vertical lines, respectively. This type of mapping represents equatorial regions quite well, but results in extreme dis\u2010 tortions near the poles. The spacing of latitude lines varies between different cylindri\u2010 cal projections, leading to different conservation properties, and different distortion near the poles. we show an example of the equidistant cylindrical pro\u2010 jection, which chooses a latitude scaling that preserves distances along meridians. Other cylindrical projections are the Mercator (projection=\u2018merc\u2019) and the cylin\u2010 drical equal-area (projection=\u2018cea\u2019) projections. # An equidistant cylinderical projection fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'cyl' , resolution = None , llcrnrlat =- 90 , urcrnrlat = 90 , llcrnrlon =- 180 , urcrnrlon = 180 , ) draw_map ( m ) # The additional arguments to Basemap for this view specify the latitude (lat) and lon\u2010 # gitude (lon) of the lower-left corner (llcrnr) and upper-right corner (urcrnr) for the # desired map, in units of degrees # # An equidistant cylinderical projection # fig = plt.figure(figsize=(8, 6), edgecolor='w') # m = Basemap(projection='merc', resolution=None, # llcrnrlat=-90, urcrnrlat=90, # llcrnrlon=-180, urcrnrlon=180, ) # draw_map(m) # The additional arguments to Basemap for this view specify the latitude (lat) and lon\u2010 # gitude (lon) of the lower-left corner (llcrnr) and upper-right corner (urcrnr) for the # desired map, in units of degrees","title":"Cylindrical projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#pseudo-cylindrical-projections","text":"Pseudo-cylindrical projections relax the requirement that meridians (lines of constant longitude) remain vertical; this can give better properties near the poles of the projec\u2010 tion. The Mollweide projection (projection=\u2018moll\u2019) is one common example of this, in which all meridians are elliptical arcs. It is constructed so as to preserve area across the map: though there are distortions near the poles, the area of small patches reflects the true area. Other pseudo-cylindrical projections are the sinusoidal (projection=\u2018sinu\u2019) and Robinson (projection=\u2018robin\u2019) projections. fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'moll' , resolution = None , lat_0 = 0 , lon_0 = 0 ) draw_map ( m ) # The extra arguments to Basemap here refer to the central latitude (lat_0) and longi\u2010 # tude (lon_0) for the desired map.","title":"Pseudo-cylindrical projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#perspective-projections","text":"Perspective projections are constructed using a particular choice of perspective point, similar to if you photographed the Earth from a particular point in space (a point which, for some projections, technically lies within the Earth!). One common exam\u2010 ple is the orthographic projection (projection=\u2018ortho\u2019), which shows one side of the globe as seen from a viewer at a very long distance. Thus, it can show only half the globe at a time. Other perspective-based projections include the gnomonic projection (projection=\u2018gnom\u2019) and stereographic projection (projection=\u2018stere\u2019). These are often the most useful for showing small portions of the map. fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , lat_0 = 50 , lon_0 = 0 ) draw_map ( m )","title":"Perspective projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#conic-projections","text":"A conic projection projects the map onto a single cone, which is then unrolled. This can lead to very good local properties, but regions far from the focus point of the cone may become very distorted. One example of this is the Lambert conformal conic projection (projection=\u2018lcc\u2019), which we saw earlier in the map of North America. It projects the map onto a cone arranged in such a way that two standard parallels (specified in Basemap by lat_1 and lat_2) have well-represented distances, with scale decreasing between them and increasing outside of them. Other useful conic projec\u2010 tions are the equidistant conic (projection=\u2018eqdc\u2019) and the Albers equal-area (pro jection=\u2018aea\u2019) projection. Conic projections, like perspective projections, tend to be good choices for representing small to medium patches of the globe fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = None , lon_0 = 0 , lat_0 = 50 , lat_1 = 45 , lat_2 = 55 , width = 1.6E7 , height = 1.2E7 ) draw_map ( m )","title":"Conic projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#other-projections","text":"If you\u2019re going to do much with map-based visualizations, I encourage you to read up on other available projections, along with their properties, advantages, and disadvan\u2010 tages. Most likely, they are available in the Basemap package. If you dig deep enough into this topic, you\u2019ll find an incredible subculture of geo-viz geeks who will be ready to argue fervently in support of their favorite projection for any given application!","title":"Other projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#drawing-a-map-background","text":"Earlier we saw the bluemarble() and shadedrelief() methods for projecting global images on the map, as well as the drawparallels() and drawmeridians() methods for drawing lines of constant latitude and longitude. The Basemap package contains a range of useful functions for drawing borders of physical features like continents, oceans, lakes, and rivers, as well as political boundaries such as countries and US states and counties. The following are some of the available drawing functions that you may wish to explore using IPython\u2019s help features: \u2022 Physical boundaries and bodies of water drawcoastlines() Draw continental coast lines drawlsmask() Draw a mask between the land and sea, for sea with projecting images on one or the other drawmapboundary() Draw the map boundary, including the fill color for oceans drawrivers() Draw rivers on the map fillcontinents() Fill the continents with a given color; optionally fill lakes with another color \u2022 Political boundaries drawcountries() Draw country boundaries drawstates() Draw US state boundaries drawcounties() Draw US county boundaries \u2022 Map features drawgreatcircle() Draw a great circle between two points drawparallels() Draw lines of constant latitude drawmeridians() Draw lines of constant longitude drawmapscale() Draw a linear scale on the map \u2022 Whole-globe images bluemarble() Project NASA\u2019s blue marble image onto the map shadedrelief() Project a shaded relief image onto the map etopo() Draw an etopo relief image onto the map warpimage() Project a user-provided image onto the map For the boundary-based features, you must set the desired resolution when creating a Basemap image. The resolution argument of the Basemap class sets the level of detail in boundaries, either \u2018c\u2019 (crude), \u2018l\u2019 (low), \u2018i\u2019 (intermediate), \u2018h\u2019 (high), \u2018f\u2019 (full), or None if no boundaries will be used. This choice is important: setting high- resolution boundaries on a global map, for example, can be very slow. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 8 )) for i , res in enumerate ([ 'l' , 'h' ]): m = Basemap ( projection = 'gnom' , lat_0 = 57.3 , lon_0 =- 6.2 , width = 90000 , height = 120000 , resolution = res , ax = ax [ i ]) m . fillcontinents ( color = '#FFDDCC' , lake_color = '#DDEEFF' ) m . drawmapboundary ( fill_color = '#DDEEFF' ) m . drawcoastlines () ax [ i ] . set_title ( \"resolution = ' {0} ''\" . format ( res )); Notice that the low-resolution coastlines are not suitable for this level of zoom, while high-resolution works just fine. The low level would work just fine for a global view, however, and would be much faster than loading the high-resolution border data for the entire globe! It might require some experimentation to find the correct resolution parameter for a given view; the best route is to start with a fast, low-resolution plot and increase the resolution as needed.","title":"Drawing a Map Background"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#plotting-data-on-maps","text":"Perhaps the most useful piece of the Basemap toolkit is the ability to over-plot a variety of data onto a map background. For simple plotting and text, any plt function works on the map; you can use the Basemap instance to project latitude and longitude coordinates to (x, y) coordinates for plotting with plt, as we saw earlier in the Seattle example. In addition to this, there are many map-specific functions available as methods of the Basemap instance. These work very similarly to their standard Matplotlib counterparts, but have an additional Boolean argument latlon, which if set to True allows you to pass raw latitudes and longitudes to the method, rather than projected (x, y) coordinates. Some of these map-specific methods are: contour()/contourf() Draw contour lines or filled contours imshow() Draw an image pcolor()/pcolormesh() Draw a pseudocolor plot for irregular/regular meshes plot() Draw lines and/or markers scatter() Draw points with markers quiver() Draw vectors barbs() Draw wind barbs drawgreatcircle() Draw a great circle","title":"Plotting Data on Maps"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 15 - Visualization with Seaborn \u00b6 Visualization with Seaborn Seaborn Versus Matplotlib Exploring Seaborn Plots Histograms, KDE, and densities Pair plots Faceted histograms Factor plots Joint distributions Bar plots Visualization with Seaborn \u00b6 Matplotlib has proven to be an incredibly useful and popular visualization tool, but even avid users will admit it often leaves much to be desired. There are several valid complaints about Matplotlib that often come up: \u2022 Prior to version 2.0, Matplotlib\u2019s defaults are not exactly the best choices. It was based off of MATLAB circa 1999, and this often shows. \u2022 Matplotlib\u2019s API is relatively low level. Doing sophisticated statistical visualiza\u2010 tion is possible, but often requires a lot of boilerplate code. \u2022 Matplotlib predated Pandas by more than a decade, and thus is not designed for use with Pandas DataFrames. In order to visualize data from a Pandas DataFrame, you must extract each Series and often concatenate them together into the right format. It would be nicer to have a plotting library that can intelligently use the DataFrame labels in a plot. An answer to these problems is Seaborn. Seaborn provides an API on top of Matplot\u2010 lib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality pro\u2010 vided by Pandas DataFrames. Seaborn Versus Matplotlib \u00b6 Here is an example of a simple random-walk plot in Matplotlib, using its classic plot formatting and colors. import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np import pandas as pd # create some data rng = np . random . RandomState ( 0 ) x = np . linspace ( 0 , 10 , 500 ) y = np . cumsum ( rng . randn ( 500 , 6 ), 0 ) x array([ 0. , 0.02004008, 0.04008016, 0.06012024, 0.08016032, 0.1002004 , 0.12024048, 0.14028056, 0.16032064, 0.18036072, 0.2004008 , 0.22044088, 0.24048096, 0.26052104, 0.28056112, 0.3006012 , 0.32064128, 0.34068136, 0.36072144, 0.38076152, 0.4008016 , 0.42084168, 0.44088176, 0.46092184, 0.48096192, 0.501002 , 0.52104208, 0.54108216, 0.56112224, 0.58116232, 0.6012024 , 0.62124248, 0.64128257, 0.66132265, 0.68136273, 0.70140281, 0.72144289, 0.74148297, 0.76152305, 0.78156313, 0.80160321, 0.82164329, 0.84168337, 0.86172345, 0.88176353, 0.90180361, 0.92184369, 0.94188377, 0.96192385, 0.98196393, 1.00200401, 1.02204409, 1.04208417, 1.06212425, 1.08216433, 1.10220441, 1.12224449, 1.14228457, 1.16232465, 1.18236473, 1.20240481, 1.22244489, 1.24248497, 1.26252505, 1.28256513, 1.30260521, 1.32264529, 1.34268537, 1.36272545, 1.38276553, 1.40280561, 1.42284569, 1.44288577, 1.46292585, 1.48296593, 1.50300601, 1.52304609, 1.54308617, 1.56312625, 1.58316633, 1.60320641, 1.62324649, 1.64328657, 1.66332665, 1.68336673, 1.70340681, 1.72344689, 1.74348697, 1.76352705, 1.78356713, 1.80360721, 1.82364729, 1.84368737, 1.86372745, 1.88376754, 1.90380762, 1.9238477 , 1.94388778, 1.96392786, 1.98396794, 2.00400802, 2.0240481 , 2.04408818, 2.06412826, 2.08416834, 2.10420842, 2.1242485 , 2.14428858, 2.16432866, 2.18436874, 2.20440882, 2.2244489 , 2.24448898, 2.26452906, 2.28456914, 2.30460922, 2.3246493 , 2.34468938, 2.36472946, 2.38476954, 2.40480962, 2.4248497 , 2.44488978, 2.46492986, 2.48496994, 2.50501002, 2.5250501 , 2.54509018, 2.56513026, 2.58517034, 2.60521042, 2.6252505 , 2.64529058, 2.66533066, 2.68537074, 2.70541082, 2.7254509 , 2.74549098, 2.76553106, 2.78557114, 2.80561122, 2.8256513 , 2.84569138, 2.86573146, 2.88577154, 2.90581162, 2.9258517 , 2.94589178, 2.96593186, 2.98597194, 3.00601202, 3.0260521 , 3.04609218, 3.06613226, 3.08617234, 3.10621242, 3.12625251, 3.14629259, 3.16633267, 3.18637275, 3.20641283, 3.22645291, 3.24649299, 3.26653307, 3.28657315, 3.30661323, 3.32665331, 3.34669339, 3.36673347, 3.38677355, 3.40681363, 3.42685371, 3.44689379, 3.46693387, 3.48697395, 3.50701403, 3.52705411, 3.54709419, 3.56713427, 3.58717435, 3.60721443, 3.62725451, 3.64729459, 3.66733467, 3.68737475, 3.70741483, 3.72745491, 3.74749499, 3.76753507, 3.78757515, 3.80761523, 3.82765531, 3.84769539, 3.86773547, 3.88777555, 3.90781563, 3.92785571, 3.94789579, 3.96793587, 3.98797595, 4.00801603, 4.02805611, 4.04809619, 4.06813627, 4.08817635, 4.10821643, 4.12825651, 4.14829659, 4.16833667, 4.18837675, 4.20841683, 4.22845691, 4.24849699, 4.26853707, 4.28857715, 4.30861723, 4.32865731, 4.34869739, 4.36873747, 4.38877756, 4.40881764, 4.42885772, 4.4488978 , 4.46893788, 4.48897796, 4.50901804, 4.52905812, 4.5490982 , 4.56913828, 4.58917836, 4.60921844, 4.62925852, 4.6492986 , 4.66933868, 4.68937876, 4.70941884, 4.72945892, 4.749499 , 4.76953908, 4.78957916, 4.80961924, 4.82965932, 4.8496994 , 4.86973948, 4.88977956, 4.90981964, 4.92985972, 4.9498998 , 4.96993988, 4.98997996, 5.01002004, 5.03006012, 5.0501002 , 5.07014028, 5.09018036, 5.11022044, 5.13026052, 5.1503006 , 5.17034068, 5.19038076, 5.21042084, 5.23046092, 5.250501 , 5.27054108, 5.29058116, 5.31062124, 5.33066132, 5.3507014 , 5.37074148, 5.39078156, 5.41082164, 5.43086172, 5.4509018 , 5.47094188, 5.49098196, 5.51102204, 5.53106212, 5.5511022 , 5.57114228, 5.59118236, 5.61122244, 5.63126253, 5.65130261, 5.67134269, 5.69138277, 5.71142285, 5.73146293, 5.75150301, 5.77154309, 5.79158317, 5.81162325, 5.83166333, 5.85170341, 5.87174349, 5.89178357, 5.91182365, 5.93186373, 5.95190381, 5.97194389, 5.99198397, 6.01202405, 6.03206413, 6.05210421, 6.07214429, 6.09218437, 6.11222445, 6.13226453, 6.15230461, 6.17234469, 6.19238477, 6.21242485, 6.23246493, 6.25250501, 6.27254509, 6.29258517, 6.31262525, 6.33266533, 6.35270541, 6.37274549, 6.39278557, 6.41282565, 6.43286573, 6.45290581, 6.47294589, 6.49298597, 6.51302605, 6.53306613, 6.55310621, 6.57314629, 6.59318637, 6.61322645, 6.63326653, 6.65330661, 6.67334669, 6.69338677, 6.71342685, 6.73346693, 6.75350701, 6.77354709, 6.79358717, 6.81362725, 6.83366733, 6.85370741, 6.87374749, 6.89378758, 6.91382766, 6.93386774, 6.95390782, 6.9739479 , 6.99398798, 7.01402806, 7.03406814, 7.05410822, 7.0741483 , 7.09418838, 7.11422846, 7.13426854, 7.15430862, 7.1743487 , 7.19438878, 7.21442886, 7.23446894, 7.25450902, 7.2745491 , 7.29458918, 7.31462926, 7.33466934, 7.35470942, 7.3747495 , 7.39478958, 7.41482966, 7.43486974, 7.45490982, 7.4749499 , 7.49498998, 7.51503006, 7.53507014, 7.55511022, 7.5751503 , 7.59519038, 7.61523046, 7.63527054, 7.65531062, 7.6753507 , 7.69539078, 7.71543086, 7.73547094, 7.75551102, 7.7755511 , 7.79559118, 7.81563126, 7.83567134, 7.85571142, 7.8757515 , 7.89579158, 7.91583166, 7.93587174, 7.95591182, 7.9759519 , 7.99599198, 8.01603206, 8.03607214, 8.05611222, 8.0761523 , 8.09619238, 8.11623246, 8.13627255, 8.15631263, 8.17635271, 8.19639279, 8.21643287, 8.23647295, 8.25651303, 8.27655311, 8.29659319, 8.31663327, 8.33667335, 8.35671343, 8.37675351, 8.39679359, 8.41683367, 8.43687375, 8.45691383, 8.47695391, 8.49699399, 8.51703407, 8.53707415, 8.55711423, 8.57715431, 8.59719439, 8.61723447, 8.63727455, 8.65731463, 8.67735471, 8.69739479, 8.71743487, 8.73747495, 8.75751503, 8.77755511, 8.79759519, 8.81763527, 8.83767535, 8.85771543, 8.87775551, 8.89779559, 8.91783567, 8.93787575, 8.95791583, 8.97795591, 8.99799599, 9.01803607, 9.03807615, 9.05811623, 9.07815631, 9.09819639, 9.11823647, 9.13827655, 9.15831663, 9.17835671, 9.19839679, 9.21843687, 9.23847695, 9.25851703, 9.27855711, 9.29859719, 9.31863727, 9.33867735, 9.35871743, 9.37875752, 9.3987976 , 9.41883768, 9.43887776, 9.45891784, 9.47895792, 9.498998 , 9.51903808, 9.53907816, 9.55911824, 9.57915832, 9.5991984 , 9.61923848, 9.63927856, 9.65931864, 9.67935872, 9.6993988 , 9.71943888, 9.73947896, 9.75951904, 9.77955912, 9.7995992 , 9.81963928, 9.83967936, 9.85971944, 9.87975952, 9.8997996 , 9.91983968, 9.93987976, 9.95991984, 9.97995992, 10. ]) y array([[ 1.76405235, 0.40015721, 0.97873798, 2.2408932 , 1.86755799, -0.97727788], [ 2.71414076, 0.2488 , 0.87551913, 2.6514917 , 2.01160156, 0.47699563], [ 3.47517849, 0.37047502, 1.31938237, 2.98516603, 3.50568063, 0.27183736], ..., [-34.82533536, -44.37245964, -32.86660099, 31.93843765, 9.67250307, -9.16537805], [-35.4875268 , -45.95006671, -33.20716103, 30.63521756, 10.13925372, -9.00427173], [-35.16749487, -43.87089005, -34.11462701, 30.44281336, 8.92673797, -9.08487024]]) # plot the data usng matplotlib defaults plt . plot ( x , y ) plt . legend ( \"ABCDEG\" , ncol = 2 , loc = 'upper left' ) <matplotlib.legend.Legend at 0x7f9257fd7e50> Although the result contains all the information we\u2019d like it to convey, it does so in a way that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of 21 st -century data visualization. import seaborn as sns sns . set () # some plotting code as above plt . plot ( x , y ) plt . legend ( \"ABCDEF\" , ncol = 2 , loc = 'upper left' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2 warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\" <matplotlib.legend.Legend at 0x7f9253d88220> Exploring Seaborn Plots \u00b6 The main idea of Seaborn is that it provides high-level commands to create a variety of plot types useful for statistical data exploration, and even some statistical model fitting. Let\u2019s take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood), but the Seaborn API is much more convenient. Histograms, KDE, and densities \u00b6 Often in statistical data visualization, all you want is to plot histograms and joint dis\u2010 tributions of variables. We have seen that this is relatively straightforward in Matplot\u2010 lib ( data = np . random . multivariate_normal ([ 0 , 0 ],[[ 5 , 2 ],[ 2 , 2 ]], size = 2000 ) data = pd . DataFrame ( data , columns = [ 'x' , 'y' ]) for col in 'xy' : plt . hist ( data [ col ], alpha = 0.5 ) Rather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with sns.kdeplot for col in 'xy' : sns . kdeplot ( data [ col ], shade = True ) Histograms and KDE can be combined using displot sns . distplot ( data [ 'x' ]) sns . distplot ( data [ 'y' ]); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) If we pass the full two-dimensional dataset to kdeplot, we will get a two-dimensional visualization of the data sns . kdeplot ( data ); ValueError: If using all scalar values, you must pass an index We can see the joint distribution and the marginal distributions together using sns.jointplot. For this plot, we\u2019ll set the style to a white background with sns . axes_style ( 'white' ): sns . jointplot ( 'x' , 'y' , data , kind = 'kde' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, data. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( There are other parameters that can be passed to jointplot\u2014for example, we can use a hexagonally based histogram instead with sns . axes_style ( 'white' ): sns . jointplot ( \"x\" , \"y\" , data , kind = 'hex' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, data. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( Pair plots \u00b6 When you generalize joint plots to datasets of larger dimensions, you end up with pair plots. This is very useful for exploring correlations between multidimensional data, when you\u2019d like to plot all pairs of values against each other. We\u2019ll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species: iris = sns . load_dataset ( 'iris' ) iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa # visulizing the multidimensional relationships among the samples is # as easy sns . pairplot ( iris , hue = 'species' , size = 2.5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/axisgrid.py:2076: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) <seaborn.axisgrid.PairGrid at 0x7f923c0e5910> Faceted histograms \u00b6 Sometimes the best way to view data is via histograms of subsets. Seaborn\u2019s FacetGrid makes this extremely simple. We\u2019ll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data tips = sns . load_dataset ( 'tips' ) tips . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 tips [ 'tip_pct' ] = 100 * tips [ 'tip' ] / tips [ 'total_bill' ] tips [ 'tip_pct' ] 0 5.944673 1 16.054159 2 16.658734 3 13.978041 4 14.680765 ... 239 20.392697 240 7.358352 241 8.822232 242 9.820426 243 15.974441 Name: tip_pct, Length: 244, dtype: float64 grid = sns . FacetGrid ( tips , row = 'sex' , col = 'time' , margin_titles = True ) grid . map ( plt . hist , \"tip_pct\" , bins = np . linspace ( 0 , 40 , 15 )); Factor plots \u00b6 Factor plots can be useful for this kind of visualization as well. This allows you to view the distribution of a parameter within bins defined by any other parameter with sns . axes_style ( style = 'ticks' ): g = sns . factorplot ( 'day' , 'total_bill' , 'sex' , data = tips , kind = 'box' ) g . set_axis_labels ( 'Day' , 'Total Bill' ); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, hue. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( Joint distributions \u00b6 Similar to the pair plot we saw earlier, we can use sns.jointplot to show the joint distribution between different datasets, along with the associated marginal distribu\u2010 tions with sns . axes_style ( 'white' ): sns . jointplot ( 'total_bill' , 'tip' , data = tips , kind = 'hex' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( # the joint plot can also do some automatic kernel density nd regression sns . jointplot ( 'total_bill' , 'tip' , data = tips , kind = 'reg' ); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( Bar plots \u00b6 Time series can be plotted with sns.factorplot planets = sns . load_dataset ( 'planets' ) planets . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 with sns . axes_style ( 'white' ): g = sns . factorplot ( 'year' , data = planets , aspect = 2 , kind = 'count' , color = 'steelblue' ) g . set_xticklabels ( step = 5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( # we can learn more by looking at the method of discovery of each of these planets with sns . axes_style ( 'white' ): g = sns . factorplot ( 'year' , data = planets , aspect = 2 , kind = 'count' , color = 'steelblue' , hue = 'method' , order = range ( 2001 , 2015 )) g . set_ylabels ( 'Number of Planets Discovered' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"15visualiztion with seaborn"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#15-visualization-with-seaborn","text":"Visualization with Seaborn Seaborn Versus Matplotlib Exploring Seaborn Plots Histograms, KDE, and densities Pair plots Faceted histograms Factor plots Joint distributions Bar plots","title":"15 - Visualization with Seaborn"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#visualization-with-seaborn","text":"Matplotlib has proven to be an incredibly useful and popular visualization tool, but even avid users will admit it often leaves much to be desired. There are several valid complaints about Matplotlib that often come up: \u2022 Prior to version 2.0, Matplotlib\u2019s defaults are not exactly the best choices. It was based off of MATLAB circa 1999, and this often shows. \u2022 Matplotlib\u2019s API is relatively low level. Doing sophisticated statistical visualiza\u2010 tion is possible, but often requires a lot of boilerplate code. \u2022 Matplotlib predated Pandas by more than a decade, and thus is not designed for use with Pandas DataFrames. In order to visualize data from a Pandas DataFrame, you must extract each Series and often concatenate them together into the right format. It would be nicer to have a plotting library that can intelligently use the DataFrame labels in a plot. An answer to these problems is Seaborn. Seaborn provides an API on top of Matplot\u2010 lib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality pro\u2010 vided by Pandas DataFrames.","title":"Visualization with Seaborn"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#seaborn-versus-matplotlib","text":"Here is an example of a simple random-walk plot in Matplotlib, using its classic plot formatting and colors. import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np import pandas as pd # create some data rng = np . random . RandomState ( 0 ) x = np . linspace ( 0 , 10 , 500 ) y = np . cumsum ( rng . randn ( 500 , 6 ), 0 ) x array([ 0. , 0.02004008, 0.04008016, 0.06012024, 0.08016032, 0.1002004 , 0.12024048, 0.14028056, 0.16032064, 0.18036072, 0.2004008 , 0.22044088, 0.24048096, 0.26052104, 0.28056112, 0.3006012 , 0.32064128, 0.34068136, 0.36072144, 0.38076152, 0.4008016 , 0.42084168, 0.44088176, 0.46092184, 0.48096192, 0.501002 , 0.52104208, 0.54108216, 0.56112224, 0.58116232, 0.6012024 , 0.62124248, 0.64128257, 0.66132265, 0.68136273, 0.70140281, 0.72144289, 0.74148297, 0.76152305, 0.78156313, 0.80160321, 0.82164329, 0.84168337, 0.86172345, 0.88176353, 0.90180361, 0.92184369, 0.94188377, 0.96192385, 0.98196393, 1.00200401, 1.02204409, 1.04208417, 1.06212425, 1.08216433, 1.10220441, 1.12224449, 1.14228457, 1.16232465, 1.18236473, 1.20240481, 1.22244489, 1.24248497, 1.26252505, 1.28256513, 1.30260521, 1.32264529, 1.34268537, 1.36272545, 1.38276553, 1.40280561, 1.42284569, 1.44288577, 1.46292585, 1.48296593, 1.50300601, 1.52304609, 1.54308617, 1.56312625, 1.58316633, 1.60320641, 1.62324649, 1.64328657, 1.66332665, 1.68336673, 1.70340681, 1.72344689, 1.74348697, 1.76352705, 1.78356713, 1.80360721, 1.82364729, 1.84368737, 1.86372745, 1.88376754, 1.90380762, 1.9238477 , 1.94388778, 1.96392786, 1.98396794, 2.00400802, 2.0240481 , 2.04408818, 2.06412826, 2.08416834, 2.10420842, 2.1242485 , 2.14428858, 2.16432866, 2.18436874, 2.20440882, 2.2244489 , 2.24448898, 2.26452906, 2.28456914, 2.30460922, 2.3246493 , 2.34468938, 2.36472946, 2.38476954, 2.40480962, 2.4248497 , 2.44488978, 2.46492986, 2.48496994, 2.50501002, 2.5250501 , 2.54509018, 2.56513026, 2.58517034, 2.60521042, 2.6252505 , 2.64529058, 2.66533066, 2.68537074, 2.70541082, 2.7254509 , 2.74549098, 2.76553106, 2.78557114, 2.80561122, 2.8256513 , 2.84569138, 2.86573146, 2.88577154, 2.90581162, 2.9258517 , 2.94589178, 2.96593186, 2.98597194, 3.00601202, 3.0260521 , 3.04609218, 3.06613226, 3.08617234, 3.10621242, 3.12625251, 3.14629259, 3.16633267, 3.18637275, 3.20641283, 3.22645291, 3.24649299, 3.26653307, 3.28657315, 3.30661323, 3.32665331, 3.34669339, 3.36673347, 3.38677355, 3.40681363, 3.42685371, 3.44689379, 3.46693387, 3.48697395, 3.50701403, 3.52705411, 3.54709419, 3.56713427, 3.58717435, 3.60721443, 3.62725451, 3.64729459, 3.66733467, 3.68737475, 3.70741483, 3.72745491, 3.74749499, 3.76753507, 3.78757515, 3.80761523, 3.82765531, 3.84769539, 3.86773547, 3.88777555, 3.90781563, 3.92785571, 3.94789579, 3.96793587, 3.98797595, 4.00801603, 4.02805611, 4.04809619, 4.06813627, 4.08817635, 4.10821643, 4.12825651, 4.14829659, 4.16833667, 4.18837675, 4.20841683, 4.22845691, 4.24849699, 4.26853707, 4.28857715, 4.30861723, 4.32865731, 4.34869739, 4.36873747, 4.38877756, 4.40881764, 4.42885772, 4.4488978 , 4.46893788, 4.48897796, 4.50901804, 4.52905812, 4.5490982 , 4.56913828, 4.58917836, 4.60921844, 4.62925852, 4.6492986 , 4.66933868, 4.68937876, 4.70941884, 4.72945892, 4.749499 , 4.76953908, 4.78957916, 4.80961924, 4.82965932, 4.8496994 , 4.86973948, 4.88977956, 4.90981964, 4.92985972, 4.9498998 , 4.96993988, 4.98997996, 5.01002004, 5.03006012, 5.0501002 , 5.07014028, 5.09018036, 5.11022044, 5.13026052, 5.1503006 , 5.17034068, 5.19038076, 5.21042084, 5.23046092, 5.250501 , 5.27054108, 5.29058116, 5.31062124, 5.33066132, 5.3507014 , 5.37074148, 5.39078156, 5.41082164, 5.43086172, 5.4509018 , 5.47094188, 5.49098196, 5.51102204, 5.53106212, 5.5511022 , 5.57114228, 5.59118236, 5.61122244, 5.63126253, 5.65130261, 5.67134269, 5.69138277, 5.71142285, 5.73146293, 5.75150301, 5.77154309, 5.79158317, 5.81162325, 5.83166333, 5.85170341, 5.87174349, 5.89178357, 5.91182365, 5.93186373, 5.95190381, 5.97194389, 5.99198397, 6.01202405, 6.03206413, 6.05210421, 6.07214429, 6.09218437, 6.11222445, 6.13226453, 6.15230461, 6.17234469, 6.19238477, 6.21242485, 6.23246493, 6.25250501, 6.27254509, 6.29258517, 6.31262525, 6.33266533, 6.35270541, 6.37274549, 6.39278557, 6.41282565, 6.43286573, 6.45290581, 6.47294589, 6.49298597, 6.51302605, 6.53306613, 6.55310621, 6.57314629, 6.59318637, 6.61322645, 6.63326653, 6.65330661, 6.67334669, 6.69338677, 6.71342685, 6.73346693, 6.75350701, 6.77354709, 6.79358717, 6.81362725, 6.83366733, 6.85370741, 6.87374749, 6.89378758, 6.91382766, 6.93386774, 6.95390782, 6.9739479 , 6.99398798, 7.01402806, 7.03406814, 7.05410822, 7.0741483 , 7.09418838, 7.11422846, 7.13426854, 7.15430862, 7.1743487 , 7.19438878, 7.21442886, 7.23446894, 7.25450902, 7.2745491 , 7.29458918, 7.31462926, 7.33466934, 7.35470942, 7.3747495 , 7.39478958, 7.41482966, 7.43486974, 7.45490982, 7.4749499 , 7.49498998, 7.51503006, 7.53507014, 7.55511022, 7.5751503 , 7.59519038, 7.61523046, 7.63527054, 7.65531062, 7.6753507 , 7.69539078, 7.71543086, 7.73547094, 7.75551102, 7.7755511 , 7.79559118, 7.81563126, 7.83567134, 7.85571142, 7.8757515 , 7.89579158, 7.91583166, 7.93587174, 7.95591182, 7.9759519 , 7.99599198, 8.01603206, 8.03607214, 8.05611222, 8.0761523 , 8.09619238, 8.11623246, 8.13627255, 8.15631263, 8.17635271, 8.19639279, 8.21643287, 8.23647295, 8.25651303, 8.27655311, 8.29659319, 8.31663327, 8.33667335, 8.35671343, 8.37675351, 8.39679359, 8.41683367, 8.43687375, 8.45691383, 8.47695391, 8.49699399, 8.51703407, 8.53707415, 8.55711423, 8.57715431, 8.59719439, 8.61723447, 8.63727455, 8.65731463, 8.67735471, 8.69739479, 8.71743487, 8.73747495, 8.75751503, 8.77755511, 8.79759519, 8.81763527, 8.83767535, 8.85771543, 8.87775551, 8.89779559, 8.91783567, 8.93787575, 8.95791583, 8.97795591, 8.99799599, 9.01803607, 9.03807615, 9.05811623, 9.07815631, 9.09819639, 9.11823647, 9.13827655, 9.15831663, 9.17835671, 9.19839679, 9.21843687, 9.23847695, 9.25851703, 9.27855711, 9.29859719, 9.31863727, 9.33867735, 9.35871743, 9.37875752, 9.3987976 , 9.41883768, 9.43887776, 9.45891784, 9.47895792, 9.498998 , 9.51903808, 9.53907816, 9.55911824, 9.57915832, 9.5991984 , 9.61923848, 9.63927856, 9.65931864, 9.67935872, 9.6993988 , 9.71943888, 9.73947896, 9.75951904, 9.77955912, 9.7995992 , 9.81963928, 9.83967936, 9.85971944, 9.87975952, 9.8997996 , 9.91983968, 9.93987976, 9.95991984, 9.97995992, 10. ]) y array([[ 1.76405235, 0.40015721, 0.97873798, 2.2408932 , 1.86755799, -0.97727788], [ 2.71414076, 0.2488 , 0.87551913, 2.6514917 , 2.01160156, 0.47699563], [ 3.47517849, 0.37047502, 1.31938237, 2.98516603, 3.50568063, 0.27183736], ..., [-34.82533536, -44.37245964, -32.86660099, 31.93843765, 9.67250307, -9.16537805], [-35.4875268 , -45.95006671, -33.20716103, 30.63521756, 10.13925372, -9.00427173], [-35.16749487, -43.87089005, -34.11462701, 30.44281336, 8.92673797, -9.08487024]]) # plot the data usng matplotlib defaults plt . plot ( x , y ) plt . legend ( \"ABCDEG\" , ncol = 2 , loc = 'upper left' ) <matplotlib.legend.Legend at 0x7f9257fd7e50> Although the result contains all the information we\u2019d like it to convey, it does so in a way that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of 21 st -century data visualization. import seaborn as sns sns . set () # some plotting code as above plt . plot ( x , y ) plt . legend ( \"ABCDEF\" , ncol = 2 , loc = 'upper left' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2 warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\" <matplotlib.legend.Legend at 0x7f9253d88220>","title":"Seaborn Versus Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#exploring-seaborn-plots","text":"The main idea of Seaborn is that it provides high-level commands to create a variety of plot types useful for statistical data exploration, and even some statistical model fitting. Let\u2019s take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood), but the Seaborn API is much more convenient.","title":"Exploring Seaborn Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#histograms-kde-and-densities","text":"Often in statistical data visualization, all you want is to plot histograms and joint dis\u2010 tributions of variables. We have seen that this is relatively straightforward in Matplot\u2010 lib ( data = np . random . multivariate_normal ([ 0 , 0 ],[[ 5 , 2 ],[ 2 , 2 ]], size = 2000 ) data = pd . DataFrame ( data , columns = [ 'x' , 'y' ]) for col in 'xy' : plt . hist ( data [ col ], alpha = 0.5 ) Rather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with sns.kdeplot for col in 'xy' : sns . kdeplot ( data [ col ], shade = True ) Histograms and KDE can be combined using displot sns . distplot ( data [ 'x' ]) sns . distplot ( data [ 'y' ]); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) If we pass the full two-dimensional dataset to kdeplot, we will get a two-dimensional visualization of the data sns . kdeplot ( data ); ValueError: If using all scalar values, you must pass an index We can see the joint distribution and the marginal distributions together using sns.jointplot. For this plot, we\u2019ll set the style to a white background with sns . axes_style ( 'white' ): sns . jointplot ( 'x' , 'y' , data , kind = 'kde' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, data. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( There are other parameters that can be passed to jointplot\u2014for example, we can use a hexagonally based histogram instead with sns . axes_style ( 'white' ): sns . jointplot ( \"x\" , \"y\" , data , kind = 'hex' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, data. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"Histograms, KDE, and densities"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#pair-plots","text":"When you generalize joint plots to datasets of larger dimensions, you end up with pair plots. This is very useful for exploring correlations between multidimensional data, when you\u2019d like to plot all pairs of values against each other. We\u2019ll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species: iris = sns . load_dataset ( 'iris' ) iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa # visulizing the multidimensional relationships among the samples is # as easy sns . pairplot ( iris , hue = 'species' , size = 2.5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/axisgrid.py:2076: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) <seaborn.axisgrid.PairGrid at 0x7f923c0e5910>","title":"Pair plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#faceted-histograms","text":"Sometimes the best way to view data is via histograms of subsets. Seaborn\u2019s FacetGrid makes this extremely simple. We\u2019ll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data tips = sns . load_dataset ( 'tips' ) tips . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 tips [ 'tip_pct' ] = 100 * tips [ 'tip' ] / tips [ 'total_bill' ] tips [ 'tip_pct' ] 0 5.944673 1 16.054159 2 16.658734 3 13.978041 4 14.680765 ... 239 20.392697 240 7.358352 241 8.822232 242 9.820426 243 15.974441 Name: tip_pct, Length: 244, dtype: float64 grid = sns . FacetGrid ( tips , row = 'sex' , col = 'time' , margin_titles = True ) grid . map ( plt . hist , \"tip_pct\" , bins = np . linspace ( 0 , 40 , 15 ));","title":"Faceted histograms"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#factor-plots","text":"Factor plots can be useful for this kind of visualization as well. This allows you to view the distribution of a parameter within bins defined by any other parameter with sns . axes_style ( style = 'ticks' ): g = sns . factorplot ( 'day' , 'total_bill' , 'sex' , data = tips , kind = 'box' ) g . set_axis_labels ( 'Day' , 'Total Bill' ); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, hue. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"Factor plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#joint-distributions","text":"Similar to the pair plot we saw earlier, we can use sns.jointplot to show the joint distribution between different datasets, along with the associated marginal distribu\u2010 tions with sns . axes_style ( 'white' ): sns . jointplot ( 'total_bill' , 'tip' , data = tips , kind = 'hex' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( # the joint plot can also do some automatic kernel density nd regression sns . jointplot ( 'total_bill' , 'tip' , data = tips , kind = 'reg' ); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"Joint distributions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#bar-plots","text":"Time series can be plotted with sns.factorplot planets = sns . load_dataset ( 'planets' ) planets . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 with sns . axes_style ( 'white' ): g = sns . factorplot ( 'year' , data = planets , aspect = 2 , kind = 'count' , color = 'steelblue' ) g . set_xticklabels ( step = 5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( # we can learn more by looking at the method of discovery of each of these planets with sns . axes_style ( 'white' ): g = sns . factorplot ( 'year' , data = planets , aspect = 2 , kind = 'count' , color = 'steelblue' , hue = 'method' , order = range ( 2001 , 2015 )) g . set_ylabels ( 'Number of Planets Discovered' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"Bar plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: Exploring Marathon Finishing Times \u00b6 Example: Exploring Marathon Finishing Times Example: Exploring Marathon Finishing Times \u00b6 Here we\u2019ll look at using Seaborn to help visualize and understand finishing results from a marathon. I\u2019ve scraped the data from sources on the Web, aggregated it and removed any identifying information, and put it on GitHub where it can be downloa\u2010 ded (if you are interested in using Python for web scraping, I would recommend Web Scraping with Python by Ryan Mitchell). We will start by downloading the data from the Web, and loading it into Pandas: # !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 836k 100 836k 0 0 9808 0 0:01:27 0:01:27 --:--:-- 11084 13626 0 0:01:02 0:00:11 0:00:51 10234 import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np import pandas as pd ! ls '01general Matplotlib tips.ipynb' 02simple_lineplots.ipynb '03simple scatter plots.ipynb' '04visualizing errors.ipynb' '05density and contour plots.ipynb' '06Histograms Binnings and Density.ipynb' '07customized plot legends.ipynb' '08customizing colorbar.ipynb' '09multiple subplots.ipynb' '10text and annotation Example.ipynb' '11customizing ticks.ipynb' '12customizing matplotlib configuration and stylesheets.ipynb' '13threedimensional plotting.ipynb' '14_geographic data with basemap.ipynb' '15visualiztion with seaborn.ipynb' cos_sinplots.png 'example California cities.ipynb' 'Example Exploring Marathon Finishing times.ipynb' 'Example Handwritten Digits.ipynb' 'example surface temperature data.ipynb' 'Example Visualizing a Mobius Strip.ipynb' gistemp250.nc.gz marathon-data.csv data = pd . read_csv ( 'marathon-data.csv' ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age gender split final 0 33 M 01:05:38 02:08:51 1 32 M 01:06:26 02:09:28 2 31 M 01:06:49 02:10:42 3 38 M 01:06:16 02:13:45 4 31 M 01:06:32 02:13:59 data . dtypes age int64 gender object split object final object dtype: object # lets convert split and final to times def convert_time ( s ): h , m , s = map ( int , s . split ( ':' )) return pd . datetools . timedelta ( hours = h , minutes = m , seconds = s ) data = pd . read_csv ( 'marathon-data.csv' , converters = { 'split' : convert_time , 'final' : convert_time }) data . head () AttributeError: module 'pandas' has no attribute 'datetools'","title":"Example Exploring Marathon Finishing times"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/#example-exploring-marathon-finishing-times","text":"Example: Exploring Marathon Finishing Times","title":"Example: Exploring Marathon Finishing Times"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/#example-exploring-marathon-finishing-times_1","text":"Here we\u2019ll look at using Seaborn to help visualize and understand finishing results from a marathon. I\u2019ve scraped the data from sources on the Web, aggregated it and removed any identifying information, and put it on GitHub where it can be downloa\u2010 ded (if you are interested in using Python for web scraping, I would recommend Web Scraping with Python by Ryan Mitchell). We will start by downloading the data from the Web, and loading it into Pandas: # !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 836k 100 836k 0 0 9808 0 0:01:27 0:01:27 --:--:-- 11084 13626 0 0:01:02 0:00:11 0:00:51 10234 import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np import pandas as pd ! ls '01general Matplotlib tips.ipynb' 02simple_lineplots.ipynb '03simple scatter plots.ipynb' '04visualizing errors.ipynb' '05density and contour plots.ipynb' '06Histograms Binnings and Density.ipynb' '07customized plot legends.ipynb' '08customizing colorbar.ipynb' '09multiple subplots.ipynb' '10text and annotation Example.ipynb' '11customizing ticks.ipynb' '12customizing matplotlib configuration and stylesheets.ipynb' '13threedimensional plotting.ipynb' '14_geographic data with basemap.ipynb' '15visualiztion with seaborn.ipynb' cos_sinplots.png 'example California cities.ipynb' 'Example Exploring Marathon Finishing times.ipynb' 'Example Handwritten Digits.ipynb' 'example surface temperature data.ipynb' 'Example Visualizing a Mobius Strip.ipynb' gistemp250.nc.gz marathon-data.csv data = pd . read_csv ( 'marathon-data.csv' ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age gender split final 0 33 M 01:05:38 02:08:51 1 32 M 01:06:26 02:09:28 2 31 M 01:06:49 02:10:42 3 38 M 01:06:16 02:13:45 4 31 M 01:06:32 02:13:59 data . dtypes age int64 gender object split object final object dtype: object # lets convert split and final to times def convert_time ( s ): h , m , s = map ( int , s . split ( ':' )) return pd . datetools . timedelta ( hours = h , minutes = m , seconds = s ) data = pd . read_csv ( 'marathon-data.csv' , converters = { 'split' : convert_time , 'final' : convert_time }) data . head () AttributeError: module 'pandas' has no attribute 'datetools'","title":"Example: Exploring Marathon Finishing Times"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: Handwritten Digits \u00b6 Example: Handwritten Digits Example: Handwritten Digits \u00b6 For an example of where this might be useful, let\u2019s look at an interesting visualization of some handwritten digits data. This data is included in Scikit-Learn, and consists of nearly 2,000 8\u00d78 thumbnails showing various handwritten digits. For now, let\u2019s start by downloading the digits data and visualizing several of the exam\u2010 ple images with plt.imshow() import matplotlib.pyplot as plt # load images of the digit 0 through 5 and visualize several of them from sklearn.datasets import load_digits digits = load_digits ( n_class = 6 ) fig , ax = plt . subplots ( 8 , 8 , figsize = ( 6 , 6 )) for i , axi in enumerate ( ax . flat ): axi . imshow ( digits . images [ i ], cmap = 'binary' ) axi . set ( xticks = [], yticks = []) Because each digit is defined by the hue of its 64 pixels, we can consider each digit to be a point lying in 64-dimensional space: each dimension represents the brightness of one pixel. But visualizing relationships in such high-dimensional spaces can be extremely difficult. One way to approach this is to use a dimensionality reduction technique such as manifold learning to reduce the dimensionality of the data while maintaining the relationships of interest. Dimensionality reduction is an example of unsupervised machine learning Deferring the discussion of these details, let\u2019s take a look at a two-dimensional mani\u2010 fold learning projection of this digits data from sklearn.manifold import Isomap iso = Isomap ( n_components = 2 ) projection = iso . fit_transform ( digits . data ) # plot the results plt . scatter ( projection [:, 0 ], projection [:, 1 ], lw = 0.1 , c = digits . target , cmap = plt . cm . get_cmap ( 'cubehelix' , 6 )) plt . colorbar ( ticks = range ( 6 ), label = 'digit value' ) plt . clim ( - 0.5 , 5.5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:348: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue. self._fit_transform(X) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient. self._set_intXint(row, col, x.flat[0]) The projection also gives us some interesting insights on the relationships within the dataset: for example, the ranges of 5 and 3 nearly overlap in this projection, indicating that some handwritten fives and threes are difficult to distinguish, and therefore more likely to be confused by an automated classification algorithm. Other values, like 0 and 1, are more distantly separated, and therefore much less likely to be con\u2010 fused. This observation agrees with our intuition, because 5 and 3 look much more similar than do 0 and 1.","title":"Example Handwritten Digits"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/#example-handwritten-digits","text":"Example: Handwritten Digits","title":"Example: Handwritten Digits"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/#example-handwritten-digits_1","text":"For an example of where this might be useful, let\u2019s look at an interesting visualization of some handwritten digits data. This data is included in Scikit-Learn, and consists of nearly 2,000 8\u00d78 thumbnails showing various handwritten digits. For now, let\u2019s start by downloading the digits data and visualizing several of the exam\u2010 ple images with plt.imshow() import matplotlib.pyplot as plt # load images of the digit 0 through 5 and visualize several of them from sklearn.datasets import load_digits digits = load_digits ( n_class = 6 ) fig , ax = plt . subplots ( 8 , 8 , figsize = ( 6 , 6 )) for i , axi in enumerate ( ax . flat ): axi . imshow ( digits . images [ i ], cmap = 'binary' ) axi . set ( xticks = [], yticks = []) Because each digit is defined by the hue of its 64 pixels, we can consider each digit to be a point lying in 64-dimensional space: each dimension represents the brightness of one pixel. But visualizing relationships in such high-dimensional spaces can be extremely difficult. One way to approach this is to use a dimensionality reduction technique such as manifold learning to reduce the dimensionality of the data while maintaining the relationships of interest. Dimensionality reduction is an example of unsupervised machine learning Deferring the discussion of these details, let\u2019s take a look at a two-dimensional mani\u2010 fold learning projection of this digits data from sklearn.manifold import Isomap iso = Isomap ( n_components = 2 ) projection = iso . fit_transform ( digits . data ) # plot the results plt . scatter ( projection [:, 0 ], projection [:, 1 ], lw = 0.1 , c = digits . target , cmap = plt . cm . get_cmap ( 'cubehelix' , 6 )) plt . colorbar ( ticks = range ( 6 ), label = 'digit value' ) plt . clim ( - 0.5 , 5.5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:348: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue. self._fit_transform(X) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient. self._set_intXint(row, col, x.flat[0]) The projection also gives us some interesting insights on the relationships within the dataset: for example, the ranges of 5 and 3 nearly overlap in this projection, indicating that some handwritten fives and threes are difficult to distinguish, and therefore more likely to be confused by an automated classification algorithm. Other values, like 0 and 1, are more distantly separated, and therefore much less likely to be con\u2010 fused. This observation agrees with our intuition, because 5 and 3 look much more similar than do 0 and 1.","title":"Example: Handwritten Digits"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: Visualizing a M\u00f6bius strip \u00b6 Example: Visualizing a M\u00f6bius strip Example: Visualizing a M\u00f6bius strip \u00b6 A M\u00f6bius strip is similar to a strip of paper glued into a loop with a half-twist. Topo\u2010 logically, it\u2019s quite interesting because despite appearances it has only a single side! Here we will visualize such an object using Matplotlib\u2019s three-dimensional tools. The key to creating the M\u00f6bius strip is to think about its parameterization: it\u2019s a two-dimensional strip, so we need two intrinsic dimensions. Let\u2019s call them \u03b8, which ranges from 0 to 2\u03c0 around the loop, and w which ranges from \u20131 to 1 across the width of the strip: from mpl_toolkits import mplot3d % matplotlib notebook import numpy as np import matplotlib.pyplot as plt theta = np . linspace ( 0 , 2 * np . pi , 30 ) w = np . linspace ( - 0.25 , 0.25 , 8 ) w , theta = np . meshgrid ( w , theta ) Now from this parameterization, we must determine the (x, y, z) positions of the embedded strip. Thinking about it, we might realize that there are two rotations happening: one is the position of the loop about its center (what we\u2019ve called \u03b8), while the other is the twist\u2010 ing of the strip about its axis (we\u2019ll call this \u03d5). For a M\u00f6bius strip, we must have the strip make half a twist during a full loop, or \u0394\u03d5 = \u0394\u03b8/2. phi = 0.5 * theta Now we use our recollection of trigonometry to derive the three-dimensional embed\u2010 ding. We\u2019ll define r, the distance of each point from the center, and use this to find the embedded x, y, z coordinates: # radius in x-y plane r = 1 + w * np . cos ( phi ) x = np . ravel ( r * np . cos ( theta )) y = np . ravel ( r * np . sin ( theta )) z = np . ravel ( w * np . sin ( theta )) Finally, to plot the object, we must make sure the triangulation is correct. The best way to do this is to define the triangulation within the underlying parameterization, and then let Matplotlib project this triangulation into the three-dimensional space of the M\u00f6bius strip. This can be accomplished as follows # triangulate in the underlying parametrization from matplotlib.tri import Triangulation tri = Triangulation ( np . ravel ( w ), np . ravel ( theta )) ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , triangles = tri . triangles , cmap = 'viridis' , linewidth = 0.2 ) ax . set_xlim ( - 1 , 1 ); ax . set_ylim ( - 1 , 1 ); ax . set_zlim ( - 1 , 1 ); <IPython.core.display.Javascript object> Combining all of these techniques, it is possible to create and display a wide variety of three-dimensional objects and patterns in Matplotlib.","title":"Example Visualizing a Mobius Strip"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/#example-visualizing-a-mobius-strip","text":"Example: Visualizing a M\u00f6bius strip","title":"Example: Visualizing a M\u00f6bius strip"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/#example-visualizing-a-mobius-strip_1","text":"A M\u00f6bius strip is similar to a strip of paper glued into a loop with a half-twist. Topo\u2010 logically, it\u2019s quite interesting because despite appearances it has only a single side! Here we will visualize such an object using Matplotlib\u2019s three-dimensional tools. The key to creating the M\u00f6bius strip is to think about its parameterization: it\u2019s a two-dimensional strip, so we need two intrinsic dimensions. Let\u2019s call them \u03b8, which ranges from 0 to 2\u03c0 around the loop, and w which ranges from \u20131 to 1 across the width of the strip: from mpl_toolkits import mplot3d % matplotlib notebook import numpy as np import matplotlib.pyplot as plt theta = np . linspace ( 0 , 2 * np . pi , 30 ) w = np . linspace ( - 0.25 , 0.25 , 8 ) w , theta = np . meshgrid ( w , theta ) Now from this parameterization, we must determine the (x, y, z) positions of the embedded strip. Thinking about it, we might realize that there are two rotations happening: one is the position of the loop about its center (what we\u2019ve called \u03b8), while the other is the twist\u2010 ing of the strip about its axis (we\u2019ll call this \u03d5). For a M\u00f6bius strip, we must have the strip make half a twist during a full loop, or \u0394\u03d5 = \u0394\u03b8/2. phi = 0.5 * theta Now we use our recollection of trigonometry to derive the three-dimensional embed\u2010 ding. We\u2019ll define r, the distance of each point from the center, and use this to find the embedded x, y, z coordinates: # radius in x-y plane r = 1 + w * np . cos ( phi ) x = np . ravel ( r * np . cos ( theta )) y = np . ravel ( r * np . sin ( theta )) z = np . ravel ( w * np . sin ( theta )) Finally, to plot the object, we must make sure the triangulation is correct. The best way to do this is to define the triangulation within the underlying parameterization, and then let Matplotlib project this triangulation into the three-dimensional space of the M\u00f6bius strip. This can be accomplished as follows # triangulate in the underlying parametrization from matplotlib.tri import Triangulation tri = Triangulation ( np . ravel ( w ), np . ravel ( theta )) ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , triangles = tri . triangles , cmap = 'viridis' , linewidth = 0.2 ) ax . set_xlim ( - 1 , 1 ); ax . set_ylim ( - 1 , 1 ); ax . set_zlim ( - 1 , 1 ); <IPython.core.display.Javascript object> Combining all of these techniques, it is possible to create and display a wide variety of three-dimensional objects and patterns in Matplotlib.","title":"Example: Visualizing a M\u00f6bius strip"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: California Cities \u00b6 Example: California Cities Example: California Cities \u00b6 Recall that in \u201cCustomizing Plot Legends\u201d, we demonstrated the use of size and color in a scatter plot to convey information about the location, size, and population of California cities. Here, we\u2019ll create this plot again, but using Basemap to put the data in context. import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap cities = pd . read_csv ( '../data/california_cities.csv' ) cities . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 city latd longd elevation_m elevation_ft population_total area_total_sq_mi area_land_sq_mi area_water_sq_mi area_total_km2 area_land_km2 area_water_km2 area_water_percent 0 0 Adelanto 34.576111 -117.432778 875.0 2871.0 31765 56.027 56.009 0.018 145.107 145.062 0.046 0.03 1 1 AgouraHills 34.153333 -118.761667 281.0 922.0 20330 7.822 7.793 0.029 20.260 20.184 0.076 0.37 2 2 Alameda 37.756111 -122.274444 NaN 33.0 75467 22.960 10.611 12.349 59.465 27.482 31.983 53.79 3 3 Albany 37.886944 -122.297778 NaN 43.0 18969 5.465 1.788 3.677 14.155 4.632 9.524 67.28 4 4 Alhambra 34.081944 -118.135000 150.0 492.0 83089 7.632 7.631 0.001 19.766 19.763 0.003 0.01 # extract the data we're interested in lat = cities [ 'latd' ] . values lon = cities [ 'longd' ] . values population = cities [ 'population_total' ] . values area = cities [ 'area_total_km2' ] . values # 1. Draw the map background fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'h' , lat_0 = 37.5 , lon_0 =- 119 , width = 1E6 , height = 1.2E6 ) m . shadedrelief () m . drawcoastlines ( color = 'gray' ) m . drawcountries ( color = 'gray' ) m . drawstates ( color = 'gray' ) # 2. scatter city data, with color reflecting population and size reflecting area m . scatter ( lon , lat , latlon = True , c = np . log10 ( population ), s = area , cmap = 'Reds' , alpha = 0.5 ) # 3. Create colobar and legned plt . colorbar ( label = r '$log_ {10} ({\\rm population})$' ) plt . clim ( 3 , 7 ) # 4. make legend with dummy points for a in [ 100 , 300 , 500 ]: plt . scatter ([],[], c = 'k' , alpha = 0.5 , s = a , label = str ( a ) + 'km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , labelspacing = 1 , loc = 'lower left' ) <matplotlib.legend.Legend at 0x7f0308093b50>","title":"example California cities"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/#example-california-cities","text":"Example: California Cities","title":"Example: California Cities"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/#example-california-cities_1","text":"Recall that in \u201cCustomizing Plot Legends\u201d, we demonstrated the use of size and color in a scatter plot to convey information about the location, size, and population of California cities. Here, we\u2019ll create this plot again, but using Basemap to put the data in context. import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap cities = pd . read_csv ( '../data/california_cities.csv' ) cities . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 city latd longd elevation_m elevation_ft population_total area_total_sq_mi area_land_sq_mi area_water_sq_mi area_total_km2 area_land_km2 area_water_km2 area_water_percent 0 0 Adelanto 34.576111 -117.432778 875.0 2871.0 31765 56.027 56.009 0.018 145.107 145.062 0.046 0.03 1 1 AgouraHills 34.153333 -118.761667 281.0 922.0 20330 7.822 7.793 0.029 20.260 20.184 0.076 0.37 2 2 Alameda 37.756111 -122.274444 NaN 33.0 75467 22.960 10.611 12.349 59.465 27.482 31.983 53.79 3 3 Albany 37.886944 -122.297778 NaN 43.0 18969 5.465 1.788 3.677 14.155 4.632 9.524 67.28 4 4 Alhambra 34.081944 -118.135000 150.0 492.0 83089 7.632 7.631 0.001 19.766 19.763 0.003 0.01 # extract the data we're interested in lat = cities [ 'latd' ] . values lon = cities [ 'longd' ] . values population = cities [ 'population_total' ] . values area = cities [ 'area_total_km2' ] . values # 1. Draw the map background fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'h' , lat_0 = 37.5 , lon_0 =- 119 , width = 1E6 , height = 1.2E6 ) m . shadedrelief () m . drawcoastlines ( color = 'gray' ) m . drawcountries ( color = 'gray' ) m . drawstates ( color = 'gray' ) # 2. scatter city data, with color reflecting population and size reflecting area m . scatter ( lon , lat , latlon = True , c = np . log10 ( population ), s = area , cmap = 'Reds' , alpha = 0.5 ) # 3. Create colobar and legned plt . colorbar ( label = r '$log_ {10} ({\\rm population})$' ) plt . clim ( 3 , 7 ) # 4. make legend with dummy points for a in [ 100 , 300 , 500 ]: plt . scatter ([],[], c = 'k' , alpha = 0.5 , s = a , label = str ( a ) + 'km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , labelspacing = 1 , loc = 'lower left' ) <matplotlib.legend.Legend at 0x7f0308093b50>","title":"Example: California Cities"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/","text":"================ by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: Surface Temperature Data \u00b6 Example: Surface Temperature Data Unfortunately, no dataset found ! Example: Surface Temperature Data \u00b6 As an example of visualizing some more continuous geographic data, let\u2019s consider the \u201cpolar vortex\u201d that hit the eastern half of the United States in January 2014. A great source for any sort of climatic data is NASA\u2019s Goddard Institute for Space Stud\u2010 ies. Here we\u2019ll use the GIS 250 temperature data, which we can download using shell commands (these commands may have to be modified on Windows machines). The data used here was downloaded on 6/12/2016, and the file size is approximately 9 MB: ! curl - O http : // data . giss . nasa . gov / pub / gistemp / gistemp250 . nc . gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 263 100 263 0 0 468 0 --:--:-- --:--:-- --:--:-- 467 ! gunzip gistemp250 . nc . gz gzip: gistemp250.nc.gz: not in gzip format The data comes in NetCDF format, which can be read in Python by the netCDF4 library. You can install this library as shown here: # ! conda install netcdf4 % matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap from PIL import Image Unfortunately, no dataset found ! \u00b6 from netCDF4 import Dataset data = Dataset ( 'gistemp250.nc' ) from netCDF4 import date2index from datetime import datetime timeindex = date2index ( datetime ( 2014 , 1 , 15 ), data . variables [ 'time' ]) lat = data . variables [ 'lat' ][:] lon = data . variables [ 'lon' ][:] lon , lat = np . meshgrid ( lon , lat ) temp_anomaly = data . variables [ 'tempanomaly' ][ timeindex ] fig = plt . figure ( figsize = ( 10 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'c' , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . shadedrelief ( scale = 0.5 ) m . pcolormesh ( lon , lat , temp_anomaly , latlon = True , cmap = 'RdBu_r' ) plt . clim ( - 8 , 8 ) m . drawcoastlines ( color = 'lightgray' ) plt . title ( 'January 2014 Temperature Anomaly' ) plt . colorbar ( label = 'temperature anomaly (\u00b0C)' );","title":"Example surface temperature data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/#example-surface-temperature-data","text":"Example: Surface Temperature Data Unfortunately, no dataset found !","title":"Example: Surface Temperature Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/#example-surface-temperature-data_1","text":"As an example of visualizing some more continuous geographic data, let\u2019s consider the \u201cpolar vortex\u201d that hit the eastern half of the United States in January 2014. A great source for any sort of climatic data is NASA\u2019s Goddard Institute for Space Stud\u2010 ies. Here we\u2019ll use the GIS 250 temperature data, which we can download using shell commands (these commands may have to be modified on Windows machines). The data used here was downloaded on 6/12/2016, and the file size is approximately 9 MB: ! curl - O http : // data . giss . nasa . gov / pub / gistemp / gistemp250 . nc . gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 263 100 263 0 0 468 0 --:--:-- --:--:-- --:--:-- 467 ! gunzip gistemp250 . nc . gz gzip: gistemp250.nc.gz: not in gzip format The data comes in NetCDF format, which can be read in Python by the netCDF4 library. You can install this library as shown here: # ! conda install netcdf4 % matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap from PIL import Image","title":"Example: Surface Temperature Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/#unfortunately-no-dataset-found","text":"from netCDF4 import Dataset data = Dataset ( 'gistemp250.nc' ) from netCDF4 import date2index from datetime import datetime timeindex = date2index ( datetime ( 2014 , 1 , 15 ), data . variables [ 'time' ]) lat = data . variables [ 'lat' ][:] lon = data . variables [ 'lon' ][:] lon , lat = np . meshgrid ( lon , lat ) temp_anomaly = data . variables [ 'tempanomaly' ][ timeindex ] fig = plt . figure ( figsize = ( 10 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'c' , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . shadedrelief ( scale = 0.5 ) m . pcolormesh ( lon , lat , temp_anomaly , latlon = True , cmap = 'RdBu_r' ) plt . clim ( - 8 , 8 ) m . drawcoastlines ( color = 'lightgray' ) plt . title ( 'January 2014 Temperature Anomaly' ) plt . colorbar ( label = 'temperature anomaly (\u00b0C)' );","title":"Unfortunately, no dataset found !"},{"location":"bootcampsnotes/","text":"BootCamps Notes \u00b6 The structure of this directory is as follow: Here you will find notes related to: \u00b6 Numpy \ud83d\udd78\ufe0f >>> 0. NumPy-Arrays >>> 1. NumPy-Indexing-and-Selection >>> 2. NumPy-Operations >>> 3. NumPy-Exercises >>> 4. NumPy-Exercises-Solutions Pandas \ud83d\udc3c >>> 0. Intro-to-Pandas >>> 1. Series >>> 2. DataFrames >>> 3. Missing-Data >>> 4. Groupby >>> 5. Operations >>> 6. Data-Input-and-Output >>> 7. Pandas-Exercises >>> 8. Pandas-Exercises-Solutions Pytorch Basics \ud83e\udd40 >>> 0. Tensor-Basics >>> 1. Tensor-Operations >>> 2. PyTorch-Basics-Exercises >>> 3. PyTorch-Basics-Exercises-Solutions Pytorch for Deeplearning Bootcamp: \ud83c\udf1f a. ANN - Artificial Neural Networks >>> 0. PyTorch-Gradients >>> 1. Linear-Regression-with-PyTorch >>> 2. DataSets-with-Pytorch >>> 3. Basic-PyTorch-NN >>> 4. a-Full-ANN-Code-Along-Regression >>> 5. b-Full-ANN-Code-Along-Classification >>> 6. Neural-Network-Exercises >>> 7. Neural-Network-Exercises-Solutions >>> 8. Recap-Saving-and-Loading-Trained-Models b. CNN - Convolutional Neural Networks >>> 0. MNIST-ANN-Code-Along >>> 1. MNIST-with-CNN >>> 2. CIFAR-CNN-Code-Along >>> 3. Loading-Real-Image-Data >>> 4. CNN-on-Custom-Images >>> 5. CNN-Exercises >>> 6. CNN-Exercises-Solutions c. RNN - Recurrent Neural Networks >>> 0. Basic-RNN >>> 1. RNN-on-a-Time-Series >>> 2. RNN-Exercises >>> 3. RNN-Exercises-Solutions d. NLP with PyTorch >>> 0. RNN-for-Text-Generation e. Using GPU >>> 0. Using-GPU-and-CUDA Tensorflow BootCamp: \ud83c\udf0a a. Colab Basics \ud83d\udc15 >>> 0 Demo >>> 1. Installing Tensorflow >>> 2. Loading Data b. Machine Learning Basics \ud83c\udf3d >>> 1. Linear Classification >>> 2. Linear Regression c. ANN - Artificial Neural Networks \ud83c\udf5e >>> 1. ANN MNIST >>> 2. ANN Regression d. CNN - Convolutional Neural Networks \ud83c\udf5f >>> 1. Fashion MNIST >>> 2. CIFAR >>> 3. Improved CIFAR e. RNN - Recurrent Neural Networks \u26c8\ufe0f >>> 1. Autoregressive Model >>> 2. Simple RNN sine >>> 3. RNN Shape >>> 4. LSTM Non-linear >>> 5. Long Distance >>> 6. RNN MNIST >>> 7. Stock Returns f. NLP - Natural Language Processing \ud83c\udf1a >>> 1. Text Preprocessing >>> 2. Spam Detection CNN >>> 3. Spam Detection RNN g. Recommendar Systems \ud83e\ude85 >>> 1. Recommendar System h. Transfer Learning \ud83e\udead >>> 1. Transfer Learning >>> 2. Transfer Learning with Data Augmentation i. GANs - Generative Adversarial Networks \ud83c\udf80 >>> 1. GAN j. Advance Tensorflow \ud83c\udf28\ufe0f >>> 1. Tensorflow serving >>> 2. Mirror Strategy >>> 3. TFLite >>> 4. TPU k. Low-Level Tensorflow \ud83e\ude84 >>> 1. Basic Computation >>> 2. Variables and Gradients >>> 3. Build your own Model soon to be added... \u00b6 matplotlib seaborn Python for DataSceince and Machine Learning","title":"Bootcamps Notes"},{"location":"bootcampsnotes/#bootcamps-notes","text":"The structure of this directory is as follow:","title":"BootCamps Notes"},{"location":"bootcampsnotes/#here-you-will-find-notes-related-to","text":"Numpy \ud83d\udd78\ufe0f >>> 0. NumPy-Arrays >>> 1. NumPy-Indexing-and-Selection >>> 2. NumPy-Operations >>> 3. NumPy-Exercises >>> 4. NumPy-Exercises-Solutions Pandas \ud83d\udc3c >>> 0. Intro-to-Pandas >>> 1. Series >>> 2. DataFrames >>> 3. Missing-Data >>> 4. Groupby >>> 5. Operations >>> 6. Data-Input-and-Output >>> 7. Pandas-Exercises >>> 8. Pandas-Exercises-Solutions Pytorch Basics \ud83e\udd40 >>> 0. Tensor-Basics >>> 1. Tensor-Operations >>> 2. PyTorch-Basics-Exercises >>> 3. PyTorch-Basics-Exercises-Solutions Pytorch for Deeplearning Bootcamp: \ud83c\udf1f a. ANN - Artificial Neural Networks >>> 0. PyTorch-Gradients >>> 1. Linear-Regression-with-PyTorch >>> 2. DataSets-with-Pytorch >>> 3. Basic-PyTorch-NN >>> 4. a-Full-ANN-Code-Along-Regression >>> 5. b-Full-ANN-Code-Along-Classification >>> 6. Neural-Network-Exercises >>> 7. Neural-Network-Exercises-Solutions >>> 8. Recap-Saving-and-Loading-Trained-Models b. CNN - Convolutional Neural Networks >>> 0. MNIST-ANN-Code-Along >>> 1. MNIST-with-CNN >>> 2. CIFAR-CNN-Code-Along >>> 3. Loading-Real-Image-Data >>> 4. CNN-on-Custom-Images >>> 5. CNN-Exercises >>> 6. CNN-Exercises-Solutions c. RNN - Recurrent Neural Networks >>> 0. Basic-RNN >>> 1. RNN-on-a-Time-Series >>> 2. RNN-Exercises >>> 3. RNN-Exercises-Solutions d. NLP with PyTorch >>> 0. RNN-for-Text-Generation e. Using GPU >>> 0. Using-GPU-and-CUDA Tensorflow BootCamp: \ud83c\udf0a a. Colab Basics \ud83d\udc15 >>> 0 Demo >>> 1. Installing Tensorflow >>> 2. Loading Data b. Machine Learning Basics \ud83c\udf3d >>> 1. Linear Classification >>> 2. Linear Regression c. ANN - Artificial Neural Networks \ud83c\udf5e >>> 1. ANN MNIST >>> 2. ANN Regression d. CNN - Convolutional Neural Networks \ud83c\udf5f >>> 1. Fashion MNIST >>> 2. CIFAR >>> 3. Improved CIFAR e. RNN - Recurrent Neural Networks \u26c8\ufe0f >>> 1. Autoregressive Model >>> 2. Simple RNN sine >>> 3. RNN Shape >>> 4. LSTM Non-linear >>> 5. Long Distance >>> 6. RNN MNIST >>> 7. Stock Returns f. NLP - Natural Language Processing \ud83c\udf1a >>> 1. Text Preprocessing >>> 2. Spam Detection CNN >>> 3. Spam Detection RNN g. Recommendar Systems \ud83e\ude85 >>> 1. Recommendar System h. Transfer Learning \ud83e\udead >>> 1. Transfer Learning >>> 2. Transfer Learning with Data Augmentation i. GANs - Generative Adversarial Networks \ud83c\udf80 >>> 1. GAN j. Advance Tensorflow \ud83c\udf28\ufe0f >>> 1. Tensorflow serving >>> 2. Mirror Strategy >>> 3. TFLite >>> 4. TPU k. Low-Level Tensorflow \ud83e\ude84 >>> 1. Basic Computation >>> 2. Variables and Gradients >>> 3. Build your own Model","title":"Here you will find notes related to:"},{"location":"bootcampsnotes/#soon-to-be-added","text":"matplotlib seaborn Python for DataSceince and Machine Learning","title":"soon to be added..."},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_MNIST/","text":"================ by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348.9MB 52kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 49.1MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 54.6MB/s 2.0.0-beta1 # Load in the data mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) # Compile the model model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # Train the model r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 10 ) WARNING: Logging before flag parsing goes to stderr. W0718 16:30:06.976897 139639418169216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 7s 111us/sample - loss: 0.2949 - accuracy: 0.9134 - val_loss: 0.1339 - val_accuracy: 0.9594 Epoch 2/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.1452 - accuracy: 0.9564 - val_loss: 0.1063 - val_accuracy: 0.9681 Epoch 3/10 60000/60000 [==============================] - 6s 95us/sample - loss: 0.1100 - accuracy: 0.9664 - val_loss: 0.0879 - val_accuracy: 0.9722 Epoch 4/10 60000/60000 [==============================] - 6s 94us/sample - loss: 0.0902 - accuracy: 0.9721 - val_loss: 0.0737 - val_accuracy: 0.9766 Epoch 5/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0765 - accuracy: 0.9758 - val_loss: 0.0747 - val_accuracy: 0.9748 Epoch 6/10 60000/60000 [==============================] - 6s 94us/sample - loss: 0.0672 - accuracy: 0.9789 - val_loss: 0.0699 - val_accuracy: 0.9772 Epoch 7/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0586 - accuracy: 0.9820 - val_loss: 0.0729 - val_accuracy: 0.9781 Epoch 8/10 60000/60000 [==============================] - 6s 94us/sample - loss: 0.0544 - accuracy: 0.9815 - val_loss: 0.0697 - val_accuracy: 0.9780 Epoch 9/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0505 - accuracy: 0.9834 - val_loss: 0.0690 - val_accuracy: 0.9789 Epoch 10/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0447 - accuracy: 0.9850 - val_loss: 0.0692 - val_accuracy: 0.9806 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7effe00f2ac8> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7effe0092518> # Evaluate the model print ( model . evaluate ( x_test , y_test )) 10000/10000 [==============================] - 1s 55us/sample - loss: 0.0692 - accuracy: 0.9806 [0.06924617666350968, 0.9806] # Plot confusion matrix from sklearn.metrics import confusion_matrix import numpy as np import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) # Do these results make sense? # It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc. Confusion matrix, without normalization [[ 972 0 1 1 0 0 2 1 2 1] [ 0 1126 2 2 0 0 2 0 3 0] [ 2 1 1015 3 1 0 1 4 4 1] [ 1 0 1 996 0 2 0 3 3 4] [ 1 1 2 1 959 0 5 3 1 9] [ 2 1 0 12 1 868 2 2 3 1] [ 5 3 1 1 1 3 939 0 5 0] [ 2 10 12 3 0 0 0 994 3 4] [ 6 0 3 2 3 1 0 2 953 4] [ 2 2 0 3 12 3 0 3 0 984]] # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( y_test [ i ], p_test [ i ])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 ANN MNIST"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_Regression/","text":"================ by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # Other imports import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D # Make the dataset N = 1000 X = np . random . random (( N , 2 )) * 6 - 3 # uniformly distributed between (-3, +3) Y = np . cos ( 2 * X [:, 0 ]) + np . cos ( 3 * X [:, 1 ]) This implements the function: \\[ y = \\cos(2x_1) + cos(3x_2) \\] # Plot it fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X [:, 0 ], X [:, 1 ], Y ) # plt.show() <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f2e953549e8> # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( 128 , input_shape = ( 2 ,), activation = 'relu' ), tf . keras . layers . Dense ( 1 ) ]) # Compile and fit opt = tf . keras . optimizers . Adam ( 0.01 ) model . compile ( optimizer = opt , loss = 'mse' ) r = model . fit ( X , Y , epochs = 100 ) Train on 1000 samples Epoch 1/100 1000/1000 [==============================] - 0s 493us/sample - loss: 0.9276 Epoch 2/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.9060 Epoch 3/100 1000/1000 [==============================] - 0s 68us/sample - loss: 0.8808 Epoch 4/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.8457 Epoch 5/100 1000/1000 [==============================] - 0s 57us/sample - loss: 0.8115 Epoch 6/100 1000/1000 [==============================] - 0s 59us/sample - loss: 0.7682 Epoch 7/100 1000/1000 [==============================] - 0s 59us/sample - loss: 0.6904 Epoch 8/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.6319 Epoch 9/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.5543 Epoch 10/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.5207 Epoch 11/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.4973 Epoch 12/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.4889 Epoch 13/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.4715 Epoch 14/100 1000/1000 [==============================] - 0s 55us/sample - loss: 0.4714 Epoch 15/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4387 Epoch 16/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.4519 Epoch 17/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4498 Epoch 18/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4180 Epoch 19/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4163 Epoch 20/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.4128 Epoch 21/100 1000/1000 [==============================] - 0s 67us/sample - loss: 0.4110 Epoch 22/100 1000/1000 [==============================] - 0s 62us/sample - loss: 0.3984 Epoch 23/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.3825 Epoch 24/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.3734 Epoch 25/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.4398 Epoch 26/100 1000/1000 [==============================] - 0s 57us/sample - loss: 0.3796 Epoch 27/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.3603 Epoch 28/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.3120 Epoch 29/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.2797 Epoch 30/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.2815 Epoch 31/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.2600 Epoch 32/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.2221 Epoch 33/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.2036 Epoch 34/100 1000/1000 [==============================] - 0s 59us/sample - loss: 0.1905 Epoch 35/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.1502 Epoch 36/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.1368 Epoch 37/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.1300 Epoch 38/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.1043 Epoch 39/100 1000/1000 [==============================] - 0s 66us/sample - loss: 0.1022 Epoch 40/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0823 Epoch 41/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0787 Epoch 42/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0593 Epoch 43/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0528 Epoch 44/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0616 Epoch 45/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0351 Epoch 46/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0273 Epoch 47/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0288 Epoch 48/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0281 Epoch 49/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0362 Epoch 50/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0585 Epoch 51/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0842 Epoch 52/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.0780 Epoch 53/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0700 Epoch 54/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0232 Epoch 55/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0149 Epoch 56/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0159 Epoch 57/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0140 Epoch 58/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0108 Epoch 59/100 1000/1000 [==============================] - 0s 66us/sample - loss: 0.0088 Epoch 60/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0077 Epoch 61/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0132 Epoch 62/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0108 Epoch 63/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0105 Epoch 64/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0077 Epoch 65/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0084 Epoch 66/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0101 Epoch 67/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0093 Epoch 68/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0082 Epoch 69/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0076 Epoch 70/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.0145 Epoch 71/100 1000/1000 [==============================] - 0s 47us/sample - loss: 0.0127 Epoch 72/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0101 Epoch 73/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0058 Epoch 74/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0051 Epoch 75/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0053 Epoch 76/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0073 Epoch 77/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0065 Epoch 78/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0066 Epoch 79/100 1000/1000 [==============================] - 0s 60us/sample - loss: 0.0202 Epoch 80/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0130 Epoch 81/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0093 Epoch 82/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0149 Epoch 83/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0147 Epoch 84/100 1000/1000 [==============================] - 0s 46us/sample - loss: 0.0085 Epoch 85/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0048 Epoch 86/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0041 Epoch 87/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0041 Epoch 88/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0106 Epoch 89/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0112 Epoch 90/100 1000/1000 [==============================] - 0s 47us/sample - loss: 0.0051 Epoch 91/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0121 Epoch 92/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0384 Epoch 93/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0463 Epoch 94/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0190 Epoch 95/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0340 Epoch 96/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0133 Epoch 97/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0062 Epoch 98/100 1000/1000 [==============================] - 0s 67us/sample - loss: 0.0043 Epoch 99/100 1000/1000 [==============================] - 0s 67us/sample - loss: 0.0050 Epoch 100/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0045 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) # Plot the prediction surface fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X [:, 0 ], X [:, 1 ], Y ) # surface plot line = np . linspace ( - 3 , 3 , 50 ) xx , yy = np . meshgrid ( line , line ) Xgrid = np . vstack (( xx . flatten (), yy . flatten ())) . T Yhat = model . predict ( Xgrid ) . flatten () ax . plot_trisurf ( Xgrid [:, 0 ], Xgrid [:, 1 ], Yhat , linewidth = 0.2 , antialiased = True ) plt . show () # Can it extrapolate? # Plot the prediction surface fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X [:, 0 ], X [:, 1 ], Y ) # surface plot line = np . linspace ( - 5 , 5 , 50 ) xx , yy = np . meshgrid ( line , line ) Xgrid = np . vstack (( xx . flatten (), yy . flatten ())) . T Yhat = model . predict ( Xgrid ) . flatten () ax . plot_trisurf ( Xgrid [:, 0 ], Xgrid [:, 1 ], Yhat , linewidth = 0.2 , antialiased = True ) plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 ANN Regression"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Mirrored_Strategy/","text":"================ by Jawad Haider Mirror Strategy Mirror Strategy \u00b6 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , GlobalMaxPooling2D , MaxPooling2D , BatchNormalization from tensorflow.keras.models import Model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API def create_model (): i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( i ) x = BatchNormalization ()( x ) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) return model strategy = tf . distribute . MirroredStrategy () # strategy = tf.distribute.experimental.CentralStorageStrategy() print ( f 'Number of devices: { strategy . num_replicas_in_sync } ' ) Number of devices: 1 with strategy . scope (): model = create_model () model . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) # Fit r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 5 ) WARNING: Logging before flag parsing goes to stderr. W0811 18:03:54.175649 140429139875584 deprecation.py:323] From /tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/layers/normalization.py:457: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 1563 steps, validate on 313 steps Epoch 1/15 1563/1563 [==============================] - 23s 14ms/step - loss: 1.3045 - accuracy: 0.5541 - val_loss: 0.9532 - val_accuracy: 0.6640 Epoch 2/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.8394 - accuracy: 0.7093 - val_loss: 0.8020 - val_accuracy: 0.7339 Epoch 3/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.6854 - accuracy: 0.7643 - val_loss: 0.8026 - val_accuracy: 0.7286 Epoch 4/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.5819 - accuracy: 0.8012 - val_loss: 0.6905 - val_accuracy: 0.7778 Epoch 5/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.4859 - accuracy: 0.8322 - val_loss: 0.6967 - val_accuracy: 0.7767 Epoch 6/15 1563/1563 [==============================] - 15s 10ms/step - loss: 0.4108 - accuracy: 0.8567 - val_loss: 0.6145 - val_accuracy: 0.8024 Epoch 7/15 1563/1563 [==============================] - 15s 10ms/step - loss: 0.3517 - accuracy: 0.8768 - val_loss: 0.6851 - val_accuracy: 0.7975 Epoch 8/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.2934 - accuracy: 0.8977 - val_loss: 0.6551 - val_accuracy: 0.8104 Epoch 9/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.2536 - accuracy: 0.9132 - val_loss: 0.6276 - val_accuracy: 0.8251 Epoch 10/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.2205 - accuracy: 0.9224 - val_loss: 0.6870 - val_accuracy: 0.8156 Epoch 11/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.1921 - accuracy: 0.9327 - val_loss: 0.7153 - val_accuracy: 0.8161 Epoch 12/15 1563/1563 [==============================] - 15s 10ms/step - loss: 0.1732 - accuracy: 0.9413 - val_loss: 0.6962 - val_accuracy: 0.8218 Epoch 13/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.1587 - accuracy: 0.9458 - val_loss: 0.7655 - val_accuracy: 0.8200 Epoch 14/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.1512 - accuracy: 0.9505 - val_loss: 0.8894 - val_accuracy: 0.8010 Epoch 15/15 1556/1563 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9550 W0811 18:07:53.185556 140432385251200 training_arrays.py:309] Your dataset ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 23445 batches). You may need to use the repeat() function when building your dataset. 50000 / 391 127.8772378516624 10000 / 79 126.58227848101266 # Compare this to non-distributed training model2 = create_model () model2 . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) r = model2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 5 ) Train on 50000 samples, validate on 10000 samples Epoch 1/5 50000/50000 [==============================] - 25s 502us/sample - loss: 1.3957 - accuracy: 0.5369 - val_loss: 1.0386 - val_accuracy: 0.6328 Epoch 2/5 50000/50000 [==============================] - 23s 465us/sample - loss: 0.8563 - accuracy: 0.7029 - val_loss: 0.8461 - val_accuracy: 0.7063 Epoch 3/5 50000/50000 [==============================] - 24s 471us/sample - loss: 0.7013 - accuracy: 0.7592 - val_loss: 0.8004 - val_accuracy: 0.7334 Epoch 4/5 50000/50000 [==============================] - 24s 473us/sample - loss: 0.5897 - accuracy: 0.7959 - val_loss: 0.6652 - val_accuracy: 0.7788 Epoch 5/5 50000/50000 [==============================] - 24s 471us/sample - loss: 0.5054 - accuracy: 0.8267 - val_loss: 0.6899 - val_accuracy: 0.7758 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Mirrored Strategy"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Mirrored_Strategy/#mirror-strategy","text":"try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , GlobalMaxPooling2D , MaxPooling2D , BatchNormalization from tensorflow.keras.models import Model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API def create_model (): i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( i ) x = BatchNormalization ()( x ) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) return model strategy = tf . distribute . MirroredStrategy () # strategy = tf.distribute.experimental.CentralStorageStrategy() print ( f 'Number of devices: { strategy . num_replicas_in_sync } ' ) Number of devices: 1 with strategy . scope (): model = create_model () model . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) # Fit r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 5 ) WARNING: Logging before flag parsing goes to stderr. W0811 18:03:54.175649 140429139875584 deprecation.py:323] From /tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/layers/normalization.py:457: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 1563 steps, validate on 313 steps Epoch 1/15 1563/1563 [==============================] - 23s 14ms/step - loss: 1.3045 - accuracy: 0.5541 - val_loss: 0.9532 - val_accuracy: 0.6640 Epoch 2/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.8394 - accuracy: 0.7093 - val_loss: 0.8020 - val_accuracy: 0.7339 Epoch 3/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.6854 - accuracy: 0.7643 - val_loss: 0.8026 - val_accuracy: 0.7286 Epoch 4/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.5819 - accuracy: 0.8012 - val_loss: 0.6905 - val_accuracy: 0.7778 Epoch 5/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.4859 - accuracy: 0.8322 - val_loss: 0.6967 - val_accuracy: 0.7767 Epoch 6/15 1563/1563 [==============================] - 15s 10ms/step - loss: 0.4108 - accuracy: 0.8567 - val_loss: 0.6145 - val_accuracy: 0.8024 Epoch 7/15 1563/1563 [==============================] - 15s 10ms/step - loss: 0.3517 - accuracy: 0.8768 - val_loss: 0.6851 - val_accuracy: 0.7975 Epoch 8/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.2934 - accuracy: 0.8977 - val_loss: 0.6551 - val_accuracy: 0.8104 Epoch 9/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.2536 - accuracy: 0.9132 - val_loss: 0.6276 - val_accuracy: 0.8251 Epoch 10/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.2205 - accuracy: 0.9224 - val_loss: 0.6870 - val_accuracy: 0.8156 Epoch 11/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.1921 - accuracy: 0.9327 - val_loss: 0.7153 - val_accuracy: 0.8161 Epoch 12/15 1563/1563 [==============================] - 15s 10ms/step - loss: 0.1732 - accuracy: 0.9413 - val_loss: 0.6962 - val_accuracy: 0.8218 Epoch 13/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.1587 - accuracy: 0.9458 - val_loss: 0.7655 - val_accuracy: 0.8200 Epoch 14/15 1563/1563 [==============================] - 16s 10ms/step - loss: 0.1512 - accuracy: 0.9505 - val_loss: 0.8894 - val_accuracy: 0.8010 Epoch 15/15 1556/1563 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9550 W0811 18:07:53.185556 140432385251200 training_arrays.py:309] Your dataset ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 23445 batches). You may need to use the repeat() function when building your dataset. 50000 / 391 127.8772378516624 10000 / 79 126.58227848101266 # Compare this to non-distributed training model2 = create_model () model2 . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) r = model2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 5 ) Train on 50000 samples, validate on 10000 samples Epoch 1/5 50000/50000 [==============================] - 25s 502us/sample - loss: 1.3957 - accuracy: 0.5369 - val_loss: 1.0386 - val_accuracy: 0.6328 Epoch 2/5 50000/50000 [==============================] - 23s 465us/sample - loss: 0.8563 - accuracy: 0.7029 - val_loss: 0.8461 - val_accuracy: 0.7063 Epoch 3/5 50000/50000 [==============================] - 24s 471us/sample - loss: 0.7013 - accuracy: 0.7592 - val_loss: 0.8004 - val_accuracy: 0.7334 Epoch 4/5 50000/50000 [==============================] - 24s 473us/sample - loss: 0.5897 - accuracy: 0.7959 - val_loss: 0.6652 - val_accuracy: 0.7788 Epoch 5/5 50000/50000 [==============================] - 24s 471us/sample - loss: 0.5054 - accuracy: 0.8267 - val_loss: 0.6899 - val_accuracy: 0.7758 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Mirror Strategy"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Serving/","text":"================ by Jawad Haider Serving Serving \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # By the way, what is a server / service / API? # Best way to learn is by example # Here is a service that simply returns your IP address in a JSON import requests r = requests . get ( 'https://api.ipify.org?format=json' ) j = r . json () print ( j ) # Our Tensorflow model server is the same, except what it does is much more # complex - it returns the predictions from a ML model! {'ip': '35.224.223.54'} # More imports import numpy as np import matplotlib.pyplot as plt import os import subprocess from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout from tensorflow.keras.models import Model # Load in the data fashion_mnist = tf . keras . datasets . fashion_mnist ( x_train , y_train ), ( x_test , y_test ) = fashion_mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) print ( \"x_test.shape:\" , x_test . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) x_test.shape: (10000, 28, 28) # the data is only 2D! # convolution expects height x width x color x_train = np . expand_dims ( x_train , - 1 ) x_test = np . expand_dims ( x_test , - 1 ) print ( x_train . shape ) (60000, 28, 28, 1) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Conv2D ( 64 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Conv2D ( 128 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 512 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) model . summary () Model: \"model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ conv2d (Conv2D) (None, 13, 13, 32) 320 _________________________________________________________________ conv2d_1 (Conv2D) (None, 6, 6, 64) 18496 _________________________________________________________________ conv2d_2 (Conv2D) (None, 2, 2, 128) 73856 _________________________________________________________________ flatten (Flatten) (None, 512) 0 _________________________________________________________________ dropout (Dropout) (None, 512) 0 _________________________________________________________________ dense (Dense) (None, 512) 262656 _________________________________________________________________ dropout_1 (Dropout) (None, 512) 0 _________________________________________________________________ dense_1 (Dense) (None, 10) 5130 ================================================================= Total params: 360,458 Trainable params: 360,458 Non-trainable params: 0 _________________________________________________________________ # Compile and fit # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) WARNING: Logging before flag parsing goes to stderr. W0810 04:12:02.177557 140535052928896 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 60000 samples, validate on 10000 samples Epoch 1/15 60000/60000 [==============================] - 13s 211us/sample - loss: 0.5198 - accuracy: 0.8071 - val_loss: 0.3919 - val_accuracy: 0.8524 Epoch 2/15 60000/60000 [==============================] - 9s 153us/sample - loss: 0.3541 - accuracy: 0.8675 - val_loss: 0.3549 - val_accuracy: 0.8657 Epoch 3/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.3042 - accuracy: 0.8855 - val_loss: 0.3162 - val_accuracy: 0.8874 Epoch 4/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.2730 - accuracy: 0.8980 - val_loss: 0.2973 - val_accuracy: 0.8944 Epoch 5/15 60000/60000 [==============================] - 9s 153us/sample - loss: 0.2498 - accuracy: 0.9059 - val_loss: 0.3015 - val_accuracy: 0.8904 Epoch 6/15 60000/60000 [==============================] - 9s 153us/sample - loss: 0.2255 - accuracy: 0.9146 - val_loss: 0.2890 - val_accuracy: 0.8939 Epoch 7/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.2101 - accuracy: 0.9189 - val_loss: 0.3112 - val_accuracy: 0.8912 Epoch 8/15 60000/60000 [==============================] - 9s 155us/sample - loss: 0.1933 - accuracy: 0.9271 - val_loss: 0.2828 - val_accuracy: 0.9031 Epoch 9/15 60000/60000 [==============================] - 9s 154us/sample - loss: 0.1812 - accuracy: 0.9319 - val_loss: 0.3133 - val_accuracy: 0.8966 Epoch 10/15 60000/60000 [==============================] - 9s 155us/sample - loss: 0.1653 - accuracy: 0.9366 - val_loss: 0.3063 - val_accuracy: 0.9015 Epoch 11/15 60000/60000 [==============================] - 10s 158us/sample - loss: 0.1526 - accuracy: 0.9410 - val_loss: 0.3212 - val_accuracy: 0.9046 Epoch 12/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.1460 - accuracy: 0.9452 - val_loss: 0.3190 - val_accuracy: 0.9028 Epoch 13/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.1385 - accuracy: 0.9467 - val_loss: 0.3406 - val_accuracy: 0.9021 Epoch 14/15 60000/60000 [==============================] - 9s 153us/sample - loss: 0.1268 - accuracy: 0.9515 - val_loss: 0.3731 - val_accuracy: 0.9014 Epoch 15/15 60000/60000 [==============================] - 9s 154us/sample - loss: 0.1242 - accuracy: 0.9527 - val_loss: 0.3547 - val_accuracy: 0.9034 # Save the model to a temporary directory import tempfile MODEL_DIR = tempfile . gettempdir () version = 1 export_path = os . path . join ( MODEL_DIR , str ( version )) print ( 'export_path = {} \\n ' . format ( export_path )) if os . path . isdir ( export_path ): print ( ' \\n Already saved a model, cleaning up \\n ' ) ! rm - r { export_path } tf . saved_model . save ( model , export_path ) print ( ' \\n Saved model:' ) ! ls - l { export_path } export_path = /tmp/1 Saved model: total 144 drwxr-xr-x 2 root root 4096 Aug 10 04:14 assets -rw-r--r-- 1 root root 136883 Aug 10 04:14 saved_model.pb drwxr-xr-x 2 root root 4096 Aug 10 04:14 variables ! saved_model_cli show -- dir { export_path } -- all MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs: signature_def['__saved_model_init_op']: The given SavedModel SignatureDef contains the following input(s): The given SavedModel SignatureDef contains the following output(s): outputs['__saved_model_init_op'] tensor_info: dtype: DT_INVALID shape: unknown_rank name: NoOp Method name is: signature_def['serving_default']: The given SavedModel SignatureDef contains the following input(s): inputs['input_1'] tensor_info: dtype: DT_FLOAT shape: (-1, 28, 28, 1) name: serving_default_input_1:0 The given SavedModel SignatureDef contains the following output(s): outputs['dense_1'] tensor_info: dtype: DT_FLOAT shape: (-1, 10) name: StatefulPartitionedCall:0 Method name is: tensorflow/serving/predict # This is the same as you would do from your command line, but without the [arch=amd64], and no sudo # You would instead do: # echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\ # curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add - ! echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee / etc / apt / sources . list . d / tensorflow - serving . list && \\ curl https : // storage . googleapis . com / tensorflow - serving - apt / tensorflow - serving . release . pub . gpg | apt - key add - ! apt update deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2943 100 2943 0 0 10819 0 --:--:-- --:--:-- --:--:-- 10819 OK Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,012 B] Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B] Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB] Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 InRelease Get:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB] Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 Release Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Release [564 B] Get:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Release.gpg [833 B] Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease Get:11 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [365 B] Get:12 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [357 B] Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [65.9 kB] Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB] Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [731 kB] Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [597 kB] Get:20 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Packages [12.3 kB] Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [29.0 kB] Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [906 kB] Get:23 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,677 kB] Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [14.2 kB] Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,257 kB] Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [10.8 kB] Get:27 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [805 kB] Fetched 6,402 kB in 3s (1,939 kB/s) Reading package lists... Done Building dependency tree Reading state information... Done 47 packages can be upgraded. Run 'apt list --upgradable' to see them. ! apt - get install tensorflow - model - server Reading package lists... Done Building dependency tree Reading state information... Done The following package was automatically installed and is no longer required: libnvidia-common-410 Use 'apt autoremove' to remove it. The following NEW packages will be installed: tensorflow-model-server 0 upgraded, 1 newly installed, 0 to remove and 47 not upgraded. Need to get 151 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 1.14.0 [151 MB] Fetched 151 MB in 2s (81.4 MB/s) Selecting previously unselected package tensorflow-model-server. (Reading database ... 131289 files and directories currently installed.) Preparing to unpack .../tensorflow-model-server_1.14.0_all.deb ... Unpacking tensorflow-model-server (1.14.0) ... Setting up tensorflow-model-server (1.14.0) ... os . environ [ \"MODEL_DIR\" ] = MODEL_DIR %% bash -- bg nohup tensorflow_model_server \\ -- rest_api_port = 8501 \\ -- model_name = fashion_model \\ -- model_base_path = \"$ {MODEL_DIR} \" > server . log 2 >& 1 Starting job # 0 in a separate thread. ! tail server . log 2019-08-10 04:14:43.484998: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve } 2019-08-10 04:14:43.486899: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2019-08-10 04:14:43.502478: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle. 2019-08-10 04:14:43.544172: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /tmp/1 2019-08-10 04:14:43.552722: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 69454 microseconds. 2019-08-10 04:14:43.552775: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:103] No warmup data file found at /tmp/1/assets.extra/tf_serving_warmup_requests 2019-08-10 04:14:43.552887: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: fashion_model version: 1} 2019-08-10 04:14:43.554155: I tensorflow_serving/model_servers/server.cc:324] Running gRPC ModelServer at 0.0.0.0:8500 ... [evhttp_server.cc : 239] RAW: Entering the event loop ... 2019-08-10 04:14:43.554756: I tensorflow_serving/model_servers/server.cc:344] Exporting HTTP/REST API at:localhost:8501 ... # Label mapping labels = '''T-shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot''' . split ( \" \\n \" ) def show ( idx , title ): plt . figure () plt . imshow ( x_test [ idx ] . reshape ( 28 , 28 ), cmap = 'gray' ) plt . axis ( 'off' ) plt . title ( ' \\n\\n {} ' . format ( title ), fontdict = { 'size' : 16 }) i = np . random . randint ( 0 , len ( x_test )) show ( i , labels [ y_test [ i ]]) # Format some data to pass to the server # { # \"signature_name\": \"serving_default\", # \"instances\": [ an N x H x W x C list ], # } import json data = json . dumps ({ \"signature_name\" : \"serving_default\" , \"instances\" : x_test [ 0 : 3 ] . tolist ()}) print ( data ) {\"signature_name\": \"serving_default\", \"instances\": [[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.00392156862745098], [0.0], [0.0], [0.027450980392156862], [0.0], [0.1450980392156863], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.00784313725490196], [0.0], [0.10588235294117647], [0.32941176470588235], [0.043137254901960784], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4666666666666667], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.34509803921568627], [0.5607843137254902], [0.43137254901960786], [0.0], [0.0], [0.0], [0.0], [0.08627450980392157], [0.36470588235294116], [0.41568627450980394], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.01568627450980392], [0.0], [0.20784313725490197], [0.5058823529411764], [0.47058823529411764], [0.5764705882352941], [0.6862745098039216], [0.615686274509804], [0.6509803921568628], [0.5294117647058824], [0.6039215686274509], [0.6588235294117647], [0.5490196078431373], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.043137254901960784], [0.5372549019607843], [0.5098039215686274], [0.5019607843137255], [0.6274509803921569], [0.6901960784313725], [0.6235294117647059], [0.6549019607843137], [0.6980392156862745], [0.5843137254901961], [0.592156862745098], [0.5647058823529412], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.00784313725490196], [0.00392156862745098], [0.0], [0.011764705882352941], [0.0], [0.0], [0.45098039215686275], [0.4470588235294118], [0.41568627450980394], [0.5372549019607843], [0.6588235294117647], [0.6], [0.611764705882353], [0.6470588235294118], [0.6549019607843137], [0.5607843137254902], [0.615686274509804], [0.6196078431372549], [0.043137254901960784], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.0], [0.0], [0.34901960784313724], [0.5450980392156862], [0.35294117647058826], [0.3686274509803922], [0.6], [0.5843137254901961], [0.5137254901960784], [0.592156862745098], [0.6627450980392157], [0.6745098039215687], [0.5607843137254902], [0.6235294117647059], [0.6627450980392157], [0.18823529411764706], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.01568627450980392], [0.00392156862745098], [0.0], [0.0], [0.0], [0.3843137254901961], [0.5333333333333333], [0.43137254901960786], [0.42745098039215684], [0.43137254901960786], [0.6352941176470588], [0.5294117647058824], [0.5647058823529412], [0.5843137254901961], [0.6235294117647059], [0.6549019607843137], [0.5647058823529412], [0.6196078431372549], [0.6627450980392157], [0.4666666666666667], [0.0]], [[0.0], [0.0], [0.00784313725490196], [0.00784313725490196], [0.00392156862745098], [0.00784313725490196], [0.0], [0.0], [0.0], [0.0], [0.10196078431372549], [0.4235294117647059], [0.4588235294117647], [0.38823529411764707], [0.43529411764705883], [0.4588235294117647], [0.5333333333333333], [0.611764705882353], [0.5254901960784314], [0.6039215686274509], [0.6039215686274509], [0.611764705882353], [0.6274509803921569], [0.5529411764705883], [0.5764705882352941], [0.611764705882353], [0.6980392156862745], [0.0]], [[0.011764705882352941], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.08235294117647059], [0.20784313725490197], [0.3607843137254902], [0.4588235294117647], [0.43529411764705883], [0.403921568627451], [0.45098039215686275], [0.5058823529411764], [0.5254901960784314], [0.5607843137254902], [0.6039215686274509], [0.6470588235294118], [0.6666666666666666], [0.6039215686274509], [0.592156862745098], [0.6039215686274509], [0.5607843137254902], [0.5411764705882353], [0.5882352941176471], [0.6470588235294118], [0.16862745098039217]], [[0.0], [0.0], [0.09019607843137255], [0.21176470588235294], [0.2549019607843137], [0.2980392156862745], [0.3333333333333333], [0.4627450980392157], [0.5019607843137255], [0.4823529411764706], [0.43529411764705883], [0.44313725490196076], [0.4627450980392157], [0.4980392156862745], [0.49019607843137253], [0.5450980392156862], [0.5215686274509804], [0.5333333333333333], [0.6274509803921569], [0.5490196078431373], [0.6078431372549019], [0.6313725490196078], [0.5647058823529412], [0.6078431372549019], [0.6745098039215687], [0.6313725490196078], [0.7411764705882353], [0.24313725490196078]], [[0.0], [0.26666666666666666], [0.3686274509803922], [0.35294117647058826], [0.43529411764705883], [0.4470588235294118], [0.43529411764705883], [0.4470588235294118], [0.45098039215686275], [0.4980392156862745], [0.5294117647058824], [0.5333333333333333], [0.5607843137254902], [0.49411764705882355], [0.4980392156862745], [0.592156862745098], [0.6039215686274509], [0.5607843137254902], [0.5803921568627451], [0.49019607843137253], [0.6352941176470588], [0.6352941176470588], [0.5647058823529412], [0.5411764705882353], [0.6], [0.6352941176470588], [0.7686274509803922], [0.22745098039215686]], [[0.27450980392156865], [0.6627450980392157], [0.5058823529411764], [0.40784313725490196], [0.3843137254901961], [0.39215686274509803], [0.3686274509803922], [0.3803921568627451], [0.3843137254901961], [0.4], [0.4235294117647059], [0.41568627450980394], [0.4666666666666667], [0.47058823529411764], [0.5058823529411764], [0.5843137254901961], [0.611764705882353], [0.6549019607843137], [0.7450980392156863], [0.7450980392156863], [0.7686274509803922], [0.7764705882352941], [0.7764705882352941], [0.7333333333333333], [0.7725490196078432], [0.7411764705882353], [0.7215686274509804], [0.1411764705882353]], [[0.06274509803921569], [0.49411764705882355], [0.6705882352941176], [0.7372549019607844], [0.7372549019607844], [0.7215686274509804], [0.6705882352941176], [0.6], [0.5294117647058824], [0.47058823529411764], [0.49411764705882355], [0.4980392156862745], [0.5725490196078431], [0.7254901960784313], [0.7647058823529411], [0.8196078431372549], [0.8156862745098039], [1.0], [0.8196078431372549], [0.6941176470588235], [0.9607843137254902], [0.9882352941176471], [0.984313725490196], [0.984313725490196], [0.9686274509803922], [0.8627450980392157], [0.807843137254902], [0.19215686274509805]], [[0.0], [0.0], [0.0], [0.047058823529411764], [0.2627450980392157], [0.41568627450980394], [0.6431372549019608], [0.7254901960784313], [0.7803921568627451], [0.8235294117647058], [0.8274509803921568], [0.8235294117647058], [0.8156862745098039], [0.7450980392156863], [0.5882352941176471], [0.3215686274509804], [0.03137254901960784], [0.0], [0.0], [0.0], [0.6980392156862745], [0.8156862745098039], [0.7372549019607844], [0.6862745098039216], [0.6352941176470588], [0.6196078431372549], [0.592156862745098], [0.043137254901960784]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.050980392156862744], [0.2627450980392157], [0.0], [0.0], [0.0], [0.0], [0.19607843137254902], [0.14901960784313725], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.03137254901960784], [0.47058823529411764], [0.8196078431372549], [0.8862745098039215], [0.9686274509803922], [0.9294117647058824], [1.0], [1.0], [1.0], [0.9686274509803922], [0.9333333333333333], [0.9215686274509803], [0.6745098039215687], [0.2823529411764706], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.5372549019607843], [0.9372549019607843], [0.9882352941176471], [0.9529411764705882], [0.9176470588235294], [0.8980392156862745], [0.9333333333333333], [0.9568627450980393], [0.9647058823529412], [0.9411764705882353], [0.9019607843137255], [0.9098039215686274], [0.9372549019607843], [0.9725490196078431], [0.984313725490196], [0.7607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.4], [1.0], [0.9058823529411765], [0.8941176470588236], [0.8901960784313725], [0.8941176470588236], [0.9137254901960784], [0.9019607843137255], [0.9019607843137255], [0.8980392156862745], [0.8941176470588236], [0.9098039215686274], [0.9098039215686274], [0.9058823529411765], [0.8901960784313725], [0.8784313725490196], [0.9882352941176471], [0.7019607843137254], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.9137254901960784], [0.9450980392156862], [0.8980392156862745], [0.9058823529411765], [1.0], [1.0], [0.9333333333333333], [0.9058823529411765], [0.8901960784313725], [0.9333333333333333], [0.9647058823529412], [0.8941176470588236], [0.9019607843137255], [0.8901960784313725], [0.9176470588235294], [0.9215686274509803], [0.8980392156862745], [0.9450980392156862], [0.0784313725490196], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.9725490196078431], [0.9450980392156862], [0.9058823529411765], [1.0], [0.5843137254901961], [0.1843137254901961], [0.9882352941176471], [0.8941176470588236], [1.0], [0.9490196078431372], [0.8470588235294118], [0.9333333333333333], [0.9098039215686274], [1.0], [0.8941176470588236], [0.8627450980392157], [0.9176470588235294], [0.9803921568627451], [0.21176470588235294], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9411764705882353], [0.9098039215686274], [1.0], [0.058823529411764705], [0.0], [1.0], [0.9294117647058824], [0.7490196078431373], [0.0], [0.0], [0.8392156862745098], [1.0], [0.050980392156862744], [0.4823529411764706], [1.0], [0.9176470588235294], [0.9882352941176471], [0.4470588235294118], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.023529411764705882], [1.0], [0.9333333333333333], [0.9372549019607843], [1.0], [0.6941176470588235], [0.0], [1.0], [1.0], [0.0], [0.5098039215686274], [0.4549019607843137], [0.1843137254901961], [0.2549019607843137], [0.16862745098039217], [0.1450980392156863], [1.0], [0.9254901960784314], [0.9764705882352941], [0.6352941176470588], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.12549019607843137], [1.0], [0.9254901960784314], [0.9607843137254902], [1.0], [0.8], [0.0], [1.0], [0.32941176470588235], [0.0], [0.1450980392156863], [0.10980392156862745], [0.12156862745098039], [0.0], [0.09803921568627451], [0.050980392156862744], [1.0], [0.9254901960784314], [0.9764705882352941], [0.7803921568627451], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.20784313725490197], [1.0], [0.9254901960784314], [0.9803921568627451], [0.9803921568627451], [0.9058823529411765], [0.00784313725490196], [1.0], [0.08235294117647059], [0.0], [0.8666666666666667], [1.0], [0.9254901960784314], [0.21176470588235294], [0.9607843137254902], [0.7764705882352941], [0.9529411764705882], [0.9333333333333333], [0.9607843137254902], [0.8745098039215686], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.3137254901960784], [1.0], [0.9294117647058824], [0.9803921568627451], [0.9411764705882353], [1.0], [0.0], [0.0], [0.15294117647058825], [0.615686274509804], [0.0], [0.0], [0.8431372549019608], [0.3686274509803922], [0.0784313725490196], [0.49411764705882355], [1.0], [0.9294117647058824], [0.9372549019607843], [0.9803921568627451], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.396078431372549], [1.0], [0.9215686274509803], [0.9921568627450981], [0.9568627450980393], [0.9529411764705882], [0.5215686274509804], [0.5411764705882353], [0.8156862745098039], [1.0], [0.788235294117647], [0.8392156862745098], [1.0], [0.9019607843137255], [0.027450980392156862], [0.6823529411764706], [1.0], [0.9411764705882353], [0.9333333333333333], [1.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.49411764705882355], [1.0], [0.9137254901960784], [1.0], [0.9725490196078431], [0.9137254901960784], [1.0], [1.0], [0.9411764705882353], [0.9098039215686274], [0.9529411764705882], [0.9529411764705882], [0.9058823529411765], [0.984313725490196], [1.0], [1.0], [0.996078431372549], [0.9529411764705882], [0.9333333333333333], [1.0], [0.011764705882352941], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.5764705882352941], [1.0], [0.9137254901960784], [0.9764705882352941], [0.7098039215686275], [0.9529411764705882], [0.8901960784313725], [0.8784313725490196], [0.9019607843137255], [0.9176470588235294], [0.9019607843137255], [0.9019607843137255], [0.9215686274509803], [0.8941176470588236], [0.9215686274509803], [0.8705882352941177], [0.8117647058823529], [1.0], [0.9254901960784314], [1.0], [0.13725490196078433], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.6392156862745098], [1.0], [0.9607843137254902], [0.8666666666666667], [0.33725490196078434], [1.0], [0.9137254901960784], [0.9137254901960784], [0.9215686274509803], [0.9254901960784314], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9098039215686274], [0.9490196078431372], [0.9058823529411765], [0.49019607843137253], [1.0], [0.9254901960784314], [1.0], [0.21568627450980393], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7098039215686275], [0.996078431372549], [1.0], [0.7843137254901961], [0.27058823529411763], [1.0], [0.8941176470588236], [0.9098039215686274], [0.9176470588235294], [0.9215686274509803], [0.9176470588235294], [0.9176470588235294], [0.9137254901960784], [0.9215686274509803], [0.9450980392156862], [0.9294117647058824], [0.27450980392156865], [1.0], [0.9215686274509803], [0.9647058823529412], [0.2235294117647059], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7725490196078432], [0.9686274509803922], [1.0], [0.7372549019607844], [0.43137254901960786], [1.0], [0.8784313725490196], [0.9137254901960784], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9411764705882353], [0.9921568627450981], [0.27058823529411763], [1.0], [0.9254901960784314], [0.9725490196078431], [0.30196078431372547], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7843137254901961], [0.9647058823529412], [1.0], [0.5843137254901961], [0.5686274509803921], [1.0], [0.8745098039215686], [0.9215686274509803], [0.9176470588235294], [0.9215686274509803], [0.9215686274509803], [0.9215686274509803], [0.9176470588235294], [0.9294117647058824], [0.9137254901960784], [1.0], [0.1843137254901961], [1.0], [0.9372549019607843], [0.9764705882352941], [0.3843137254901961], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.8], [0.9529411764705882], [1.0], [0.43529411764705883], [0.6784313725490196], [1.0], [0.8901960784313725], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9215686274509803], [0.9215686274509803], [0.9372549019607843], [0.8980392156862745], [1.0], [0.07450980392156863], [0.8901960784313725], [0.9647058823529412], [0.9764705882352941], [0.43137254901960786], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7686274509803922], [0.9411764705882353], [1.0], [0.42745098039215684], [0.8352941176470589], [0.9803921568627451], [0.8980392156862745], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9294117647058824], [0.9254901960784314], [0.9294117647058824], [0.8862745098039215], [1.0], [0.21568627450980393], [0.796078431372549], [0.984313725490196], [0.9607843137254902], [0.47058823529411764], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7529411764705882], [0.9529411764705882], [1.0], [0.4470588235294118], [0.9098039215686274], [0.9411764705882353], [0.9098039215686274], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9176470588235294], [0.9294117647058824], [0.9254901960784314], [0.9215686274509803], [0.8980392156862745], [1.0], [0.5254901960784314], [0.6705882352941176], [0.9882352941176471], [0.9568627450980393], [0.5372549019607843], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7411764705882353], [0.984313725490196], [1.0], [0.6039215686274509], [0.9333333333333333], [0.9137254901960784], [0.9254901960784314], [0.9176470588235294], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9333333333333333], [0.9254901960784314], [0.9215686274509803], [0.9098039215686274], [1.0], [0.6509803921568628], [0.49019607843137253], [1.0], [0.9529411764705882], [0.5568627450980392], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7176470588235294], [0.9882352941176471], [1.0], [0.6705882352941176], [0.9686274509803922], [0.9098039215686274], [0.9176470588235294], [0.9176470588235294], [0.9137254901960784], [0.9137254901960784], [0.9098039215686274], [0.9176470588235294], [0.9137254901960784], [0.9176470588235294], [0.9137254901960784], [0.9411764705882353], [0.8745098039215686], [0.5019607843137255], [1.0], [0.9490196078431372], [0.592156862745098], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.6980392156862745], [0.9529411764705882], [1.0], [0.2235294117647059], [0.9333333333333333], [0.9450980392156862], [0.9333333333333333], [0.9333333333333333], [0.9333333333333333], [0.9294117647058824], [0.9254901960784314], [0.9294117647058824], [0.9294117647058824], [0.9411764705882353], [0.9294117647058824], [0.996078431372549], [0.6901960784313725], [0.20392156862745098], [1.0], [0.9372549019607843], [0.615686274509804], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7372549019607844], [0.9411764705882353], [0.9803921568627451], [0.24313725490196078], [0.8549019607843137], [1.0], [0.8627450980392157], [0.8705882352941177], [0.8705882352941177], [0.8705882352941177], [0.8745098039215686], [0.8745098039215686], [0.8784313725490196], [0.8705882352941177], [0.8549019607843137], [1.0], [0.6039215686274509], [0.12549019607843137], [1.0], [0.9254901960784314], [0.7372549019607844], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.5098039215686274], [0.9607843137254902], [0.9490196078431372], [0.09411764705882353], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.13333333333333333], [0.9490196078431372], [0.9568627450980393], [0.5294117647058824], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.2980392156862745], [1.0], [0.9764705882352941], [0.08627450980392157], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.15294117647058825], [0.9764705882352941], [1.0], [0.4823529411764706], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.19215686274509805], [0.803921568627451], [0.7725490196078432], [0.043137254901960784], [0.0], [0.01568627450980392], [0.00392156862745098], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.011764705882352941], [0.0], [0.011764705882352941], [0.6823529411764706], [0.7411764705882353], [0.2627450980392157], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.2627450980392157], [0.6941176470588235], [0.5058823529411764], [0.6], [0.4588235294117647], [0.5058823529411764], [0.5725490196078431], [0.5529411764705883], [0.6862745098039216], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.7686274509803922], [1.0], [1.0], [1.0], [0.9450980392156862], [0.984313725490196], [1.0], [0.9607843137254902], [1.0], [0.2980392156862745], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9529411764705882], [0.9294117647058824], [0.8509803921568627], [0.8941176470588236], [0.9058823529411765], [0.8705882352941177], [0.8549019607843137], [0.8588235294117647], [1.0], [0.4549019607843137], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9215686274509803], [0.9058823529411765], [0.9137254901960784], [0.8862745098039215], [0.8823529411764706], [0.8980392156862745], [0.8705882352941177], [1.0], [0.5686274509803921], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.09019607843137255], [1.0], [0.9019607843137255], [0.8980392156862745], [0.9137254901960784], [0.8980392156862745], [0.8823529411764706], [0.8901960784313725], [0.8666666666666667], [0.9450980392156862], [0.6549019607843137], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.2627450980392157], [1.0], [0.8823529411764706], [0.9176470588235294], [0.9058823529411765], [0.8862745098039215], [0.8901960784313725], [0.8941176470588236], [0.8784313725490196], [0.9176470588235294], [0.7333333333333333], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4470588235294118], [0.9764705882352941], [0.8509803921568627], [0.9215686274509803], [0.9333333333333333], [0.9607843137254902], [0.8901960784313725], [0.8901960784313725], [0.8823529411764706], [0.9450980392156862], [0.6901960784313725], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.6549019607843137], [0.9686274509803922], [0.8901960784313725], [0.9058823529411765], [0.9803921568627451], [0.7843137254901961], [0.9725490196078431], [0.9058823529411765], [0.8784313725490196], [0.984313725490196], [0.5764705882352941], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8156862745098039], [0.9490196078431372], [0.8823529411764706], [0.9529411764705882], [0.8823529411764706], [0.0], [1.0], [0.9137254901960784], [0.8862745098039215], [1.0], [0.5058823529411764], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8745098039215686], [0.9333333333333333], [0.8745098039215686], [1.0], [0.6313725490196078], [0.0], [1.0], [0.9254901960784314], [0.8745098039215686], [1.0], [0.5294117647058824], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9607843137254902], [0.9215686274509803], [0.8705882352941177], [1.0], [0.2823529411764706], [0.0], [0.9725490196078431], [0.996078431372549], [0.8509803921568627], [1.0], [0.5686274509803921], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9137254901960784], [0.8862745098039215], [1.0], [0.027450980392156862], [0.0], [0.7490196078431373], [0.9725490196078431], [0.8627450980392157], [1.0], [0.49411764705882355], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9137254901960784], [0.9058823529411765], [0.984313725490196], [0.0], [0.0], [0.6235294117647059], [0.984313725490196], [0.8666666666666667], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9098039215686274], [0.9254901960784314], [0.8470588235294118], [0.0], [0.0], [0.5137254901960784], [0.9921568627450981], [0.8627450980392157], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.8941176470588236], [0.9529411764705882], [0.6745098039215687], [0.0], [0.0], [0.2235294117647059], [0.9764705882352941], [0.8705882352941177], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9019607843137255], [0.9568627450980393], [0.5450980392156862], [0.0], [0.0], [0.0392156862745098], [1.0], [0.8901960784313725], [1.0], [0.39215686274509803], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8901960784313725], [0.9294117647058824], [0.9490196078431372], [0.44313725490196076], [0.0], [0.0], [0.023529411764705882], [1.0], [0.9019607843137255], [1.0], [0.34901960784313724], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8], [0.9372549019607843], [0.9607843137254902], [0.592156862745098], [0.0], [0.0], [0.0], [1.0], [0.8901960784313725], [1.0], [0.38823529411764707], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.592156862745098], [0.9607843137254902], [0.9333333333333333], [0.7764705882352941], [0.0], [0.0], [0.0], [1.0], [0.9176470588235294], [1.0], [0.3607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.34901960784313724], [0.9725490196078431], [0.9137254901960784], [0.9725490196078431], [0.0], [0.0], [0.0], [0.9882352941176471], [0.9294117647058824], [1.0], [0.35294117647058826], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.12156862745098039], [0.9411764705882353], [0.8980392156862745], [0.8862745098039215], [0.0], [0.0], [0.0], [0.9372549019607843], [0.9333333333333333], [1.0], [0.3607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8862745098039215], [0.9137254901960784], [0.9294117647058824], [0.13333333333333333], [0.0], [0.0], [0.9176470588235294], [0.9333333333333333], [1.0], [0.37254901960784315], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9137254901960784], [0.9254901960784314], [0.9568627450980393], [0.26666666666666666], [0.0], [0.0], [0.8196078431372549], [0.9450980392156862], [0.9294117647058824], [0.3843137254901961], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.596078431372549], [0.9490196078431372], [0.9607843137254902], [0.5019607843137255], [0.0], [0.0], [0.7764705882352941], [0.9450980392156862], [0.9333333333333333], [0.3176470588235294], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.28627450980392155], [0.9647058823529412], [0.9450980392156862], [0.8274509803921568], [0.0], [0.0], [0.792156862745098], [0.9411764705882353], [0.9294117647058824], [0.2901960784313726], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.01568627450980392], [0.0], [0.0], [0.8980392156862745], [0.9254901960784314], [0.8196078431372549], [0.0], [0.0], [0.6196078431372549], [0.9686274509803922], [0.9333333333333333], [0.38823529411764707], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.7803921568627451], [1.0], [0.9686274509803922], [0.22745098039215686], [0.0], [0.6313725490196078], [1.0], [0.9882352941176471], [0.4666666666666667], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.3843137254901961], [0.6235294117647059], [0.2784313725490196], [0.0], [0.0], [0.26666666666666666], [0.6901960784313725], [0.6431372549019608], [0.22745098039215686], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]]} headers = { \"content-type\" : \"application/json\" } r = requests . post ( 'http://localhost:8501/v1/models/fashion_model:predict' , data = data , headers = headers ) j = r . json () print ( j . keys ()) print ( j ) dict_keys(['predictions']) {'predictions': [[8.25938809e-16, 2.94385077e-17, 9.78797075e-16, 2.2219498e-16, 2.59219783e-13, 1.91701793e-10, 1.40012654e-16, 6.76713e-11, 7.24115397e-18, 1.0], [1.04329297e-06, 1.63656903e-12, 0.999997139, 7.85040505e-11, 1.13352101e-08, 2.28772096e-14, 1.75158141e-06, 4.41351522e-19, 1.66081106e-15, 2.36975185e-17], [2.25010535e-13, 1.0, 4.64146882e-14, 5.00870963e-17, 6.15678637e-14, 4.10723891e-21, 2.84594985e-18, 1.07107688e-30, 1.73393987e-22, 6.02925506e-22]]} # It looks like a 2-D array, let's check its shape pred = np . array ( j [ 'predictions' ]) print ( pred . shape ) # This is the N x K output array from the model # pred[n,k] is the probability that we believe the nth sample belongs to the kth class (3, 10) # Get the predicted classes pred = pred . argmax ( axis = 1 ) # Map them back to strings pred = [ labels [ i ] for i in pred ] print ( pred ) ['Ankle boot', 'Pullover', 'Trouser'] # Get the true labels actual = [ labels [ i ] for i in y_test [: 3 ]] print ( actual ) ['Ankle boot', 'Pullover', 'Trouser'] for i in range ( 0 , 3 ): show ( i , f \"True: { actual [ i ] } , Predicted: { pred [ i ] } \" ) # Allows you to select a model by version headers = { \"content-type\" : \"application/json\" } r = requests . post ( 'http://localhost:8501/v1/models/fashion_model/versions/1:predict' , data = data , headers = headers ) j = r . json () pred = np . array ( j [ 'predictions' ]) pred = pred . argmax ( axis = 1 ) pred = [ labels [ i ] for i in pred ] for i in range ( 0 , 3 ): show ( i , f \"True: { actual [ i ] } , Predicted: { pred [ i ] } \" ) # Let's make a new model version # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Flatten ()( x ) x = Dense ( K , activation = 'softmax' )( x ) model2 = Model ( i , x ) model2 . summary () Model: \"model_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 13, 13, 32) 320 _________________________________________________________________ flatten_1 (Flatten) (None, 5408) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 54090 ================================================================= Total params: 54,410 Trainable params: 54,410 Non-trainable params: 0 _________________________________________________________________ # Compile and fit # Note: make sure you are using the GPU for this! model2 . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) Train on 60000 samples, validate on 10000 samples Epoch 1/15 60000/60000 [==============================] - 5s 86us/sample - loss: 0.4692 - accuracy: 0.8346 - val_loss: 0.3850 - val_accuracy: 0.8629 Epoch 2/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.3368 - accuracy: 0.8807 - val_loss: 0.3679 - val_accuracy: 0.8678 Epoch 3/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.3004 - accuracy: 0.8919 - val_loss: 0.3298 - val_accuracy: 0.8799 Epoch 4/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2762 - accuracy: 0.9006 - val_loss: 0.3151 - val_accuracy: 0.8866 Epoch 5/15 60000/60000 [==============================] - 5s 82us/sample - loss: 0.2587 - accuracy: 0.9067 - val_loss: 0.3167 - val_accuracy: 0.8877 Epoch 6/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2443 - accuracy: 0.9115 - val_loss: 0.3120 - val_accuracy: 0.8881 Epoch 7/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2325 - accuracy: 0.9155 - val_loss: 0.3009 - val_accuracy: 0.8935 Epoch 8/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2212 - accuracy: 0.9199 - val_loss: 0.3009 - val_accuracy: 0.8941 Epoch 9/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2115 - accuracy: 0.9243 - val_loss: 0.2936 - val_accuracy: 0.8963 Epoch 10/15 60000/60000 [==============================] - 5s 79us/sample - loss: 0.2015 - accuracy: 0.9280 - val_loss: 0.2981 - val_accuracy: 0.8966 Epoch 11/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.1926 - accuracy: 0.9312 - val_loss: 0.2968 - val_accuracy: 0.8987 Epoch 12/15 60000/60000 [==============================] - 5s 81us/sample - loss: 0.1855 - accuracy: 0.9340 - val_loss: 0.3009 - val_accuracy: 0.8970 Epoch 13/15 60000/60000 [==============================] - 5s 81us/sample - loss: 0.1785 - accuracy: 0.9361 - val_loss: 0.3001 - val_accuracy: 0.8985 Epoch 14/15 60000/60000 [==============================] - 5s 83us/sample - loss: 0.1718 - accuracy: 0.9385 - val_loss: 0.3085 - val_accuracy: 0.8981 Epoch 15/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.1654 - accuracy: 0.9404 - val_loss: 0.3049 - val_accuracy: 0.9005 # Save version 2 of the model version = 2 export_path = os . path . join ( MODEL_DIR , str ( version )) print ( 'export_path = {} \\n ' . format ( export_path )) if os . path . isdir ( export_path ): print ( ' \\n Already saved a model, cleaning up \\n ' ) ! rm - r { export_path } tf . saved_model . save ( model2 , export_path ) print ( ' \\n Saved model:' ) ! ls - l { export_path } export_path = /tmp/2 Saved model: total 76 drwxr-xr-x 2 root root 4096 Aug 10 04:34 assets -rw-r--r-- 1 root root 66590 Aug 10 04:34 saved_model.pb drwxr-xr-x 2 root root 4096 Aug 10 04:34 variables # Will Tensorflow serving know about the new model without restarting? headers = { \"content-type\" : \"application/json\" } r = requests . post ( 'http://localhost:8501/v1/models/fashion_model/versions/2:predict' , data = data , headers = headers ) j = r . json () pred = np . array ( j [ 'predictions' ]) pred = pred . argmax ( axis = 1 ) pred = [ labels [ i ] for i in pred ] for i in range ( 0 , 3 ): show ( i , f \"True: { actual [ i ] } , Predicted: { pred [ i ] } \" ) # What if we use a version number that does not exist? headers = { \"content-type\" : \"application/json\" } r = requests . post ( 'http://localhost:8501/v1/models/fashion_model/versions/3:predict' , data = data , headers = headers ) j = r . json () print ( j ) {'error': 'Servable not found for request: Specific(fashion_model, 3)'} Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Serving"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Serving/#serving","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # By the way, what is a server / service / API? # Best way to learn is by example # Here is a service that simply returns your IP address in a JSON import requests r = requests . get ( 'https://api.ipify.org?format=json' ) j = r . json () print ( j ) # Our Tensorflow model server is the same, except what it does is much more # complex - it returns the predictions from a ML model! {'ip': '35.224.223.54'} # More imports import numpy as np import matplotlib.pyplot as plt import os import subprocess from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout from tensorflow.keras.models import Model # Load in the data fashion_mnist = tf . keras . datasets . fashion_mnist ( x_train , y_train ), ( x_test , y_test ) = fashion_mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) print ( \"x_test.shape:\" , x_test . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) x_test.shape: (10000, 28, 28) # the data is only 2D! # convolution expects height x width x color x_train = np . expand_dims ( x_train , - 1 ) x_test = np . expand_dims ( x_test , - 1 ) print ( x_train . shape ) (60000, 28, 28, 1) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Conv2D ( 64 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Conv2D ( 128 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 512 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) model . summary () Model: \"model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ conv2d (Conv2D) (None, 13, 13, 32) 320 _________________________________________________________________ conv2d_1 (Conv2D) (None, 6, 6, 64) 18496 _________________________________________________________________ conv2d_2 (Conv2D) (None, 2, 2, 128) 73856 _________________________________________________________________ flatten (Flatten) (None, 512) 0 _________________________________________________________________ dropout (Dropout) (None, 512) 0 _________________________________________________________________ dense (Dense) (None, 512) 262656 _________________________________________________________________ dropout_1 (Dropout) (None, 512) 0 _________________________________________________________________ dense_1 (Dense) (None, 10) 5130 ================================================================= Total params: 360,458 Trainable params: 360,458 Non-trainable params: 0 _________________________________________________________________ # Compile and fit # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) WARNING: Logging before flag parsing goes to stderr. W0810 04:12:02.177557 140535052928896 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 60000 samples, validate on 10000 samples Epoch 1/15 60000/60000 [==============================] - 13s 211us/sample - loss: 0.5198 - accuracy: 0.8071 - val_loss: 0.3919 - val_accuracy: 0.8524 Epoch 2/15 60000/60000 [==============================] - 9s 153us/sample - loss: 0.3541 - accuracy: 0.8675 - val_loss: 0.3549 - val_accuracy: 0.8657 Epoch 3/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.3042 - accuracy: 0.8855 - val_loss: 0.3162 - val_accuracy: 0.8874 Epoch 4/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.2730 - accuracy: 0.8980 - val_loss: 0.2973 - val_accuracy: 0.8944 Epoch 5/15 60000/60000 [==============================] - 9s 153us/sample - loss: 0.2498 - accuracy: 0.9059 - val_loss: 0.3015 - val_accuracy: 0.8904 Epoch 6/15 60000/60000 [==============================] - 9s 153us/sample - loss: 0.2255 - accuracy: 0.9146 - val_loss: 0.2890 - val_accuracy: 0.8939 Epoch 7/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.2101 - accuracy: 0.9189 - val_loss: 0.3112 - val_accuracy: 0.8912 Epoch 8/15 60000/60000 [==============================] - 9s 155us/sample - loss: 0.1933 - accuracy: 0.9271 - val_loss: 0.2828 - val_accuracy: 0.9031 Epoch 9/15 60000/60000 [==============================] - 9s 154us/sample - loss: 0.1812 - accuracy: 0.9319 - val_loss: 0.3133 - val_accuracy: 0.8966 Epoch 10/15 60000/60000 [==============================] - 9s 155us/sample - loss: 0.1653 - accuracy: 0.9366 - val_loss: 0.3063 - val_accuracy: 0.9015 Epoch 11/15 60000/60000 [==============================] - 10s 158us/sample - loss: 0.1526 - accuracy: 0.9410 - val_loss: 0.3212 - val_accuracy: 0.9046 Epoch 12/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.1460 - accuracy: 0.9452 - val_loss: 0.3190 - val_accuracy: 0.9028 Epoch 13/15 60000/60000 [==============================] - 9s 152us/sample - loss: 0.1385 - accuracy: 0.9467 - val_loss: 0.3406 - val_accuracy: 0.9021 Epoch 14/15 60000/60000 [==============================] - 9s 153us/sample - loss: 0.1268 - accuracy: 0.9515 - val_loss: 0.3731 - val_accuracy: 0.9014 Epoch 15/15 60000/60000 [==============================] - 9s 154us/sample - loss: 0.1242 - accuracy: 0.9527 - val_loss: 0.3547 - val_accuracy: 0.9034 # Save the model to a temporary directory import tempfile MODEL_DIR = tempfile . gettempdir () version = 1 export_path = os . path . join ( MODEL_DIR , str ( version )) print ( 'export_path = {} \\n ' . format ( export_path )) if os . path . isdir ( export_path ): print ( ' \\n Already saved a model, cleaning up \\n ' ) ! rm - r { export_path } tf . saved_model . save ( model , export_path ) print ( ' \\n Saved model:' ) ! ls - l { export_path } export_path = /tmp/1 Saved model: total 144 drwxr-xr-x 2 root root 4096 Aug 10 04:14 assets -rw-r--r-- 1 root root 136883 Aug 10 04:14 saved_model.pb drwxr-xr-x 2 root root 4096 Aug 10 04:14 variables ! saved_model_cli show -- dir { export_path } -- all MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs: signature_def['__saved_model_init_op']: The given SavedModel SignatureDef contains the following input(s): The given SavedModel SignatureDef contains the following output(s): outputs['__saved_model_init_op'] tensor_info: dtype: DT_INVALID shape: unknown_rank name: NoOp Method name is: signature_def['serving_default']: The given SavedModel SignatureDef contains the following input(s): inputs['input_1'] tensor_info: dtype: DT_FLOAT shape: (-1, 28, 28, 1) name: serving_default_input_1:0 The given SavedModel SignatureDef contains the following output(s): outputs['dense_1'] tensor_info: dtype: DT_FLOAT shape: (-1, 10) name: StatefulPartitionedCall:0 Method name is: tensorflow/serving/predict # This is the same as you would do from your command line, but without the [arch=amd64], and no sudo # You would instead do: # echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\ # curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add - ! echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee / etc / apt / sources . list . d / tensorflow - serving . list && \\ curl https : // storage . googleapis . com / tensorflow - serving - apt / tensorflow - serving . release . pub . gpg | apt - key add - ! apt update deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2943 100 2943 0 0 10819 0 --:--:-- --:--:-- --:--:-- 10819 OK Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,012 B] Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B] Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB] Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 InRelease Get:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB] Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 Release Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Release [564 B] Get:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Release.gpg [833 B] Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease Get:11 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [365 B] Get:12 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [357 B] Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [65.9 kB] Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB] Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [731 kB] Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [597 kB] Get:20 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Packages [12.3 kB] Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [29.0 kB] Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [906 kB] Get:23 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,677 kB] Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [14.2 kB] Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,257 kB] Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [10.8 kB] Get:27 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [805 kB] Fetched 6,402 kB in 3s (1,939 kB/s) Reading package lists... Done Building dependency tree Reading state information... Done 47 packages can be upgraded. Run 'apt list --upgradable' to see them. ! apt - get install tensorflow - model - server Reading package lists... Done Building dependency tree Reading state information... Done The following package was automatically installed and is no longer required: libnvidia-common-410 Use 'apt autoremove' to remove it. The following NEW packages will be installed: tensorflow-model-server 0 upgraded, 1 newly installed, 0 to remove and 47 not upgraded. Need to get 151 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 1.14.0 [151 MB] Fetched 151 MB in 2s (81.4 MB/s) Selecting previously unselected package tensorflow-model-server. (Reading database ... 131289 files and directories currently installed.) Preparing to unpack .../tensorflow-model-server_1.14.0_all.deb ... Unpacking tensorflow-model-server (1.14.0) ... Setting up tensorflow-model-server (1.14.0) ... os . environ [ \"MODEL_DIR\" ] = MODEL_DIR %% bash -- bg nohup tensorflow_model_server \\ -- rest_api_port = 8501 \\ -- model_name = fashion_model \\ -- model_base_path = \"$ {MODEL_DIR} \" > server . log 2 >& 1 Starting job # 0 in a separate thread. ! tail server . log 2019-08-10 04:14:43.484998: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve } 2019-08-10 04:14:43.486899: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2019-08-10 04:14:43.502478: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle. 2019-08-10 04:14:43.544172: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /tmp/1 2019-08-10 04:14:43.552722: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 69454 microseconds. 2019-08-10 04:14:43.552775: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:103] No warmup data file found at /tmp/1/assets.extra/tf_serving_warmup_requests 2019-08-10 04:14:43.552887: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: fashion_model version: 1} 2019-08-10 04:14:43.554155: I tensorflow_serving/model_servers/server.cc:324] Running gRPC ModelServer at 0.0.0.0:8500 ... [evhttp_server.cc : 239] RAW: Entering the event loop ... 2019-08-10 04:14:43.554756: I tensorflow_serving/model_servers/server.cc:344] Exporting HTTP/REST API at:localhost:8501 ... # Label mapping labels = '''T-shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot''' . split ( \" \\n \" ) def show ( idx , title ): plt . figure () plt . imshow ( x_test [ idx ] . reshape ( 28 , 28 ), cmap = 'gray' ) plt . axis ( 'off' ) plt . title ( ' \\n\\n {} ' . format ( title ), fontdict = { 'size' : 16 }) i = np . random . randint ( 0 , len ( x_test )) show ( i , labels [ y_test [ i ]]) # Format some data to pass to the server # { # \"signature_name\": \"serving_default\", # \"instances\": [ an N x H x W x C list ], # } import json data = json . dumps ({ \"signature_name\" : \"serving_default\" , \"instances\" : x_test [ 0 : 3 ] . tolist ()}) print ( data ) {\"signature_name\": \"serving_default\", \"instances\": [[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.00392156862745098], [0.0], [0.0], [0.027450980392156862], [0.0], [0.1450980392156863], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.00784313725490196], [0.0], [0.10588235294117647], [0.32941176470588235], [0.043137254901960784], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4666666666666667], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.34509803921568627], [0.5607843137254902], [0.43137254901960786], [0.0], [0.0], [0.0], [0.0], [0.08627450980392157], [0.36470588235294116], [0.41568627450980394], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.01568627450980392], [0.0], [0.20784313725490197], [0.5058823529411764], [0.47058823529411764], [0.5764705882352941], [0.6862745098039216], [0.615686274509804], [0.6509803921568628], [0.5294117647058824], [0.6039215686274509], [0.6588235294117647], [0.5490196078431373], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.043137254901960784], [0.5372549019607843], [0.5098039215686274], [0.5019607843137255], [0.6274509803921569], [0.6901960784313725], [0.6235294117647059], [0.6549019607843137], [0.6980392156862745], [0.5843137254901961], [0.592156862745098], [0.5647058823529412], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.00784313725490196], [0.00392156862745098], [0.0], [0.011764705882352941], [0.0], [0.0], [0.45098039215686275], [0.4470588235294118], [0.41568627450980394], [0.5372549019607843], [0.6588235294117647], [0.6], [0.611764705882353], [0.6470588235294118], [0.6549019607843137], [0.5607843137254902], [0.615686274509804], [0.6196078431372549], [0.043137254901960784], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.0], [0.0], [0.34901960784313724], [0.5450980392156862], [0.35294117647058826], [0.3686274509803922], [0.6], [0.5843137254901961], [0.5137254901960784], [0.592156862745098], [0.6627450980392157], [0.6745098039215687], [0.5607843137254902], [0.6235294117647059], [0.6627450980392157], [0.18823529411764706], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.01568627450980392], [0.00392156862745098], [0.0], [0.0], [0.0], [0.3843137254901961], [0.5333333333333333], [0.43137254901960786], [0.42745098039215684], [0.43137254901960786], [0.6352941176470588], [0.5294117647058824], [0.5647058823529412], [0.5843137254901961], [0.6235294117647059], [0.6549019607843137], [0.5647058823529412], [0.6196078431372549], [0.6627450980392157], [0.4666666666666667], [0.0]], [[0.0], [0.0], [0.00784313725490196], [0.00784313725490196], [0.00392156862745098], [0.00784313725490196], [0.0], [0.0], [0.0], [0.0], [0.10196078431372549], [0.4235294117647059], [0.4588235294117647], [0.38823529411764707], [0.43529411764705883], [0.4588235294117647], [0.5333333333333333], [0.611764705882353], [0.5254901960784314], [0.6039215686274509], [0.6039215686274509], [0.611764705882353], [0.6274509803921569], [0.5529411764705883], [0.5764705882352941], [0.611764705882353], [0.6980392156862745], [0.0]], [[0.011764705882352941], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.08235294117647059], [0.20784313725490197], [0.3607843137254902], [0.4588235294117647], [0.43529411764705883], [0.403921568627451], [0.45098039215686275], [0.5058823529411764], [0.5254901960784314], [0.5607843137254902], [0.6039215686274509], [0.6470588235294118], [0.6666666666666666], [0.6039215686274509], [0.592156862745098], [0.6039215686274509], [0.5607843137254902], [0.5411764705882353], [0.5882352941176471], [0.6470588235294118], [0.16862745098039217]], [[0.0], [0.0], [0.09019607843137255], [0.21176470588235294], [0.2549019607843137], [0.2980392156862745], [0.3333333333333333], [0.4627450980392157], [0.5019607843137255], [0.4823529411764706], [0.43529411764705883], [0.44313725490196076], [0.4627450980392157], [0.4980392156862745], [0.49019607843137253], [0.5450980392156862], [0.5215686274509804], [0.5333333333333333], [0.6274509803921569], [0.5490196078431373], [0.6078431372549019], [0.6313725490196078], [0.5647058823529412], [0.6078431372549019], [0.6745098039215687], [0.6313725490196078], [0.7411764705882353], [0.24313725490196078]], [[0.0], [0.26666666666666666], [0.3686274509803922], [0.35294117647058826], [0.43529411764705883], [0.4470588235294118], [0.43529411764705883], [0.4470588235294118], [0.45098039215686275], [0.4980392156862745], [0.5294117647058824], [0.5333333333333333], [0.5607843137254902], [0.49411764705882355], [0.4980392156862745], [0.592156862745098], [0.6039215686274509], [0.5607843137254902], [0.5803921568627451], [0.49019607843137253], [0.6352941176470588], [0.6352941176470588], [0.5647058823529412], [0.5411764705882353], [0.6], [0.6352941176470588], [0.7686274509803922], [0.22745098039215686]], [[0.27450980392156865], [0.6627450980392157], [0.5058823529411764], [0.40784313725490196], [0.3843137254901961], [0.39215686274509803], [0.3686274509803922], [0.3803921568627451], [0.3843137254901961], [0.4], [0.4235294117647059], [0.41568627450980394], [0.4666666666666667], [0.47058823529411764], [0.5058823529411764], [0.5843137254901961], [0.611764705882353], [0.6549019607843137], [0.7450980392156863], [0.7450980392156863], [0.7686274509803922], [0.7764705882352941], [0.7764705882352941], [0.7333333333333333], [0.7725490196078432], [0.7411764705882353], [0.7215686274509804], [0.1411764705882353]], [[0.06274509803921569], [0.49411764705882355], [0.6705882352941176], [0.7372549019607844], [0.7372549019607844], [0.7215686274509804], [0.6705882352941176], [0.6], [0.5294117647058824], [0.47058823529411764], [0.49411764705882355], [0.4980392156862745], [0.5725490196078431], [0.7254901960784313], [0.7647058823529411], [0.8196078431372549], [0.8156862745098039], [1.0], [0.8196078431372549], [0.6941176470588235], [0.9607843137254902], [0.9882352941176471], [0.984313725490196], [0.984313725490196], [0.9686274509803922], [0.8627450980392157], [0.807843137254902], [0.19215686274509805]], [[0.0], [0.0], [0.0], [0.047058823529411764], [0.2627450980392157], [0.41568627450980394], [0.6431372549019608], [0.7254901960784313], [0.7803921568627451], [0.8235294117647058], [0.8274509803921568], [0.8235294117647058], [0.8156862745098039], [0.7450980392156863], [0.5882352941176471], [0.3215686274509804], [0.03137254901960784], [0.0], [0.0], [0.0], [0.6980392156862745], [0.8156862745098039], [0.7372549019607844], [0.6862745098039216], [0.6352941176470588], [0.6196078431372549], [0.592156862745098], [0.043137254901960784]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.050980392156862744], [0.2627450980392157], [0.0], [0.0], [0.0], [0.0], [0.19607843137254902], [0.14901960784313725], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.03137254901960784], [0.47058823529411764], [0.8196078431372549], [0.8862745098039215], [0.9686274509803922], [0.9294117647058824], [1.0], [1.0], [1.0], [0.9686274509803922], [0.9333333333333333], [0.9215686274509803], [0.6745098039215687], [0.2823529411764706], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.5372549019607843], [0.9372549019607843], [0.9882352941176471], [0.9529411764705882], [0.9176470588235294], [0.8980392156862745], [0.9333333333333333], [0.9568627450980393], [0.9647058823529412], [0.9411764705882353], [0.9019607843137255], [0.9098039215686274], [0.9372549019607843], [0.9725490196078431], [0.984313725490196], [0.7607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.4], [1.0], [0.9058823529411765], [0.8941176470588236], [0.8901960784313725], [0.8941176470588236], [0.9137254901960784], [0.9019607843137255], [0.9019607843137255], [0.8980392156862745], [0.8941176470588236], [0.9098039215686274], [0.9098039215686274], [0.9058823529411765], [0.8901960784313725], [0.8784313725490196], [0.9882352941176471], [0.7019607843137254], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.9137254901960784], [0.9450980392156862], [0.8980392156862745], [0.9058823529411765], [1.0], [1.0], [0.9333333333333333], [0.9058823529411765], [0.8901960784313725], [0.9333333333333333], [0.9647058823529412], [0.8941176470588236], [0.9019607843137255], [0.8901960784313725], [0.9176470588235294], [0.9215686274509803], [0.8980392156862745], [0.9450980392156862], [0.0784313725490196], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.9725490196078431], [0.9450980392156862], [0.9058823529411765], [1.0], [0.5843137254901961], [0.1843137254901961], [0.9882352941176471], [0.8941176470588236], [1.0], [0.9490196078431372], [0.8470588235294118], [0.9333333333333333], [0.9098039215686274], [1.0], [0.8941176470588236], [0.8627450980392157], [0.9176470588235294], [0.9803921568627451], [0.21176470588235294], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9411764705882353], [0.9098039215686274], [1.0], [0.058823529411764705], [0.0], [1.0], [0.9294117647058824], [0.7490196078431373], [0.0], [0.0], [0.8392156862745098], [1.0], [0.050980392156862744], [0.4823529411764706], [1.0], [0.9176470588235294], [0.9882352941176471], [0.4470588235294118], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.023529411764705882], [1.0], [0.9333333333333333], [0.9372549019607843], [1.0], [0.6941176470588235], [0.0], [1.0], [1.0], [0.0], [0.5098039215686274], [0.4549019607843137], [0.1843137254901961], [0.2549019607843137], [0.16862745098039217], [0.1450980392156863], [1.0], [0.9254901960784314], [0.9764705882352941], [0.6352941176470588], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.12549019607843137], [1.0], [0.9254901960784314], [0.9607843137254902], [1.0], [0.8], [0.0], [1.0], [0.32941176470588235], [0.0], [0.1450980392156863], [0.10980392156862745], [0.12156862745098039], [0.0], [0.09803921568627451], [0.050980392156862744], [1.0], [0.9254901960784314], [0.9764705882352941], [0.7803921568627451], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.20784313725490197], [1.0], [0.9254901960784314], [0.9803921568627451], [0.9803921568627451], [0.9058823529411765], [0.00784313725490196], [1.0], [0.08235294117647059], [0.0], [0.8666666666666667], [1.0], [0.9254901960784314], [0.21176470588235294], [0.9607843137254902], [0.7764705882352941], [0.9529411764705882], [0.9333333333333333], [0.9607843137254902], [0.8745098039215686], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.3137254901960784], [1.0], [0.9294117647058824], [0.9803921568627451], [0.9411764705882353], [1.0], [0.0], [0.0], [0.15294117647058825], [0.615686274509804], [0.0], [0.0], [0.8431372549019608], [0.3686274509803922], [0.0784313725490196], [0.49411764705882355], [1.0], [0.9294117647058824], [0.9372549019607843], [0.9803921568627451], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.396078431372549], [1.0], [0.9215686274509803], [0.9921568627450981], [0.9568627450980393], [0.9529411764705882], [0.5215686274509804], [0.5411764705882353], [0.8156862745098039], [1.0], [0.788235294117647], [0.8392156862745098], [1.0], [0.9019607843137255], [0.027450980392156862], [0.6823529411764706], [1.0], [0.9411764705882353], [0.9333333333333333], [1.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.49411764705882355], [1.0], [0.9137254901960784], [1.0], [0.9725490196078431], [0.9137254901960784], [1.0], [1.0], [0.9411764705882353], [0.9098039215686274], [0.9529411764705882], [0.9529411764705882], [0.9058823529411765], [0.984313725490196], [1.0], [1.0], [0.996078431372549], [0.9529411764705882], [0.9333333333333333], [1.0], [0.011764705882352941], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.5764705882352941], [1.0], [0.9137254901960784], [0.9764705882352941], [0.7098039215686275], [0.9529411764705882], [0.8901960784313725], [0.8784313725490196], [0.9019607843137255], [0.9176470588235294], [0.9019607843137255], [0.9019607843137255], [0.9215686274509803], [0.8941176470588236], [0.9215686274509803], [0.8705882352941177], [0.8117647058823529], [1.0], [0.9254901960784314], [1.0], [0.13725490196078433], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.6392156862745098], [1.0], [0.9607843137254902], [0.8666666666666667], [0.33725490196078434], [1.0], [0.9137254901960784], [0.9137254901960784], [0.9215686274509803], [0.9254901960784314], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9098039215686274], [0.9490196078431372], [0.9058823529411765], [0.49019607843137253], [1.0], [0.9254901960784314], [1.0], [0.21568627450980393], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7098039215686275], [0.996078431372549], [1.0], [0.7843137254901961], [0.27058823529411763], [1.0], [0.8941176470588236], [0.9098039215686274], [0.9176470588235294], [0.9215686274509803], [0.9176470588235294], [0.9176470588235294], [0.9137254901960784], [0.9215686274509803], [0.9450980392156862], [0.9294117647058824], [0.27450980392156865], [1.0], [0.9215686274509803], [0.9647058823529412], [0.2235294117647059], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7725490196078432], [0.9686274509803922], [1.0], [0.7372549019607844], [0.43137254901960786], [1.0], [0.8784313725490196], [0.9137254901960784], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9176470588235294], [0.9411764705882353], [0.9921568627450981], [0.27058823529411763], [1.0], [0.9254901960784314], [0.9725490196078431], [0.30196078431372547], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7843137254901961], [0.9647058823529412], [1.0], [0.5843137254901961], [0.5686274509803921], [1.0], [0.8745098039215686], [0.9215686274509803], [0.9176470588235294], [0.9215686274509803], [0.9215686274509803], [0.9215686274509803], [0.9176470588235294], [0.9294117647058824], [0.9137254901960784], [1.0], [0.1843137254901961], [1.0], [0.9372549019607843], [0.9764705882352941], [0.3843137254901961], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.8], [0.9529411764705882], [1.0], [0.43529411764705883], [0.6784313725490196], [1.0], [0.8901960784313725], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9215686274509803], [0.9215686274509803], [0.9372549019607843], [0.8980392156862745], [1.0], [0.07450980392156863], [0.8901960784313725], [0.9647058823529412], [0.9764705882352941], [0.43137254901960786], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7686274509803922], [0.9411764705882353], [1.0], [0.42745098039215684], [0.8352941176470589], [0.9803921568627451], [0.8980392156862745], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9294117647058824], [0.9254901960784314], [0.9294117647058824], [0.8862745098039215], [1.0], [0.21568627450980393], [0.796078431372549], [0.984313725490196], [0.9607843137254902], [0.47058823529411764], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7529411764705882], [0.9529411764705882], [1.0], [0.4470588235294118], [0.9098039215686274], [0.9411764705882353], [0.9098039215686274], [0.9215686274509803], [0.9215686274509803], [0.9254901960784314], [0.9176470588235294], [0.9294117647058824], [0.9254901960784314], [0.9215686274509803], [0.8980392156862745], [1.0], [0.5254901960784314], [0.6705882352941176], [0.9882352941176471], [0.9568627450980393], [0.5372549019607843], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7411764705882353], [0.984313725490196], [1.0], [0.6039215686274509], [0.9333333333333333], [0.9137254901960784], [0.9254901960784314], [0.9176470588235294], [0.9215686274509803], [0.9254901960784314], [0.9215686274509803], [0.9333333333333333], [0.9254901960784314], [0.9215686274509803], [0.9098039215686274], [1.0], [0.6509803921568628], [0.49019607843137253], [1.0], [0.9529411764705882], [0.5568627450980392], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7176470588235294], [0.9882352941176471], [1.0], [0.6705882352941176], [0.9686274509803922], [0.9098039215686274], [0.9176470588235294], [0.9176470588235294], [0.9137254901960784], [0.9137254901960784], [0.9098039215686274], [0.9176470588235294], [0.9137254901960784], [0.9176470588235294], [0.9137254901960784], [0.9411764705882353], [0.8745098039215686], [0.5019607843137255], [1.0], [0.9490196078431372], [0.592156862745098], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.6980392156862745], [0.9529411764705882], [1.0], [0.2235294117647059], [0.9333333333333333], [0.9450980392156862], [0.9333333333333333], [0.9333333333333333], [0.9333333333333333], [0.9294117647058824], [0.9254901960784314], [0.9294117647058824], [0.9294117647058824], [0.9411764705882353], [0.9294117647058824], [0.996078431372549], [0.6901960784313725], [0.20392156862745098], [1.0], [0.9372549019607843], [0.615686274509804], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.7372549019607844], [0.9411764705882353], [0.9803921568627451], [0.24313725490196078], [0.8549019607843137], [1.0], [0.8627450980392157], [0.8705882352941177], [0.8705882352941177], [0.8705882352941177], [0.8745098039215686], [0.8745098039215686], [0.8784313725490196], [0.8705882352941177], [0.8549019607843137], [1.0], [0.6039215686274509], [0.12549019607843137], [1.0], [0.9254901960784314], [0.7372549019607844], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.5098039215686274], [0.9607843137254902], [0.9490196078431372], [0.09411764705882353], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.13333333333333333], [0.9490196078431372], [0.9568627450980393], [0.5294117647058824], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.2980392156862745], [1.0], [0.9764705882352941], [0.08627450980392157], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.15294117647058825], [0.9764705882352941], [1.0], [0.4823529411764706], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.19215686274509805], [0.803921568627451], [0.7725490196078432], [0.043137254901960784], [0.0], [0.01568627450980392], [0.00392156862745098], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.00784313725490196], [0.011764705882352941], [0.0], [0.011764705882352941], [0.6823529411764706], [0.7411764705882353], [0.2627450980392157], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.2627450980392157], [0.6941176470588235], [0.5058823529411764], [0.6], [0.4588235294117647], [0.5058823529411764], [0.5725490196078431], [0.5529411764705883], [0.6862745098039216], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.7686274509803922], [1.0], [1.0], [1.0], [0.9450980392156862], [0.984313725490196], [1.0], [0.9607843137254902], [1.0], [0.2980392156862745], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9529411764705882], [0.9294117647058824], [0.8509803921568627], [0.8941176470588236], [0.9058823529411765], [0.8705882352941177], [0.8549019607843137], [0.8588235294117647], [1.0], [0.4549019607843137], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9215686274509803], [0.9058823529411765], [0.9137254901960784], [0.8862745098039215], [0.8823529411764706], [0.8980392156862745], [0.8705882352941177], [1.0], [0.5686274509803921], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.09019607843137255], [1.0], [0.9019607843137255], [0.8980392156862745], [0.9137254901960784], [0.8980392156862745], [0.8823529411764706], [0.8901960784313725], [0.8666666666666667], [0.9450980392156862], [0.6549019607843137], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.2627450980392157], [1.0], [0.8823529411764706], [0.9176470588235294], [0.9058823529411765], [0.8862745098039215], [0.8901960784313725], [0.8941176470588236], [0.8784313725490196], [0.9176470588235294], [0.7333333333333333], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4470588235294118], [0.9764705882352941], [0.8509803921568627], [0.9215686274509803], [0.9333333333333333], [0.9607843137254902], [0.8901960784313725], [0.8901960784313725], [0.8823529411764706], [0.9450980392156862], [0.6901960784313725], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.6549019607843137], [0.9686274509803922], [0.8901960784313725], [0.9058823529411765], [0.9803921568627451], [0.7843137254901961], [0.9725490196078431], [0.9058823529411765], [0.8784313725490196], [0.984313725490196], [0.5764705882352941], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8156862745098039], [0.9490196078431372], [0.8823529411764706], [0.9529411764705882], [0.8823529411764706], [0.0], [1.0], [0.9137254901960784], [0.8862745098039215], [1.0], [0.5058823529411764], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8745098039215686], [0.9333333333333333], [0.8745098039215686], [1.0], [0.6313725490196078], [0.0], [1.0], [0.9254901960784314], [0.8745098039215686], [1.0], [0.5294117647058824], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9607843137254902], [0.9215686274509803], [0.8705882352941177], [1.0], [0.2823529411764706], [0.0], [0.9725490196078431], [0.996078431372549], [0.8509803921568627], [1.0], [0.5686274509803921], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9137254901960784], [0.8862745098039215], [1.0], [0.027450980392156862], [0.0], [0.7490196078431373], [0.9725490196078431], [0.8627450980392157], [1.0], [0.49411764705882355], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9137254901960784], [0.9058823529411765], [0.984313725490196], [0.0], [0.0], [0.6235294117647059], [0.984313725490196], [0.8666666666666667], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9098039215686274], [0.9254901960784314], [0.8470588235294118], [0.0], [0.0], [0.5137254901960784], [0.9921568627450981], [0.8627450980392157], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.8941176470588236], [0.9529411764705882], [0.6745098039215687], [0.0], [0.0], [0.2235294117647059], [0.9764705882352941], [0.8705882352941177], [1.0], [0.43529411764705883], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.9019607843137255], [0.9568627450980393], [0.5450980392156862], [0.0], [0.0], [0.0392156862745098], [1.0], [0.8901960784313725], [1.0], [0.39215686274509803], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8901960784313725], [0.9294117647058824], [0.9490196078431372], [0.44313725490196076], [0.0], [0.0], [0.023529411764705882], [1.0], [0.9019607843137255], [1.0], [0.34901960784313724], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8], [0.9372549019607843], [0.9607843137254902], [0.592156862745098], [0.0], [0.0], [0.0], [1.0], [0.8901960784313725], [1.0], [0.38823529411764707], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.592156862745098], [0.9607843137254902], [0.9333333333333333], [0.7764705882352941], [0.0], [0.0], [0.0], [1.0], [0.9176470588235294], [1.0], [0.3607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.34901960784313724], [0.9725490196078431], [0.9137254901960784], [0.9725490196078431], [0.0], [0.0], [0.0], [0.9882352941176471], [0.9294117647058824], [1.0], [0.35294117647058826], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.12156862745098039], [0.9411764705882353], [0.8980392156862745], [0.8862745098039215], [0.0], [0.0], [0.0], [0.9372549019607843], [0.9333333333333333], [1.0], [0.3607843137254902], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8862745098039215], [0.9137254901960784], [0.9294117647058824], [0.13333333333333333], [0.0], [0.0], [0.9176470588235294], [0.9333333333333333], [1.0], [0.37254901960784315], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9137254901960784], [0.9254901960784314], [0.9568627450980393], [0.26666666666666666], [0.0], [0.0], [0.8196078431372549], [0.9450980392156862], [0.9294117647058824], [0.3843137254901961], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.596078431372549], [0.9490196078431372], [0.9607843137254902], [0.5019607843137255], [0.0], [0.0], [0.7764705882352941], [0.9450980392156862], [0.9333333333333333], [0.3176470588235294], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.28627450980392155], [0.9647058823529412], [0.9450980392156862], [0.8274509803921568], [0.0], [0.0], [0.792156862745098], [0.9411764705882353], [0.9294117647058824], [0.2901960784313726], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.01568627450980392], [0.0], [0.0], [0.8980392156862745], [0.9254901960784314], [0.8196078431372549], [0.0], [0.0], [0.6196078431372549], [0.9686274509803922], [0.9333333333333333], [0.38823529411764707], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.7803921568627451], [1.0], [0.9686274509803922], [0.22745098039215686], [0.0], [0.6313725490196078], [1.0], [0.9882352941176471], [0.4666666666666667], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.3843137254901961], [0.6235294117647059], [0.2784313725490196], [0.0], [0.0], [0.26666666666666666], [0.6901960784313725], [0.6431372549019608], [0.22745098039215686], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]]} headers = { \"content-type\" : \"application/json\" } r = requests . post ( 'http://localhost:8501/v1/models/fashion_model:predict' , data = data , headers = headers ) j = r . json () print ( j . keys ()) print ( j ) dict_keys(['predictions']) {'predictions': [[8.25938809e-16, 2.94385077e-17, 9.78797075e-16, 2.2219498e-16, 2.59219783e-13, 1.91701793e-10, 1.40012654e-16, 6.76713e-11, 7.24115397e-18, 1.0], [1.04329297e-06, 1.63656903e-12, 0.999997139, 7.85040505e-11, 1.13352101e-08, 2.28772096e-14, 1.75158141e-06, 4.41351522e-19, 1.66081106e-15, 2.36975185e-17], [2.25010535e-13, 1.0, 4.64146882e-14, 5.00870963e-17, 6.15678637e-14, 4.10723891e-21, 2.84594985e-18, 1.07107688e-30, 1.73393987e-22, 6.02925506e-22]]} # It looks like a 2-D array, let's check its shape pred = np . array ( j [ 'predictions' ]) print ( pred . shape ) # This is the N x K output array from the model # pred[n,k] is the probability that we believe the nth sample belongs to the kth class (3, 10) # Get the predicted classes pred = pred . argmax ( axis = 1 ) # Map them back to strings pred = [ labels [ i ] for i in pred ] print ( pred ) ['Ankle boot', 'Pullover', 'Trouser'] # Get the true labels actual = [ labels [ i ] for i in y_test [: 3 ]] print ( actual ) ['Ankle boot', 'Pullover', 'Trouser'] for i in range ( 0 , 3 ): show ( i , f \"True: { actual [ i ] } , Predicted: { pred [ i ] } \" ) # Allows you to select a model by version headers = { \"content-type\" : \"application/json\" } r = requests . post ( 'http://localhost:8501/v1/models/fashion_model/versions/1:predict' , data = data , headers = headers ) j = r . json () pred = np . array ( j [ 'predictions' ]) pred = pred . argmax ( axis = 1 ) pred = [ labels [ i ] for i in pred ] for i in range ( 0 , 3 ): show ( i , f \"True: { actual [ i ] } , Predicted: { pred [ i ] } \" ) # Let's make a new model version # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Flatten ()( x ) x = Dense ( K , activation = 'softmax' )( x ) model2 = Model ( i , x ) model2 . summary () Model: \"model_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 13, 13, 32) 320 _________________________________________________________________ flatten_1 (Flatten) (None, 5408) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 54090 ================================================================= Total params: 54,410 Trainable params: 54,410 Non-trainable params: 0 _________________________________________________________________ # Compile and fit # Note: make sure you are using the GPU for this! model2 . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) Train on 60000 samples, validate on 10000 samples Epoch 1/15 60000/60000 [==============================] - 5s 86us/sample - loss: 0.4692 - accuracy: 0.8346 - val_loss: 0.3850 - val_accuracy: 0.8629 Epoch 2/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.3368 - accuracy: 0.8807 - val_loss: 0.3679 - val_accuracy: 0.8678 Epoch 3/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.3004 - accuracy: 0.8919 - val_loss: 0.3298 - val_accuracy: 0.8799 Epoch 4/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2762 - accuracy: 0.9006 - val_loss: 0.3151 - val_accuracy: 0.8866 Epoch 5/15 60000/60000 [==============================] - 5s 82us/sample - loss: 0.2587 - accuracy: 0.9067 - val_loss: 0.3167 - val_accuracy: 0.8877 Epoch 6/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2443 - accuracy: 0.9115 - val_loss: 0.3120 - val_accuracy: 0.8881 Epoch 7/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2325 - accuracy: 0.9155 - val_loss: 0.3009 - val_accuracy: 0.8935 Epoch 8/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2212 - accuracy: 0.9199 - val_loss: 0.3009 - val_accuracy: 0.8941 Epoch 9/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.2115 - accuracy: 0.9243 - val_loss: 0.2936 - val_accuracy: 0.8963 Epoch 10/15 60000/60000 [==============================] - 5s 79us/sample - loss: 0.2015 - accuracy: 0.9280 - val_loss: 0.2981 - val_accuracy: 0.8966 Epoch 11/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.1926 - accuracy: 0.9312 - val_loss: 0.2968 - val_accuracy: 0.8987 Epoch 12/15 60000/60000 [==============================] - 5s 81us/sample - loss: 0.1855 - accuracy: 0.9340 - val_loss: 0.3009 - val_accuracy: 0.8970 Epoch 13/15 60000/60000 [==============================] - 5s 81us/sample - loss: 0.1785 - accuracy: 0.9361 - val_loss: 0.3001 - val_accuracy: 0.8985 Epoch 14/15 60000/60000 [==============================] - 5s 83us/sample - loss: 0.1718 - accuracy: 0.9385 - val_loss: 0.3085 - val_accuracy: 0.8981 Epoch 15/15 60000/60000 [==============================] - 5s 80us/sample - loss: 0.1654 - accuracy: 0.9404 - val_loss: 0.3049 - val_accuracy: 0.9005 # Save version 2 of the model version = 2 export_path = os . path . join ( MODEL_DIR , str ( version )) print ( 'export_path = {} \\n ' . format ( export_path )) if os . path . isdir ( export_path ): print ( ' \\n Already saved a model, cleaning up \\n ' ) ! rm - r { export_path } tf . saved_model . save ( model2 , export_path ) print ( ' \\n Saved model:' ) ! ls - l { export_path } export_path = /tmp/2 Saved model: total 76 drwxr-xr-x 2 root root 4096 Aug 10 04:34 assets -rw-r--r-- 1 root root 66590 Aug 10 04:34 saved_model.pb drwxr-xr-x 2 root root 4096 Aug 10 04:34 variables # Will Tensorflow serving know about the new model without restarting? headers = { \"content-type\" : \"application/json\" } r = requests . post ( 'http://localhost:8501/v1/models/fashion_model/versions/2:predict' , data = data , headers = headers ) j = r . json () pred = np . array ( j [ 'predictions' ]) pred = pred . argmax ( axis = 1 ) pred = [ labels [ i ] for i in pred ] for i in range ( 0 , 3 ): show ( i , f \"True: { actual [ i ] } , Predicted: { pred [ i ] } \" ) # What if we use a version number that does not exist? headers = { \"content-type\" : \"application/json\" } r = requests . post ( 'http://localhost:8501/v1/models/fashion_model/versions/3:predict' , data = data , headers = headers ) j = r . json () print ( j ) {'error': 'Servable not found for request: Specific(fashion_model, 3)'} Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Serving"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_TFLite/","text":"================ by Jawad Haider TFLite TFLite \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-rc0 # Load in the data mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) # Compile the model model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # Train the model r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 10 ) Train on 60000 samples, validate on 10000 samples Epoch 1/10 WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa310296c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa310296c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' 60000/60000 [==============================] - 8s 134us/sample - loss: 0.3042 - accuracy: 0.9109 - val_loss: 0.1454 - val_accuracy: 0.9561 Epoch 2/10 60000/60000 [==============================] - 7s 122us/sample - loss: 0.1469 - accuracy: 0.9558 - val_loss: 0.1036 - val_accuracy: 0.9693 Epoch 3/10 60000/60000 [==============================] - 7s 115us/sample - loss: 0.1100 - accuracy: 0.9680 - val_loss: 0.0922 - val_accuracy: 0.9723 Epoch 4/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0889 - accuracy: 0.9722 - val_loss: 0.0814 - val_accuracy: 0.9747 Epoch 5/10 60000/60000 [==============================] - 7s 112us/sample - loss: 0.0763 - accuracy: 0.9767 - val_loss: 0.0762 - val_accuracy: 0.9769 Epoch 6/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0682 - accuracy: 0.9785 - val_loss: 0.0692 - val_accuracy: 0.9788 Epoch 7/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0565 - accuracy: 0.9816 - val_loss: 0.0746 - val_accuracy: 0.9781 Epoch 8/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0521 - accuracy: 0.9829 - val_loss: 0.0700 - val_accuracy: 0.9789 Epoch 9/10 60000/60000 [==============================] - 7s 109us/sample - loss: 0.0497 - accuracy: 0.9842 - val_loss: 0.0720 - val_accuracy: 0.9790 Epoch 10/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0460 - accuracy: 0.9846 - val_loss: 0.0719 - val_accuracy: 0.9792 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fa2c8840e48> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fa2c876e358> # Evaluate the model print ( model . evaluate ( x_test , y_test )) 10000/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 74us/sample - loss: 0.0362 - accuracy: 0.9792 [0.0719173606941069, 0.9792] # Convert the model to TFLite format # Create a converter object converter = tf . lite . TFLiteConverter . from_keras_model ( model ) # Convert the model tflite_model = converter . convert () # Save to file with open ( \"converted_model.tflite\" , \"wb\" ) as f : f . write ( tflite_model ) WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa3103f3048> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa3103f3048> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' # Check if it exists ! ls converted_model.tflite keras_model.h5 sample_data Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 TFLite"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_TFLite/#tflite","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-rc0 # Load in the data mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) # Compile the model model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # Train the model r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 10 ) Train on 60000 samples, validate on 10000 samples Epoch 1/10 WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa310296c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa310296c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' 60000/60000 [==============================] - 8s 134us/sample - loss: 0.3042 - accuracy: 0.9109 - val_loss: 0.1454 - val_accuracy: 0.9561 Epoch 2/10 60000/60000 [==============================] - 7s 122us/sample - loss: 0.1469 - accuracy: 0.9558 - val_loss: 0.1036 - val_accuracy: 0.9693 Epoch 3/10 60000/60000 [==============================] - 7s 115us/sample - loss: 0.1100 - accuracy: 0.9680 - val_loss: 0.0922 - val_accuracy: 0.9723 Epoch 4/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0889 - accuracy: 0.9722 - val_loss: 0.0814 - val_accuracy: 0.9747 Epoch 5/10 60000/60000 [==============================] - 7s 112us/sample - loss: 0.0763 - accuracy: 0.9767 - val_loss: 0.0762 - val_accuracy: 0.9769 Epoch 6/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0682 - accuracy: 0.9785 - val_loss: 0.0692 - val_accuracy: 0.9788 Epoch 7/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0565 - accuracy: 0.9816 - val_loss: 0.0746 - val_accuracy: 0.9781 Epoch 8/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0521 - accuracy: 0.9829 - val_loss: 0.0700 - val_accuracy: 0.9789 Epoch 9/10 60000/60000 [==============================] - 7s 109us/sample - loss: 0.0497 - accuracy: 0.9842 - val_loss: 0.0720 - val_accuracy: 0.9790 Epoch 10/10 60000/60000 [==============================] - 7s 110us/sample - loss: 0.0460 - accuracy: 0.9846 - val_loss: 0.0719 - val_accuracy: 0.9792 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fa2c8840e48> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fa2c876e358> # Evaluate the model print ( model . evaluate ( x_test , y_test )) 10000/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 74us/sample - loss: 0.0362 - accuracy: 0.9792 [0.0719173606941069, 0.9792] # Convert the model to TFLite format # Create a converter object converter = tf . lite . TFLiteConverter . from_keras_model ( model ) # Convert the model tflite_model = converter . convert () # Save to file with open ( \"converted_model.tflite\" , \"wb\" ) as f : f . write ( tflite_model ) WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa3103f3048> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa3103f3048> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' # Check if it exists ! ls converted_model.tflite keras_model.h5 sample_data Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TFLite"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TPU/","text":"================ by Jawad Haider TPU TPU \u00b6 import tensorflow as tf import numpy as np import pandas as pd from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , \\ GlobalMaxPooling2D , MaxPooling2D , BatchNormalization from tensorflow.keras.models import Model resolver = tf . distribute . cluster_resolver . TPUClusterResolver ( tpu = '' ) tf . config . experimental_connect_to_cluster ( resolver ) tf . tpu . experimental . initialize_tpu_system ( resolver ) print ( \"All devices: \" , tf . config . list_logical_devices ( 'TPU' )) INFO:tensorflow:Deallocate tpu buffers before initializing tpu system. INFO:tensorflow:Deallocate tpu buffers before initializing tpu system. INFO:tensorflow:Initializing the TPU system: grpc://10.117.37.210:8470 INFO:tensorflow:Initializing the TPU system: grpc://10.117.37.210:8470 INFO:tensorflow:Finished initializing TPU system. INFO:tensorflow:Finished initializing TPU system. All devices: [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')] strategy = tf . distribute . TPUStrategy ( resolver ) INFO:tensorflow:Found TPU system: INFO:tensorflow:Found TPU system: INFO:tensorflow:*** Num TPU Cores: 8 INFO:tensorflow:*** Num TPU Cores: 8 INFO:tensorflow:*** Num TPU Workers: 1 INFO:tensorflow:*** Num TPU Workers: 1 INFO:tensorflow:*** Num TPU Cores Per Worker: 8 INFO:tensorflow:*** Num TPU Cores Per Worker: 8 INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0) # Note: model creation must be in strategy scope # We will define the function now, but this code # won't run outside the scope def create_model (): i = Input ( shape = ( 32 , 32 , 3 )) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( i ) x = BatchNormalization ()( x ) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 10 )( x ) model = Model ( i , x ) return model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 2s 0us/step 170508288/170498071 [==============================] - 2s 0us/step x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) train_dataset = tf . data . Dataset . from_tensor_slices (( x_train , y_train )) test_dataset = tf . data . Dataset . from_tensor_slices (( x_test , y_test )) with strategy . scope (): model = create_model () model . compile ( optimizer = 'adam' , loss = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ), metrics = [ 'sparse_categorical_accuracy' ]) batch_size = 256 # reshuffle_each_iteration=None is default but is later set to True if None # thus \"True\" is the actual default train_dataset = train_dataset . shuffle ( 1000 ) . batch ( batch_size ) test_dataset = test_dataset . batch ( batch_size ) model . fit ( train_dataset , epochs = 5 , validation_data = test_dataset ) Epoch 1/5 WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version. Instructions for updating: use `experimental_local_results` instead. WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version. Instructions for updating: use `experimental_local_results` instead. 196/196 [==============================] - 23s 73ms/step - loss: 1.6891 - sparse_categorical_accuracy: 0.4619 - val_loss: 4.2676 - val_sparse_categorical_accuracy: 0.1514 Epoch 2/5 196/196 [==============================] - 9s 46ms/step - loss: 1.0112 - sparse_categorical_accuracy: 0.6388 - val_loss: 2.1024 - val_sparse_categorical_accuracy: 0.3468 Epoch 3/5 196/196 [==============================] - 9s 44ms/step - loss: 0.7856 - sparse_categorical_accuracy: 0.7238 - val_loss: 0.8410 - val_sparse_categorical_accuracy: 0.7061 Epoch 4/5 196/196 [==============================] - 9s 45ms/step - loss: 0.6428 - sparse_categorical_accuracy: 0.7735 - val_loss: 0.6944 - val_sparse_categorical_accuracy: 0.7592 Epoch 5/5 196/196 [==============================] - 9s 46ms/step - loss: 0.5314 - sparse_categorical_accuracy: 0.8134 - val_loss: 0.6975 - val_sparse_categorical_accuracy: 0.7645 <keras.callbacks.History at 0x7ff8093771d0> model . save ( 'mymodel.h5' ) with strategy . scope (): model = tf . keras . models . load_model ( 'mymodel.h5' ) out = model . predict ( x_test [: 1 ]) print ( out ) [[-1.595037 -2.7977936 0.59842265 6.563197 -1.4122396 4.490996 4.286044 -2.2540898 -3.8138728 -2.6489956 ]] # Note: old bug # https://www.kaggle.com/c/flower-classification-with-tpus/discussion/148615 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TPU"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TPU/#tpu","text":"import tensorflow as tf import numpy as np import pandas as pd from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , \\ GlobalMaxPooling2D , MaxPooling2D , BatchNormalization from tensorflow.keras.models import Model resolver = tf . distribute . cluster_resolver . TPUClusterResolver ( tpu = '' ) tf . config . experimental_connect_to_cluster ( resolver ) tf . tpu . experimental . initialize_tpu_system ( resolver ) print ( \"All devices: \" , tf . config . list_logical_devices ( 'TPU' )) INFO:tensorflow:Deallocate tpu buffers before initializing tpu system. INFO:tensorflow:Deallocate tpu buffers before initializing tpu system. INFO:tensorflow:Initializing the TPU system: grpc://10.117.37.210:8470 INFO:tensorflow:Initializing the TPU system: grpc://10.117.37.210:8470 INFO:tensorflow:Finished initializing TPU system. INFO:tensorflow:Finished initializing TPU system. All devices: [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')] strategy = tf . distribute . TPUStrategy ( resolver ) INFO:tensorflow:Found TPU system: INFO:tensorflow:Found TPU system: INFO:tensorflow:*** Num TPU Cores: 8 INFO:tensorflow:*** Num TPU Cores: 8 INFO:tensorflow:*** Num TPU Workers: 1 INFO:tensorflow:*** Num TPU Workers: 1 INFO:tensorflow:*** Num TPU Cores Per Worker: 8 INFO:tensorflow:*** Num TPU Cores Per Worker: 8 INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0) # Note: model creation must be in strategy scope # We will define the function now, but this code # won't run outside the scope def create_model (): i = Input ( shape = ( 32 , 32 , 3 )) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( i ) x = BatchNormalization ()( x ) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 10 )( x ) model = Model ( i , x ) return model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 2s 0us/step 170508288/170498071 [==============================] - 2s 0us/step x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) train_dataset = tf . data . Dataset . from_tensor_slices (( x_train , y_train )) test_dataset = tf . data . Dataset . from_tensor_slices (( x_test , y_test )) with strategy . scope (): model = create_model () model . compile ( optimizer = 'adam' , loss = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ), metrics = [ 'sparse_categorical_accuracy' ]) batch_size = 256 # reshuffle_each_iteration=None is default but is later set to True if None # thus \"True\" is the actual default train_dataset = train_dataset . shuffle ( 1000 ) . batch ( batch_size ) test_dataset = test_dataset . batch ( batch_size ) model . fit ( train_dataset , epochs = 5 , validation_data = test_dataset ) Epoch 1/5 WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version. Instructions for updating: use `experimental_local_results` instead. WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version. Instructions for updating: use `experimental_local_results` instead. 196/196 [==============================] - 23s 73ms/step - loss: 1.6891 - sparse_categorical_accuracy: 0.4619 - val_loss: 4.2676 - val_sparse_categorical_accuracy: 0.1514 Epoch 2/5 196/196 [==============================] - 9s 46ms/step - loss: 1.0112 - sparse_categorical_accuracy: 0.6388 - val_loss: 2.1024 - val_sparse_categorical_accuracy: 0.3468 Epoch 3/5 196/196 [==============================] - 9s 44ms/step - loss: 0.7856 - sparse_categorical_accuracy: 0.7238 - val_loss: 0.8410 - val_sparse_categorical_accuracy: 0.7061 Epoch 4/5 196/196 [==============================] - 9s 45ms/step - loss: 0.6428 - sparse_categorical_accuracy: 0.7735 - val_loss: 0.6944 - val_sparse_categorical_accuracy: 0.7592 Epoch 5/5 196/196 [==============================] - 9s 46ms/step - loss: 0.5314 - sparse_categorical_accuracy: 0.8134 - val_loss: 0.6975 - val_sparse_categorical_accuracy: 0.7645 <keras.callbacks.History at 0x7ff8093771d0> model . save ( 'mymodel.h5' ) with strategy . scope (): model = tf . keras . models . load_model ( 'mymodel.h5' ) out = model . predict ( x_test [: 1 ]) print ( out ) [[-1.595037 -2.7977936 0.59842265 6.563197 -1.4122396 4.490996 4.286044 -2.2540898 -3.8138728 -2.6489956 ]] # Note: old bug # https://www.kaggle.com/c/flower-classification-with-tpus/discussion/148615 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TPU"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR/","text":"================ by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , GlobalMaxPooling2D from tensorflow.keras.models import Model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Conv2D ( 64 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Conv2D ( 128 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Flatten ()( x ) x = Dropout ( 0.5 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile and fit # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) Train on 50000 samples, validate on 10000 samples Epoch 1/15 50000/50000 [==============================] - 17s 336us/sample - loss: 1.5736 - accuracy: 0.4255 - val_loss: 1.3048 - val_accuracy: 0.5317 Epoch 2/15 50000/50000 [==============================] - 16s 320us/sample - loss: 1.2732 - accuracy: 0.5435 - val_loss: 1.1139 - val_accuracy: 0.6004 Epoch 3/15 50000/50000 [==============================] - 16s 321us/sample - loss: 1.1454 - accuracy: 0.5909 - val_loss: 1.0765 - val_accuracy: 0.6176 Epoch 4/15 50000/50000 [==============================] - 16s 325us/sample - loss: 1.0497 - accuracy: 0.6259 - val_loss: 0.9637 - val_accuracy: 0.6603 Epoch 5/15 50000/50000 [==============================] - 16s 325us/sample - loss: 0.9764 - accuracy: 0.6546 - val_loss: 0.9363 - val_accuracy: 0.6728 Epoch 6/15 50000/50000 [==============================] - 16s 323us/sample - loss: 0.9164 - accuracy: 0.6735 - val_loss: 0.9153 - val_accuracy: 0.6772 Epoch 7/15 50000/50000 [==============================] - 16s 322us/sample - loss: 0.8660 - accuracy: 0.6944 - val_loss: 0.8996 - val_accuracy: 0.6818 Epoch 8/15 50000/50000 [==============================] - 17s 330us/sample - loss: 0.8213 - accuracy: 0.7083 - val_loss: 0.8806 - val_accuracy: 0.6954 Epoch 9/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.7807 - accuracy: 0.7233 - val_loss: 0.8833 - val_accuracy: 0.6951 Epoch 10/15 50000/50000 [==============================] - 16s 327us/sample - loss: 0.7470 - accuracy: 0.7359 - val_loss: 0.8368 - val_accuracy: 0.7058 Epoch 11/15 50000/50000 [==============================] - 16s 325us/sample - loss: 0.7121 - accuracy: 0.7462 - val_loss: 0.8376 - val_accuracy: 0.7092 Epoch 12/15 50000/50000 [==============================] - 17s 330us/sample - loss: 0.6808 - accuracy: 0.7560 - val_loss: 0.8430 - val_accuracy: 0.7115 Epoch 13/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.6576 - accuracy: 0.7671 - val_loss: 0.8284 - val_accuracy: 0.7110 Epoch 14/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.6402 - accuracy: 0.7702 - val_loss: 0.8487 - val_accuracy: 0.7067 Epoch 15/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.6232 - accuracy: 0.7769 - val_loss: 0.8320 - val_accuracy: 0.7136 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f04d6f0db00> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f04d6b34208> # Plot confusion matrix from sklearn.metrics import confusion_matrix import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) Confusion matrix, without normalization [[741 16 36 26 12 2 6 11 113 37] [ 14 781 9 9 2 4 6 2 47 126] [ 62 9 545 91 102 66 50 29 28 18] [ 20 9 51 548 51 171 58 33 30 29] [ 40 10 62 79 617 43 47 78 19 5] [ 15 2 52 214 38 572 13 55 21 18] [ 6 9 45 101 43 44 711 7 18 16] [ 24 3 38 48 59 64 5 726 6 27] [ 49 30 9 17 6 3 1 3 865 17] [ 35 105 16 19 4 7 1 11 46 756]] # label mapping labels = '''airplane automobile bird cat deer dog frog horse ship truck''' . split () # Show some misclassified examples # TODO: add label names misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( labels [ y_test [ i ]], labels [ p_test [ i ]])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 CIFAR"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR_Improved/","text":"================ by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.2.0-rc2 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , GlobalMaxPooling2D , MaxPooling2D , BatchNormalization from tensorflow.keras.models import Model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 13s 0us/step x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) # x = Conv2D(32, (3, 3), strides=2, activation='relu')(i) # x = Conv2D(64, (3, 3), strides=2, activation='relu')(x) # x = Conv2D(128, (3, 3), strides=2, activation='relu')(x) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( i ) x = BatchNormalization ()( x ) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) # x = Dropout(0.2)(x) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) # x = Dropout(0.2)(x) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) # x = Dropout(0.2)(x) # x = GlobalMaxPooling2D()(x) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # Fit r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 50 ) Epoch 1/50 1465/1563 [===========================>..] - ETA: 0s - loss: 1.2974 - accuracy: 0.5487 KeyboardInterrupt: ignored # Fit with data augmentation # Note: if you run this AFTER calling the previous model.fit(), it will CONTINUE training where it left off batch_size = 32 data_generator = tf . keras . preprocessing . image . ImageDataGenerator ( width_shift_range = 0.1 , height_shift_range = 0.1 , horizontal_flip = True ) train_generator = data_generator . flow ( x_train , y_train , batch_size ) steps_per_epoch = x_train . shape [ 0 ] // batch_size r = model . fit ( train_generator , validation_data = ( x_test , y_test ), steps_per_epoch = steps_per_epoch , epochs = 50 ) Epoch 1/50 1562/1562 [==============================] - 27s 17ms/step - loss: 0.9854 - accuracy: 0.6597 - val_loss: 0.9380 - val_accuracy: 0.6898 Epoch 2/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.8444 - accuracy: 0.7101 - val_loss: 0.8461 - val_accuracy: 0.7158 Epoch 3/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.7506 - accuracy: 0.7444 - val_loss: 0.7562 - val_accuracy: 0.7485 Epoch 4/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.6825 - accuracy: 0.7695 - val_loss: 0.6062 - val_accuracy: 0.7959 Epoch 5/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.6324 - accuracy: 0.7856 - val_loss: 0.6161 - val_accuracy: 0.7971 Epoch 6/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.5944 - accuracy: 0.7997 - val_loss: 0.6473 - val_accuracy: 0.7832 Epoch 7/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.5502 - accuracy: 0.8109 - val_loss: 0.6116 - val_accuracy: 0.8007 Epoch 8/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.5210 - accuracy: 0.8227 - val_loss: 0.6694 - val_accuracy: 0.7861 Epoch 9/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4947 - accuracy: 0.8300 - val_loss: 0.4850 - val_accuracy: 0.8358 Epoch 10/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4690 - accuracy: 0.8407 - val_loss: 0.5492 - val_accuracy: 0.8174 Epoch 11/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4480 - accuracy: 0.8479 - val_loss: 0.5357 - val_accuracy: 0.8212 Epoch 12/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4283 - accuracy: 0.8523 - val_loss: 0.5085 - val_accuracy: 0.8319 Epoch 13/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4125 - accuracy: 0.8577 - val_loss: 0.5201 - val_accuracy: 0.8308 Epoch 14/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.3929 - accuracy: 0.8662 - val_loss: 0.4446 - val_accuracy: 0.8510 Epoch 15/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3779 - accuracy: 0.8694 - val_loss: 0.4738 - val_accuracy: 0.8506 Epoch 16/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3706 - accuracy: 0.8721 - val_loss: 0.4617 - val_accuracy: 0.8504 Epoch 17/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3470 - accuracy: 0.8812 - val_loss: 0.4172 - val_accuracy: 0.8627 Epoch 18/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3405 - accuracy: 0.8818 - val_loss: 0.4572 - val_accuracy: 0.8587 Epoch 19/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3312 - accuracy: 0.8868 - val_loss: 0.4150 - val_accuracy: 0.8654 Epoch 20/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3203 - accuracy: 0.8880 - val_loss: 0.5443 - val_accuracy: 0.8273 Epoch 21/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3108 - accuracy: 0.8919 - val_loss: 0.4421 - val_accuracy: 0.8605 Epoch 22/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3017 - accuracy: 0.8964 - val_loss: 0.4778 - val_accuracy: 0.8537 Epoch 23/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2978 - accuracy: 0.8975 - val_loss: 0.4370 - val_accuracy: 0.8621 Epoch 24/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2830 - accuracy: 0.9023 - val_loss: 0.4270 - val_accuracy: 0.8676 Epoch 25/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2796 - accuracy: 0.9035 - val_loss: 0.4009 - val_accuracy: 0.8748 Epoch 26/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2714 - accuracy: 0.9069 - val_loss: 0.4017 - val_accuracy: 0.8719 Epoch 27/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2663 - accuracy: 0.9080 - val_loss: 0.4199 - val_accuracy: 0.8669 Epoch 28/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2547 - accuracy: 0.9121 - val_loss: 0.4094 - val_accuracy: 0.8703 Epoch 29/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2574 - accuracy: 0.9110 - val_loss: 0.4227 - val_accuracy: 0.8698 Epoch 30/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2464 - accuracy: 0.9141 - val_loss: 0.4117 - val_accuracy: 0.8649 Epoch 31/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2439 - accuracy: 0.9157 - val_loss: 0.4096 - val_accuracy: 0.8758 Epoch 32/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2411 - accuracy: 0.9159 - val_loss: 0.4118 - val_accuracy: 0.8705 Epoch 33/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2330 - accuracy: 0.9194 - val_loss: 0.3841 - val_accuracy: 0.8764 Epoch 34/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2312 - accuracy: 0.9201 - val_loss: 0.4127 - val_accuracy: 0.8708 Epoch 35/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2272 - accuracy: 0.9209 - val_loss: 0.4259 - val_accuracy: 0.8762 Epoch 36/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2204 - accuracy: 0.9241 - val_loss: 0.4246 - val_accuracy: 0.8769 Epoch 37/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2150 - accuracy: 0.9251 - val_loss: 0.3939 - val_accuracy: 0.8797 Epoch 38/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2156 - accuracy: 0.9261 - val_loss: 0.4005 - val_accuracy: 0.8790 Epoch 39/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2135 - accuracy: 0.9268 - val_loss: 0.3959 - val_accuracy: 0.8773 Epoch 40/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2121 - accuracy: 0.9263 - val_loss: 0.4072 - val_accuracy: 0.8742 Epoch 41/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2087 - accuracy: 0.9277 - val_loss: 0.4234 - val_accuracy: 0.8769 Epoch 42/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2023 - accuracy: 0.9310 - val_loss: 0.3904 - val_accuracy: 0.8812 Epoch 43/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1986 - accuracy: 0.9311 - val_loss: 0.3859 - val_accuracy: 0.8806 Epoch 44/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1936 - accuracy: 0.9341 - val_loss: 0.4627 - val_accuracy: 0.8703 Epoch 45/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1907 - accuracy: 0.9342 - val_loss: 0.4460 - val_accuracy: 0.8646 Epoch 46/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1884 - accuracy: 0.9341 - val_loss: 0.4511 - val_accuracy: 0.8658 Epoch 47/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1877 - accuracy: 0.9344 - val_loss: 0.3790 - val_accuracy: 0.8831 Epoch 48/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1851 - accuracy: 0.9366 - val_loss: 0.4208 - val_accuracy: 0.8770 Epoch 49/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1869 - accuracy: 0.9364 - val_loss: 0.3946 - val_accuracy: 0.8841 Epoch 50/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1767 - accuracy: 0.9397 - val_loss: 0.4432 - val_accuracy: 0.8781 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f1d5a2856d8> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f1d5a26ada0> # Plot confusion matrix from sklearn.metrics import confusion_matrix import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) Confusion matrix, without normalization [[908 14 12 2 1 0 2 2 33 26] [ 5 957 0 0 0 1 3 0 4 30] [ 42 4 817 18 28 18 43 11 7 12] [ 22 18 27 742 29 62 48 19 13 20] [ 16 2 39 22 847 16 34 15 5 4] [ 5 10 22 94 23 790 21 23 4 8] [ 4 2 16 18 4 2 946 1 4 3] [ 15 5 6 15 27 6 5 908 6 7] [ 37 12 1 3 0 0 1 2 921 23] [ 9 37 0 0 1 1 2 1 4 945]] # label mapping labels = '''airplane automobile bird cat deer dog frog horse ship truck''' . split () # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( labels [ y_test [ i ]], labels [ p_test [ i ]])); # Now that the model is so large, it's useful to summarize it model . summary () Model: \"model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 32, 32, 3)] 0 _________________________________________________________________ conv2d (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ batch_normalization (BatchNo (None, 32, 32, 32) 128 _________________________________________________________________ conv2d_1 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 32) 128 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ batch_normalization_2 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ batch_normalization_3 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 8, 8, 128) 73856 _________________________________________________________________ batch_normalization_4 (Batch (None, 8, 8, 128) 512 _________________________________________________________________ conv2d_5 (Conv2D) (None, 8, 8, 128) 147584 _________________________________________________________________ batch_normalization_5 (Batch (None, 8, 8, 128) 512 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 2048) 0 _________________________________________________________________ dropout (Dropout) (None, 2048) 0 _________________________________________________________________ dense (Dense) (None, 1024) 2098176 _________________________________________________________________ dropout_1 (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 10) 10250 ================================================================= Total params: 2,397,226 Trainable params: 2,396,330 Non-trainable params: 896 _________________________________________________________________ Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 CIFAR Improved"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_Fashion_MNIST/","text":"================ by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.1.0-rc1 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout from tensorflow.keras.models import Model # Load in the data fashion_mnist = tf . keras . datasets . fashion_mnist ( x_train , y_train ), ( x_test , y_test ) = fashion_mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # the data is only 2D! # convolution expects height x width x color x_train = np . expand_dims ( x_train , - 1 ) x_test = np . expand_dims ( x_test , - 1 ) print ( x_train . shape ) (60000, 28, 28, 1) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Conv2D ( 64 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Conv2D ( 128 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 512 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile and fit # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) Train on 60000 samples, validate on 10000 samples Epoch 1/15 60000/60000 [==============================] - 11s 175us/sample - loss: 0.5238 - accuracy: 0.8055 - val_loss: 0.3963 - val_accuracy: 0.8485 Epoch 2/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.3679 - accuracy: 0.8616 - val_loss: 0.3406 - val_accuracy: 0.8744 Epoch 3/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.3158 - accuracy: 0.8802 - val_loss: 0.3336 - val_accuracy: 0.8733 Epoch 4/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.2813 - accuracy: 0.8942 - val_loss: 0.3047 - val_accuracy: 0.8878 Epoch 5/15 60000/60000 [==============================] - 7s 109us/sample - loss: 0.2563 - accuracy: 0.9026 - val_loss: 0.3044 - val_accuracy: 0.8916 Epoch 6/15 60000/60000 [==============================] - 7s 110us/sample - loss: 0.2363 - accuracy: 0.9104 - val_loss: 0.2950 - val_accuracy: 0.8955 Epoch 7/15 60000/60000 [==============================] - 7s 108us/sample - loss: 0.2166 - accuracy: 0.9181 - val_loss: 0.2928 - val_accuracy: 0.8985 Epoch 8/15 60000/60000 [==============================] - 7s 111us/sample - loss: 0.1986 - accuracy: 0.9243 - val_loss: 0.2932 - val_accuracy: 0.9001 Epoch 9/15 60000/60000 [==============================] - 7s 112us/sample - loss: 0.1863 - accuracy: 0.9294 - val_loss: 0.3185 - val_accuracy: 0.8923 Epoch 10/15 60000/60000 [==============================] - 7s 109us/sample - loss: 0.1712 - accuracy: 0.9355 - val_loss: 0.3151 - val_accuracy: 0.8970 Epoch 11/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1582 - accuracy: 0.9398 - val_loss: 0.3154 - val_accuracy: 0.8998 Epoch 12/15 60000/60000 [==============================] - 6s 108us/sample - loss: 0.1509 - accuracy: 0.9414 - val_loss: 0.3235 - val_accuracy: 0.8979 Epoch 13/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1412 - accuracy: 0.9461 - val_loss: 0.3347 - val_accuracy: 0.9013 Epoch 14/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1326 - accuracy: 0.9489 - val_loss: 0.3652 - val_accuracy: 0.8961 Epoch 15/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1266 - accuracy: 0.9520 - val_loss: 0.3516 - val_accuracy: 0.9001 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f23402c8860> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f23243183c8> # Plot confusion matrix from sklearn.metrics import confusion_matrix import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) Confusion matrix, without normalization [[858 0 27 15 3 1 91 0 5 0] [ 1 975 2 14 3 0 3 0 2 0] [ 13 1 881 12 51 0 41 0 1 0] [ 12 6 20 887 53 1 20 0 1 0] [ 0 1 74 10 875 0 38 0 2 0] [ 0 0 1 0 0 977 0 12 1 9] [115 0 87 26 123 0 641 0 8 0] [ 0 0 0 0 0 10 0 962 0 28] [ 1 0 4 2 8 1 5 2 975 2] [ 0 0 0 0 0 4 1 25 0 970]] # Label mapping labels = '''T-shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot''' . split ( \" \\n \" ) # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ] . reshape ( 28 , 28 ), cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( labels [ y_test [ i ]], labels [ p_test [ i ]])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Fashion MNIST"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Demo/","text":"================ by Jawad Haider This is my title import numpy as np import matplotlib.pyplot as plt x = np . linspace ( 0 , 10 * np . pi , 1000 ) y = np . sin ( x ) plt . plot ( x , y ) This is my title \u00b6 Here is some regular text. import numpy as np import sklearn print ( sklearn . __version__ ) import numpy print ( numpy . __version__ ) import scipy print ( scipy . __version__ ) import matplotlib print ( matplotlib . __version__ ) import pandas print ( pandas . __version__ ) import torch print ( torch . __version__ ) import seaborn print ( seaborn . __version__ ) import wordcloud print ( wordcloud . __version__ ) import bs4 print ( bs4 . __version__ ) import requests print ( requests . __version__ ) import theano print ( theano . __version__ ) import networkx print ( networkx . __version__ ) import cv2 print ( cv2 . __version__ ) import gym print ( gym . __version__ ) 0.21.2 1.16.4 1.3.0 3.0.3 0.24.2 1.1.0 0.9.0 1.5.0 4.6.3 2.21.0 1.0.4 2.3 3.4.3 0.10.11 ! ls sample_data ! ls sample_data anscombe.json mnist_test.csv california_housing_test.csv mnist_train_small.csv california_housing_train.csv README.md import json json . loads ( open ( 'sample_data/anscombe.json' ) . read ()) [{'Series': 'I', 'X': 10.0, 'Y': 8.04}, {'Series': 'I', 'X': 8.0, 'Y': 6.95}, {'Series': 'I', 'X': 13.0, 'Y': 7.58}, {'Series': 'I', 'X': 9.0, 'Y': 8.81}, {'Series': 'I', 'X': 11.0, 'Y': 8.33}, {'Series': 'I', 'X': 14.0, 'Y': 9.96}, {'Series': 'I', 'X': 6.0, 'Y': 7.24}, {'Series': 'I', 'X': 4.0, 'Y': 4.26}, {'Series': 'I', 'X': 12.0, 'Y': 10.84}, {'Series': 'I', 'X': 7.0, 'Y': 4.81}, {'Series': 'I', 'X': 5.0, 'Y': 5.68}, {'Series': 'II', 'X': 10.0, 'Y': 9.14}, {'Series': 'II', 'X': 8.0, 'Y': 8.14}, {'Series': 'II', 'X': 13.0, 'Y': 8.74}, {'Series': 'II', 'X': 9.0, 'Y': 8.77}, {'Series': 'II', 'X': 11.0, 'Y': 9.26}, {'Series': 'II', 'X': 14.0, 'Y': 8.1}, {'Series': 'II', 'X': 6.0, 'Y': 6.13}, {'Series': 'II', 'X': 4.0, 'Y': 3.1}, {'Series': 'II', 'X': 12.0, 'Y': 9.13}, {'Series': 'II', 'X': 7.0, 'Y': 7.26}, {'Series': 'II', 'X': 5.0, 'Y': 4.74}, {'Series': 'III', 'X': 10.0, 'Y': 7.46}, {'Series': 'III', 'X': 8.0, 'Y': 6.77}, {'Series': 'III', 'X': 13.0, 'Y': 12.74}, {'Series': 'III', 'X': 9.0, 'Y': 7.11}, {'Series': 'III', 'X': 11.0, 'Y': 7.81}, {'Series': 'III', 'X': 14.0, 'Y': 8.84}, {'Series': 'III', 'X': 6.0, 'Y': 6.08}, {'Series': 'III', 'X': 4.0, 'Y': 5.39}, {'Series': 'III', 'X': 12.0, 'Y': 8.15}, {'Series': 'III', 'X': 7.0, 'Y': 6.42}, {'Series': 'III', 'X': 5.0, 'Y': 5.73}, {'Series': 'IV', 'X': 8.0, 'Y': 6.58}, {'Series': 'IV', 'X': 8.0, 'Y': 5.76}, {'Series': 'IV', 'X': 8.0, 'Y': 7.71}, {'Series': 'IV', 'X': 8.0, 'Y': 8.84}, {'Series': 'IV', 'X': 8.0, 'Y': 8.47}, {'Series': 'IV', 'X': 8.0, 'Y': 7.04}, {'Series': 'IV', 'X': 8.0, 'Y': 5.25}, {'Series': 'IV', 'X': 19.0, 'Y': 12.5}, {'Series': 'IV', 'X': 8.0, 'Y': 5.56}, {'Series': 'IV', 'X': 8.0, 'Y': 7.91}, {'Series': 'IV', 'X': 8.0, 'Y': 6.89}] a = 5 print ( a ) NameError: ignored Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Demo"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Demo/#this-is-my-title","text":"Here is some regular text. import numpy as np import sklearn print ( sklearn . __version__ ) import numpy print ( numpy . __version__ ) import scipy print ( scipy . __version__ ) import matplotlib print ( matplotlib . __version__ ) import pandas print ( pandas . __version__ ) import torch print ( torch . __version__ ) import seaborn print ( seaborn . __version__ ) import wordcloud print ( wordcloud . __version__ ) import bs4 print ( bs4 . __version__ ) import requests print ( requests . __version__ ) import theano print ( theano . __version__ ) import networkx print ( networkx . __version__ ) import cv2 print ( cv2 . __version__ ) import gym print ( gym . __version__ ) 0.21.2 1.16.4 1.3.0 3.0.3 0.24.2 1.1.0 0.9.0 1.5.0 4.6.3 2.21.0 1.0.4 2.3 3.4.3 0.10.11 ! ls sample_data ! ls sample_data anscombe.json mnist_test.csv california_housing_test.csv mnist_train_small.csv california_housing_train.csv README.md import json json . loads ( open ( 'sample_data/anscombe.json' ) . read ()) [{'Series': 'I', 'X': 10.0, 'Y': 8.04}, {'Series': 'I', 'X': 8.0, 'Y': 6.95}, {'Series': 'I', 'X': 13.0, 'Y': 7.58}, {'Series': 'I', 'X': 9.0, 'Y': 8.81}, {'Series': 'I', 'X': 11.0, 'Y': 8.33}, {'Series': 'I', 'X': 14.0, 'Y': 9.96}, {'Series': 'I', 'X': 6.0, 'Y': 7.24}, {'Series': 'I', 'X': 4.0, 'Y': 4.26}, {'Series': 'I', 'X': 12.0, 'Y': 10.84}, {'Series': 'I', 'X': 7.0, 'Y': 4.81}, {'Series': 'I', 'X': 5.0, 'Y': 5.68}, {'Series': 'II', 'X': 10.0, 'Y': 9.14}, {'Series': 'II', 'X': 8.0, 'Y': 8.14}, {'Series': 'II', 'X': 13.0, 'Y': 8.74}, {'Series': 'II', 'X': 9.0, 'Y': 8.77}, {'Series': 'II', 'X': 11.0, 'Y': 9.26}, {'Series': 'II', 'X': 14.0, 'Y': 8.1}, {'Series': 'II', 'X': 6.0, 'Y': 6.13}, {'Series': 'II', 'X': 4.0, 'Y': 3.1}, {'Series': 'II', 'X': 12.0, 'Y': 9.13}, {'Series': 'II', 'X': 7.0, 'Y': 7.26}, {'Series': 'II', 'X': 5.0, 'Y': 4.74}, {'Series': 'III', 'X': 10.0, 'Y': 7.46}, {'Series': 'III', 'X': 8.0, 'Y': 6.77}, {'Series': 'III', 'X': 13.0, 'Y': 12.74}, {'Series': 'III', 'X': 9.0, 'Y': 7.11}, {'Series': 'III', 'X': 11.0, 'Y': 7.81}, {'Series': 'III', 'X': 14.0, 'Y': 8.84}, {'Series': 'III', 'X': 6.0, 'Y': 6.08}, {'Series': 'III', 'X': 4.0, 'Y': 5.39}, {'Series': 'III', 'X': 12.0, 'Y': 8.15}, {'Series': 'III', 'X': 7.0, 'Y': 6.42}, {'Series': 'III', 'X': 5.0, 'Y': 5.73}, {'Series': 'IV', 'X': 8.0, 'Y': 6.58}, {'Series': 'IV', 'X': 8.0, 'Y': 5.76}, {'Series': 'IV', 'X': 8.0, 'Y': 7.71}, {'Series': 'IV', 'X': 8.0, 'Y': 8.84}, {'Series': 'IV', 'X': 8.0, 'Y': 8.47}, {'Series': 'IV', 'X': 8.0, 'Y': 7.04}, {'Series': 'IV', 'X': 8.0, 'Y': 5.25}, {'Series': 'IV', 'X': 19.0, 'Y': 12.5}, {'Series': 'IV', 'X': 8.0, 'Y': 5.56}, {'Series': 'IV', 'X': 8.0, 'Y': 7.91}, {'Series': 'IV', 'X': 8.0, 'Y': 6.89}] a = 5 print ( a ) NameError: ignored Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"This is my title"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Installing_Tensorflow/","text":"================ by Jawad Haider # What's already installed? # import tensorflow as tf # print(tf.__version__) 1.14.0 # Install TensorFlow 2.0 # You can run regular shell commands by prepending ! # !pip install -q tensorflow==2.0.0-beta1 # GPU version # !pip install -q tensorflow-gpu==2.0.0-beta1 ##### UPDATE 2020 ##### # new feature of colab - you can just use this try : % tensorflow_version 2. x # Colab only. except Exception : pass `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. # Check Tensorflow version again import tensorflow as tf print ( tf . __version__ ) 2.2.0-rc2 # How to install a library permanently? # https://stackoverflow.com/questions/55253498/how-do-i-install-a-library-permanently-in-colab # More fun with ! ! ls sample_data # More fun with ! # Nice! Looks like we already have some useful data to work with ! ls sample_data anscombe.json mnist_test.csv california_housing_test.csv mnist_train_small.csv california_housing_train.csv README.md Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ s","title":"TF2 0 Installing Tensorflow"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/","text":"================ by Jawad Haider Part 1: Using wget Part 2: Using tf.keras Part 3: Upload the file yourself Part 4: Access files from Google Drive Part 1: Using wget \u00b6 # download the data from a URL # source: https://archive.ics.uci.edu/ml/datasets/Arrhythmia # alternate URL: https://lazyprogrammer.me/course_files/arrhythmia.data #!wget --no-check-certificate https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data ! wget https : // lazyprogrammer . me / course_files / arrhythmia . data --2020-04-26 07:33:37-- https://lazyprogrammer.me/course_files/arrhythmia.data Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.31.80.48, 104.31.81.48, 2606:4700:3035::681f:5130, ... Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.31.80.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 402355 (393K) Saving to: \u2018arrhythmia.data\u2019 arrhythmia.data 100%[===================>] 392.92K 1.09MB/s in 0.4s 2020-04-26 07:33:38 (1.09 MB/s) - \u2018arrhythmia.data\u2019 saved [402355/402355] # list files in current directory ! ls arrhythmia.data sample_data # check if the data has a header ! head arrhythmia . data 75,0,190,80,91,193,371,174,121,-16,13,64,-2,?,63,0,52,44,0,0,32,0,0,0,0,0,0,0,44,20,36,0,28,0,0,0,0,0,0,52,40,0,0,0,60,0,0,0,0,0,0,52,0,0,0,0,0,0,0,0,0,0,0,0,56,36,0,0,32,0,0,0,0,0,0,48,32,0,0,0,56,0,0,0,0,0,0,80,0,0,0,0,0,0,0,0,0,0,0,0,40,52,0,0,28,0,0,0,0,0,0,0,48,48,0,0,32,0,0,0,0,0,0,0,52,52,0,0,36,0,0,0,0,0,0,0,52,48,0,0,32,0,0,0,0,0,0,0,56,44,0,0,32,0,0,0,0,0,0,-0.2,0.0,6.1,-1.0,0.0,0.0,0.6,2.1,13.6,30.8,0.0,0.0,1.7,-1.0,0.6,0.0,1.3,1.5,3.7,14.5,0.1,-5.2,1.4,0.0,0.0,0.0,0.8,-0.6,-10.7,-15.6,0.4,-3.9,0.0,0.0,0.0,0.0,-0.8,-1.7,-10.1,-22.0,0.0,0.0,5.7,-1.0,0.0,0.0,-0.1,1.2,14.1,22.5,0.0,-2.5,0.8,0.0,0.0,0.0,1.0,0.4,-4.8,-2.7,0.1,-6.0,0.0,0.0,0.0,0.0,-0.8,-0.6,-24.0,-29.7,0.0,0.0,2.0,-6.4,0.0,0.0,0.2,2.9,-12.6,15.2,-0.1,0.0,8.4,-10.0,0.0,0.0,0.6,5.9,-3.9,52.7,-0.3,0.0,15.2,-8.4,0.0,0.0,0.9,5.1,17.7,70.7,-0.4,0.0,13.5,-4.0,0.0,0.0,0.9,3.9,25.5,62.9,-0.3,0.0,9.0,-0.9,0.0,0.0,0.9,2.9,23.3,49.4,8 56,1,165,64,81,174,401,149,39,25,37,-17,31,?,53,0,48,0,0,0,24,0,0,0,0,0,0,0,64,0,0,0,24,0,0,0,0,0,0,32,24,0,0,0,40,0,0,0,0,0,0,48,0,0,0,0,0,0,0,0,0,0,0,0,44,20,0,0,24,0,0,0,0,0,0,0,60,0,0,0,20,0,0,0,0,0,0,0,24,52,0,0,16,0,0,0,0,0,0,0,32,52,0,0,20,0,0,0,0,0,0,0,44,48,0,0,32,0,0,0,0,0,0,0,48,44,0,0,32,0,0,0,0,0,0,0,48,40,0,0,28,0,0,0,0,0,0,0,48,0,0,0,28,0,0,0,0,0,0,-0.6,0.0,7.2,0.0,0.0,0.0,0.4,1.5,17.2,26.5,0.0,0.0,5.5,0.0,0.0,0.0,0.1,1.7,17.6,29.5,0.3,-1.6,0.9,0.0,0.0,0.0,-0.3,0.4,-1.5,1.3,0.1,-6.4,0.0,0.0,0.0,0.0,-0.3,-1.6,-15.3,-25.5,-0.3,0.0,4.2,-0.9,0.0,0.0,0.4,0.7,8.3,12.3,0.2,0.0,2.2,0.0,0.0,0.0,-0.2,0.8,6.6,11.7,0.4,0.0,1.0,-8.8,0.0,0.0,0.5,-0.6,-21.6,-26.8,0.4,0.0,2.6,-7.9,0.0,0.0,0.8,2.0,-16.4,1.2,0.0,0.0,5.8,-7.7,0.0,0.0,0.9,3.8,-5.7,27.7,-0.2,0.0,9.5,-5.0,0.0,0.0,0.5,2.6,11.8,34.6,-0.4,0.0,11.0,-2.4,0.0,0.0,0.4,2.6,21.6,43.4,-0.5,0.0,8.5,0.0,0.0,0.0,0.2,2.1,20.4,38.8,6 54,0,172,95,138,163,386,185,102,96,34,70,66,23,75,0,40,80,0,0,24,0,0,0,0,0,0,20,56,52,0,0,40,0,0,0,0,0,0,28,116,0,0,0,52,0,0,0,0,0,0,52,64,0,0,0,88,0,0,0,0,0,0,0,36,92,0,0,24,0,0,0,0,0,0,0,128,0,0,0,24,0,1,0,0,0,0,0,24,36,76,0,100,0,0,0,0,0,0,0,40,28,60,0,96,0,0,0,0,0,0,0,48,20,56,24,32,0,0,0,0,0,0,0,44,88,0,0,28,0,0,0,0,0,0,0,44,76,0,0,28,0,0,0,0,0,0,0,44,72,0,0,24,0,0,0,0,0,0,1.0,0.0,4.5,-2.8,0.0,0.0,0.3,2.5,-2.2,19.8,0.8,-0.4,6.4,-1.3,0.0,0.0,0.7,2.7,14.2,37.9,-0.2,-0.6,4.4,0.0,0.0,0.0,0.5,0.2,24.7,26.2,-1.0,-5.3,1.8,0.0,0.0,0.0,-0.5,-2.5,-8.0,-28.5,0.5,0.0,1.7,-2.7,0.0,0.0,-0.2,1.0,-9.4,-1.2,0.4,0.0,4.9,0.0,0.0,0.0,0.6,1.4,31.3,42.7,-0.8,0.0,0.7,-3.8,6.5,0.0,0.3,-3.3,18.7,-13.6,-0.9,0.0,2.2,-4.1,7.4,0.0,0.5,-2.4,20.9,-2.6,0.0,0.0,5.8,-4.1,4.0,-0.5,0.4,0.3,20.4,23.3,0.7,0.0,10.0,-5.7,0.0,0.0,0.5,2.2,-3.0,20.7,1.3,0.0,11.1,-3.4,0.0,0.0,0.4,3.4,11.5,48.2,0.9,0.0,9.5,-2.4,0.0,0.0,0.3,3.4,12.3,49.0,10 55,0,175,94,100,202,380,179,143,28,11,-5,20,?,71,0,72,20,0,0,48,0,0,0,0,0,0,0,64,36,0,0,36,0,0,0,0,0,0,20,52,48,0,0,56,0,0,0,0,0,0,64,32,0,0,0,72,0,0,0,0,0,0,0,60,12,0,0,44,0,0,0,0,0,0,0,60,44,0,0,32,0,0,0,0,0,0,56,0,0,0,0,0,0,0,0,0,0,0,0,40,44,0,0,20,0,0,0,0,0,0,0,52,40,0,0,32,0,0,0,0,0,0,0,56,48,0,0,36,0,0,0,0,0,0,0,60,48,0,0,36,0,0,0,0,0,0,0,64,40,0,0,40,0,0,0,0,0,0,0.9,0.0,7.8,-0.7,0.0,0.0,1.1,1.9,27.3,45.1,0.1,0.0,9.1,-2.6,0.0,0.0,0.4,1.5,24.5,36.8,-0.4,-0.4,1.6,-2.2,0.0,0.0,-1.0,-0.9,-1.5,-9.2,-0.4,-8.2,1.8,0.0,0.0,0.0,-0.7,-1.7,-23.4,-35.6,0.9,0.0,3.2,-0.4,0.0,0.0,0.7,1.2,9.4,18.0,-0.1,0.0,5.1,-2.5,0.0,0.0,0.3,0.6,9.8,12.6,1.6,-6.5,0.0,0.0,0.0,0.0,-0.4,-0.4,-18.2,-22.4,2.1,0.0,1.2,-6.9,0.0,0.0,-0.5,2.9,-12.7,18.0,0.7,0.0,9.0,-7.9,0.0,0.0,0.1,4.1,7.6,51.0,0.4,0.0,15.0,-5.5,0.0,0.0,0.1,3.3,28.8,63.1,0.1,0.0,15.2,-3.7,0.0,0.0,0.6,3.0,36.8,68.0,0.1,0.0,12.2,-2.2,0.0,0.0,0.4,2.6,34.6,61.6,1 75,0,190,80,88,181,360,177,103,-16,13,61,3,?,?,0,48,40,0,0,28,0,0,0,0,0,0,0,40,24,0,0,24,0,0,0,0,0,0,52,36,0,0,0,60,0,0,0,0,0,0,48,28,0,0,0,56,0,0,0,0,0,0,0,48,36,0,0,28,0,0,0,0,0,0,44,0,0,0,0,0,0,0,0,0,0,0,88,0,0,0,0,0,0,0,0,0,0,0,0,40,52,0,0,28,0,0,0,0,0,0,0,48,48,0,0,32,0,0,0,0,0,0,0,48,52,0,0,32,0,0,0,0,0,0,0,52,44,0,0,28,0,0,0,0,0,0,0,52,48,0,0,32,0,0,0,0,0,0,0.0,0.0,5.2,-1.4,0.0,0.0,0.9,2.3,9.6,31.6,0.1,0.0,1.6,-0.5,0.0,0.0,1.9,1.7,2.6,18.9,0.2,-3.8,1.2,0.0,0.0,0.0,1.0,-0.6,-7.7,-13.4,-0.1,-3.4,0.8,0.0,0.0,0.0,-1.4,-1.5,-7.0,-17.8,-0.1,0.0,4.4,-1.3,0.0,0.0,-0.1,1.1,8.2,16.5,0.6,-1.6,0.0,0.0,0.0,0.0,1.4,0.3,-3.5,-1.9,0.0,-5.7,0.0,0.0,0.0,0.0,-0.4,-0.5,-25.0,-30.0,-0.2,0.0,1.6,-6.0,0.0,0.0,-0.7,2.1,-12.4,8.6,-0.5,0.0,8.5,-10.2,0.0,0.0,-1.0,4.7,-4.0,43.0,-0.2,0.0,15.2,-7.8,0.0,0.0,-0.1,4.9,16.2,63.2,-0.2,0.0,9.1,-0.9,0.0,0.0,-0.2,2.9,21.7,48.9,-0.4,0.0,13.1,-3.6,0.0,0.0,-0.1,3.9,25.4,62.8,7 13,0,169,51,100,167,321,174,91,107,66,52,88,?,84,0,36,48,0,0,20,0,0,0,0,0,0,20,44,36,0,0,44,0,0,0,0,0,0,24,64,0,0,0,48,0,0,0,0,0,0,44,36,0,0,0,52,0,0,0,0,0,0,0,28,64,0,0,16,0,0,0,0,0,0,24,44,40,0,0,44,0,0,0,0,0,0,0,36,60,0,0,24,0,0,0,0,0,0,20,32,60,0,0,40,0,0,0,0,0,0,24,32,60,0,0,44,0,0,0,0,0,0,0,52,40,0,0,36,0,0,0,0,0,0,0,44,40,0,0,32,0,0,0,0,0,0,20,36,56,0,0,40,0,0,0,0,0,0,0.5,0.0,2.7,-6.4,0.0,0.0,0.9,1.7,-10.5,7.1,0.1,-1.2,19.1,-2.3,0.0,0.0,1.4,4.3,36.7,84.8,-0.4,-2.3,21.7,0.0,0.0,0.0,0.7,2.6,66.7,95.8,-0.2,-9.0,3.2,0.0,0.0,0.0,-1.1,-2.9,-14.1,-39.0,0.5,0.0,1.8,-12.9,0.0,0.0,0.4,-0.4,-38.7,-42.1,-0.1,-1.6,19.9,-0.7,0.0,0.0,1.0,3.3,40.4,65.4,0.4,0.0,6.7,-24.4,0.0,0.0,-1.2,0.4,-61.2,-59.9,0.9,-0.5,11.9,-43.3,0.0,0.0,0.8,3.4,-111.4,-95.1,2.0,-0.8,19.8,-48.4,0.0,0.0,1.6,8.7,-114.5,-72.8,2.0,0.0,31.0,-25.7,0.0,0.0,0.8,5.9,29.2,85.8,0.6,0.0,19.5,-11.4,0.0,0.0,0.8,3.3,20.1,49.1,0.0,-0.6,12.2,-2.8,0.0,0.0,0.9,2.2,13.5,31.1,14 40,1,160,52,77,129,377,133,77,77,49,75,65,?,70,0,44,0,0,0,24,0,0,0,0,0,0,0,40,32,0,0,24,0,0,0,0,0,0,0,44,28,0,0,24,0,0,0,0,0,0,44,16,0,0,0,48,0,0,0,0,0,0,36,0,0,0,0,0,0,0,0,0,0,0,0,44,16,0,0,24,0,0,0,0,0,0,0,20,56,0,0,12,0,0,0,0,0,0,0,24,56,0,0,16,0,0,0,0,0,0,0,36,48,0,0,24,0,0,0,0,0,0,0,40,44,0,0,28,0,0,0,0,0,0,0,40,44,0,0,24,0,0,0,0,0,0,0,44,0,0,0,24,0,0,0,0,0,0,-0.5,0.0,1.8,0.0,0.0,0.0,0.2,1.0,3.9,10.5,-0.1,0.0,7.6,-1.1,0.0,0.0,0.5,1.4,13.5,22.7,0.0,0.0,5.9,-0.5,0.0,0.0,0.3,0.6,12.2,15.0,0.1,-4.6,0.6,0.0,0.0,0.0,-0.4,-0.9,-9.7,-14.7,0.2,-2.1,0.0,0.0,0.0,0.0,-0.3,0.4,-3.7,-1.4,-0.2,0.0,6.8,-0.9,0.0,0.0,0.7,0.7,14.2,17.1,1.3,0.0,1.3,-11.5,0.0,0.0,-0.3,1.7,-30.9,-13.9,1.7,0.0,2.3,-17.5,0.0,0.0,-0.6,4.5,-46.3,-1.3,1.1,0.0,3.7,-11.0,0.0,0.0,-0.5,4.1,-19.8,21.2,0.1,0.0,7.7,-6.4,0.0,0.0,0.4,1.9,1.4,15.4,0.0,0.0,7.4,-2.5,0.0,0.0,0.4,1.3,9.3,18.9,-0.4,0.0,6.5,0.0,0.0,0.0,0.4,1.0,14.3,20.5,1 49,1,162,54,78,0,376,157,70,67,7,8,51,?,67,0,44,36,0,0,24,0,0,0,0,0,0,0,52,32,0,0,28,0,0,0,0,0,0,0,56,28,0,0,24,0,0,0,0,0,0,48,32,0,0,0,56,0,0,0,0,0,0,52,0,0,0,0,0,0,0,0,0,0,0,0,52,28,0,0,28,0,0,0,0,0,0,0,20,44,0,0,8,0,0,0,0,0,0,0,24,48,0,0,16,0,0,0,0,0,0,0,36,44,0,0,24,0,0,0,0,0,0,0,44,48,0,0,28,0,0,0,0,0,0,0,48,44,0,0,28,0,0,0,0,0,0,0,48,40,0,0,24,0,0,0,0,0,0,-0.3,0.0,4.1,-1.1,0.0,0.0,0.8,1.0,7.1,13.7,-0.3,0.0,8.4,-1.5,0.0,0.0,0.6,0.7,19.4,22.9,0.0,0.0,4.4,-0.8,0.0,0.0,-0.3,-0.6,11.2,6.9,0.1,-6.3,1.3,0.0,0.0,0.0,-0.6,-0.8,-13.1,-17.9,0.1,-0.8,0.0,0.0,0.0,0.0,0.6,0.7,-2.0,2.9,-0.2,0.0,6.3,-1.2,0.0,0.0,0.2,0.3,14.7,16.8,0.7,0.0,0.5,-7.3,0.0,0.0,0.2,-0.1,-15.5,-16.4,0.9,0.0,0.7,-8.9,0.0,0.0,0.6,2.5,-20.5,4.0,0.8,0.0,2.1,-9.0,0.0,0.0,0.6,3.8,-16.1,21.1,0.1,0.0,6.6,-4.1,0.0,0.0,0.3,1.4,4.7,14.2,-0.2,0.0,8.5,-2.7,0.0,0.0,0.1,0.8,14.5,20.9,-0.3,0.0,8.2,-1.9,0.0,0.0,0.1,0.5,15.8,19.8,1 44,0,168,56,84,118,354,160,63,61,69,78,66,84,64,0,40,0,0,0,20,0,0,0,0,0,0,0,44,12,0,0,28,0,0,0,0,0,0,0,36,8,0,0,20,0,0,0,0,0,0,40,12,0,0,0,44,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,36,12,0,0,20,0,0,0,0,0,0,0,20,56,0,0,12,0,0,0,0,0,0,0,24,48,0,0,12,0,0,0,0,0,0,0,28,44,0,0,16,0,0,0,0,0,0,0,44,32,0,0,32,0,0,0,0,0,0,0,44,28,0,0,28,0,0,0,0,0,0,0,40,24,0,0,24,0,0,0,0,0,0,0.1,0.0,2.3,0.0,0.0,0.0,0.4,1.0,4.6,11.6,1.2,0.0,5.4,-0.7,0.0,0.0,1.8,2.8,11.4,31.0,1.1,0.0,3.0,-0.4,0.0,0.0,1.4,1.8,5.3,17.9,-0.7,-3.9,0.5,0.0,0.0,0.0,-1.1,-1.9,-7.5,-20.4,-0.5,0.0,0.0,0.0,0.0,0.0,-0.6,-0.5,0.0,-3.4,1.1,0.0,4.2,-0.5,0.0,0.0,1.6,2.3,7.2,22.8,0.5,0.0,0.9,-5.5,0.0,0.0,-0.7,1.0,-14.5,-5.3,0.7,0.0,1.2,-6.4,0.0,0.0,-0.5,2.6,-13.9,10.0,1.5,0.0,2.4,-10.3,0.0,0.0,0.3,6.8,-19.3,43.2,0.8,0.0,7.9,-7.3,0.0,0.0,0.9,6.5,5.7,62.9,0.1,0.0,9.3,-3.8,0.0,0.0,0.8,3.8,15.1,48.5,0.1,0.0,7.0,-1.3,0.0,0.0,0.6,2.1,12.5,30.9,1 50,1,167,67,89,130,383,156,73,85,34,70,71,?,63,0,44,40,0,0,28,0,0,0,0,0,0,0,56,24,0,0,32,0,0,0,0,0,0,0,72,0,0,0,28,0,0,0,0,0,0,56,28,0,0,0,60,0,0,0,0,0,0,0,28,56,0,0,16,0,0,0,0,0,0,0,60,0,0,0,32,0,0,0,0,0,0,0,24,36,32,0,68,0,0,0,0,0,0,0,36,44,0,0,20,0,0,0,0,0,0,0,40,48,0,0,24,0,0,0,0,0,0,0,56,40,0,0,40,0,0,0,0,0,0,0,52,36,0,0,32,0,0,0,0,0,0,20,44,36,0,0,44,0,0,0,0,0,0,-0.1,0.0,3.5,-2.0,0.0,0.0,0.4,1.3,3.7,13.5,0.0,0.0,9.9,-0.8,0.0,0.0,1.2,1.2,26.8,35.2,0.0,0.0,8.3,0.0,0.0,0.0,0.8,0.3,29.8,32.0,0.1,-6.1,1.1,0.0,0.0,0.0,-0.6,-1.2,-15.5,-24.1,0.0,0.0,0.6,-4.1,0.0,0.0,-0.1,0.8,-10.6,-4.9,-0.2,0.0,8.9,0.0,0.0,0.0,0.8,0.7,26.7,30.2,0.1,0.0,1.3,-5.4,1.9,0.0,0.2,0.8,-5.2,2.1,0.8,0.0,4.4,-8.5,0.0,0.0,0.8,3.9,-10.8,25.0,0.4,0.0,4.3,-7.3,0.0,0.0,1.1,4.0,-8.9,27.9,-0.5,0.0,7.0,-3.2,0.0,0.0,1.1,1.3,13.2,22.3,-0.5,0.0,10.9,-2.5,0.0,0.0,1.0,1.0,23.8,29.6,-0.5,-0.6,10.8,-1.7,0.0,0.0,0.8,0.9,20.1,25.1,10 # check the data import pandas as pd df = pd . read_csv ( 'arrhythmia.data' , header = None ) # since the data has many columns, take just the first few and name them (as per the documentation) data = df [[ 0 , 1 , 2 , 3 , 4 , 5 ]] data . columns = [ 'age' , 'sex' , 'height' , 'weight' , 'QRS duration' , 'P-R interval' ] import matplotlib.pyplot as plt plt . rcParams [ 'figure.figsize' ] = [ 15 , 15 ] # make the plot bigger so the subplots don't overlap data . hist (); # use a semicolon to supress return value from pandas.plotting import scatter_matrix scatter_matrix ( data ); Part 2: Using tf.keras \u00b6 # use keras get_file to download the auto MPG dataset # source: https://archive.ics.uci.edu/ml/datasets/Auto+MPG #url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data' ### alternate URL url = 'https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/auto-mpg.data' # Install TensorFlow import tensorflow as tf print ( tf . __version__ ) 2.2.0-rc2 # check out the documentation for other arguments tf . keras . utils . get_file ( 'auto-mpg.data' , url ) Downloading data from https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/auto-mpg.data 32768/30286 [================================] - 0s 1us/step '/root/.keras/datasets/auto-mpg.data' ! head / root /. keras / datasets / auto - mpg . data 18.0 8 307.0 130.0 3504. 12.0 70 1 \"chevrolet chevelle malibu\" 15.0 8 350.0 165.0 3693. 11.5 70 1 \"buick skylark 320\" 18.0 8 318.0 150.0 3436. 11.0 70 1 \"plymouth satellite\" 16.0 8 304.0 150.0 3433. 12.0 70 1 \"amc rebel sst\" 17.0 8 302.0 140.0 3449. 10.5 70 1 \"ford torino\" 15.0 8 429.0 198.0 4341. 10.0 70 1 \"ford galaxie 500\" 14.0 8 454.0 220.0 4354. 9.0 70 1 \"chevrolet impala\" 14.0 8 440.0 215.0 4312. 8.5 70 1 \"plymouth fury iii\" 14.0 8 455.0 225.0 4425. 10.0 70 1 \"pontiac catalina\" 15.0 8 390.0 190.0 3850. 8.5 70 1 \"amc ambassador dpl\" # unless you specify an alternative path, the data will go into /root/.keras/datasets/ df = pd . read_csv ( '/root/.keras/datasets/auto-mpg.data' , header = None , delim_whitespace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 0 18.0 8 307.0 130.0 3504.0 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165.0 3693.0 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150.0 3436.0 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150.0 3433.0 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140.0 3449.0 10.5 70 1 ford torino Part 3: Upload the file yourself \u00b6 # another method: upload your own file ##### PLEASE NOTE: IT DOES NOT MATTER WHICH FILE YOU UPLOAD ##### YOU CAN UPLOAD ANY FILE YOU WANT ##### IN FACT, YOU ARE ENCOURAGED TO EXPLORE ON YOUR OWN # if you must, then get the file from here: # https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/daily-minimum-temperatures-in-me.csv from google.colab import files uploaded = files . upload () <input type=\"file\" id=\"files-f85a97ca-0bfe-4c55-8369-d830289c8925\" name=\"files[]\" multiple disabled /> <output id=\"result-f85a97ca-0bfe-4c55-8369-d830289c8925\"> Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. </output> <script src=\"/nbextensions/google.colab/files.js\"></script> Saving daily-minimum-temperatures-in-me.csv to daily-minimum-temperatures-in-me.csv uploaded {'daily-minimum-temperatures-in-me.csv': b'\"Date\",\"Daily minimum temperatures in Melbourne, Australia, 1981-1990\"\\r\\n\"1981-01-01\",20.7\\r\\n\"1981-01-02\",17.9\\r\\n\"1981-01-03\",18.8\\r\\n\"1981-01-04\",14.6\\r\\n\"1981-01-05\",15.8\\r\\n\"1981-01-06\",15.8\\r\\n\"1981-01-07\",15.8\\r\\n\"1981-01-08\",17.4\\r\\n\"1981-01-09\",21.8\\r\\n\"1981-01-10\",20.0\\r\\n\"1981-01-11\",16.2\\r\\n\"1981-01-12\",13.3\\r\\n\"1981-01-13\",16.7\\r\\n\"1981-01-14\",21.5\\r\\n\"1981-01-15\",25.0\\r\\n\"1981-01-16\",20.7\\r\\n\"1981-01-17\",20.6\\r\\n\"1981-01-18\",24.8\\r\\n\"1981-01-19\",17.7\\r\\n\"1981-01-20\",15.5\\r\\n\"1981-01-21\",18.2\\r\\n\"1981-01-22\",12.1\\r\\n\"1981-01-23\",14.4\\r\\n\"1981-01-24\",16.0\\r\\n\"1981-01-25\",16.5\\r\\n\"1981-01-26\",18.7\\r\\n\"1981-01-27\",19.4\\r\\n\"1981-01-28\",17.2\\r\\n\"1981-01-29\",15.5\\r\\n\"1981-01-30\",15.1\\r\\n\"1981-01-31\",15.4\\r\\n\"1981-02-01\",15.3\\r\\n\"1981-02-02\",18.8\\r\\n\"1981-02-03\",21.9\\r\\n\"1981-02-04\",19.9\\r\\n\"1981-02-05\",16.6\\r\\n\"1981-02-06\",16.8\\r\\n\"1981-02-07\",14.6\\r\\n\"1981-02-08\",17.1\\r\\n\"1981-02-09\",25.0\\r\\n\"1981-02-10\",15.0\\r\\n\"1981-02-11\",13.7\\r\\n\"1981-02-12\",13.9\\r\\n\"1981-02-13\",18.3\\r\\n\"1981-02-14\",22.0\\r\\n\"1981-02-15\",22.1\\r\\n\"1981-02-16\",21.2\\r\\n\"1981-02-17\",18.4\\r\\n\"1981-02-18\",16.6\\r\\n\"1981-02-19\",16.1\\r\\n\"1981-02-20\",15.7\\r\\n\"1981-02-21\",16.6\\r\\n\"1981-02-22\",16.5\\r\\n\"1981-02-23\",14.4\\r\\n\"1981-02-24\",14.4\\r\\n\"1981-02-25\",18.5\\r\\n\"1981-02-26\",16.9\\r\\n\"1981-02-27\",17.5\\r\\n\"1981-02-28\",21.2\\r\\n\"1981-03-01\",17.8\\r\\n\"1981-03-02\",18.6\\r\\n\"1981-03-03\",17.0\\r\\n\"1981-03-04\",16.0\\r\\n\"1981-03-05\",13.3\\r\\n\"1981-03-06\",14.3\\r\\n\"1981-03-07\",11.4\\r\\n\"1981-03-08\",16.3\\r\\n\"1981-03-09\",16.1\\r\\n\"1981-03-10\",11.8\\r\\n\"1981-03-11\",12.2\\r\\n\"1981-03-12\",14.7\\r\\n\"1981-03-13\",11.8\\r\\n\"1981-03-14\",11.3\\r\\n\"1981-03-15\",10.6\\r\\n\"1981-03-16\",11.7\\r\\n\"1981-03-17\",14.2\\r\\n\"1981-03-18\",11.2\\r\\n\"1981-03-19\",16.9\\r\\n\"1981-03-20\",16.7\\r\\n\"1981-03-21\",8.1\\r\\n\"1981-03-22\",8.0\\r\\n\"1981-03-23\",8.8\\r\\n\"1981-03-24\",13.4\\r\\n\"1981-03-25\",10.9\\r\\n\"1981-03-26\",13.4\\r\\n\"1981-03-27\",11.0\\r\\n\"1981-03-28\",15.0\\r\\n\"1981-03-29\",15.7\\r\\n\"1981-03-30\",14.5\\r\\n\"1981-03-31\",15.8\\r\\n\"1981-04-01\",16.7\\r\\n\"1981-04-02\",16.8\\r\\n\"1981-04-03\",17.5\\r\\n\"1981-04-04\",17.1\\r\\n\"1981-04-05\",18.1\\r\\n\"1981-04-06\",16.6\\r\\n\"1981-04-07\",10.0\\r\\n\"1981-04-08\",14.9\\r\\n\"1981-04-09\",15.9\\r\\n\"1981-04-10\",13.0\\r\\n\"1981-04-11\",7.6\\r\\n\"1981-04-12\",11.5\\r\\n\"1981-04-13\",13.5\\r\\n\"1981-04-14\",13.0\\r\\n\"1981-04-15\",13.3\\r\\n\"1981-04-16\",12.1\\r\\n\"1981-04-17\",12.4\\r\\n\"1981-04-18\",13.2\\r\\n\"1981-04-19\",13.8\\r\\n\"1981-04-20\",10.6\\r\\n\"1981-04-21\",9.0\\r\\n\"1981-04-22\",10.0\\r\\n\"1981-04-23\",9.8\\r\\n\"1981-04-24\",11.5\\r\\n\"1981-04-25\",8.9\\r\\n\"1981-04-26\",7.4\\r\\n\"1981-04-27\",9.9\\r\\n\"1981-04-28\",9.3\\r\\n\"1981-04-29\",9.9\\r\\n\"1981-04-30\",7.4\\r\\n\"1981-05-01\",8.6\\r\\n\"1981-05-02\",11.9\\r\\n\"1981-05-03\",14.0\\r\\n\"1981-05-04\",8.6\\r\\n\"1981-05-05\",10.0\\r\\n\"1981-05-06\",13.5\\r\\n\"1981-05-07\",12.0\\r\\n\"1981-05-08\",10.5\\r\\n\"1981-05-09\",10.7\\r\\n\"1981-05-10\",8.1\\r\\n\"1981-05-11\",10.1\\r\\n\"1981-05-12\",10.6\\r\\n\"1981-05-13\",5.3\\r\\n\"1981-05-14\",6.6\\r\\n\"1981-05-15\",8.5\\r\\n\"1981-05-16\",11.2\\r\\n\"1981-05-17\",9.8\\r\\n\"1981-05-18\",5.9\\r\\n\"1981-05-19\",3.2\\r\\n\"1981-05-20\",2.1\\r\\n\"1981-05-21\",3.4\\r\\n\"1981-05-22\",5.4\\r\\n\"1981-05-23\",9.6\\r\\n\"1981-05-24\",11.5\\r\\n\"1981-05-25\",12.3\\r\\n\"1981-05-26\",12.6\\r\\n\"1981-05-27\",11.0\\r\\n\"1981-05-28\",11.2\\r\\n\"1981-05-29\",11.4\\r\\n\"1981-05-30\",11.8\\r\\n\"1981-05-31\",12.8\\r\\n\"1981-06-01\",11.6\\r\\n\"1981-06-02\",10.6\\r\\n\"1981-06-03\",9.8\\r\\n\"1981-06-04\",11.2\\r\\n\"1981-06-05\",5.7\\r\\n\"1981-06-06\",7.1\\r\\n\"1981-06-07\",2.5\\r\\n\"1981-06-08\",3.5\\r\\n\"1981-06-09\",4.6\\r\\n\"1981-06-10\",11.0\\r\\n\"1981-06-11\",5.7\\r\\n\"1981-06-12\",7.7\\r\\n\"1981-06-13\",10.4\\r\\n\"1981-06-14\",11.4\\r\\n\"1981-06-15\",9.2\\r\\n\"1981-06-16\",6.1\\r\\n\"1981-06-17\",2.7\\r\\n\"1981-06-18\",4.3\\r\\n\"1981-06-19\",6.3\\r\\n\"1981-06-20\",3.8\\r\\n\"1981-06-21\",4.4\\r\\n\"1981-06-22\",7.1\\r\\n\"1981-06-23\",4.8\\r\\n\"1981-06-24\",5.8\\r\\n\"1981-06-25\",6.2\\r\\n\"1981-06-26\",7.3\\r\\n\"1981-06-27\",9.2\\r\\n\"1981-06-28\",10.2\\r\\n\"1981-06-29\",9.5\\r\\n\"1981-06-30\",9.5\\r\\n\"1981-07-01\",10.7\\r\\n\"1981-07-02\",10.0\\r\\n\"1981-07-03\",6.5\\r\\n\"1981-07-04\",7.0\\r\\n\"1981-07-05\",7.4\\r\\n\"1981-07-06\",8.1\\r\\n\"1981-07-07\",6.6\\r\\n\"1981-07-08\",8.3\\r\\n\"1981-07-09\",8.9\\r\\n\"1981-07-10\",4.6\\r\\n\"1981-07-11\",6.8\\r\\n\"1981-07-12\",5.7\\r\\n\"1981-07-13\",6.1\\r\\n\"1981-07-14\",7.0\\r\\n\"1981-07-15\",7.2\\r\\n\"1981-07-16\",6.3\\r\\n\"1981-07-17\",8.8\\r\\n\"1981-07-18\",5.0\\r\\n\"1981-07-19\",7.4\\r\\n\"1981-07-20\",10.1\\r\\n\"1981-07-21\",12.0\\r\\n\"1981-07-22\",9.0\\r\\n\"1981-07-23\",8.9\\r\\n\"1981-07-24\",9.8\\r\\n\"1981-07-25\",9.0\\r\\n\"1981-07-26\",9.2\\r\\n\"1981-07-27\",7.7\\r\\n\"1981-07-28\",8.0\\r\\n\"1981-07-29\",6.1\\r\\n\"1981-07-30\",3.5\\r\\n\"1981-07-31\",3.2\\r\\n\"1981-08-01\",5.7\\r\\n\"1981-08-02\",7.7\\r\\n\"1981-08-03\",9.0\\r\\n\"1981-08-04\",10.0\\r\\n\"1981-08-05\",6.2\\r\\n\"1981-08-06\",6.9\\r\\n\"1981-08-07\",6.5\\r\\n\"1981-08-08\",6.8\\r\\n\"1981-08-09\",7.0\\r\\n\"1981-08-10\",5.2\\r\\n\"1981-08-11\",3.0\\r\\n\"1981-08-12\",5.6\\r\\n\"1981-08-13\",7.9\\r\\n\"1981-08-14\",9.0\\r\\n\"1981-08-15\",8.6\\r\\n\"1981-08-16\",10.3\\r\\n\"1981-08-17\",10.5\\r\\n\"1981-08-18\",7.6\\r\\n\"1981-08-19\",9.7\\r\\n\"1981-08-20\",12.5\\r\\n\"1981-08-21\",7.4\\r\\n\"1981-08-22\",7.9\\r\\n\"1981-08-23\",3.9\\r\\n\"1981-08-24\",6.6\\r\\n\"1981-08-25\",4.6\\r\\n\"1981-08-26\",7.0\\r\\n\"1981-08-27\",6.0\\r\\n\"1981-08-28\",5.5\\r\\n\"1981-08-29\",8.1\\r\\n\"1981-08-30\",5.5\\r\\n\"1981-08-31\",6.2\\r\\n\"1981-09-01\",8.0\\r\\n\"1981-09-02\",10.3\\r\\n\"1981-09-03\",9.8\\r\\n\"1981-09-04\",9.6\\r\\n\"1981-09-05\",8.5\\r\\n\"1981-09-06\",7.5\\r\\n\"1981-09-07\",11.2\\r\\n\"1981-09-08\",14.6\\r\\n\"1981-09-09\",11.7\\r\\n\"1981-09-10\",7.8\\r\\n\"1981-09-11\",12.3\\r\\n\"1981-09-12\",10.1\\r\\n\"1981-09-13\",11.5\\r\\n\"1981-09-14\",7.3\\r\\n\"1981-09-15\",10.9\\r\\n\"1981-09-16\",14.1\\r\\n\"1981-09-17\",10.7\\r\\n\"1981-09-18\",16.9\\r\\n\"1981-09-19\",10.5\\r\\n\"1981-09-20\",6.5\\r\\n\"1981-09-21\",11.0\\r\\n\"1981-09-22\",6.3\\r\\n\"1981-09-23\",10.5\\r\\n\"1981-09-24\",7.2\\r\\n\"1981-09-25\",7.6\\r\\n\"1981-09-26\",10.7\\r\\n\"1981-09-27\",7.8\\r\\n\"1981-09-28\",9.6\\r\\n\"1981-09-29\",11.4\\r\\n\"1981-09-30\",12.4\\r\\n\"1981-10-01\",8.9\\r\\n\"1981-10-02\",13.2\\r\\n\"1981-10-03\",8.6\\r\\n\"1981-10-04\",6.2\\r\\n\"1981-10-05\",11.4\\r\\n\"1981-10-06\",13.2\\r\\n\"1981-10-07\",14.3\\r\\n\"1981-10-08\",7.3\\r\\n\"1981-10-09\",12.9\\r\\n\"1981-10-10\",7.8\\r\\n\"1981-10-11\",6.2\\r\\n\"1981-10-12\",5.6\\r\\n\"1981-10-13\",10.0\\r\\n\"1981-10-14\",13.3\\r\\n\"1981-10-15\",8.3\\r\\n\"1981-10-16\",10.2\\r\\n\"1981-10-17\",8.6\\r\\n\"1981-10-18\",7.3\\r\\n\"1981-10-19\",10.4\\r\\n\"1981-10-20\",11.2\\r\\n\"1981-10-21\",13.2\\r\\n\"1981-10-22\",11.4\\r\\n\"1981-10-23\",9.1\\r\\n\"1981-10-24\",6.6\\r\\n\"1981-10-25\",8.4\\r\\n\"1981-10-26\",9.7\\r\\n\"1981-10-27\",13.2\\r\\n\"1981-10-28\",12.5\\r\\n\"1981-10-29\",11.0\\r\\n\"1981-10-30\",11.0\\r\\n\"1981-10-31\",11.7\\r\\n\"1981-11-01\",9.2\\r\\n\"1981-11-02\",11.5\\r\\n\"1981-11-03\",13.6\\r\\n\"1981-11-04\",13.7\\r\\n\"1981-11-05\",10.4\\r\\n\"1981-11-06\",11.5\\r\\n\"1981-11-07\",7.6\\r\\n\"1981-11-08\",9.6\\r\\n\"1981-11-09\",14.2\\r\\n\"1981-11-10\",15.7\\r\\n\"1981-11-11\",10.5\\r\\n\"1981-11-12\",10.5\\r\\n\"1981-11-13\",9.7\\r\\n\"1981-11-14\",9.5\\r\\n\"1981-11-15\",11.3\\r\\n\"1981-11-16\",8.9\\r\\n\"1981-11-17\",9.4\\r\\n\"1981-11-18\",11.9\\r\\n\"1981-11-19\",11.7\\r\\n\"1981-11-20\",13.4\\r\\n\"1981-11-21\",12.6\\r\\n\"1981-11-22\",10.1\\r\\n\"1981-11-23\",15.8\\r\\n\"1981-11-24\",13.6\\r\\n\"1981-11-25\",11.9\\r\\n\"1981-11-26\",9.9\\r\\n\"1981-11-27\",12.6\\r\\n\"1981-11-28\",17.8\\r\\n\"1981-11-29\",15.0\\r\\n\"1981-11-30\",13.6\\r\\n\"1981-12-01\",13.4\\r\\n\"1981-12-02\",10.5\\r\\n\"1981-12-03\",14.2\\r\\n\"1981-12-04\",11.5\\r\\n\"1981-12-05\",13.0\\r\\n\"1981-12-06\",15.0\\r\\n\"1981-12-07\",14.7\\r\\n\"1981-12-08\",12.6\\r\\n\"1981-12-09\",12.5\\r\\n\"1981-12-10\",13.5\\r\\n\"1981-12-11\",14.8\\r\\n\"1981-12-12\",17.2\\r\\n\"1981-12-13\",9.7\\r\\n\"1981-12-14\",12.1\\r\\n\"1981-12-15\",12.8\\r\\n\"1981-12-16\",11.2\\r\\n\"1981-12-17\",16.4\\r\\n\"1981-12-18\",15.6\\r\\n\"1981-12-19\",13.3\\r\\n\"1981-12-20\",11.0\\r\\n\"1981-12-21\",11.1\\r\\n\"1981-12-22\",15.0\\r\\n\"1981-12-23\",12.8\\r\\n\"1981-12-24\",15.0\\r\\n\"1981-12-25\",14.2\\r\\n\"1981-12-26\",14.0\\r\\n\"1981-12-27\",15.5\\r\\n\"1981-12-28\",13.3\\r\\n\"1981-12-29\",15.6\\r\\n\"1981-12-30\",15.2\\r\\n\"1981-12-31\",17.4\\r\\n\"1982-01-01\",17.0\\r\\n\"1982-01-02\",15.0\\r\\n\"1982-01-03\",13.5\\r\\n\"1982-01-04\",15.2\\r\\n\"1982-01-05\",13.0\\r\\n\"1982-01-06\",12.5\\r\\n\"1982-01-07\",14.1\\r\\n\"1982-01-08\",14.8\\r\\n\"1982-01-09\",16.2\\r\\n\"1982-01-10\",15.8\\r\\n\"1982-01-11\",19.1\\r\\n\"1982-01-12\",22.2\\r\\n\"1982-01-13\",15.9\\r\\n\"1982-01-14\",13.0\\r\\n\"1982-01-15\",14.1\\r\\n\"1982-01-16\",15.8\\r\\n\"1982-01-17\",24.0\\r\\n\"1982-01-18\",18.0\\r\\n\"1982-01-19\",19.7\\r\\n\"1982-01-20\",25.2\\r\\n\"1982-01-21\",20.5\\r\\n\"1982-01-22\",19.3\\r\\n\"1982-01-23\",15.8\\r\\n\"1982-01-24\",17.0\\r\\n\"1982-01-25\",18.4\\r\\n\"1982-01-26\",13.3\\r\\n\"1982-01-27\",14.6\\r\\n\"1982-01-28\",12.5\\r\\n\"1982-01-29\",17.0\\r\\n\"1982-01-30\",17.1\\r\\n\"1982-01-31\",14.0\\r\\n\"1982-02-01\",14.6\\r\\n\"1982-02-02\",13.3\\r\\n\"1982-02-03\",14.8\\r\\n\"1982-02-04\",15.1\\r\\n\"1982-02-05\",13.1\\r\\n\"1982-02-06\",13.6\\r\\n\"1982-02-07\",19.5\\r\\n\"1982-02-08\",22.7\\r\\n\"1982-02-09\",17.2\\r\\n\"1982-02-10\",13.5\\r\\n\"1982-02-11\",15.4\\r\\n\"1982-02-12\",17.0\\r\\n\"1982-02-13\",19.2\\r\\n\"1982-02-14\",22.8\\r\\n\"1982-02-15\",26.3\\r\\n\"1982-02-16\",18.2\\r\\n\"1982-02-17\",17.0\\r\\n\"1982-02-18\",14.8\\r\\n\"1982-02-19\",12.8\\r\\n\"1982-02-20\",15.5\\r\\n\"1982-02-21\",15.6\\r\\n\"1982-02-22\",13.1\\r\\n\"1982-02-23\",15.2\\r\\n\"1982-02-24\",14.1\\r\\n\"1982-02-25\",12.5\\r\\n\"1982-02-26\",14.6\\r\\n\"1982-02-27\",10.4\\r\\n\"1982-02-28\",13.9\\r\\n\"1982-03-01\",11.9\\r\\n\"1982-03-02\",13.5\\r\\n\"1982-03-03\",9.8\\r\\n\"1982-03-04\",14.0\\r\\n\"1982-03-05\",21.5\\r\\n\"1982-03-06\",19.5\\r\\n\"1982-03-07\",16.7\\r\\n\"1982-03-08\",19.1\\r\\n\"1982-03-09\",11.0\\r\\n\"1982-03-10\",9.0\\r\\n\"1982-03-11\",10.0\\r\\n\"1982-03-12\",14.6\\r\\n\"1982-03-13\",12.5\\r\\n\"1982-03-14\",17.2\\r\\n\"1982-03-15\",19.2\\r\\n\"1982-03-16\",22.2\\r\\n\"1982-03-17\",15.7\\r\\n\"1982-03-18\",14.2\\r\\n\"1982-03-19\",9.8\\r\\n\"1982-03-20\",14.0\\r\\n\"1982-03-21\",17.5\\r\\n\"1982-03-22\",20.7\\r\\n\"1982-03-23\",15.6\\r\\n\"1982-03-24\",13.2\\r\\n\"1982-03-25\",14.5\\r\\n\"1982-03-26\",16.8\\r\\n\"1982-03-27\",17.2\\r\\n\"1982-03-28\",13.4\\r\\n\"1982-03-29\",14.2\\r\\n\"1982-03-30\",14.3\\r\\n\"1982-03-31\",10.2\\r\\n\"1982-04-01\",10.4\\r\\n\"1982-04-02\",12.3\\r\\n\"1982-04-03\",11.9\\r\\n\"1982-04-04\",11.2\\r\\n\"1982-04-05\",8.5\\r\\n\"1982-04-06\",12.0\\r\\n\"1982-04-07\",12.4\\r\\n\"1982-04-08\",12.9\\r\\n\"1982-04-09\",10.1\\r\\n\"1982-04-10\",15.0\\r\\n\"1982-04-11\",13.6\\r\\n\"1982-04-12\",12.4\\r\\n\"1982-04-13\",13.6\\r\\n\"1982-04-14\",16.1\\r\\n\"1982-04-15\",19.5\\r\\n\"1982-04-16\",14.2\\r\\n\"1982-04-17\",9.3\\r\\n\"1982-04-18\",10.1\\r\\n\"1982-04-19\",7.4\\r\\n\"1982-04-20\",8.6\\r\\n\"1982-04-21\",7.8\\r\\n\"1982-04-22\",9.1\\r\\n\"1982-04-23\",13.0\\r\\n\"1982-04-24\",16.5\\r\\n\"1982-04-25\",12.9\\r\\n\"1982-04-26\",6.9\\r\\n\"1982-04-27\",6.9\\r\\n\"1982-04-28\",8.7\\r\\n\"1982-04-29\",10.0\\r\\n\"1982-04-30\",10.8\\r\\n\"1982-05-01\",7.5\\r\\n\"1982-05-02\",6.3\\r\\n\"1982-05-03\",11.9\\r\\n\"1982-05-04\",13.8\\r\\n\"1982-05-05\",11.8\\r\\n\"1982-05-06\",11.0\\r\\n\"1982-05-07\",10.1\\r\\n\"1982-05-08\",8.5\\r\\n\"1982-05-09\",5.5\\r\\n\"1982-05-10\",7.6\\r\\n\"1982-05-11\",8.7\\r\\n\"1982-05-12\",10.8\\r\\n\"1982-05-13\",11.2\\r\\n\"1982-05-14\",9.1\\r\\n\"1982-05-15\",3.7\\r\\n\"1982-05-16\",4.6\\r\\n\"1982-05-17\",6.6\\r\\n\"1982-05-18\",13.2\\r\\n\"1982-05-19\",15.2\\r\\n\"1982-05-20\",7.6\\r\\n\"1982-05-21\",8.4\\r\\n\"1982-05-22\",6.0\\r\\n\"1982-05-23\",8.3\\r\\n\"1982-05-24\",8.6\\r\\n\"1982-05-25\",11.1\\r\\n\"1982-05-26\",12.1\\r\\n\"1982-05-27\",12.9\\r\\n\"1982-05-28\",14.0\\r\\n\"1982-05-29\",12.5\\r\\n\"1982-05-30\",11.5\\r\\n\"1982-05-31\",7.0\\r\\n\"1982-06-01\",7.1\\r\\n\"1982-06-02\",9.0\\r\\n\"1982-06-03\",3.1\\r\\n\"1982-06-04\",2.5\\r\\n\"1982-06-05\",0.0\\r\\n\"1982-06-06\",1.6\\r\\n\"1982-06-07\",2.6\\r\\n\"1982-06-08\",5.7\\r\\n\"1982-06-09\",2.3\\r\\n\"1982-06-10\",4.5\\r\\n\"1982-06-11\",8.2\\r\\n\"1982-06-12\",6.9\\r\\n\"1982-06-13\",7.3\\r\\n\"1982-06-14\",6.0\\r\\n\"1982-06-15\",7.3\\r\\n\"1982-06-16\",7.6\\r\\n\"1982-06-17\",8.0\\r\\n\"1982-06-18\",8.0\\r\\n\"1982-06-19\",6.8\\r\\n\"1982-06-20\",7.3\\r\\n\"1982-06-21\",6.2\\r\\n\"1982-06-22\",6.9\\r\\n\"1982-06-23\",8.9\\r\\n\"1982-06-24\",4.0\\r\\n\"1982-06-25\",1.3\\r\\n\"1982-06-26\",0.8\\r\\n\"1982-06-27\",4.3\\r\\n\"1982-06-28\",7.3\\r\\n\"1982-06-29\",7.7\\r\\n\"1982-06-30\",9.0\\r\\n\"1982-07-01\",4.2\\r\\n\"1982-07-02\",1.6\\r\\n\"1982-07-03\",2.6\\r\\n\"1982-07-04\",3.4\\r\\n\"1982-07-05\",3.9\\r\\n\"1982-07-06\",7.0\\r\\n\"1982-07-07\",7.8\\r\\n\"1982-07-08\",5.3\\r\\n\"1982-07-09\",2.4\\r\\n\"1982-07-10\",2.8\\r\\n\"1982-07-11\",4.0\\r\\n\"1982-07-12\",7.5\\r\\n\"1982-07-13\",7.8\\r\\n\"1982-07-14\",5.6\\r\\n\"1982-07-15\",3.3\\r\\n\"1982-07-16\",5.0\\r\\n\"1982-07-17\",3.7\\r\\n\"1982-07-18\",3.9\\r\\n\"1982-07-19\",5.2\\r\\n\"1982-07-20\",?0.2\\r\\n\"1982-07-21\",?0.8\\r\\n\"1982-07-22\",0.9\\r\\n\"1982-07-23\",3.5\\r\\n\"1982-07-24\",6.6\\r\\n\"1982-07-25\",9.5\\r\\n\"1982-07-26\",9.0\\r\\n\"1982-07-27\",3.5\\r\\n\"1982-07-28\",4.5\\r\\n\"1982-07-29\",5.7\\r\\n\"1982-07-30\",5.6\\r\\n\"1982-07-31\",7.1\\r\\n\"1982-08-01\",9.7\\r\\n\"1982-08-02\",8.3\\r\\n\"1982-08-03\",9.1\\r\\n\"1982-08-04\",2.8\\r\\n\"1982-08-05\",2.2\\r\\n\"1982-08-06\",4.5\\r\\n\"1982-08-07\",3.8\\r\\n\"1982-08-08\",3.8\\r\\n\"1982-08-09\",6.2\\r\\n\"1982-08-10\",11.5\\r\\n\"1982-08-11\",10.2\\r\\n\"1982-08-12\",7.9\\r\\n\"1982-08-13\",9.0\\r\\n\"1982-08-14\",9.5\\r\\n\"1982-08-15\",6.0\\r\\n\"1982-08-16\",8.2\\r\\n\"1982-08-17\",9.2\\r\\n\"1982-08-18\",4.3\\r\\n\"1982-08-19\",6.6\\r\\n\"1982-08-20\",9.4\\r\\n\"1982-08-21\",13.2\\r\\n\"1982-08-22\",6.6\\r\\n\"1982-08-23\",5.1\\r\\n\"1982-08-24\",12.1\\r\\n\"1982-08-25\",11.2\\r\\n\"1982-08-26\",8.5\\r\\n\"1982-08-27\",4.6\\r\\n\"1982-08-28\",7.0\\r\\n\"1982-08-29\",14.2\\r\\n\"1982-08-30\",12.7\\r\\n\"1982-08-31\",7.6\\r\\n\"1982-09-01\",4.0\\r\\n\"1982-09-02\",10.0\\r\\n\"1982-09-03\",10.5\\r\\n\"1982-09-04\",5.0\\r\\n\"1982-09-05\",4.5\\r\\n\"1982-09-06\",8.2\\r\\n\"1982-09-07\",4.3\\r\\n\"1982-09-08\",9.8\\r\\n\"1982-09-09\",5.8\\r\\n\"1982-09-10\",5.0\\r\\n\"1982-09-11\",8.5\\r\\n\"1982-09-12\",9.0\\r\\n\"1982-09-13\",3.6\\r\\n\"1982-09-14\",6.7\\r\\n\"1982-09-15\",6.7\\r\\n\"1982-09-16\",10.1\\r\\n\"1982-09-17\",15.0\\r\\n\"1982-09-18\",8.9\\r\\n\"1982-09-19\",5.7\\r\\n\"1982-09-20\",4.2\\r\\n\"1982-09-21\",4.0\\r\\n\"1982-09-22\",5.3\\r\\n\"1982-09-23\",6.3\\r\\n\"1982-09-24\",8.5\\r\\n\"1982-09-25\",11.5\\r\\n\"1982-09-26\",7.7\\r\\n\"1982-09-27\",9.2\\r\\n\"1982-09-28\",7.8\\r\\n\"1982-09-29\",6.3\\r\\n\"1982-09-30\",6.3\\r\\n\"1982-10-01\",8.6\\r\\n\"1982-10-02\",6.1\\r\\n\"1982-10-03\",13.2\\r\\n\"1982-10-04\",9.9\\r\\n\"1982-10-05\",4.7\\r\\n\"1982-10-06\",5.8\\r\\n\"1982-10-07\",14.9\\r\\n\"1982-10-08\",10.7\\r\\n\"1982-10-09\",8.6\\r\\n\"1982-10-10\",9.4\\r\\n\"1982-10-11\",5.7\\r\\n\"1982-10-12\",10.9\\r\\n\"1982-10-13\",13.1\\r\\n\"1982-10-14\",10.4\\r\\n\"1982-10-15\",8.2\\r\\n\"1982-10-16\",9.8\\r\\n\"1982-10-17\",7.5\\r\\n\"1982-10-18\",5.8\\r\\n\"1982-10-19\",9.8\\r\\n\"1982-10-20\",7.9\\r\\n\"1982-10-21\",8.7\\r\\n\"1982-10-22\",10.0\\r\\n\"1982-10-23\",10.6\\r\\n\"1982-10-24\",8.0\\r\\n\"1982-10-25\",10.2\\r\\n\"1982-10-26\",15.1\\r\\n\"1982-10-27\",13.9\\r\\n\"1982-10-28\",9.2\\r\\n\"1982-10-29\",9.0\\r\\n\"1982-10-30\",13.2\\r\\n\"1982-10-31\",7.0\\r\\n\"1982-11-01\",10.6\\r\\n\"1982-11-02\",6.9\\r\\n\"1982-11-03\",9.5\\r\\n\"1982-11-04\",12.5\\r\\n\"1982-11-05\",13.6\\r\\n\"1982-11-06\",17.7\\r\\n\"1982-11-07\",16.0\\r\\n\"1982-11-08\",11.3\\r\\n\"1982-11-09\",10.5\\r\\n\"1982-11-10\",14.4\\r\\n\"1982-11-11\",10.3\\r\\n\"1982-11-12\",9.0\\r\\n\"1982-11-13\",11.1\\r\\n\"1982-11-14\",14.5\\r\\n\"1982-11-15\",18.0\\r\\n\"1982-11-16\",12.8\\r\\n\"1982-11-17\",10.7\\r\\n\"1982-11-18\",9.1\\r\\n\"1982-11-19\",8.7\\r\\n\"1982-11-20\",12.4\\r\\n\"1982-11-21\",12.6\\r\\n\"1982-11-22\",10.3\\r\\n\"1982-11-23\",13.7\\r\\n\"1982-11-24\",16.0\\r\\n\"1982-11-25\",15.8\\r\\n\"1982-11-26\",12.1\\r\\n\"1982-11-27\",12.5\\r\\n\"1982-11-28\",12.2\\r\\n\"1982-11-29\",13.7\\r\\n\"1982-11-30\",16.1\\r\\n\"1982-12-01\",15.5\\r\\n\"1982-12-02\",10.3\\r\\n\"1982-12-03\",10.5\\r\\n\"1982-12-04\",11.0\\r\\n\"1982-12-05\",11.9\\r\\n\"1982-12-06\",13.0\\r\\n\"1982-12-07\",12.2\\r\\n\"1982-12-08\",10.6\\r\\n\"1982-12-09\",13.0\\r\\n\"1982-12-10\",13.0\\r\\n\"1982-12-11\",12.2\\r\\n\"1982-12-12\",12.6\\r\\n\"1982-12-13\",18.7\\r\\n\"1982-12-14\",15.2\\r\\n\"1982-12-15\",15.3\\r\\n\"1982-12-16\",13.9\\r\\n\"1982-12-17\",15.8\\r\\n\"1982-12-18\",13.0\\r\\n\"1982-12-19\",13.0\\r\\n\"1982-12-20\",13.7\\r\\n\"1982-12-21\",12.0\\r\\n\"1982-12-22\",10.8\\r\\n\"1982-12-23\",15.6\\r\\n\"1982-12-24\",15.3\\r\\n\"1982-12-25\",13.9\\r\\n\"1982-12-26\",13.0\\r\\n\"1982-12-27\",15.3\\r\\n\"1982-12-28\",16.3\\r\\n\"1982-12-29\",15.8\\r\\n\"1982-12-30\",17.7\\r\\n\"1982-12-31\",16.3\\r\\n\"1983-01-01\",18.4\\r\\n\"1983-01-02\",15.0\\r\\n\"1983-01-03\",10.9\\r\\n\"1983-01-04\",11.4\\r\\n\"1983-01-05\",14.8\\r\\n\"1983-01-06\",12.1\\r\\n\"1983-01-07\",12.8\\r\\n\"1983-01-08\",16.2\\r\\n\"1983-01-09\",15.5\\r\\n\"1983-01-10\",13.0\\r\\n\"1983-01-11\",10.5\\r\\n\"1983-01-12\",9.1\\r\\n\"1983-01-13\",10.5\\r\\n\"1983-01-14\",11.8\\r\\n\"1983-01-15\",12.7\\r\\n\"1983-01-16\",12.7\\r\\n\"1983-01-17\",11.5\\r\\n\"1983-01-18\",13.8\\r\\n\"1983-01-19\",13.3\\r\\n\"1983-01-20\",11.6\\r\\n\"1983-01-21\",15.4\\r\\n\"1983-01-22\",12.4\\r\\n\"1983-01-23\",16.9\\r\\n\"1983-01-24\",14.7\\r\\n\"1983-01-25\",10.6\\r\\n\"1983-01-26\",15.6\\r\\n\"1983-01-27\",10.7\\r\\n\"1983-01-28\",12.6\\r\\n\"1983-01-29\",13.8\\r\\n\"1983-01-30\",14.3\\r\\n\"1983-01-31\",14.0\\r\\n\"1983-02-01\",18.1\\r\\n\"1983-02-02\",17.3\\r\\n\"1983-02-03\",13.0\\r\\n\"1983-02-04\",16.0\\r\\n\"1983-02-05\",14.9\\r\\n\"1983-02-06\",16.2\\r\\n\"1983-02-07\",20.3\\r\\n\"1983-02-08\",22.5\\r\\n\"1983-02-09\",17.2\\r\\n\"1983-02-10\",15.9\\r\\n\"1983-02-11\",16.8\\r\\n\"1983-02-12\",13.8\\r\\n\"1983-02-13\",12.8\\r\\n\"1983-02-14\",14.0\\r\\n\"1983-02-15\",17.5\\r\\n\"1983-02-16\",21.5\\r\\n\"1983-02-17\",16.8\\r\\n\"1983-02-18\",13.6\\r\\n\"1983-02-19\",14.5\\r\\n\"1983-02-20\",14.2\\r\\n\"1983-02-21\",15.7\\r\\n\"1983-02-22\",19.7\\r\\n\"1983-02-23\",17.4\\r\\n\"1983-02-24\",14.4\\r\\n\"1983-02-25\",16.9\\r\\n\"1983-02-26\",19.1\\r\\n\"1983-02-27\",20.4\\r\\n\"1983-02-28\",20.1\\r\\n\"1983-03-01\",19.9\\r\\n\"1983-03-02\",22.0\\r\\n\"1983-03-03\",20.5\\r\\n\"1983-03-04\",22.1\\r\\n\"1983-03-05\",20.6\\r\\n\"1983-03-06\",15.0\\r\\n\"1983-03-07\",20.6\\r\\n\"1983-03-08\",21.5\\r\\n\"1983-03-09\",16.2\\r\\n\"1983-03-10\",14.1\\r\\n\"1983-03-11\",14.5\\r\\n\"1983-03-12\",21.1\\r\\n\"1983-03-13\",15.9\\r\\n\"1983-03-14\",15.2\\r\\n\"1983-03-15\",13.1\\r\\n\"1983-03-16\",13.2\\r\\n\"1983-03-17\",12.5\\r\\n\"1983-03-18\",15.2\\r\\n\"1983-03-19\",17.6\\r\\n\"1983-03-20\",15.5\\r\\n\"1983-03-21\",16.7\\r\\n\"1983-03-22\",16.3\\r\\n\"1983-03-23\",15.1\\r\\n\"1983-03-24\",12.7\\r\\n\"1983-03-25\",10.0\\r\\n\"1983-03-26\",11.4\\r\\n\"1983-03-27\",12.6\\r\\n\"1983-03-28\",10.7\\r\\n\"1983-03-29\",10.0\\r\\n\"1983-03-30\",13.9\\r\\n\"1983-03-31\",13.4\\r\\n\"1983-04-01\",12.5\\r\\n\"1983-04-02\",12.8\\r\\n\"1983-04-03\",7.8\\r\\n\"1983-04-04\",11.1\\r\\n\"1983-04-05\",10.7\\r\\n\"1983-04-06\",7.1\\r\\n\"1983-04-07\",6.7\\r\\n\"1983-04-08\",5.7\\r\\n\"1983-04-09\",9.1\\r\\n\"1983-04-10\",15.2\\r\\n\"1983-04-11\",15.5\\r\\n\"1983-04-12\",11.1\\r\\n\"1983-04-13\",11.7\\r\\n\"1983-04-14\",11.5\\r\\n\"1983-04-15\",9.8\\r\\n\"1983-04-16\",6.2\\r\\n\"1983-04-17\",6.7\\r\\n\"1983-04-18\",7.5\\r\\n\"1983-04-19\",8.8\\r\\n\"1983-04-20\",8.0\\r\\n\"1983-04-21\",10.4\\r\\n\"1983-04-22\",14.5\\r\\n\"1983-04-23\",16.5\\r\\n\"1983-04-24\",14.1\\r\\n\"1983-04-25\",10.5\\r\\n\"1983-04-26\",12.6\\r\\n\"1983-04-27\",13.0\\r\\n\"1983-04-28\",8.7\\r\\n\"1983-04-29\",10.1\\r\\n\"1983-04-30\",12.0\\r\\n\"1983-05-01\",12.5\\r\\n\"1983-05-02\",13.5\\r\\n\"1983-05-03\",13.7\\r\\n\"1983-05-04\",13.5\\r\\n\"1983-05-05\",10.7\\r\\n\"1983-05-06\",13.0\\r\\n\"1983-05-07\",11.6\\r\\n\"1983-05-08\",13.0\\r\\n\"1983-05-09\",11.2\\r\\n\"1983-05-10\",13.5\\r\\n\"1983-05-11\",12.9\\r\\n\"1983-05-12\",6.8\\r\\n\"1983-05-13\",10.0\\r\\n\"1983-05-14\",14.5\\r\\n\"1983-05-15\",11.7\\r\\n\"1983-05-16\",6.7\\r\\n\"1983-05-17\",4.6\\r\\n\"1983-05-18\",4.9\\r\\n\"1983-05-19\",7.4\\r\\n\"1983-05-20\",8.3\\r\\n\"1983-05-21\",7.5\\r\\n\"1983-05-22\",6.2\\r\\n\"1983-05-23\",7.8\\r\\n\"1983-05-24\",13.2\\r\\n\"1983-05-25\",11.9\\r\\n\"1983-05-26\",6.5\\r\\n\"1983-05-27\",8.3\\r\\n\"1983-05-28\",12.1\\r\\n\"1983-05-29\",9.3\\r\\n\"1983-05-30\",7.5\\r\\n\"1983-05-31\",9.3\\r\\n\"1983-06-01\",11.0\\r\\n\"1983-06-02\",10.8\\r\\n\"1983-06-03\",5.3\\r\\n\"1983-06-04\",7.6\\r\\n\"1983-06-05\",5.6\\r\\n\"1983-06-06\",7.2\\r\\n\"1983-06-07\",9.6\\r\\n\"1983-06-08\",7.0\\r\\n\"1983-06-09\",8.3\\r\\n\"1983-06-10\",7.8\\r\\n\"1983-06-11\",4.7\\r\\n\"1983-06-12\",6.8\\r\\n\"1983-06-13\",7.2\\r\\n\"1983-06-14\",8.3\\r\\n\"1983-06-15\",9.5\\r\\n\"1983-06-16\",4.7\\r\\n\"1983-06-17\",3.0\\r\\n\"1983-06-18\",1.5\\r\\n\"1983-06-19\",2.5\\r\\n\"1983-06-20\",6.2\\r\\n\"1983-06-21\",11.6\\r\\n\"1983-06-22\",6.6\\r\\n\"1983-06-23\",6.6\\r\\n\"1983-06-24\",8.0\\r\\n\"1983-06-25\",7.9\\r\\n\"1983-06-26\",3.3\\r\\n\"1983-06-27\",3.9\\r\\n\"1983-06-28\",6.0\\r\\n\"1983-06-29\",4.0\\r\\n\"1983-06-30\",5.5\\r\\n\"1983-07-01\",8.5\\r\\n\"1983-07-02\",9.8\\r\\n\"1983-07-03\",9.5\\r\\n\"1983-07-04\",7.2\\r\\n\"1983-07-05\",8.1\\r\\n\"1983-07-06\",8.0\\r\\n\"1983-07-07\",8.5\\r\\n\"1983-07-08\",8.8\\r\\n\"1983-07-09\",8.3\\r\\n\"1983-07-10\",2.4\\r\\n\"1983-07-11\",4.9\\r\\n\"1983-07-12\",5.9\\r\\n\"1983-07-13\",6.7\\r\\n\"1983-07-14\",8.4\\r\\n\"1983-07-15\",6.5\\r\\n\"1983-07-16\",7.9\\r\\n\"1983-07-17\",4.1\\r\\n\"1983-07-18\",5.4\\r\\n\"1983-07-19\",7.5\\r\\n\"1983-07-20\",3.9\\r\\n\"1983-07-21\",2.5\\r\\n\"1983-07-22\",5.3\\r\\n\"1983-07-23\",6.6\\r\\n\"1983-07-24\",0.0\\r\\n\"1983-07-25\",0.7\\r\\n\"1983-07-26\",7.6\\r\\n\"1983-07-27\",12.3\\r\\n\"1983-07-28\",9.2\\r\\n\"1983-07-29\",9.6\\r\\n\"1983-07-30\",9.5\\r\\n\"1983-07-31\",10.0\\r\\n\"1983-08-01\",7.7\\r\\n\"1983-08-02\",8.0\\r\\n\"1983-08-03\",8.3\\r\\n\"1983-08-04\",8.3\\r\\n\"1983-08-05\",4.5\\r\\n\"1983-08-06\",6.5\\r\\n\"1983-08-07\",9.4\\r\\n\"1983-08-08\",9.4\\r\\n\"1983-08-09\",10.5\\r\\n\"1983-08-10\",10.7\\r\\n\"1983-08-11\",9.9\\r\\n\"1983-08-12\",7.6\\r\\n\"1983-08-13\",5.8\\r\\n\"1983-08-14\",8.5\\r\\n\"1983-08-15\",13.8\\r\\n\"1983-08-16\",14.3\\r\\n\"1983-08-17\",8.3\\r\\n\"1983-08-18\",5.3\\r\\n\"1983-08-19\",3.0\\r\\n\"1983-08-20\",5.2\\r\\n\"1983-08-21\",10.3\\r\\n\"1983-08-22\",11.1\\r\\n\"1983-08-23\",10.5\\r\\n\"1983-08-24\",9.0\\r\\n\"1983-08-25\",13.0\\r\\n\"1983-08-26\",6.4\\r\\n\"1983-08-27\",8.4\\r\\n\"1983-08-28\",6.7\\r\\n\"1983-08-29\",8.3\\r\\n\"1983-08-30\",11.2\\r\\n\"1983-08-31\",10.0\\r\\n\"1983-09-01\",10.1\\r\\n\"1983-09-02\",10.6\\r\\n\"1983-09-03\",10.9\\r\\n\"1983-09-04\",5.7\\r\\n\"1983-09-05\",9.5\\r\\n\"1983-09-06\",10.4\\r\\n\"1983-09-07\",11.1\\r\\n\"1983-09-08\",12.2\\r\\n\"1983-09-09\",10.6\\r\\n\"1983-09-10\",8.8\\r\\n\"1983-09-11\",9.2\\r\\n\"1983-09-12\",5.5\\r\\n\"1983-09-13\",7.1\\r\\n\"1983-09-14\",6.5\\r\\n\"1983-09-15\",4.3\\r\\n\"1983-09-16\",5.0\\r\\n\"1983-09-17\",11.2\\r\\n\"1983-09-18\",7.5\\r\\n\"1983-09-19\",12.0\\r\\n\"1983-09-20\",13.6\\r\\n\"1983-09-21\",8.3\\r\\n\"1983-09-22\",8.5\\r\\n\"1983-09-23\",12.9\\r\\n\"1983-09-24\",7.7\\r\\n\"1983-09-25\",7.6\\r\\n\"1983-09-26\",3.5\\r\\n\"1983-09-27\",10.4\\r\\n\"1983-09-28\",15.4\\r\\n\"1983-09-29\",10.6\\r\\n\"1983-09-30\",9.6\\r\\n\"1983-10-01\",9.3\\r\\n\"1983-10-02\",13.9\\r\\n\"1983-10-03\",7.7\\r\\n\"1983-10-04\",9.5\\r\\n\"1983-10-05\",7.6\\r\\n\"1983-10-06\",6.9\\r\\n\"1983-10-07\",6.8\\r\\n\"1983-10-08\",5.8\\r\\n\"1983-10-09\",6.0\\r\\n\"1983-10-10\",8.3\\r\\n\"1983-10-11\",9.1\\r\\n\"1983-10-12\",12.5\\r\\n\"1983-10-13\",13.2\\r\\n\"1983-10-14\",16.2\\r\\n\"1983-10-15\",12.5\\r\\n\"1983-10-16\",11.8\\r\\n\"1983-10-17\",10.6\\r\\n\"1983-10-18\",10.0\\r\\n\"1983-10-19\",12.2\\r\\n\"1983-10-20\",8.9\\r\\n\"1983-10-21\",10.3\\r\\n\"1983-10-22\",7.5\\r\\n\"1983-10-23\",11.6\\r\\n\"1983-10-24\",12.6\\r\\n\"1983-10-25\",12.9\\r\\n\"1983-10-26\",11.7\\r\\n\"1983-10-27\",14.0\\r\\n\"1983-10-28\",12.3\\r\\n\"1983-10-29\",9.0\\r\\n\"1983-10-30\",9.2\\r\\n\"1983-10-31\",9.8\\r\\n\"1983-11-01\",11.8\\r\\n\"1983-11-02\",10.6\\r\\n\"1983-11-03\",12.6\\r\\n\"1983-11-04\",11.0\\r\\n\"1983-11-05\",8.2\\r\\n\"1983-11-06\",7.5\\r\\n\"1983-11-07\",13.6\\r\\n\"1983-11-08\",14.8\\r\\n\"1983-11-09\",10.9\\r\\n\"1983-11-10\",7.7\\r\\n\"1983-11-11\",10.2\\r\\n\"1983-11-12\",10.8\\r\\n\"1983-11-13\",10.8\\r\\n\"1983-11-14\",12.5\\r\\n\"1983-11-15\",13.2\\r\\n\"1983-11-16\",8.7\\r\\n\"1983-11-17\",5.7\\r\\n\"1983-11-18\",9.8\\r\\n\"1983-11-19\",7.3\\r\\n\"1983-11-20\",10.8\\r\\n\"1983-11-21\",10.0\\r\\n\"1983-11-22\",16.2\\r\\n\"1983-11-23\",15.0\\r\\n\"1983-11-24\",14.5\\r\\n\"1983-11-25\",15.9\\r\\n\"1983-11-26\",14.9\\r\\n\"1983-11-27\",14.2\\r\\n\"1983-11-28\",15.8\\r\\n\"1983-11-29\",17.2\\r\\n\"1983-11-30\",17.6\\r\\n\"1983-12-01\",12.1\\r\\n\"1983-12-02\",11.4\\r\\n\"1983-12-03\",13.0\\r\\n\"1983-12-04\",13.2\\r\\n\"1983-12-05\",12.0\\r\\n\"1983-12-06\",15.3\\r\\n\"1983-12-07\",12.7\\r\\n\"1983-12-08\",12.1\\r\\n\"1983-12-09\",13.8\\r\\n\"1983-12-10\",10.9\\r\\n\"1983-12-11\",12.0\\r\\n\"1983-12-12\",16.5\\r\\n\"1983-12-13\",15.0\\r\\n\"1983-12-14\",11.2\\r\\n\"1983-12-15\",13.9\\r\\n\"1983-12-16\",15.0\\r\\n\"1983-12-17\",14.8\\r\\n\"1983-12-18\",15.0\\r\\n\"1983-12-19\",13.3\\r\\n\"1983-12-20\",20.4\\r\\n\"1983-12-21\",18.0\\r\\n\"1983-12-22\",12.2\\r\\n\"1983-12-23\",16.7\\r\\n\"1983-12-24\",13.8\\r\\n\"1983-12-25\",17.5\\r\\n\"1983-12-26\",15.0\\r\\n\"1983-12-27\",13.9\\r\\n\"1983-12-28\",11.1\\r\\n\"1983-12-29\",16.1\\r\\n\"1983-12-30\",20.4\\r\\n\"1983-12-31\",18.0\\r\\n\"1984-01-01\",19.5\\r\\n\"1984-01-02\",17.1\\r\\n\"1984-01-03\",17.1\\r\\n\"1984-01-04\",12.0\\r\\n\"1984-01-05\",11.0\\r\\n\"1984-01-06\",16.3\\r\\n\"1984-01-07\",16.1\\r\\n\"1984-01-08\",13.0\\r\\n\"1984-01-09\",13.4\\r\\n\"1984-01-10\",15.2\\r\\n\"1984-01-11\",12.5\\r\\n\"1984-01-12\",14.3\\r\\n\"1984-01-13\",16.5\\r\\n\"1984-01-14\",18.6\\r\\n\"1984-01-15\",18.0\\r\\n\"1984-01-16\",18.2\\r\\n\"1984-01-17\",11.4\\r\\n\"1984-01-18\",11.9\\r\\n\"1984-01-19\",12.2\\r\\n\"1984-01-20\",14.8\\r\\n\"1984-01-21\",13.1\\r\\n\"1984-01-22\",12.7\\r\\n\"1984-01-23\",10.5\\r\\n\"1984-01-24\",13.8\\r\\n\"1984-01-25\",18.8\\r\\n\"1984-01-26\",13.9\\r\\n\"1984-01-27\",11.2\\r\\n\"1984-01-28\",10.6\\r\\n\"1984-01-29\",14.7\\r\\n\"1984-01-30\",13.1\\r\\n\"1984-01-31\",12.1\\r\\n\"1984-02-01\",14.7\\r\\n\"1984-02-02\",11.1\\r\\n\"1984-02-03\",13.0\\r\\n\"1984-02-04\",15.6\\r\\n\"1984-02-05\",14.2\\r\\n\"1984-02-06\",15.5\\r\\n\"1984-02-07\",18.0\\r\\n\"1984-02-08\",15.0\\r\\n\"1984-02-09\",15.9\\r\\n\"1984-02-10\",15.5\\r\\n\"1984-02-11\",15.8\\r\\n\"1984-02-12\",16.6\\r\\n\"1984-02-13\",13.6\\r\\n\"1984-02-14\",13.8\\r\\n\"1984-02-15\",14.6\\r\\n\"1984-02-16\",15.6\\r\\n\"1984-02-17\",16.6\\r\\n\"1984-02-18\",14.3\\r\\n\"1984-02-19\",16.3\\r\\n\"1984-02-20\",18.9\\r\\n\"1984-02-21\",18.7\\r\\n\"1984-02-22\",14.5\\r\\n\"1984-02-23\",16.5\\r\\n\"1984-02-24\",14.1\\r\\n\"1984-02-25\",13.5\\r\\n\"1984-02-26\",11.7\\r\\n\"1984-02-27\",15.1\\r\\n\"1984-02-28\",11.2\\r\\n\"1984-02-29\",13.5\\r\\n\"1984-03-01\",12.6\\r\\n\"1984-03-02\",8.8\\r\\n\"1984-03-03\",10.5\\r\\n\"1984-03-04\",12.1\\r\\n\"1984-03-05\",14.5\\r\\n\"1984-03-06\",19.5\\r\\n\"1984-03-07\",14.0\\r\\n\"1984-03-08\",13.8\\r\\n\"1984-03-09\",10.5\\r\\n\"1984-03-10\",13.8\\r\\n\"1984-03-11\",11.4\\r\\n\"1984-03-12\",15.6\\r\\n\"1984-03-13\",11.1\\r\\n\"1984-03-14\",12.1\\r\\n\"1984-03-15\",14.2\\r\\n\"1984-03-16\",10.9\\r\\n\"1984-03-17\",14.2\\r\\n\"1984-03-18\",13.8\\r\\n\"1984-03-19\",15.1\\r\\n\"1984-03-20\",14.0\\r\\n\"1984-03-21\",12.1\\r\\n\"1984-03-22\",13.8\\r\\n\"1984-03-23\",16.6\\r\\n\"1984-03-24\",17.8\\r\\n\"1984-03-25\",9.4\\r\\n\"1984-03-26\",10.2\\r\\n\"1984-03-27\",7.4\\r\\n\"1984-03-28\",8.7\\r\\n\"1984-03-29\",14.0\\r\\n\"1984-03-30\",15.3\\r\\n\"1984-03-31\",11.1\\r\\n\"1984-04-01\",9.7\\r\\n\"1984-04-02\",10.3\\r\\n\"1984-04-03\",9.2\\r\\n\"1984-04-04\",8.2\\r\\n\"1984-04-05\",9.7\\r\\n\"1984-04-06\",12.4\\r\\n\"1984-04-07\",12.5\\r\\n\"1984-04-08\",9.0\\r\\n\"1984-04-09\",9.7\\r\\n\"1984-04-10\",10.1\\r\\n\"1984-04-11\",11.2\\r\\n\"1984-04-12\",12.0\\r\\n\"1984-04-13\",11.1\\r\\n\"1984-04-14\",10.8\\r\\n\"1984-04-15\",12.8\\r\\n\"1984-04-16\",9.8\\r\\n\"1984-04-17\",13.7\\r\\n\"1984-04-18\",11.0\\r\\n\"1984-04-19\",13.2\\r\\n\"1984-04-20\",13.0\\r\\n\"1984-04-21\",10.2\\r\\n\"1984-04-22\",13.2\\r\\n\"1984-04-23\",9.3\\r\\n\"1984-04-24\",11.1\\r\\n\"1984-04-25\",10.3\\r\\n\"1984-04-26\",8.7\\r\\n\"1984-04-27\",11.7\\r\\n\"1984-04-28\",12.5\\r\\n\"1984-04-29\",6.5\\r\\n\"1984-04-30\",9.6\\r\\n\"1984-05-01\",13.8\\r\\n\"1984-05-02\",14.7\\r\\n\"1984-05-03\",9.1\\r\\n\"1984-05-04\",4.8\\r\\n\"1984-05-05\",3.3\\r\\n\"1984-05-06\",3.5\\r\\n\"1984-05-07\",5.7\\r\\n\"1984-05-08\",5.5\\r\\n\"1984-05-09\",7.0\\r\\n\"1984-05-10\",9.5\\r\\n\"1984-05-11\",9.9\\r\\n\"1984-05-12\",4.9\\r\\n\"1984-05-13\",6.3\\r\\n\"1984-05-14\",4.8\\r\\n\"1984-05-15\",6.2\\r\\n\"1984-05-16\",7.1\\r\\n\"1984-05-17\",7.5\\r\\n\"1984-05-18\",9.4\\r\\n\"1984-05-19\",8.7\\r\\n\"1984-05-20\",9.5\\r\\n\"1984-05-21\",12.1\\r\\n\"1984-05-22\",9.5\\r\\n\"1984-05-23\",9.3\\r\\n\"1984-05-24\",8.5\\r\\n\"1984-05-25\",8.0\\r\\n\"1984-05-26\",9.8\\r\\n\"1984-05-27\",6.2\\r\\n\"1984-05-28\",7.3\\r\\n\"1984-05-29\",10.9\\r\\n\"1984-05-30\",10.0\\r\\n\"1984-05-31\",8.7\\r\\n\"1984-06-01\",9.0\\r\\n\"1984-06-02\",10.8\\r\\n\"1984-06-03\",12.4\\r\\n\"1984-06-04\",7.2\\r\\n\"1984-06-05\",7.2\\r\\n\"1984-06-06\",11.1\\r\\n\"1984-06-07\",9.3\\r\\n\"1984-06-08\",10.1\\r\\n\"1984-06-09\",3.9\\r\\n\"1984-06-10\",5.0\\r\\n\"1984-06-11\",8.2\\r\\n\"1984-06-12\",2.8\\r\\n\"1984-06-13\",4.3\\r\\n\"1984-06-14\",8.1\\r\\n\"1984-06-15\",11.1\\r\\n\"1984-06-16\",4.7\\r\\n\"1984-06-17\",5.3\\r\\n\"1984-06-18\",10.0\\r\\n\"1984-06-19\",5.6\\r\\n\"1984-06-20\",2.2\\r\\n\"1984-06-21\",7.1\\r\\n\"1984-06-22\",8.3\\r\\n\"1984-06-23\",8.6\\r\\n\"1984-06-24\",10.1\\r\\n\"1984-06-25\",8.3\\r\\n\"1984-06-26\",7.2\\r\\n\"1984-06-27\",7.7\\r\\n\"1984-06-28\",7.8\\r\\n\"1984-06-29\",9.1\\r\\n\"1984-06-30\",9.4\\r\\n\"1984-07-01\",7.8\\r\\n\"1984-07-02\",2.6\\r\\n\"1984-07-03\",2.4\\r\\n\"1984-07-04\",3.9\\r\\n\"1984-07-05\",1.3\\r\\n\"1984-07-06\",2.1\\r\\n\"1984-07-07\",7.4\\r\\n\"1984-07-08\",7.2\\r\\n\"1984-07-09\",8.8\\r\\n\"1984-07-10\",8.9\\r\\n\"1984-07-11\",8.8\\r\\n\"1984-07-12\",8.0\\r\\n\"1984-07-13\",0.7\\r\\n\"1984-07-14\",?0.1\\r\\n\"1984-07-15\",0.9\\r\\n\"1984-07-16\",7.8\\r\\n\"1984-07-17\",7.2\\r\\n\"1984-07-18\",8.0\\r\\n\"1984-07-19\",4.6\\r\\n\"1984-07-20\",5.2\\r\\n\"1984-07-21\",5.8\\r\\n\"1984-07-22\",6.8\\r\\n\"1984-07-23\",8.1\\r\\n\"1984-07-24\",7.5\\r\\n\"1984-07-25\",5.4\\r\\n\"1984-07-26\",4.6\\r\\n\"1984-07-27\",6.4\\r\\n\"1984-07-28\",9.7\\r\\n\"1984-07-29\",7.0\\r\\n\"1984-07-30\",10.0\\r\\n\"1984-07-31\",10.6\\r\\n\"1984-08-01\",11.5\\r\\n\"1984-08-02\",10.2\\r\\n\"1984-08-03\",11.1\\r\\n\"1984-08-04\",11.0\\r\\n\"1984-08-05\",8.9\\r\\n\"1984-08-06\",9.9\\r\\n\"1984-08-07\",11.7\\r\\n\"1984-08-08\",11.6\\r\\n\"1984-08-09\",9.0\\r\\n\"1984-08-10\",6.3\\r\\n\"1984-08-11\",8.7\\r\\n\"1984-08-12\",8.5\\r\\n\"1984-08-13\",8.5\\r\\n\"1984-08-14\",8.0\\r\\n\"1984-08-15\",6.0\\r\\n\"1984-08-16\",8.0\\r\\n\"1984-08-17\",8.5\\r\\n\"1984-08-18\",7.7\\r\\n\"1984-08-19\",8.4\\r\\n\"1984-08-20\",9.0\\r\\n\"1984-08-21\",8.3\\r\\n\"1984-08-22\",6.8\\r\\n\"1984-08-23\",9.3\\r\\n\"1984-08-24\",6.7\\r\\n\"1984-08-25\",9.0\\r\\n\"1984-08-26\",7.3\\r\\n\"1984-08-27\",6.3\\r\\n\"1984-08-28\",7.9\\r\\n\"1984-08-29\",5.2\\r\\n\"1984-08-30\",9.0\\r\\n\"1984-08-31\",11.3\\r\\n\"1984-09-01\",9.2\\r\\n\"1984-09-02\",11.3\\r\\n\"1984-09-03\",7.0\\r\\n\"1984-09-04\",8.0\\r\\n\"1984-09-05\",4.6\\r\\n\"1984-09-06\",8.5\\r\\n\"1984-09-07\",9.5\\r\\n\"1984-09-08\",9.4\\r\\n\"1984-09-09\",10.5\\r\\n\"1984-09-10\",9.7\\r\\n\"1984-09-11\",4.9\\r\\n\"1984-09-12\",8.0\\r\\n\"1984-09-13\",5.8\\r\\n\"1984-09-14\",5.5\\r\\n\"1984-09-15\",10.9\\r\\n\"1984-09-16\",11.7\\r\\n\"1984-09-17\",9.2\\r\\n\"1984-09-18\",8.9\\r\\n\"1984-09-19\",11.3\\r\\n\"1984-09-20\",8.6\\r\\n\"1984-09-21\",6.2\\r\\n\"1984-09-22\",6.6\\r\\n\"1984-09-23\",9.1\\r\\n\"1984-09-24\",6.1\\r\\n\"1984-09-25\",7.5\\r\\n\"1984-09-26\",10.7\\r\\n\"1984-09-27\",6.3\\r\\n\"1984-09-28\",5.5\\r\\n\"1984-09-29\",6.7\\r\\n\"1984-09-30\",4.2\\r\\n\"1984-10-01\",11.3\\r\\n\"1984-10-02\",16.3\\r\\n\"1984-10-03\",10.5\\r\\n\"1984-10-04\",10.3\\r\\n\"1984-10-05\",7.9\\r\\n\"1984-10-06\",7.7\\r\\n\"1984-10-07\",16.0\\r\\n\"1984-10-08\",14.6\\r\\n\"1984-10-09\",12.5\\r\\n\"1984-10-10\",8.1\\r\\n\"1984-10-11\",12.2\\r\\n\"1984-10-12\",17.2\\r\\n\"1984-10-13\",9.4\\r\\n\"1984-10-14\",8.7\\r\\n\"1984-10-15\",5.9\\r\\n\"1984-10-16\",4.8\\r\\n\"1984-10-17\",7.4\\r\\n\"1984-10-18\",9.4\\r\\n\"1984-10-19\",9.7\\r\\n\"1984-10-20\",9.9\\r\\n\"1984-10-21\",6.5\\r\\n\"1984-10-22\",9.8\\r\\n\"1984-10-23\",18.2\\r\\n\"1984-10-24\",11.3\\r\\n\"1984-10-25\",9.1\\r\\n\"1984-10-26\",9.6\\r\\n\"1984-10-27\",13.5\\r\\n\"1984-10-28\",10.7\\r\\n\"1984-10-29\",10.0\\r\\n\"1984-10-30\",8.5\\r\\n\"1984-10-31\",12.6\\r\\n\"1984-11-01\",16.6\\r\\n\"1984-11-02\",11.6\\r\\n\"1984-11-03\",12.2\\r\\n\"1984-11-04\",11.2\\r\\n\"1984-11-05\",9.2\\r\\n\"1984-11-06\",9.9\\r\\n\"1984-11-07\",11.9\\r\\n\"1984-11-08\",15.6\\r\\n\"1984-11-09\",19.0\\r\\n\"1984-11-10\",12.8\\r\\n\"1984-11-11\",12.2\\r\\n\"1984-11-12\",12.0\\r\\n\"1984-11-13\",11.1\\r\\n\"1984-11-14\",11.8\\r\\n\"1984-11-15\",7.6\\r\\n\"1984-11-16\",13.0\\r\\n\"1984-11-17\",12.7\\r\\n\"1984-11-18\",16.0\\r\\n\"1984-11-19\",14.8\\r\\n\"1984-11-20\",14.2\\r\\n\"1984-11-21\",10.0\\r\\n\"1984-11-22\",8.8\\r\\n\"1984-11-23\",11.6\\r\\n\"1984-11-24\",8.6\\r\\n\"1984-11-25\",14.6\\r\\n\"1984-11-26\",24.3\\r\\n\"1984-11-27\",11.6\\r\\n\"1984-11-28\",10.8\\r\\n\"1984-11-29\",12.0\\r\\n\"1984-11-30\",11.0\\r\\n\"1984-12-01\",12.6\\r\\n\"1984-12-02\",10.8\\r\\n\"1984-12-03\",9.1\\r\\n\"1984-12-04\",11.0\\r\\n\"1984-12-05\",13.0\\r\\n\"1984-12-06\",12.8\\r\\n\"1984-12-07\",9.9\\r\\n\"1984-12-08\",11.6\\r\\n\"1984-12-09\",10.5\\r\\n\"1984-12-10\",15.9\\r\\n\"1984-12-11\",12.2\\r\\n\"1984-12-12\",13.0\\r\\n\"1984-12-13\",12.5\\r\\n\"1984-12-14\",12.5\\r\\n\"1984-12-15\",11.4\\r\\n\"1984-12-16\",12.1\\r\\n\"1984-12-17\",16.8\\r\\n\"1984-12-18\",12.1\\r\\n\"1984-12-19\",11.3\\r\\n\"1984-12-20\",10.4\\r\\n\"1984-12-21\",14.2\\r\\n\"1984-12-22\",11.4\\r\\n\"1984-12-23\",13.7\\r\\n\"1984-12-24\",16.5\\r\\n\"1984-12-25\",12.8\\r\\n\"1984-12-26\",12.2\\r\\n\"1984-12-27\",12.0\\r\\n\"1984-12-28\",12.6\\r\\n\"1984-12-29\",16.0\\r\\n\"1984-12-30\",16.4\\r\\n\"1985-01-01\",13.3\\r\\n\"1985-01-02\",15.2\\r\\n\"1985-01-03\",13.1\\r\\n\"1985-01-04\",12.7\\r\\n\"1985-01-05\",14.6\\r\\n\"1985-01-06\",11.0\\r\\n\"1985-01-07\",13.2\\r\\n\"1985-01-08\",12.2\\r\\n\"1985-01-09\",14.4\\r\\n\"1985-01-10\",13.7\\r\\n\"1985-01-11\",14.5\\r\\n\"1985-01-12\",14.1\\r\\n\"1985-01-13\",14.4\\r\\n\"1985-01-14\",19.7\\r\\n\"1985-01-15\",16.5\\r\\n\"1985-01-16\",15.9\\r\\n\"1985-01-17\",11.8\\r\\n\"1985-01-18\",12.0\\r\\n\"1985-01-19\",11.4\\r\\n\"1985-01-20\",14.4\\r\\n\"1985-01-21\",12.4\\r\\n\"1985-01-22\",15.1\\r\\n\"1985-01-23\",15.6\\r\\n\"1985-01-24\",15.2\\r\\n\"1985-01-25\",12.8\\r\\n\"1985-01-26\",13.3\\r\\n\"1985-01-27\",17.5\\r\\n\"1985-01-28\",15.4\\r\\n\"1985-01-29\",13.5\\r\\n\"1985-01-30\",16.7\\r\\n\"1985-01-31\",15.2\\r\\n\"1985-02-01\",14.9\\r\\n\"1985-02-02\",10.2\\r\\n\"1985-02-03\",13.6\\r\\n\"1985-02-04\",19.0\\r\\n\"1985-02-05\",15.7\\r\\n\"1985-02-06\",18.0\\r\\n\"1985-02-07\",14.8\\r\\n\"1985-02-08\",13.9\\r\\n\"1985-02-09\",13.0\\r\\n\"1985-02-10\",15.3\\r\\n\"1985-02-11\",14.3\\r\\n\"1985-02-12\",15.6\\r\\n\"1985-02-13\",16.0\\r\\n\"1985-02-14\",14.9\\r\\n\"1985-02-15\",11.1\\r\\n\"1985-02-16\",14.8\\r\\n\"1985-02-17\",13.0\\r\\n\"1985-02-18\",12.2\\r\\n\"1985-02-19\",10.9\\r\\n\"1985-02-20\",14.6\\r\\n\"1985-02-21\",16.6\\r\\n\"1985-02-22\",18.1\\r\\n\"1985-02-23\",13.4\\r\\n\"1985-02-24\",10.3\\r\\n\"1985-02-25\",13.6\\r\\n\"1985-02-26\",13.8\\r\\n\"1985-02-27\",10.3\\r\\n\"1985-02-28\",11.0\\r\\n\"1985-03-01\",14.3\\r\\n\"1985-03-02\",15.5\\r\\n\"1985-03-03\",14.7\\r\\n\"1985-03-04\",12.7\\r\\n\"1985-03-05\",10.7\\r\\n\"1985-03-06\",12.6\\r\\n\"1985-03-07\",9.8\\r\\n\"1985-03-08\",13.2\\r\\n\"1985-03-09\",15.2\\r\\n\"1985-03-10\",16.6\\r\\n\"1985-03-11\",21.0\\r\\n\"1985-03-12\",22.4\\r\\n\"1985-03-13\",17.0\\r\\n\"1985-03-14\",21.7\\r\\n\"1985-03-15\",21.4\\r\\n\"1985-03-16\",18.6\\r\\n\"1985-03-17\",16.2\\r\\n\"1985-03-18\",16.8\\r\\n\"1985-03-19\",17.0\\r\\n\"1985-03-20\",18.4\\r\\n\"1985-03-21\",17.2\\r\\n\"1985-03-22\",18.4\\r\\n\"1985-03-23\",18.8\\r\\n\"1985-03-24\",16.5\\r\\n\"1985-03-25\",13.3\\r\\n\"1985-03-26\",12.2\\r\\n\"1985-03-27\",11.3\\r\\n\"1985-03-28\",13.8\\r\\n\"1985-03-29\",16.6\\r\\n\"1985-03-30\",14.0\\r\\n\"1985-03-31\",14.3\\r\\n\"1985-04-01\",16.4\\r\\n\"1985-04-02\",11.9\\r\\n\"1985-04-03\",15.7\\r\\n\"1985-04-04\",17.6\\r\\n\"1985-04-05\",17.5\\r\\n\"1985-04-06\",15.9\\r\\n\"1985-04-07\",16.2\\r\\n\"1985-04-08\",16.0\\r\\n\"1985-04-09\",15.9\\r\\n\"1985-04-10\",16.2\\r\\n\"1985-04-11\",16.2\\r\\n\"1985-04-12\",19.5\\r\\n\"1985-04-13\",18.2\\r\\n\"1985-04-14\",21.8\\r\\n\"1985-04-15\",15.1\\r\\n\"1985-04-16\",11.0\\r\\n\"1985-04-17\",8.1\\r\\n\"1985-04-18\",9.5\\r\\n\"1985-04-19\",9.3\\r\\n\"1985-04-20\",10.6\\r\\n\"1985-04-21\",6.3\\r\\n\"1985-04-22\",8.6\\r\\n\"1985-04-23\",6.8\\r\\n\"1985-04-24\",8.7\\r\\n\"1985-04-25\",8.4\\r\\n\"1985-04-26\",9.3\\r\\n\"1985-04-27\",10.0\\r\\n\"1985-04-28\",10.5\\r\\n\"1985-04-29\",12.0\\r\\n\"1985-04-30\",10.1\\r\\n\"1985-05-01\",9.4\\r\\n\"1985-05-02\",10.1\\r\\n\"1985-05-03\",8.0\\r\\n\"1985-05-04\",10.6\\r\\n\"1985-05-05\",13.6\\r\\n\"1985-05-06\",15.4\\r\\n\"1985-05-07\",9.0\\r\\n\"1985-05-08\",10.4\\r\\n\"1985-05-09\",11.0\\r\\n\"1985-05-10\",12.1\\r\\n\"1985-05-11\",13.4\\r\\n\"1985-05-12\",11.3\\r\\n\"1985-05-13\",6.7\\r\\n\"1985-05-14\",9.8\\r\\n\"1985-05-15\",10.8\\r\\n\"1985-05-16\",7.8\\r\\n\"1985-05-17\",4.5\\r\\n\"1985-05-18\",7.6\\r\\n\"1985-05-19\",6.9\\r\\n\"1985-05-20\",7.5\\r\\n\"1985-05-21\",8.5\\r\\n\"1985-05-22\",5.5\\r\\n\"1985-05-23\",9.5\\r\\n\"1985-05-24\",7.3\\r\\n\"1985-05-25\",5.4\\r\\n\"1985-05-26\",5.5\\r\\n\"1985-05-27\",8.1\\r\\n\"1985-05-28\",11.2\\r\\n\"1985-05-29\",13.4\\r\\n\"1985-05-30\",11.6\\r\\n\"1985-05-31\",10.1\\r\\n\"1985-06-01\",4.3\\r\\n\"1985-06-02\",5.5\\r\\n\"1985-06-03\",4.4\\r\\n\"1985-06-04\",5.9\\r\\n\"1985-06-05\",5.7\\r\\n\"1985-06-06\",8.2\\r\\n\"1985-06-07\",8.2\\r\\n\"1985-06-08\",4.2\\r\\n\"1985-06-09\",6.5\\r\\n\"1985-06-10\",10.0\\r\\n\"1985-06-11\",8.8\\r\\n\"1985-06-12\",6.6\\r\\n\"1985-06-13\",7.8\\r\\n\"1985-06-14\",10.1\\r\\n\"1985-06-15\",7.1\\r\\n\"1985-06-16\",7.7\\r\\n\"1985-06-17\",8.5\\r\\n\"1985-06-18\",7.3\\r\\n\"1985-06-19\",6.9\\r\\n\"1985-06-20\",8.4\\r\\n\"1985-06-21\",7.1\\r\\n\"1985-06-22\",6.3\\r\\n\"1985-06-23\",0.6\\r\\n\"1985-06-24\",1.6\\r\\n\"1985-06-25\",7.0\\r\\n\"1985-06-26\",8.3\\r\\n\"1985-06-27\",8.0\\r\\n\"1985-06-28\",10.2\\r\\n\"1985-06-29\",10.6\\r\\n\"1985-06-30\",10.4\\r\\n\"1985-07-01\",11.6\\r\\n\"1985-07-02\",11.0\\r\\n\"1985-07-03\",10.7\\r\\n\"1985-07-04\",7.3\\r\\n\"1985-07-05\",4.2\\r\\n\"1985-07-06\",4.7\\r\\n\"1985-07-07\",5.6\\r\\n\"1985-07-08\",7.7\\r\\n\"1985-07-09\",7.5\\r\\n\"1985-07-10\",4.9\\r\\n\"1985-07-11\",5.9\\r\\n\"1985-07-12\",7.8\\r\\n\"1985-07-13\",5.8\\r\\n\"1985-07-14\",7.0\\r\\n\"1985-07-15\",8.4\\r\\n\"1985-07-16\",6.2\\r\\n\"1985-07-17\",7.5\\r\\n\"1985-07-18\",4.8\\r\\n\"1985-07-19\",3.3\\r\\n\"1985-07-20\",3.2\\r\\n\"1985-07-21\",7.0\\r\\n\"1985-07-22\",8.4\\r\\n\"1985-07-23\",0.3\\r\\n\"1985-07-24\",0.3\\r\\n\"1985-07-25\",2.1\\r\\n\"1985-07-26\",8.5\\r\\n\"1985-07-27\",1.4\\r\\n\"1985-07-28\",4.1\\r\\n\"1985-07-29\",10.3\\r\\n\"1985-07-30\",6.6\\r\\n\"1985-07-31\",6.1\\r\\n\"1985-08-01\",7.0\\r\\n\"1985-08-02\",5.1\\r\\n\"1985-08-03\",6.3\\r\\n\"1985-08-04\",6.9\\r\\n\"1985-08-05\",11.4\\r\\n\"1985-08-06\",10.4\\r\\n\"1985-08-07\",10.3\\r\\n\"1985-08-08\",9.2\\r\\n\"1985-08-09\",7.2\\r\\n\"1985-08-10\",7.5\\r\\n\"1985-08-11\",4.0\\r\\n\"1985-08-12\",5.6\\r\\n\"1985-08-13\",6.7\\r\\n\"1985-08-14\",8.4\\r\\n\"1985-08-15\",11.0\\r\\n\"1985-08-16\",8.4\\r\\n\"1985-08-17\",8.8\\r\\n\"1985-08-18\",8.6\\r\\n\"1985-08-19\",8.3\\r\\n\"1985-08-20\",4.0\\r\\n\"1985-08-21\",3.6\\r\\n\"1985-08-22\",5.7\\r\\n\"1985-08-23\",10.6\\r\\n\"1985-08-24\",6.9\\r\\n\"1985-08-25\",10.0\\r\\n\"1985-08-26\",9.8\\r\\n\"1985-08-27\",7.2\\r\\n\"1985-08-28\",10.5\\r\\n\"1985-08-29\",3.6\\r\\n\"1985-08-30\",5.3\\r\\n\"1985-08-31\",8.4\\r\\n\"1985-09-01\",10.3\\r\\n\"1985-09-02\",7.9\\r\\n\"1985-09-03\",8.5\\r\\n\"1985-09-04\",7.9\\r\\n\"1985-09-05\",8.0\\r\\n\"1985-09-06\",9.8\\r\\n\"1985-09-07\",6.7\\r\\n\"1985-09-08\",4.8\\r\\n\"1985-09-09\",9.9\\r\\n\"1985-09-10\",12.8\\r\\n\"1985-09-11\",10.9\\r\\n\"1985-09-12\",11.7\\r\\n\"1985-09-13\",11.7\\r\\n\"1985-09-14\",11.0\\r\\n\"1985-09-15\",8.2\\r\\n\"1985-09-16\",7.5\\r\\n\"1985-09-17\",5.4\\r\\n\"1985-09-18\",7.2\\r\\n\"1985-09-19\",9.7\\r\\n\"1985-09-20\",8.4\\r\\n\"1985-09-21\",9.0\\r\\n\"1985-09-22\",8.7\\r\\n\"1985-09-23\",6.6\\r\\n\"1985-09-24\",11.6\\r\\n\"1985-09-25\",13.1\\r\\n\"1985-09-26\",6.7\\r\\n\"1985-09-27\",6.5\\r\\n\"1985-09-28\",7.7\\r\\n\"1985-09-29\",8.7\\r\\n\"1985-09-30\",7.2\\r\\n\"1985-10-01\",10.5\\r\\n\"1985-10-02\",8.6\\r\\n\"1985-10-03\",7.2\\r\\n\"1985-10-04\",11.4\\r\\n\"1985-10-05\",16.2\\r\\n\"1985-10-06\",6.1\\r\\n\"1985-10-07\",9.6\\r\\n\"1985-10-08\",11.1\\r\\n\"1985-10-09\",13.6\\r\\n\"1985-10-10\",10.7\\r\\n\"1985-10-11\",14.7\\r\\n\"1985-10-12\",11.6\\r\\n\"1985-10-13\",7.3\\r\\n\"1985-10-14\",8.0\\r\\n\"1985-10-15\",9.6\\r\\n\"1985-10-16\",16.0\\r\\n\"1985-10-17\",15.1\\r\\n\"1985-10-18\",12.8\\r\\n\"1985-10-19\",6.2\\r\\n\"1985-10-20\",7.1\\r\\n\"1985-10-21\",8.4\\r\\n\"1985-10-22\",10.0\\r\\n\"1985-10-23\",12.7\\r\\n\"1985-10-24\",10.0\\r\\n\"1985-10-25\",10.2\\r\\n\"1985-10-26\",6.5\\r\\n\"1985-10-27\",9.2\\r\\n\"1985-10-28\",11.9\\r\\n\"1985-10-29\",14.7\\r\\n\"1985-10-30\",11.4\\r\\n\"1985-10-31\",6.8\\r\\n\"1985-11-01\",7.4\\r\\n\"1985-11-02\",11.2\\r\\n\"1985-11-03\",9.2\\r\\n\"1985-11-04\",12.6\\r\\n\"1985-11-05\",16.0\\r\\n\"1985-11-06\",17.1\\r\\n\"1985-11-07\",15.3\\r\\n\"1985-11-08\",13.3\\r\\n\"1985-11-09\",15.4\\r\\n\"1985-11-10\",13.2\\r\\n\"1985-11-11\",14.4\\r\\n\"1985-11-12\",14.0\\r\\n\"1985-11-13\",15.5\\r\\n\"1985-11-14\",21.0\\r\\n\"1985-11-15\",10.0\\r\\n\"1985-11-16\",9.6\\r\\n\"1985-11-17\",12.0\\r\\n\"1985-11-18\",12.2\\r\\n\"1985-11-19\",11.3\\r\\n\"1985-11-20\",13.2\\r\\n\"1985-11-21\",10.5\\r\\n\"1985-11-22\",10.1\\r\\n\"1985-11-23\",8.8\\r\\n\"1985-11-24\",13.7\\r\\n\"1985-11-25\",16.2\\r\\n\"1985-11-26\",16.0\\r\\n\"1985-11-27\",14.0\\r\\n\"1985-11-28\",13.7\\r\\n\"1985-11-29\",12.5\\r\\n\"1985-11-30\",12.8\\r\\n\"1985-12-01\",12.3\\r\\n\"1985-12-02\",15.2\\r\\n\"1985-12-03\",15.0\\r\\n\"1985-12-04\",16.4\\r\\n\"1985-12-05\",16.1\\r\\n\"1985-12-06\",14.6\\r\\n\"1985-12-07\",18.2\\r\\n\"1985-12-08\",16.4\\r\\n\"1985-12-09\",16.6\\r\\n\"1985-12-10\",14.7\\r\\n\"1985-12-11\",15.8\\r\\n\"1985-12-12\",14.1\\r\\n\"1985-12-13\",13.5\\r\\n\"1985-12-14\",13.6\\r\\n\"1985-12-15\",13.7\\r\\n\"1985-12-16\",13.6\\r\\n\"1985-12-17\",12.1\\r\\n\"1985-12-18\",12.7\\r\\n\"1985-12-19\",13.3\\r\\n\"1985-12-20\",14.2\\r\\n\"1985-12-21\",15.0\\r\\n\"1985-12-22\",13.7\\r\\n\"1985-12-23\",12.0\\r\\n\"1985-12-24\",13.1\\r\\n\"1985-12-25\",13.2\\r\\n\"1985-12-26\",13.3\\r\\n\"1985-12-27\",11.5\\r\\n\"1985-12-28\",10.8\\r\\n\"1985-12-29\",12.0\\r\\n\"1985-12-30\",16.3\\r\\n\"1985-12-31\",14.4\\r\\n\"1986-01-01\",12.9\\r\\n\"1986-01-02\",13.8\\r\\n\"1986-01-03\",10.6\\r\\n\"1986-01-04\",12.6\\r\\n\"1986-01-05\",13.7\\r\\n\"1986-01-06\",12.6\\r\\n\"1986-01-07\",13.1\\r\\n\"1986-01-08\",15.4\\r\\n\"1986-01-09\",11.9\\r\\n\"1986-01-10\",13.8\\r\\n\"1986-01-11\",14.4\\r\\n\"1986-01-12\",15.2\\r\\n\"1986-01-13\",12.5\\r\\n\"1986-01-14\",12.2\\r\\n\"1986-01-15\",16.1\\r\\n\"1986-01-16\",14.6\\r\\n\"1986-01-17\",11.6\\r\\n\"1986-01-18\",13.1\\r\\n\"1986-01-19\",12.8\\r\\n\"1986-01-20\",15.2\\r\\n\"1986-01-21\",13.8\\r\\n\"1986-01-22\",15.0\\r\\n\"1986-01-23\",13.5\\r\\n\"1986-01-24\",11.8\\r\\n\"1986-01-25\",15.3\\r\\n\"1986-01-26\",13.5\\r\\n\"1986-01-27\",15.3\\r\\n\"1986-01-28\",13.8\\r\\n\"1986-01-29\",15.8\\r\\n\"1986-01-30\",17.4\\r\\n\"1986-01-31\",15.3\\r\\n\"1986-02-01\",14.6\\r\\n\"1986-02-02\",14.8\\r\\n\"1986-02-03\",10.7\\r\\n\"1986-02-04\",11.6\\r\\n\"1986-02-05\",13.6\\r\\n\"1986-02-06\",14.4\\r\\n\"1986-02-07\",11.8\\r\\n\"1986-02-08\",15.8\\r\\n\"1986-02-09\",16.0\\r\\n\"1986-02-10\",11.8\\r\\n\"1986-02-11\",14.5\\r\\n\"1986-02-12\",10.7\\r\\n\"1986-02-13\",14.2\\r\\n\"1986-02-14\",19.5\\r\\n\"1986-02-15\",21.4\\r\\n\"1986-02-16\",17.9\\r\\n\"1986-02-17\",17.4\\r\\n\"1986-02-18\",12.7\\r\\n\"1986-02-19\",13.8\\r\\n\"1986-02-20\",14.0\\r\\n\"1986-02-21\",15.0\\r\\n\"1986-02-22\",14.5\\r\\n\"1986-02-23\",13.1\\r\\n\"1986-02-24\",11.4\\r\\n\"1986-02-25\",12.5\\r\\n\"1986-02-26\",12.0\\r\\n\"1986-02-27\",13.4\\r\\n\"1986-02-28\",14.4\\r\\n\"1986-03-01\",17.7\\r\\n\"1986-03-02\",13.9\\r\\n\"1986-03-03\",13.3\\r\\n\"1986-03-04\",14.6\\r\\n\"1986-03-05\",16.4\\r\\n\"1986-03-06\",16.8\\r\\n\"1986-03-07\",20.0\\r\\n\"1986-03-08\",12.5\\r\\n\"1986-03-09\",12.7\\r\\n\"1986-03-10\",11.7\\r\\n\"1986-03-11\",12.7\\r\\n\"1986-03-12\",8.6\\r\\n\"1986-03-13\",11.9\\r\\n\"1986-03-14\",16.0\\r\\n\"1986-03-15\",15.2\\r\\n\"1986-03-16\",13.4\\r\\n\"1986-03-17\",11.6\\r\\n\"1986-03-18\",11.1\\r\\n\"1986-03-19\",15.6\\r\\n\"1986-03-20\",17.0\\r\\n\"1986-03-21\",18.5\\r\\n\"1986-03-22\",17.4\\r\\n\"1986-03-23\",16.5\\r\\n\"1986-03-24\",16.2\\r\\n\"1986-03-25\",16.1\\r\\n\"1986-03-26\",13.2\\r\\n\"1986-03-27\",18.0\\r\\n\"1986-03-28\",12.8\\r\\n\"1986-03-29\",11.7\\r\\n\"1986-03-30\",16.7\\r\\n\"1986-03-31\",15.6\\r\\n\"1986-04-01\",10.2\\r\\n\"1986-04-02\",10.3\\r\\n\"1986-04-03\",15.0\\r\\n\"1986-04-04\",18.0\\r\\n\"1986-04-05\",13.8\\r\\n\"1986-04-06\",10.5\\r\\n\"1986-04-07\",11.8\\r\\n\"1986-04-08\",7.2\\r\\n\"1986-04-09\",11.6\\r\\n\"1986-04-10\",7.4\\r\\n\"1986-04-11\",14.2\\r\\n\"1986-04-12\",12.2\\r\\n\"1986-04-13\",9.0\\r\\n\"1986-04-14\",12.3\\r\\n\"1986-04-15\",19.7\\r\\n\"1986-04-16\",12.8\\r\\n\"1986-04-17\",12.4\\r\\n\"1986-04-18\",12.0\\r\\n\"1986-04-19\",12.0\\r\\n\"1986-04-20\",11.1\\r\\n\"1986-04-21\",12.7\\r\\n\"1986-04-22\",14.2\\r\\n\"1986-04-23\",11.6\\r\\n\"1986-04-24\",12.0\\r\\n\"1986-04-25\",11.5\\r\\n\"1986-04-26\",8.3\\r\\n\"1986-04-27\",10.5\\r\\n\"1986-04-28\",9.0\\r\\n\"1986-04-29\",6.9\\r\\n\"1986-04-30\",9.4\\r\\n\"1986-05-01\",11.1\\r\\n\"1986-05-02\",9.1\\r\\n\"1986-05-03\",7.7\\r\\n\"1986-05-04\",10.0\\r\\n\"1986-05-05\",10.4\\r\\n\"1986-05-06\",8.0\\r\\n\"1986-05-07\",9.8\\r\\n\"1986-05-08\",12.4\\r\\n\"1986-05-09\",12.9\\r\\n\"1986-05-10\",12.3\\r\\n\"1986-05-11\",6.9\\r\\n\"1986-05-12\",10.5\\r\\n\"1986-05-13\",11.0\\r\\n\"1986-05-14\",9.7\\r\\n\"1986-05-15\",11.1\\r\\n\"1986-05-16\",11.5\\r\\n\"1986-05-17\",13.4\\r\\n\"1986-05-18\",10.9\\r\\n\"1986-05-19\",12.0\\r\\n\"1986-05-20\",12.1\\r\\n\"1986-05-21\",10.4\\r\\n\"1986-05-22\",10.0\\r\\n\"1986-05-23\",9.6\\r\\n\"1986-05-24\",11.3\\r\\n\"1986-05-25\",8.5\\r\\n\"1986-05-26\",6.3\\r\\n\"1986-05-27\",8.2\\r\\n\"1986-05-28\",10.7\\r\\n\"1986-05-29\",10.3\\r\\n\"1986-05-30\",9.5\\r\\n\"1986-05-31\",10.9\\r\\n\"1986-06-01\",10.9\\r\\n\"1986-06-02\",4.3\\r\\n\"1986-06-03\",5.2\\r\\n\"1986-06-04\",11.0\\r\\n\"1986-06-05\",11.6\\r\\n\"1986-06-06\",10.6\\r\\n\"1986-06-07\",9.4\\r\\n\"1986-06-08\",10.0\\r\\n\"1986-06-09\",9.6\\r\\n\"1986-06-10\",9.5\\r\\n\"1986-06-11\",9.7\\r\\n\"1986-06-12\",9.6\\r\\n\"1986-06-13\",7.0\\r\\n\"1986-06-14\",7.0\\r\\n\"1986-06-15\",6.8\\r\\n\"1986-06-16\",6.9\\r\\n\"1986-06-17\",8.0\\r\\n\"1986-06-18\",7.6\\r\\n\"1986-06-19\",8.6\\r\\n\"1986-06-20\",5.7\\r\\n\"1986-06-21\",5.5\\r\\n\"1986-06-22\",5.7\\r\\n\"1986-06-23\",5.7\\r\\n\"1986-06-24\",6.6\\r\\n\"1986-06-25\",6.0\\r\\n\"1986-06-26\",6.9\\r\\n\"1986-06-27\",7.7\\r\\n\"1986-06-28\",8.0\\r\\n\"1986-06-29\",3.9\\r\\n\"1986-06-30\",0.8\\r\\n\"1986-07-01\",2.8\\r\\n\"1986-07-02\",8.0\\r\\n\"1986-07-03\",9.8\\r\\n\"1986-07-04\",11.4\\r\\n\"1986-07-05\",8.6\\r\\n\"1986-07-06\",5.2\\r\\n\"1986-07-07\",6.6\\r\\n\"1986-07-08\",5.7\\r\\n\"1986-07-09\",4.6\\r\\n\"1986-07-10\",5.8\\r\\n\"1986-07-11\",7.0\\r\\n\"1986-07-12\",4.8\\r\\n\"1986-07-13\",4.4\\r\\n\"1986-07-14\",4.4\\r\\n\"1986-07-15\",7.9\\r\\n\"1986-07-16\",10.6\\r\\n\"1986-07-17\",5.0\\r\\n\"1986-07-18\",7.6\\r\\n\"1986-07-19\",9.2\\r\\n\"1986-07-20\",9.7\\r\\n\"1986-07-21\",8.8\\r\\n\"1986-07-22\",6.8\\r\\n\"1986-07-23\",9.4\\r\\n\"1986-07-24\",11.0\\r\\n\"1986-07-25\",2.5\\r\\n\"1986-07-26\",2.1\\r\\n\"1986-07-27\",5.4\\r\\n\"1986-07-28\",6.2\\r\\n\"1986-07-29\",7.8\\r\\n\"1986-07-30\",7.4\\r\\n\"1986-07-31\",9.3\\r\\n\"1986-08-01\",9.3\\r\\n\"1986-08-02\",9.5\\r\\n\"1986-08-03\",8.5\\r\\n\"1986-08-04\",10.0\\r\\n\"1986-08-05\",7.7\\r\\n\"1986-08-06\",9.3\\r\\n\"1986-08-07\",9.1\\r\\n\"1986-08-08\",3.5\\r\\n\"1986-08-09\",3.6\\r\\n\"1986-08-10\",2.5\\r\\n\"1986-08-11\",1.7\\r\\n\"1986-08-12\",2.7\\r\\n\"1986-08-13\",2.9\\r\\n\"1986-08-14\",5.3\\r\\n\"1986-08-15\",7.7\\r\\n\"1986-08-16\",9.1\\r\\n\"1986-08-17\",9.4\\r\\n\"1986-08-18\",7.3\\r\\n\"1986-08-19\",8.4\\r\\n\"1986-08-20\",9.2\\r\\n\"1986-08-21\",6.6\\r\\n\"1986-08-22\",9.7\\r\\n\"1986-08-23\",12.4\\r\\n\"1986-08-24\",10.2\\r\\n\"1986-08-25\",5.9\\r\\n\"1986-08-26\",7.1\\r\\n\"1986-08-27\",7.5\\r\\n\"1986-08-28\",9.7\\r\\n\"1986-08-29\",12.2\\r\\n\"1986-08-30\",5.6\\r\\n\"1986-08-31\",5.4\\r\\n\"1986-09-01\",8.3\\r\\n\"1986-09-02\",10.6\\r\\n\"1986-09-03\",9.1\\r\\n\"1986-09-04\",11.3\\r\\n\"1986-09-05\",10.9\\r\\n\"1986-09-06\",8.9\\r\\n\"1986-09-07\",6.3\\r\\n\"1986-09-08\",9.0\\r\\n\"1986-09-09\",6.1\\r\\n\"1986-09-10\",9.1\\r\\n\"1986-09-11\",9.6\\r\\n\"1986-09-12\",6.0\\r\\n\"1986-09-13\",10.0\\r\\n\"1986-09-14\",11.0\\r\\n\"1986-09-15\",6.2\\r\\n\"1986-09-16\",8.3\\r\\n\"1986-09-17\",11.3\\r\\n\"1986-09-18\",11.3\\r\\n\"1986-09-19\",6.7\\r\\n\"1986-09-20\",6.6\\r\\n\"1986-09-21\",11.4\\r\\n\"1986-09-22\",6.9\\r\\n\"1986-09-23\",10.6\\r\\n\"1986-09-24\",8.6\\r\\n\"1986-09-25\",11.3\\r\\n\"1986-09-26\",12.5\\r\\n\"1986-09-27\",9.9\\r\\n\"1986-09-28\",6.9\\r\\n\"1986-09-29\",5.5\\r\\n\"1986-09-30\",7.8\\r\\n\"1986-10-01\",11.0\\r\\n\"1986-10-02\",16.2\\r\\n\"1986-10-03\",9.9\\r\\n\"1986-10-04\",8.7\\r\\n\"1986-10-05\",10.5\\r\\n\"1986-10-06\",12.2\\r\\n\"1986-10-07\",10.6\\r\\n\"1986-10-08\",8.3\\r\\n\"1986-10-09\",5.5\\r\\n\"1986-10-10\",9.0\\r\\n\"1986-10-11\",6.4\\r\\n\"1986-10-12\",7.2\\r\\n\"1986-10-13\",12.9\\r\\n\"1986-10-14\",12.0\\r\\n\"1986-10-15\",7.3\\r\\n\"1986-10-16\",9.7\\r\\n\"1986-10-17\",8.4\\r\\n\"1986-10-18\",14.7\\r\\n\"1986-10-19\",9.5\\r\\n\"1986-10-20\",7.9\\r\\n\"1986-10-21\",6.8\\r\\n\"1986-10-22\",12.6\\r\\n\"1986-10-23\",5.2\\r\\n\"1986-10-24\",7.5\\r\\n\"1986-10-25\",8.7\\r\\n\"1986-10-26\",7.6\\r\\n\"1986-10-27\",9.0\\r\\n\"1986-10-28\",7.2\\r\\n\"1986-10-29\",10.7\\r\\n\"1986-10-30\",13.1\\r\\n\"1986-10-31\",13.9\\r\\n\"1986-11-01\",10.8\\r\\n\"1986-11-02\",10.4\\r\\n\"1986-11-03\",9.1\\r\\n\"1986-11-04\",16.0\\r\\n\"1986-11-05\",21.0\\r\\n\"1986-11-06\",16.2\\r\\n\"1986-11-07\",8.6\\r\\n\"1986-11-08\",9.2\\r\\n\"1986-11-09\",12.5\\r\\n\"1986-11-10\",9.7\\r\\n\"1986-11-11\",12.5\\r\\n\"1986-11-12\",10.3\\r\\n\"1986-11-13\",12.0\\r\\n\"1986-11-14\",11.0\\r\\n\"1986-11-15\",14.8\\r\\n\"1986-11-16\",15.0\\r\\n\"1986-11-17\",15.3\\r\\n\"1986-11-18\",10.3\\r\\n\"1986-11-19\",10.7\\r\\n\"1986-11-20\",10.5\\r\\n\"1986-11-21\",8.9\\r\\n\"1986-11-22\",8.1\\r\\n\"1986-11-23\",11.5\\r\\n\"1986-11-24\",12.8\\r\\n\"1986-11-25\",9.1\\r\\n\"1986-11-26\",14.6\\r\\n\"1986-11-27\",11.6\\r\\n\"1986-11-28\",11.2\\r\\n\"1986-11-29\",12.6\\r\\n\"1986-11-30\",7.5\\r\\n\"1986-12-01\",11.0\\r\\n\"1986-12-02\",14.5\\r\\n\"1986-12-03\",18.5\\r\\n\"1986-12-04\",15.4\\r\\n\"1986-12-05\",13.1\\r\\n\"1986-12-06\",16.3\\r\\n\"1986-12-07\",20.2\\r\\n\"1986-12-08\",11.5\\r\\n\"1986-12-09\",12.4\\r\\n\"1986-12-10\",10.9\\r\\n\"1986-12-11\",12.7\\r\\n\"1986-12-12\",12.2\\r\\n\"1986-12-13\",12.4\\r\\n\"1986-12-14\",9.8\\r\\n\"1986-12-15\",8.5\\r\\n\"1986-12-16\",14.7\\r\\n\"1986-12-17\",12.0\\r\\n\"1986-12-18\",10.3\\r\\n\"1986-12-19\",11.0\\r\\n\"1986-12-20\",10.2\\r\\n\"1986-12-21\",12.6\\r\\n\"1986-12-22\",11.6\\r\\n\"1986-12-23\",9.7\\r\\n\"1986-12-24\",13.4\\r\\n\"1986-12-25\",10.5\\r\\n\"1986-12-26\",14.7\\r\\n\"1986-12-27\",14.6\\r\\n\"1986-12-28\",14.2\\r\\n\"1986-12-29\",13.2\\r\\n\"1986-12-30\",11.7\\r\\n\"1986-12-31\",17.2\\r\\n\"1987-01-01\",12.3\\r\\n\"1987-01-02\",13.8\\r\\n\"1987-01-03\",15.3\\r\\n\"1987-01-04\",15.6\\r\\n\"1987-01-05\",16.2\\r\\n\"1987-01-06\",16.3\\r\\n\"1987-01-07\",16.8\\r\\n\"1987-01-08\",11.0\\r\\n\"1987-01-09\",8.5\\r\\n\"1987-01-10\",13.2\\r\\n\"1987-01-11\",13.0\\r\\n\"1987-01-12\",12.4\\r\\n\"1987-01-13\",13.0\\r\\n\"1987-01-14\",16.6\\r\\n\"1987-01-15\",12.0\\r\\n\"1987-01-16\",12.4\\r\\n\"1987-01-17\",15.0\\r\\n\"1987-01-18\",11.8\\r\\n\"1987-01-19\",11.6\\r\\n\"1987-01-20\",12.2\\r\\n\"1987-01-21\",13.7\\r\\n\"1987-01-22\",11.2\\r\\n\"1987-01-23\",12.4\\r\\n\"1987-01-24\",11.5\\r\\n\"1987-01-25\",13.8\\r\\n\"1987-01-26\",15.7\\r\\n\"1987-01-27\",12.9\\r\\n\"1987-01-28\",11.5\\r\\n\"1987-01-29\",11.0\\r\\n\"1987-01-30\",12.7\\r\\n\"1987-01-31\",14.9\\r\\n\"1987-02-01\",16.5\\r\\n\"1987-02-02\",12.8\\r\\n\"1987-02-03\",12.7\\r\\n\"1987-02-04\",12.7\\r\\n\"1987-02-05\",11.6\\r\\n\"1987-02-06\",13.3\\r\\n\"1987-02-07\",15.2\\r\\n\"1987-02-08\",16.4\\r\\n\"1987-02-09\",11.9\\r\\n\"1987-02-10\",15.1\\r\\n\"1987-02-11\",10.6\\r\\n\"1987-02-12\",13.6\\r\\n\"1987-02-13\",12.1\\r\\n\"1987-02-14\",16.0\\r\\n\"1987-02-15\",16.8\\r\\n\"1987-02-16\",16.6\\r\\n\"1987-02-17\",15.6\\r\\n\"1987-02-18\",15.2\\r\\n\"1987-02-19\",17.7\\r\\n\"1987-02-20\",21.0\\r\\n\"1987-02-21\",13.4\\r\\n\"1987-02-22\",10.5\\r\\n\"1987-02-23\",9.5\\r\\n\"1987-02-24\",12.0\\r\\n\"1987-02-25\",10.4\\r\\n\"1987-02-26\",11.5\\r\\n\"1987-02-27\",13.2\\r\\n\"1987-02-28\",15.0\\r\\n\"1987-03-01\",14.1\\r\\n\"1987-03-02\",12.4\\r\\n\"1987-03-03\",13.4\\r\\n\"1987-03-04\",12.5\\r\\n\"1987-03-05\",14.3\\r\\n\"1987-03-06\",17.6\\r\\n\"1987-03-07\",10.4\\r\\n\"1987-03-08\",9.9\\r\\n\"1987-03-09\",10.2\\r\\n\"1987-03-10\",11.3\\r\\n\"1987-03-11\",9.5\\r\\n\"1987-03-12\",11.8\\r\\n\"1987-03-13\",11.5\\r\\n\"1987-03-14\",10.5\\r\\n\"1987-03-15\",10.8\\r\\n\"1987-03-16\",13.0\\r\\n\"1987-03-17\",18.5\\r\\n\"1987-03-18\",18.7\\r\\n\"1987-03-19\",15.0\\r\\n\"1987-03-20\",13.0\\r\\n\"1987-03-21\",11.3\\r\\n\"1987-03-22\",13.0\\r\\n\"1987-03-23\",13.3\\r\\n\"1987-03-24\",11.0\\r\\n\"1987-03-25\",10.3\\r\\n\"1987-03-26\",13.0\\r\\n\"1987-03-27\",12.3\\r\\n\"1987-03-28\",15.6\\r\\n\"1987-03-29\",10.2\\r\\n\"1987-03-30\",10.8\\r\\n\"1987-03-31\",12.0\\r\\n\"1987-04-01\",13.3\\r\\n\"1987-04-02\",11.7\\r\\n\"1987-04-03\",12.5\\r\\n\"1987-04-04\",13.7\\r\\n\"1987-04-05\",14.9\\r\\n\"1987-04-06\",20.2\\r\\n\"1987-04-07\",16.3\\r\\n\"1987-04-08\",13.9\\r\\n\"1987-04-09\",10.1\\r\\n\"1987-04-10\",7.3\\r\\n\"1987-04-11\",14.0\\r\\n\"1987-04-12\",17.7\\r\\n\"1987-04-13\",16.3\\r\\n\"1987-04-14\",10.6\\r\\n\"1987-04-15\",9.7\\r\\n\"1987-04-16\",7.8\\r\\n\"1987-04-17\",10.4\\r\\n\"1987-04-18\",10.4\\r\\n\"1987-04-19\",14.1\\r\\n\"1987-04-20\",7.1\\r\\n\"1987-04-21\",8.1\\r\\n\"1987-04-22\",7.8\\r\\n\"1987-04-23\",10.6\\r\\n\"1987-04-24\",9.1\\r\\n\"1987-04-25\",9.0\\r\\n\"1987-04-26\",11.9\\r\\n\"1987-04-27\",17.1\\r\\n\"1987-04-28\",16.8\\r\\n\"1987-04-29\",13.5\\r\\n\"1987-04-30\",11.6\\r\\n\"1987-05-01\",7.0\\r\\n\"1987-05-02\",9.7\\r\\n\"1987-05-03\",9.9\\r\\n\"1987-05-04\",11.2\\r\\n\"1987-05-05\",11.3\\r\\n\"1987-05-06\",11.8\\r\\n\"1987-05-07\",9.9\\r\\n\"1987-05-08\",7.1\\r\\n\"1987-05-09\",9.6\\r\\n\"1987-05-10\",9.8\\r\\n\"1987-05-11\",10.6\\r\\n\"1987-05-12\",12.8\\r\\n\"1987-05-13\",16.5\\r\\n\"1987-05-14\",11.7\\r\\n\"1987-05-15\",12.3\\r\\n\"1987-05-16\",12.2\\r\\n\"1987-05-17\",11.8\\r\\n\"1987-05-18\",10.7\\r\\n\"1987-05-19\",10.2\\r\\n\"1987-05-20\",10.0\\r\\n\"1987-05-21\",8.3\\r\\n\"1987-05-22\",6.6\\r\\n\"1987-05-23\",9.5\\r\\n\"1987-05-24\",12.3\\r\\n\"1987-05-25\",7.6\\r\\n\"1987-05-26\",9.3\\r\\n\"1987-05-27\",5.0\\r\\n\"1987-05-28\",4.3\\r\\n\"1987-05-29\",6.4\\r\\n\"1987-05-30\",10.8\\r\\n\"1987-05-31\",7.8\\r\\n\"1987-06-01\",8.5\\r\\n\"1987-06-02\",9.7\\r\\n\"1987-06-03\",10.0\\r\\n\"1987-06-04\",11.0\\r\\n\"1987-06-05\",10.2\\r\\n\"1987-06-06\",6.6\\r\\n\"1987-06-07\",6.1\\r\\n\"1987-06-08\",5.9\\r\\n\"1987-06-09\",8.9\\r\\n\"1987-06-10\",13.0\\r\\n\"1987-06-11\",12.6\\r\\n\"1987-06-12\",5.4\\r\\n\"1987-06-13\",6.0\\r\\n\"1987-06-14\",7.8\\r\\n\"1987-06-15\",9.0\\r\\n\"1987-06-16\",4.2\\r\\n\"1987-06-17\",3.0\\r\\n\"1987-06-18\",4.5\\r\\n\"1987-06-19\",6.2\\r\\n\"1987-06-20\",11.9\\r\\n\"1987-06-21\",11.8\\r\\n\"1987-06-22\",9.4\\r\\n\"1987-06-23\",9.6\\r\\n\"1987-06-24\",9.4\\r\\n\"1987-06-25\",7.0\\r\\n\"1987-06-26\",8.9\\r\\n\"1987-06-27\",9.3\\r\\n\"1987-06-28\",6.8\\r\\n\"1987-06-29\",7.5\\r\\n\"1987-06-30\",8.0\\r\\n\"1987-07-01\",8.3\\r\\n\"1987-07-02\",2.7\\r\\n\"1987-07-03\",3.9\\r\\n\"1987-07-04\",4.1\\r\\n\"1987-07-05\",5.0\\r\\n\"1987-07-06\",5.8\\r\\n\"1987-07-07\",4.4\\r\\n\"1987-07-08\",4.1\\r\\n\"1987-07-09\",5.8\\r\\n\"1987-07-10\",9.1\\r\\n\"1987-07-11\",7.9\\r\\n\"1987-07-12\",5.0\\r\\n\"1987-07-13\",2.8\\r\\n\"1987-07-14\",4.7\\r\\n\"1987-07-15\",8.9\\r\\n\"1987-07-16\",5.4\\r\\n\"1987-07-17\",7.1\\r\\n\"1987-07-18\",9.0\\r\\n\"1987-07-19\",9.4\\r\\n\"1987-07-20\",6.3\\r\\n\"1987-07-21\",7.0\\r\\n\"1987-07-22\",6.4\\r\\n\"1987-07-23\",6.7\\r\\n\"1987-07-24\",1.5\\r\\n\"1987-07-25\",2.9\\r\\n\"1987-07-26\",4.8\\r\\n\"1987-07-27\",6.3\\r\\n\"1987-07-28\",5.7\\r\\n\"1987-07-29\",7.0\\r\\n\"1987-07-30\",8.8\\r\\n\"1987-07-31\",8.7\\r\\n\"1987-08-01\",9.0\\r\\n\"1987-08-02\",9.6\\r\\n\"1987-08-03\",8.0\\r\\n\"1987-08-04\",8.4\\r\\n\"1987-08-05\",8.1\\r\\n\"1987-08-06\",9.0\\r\\n\"1987-08-07\",5.3\\r\\n\"1987-08-08\",8.9\\r\\n\"1987-08-09\",8.7\\r\\n\"1987-08-10\",4.9\\r\\n\"1987-08-11\",7.0\\r\\n\"1987-08-12\",7.5\\r\\n\"1987-08-13\",7.0\\r\\n\"1987-08-14\",9.1\\r\\n\"1987-08-15\",11.8\\r\\n\"1987-08-16\",9.9\\r\\n\"1987-08-17\",5.6\\r\\n\"1987-08-18\",4.2\\r\\n\"1987-08-19\",4.3\\r\\n\"1987-08-20\",8.0\\r\\n\"1987-08-21\",5.1\\r\\n\"1987-08-22\",9.4\\r\\n\"1987-08-23\",9.1\\r\\n\"1987-08-24\",9.7\\r\\n\"1987-08-25\",10.6\\r\\n\"1987-08-26\",8.6\\r\\n\"1987-08-27\",10.1\\r\\n\"1987-08-28\",11.0\\r\\n\"1987-08-29\",9.7\\r\\n\"1987-08-30\",5.0\\r\\n\"1987-08-31\",6.1\\r\\n\"1987-09-01\",5.4\\r\\n\"1987-09-02\",5.8\\r\\n\"1987-09-03\",7.3\\r\\n\"1987-09-04\",6.3\\r\\n\"1987-09-05\",4.8\\r\\n\"1987-09-06\",7.6\\r\\n\"1987-09-07\",8.1\\r\\n\"1987-09-08\",9.5\\r\\n\"1987-09-09\",10.3\\r\\n\"1987-09-10\",7.0\\r\\n\"1987-09-11\",9.0\\r\\n\"1987-09-12\",10.2\\r\\n\"1987-09-13\",6.8\\r\\n\"1987-09-14\",9.3\\r\\n\"1987-09-15\",9.8\\r\\n\"1987-09-16\",10.7\\r\\n\"1987-09-17\",7.8\\r\\n\"1987-09-18\",9.2\\r\\n\"1987-09-19\",15.0\\r\\n\"1987-09-20\",7.8\\r\\n\"1987-09-21\",5.3\\r\\n\"1987-09-22\",9.5\\r\\n\"1987-09-23\",7.6\\r\\n\"1987-09-24\",14.0\\r\\n\"1987-09-25\",14.9\\r\\n\"1987-09-26\",14.9\\r\\n\"1987-09-27\",19.2\\r\\n\"1987-09-28\",17.0\\r\\n\"1987-09-29\",13.0\\r\\n\"1987-09-30\",11.2\\r\\n\"1987-10-01\",9.5\\r\\n\"1987-10-02\",10.3\\r\\n\"1987-10-03\",9.3\\r\\n\"1987-10-04\",11.3\\r\\n\"1987-10-05\",6.5\\r\\n\"1987-10-06\",12.0\\r\\n\"1987-10-07\",8.3\\r\\n\"1987-10-08\",8.7\\r\\n\"1987-10-09\",8.7\\r\\n\"1987-10-10\",10.2\\r\\n\"1987-10-11\",6.9\\r\\n\"1987-10-12\",4.9\\r\\n\"1987-10-13\",10.0\\r\\n\"1987-10-14\",7.6\\r\\n\"1987-10-15\",14.5\\r\\n\"1987-10-16\",13.2\\r\\n\"1987-10-17\",9.9\\r\\n\"1987-10-18\",10.1\\r\\n\"1987-10-19\",11.3\\r\\n\"1987-10-20\",10.4\\r\\n\"1987-10-21\",10.9\\r\\n\"1987-10-22\",9.2\\r\\n\"1987-10-23\",10.5\\r\\n\"1987-10-24\",11.4\\r\\n\"1987-10-25\",13.5\\r\\n\"1987-10-26\",9.8\\r\\n\"1987-10-27\",13.1\\r\\n\"1987-10-28\",9.7\\r\\n\"1987-10-29\",11.4\\r\\n\"1987-10-30\",9.9\\r\\n\"1987-10-31\",14.4\\r\\n\"1987-11-01\",19.0\\r\\n\"1987-11-02\",23.0\\r\\n\"1987-11-03\",15.4\\r\\n\"1987-11-04\",9.6\\r\\n\"1987-11-05\",10.8\\r\\n\"1987-11-06\",12.1\\r\\n\"1987-11-07\",11.0\\r\\n\"1987-11-08\",12.6\\r\\n\"1987-11-09\",14.7\\r\\n\"1987-11-10\",11.1\\r\\n\"1987-11-11\",10.1\\r\\n\"1987-11-12\",11.4\\r\\n\"1987-11-13\",13.0\\r\\n\"1987-11-14\",11.9\\r\\n\"1987-11-15\",9.5\\r\\n\"1987-11-16\",13.5\\r\\n\"1987-11-17\",15.2\\r\\n\"1987-11-18\",18.4\\r\\n\"1987-11-19\",24.1\\r\\n\"1987-11-20\",14.1\\r\\n\"1987-11-21\",10.7\\r\\n\"1987-11-22\",8.7\\r\\n\"1987-11-23\",13.3\\r\\n\"1987-11-24\",11.6\\r\\n\"1987-11-25\",9.9\\r\\n\"1987-11-26\",10.8\\r\\n\"1987-11-27\",11.5\\r\\n\"1987-11-28\",10.0\\r\\n\"1987-11-29\",13.9\\r\\n\"1987-11-30\",13.6\\r\\n\"1987-12-01\",11.9\\r\\n\"1987-12-02\",11.1\\r\\n\"1987-12-03\",8.2\\r\\n\"1987-12-04\",9.4\\r\\n\"1987-12-05\",12.7\\r\\n\"1987-12-06\",11.6\\r\\n\"1987-12-07\",11.0\\r\\n\"1987-12-08\",11.3\\r\\n\"1987-12-09\",13.4\\r\\n\"1987-12-10\",14.9\\r\\n\"1987-12-11\",15.2\\r\\n\"1987-12-12\",13.9\\r\\n\"1987-12-13\",15.0\\r\\n\"1987-12-14\",16.2\\r\\n\"1987-12-15\",17.7\\r\\n\"1987-12-16\",20.5\\r\\n\"1987-12-17\",14.7\\r\\n\"1987-12-18\",12.5\\r\\n\"1987-12-19\",10.9\\r\\n\"1987-12-20\",12.8\\r\\n\"1987-12-21\",12.7\\r\\n\"1987-12-22\",11.2\\r\\n\"1987-12-23\",11.4\\r\\n\"1987-12-24\",11.2\\r\\n\"1987-12-25\",12.1\\r\\n\"1987-12-26\",12.7\\r\\n\"1987-12-27\",16.2\\r\\n\"1987-12-28\",14.2\\r\\n\"1987-12-29\",14.3\\r\\n\"1987-12-30\",13.3\\r\\n\"1987-12-31\",16.7\\r\\n\"1988-01-01\",15.3\\r\\n\"1988-01-02\",14.3\\r\\n\"1988-01-03\",13.5\\r\\n\"1988-01-04\",15.0\\r\\n\"1988-01-05\",13.6\\r\\n\"1988-01-06\",15.2\\r\\n\"1988-01-07\",17.0\\r\\n\"1988-01-08\",18.7\\r\\n\"1988-01-09\",16.5\\r\\n\"1988-01-10\",17.4\\r\\n\"1988-01-11\",18.3\\r\\n\"1988-01-12\",18.3\\r\\n\"1988-01-13\",22.4\\r\\n\"1988-01-14\",21.4\\r\\n\"1988-01-15\",20.9\\r\\n\"1988-01-16\",17.6\\r\\n\"1988-01-17\",15.5\\r\\n\"1988-01-18\",16.6\\r\\n\"1988-01-19\",16.2\\r\\n\"1988-01-20\",15.6\\r\\n\"1988-01-21\",14.5\\r\\n\"1988-01-22\",14.0\\r\\n\"1988-01-23\",15.6\\r\\n\"1988-01-24\",12.3\\r\\n\"1988-01-25\",11.6\\r\\n\"1988-01-26\",12.6\\r\\n\"1988-01-27\",14.9\\r\\n\"1988-01-28\",17.3\\r\\n\"1988-01-29\",21.4\\r\\n\"1988-01-30\",23.4\\r\\n\"1988-01-31\",14.4\\r\\n\"1988-02-01\",14.1\\r\\n\"1988-02-02\",15.0\\r\\n\"1988-02-03\",14.5\\r\\n\"1988-02-04\",15.1\\r\\n\"1988-02-05\",13.9\\r\\n\"1988-02-06\",13.4\\r\\n\"1988-02-07\",9.2\\r\\n\"1988-02-08\",12.5\\r\\n\"1988-02-09\",15.1\\r\\n\"1988-02-10\",12.1\\r\\n\"1988-02-11\",14.5\\r\\n\"1988-02-12\",16.3\\r\\n\"1988-02-13\",16.5\\r\\n\"1988-02-14\",14.9\\r\\n\"1988-02-15\",13.2\\r\\n\"1988-02-16\",11.8\\r\\n\"1988-02-17\",13.6\\r\\n\"1988-02-18\",16.2\\r\\n\"1988-02-19\",14.1\\r\\n\"1988-02-20\",13.5\\r\\n\"1988-02-21\",15.0\\r\\n\"1988-02-22\",14.8\\r\\n\"1988-02-23\",16.2\\r\\n\"1988-02-24\",16.2\\r\\n\"1988-02-25\",13.3\\r\\n\"1988-02-26\",15.3\\r\\n\"1988-02-27\",18.4\\r\\n\"1988-02-28\",16.2\\r\\n\"1988-02-29\",16.3\\r\\n\"1988-03-01\",12.4\\r\\n\"1988-03-02\",15.6\\r\\n\"1988-03-03\",14.9\\r\\n\"1988-03-04\",14.8\\r\\n\"1988-03-05\",12.7\\r\\n\"1988-03-06\",14.2\\r\\n\"1988-03-07\",16.8\\r\\n\"1988-03-08\",16.7\\r\\n\"1988-03-09\",16.2\\r\\n\"1988-03-10\",14.5\\r\\n\"1988-03-11\",10.0\\r\\n\"1988-03-12\",12.6\\r\\n\"1988-03-13\",11.9\\r\\n\"1988-03-14\",11.8\\r\\n\"1988-03-15\",13.4\\r\\n\"1988-03-16\",14.5\\r\\n\"1988-03-17\",15.7\\r\\n\"1988-03-18\",15.3\\r\\n\"1988-03-19\",13.9\\r\\n\"1988-03-20\",13.7\\r\\n\"1988-03-21\",15.1\\r\\n\"1988-03-22\",15.6\\r\\n\"1988-03-23\",14.4\\r\\n\"1988-03-24\",13.9\\r\\n\"1988-03-25\",16.2\\r\\n\"1988-03-26\",16.7\\r\\n\"1988-03-27\",15.5\\r\\n\"1988-03-28\",16.4\\r\\n\"1988-03-29\",17.5\\r\\n\"1988-03-30\",18.2\\r\\n\"1988-03-31\",16.1\\r\\n\"1988-04-01\",16.5\\r\\n\"1988-04-02\",14.6\\r\\n\"1988-04-03\",16.4\\r\\n\"1988-04-04\",13.6\\r\\n\"1988-04-05\",15.9\\r\\n\"1988-04-06\",11.9\\r\\n\"1988-04-07\",14.7\\r\\n\"1988-04-08\",9.4\\r\\n\"1988-04-09\",6.6\\r\\n\"1988-04-10\",7.9\\r\\n\"1988-04-11\",11.0\\r\\n\"1988-04-12\",15.7\\r\\n\"1988-04-13\",15.2\\r\\n\"1988-04-14\",15.9\\r\\n\"1988-04-15\",10.6\\r\\n\"1988-04-16\",8.3\\r\\n\"1988-04-17\",8.6\\r\\n\"1988-04-18\",12.7\\r\\n\"1988-04-19\",10.5\\r\\n\"1988-04-20\",12.0\\r\\n\"1988-04-21\",11.1\\r\\n\"1988-04-22\",13.0\\r\\n\"1988-04-23\",12.4\\r\\n\"1988-04-24\",13.3\\r\\n\"1988-04-25\",15.9\\r\\n\"1988-04-26\",12.0\\r\\n\"1988-04-27\",13.7\\r\\n\"1988-04-28\",17.6\\r\\n\"1988-04-29\",14.3\\r\\n\"1988-04-30\",13.7\\r\\n\"1988-05-01\",15.2\\r\\n\"1988-05-02\",14.5\\r\\n\"1988-05-03\",14.9\\r\\n\"1988-05-04\",15.5\\r\\n\"1988-05-05\",16.4\\r\\n\"1988-05-06\",14.5\\r\\n\"1988-05-07\",12.6\\r\\n\"1988-05-08\",13.6\\r\\n\"1988-05-09\",11.2\\r\\n\"1988-05-10\",11.0\\r\\n\"1988-05-11\",12.0\\r\\n\"1988-05-12\",6.8\\r\\n\"1988-05-13\",10.6\\r\\n\"1988-05-14\",13.1\\r\\n\"1988-05-15\",13.5\\r\\n\"1988-05-16\",11.7\\r\\n\"1988-05-17\",13.2\\r\\n\"1988-05-18\",12.0\\r\\n\"1988-05-19\",10.4\\r\\n\"1988-05-20\",10.0\\r\\n\"1988-05-21\",8.2\\r\\n\"1988-05-22\",9.4\\r\\n\"1988-05-23\",10.3\\r\\n\"1988-05-24\",8.1\\r\\n\"1988-05-25\",8.7\\r\\n\"1988-05-26\",12.6\\r\\n\"1988-05-27\",10.9\\r\\n\"1988-05-28\",8.7\\r\\n\"1988-05-29\",9.3\\r\\n\"1988-05-30\",6.3\\r\\n\"1988-05-31\",7.8\\r\\n\"1988-06-01\",10.0\\r\\n\"1988-06-02\",11.0\\r\\n\"1988-06-03\",11.1\\r\\n\"1988-06-04\",12.6\\r\\n\"1988-06-05\",10.2\\r\\n\"1988-06-06\",11.1\\r\\n\"1988-06-07\",8.7\\r\\n\"1988-06-08\",9.5\\r\\n\"1988-06-09\",9.7\\r\\n\"1988-06-10\",8.2\\r\\n\"1988-06-11\",5.0\\r\\n\"1988-06-12\",6.5\\r\\n\"1988-06-13\",12.1\\r\\n\"1988-06-14\",8.9\\r\\n\"1988-06-15\",6.1\\r\\n\"1988-06-16\",2.8\\r\\n\"1988-06-17\",3.7\\r\\n\"1988-06-18\",6.8\\r\\n\"1988-06-19\",6.6\\r\\n\"1988-06-20\",7.0\\r\\n\"1988-06-21\",7.3\\r\\n\"1988-06-22\",7.9\\r\\n\"1988-06-23\",10.6\\r\\n\"1988-06-24\",8.1\\r\\n\"1988-06-25\",6.7\\r\\n\"1988-06-26\",8.0\\r\\n\"1988-06-27\",10.0\\r\\n\"1988-06-28\",6.7\\r\\n\"1988-06-29\",9.4\\r\\n\"1988-06-30\",9.3\\r\\n\"1988-07-01\",6.0\\r\\n\"1988-07-02\",5.8\\r\\n\"1988-07-03\",4.9\\r\\n\"1988-07-04\",5.0\\r\\n\"1988-07-05\",8.4\\r\\n\"1988-07-06\",12.3\\r\\n\"1988-07-07\",13.0\\r\\n\"1988-07-08\",11.4\\r\\n\"1988-07-09\",6.8\\r\\n\"1988-07-10\",7.6\\r\\n\"1988-07-11\",12.4\\r\\n\"1988-07-12\",7.1\\r\\n\"1988-07-13\",7.5\\r\\n\"1988-07-14\",10.0\\r\\n\"1988-07-15\",5.3\\r\\n\"1988-07-16\",6.3\\r\\n\"1988-07-17\",8.0\\r\\n\"1988-07-18\",8.3\\r\\n\"1988-07-19\",9.3\\r\\n\"1988-07-20\",9.5\\r\\n\"1988-07-21\",5.6\\r\\n\"1988-07-22\",7.0\\r\\n\"1988-07-23\",8.5\\r\\n\"1988-07-24\",8.5\\r\\n\"1988-07-25\",8.2\\r\\n\"1988-07-26\",8.5\\r\\n\"1988-07-27\",9.6\\r\\n\"1988-07-28\",9.7\\r\\n\"1988-07-29\",7.1\\r\\n\"1988-07-30\",8.4\\r\\n\"1988-07-31\",9.2\\r\\n\"1988-08-01\",9.8\\r\\n\"1988-08-02\",8.1\\r\\n\"1988-08-03\",9.4\\r\\n\"1988-08-04\",10.0\\r\\n\"1988-08-05\",5.1\\r\\n\"1988-08-06\",6.7\\r\\n\"1988-08-07\",6.9\\r\\n\"1988-08-08\",6.8\\r\\n\"1988-08-09\",8.6\\r\\n\"1988-08-10\",9.1\\r\\n\"1988-08-11\",3.9\\r\\n\"1988-08-12\",4.8\\r\\n\"1988-08-13\",8.4\\r\\n\"1988-08-14\",11.6\\r\\n\"1988-08-15\",12.1\\r\\n\"1988-08-16\",12.4\\r\\n\"1988-08-17\",10.0\\r\\n\"1988-08-18\",10.1\\r\\n\"1988-08-19\",9.7\\r\\n\"1988-08-20\",11.7\\r\\n\"1988-08-21\",7.9\\r\\n\"1988-08-22\",8.6\\r\\n\"1988-08-23\",7.7\\r\\n\"1988-08-24\",5.8\\r\\n\"1988-08-25\",8.7\\r\\n\"1988-08-26\",10.6\\r\\n\"1988-08-27\",6.7\\r\\n\"1988-08-28\",8.8\\r\\n\"1988-08-29\",9.7\\r\\n\"1988-08-30\",9.0\\r\\n\"1988-08-31\",11.8\\r\\n\"1988-09-01\",15.2\\r\\n\"1988-09-02\",10.0\\r\\n\"1988-09-03\",10.5\\r\\n\"1988-09-04\",5.5\\r\\n\"1988-09-05\",9.4\\r\\n\"1988-09-06\",8.8\\r\\n\"1988-09-07\",5.3\\r\\n\"1988-09-08\",13.0\\r\\n\"1988-09-09\",15.2\\r\\n\"1988-09-10\",13.2\\r\\n\"1988-09-11\",11.5\\r\\n\"1988-09-12\",6.8\\r\\n\"1988-09-13\",4.7\\r\\n\"1988-09-14\",5.2\\r\\n\"1988-09-15\",6.8\\r\\n\"1988-09-16\",10.7\\r\\n\"1988-09-17\",10.1\\r\\n\"1988-09-18\",10.0\\r\\n\"1988-09-19\",9.8\\r\\n\"1988-09-20\",5.5\\r\\n\"1988-09-21\",13.5\\r\\n\"1988-09-22\",16.6\\r\\n\"1988-09-23\",8.4\\r\\n\"1988-09-24\",8.2\\r\\n\"1988-09-25\",11.1\\r\\n\"1988-09-26\",10.8\\r\\n\"1988-09-27\",8.8\\r\\n\"1988-09-28\",10.8\\r\\n\"1988-09-29\",8.7\\r\\n\"1988-09-30\",12.4\\r\\n\"1988-10-01\",9.0\\r\\n\"1988-10-02\",13.5\\r\\n\"1988-10-03\",14.7\\r\\n\"1988-10-04\",10.9\\r\\n\"1988-10-05\",8.5\\r\\n\"1988-10-06\",6.0\\r\\n\"1988-10-07\",12.7\\r\\n\"1988-10-08\",11.1\\r\\n\"1988-10-09\",8.7\\r\\n\"1988-10-10\",12.3\\r\\n\"1988-10-11\",13.3\\r\\n\"1988-10-12\",5.6\\r\\n\"1988-10-13\",13.7\\r\\n\"1988-10-14\",8.5\\r\\n\"1988-10-15\",11.2\\r\\n\"1988-10-16\",8.7\\r\\n\"1988-10-17\",11.7\\r\\n\"1988-10-18\",12.5\\r\\n\"1988-10-19\",8.2\\r\\n\"1988-10-20\",15.6\\r\\n\"1988-10-21\",10.3\\r\\n\"1988-10-22\",11.4\\r\\n\"1988-10-23\",9.7\\r\\n\"1988-10-24\",6.3\\r\\n\"1988-10-25\",14.3\\r\\n\"1988-10-26\",11.3\\r\\n\"1988-10-27\",7.3\\r\\n\"1988-10-28\",12.8\\r\\n\"1988-10-29\",11.9\\r\\n\"1988-10-30\",14.3\\r\\n\"1988-10-31\",11.6\\r\\n\"1988-11-01\",13.2\\r\\n\"1988-11-02\",15.5\\r\\n\"1988-11-03\",14.1\\r\\n\"1988-11-04\",9.5\\r\\n\"1988-11-05\",7.2\\r\\n\"1988-11-06\",11.8\\r\\n\"1988-11-07\",16.8\\r\\n\"1988-11-08\",12.5\\r\\n\"1988-11-09\",9.4\\r\\n\"1988-11-10\",11.9\\r\\n\"1988-11-11\",10.3\\r\\n\"1988-11-12\",16.9\\r\\n\"1988-11-13\",17.5\\r\\n\"1988-11-14\",7.5\\r\\n\"1988-11-15\",8.6\\r\\n\"1988-11-16\",11.1\\r\\n\"1988-11-17\",11.5\\r\\n\"1988-11-18\",10.7\\r\\n\"1988-11-19\",15.7\\r\\n\"1988-11-20\",12.8\\r\\n\"1988-11-21\",13.0\\r\\n\"1988-11-22\",12.9\\r\\n\"1988-11-23\",14.3\\r\\n\"1988-11-24\",13.7\\r\\n\"1988-11-25\",12.1\\r\\n\"1988-11-26\",11.9\\r\\n\"1988-11-27\",11.8\\r\\n\"1988-11-28\",11.4\\r\\n\"1988-11-29\",10.3\\r\\n\"1988-11-30\",11.7\\r\\n\"1988-12-01\",12.0\\r\\n\"1988-12-02\",17.4\\r\\n\"1988-12-03\",16.8\\r\\n\"1988-12-04\",16.2\\r\\n\"1988-12-05\",13.0\\r\\n\"1988-12-06\",12.5\\r\\n\"1988-12-07\",12.4\\r\\n\"1988-12-08\",16.1\\r\\n\"1988-12-09\",20.2\\r\\n\"1988-12-10\",14.3\\r\\n\"1988-12-11\",11.0\\r\\n\"1988-12-12\",14.4\\r\\n\"1988-12-13\",15.7\\r\\n\"1988-12-14\",19.7\\r\\n\"1988-12-15\",20.7\\r\\n\"1988-12-16\",23.9\\r\\n\"1988-12-17\",16.6\\r\\n\"1988-12-18\",17.5\\r\\n\"1988-12-19\",14.9\\r\\n\"1988-12-20\",13.6\\r\\n\"1988-12-21\",11.9\\r\\n\"1988-12-22\",15.2\\r\\n\"1988-12-23\",17.3\\r\\n\"1988-12-24\",19.8\\r\\n\"1988-12-25\",15.8\\r\\n\"1988-12-26\",9.5\\r\\n\"1988-12-27\",12.9\\r\\n\"1988-12-28\",12.9\\r\\n\"1988-12-29\",14.8\\r\\n\"1988-12-30\",14.1\\r\\n\"1989-01-01\",14.3\\r\\n\"1989-01-02\",17.4\\r\\n\"1989-01-03\",18.5\\r\\n\"1989-01-04\",16.8\\r\\n\"1989-01-05\",11.5\\r\\n\"1989-01-06\",9.5\\r\\n\"1989-01-07\",12.2\\r\\n\"1989-01-08\",15.7\\r\\n\"1989-01-09\",16.3\\r\\n\"1989-01-10\",13.6\\r\\n\"1989-01-11\",12.6\\r\\n\"1989-01-12\",13.8\\r\\n\"1989-01-13\",12.1\\r\\n\"1989-01-14\",13.4\\r\\n\"1989-01-15\",17.3\\r\\n\"1989-01-16\",19.4\\r\\n\"1989-01-17\",16.6\\r\\n\"1989-01-18\",13.9\\r\\n\"1989-01-19\",13.1\\r\\n\"1989-01-20\",16.0\\r\\n\"1989-01-21\",14.5\\r\\n\"1989-01-22\",15.0\\r\\n\"1989-01-23\",12.6\\r\\n\"1989-01-24\",12.5\\r\\n\"1989-01-25\",15.2\\r\\n\"1989-01-26\",16.2\\r\\n\"1989-01-27\",16.5\\r\\n\"1989-01-28\",20.1\\r\\n\"1989-01-29\",20.6\\r\\n\"1989-01-30\",16.9\\r\\n\"1989-01-31\",16.5\\r\\n\"1989-02-01\",16.1\\r\\n\"1989-02-02\",14.4\\r\\n\"1989-02-03\",16.3\\r\\n\"1989-02-04\",15.7\\r\\n\"1989-02-05\",14.2\\r\\n\"1989-02-06\",13.2\\r\\n\"1989-02-07\",16.8\\r\\n\"1989-02-08\",18.5\\r\\n\"1989-02-09\",16.7\\r\\n\"1989-02-10\",15.3\\r\\n\"1989-02-11\",15.9\\r\\n\"1989-02-12\",15.2\\r\\n\"1989-02-13\",17.5\\r\\n\"1989-02-14\",18.3\\r\\n\"1989-02-15\",19.4\\r\\n\"1989-02-16\",19.4\\r\\n\"1989-02-17\",19.5\\r\\n\"1989-02-18\",20.5\\r\\n\"1989-02-19\",15.7\\r\\n\"1989-02-20\",15.0\\r\\n\"1989-02-21\",16.1\\r\\n\"1989-02-22\",14.3\\r\\n\"1989-02-23\",13.0\\r\\n\"1989-02-24\",16.2\\r\\n\"1989-02-25\",17.7\\r\\n\"1989-02-26\",13.2\\r\\n\"1989-02-27\",15.8\\r\\n\"1989-02-28\",18.5\\r\\n\"1989-03-01\",20.4\\r\\n\"1989-03-02\",22.0\\r\\n\"1989-03-03\",19.7\\r\\n\"1989-03-04\",19.6\\r\\n\"1989-03-05\",20.3\\r\\n\"1989-03-06\",18.3\\r\\n\"1989-03-07\",18.9\\r\\n\"1989-03-08\",20.3\\r\\n\"1989-03-09\",21.4\\r\\n\"1989-03-10\",18.3\\r\\n\"1989-03-11\",17.8\\r\\n\"1989-03-12\",17.7\\r\\n\"1989-03-13\",12.8\\r\\n\"1989-03-14\",15.1\\r\\n\"1989-03-15\",15.0\\r\\n\"1989-03-16\",14.8\\r\\n\"1989-03-17\",12.0\\r\\n\"1989-03-18\",12.5\\r\\n\"1989-03-19\",15.0\\r\\n\"1989-03-20\",17.1\\r\\n\"1989-03-21\",17.3\\r\\n\"1989-03-22\",16.9\\r\\n\"1989-03-23\",16.5\\r\\n\"1989-03-24\",13.6\\r\\n\"1989-03-25\",13.2\\r\\n\"1989-03-26\",9.4\\r\\n\"1989-03-27\",9.5\\r\\n\"1989-03-28\",11.8\\r\\n\"1989-03-29\",10.4\\r\\n\"1989-03-30\",9.7\\r\\n\"1989-03-31\",12.6\\r\\n\"1989-04-01\",13.3\\r\\n\"1989-04-02\",15.1\\r\\n\"1989-04-03\",14.2\\r\\n\"1989-04-04\",14.2\\r\\n\"1989-04-05\",19.2\\r\\n\"1989-04-06\",12.6\\r\\n\"1989-04-07\",14.2\\r\\n\"1989-04-08\",11.9\\r\\n\"1989-04-09\",13.9\\r\\n\"1989-04-10\",13.5\\r\\n\"1989-04-11\",15.3\\r\\n\"1989-04-12\",13.9\\r\\n\"1989-04-13\",14.0\\r\\n\"1989-04-14\",12.9\\r\\n\"1989-04-15\",8.5\\r\\n\"1989-04-16\",11.4\\r\\n\"1989-04-17\",10.9\\r\\n\"1989-04-18\",12.0\\r\\n\"1989-04-19\",8.6\\r\\n\"1989-04-20\",9.0\\r\\n\"1989-04-21\",9.6\\r\\n\"1989-04-22\",10.2\\r\\n\"1989-04-23\",9.8\\r\\n\"1989-04-24\",8.3\\r\\n\"1989-04-25\",11.0\\r\\n\"1989-04-26\",11.9\\r\\n\"1989-04-27\",14.0\\r\\n\"1989-04-28\",15.8\\r\\n\"1989-04-29\",14.5\\r\\n\"1989-04-30\",13.2\\r\\n\"1989-05-01\",14.2\\r\\n\"1989-05-02\",14.6\\r\\n\"1989-05-03\",11.8\\r\\n\"1989-05-04\",14.4\\r\\n\"1989-05-05\",10.4\\r\\n\"1989-05-06\",10.3\\r\\n\"1989-05-07\",10.8\\r\\n\"1989-05-08\",10.5\\r\\n\"1989-05-09\",9.5\\r\\n\"1989-05-10\",12.5\\r\\n\"1989-05-11\",13.7\\r\\n\"1989-05-12\",12.7\\r\\n\"1989-05-13\",11.9\\r\\n\"1989-05-14\",11.4\\r\\n\"1989-05-15\",9.7\\r\\n\"1989-05-16\",8.3\\r\\n\"1989-05-17\",8.1\\r\\n\"1989-05-18\",11.7\\r\\n\"1989-05-19\",11.6\\r\\n\"1989-05-20\",7.4\\r\\n\"1989-05-21\",5.2\\r\\n\"1989-05-22\",11.0\\r\\n\"1989-05-23\",9.5\\r\\n\"1989-05-24\",9.2\\r\\n\"1989-05-25\",10.7\\r\\n\"1989-05-26\",9.0\\r\\n\"1989-05-27\",10.2\\r\\n\"1989-05-28\",10.3\\r\\n\"1989-05-29\",12.1\\r\\n\"1989-05-30\",13.2\\r\\n\"1989-05-31\",6.6\\r\\n\"1989-06-01\",2.3\\r\\n\"1989-06-02\",1.4\\r\\n\"1989-06-03\",2.1\\r\\n\"1989-06-04\",6.6\\r\\n\"1989-06-05\",8.9\\r\\n\"1989-06-06\",7.8\\r\\n\"1989-06-07\",9.0\\r\\n\"1989-06-08\",10.3\\r\\n\"1989-06-09\",7.9\\r\\n\"1989-06-10\",7.2\\r\\n\"1989-06-11\",8.6\\r\\n\"1989-06-12\",8.8\\r\\n\"1989-06-13\",6.2\\r\\n\"1989-06-14\",9.5\\r\\n\"1989-06-15\",10.2\\r\\n\"1989-06-16\",9.7\\r\\n\"1989-06-17\",11.2\\r\\n\"1989-06-18\",10.2\\r\\n\"1989-06-19\",10.1\\r\\n\"1989-06-20\",8.1\\r\\n\"1989-06-21\",6.6\\r\\n\"1989-06-22\",5.0\\r\\n\"1989-06-23\",4.7\\r\\n\"1989-06-24\",5.3\\r\\n\"1989-06-25\",4.5\\r\\n\"1989-06-26\",2.3\\r\\n\"1989-06-27\",1.4\\r\\n\"1989-06-28\",0.5\\r\\n\"1989-06-29\",2.4\\r\\n\"1989-06-30\",8.0\\r\\n\"1989-07-01\",6.0\\r\\n\"1989-07-02\",7.1\\r\\n\"1989-07-03\",9.7\\r\\n\"1989-07-04\",6.9\\r\\n\"1989-07-05\",5.3\\r\\n\"1989-07-06\",7.0\\r\\n\"1989-07-07\",6.2\\r\\n\"1989-07-08\",7.0\\r\\n\"1989-07-09\",9.7\\r\\n\"1989-07-10\",8.0\\r\\n\"1989-07-11\",8.5\\r\\n\"1989-07-12\",7.1\\r\\n\"1989-07-13\",7.5\\r\\n\"1989-07-14\",3.3\\r\\n\"1989-07-15\",1.8\\r\\n\"1989-07-16\",2.6\\r\\n\"1989-07-17\",5.3\\r\\n\"1989-07-18\",5.8\\r\\n\"1989-07-19\",5.8\\r\\n\"1989-07-20\",7.2\\r\\n\"1989-07-21\",5.3\\r\\n\"1989-07-22\",1.6\\r\\n\"1989-07-23\",3.1\\r\\n\"1989-07-24\",5.3\\r\\n\"1989-07-25\",7.7\\r\\n\"1989-07-26\",4.2\\r\\n\"1989-07-27\",5.5\\r\\n\"1989-07-28\",9.0\\r\\n\"1989-07-29\",11.2\\r\\n\"1989-07-30\",8.0\\r\\n\"1989-07-31\",7.6\\r\\n\"1989-08-01\",3.7\\r\\n\"1989-08-02\",7.5\\r\\n\"1989-08-03\",8.1\\r\\n\"1989-08-04\",8.4\\r\\n\"1989-08-05\",7.1\\r\\n\"1989-08-06\",7.6\\r\\n\"1989-08-07\",7.6\\r\\n\"1989-08-08\",5.6\\r\\n\"1989-08-09\",7.0\\r\\n\"1989-08-10\",10.5\\r\\n\"1989-08-11\",7.3\\r\\n\"1989-08-12\",7.8\\r\\n\"1989-08-13\",5.8\\r\\n\"1989-08-14\",3.8\\r\\n\"1989-08-15\",5.8\\r\\n\"1989-08-16\",6.7\\r\\n\"1989-08-17\",6.6\\r\\n\"1989-08-18\",6.6\\r\\n\"1989-08-19\",9.0\\r\\n\"1989-08-20\",8.1\\r\\n\"1989-08-21\",5.1\\r\\n\"1989-08-22\",8.6\\r\\n\"1989-08-23\",7.0\\r\\n\"1989-08-24\",5.5\\r\\n\"1989-08-25\",7.4\\r\\n\"1989-08-26\",6.2\\r\\n\"1989-08-27\",4.2\\r\\n\"1989-08-28\",6.3\\r\\n\"1989-08-29\",7.0\\r\\n\"1989-08-30\",4.0\\r\\n\"1989-08-31\",8.0\\r\\n\"1989-09-01\",8.8\\r\\n\"1989-09-02\",8.8\\r\\n\"1989-09-03\",6.1\\r\\n\"1989-09-04\",8.6\\r\\n\"1989-09-05\",8.9\\r\\n\"1989-09-06\",7.8\\r\\n\"1989-09-07\",5.0\\r\\n\"1989-09-08\",7.0\\r\\n\"1989-09-09\",13.3\\r\\n\"1989-09-10\",7.9\\r\\n\"1989-09-11\",7.5\\r\\n\"1989-09-12\",8.3\\r\\n\"1989-09-13\",7.2\\r\\n\"1989-09-14\",6.5\\r\\n\"1989-09-15\",8.9\\r\\n\"1989-09-16\",7.4\\r\\n\"1989-09-17\",9.9\\r\\n\"1989-09-18\",9.3\\r\\n\"1989-09-19\",10.6\\r\\n\"1989-09-20\",8.6\\r\\n\"1989-09-21\",7.2\\r\\n\"1989-09-22\",12.6\\r\\n\"1989-09-23\",7.8\\r\\n\"1989-09-24\",6.3\\r\\n\"1989-09-25\",9.2\\r\\n\"1989-09-26\",5.8\\r\\n\"1989-09-27\",9.0\\r\\n\"1989-09-28\",5.0\\r\\n\"1989-09-29\",11.9\\r\\n\"1989-09-30\",13.4\\r\\n\"1989-10-01\",10.5\\r\\n\"1989-10-02\",6.2\\r\\n\"1989-10-03\",5.1\\r\\n\"1989-10-04\",9.5\\r\\n\"1989-10-05\",11.7\\r\\n\"1989-10-06\",9.2\\r\\n\"1989-10-07\",7.3\\r\\n\"1989-10-08\",9.7\\r\\n\"1989-10-09\",9.4\\r\\n\"1989-10-10\",10.0\\r\\n\"1989-10-11\",10.9\\r\\n\"1989-10-12\",11.0\\r\\n\"1989-10-13\",10.9\\r\\n\"1989-10-14\",8.0\\r\\n\"1989-10-15\",11.2\\r\\n\"1989-10-16\",7.5\\r\\n\"1989-10-17\",7.2\\r\\n\"1989-10-18\",13.2\\r\\n\"1989-10-19\",12.9\\r\\n\"1989-10-20\",9.4\\r\\n\"1989-10-21\",10.2\\r\\n\"1989-10-22\",9.5\\r\\n\"1989-10-23\",12.4\\r\\n\"1989-10-24\",10.2\\r\\n\"1989-10-25\",13.4\\r\\n\"1989-10-26\",11.6\\r\\n\"1989-10-27\",8.0\\r\\n\"1989-10-28\",9.0\\r\\n\"1989-10-29\",9.3\\r\\n\"1989-10-30\",13.5\\r\\n\"1989-10-31\",8.0\\r\\n\"1989-11-01\",8.1\\r\\n\"1989-11-02\",10.0\\r\\n\"1989-11-03\",8.5\\r\\n\"1989-11-04\",12.5\\r\\n\"1989-11-05\",15.0\\r\\n\"1989-11-06\",13.3\\r\\n\"1989-11-07\",11.0\\r\\n\"1989-11-08\",11.9\\r\\n\"1989-11-09\",8.3\\r\\n\"1989-11-10\",9.7\\r\\n\"1989-11-11\",11.3\\r\\n\"1989-11-12\",12.5\\r\\n\"1989-11-13\",9.4\\r\\n\"1989-11-14\",11.4\\r\\n\"1989-11-15\",13.2\\r\\n\"1989-11-16\",13.8\\r\\n\"1989-11-17\",16.0\\r\\n\"1989-11-18\",10.9\\r\\n\"1989-11-19\",11.9\\r\\n\"1989-11-20\",12.4\\r\\n\"1989-11-21\",13.2\\r\\n\"1989-11-22\",15.5\\r\\n\"1989-11-23\",21.6\\r\\n\"1989-11-24\",14.9\\r\\n\"1989-11-25\",14.4\\r\\n\"1989-11-26\",12.9\\r\\n\"1989-11-27\",13.1\\r\\n\"1989-11-28\",14.0\\r\\n\"1989-11-29\",17.9\\r\\n\"1989-11-30\",17.7\\r\\n\"1989-12-01\",16.3\\r\\n\"1989-12-02\",18.3\\r\\n\"1989-12-03\",13.7\\r\\n\"1989-12-04\",13.3\\r\\n\"1989-12-05\",10.6\\r\\n\"1989-12-06\",14.1\\r\\n\"1989-12-07\",16.0\\r\\n\"1989-12-08\",16.5\\r\\n\"1989-12-09\",14.1\\r\\n\"1989-12-10\",18.7\\r\\n\"1989-12-11\",16.2\\r\\n\"1989-12-12\",14.8\\r\\n\"1989-12-13\",12.6\\r\\n\"1989-12-14\",10.4\\r\\n\"1989-12-15\",12.2\\r\\n\"1989-12-16\",12.6\\r\\n\"1989-12-17\",12.1\\r\\n\"1989-12-18\",17.3\\r\\n\"1989-12-19\",16.4\\r\\n\"1989-12-20\",12.6\\r\\n\"1989-12-21\",12.3\\r\\n\"1989-12-22\",11.8\\r\\n\"1989-12-23\",12.0\\r\\n\"1989-12-24\",12.7\\r\\n\"1989-12-25\",16.4\\r\\n\"1989-12-26\",16.0\\r\\n\"1989-12-27\",13.3\\r\\n\"1989-12-28\",11.7\\r\\n\"1989-12-29\",10.4\\r\\n\"1989-12-30\",14.4\\r\\n\"1989-12-31\",12.7\\r\\n\"1990-01-01\",14.8\\r\\n\"1990-01-02\",13.3\\r\\n\"1990-01-03\",15.6\\r\\n\"1990-01-04\",14.5\\r\\n\"1990-01-05\",14.3\\r\\n\"1990-01-06\",15.3\\r\\n\"1990-01-07\",16.4\\r\\n\"1990-01-08\",14.8\\r\\n\"1990-01-09\",17.4\\r\\n\"1990-01-10\",18.8\\r\\n\"1990-01-11\",22.1\\r\\n\"1990-01-12\",19.0\\r\\n\"1990-01-13\",15.5\\r\\n\"1990-01-14\",15.8\\r\\n\"1990-01-15\",14.7\\r\\n\"1990-01-16\",10.7\\r\\n\"1990-01-17\",11.5\\r\\n\"1990-01-18\",15.0\\r\\n\"1990-01-19\",14.5\\r\\n\"1990-01-20\",14.5\\r\\n\"1990-01-21\",13.3\\r\\n\"1990-01-22\",14.3\\r\\n\"1990-01-23\",14.3\\r\\n\"1990-01-24\",20.5\\r\\n\"1990-01-25\",15.0\\r\\n\"1990-01-26\",17.1\\r\\n\"1990-01-27\",16.9\\r\\n\"1990-01-28\",16.9\\r\\n\"1990-01-29\",13.6\\r\\n\"1990-01-30\",16.4\\r\\n\"1990-01-31\",16.1\\r\\n\"1990-02-01\",12.0\\r\\n\"1990-02-02\",12.2\\r\\n\"1990-02-03\",14.8\\r\\n\"1990-02-04\",14.8\\r\\n\"1990-02-05\",14.4\\r\\n\"1990-02-06\",12.9\\r\\n\"1990-02-07\",13.4\\r\\n\"1990-02-08\",15.9\\r\\n\"1990-02-09\",16.1\\r\\n\"1990-02-10\",17.6\\r\\n\"1990-02-11\",15.6\\r\\n\"1990-02-12\",15.0\\r\\n\"1990-02-13\",13.0\\r\\n\"1990-02-14\",14.1\\r\\n\"1990-02-15\",17.3\\r\\n\"1990-02-16\",15.7\\r\\n\"1990-02-17\",18.6\\r\\n\"1990-02-18\",12.7\\r\\n\"1990-02-19\",14.0\\r\\n\"1990-02-20\",13.7\\r\\n\"1990-02-21\",16.3\\r\\n\"1990-02-22\",20.0\\r\\n\"1990-02-23\",17.0\\r\\n\"1990-02-24\",15.2\\r\\n\"1990-02-25\",16.5\\r\\n\"1990-02-26\",16.5\\r\\n\"1990-02-27\",17.3\\r\\n\"1990-02-28\",19.1\\r\\n\"1990-03-01\",19.3\\r\\n\"1990-03-02\",17.3\\r\\n\"1990-03-03\",19.0\\r\\n\"1990-03-04\",19.8\\r\\n\"1990-03-05\",19.3\\r\\n\"1990-03-06\",17.2\\r\\n\"1990-03-07\",14.2\\r\\n\"1990-03-08\",10.3\\r\\n\"1990-03-09\",13.0\\r\\n\"1990-03-10\",15.3\\r\\n\"1990-03-11\",15.0\\r\\n\"1990-03-12\",12.1\\r\\n\"1990-03-13\",9.2\\r\\n\"1990-03-14\",11.0\\r\\n\"1990-03-15\",15.0\\r\\n\"1990-03-16\",11.6\\r\\n\"1990-03-17\",11.6\\r\\n\"1990-03-18\",15.1\\r\\n\"1990-03-19\",15.0\\r\\n\"1990-03-20\",13.6\\r\\n\"1990-03-21\",12.5\\r\\n\"1990-03-22\",14.3\\r\\n\"1990-03-23\",16.0\\r\\n\"1990-03-24\",17.4\\r\\n\"1990-03-25\",16.9\\r\\n\"1990-03-26\",18.0\\r\\n\"1990-03-27\",20.6\\r\\n\"1990-03-28\",14.2\\r\\n\"1990-03-29\",10.9\\r\\n\"1990-03-30\",11.9\\r\\n\"1990-03-31\",13.3\\r\\n\"1990-04-01\",15.3\\r\\n\"1990-04-02\",14.7\\r\\n\"1990-04-03\",11.0\\r\\n\"1990-04-04\",12.2\\r\\n\"1990-04-05\",14.2\\r\\n\"1990-04-06\",17.0\\r\\n\"1990-04-07\",15.8\\r\\n\"1990-04-08\",15.2\\r\\n\"1990-04-09\",15.1\\r\\n\"1990-04-10\",14.7\\r\\n\"1990-04-11\",18.5\\r\\n\"1990-04-12\",16.4\\r\\n\"1990-04-13\",18.4\\r\\n\"1990-04-14\",15.1\\r\\n\"1990-04-15\",9.9\\r\\n\"1990-04-16\",10.2\\r\\n\"1990-04-17\",12.6\\r\\n\"1990-04-18\",13.2\\r\\n\"1990-04-19\",11.5\\r\\n\"1990-04-20\",13.8\\r\\n\"1990-04-21\",14.5\\r\\n\"1990-04-22\",14.7\\r\\n\"1990-04-23\",11.2\\r\\n\"1990-04-24\",12.7\\r\\n\"1990-04-25\",13.7\\r\\n\"1990-04-26\",11.5\\r\\n\"1990-04-27\",10.4\\r\\n\"1990-04-28\",8.9\\r\\n\"1990-04-29\",11.1\\r\\n\"1990-04-30\",9.5\\r\\n\"1990-05-01\",13.0\\r\\n\"1990-05-02\",13.9\\r\\n\"1990-05-03\",12.6\\r\\n\"1990-05-04\",14.3\\r\\n\"1990-05-05\",16.0\\r\\n\"1990-05-06\",13.3\\r\\n\"1990-05-07\",7.0\\r\\n\"1990-05-08\",4.9\\r\\n\"1990-05-09\",6.9\\r\\n\"1990-05-10\",13.7\\r\\n\"1990-05-11\",10.6\\r\\n\"1990-05-12\",12.3\\r\\n\"1990-05-13\",11.1\\r\\n\"1990-05-14\",10.2\\r\\n\"1990-05-15\",9.5\\r\\n\"1990-05-16\",8.9\\r\\n\"1990-05-17\",13.4\\r\\n\"1990-05-18\",9.1\\r\\n\"1990-05-19\",9.4\\r\\n\"1990-05-20\",8.7\\r\\n\"1990-05-21\",5.8\\r\\n\"1990-05-22\",4.5\\r\\n\"1990-05-23\",7.2\\r\\n\"1990-05-24\",10.0\\r\\n\"1990-05-25\",10.5\\r\\n\"1990-05-26\",10.7\\r\\n\"1990-05-27\",8.2\\r\\n\"1990-05-28\",6.1\\r\\n\"1990-05-29\",4.5\\r\\n\"1990-05-30\",6.1\\r\\n\"1990-05-31\",9.8\\r\\n\"1990-06-01\",9.7\\r\\n\"1990-06-02\",8.2\\r\\n\"1990-06-03\",8.4\\r\\n\"1990-06-04\",8.5\\r\\n\"1990-06-05\",10.4\\r\\n\"1990-06-06\",6.8\\r\\n\"1990-06-07\",6.0\\r\\n\"1990-06-08\",6.6\\r\\n\"1990-06-09\",7.8\\r\\n\"1990-06-10\",10.3\\r\\n\"1990-06-11\",7.2\\r\\n\"1990-06-12\",7.4\\r\\n\"1990-06-13\",11.4\\r\\n\"1990-06-14\",5.4\\r\\n\"1990-06-15\",4.4\\r\\n\"1990-06-16\",6.4\\r\\n\"1990-06-17\",9.3\\r\\n\"1990-06-18\",7.7\\r\\n\"1990-06-19\",8.1\\r\\n\"1990-06-20\",8.3\\r\\n\"1990-06-21\",9.1\\r\\n\"1990-06-22\",7.7\\r\\n\"1990-06-23\",10.6\\r\\n\"1990-06-24\",8.2\\r\\n\"1990-06-25\",7.9\\r\\n\"1990-06-26\",5.2\\r\\n\"1990-06-27\",5.9\\r\\n\"1990-06-28\",3.7\\r\\n\"1990-06-29\",5.6\\r\\n\"1990-06-30\",9.4\\r\\n\"1990-07-01\",7.4\\r\\n\"1990-07-02\",7.3\\r\\n\"1990-07-03\",7.7\\r\\n\"1990-07-04\",7.7\\r\\n\"1990-07-05\",9.3\\r\\n\"1990-07-06\",4.4\\r\\n\"1990-07-07\",5.7\\r\\n\"1990-07-08\",10.2\\r\\n\"1990-07-09\",10.2\\r\\n\"1990-07-10\",9.3\\r\\n\"1990-07-11\",5.4\\r\\n\"1990-07-12\",5.0\\r\\n\"1990-07-13\",7.6\\r\\n\"1990-07-14\",9.6\\r\\n\"1990-07-15\",10.4\\r\\n\"1990-07-16\",11.2\\r\\n\"1990-07-17\",9.1\\r\\n\"1990-07-18\",11.2\\r\\n\"1990-07-19\",6.8\\r\\n\"1990-07-20\",8.3\\r\\n\"1990-07-21\",9.7\\r\\n\"1990-07-22\",9.6\\r\\n\"1990-07-23\",9.8\\r\\n\"1990-07-24\",10.8\\r\\n\"1990-07-25\",9.2\\r\\n\"1990-07-26\",6.5\\r\\n\"1990-07-27\",8.1\\r\\n\"1990-07-28\",7.3\\r\\n\"1990-07-29\",7.9\\r\\n\"1990-07-30\",6.0\\r\\n\"1990-07-31\",5.0\\r\\n\"1990-08-01\",6.8\\r\\n\"1990-08-02\",9.8\\r\\n\"1990-08-03\",5.7\\r\\n\"1990-08-04\",8.6\\r\\n\"1990-08-05\",10.6\\r\\n\"1990-08-06\",7.8\\r\\n\"1990-08-07\",7.7\\r\\n\"1990-08-08\",8.6\\r\\n\"1990-08-09\",6.5\\r\\n\"1990-08-10\",6.9\\r\\n\"1990-08-11\",6.4\\r\\n\"1990-08-12\",8.5\\r\\n\"1990-08-13\",7.8\\r\\n\"1990-08-14\",9.3\\r\\n\"1990-08-15\",8.4\\r\\n\"1990-08-16\",7.8\\r\\n\"1990-08-17\",7.4\\r\\n\"1990-08-18\",7.7\\r\\n\"1990-08-19\",8.9\\r\\n\"1990-08-20\",9.7\\r\\n\"1990-08-21\",9.9\\r\\n\"1990-08-22\",6.1\\r\\n\"1990-08-23\",6.6\\r\\n\"1990-08-24\",7.6\\r\\n\"1990-08-25\",7.4\\r\\n\"1990-08-26\",8.0\\r\\n\"1990-08-27\",2.1\\r\\n\"1990-08-28\",5.9\\r\\n\"1990-08-29\",11.6\\r\\n\"1990-08-30\",8.6\\r\\n\"1990-08-31\",7.9\\r\\n\"1990-09-01\",6.0\\r\\n\"1990-09-02\",9.5\\r\\n\"1990-09-03\",8.6\\r\\n\"1990-09-04\",7.6\\r\\n\"1990-09-05\",10.4\\r\\n\"1990-09-06\",10.3\\r\\n\"1990-09-07\",7.5\\r\\n\"1990-09-08\",3.0\\r\\n\"1990-09-09\",5.3\\r\\n\"1990-09-10\",10.5\\r\\n\"1990-09-11\",14.6\\r\\n\"1990-09-12\",12.6\\r\\n\"1990-09-13\",9.8\\r\\n\"1990-09-14\",7.2\\r\\n\"1990-09-15\",10.1\\r\\n\"1990-09-16\",10.4\\r\\n\"1990-09-17\",3.7\\r\\n\"1990-09-18\",7.3\\r\\n\"1990-09-19\",11.6\\r\\n\"1990-09-20\",16.3\\r\\n\"1990-09-21\",9.6\\r\\n\"1990-09-22\",6.8\\r\\n\"1990-09-23\",5.2\\r\\n\"1990-09-24\",10.6\\r\\n\"1990-09-25\",16.3\\r\\n\"1990-09-26\",9.8\\r\\n\"1990-09-27\",4.6\\r\\n\"1990-09-28\",11.1\\r\\n\"1990-09-29\",8.7\\r\\n\"1990-09-30\",10.0\\r\\n\"1990-10-01\",11.3\\r\\n\"1990-10-02\",10.5\\r\\n\"1990-10-03\",9.9\\r\\n\"1990-10-04\",11.0\\r\\n\"1990-10-05\",14.0\\r\\n\"1990-10-06\",9.2\\r\\n\"1990-10-07\",9.8\\r\\n\"1990-10-08\",6.0\\r\\n\"1990-10-09\",9.8\\r\\n\"1990-10-10\",9.2\\r\\n\"1990-10-11\",11.8\\r\\n\"1990-10-12\",10.3\\r\\n\"1990-10-13\",7.5\\r\\n\"1990-10-14\",7.7\\r\\n\"1990-10-15\",15.8\\r\\n\"1990-10-16\",14.6\\r\\n\"1990-10-17\",10.5\\r\\n\"1990-10-18\",11.3\\r\\n\"1990-10-19\",10.9\\r\\n\"1990-10-20\",6.4\\r\\n\"1990-10-21\",10.9\\r\\n\"1990-10-22\",9.0\\r\\n\"1990-10-23\",10.9\\r\\n\"1990-10-24\",12.4\\r\\n\"1990-10-25\",11.6\\r\\n\"1990-10-26\",13.3\\r\\n\"1990-10-27\",14.4\\r\\n\"1990-10-28\",18.4\\r\\n\"1990-10-29\",13.6\\r\\n\"1990-10-30\",14.9\\r\\n\"1990-10-31\",14.8\\r\\n\"1990-11-01\",15.4\\r\\n\"1990-11-02\",11.8\\r\\n\"1990-11-03\",13.0\\r\\n\"1990-11-04\",11.1\\r\\n\"1990-11-05\",12.5\\r\\n\"1990-11-06\",18.3\\r\\n\"1990-11-07\",19.2\\r\\n\"1990-11-08\",15.4\\r\\n\"1990-11-09\",13.1\\r\\n\"1990-11-10\",11.5\\r\\n\"1990-11-11\",8.6\\r\\n\"1990-11-12\",12.6\\r\\n\"1990-11-13\",13.8\\r\\n\"1990-11-14\",14.6\\r\\n\"1990-11-15\",13.2\\r\\n\"1990-11-16\",12.3\\r\\n\"1990-11-17\",8.8\\r\\n\"1990-11-18\",10.7\\r\\n\"1990-11-19\",9.9\\r\\n\"1990-11-20\",8.3\\r\\n\"1990-11-21\",15.0\\r\\n\"1990-11-22\",12.2\\r\\n\"1990-11-23\",10.5\\r\\n\"1990-11-24\",11.1\\r\\n\"1990-11-25\",13.0\\r\\n\"1990-11-26\",12.9\\r\\n\"1990-11-27\",8.8\\r\\n\"1990-11-28\",14.7\\r\\n\"1990-11-29\",14.7\\r\\n\"1990-11-30\",12.7\\r\\n\"1990-12-01\",13.3\\r\\n\"1990-12-02\",13.2\\r\\n\"1990-12-03\",16.2\\r\\n\"1990-12-04\",17.3\\r\\n\"1990-12-05\",20.5\\r\\n\"1990-12-06\",20.2\\r\\n\"1990-12-07\",19.4\\r\\n\"1990-12-08\",15.5\\r\\n\"1990-12-09\",14.1\\r\\n\"1990-12-10\",11.0\\r\\n\"1990-12-11\",11.1\\r\\n\"1990-12-12\",14.0\\r\\n\"1990-12-13\",11.4\\r\\n\"1990-12-14\",12.5\\r\\n\"1990-12-15\",13.4\\r\\n\"1990-12-16\",13.6\\r\\n\"1990-12-17\",13.9\\r\\n\"1990-12-18\",17.2\\r\\n\"1990-12-19\",14.7\\r\\n\"1990-12-20\",15.4\\r\\n\"1990-12-21\",13.1\\r\\n\"1990-12-22\",13.2\\r\\n\"1990-12-23\",13.9\\r\\n\"1990-12-24\",10.0\\r\\n\"1990-12-25\",12.9\\r\\n\"1990-12-26\",14.6\\r\\n\"1990-12-27\",14.0\\r\\n\"1990-12-28\",13.6\\r\\n\"1990-12-29\",13.5\\r\\n\"1990-12-30\",15.7\\r\\n\"1990-12-31\",13.0\\r\\n\\r\\nDaily minimum temperatures in Melbourne, Australia, 1981-1990\\r\\n\\r\\n'} # file is uploaded to the current directory ! ls arrhythmia.data daily-minimum-temperatures-in-me.csv sample_data # open the file # the last few lines are junk df = pd . read_csv ( 'daily-minimum-temperatures-in-me.csv' , error_bad_lines = False ) df . head () b'Skipping line 3653: expected 2 fields, saw 3\\n' .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Daily minimum temperatures in Melbourne, Australia, 1981-1990 0 1981-01-01 20.7 1 1981-01-02 17.9 2 1981-01-03 18.8 3 1981-01-04 14.6 4 1981-01-05 15.8 # upload a Python file with some useful functions (meant for fake_util.py) from google.colab import files uploaded = files . upload () <input type=\"file\" id=\"files-c590b543-ffcc-4177-91d3-62725b363169\" name=\"files[]\" multiple disabled /> <output id=\"result-c590b543-ffcc-4177-91d3-62725b363169\"> Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. </output> <script src=\"/nbextensions/google.colab/files.js\"></script> Saving fake_util.py to fake_util.py from fake_util import my_useful_function my_useful_function () hello world ! pwd /content Part 4: Access files from Google Drive \u00b6 # Access files from your Google Drive from google.colab import drive drive . mount ( '/content/gdrive' ) # Check current directory - now gdrive is there ! ls # What's in gdrive? ! ls gdrive # Whoa! Look at all this great VIP content! ! ls '/content/gdrive/My Drive/' Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Loading Data"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/#part-1-using-wget","text":"# download the data from a URL # source: https://archive.ics.uci.edu/ml/datasets/Arrhythmia # alternate URL: https://lazyprogrammer.me/course_files/arrhythmia.data #!wget --no-check-certificate https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data ! wget https : // lazyprogrammer . me / course_files / arrhythmia . data --2020-04-26 07:33:37-- https://lazyprogrammer.me/course_files/arrhythmia.data Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.31.80.48, 104.31.81.48, 2606:4700:3035::681f:5130, ... Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.31.80.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 402355 (393K) Saving to: \u2018arrhythmia.data\u2019 arrhythmia.data 100%[===================>] 392.92K 1.09MB/s in 0.4s 2020-04-26 07:33:38 (1.09 MB/s) - \u2018arrhythmia.data\u2019 saved [402355/402355] # list files in current directory ! ls arrhythmia.data sample_data # check if the data has a header ! head arrhythmia . data 75,0,190,80,91,193,371,174,121,-16,13,64,-2,?,63,0,52,44,0,0,32,0,0,0,0,0,0,0,44,20,36,0,28,0,0,0,0,0,0,52,40,0,0,0,60,0,0,0,0,0,0,52,0,0,0,0,0,0,0,0,0,0,0,0,56,36,0,0,32,0,0,0,0,0,0,48,32,0,0,0,56,0,0,0,0,0,0,80,0,0,0,0,0,0,0,0,0,0,0,0,40,52,0,0,28,0,0,0,0,0,0,0,48,48,0,0,32,0,0,0,0,0,0,0,52,52,0,0,36,0,0,0,0,0,0,0,52,48,0,0,32,0,0,0,0,0,0,0,56,44,0,0,32,0,0,0,0,0,0,-0.2,0.0,6.1,-1.0,0.0,0.0,0.6,2.1,13.6,30.8,0.0,0.0,1.7,-1.0,0.6,0.0,1.3,1.5,3.7,14.5,0.1,-5.2,1.4,0.0,0.0,0.0,0.8,-0.6,-10.7,-15.6,0.4,-3.9,0.0,0.0,0.0,0.0,-0.8,-1.7,-10.1,-22.0,0.0,0.0,5.7,-1.0,0.0,0.0,-0.1,1.2,14.1,22.5,0.0,-2.5,0.8,0.0,0.0,0.0,1.0,0.4,-4.8,-2.7,0.1,-6.0,0.0,0.0,0.0,0.0,-0.8,-0.6,-24.0,-29.7,0.0,0.0,2.0,-6.4,0.0,0.0,0.2,2.9,-12.6,15.2,-0.1,0.0,8.4,-10.0,0.0,0.0,0.6,5.9,-3.9,52.7,-0.3,0.0,15.2,-8.4,0.0,0.0,0.9,5.1,17.7,70.7,-0.4,0.0,13.5,-4.0,0.0,0.0,0.9,3.9,25.5,62.9,-0.3,0.0,9.0,-0.9,0.0,0.0,0.9,2.9,23.3,49.4,8 56,1,165,64,81,174,401,149,39,25,37,-17,31,?,53,0,48,0,0,0,24,0,0,0,0,0,0,0,64,0,0,0,24,0,0,0,0,0,0,32,24,0,0,0,40,0,0,0,0,0,0,48,0,0,0,0,0,0,0,0,0,0,0,0,44,20,0,0,24,0,0,0,0,0,0,0,60,0,0,0,20,0,0,0,0,0,0,0,24,52,0,0,16,0,0,0,0,0,0,0,32,52,0,0,20,0,0,0,0,0,0,0,44,48,0,0,32,0,0,0,0,0,0,0,48,44,0,0,32,0,0,0,0,0,0,0,48,40,0,0,28,0,0,0,0,0,0,0,48,0,0,0,28,0,0,0,0,0,0,-0.6,0.0,7.2,0.0,0.0,0.0,0.4,1.5,17.2,26.5,0.0,0.0,5.5,0.0,0.0,0.0,0.1,1.7,17.6,29.5,0.3,-1.6,0.9,0.0,0.0,0.0,-0.3,0.4,-1.5,1.3,0.1,-6.4,0.0,0.0,0.0,0.0,-0.3,-1.6,-15.3,-25.5,-0.3,0.0,4.2,-0.9,0.0,0.0,0.4,0.7,8.3,12.3,0.2,0.0,2.2,0.0,0.0,0.0,-0.2,0.8,6.6,11.7,0.4,0.0,1.0,-8.8,0.0,0.0,0.5,-0.6,-21.6,-26.8,0.4,0.0,2.6,-7.9,0.0,0.0,0.8,2.0,-16.4,1.2,0.0,0.0,5.8,-7.7,0.0,0.0,0.9,3.8,-5.7,27.7,-0.2,0.0,9.5,-5.0,0.0,0.0,0.5,2.6,11.8,34.6,-0.4,0.0,11.0,-2.4,0.0,0.0,0.4,2.6,21.6,43.4,-0.5,0.0,8.5,0.0,0.0,0.0,0.2,2.1,20.4,38.8,6 54,0,172,95,138,163,386,185,102,96,34,70,66,23,75,0,40,80,0,0,24,0,0,0,0,0,0,20,56,52,0,0,40,0,0,0,0,0,0,28,116,0,0,0,52,0,0,0,0,0,0,52,64,0,0,0,88,0,0,0,0,0,0,0,36,92,0,0,24,0,0,0,0,0,0,0,128,0,0,0,24,0,1,0,0,0,0,0,24,36,76,0,100,0,0,0,0,0,0,0,40,28,60,0,96,0,0,0,0,0,0,0,48,20,56,24,32,0,0,0,0,0,0,0,44,88,0,0,28,0,0,0,0,0,0,0,44,76,0,0,28,0,0,0,0,0,0,0,44,72,0,0,24,0,0,0,0,0,0,1.0,0.0,4.5,-2.8,0.0,0.0,0.3,2.5,-2.2,19.8,0.8,-0.4,6.4,-1.3,0.0,0.0,0.7,2.7,14.2,37.9,-0.2,-0.6,4.4,0.0,0.0,0.0,0.5,0.2,24.7,26.2,-1.0,-5.3,1.8,0.0,0.0,0.0,-0.5,-2.5,-8.0,-28.5,0.5,0.0,1.7,-2.7,0.0,0.0,-0.2,1.0,-9.4,-1.2,0.4,0.0,4.9,0.0,0.0,0.0,0.6,1.4,31.3,42.7,-0.8,0.0,0.7,-3.8,6.5,0.0,0.3,-3.3,18.7,-13.6,-0.9,0.0,2.2,-4.1,7.4,0.0,0.5,-2.4,20.9,-2.6,0.0,0.0,5.8,-4.1,4.0,-0.5,0.4,0.3,20.4,23.3,0.7,0.0,10.0,-5.7,0.0,0.0,0.5,2.2,-3.0,20.7,1.3,0.0,11.1,-3.4,0.0,0.0,0.4,3.4,11.5,48.2,0.9,0.0,9.5,-2.4,0.0,0.0,0.3,3.4,12.3,49.0,10 55,0,175,94,100,202,380,179,143,28,11,-5,20,?,71,0,72,20,0,0,48,0,0,0,0,0,0,0,64,36,0,0,36,0,0,0,0,0,0,20,52,48,0,0,56,0,0,0,0,0,0,64,32,0,0,0,72,0,0,0,0,0,0,0,60,12,0,0,44,0,0,0,0,0,0,0,60,44,0,0,32,0,0,0,0,0,0,56,0,0,0,0,0,0,0,0,0,0,0,0,40,44,0,0,20,0,0,0,0,0,0,0,52,40,0,0,32,0,0,0,0,0,0,0,56,48,0,0,36,0,0,0,0,0,0,0,60,48,0,0,36,0,0,0,0,0,0,0,64,40,0,0,40,0,0,0,0,0,0,0.9,0.0,7.8,-0.7,0.0,0.0,1.1,1.9,27.3,45.1,0.1,0.0,9.1,-2.6,0.0,0.0,0.4,1.5,24.5,36.8,-0.4,-0.4,1.6,-2.2,0.0,0.0,-1.0,-0.9,-1.5,-9.2,-0.4,-8.2,1.8,0.0,0.0,0.0,-0.7,-1.7,-23.4,-35.6,0.9,0.0,3.2,-0.4,0.0,0.0,0.7,1.2,9.4,18.0,-0.1,0.0,5.1,-2.5,0.0,0.0,0.3,0.6,9.8,12.6,1.6,-6.5,0.0,0.0,0.0,0.0,-0.4,-0.4,-18.2,-22.4,2.1,0.0,1.2,-6.9,0.0,0.0,-0.5,2.9,-12.7,18.0,0.7,0.0,9.0,-7.9,0.0,0.0,0.1,4.1,7.6,51.0,0.4,0.0,15.0,-5.5,0.0,0.0,0.1,3.3,28.8,63.1,0.1,0.0,15.2,-3.7,0.0,0.0,0.6,3.0,36.8,68.0,0.1,0.0,12.2,-2.2,0.0,0.0,0.4,2.6,34.6,61.6,1 75,0,190,80,88,181,360,177,103,-16,13,61,3,?,?,0,48,40,0,0,28,0,0,0,0,0,0,0,40,24,0,0,24,0,0,0,0,0,0,52,36,0,0,0,60,0,0,0,0,0,0,48,28,0,0,0,56,0,0,0,0,0,0,0,48,36,0,0,28,0,0,0,0,0,0,44,0,0,0,0,0,0,0,0,0,0,0,88,0,0,0,0,0,0,0,0,0,0,0,0,40,52,0,0,28,0,0,0,0,0,0,0,48,48,0,0,32,0,0,0,0,0,0,0,48,52,0,0,32,0,0,0,0,0,0,0,52,44,0,0,28,0,0,0,0,0,0,0,52,48,0,0,32,0,0,0,0,0,0,0.0,0.0,5.2,-1.4,0.0,0.0,0.9,2.3,9.6,31.6,0.1,0.0,1.6,-0.5,0.0,0.0,1.9,1.7,2.6,18.9,0.2,-3.8,1.2,0.0,0.0,0.0,1.0,-0.6,-7.7,-13.4,-0.1,-3.4,0.8,0.0,0.0,0.0,-1.4,-1.5,-7.0,-17.8,-0.1,0.0,4.4,-1.3,0.0,0.0,-0.1,1.1,8.2,16.5,0.6,-1.6,0.0,0.0,0.0,0.0,1.4,0.3,-3.5,-1.9,0.0,-5.7,0.0,0.0,0.0,0.0,-0.4,-0.5,-25.0,-30.0,-0.2,0.0,1.6,-6.0,0.0,0.0,-0.7,2.1,-12.4,8.6,-0.5,0.0,8.5,-10.2,0.0,0.0,-1.0,4.7,-4.0,43.0,-0.2,0.0,15.2,-7.8,0.0,0.0,-0.1,4.9,16.2,63.2,-0.2,0.0,9.1,-0.9,0.0,0.0,-0.2,2.9,21.7,48.9,-0.4,0.0,13.1,-3.6,0.0,0.0,-0.1,3.9,25.4,62.8,7 13,0,169,51,100,167,321,174,91,107,66,52,88,?,84,0,36,48,0,0,20,0,0,0,0,0,0,20,44,36,0,0,44,0,0,0,0,0,0,24,64,0,0,0,48,0,0,0,0,0,0,44,36,0,0,0,52,0,0,0,0,0,0,0,28,64,0,0,16,0,0,0,0,0,0,24,44,40,0,0,44,0,0,0,0,0,0,0,36,60,0,0,24,0,0,0,0,0,0,20,32,60,0,0,40,0,0,0,0,0,0,24,32,60,0,0,44,0,0,0,0,0,0,0,52,40,0,0,36,0,0,0,0,0,0,0,44,40,0,0,32,0,0,0,0,0,0,20,36,56,0,0,40,0,0,0,0,0,0,0.5,0.0,2.7,-6.4,0.0,0.0,0.9,1.7,-10.5,7.1,0.1,-1.2,19.1,-2.3,0.0,0.0,1.4,4.3,36.7,84.8,-0.4,-2.3,21.7,0.0,0.0,0.0,0.7,2.6,66.7,95.8,-0.2,-9.0,3.2,0.0,0.0,0.0,-1.1,-2.9,-14.1,-39.0,0.5,0.0,1.8,-12.9,0.0,0.0,0.4,-0.4,-38.7,-42.1,-0.1,-1.6,19.9,-0.7,0.0,0.0,1.0,3.3,40.4,65.4,0.4,0.0,6.7,-24.4,0.0,0.0,-1.2,0.4,-61.2,-59.9,0.9,-0.5,11.9,-43.3,0.0,0.0,0.8,3.4,-111.4,-95.1,2.0,-0.8,19.8,-48.4,0.0,0.0,1.6,8.7,-114.5,-72.8,2.0,0.0,31.0,-25.7,0.0,0.0,0.8,5.9,29.2,85.8,0.6,0.0,19.5,-11.4,0.0,0.0,0.8,3.3,20.1,49.1,0.0,-0.6,12.2,-2.8,0.0,0.0,0.9,2.2,13.5,31.1,14 40,1,160,52,77,129,377,133,77,77,49,75,65,?,70,0,44,0,0,0,24,0,0,0,0,0,0,0,40,32,0,0,24,0,0,0,0,0,0,0,44,28,0,0,24,0,0,0,0,0,0,44,16,0,0,0,48,0,0,0,0,0,0,36,0,0,0,0,0,0,0,0,0,0,0,0,44,16,0,0,24,0,0,0,0,0,0,0,20,56,0,0,12,0,0,0,0,0,0,0,24,56,0,0,16,0,0,0,0,0,0,0,36,48,0,0,24,0,0,0,0,0,0,0,40,44,0,0,28,0,0,0,0,0,0,0,40,44,0,0,24,0,0,0,0,0,0,0,44,0,0,0,24,0,0,0,0,0,0,-0.5,0.0,1.8,0.0,0.0,0.0,0.2,1.0,3.9,10.5,-0.1,0.0,7.6,-1.1,0.0,0.0,0.5,1.4,13.5,22.7,0.0,0.0,5.9,-0.5,0.0,0.0,0.3,0.6,12.2,15.0,0.1,-4.6,0.6,0.0,0.0,0.0,-0.4,-0.9,-9.7,-14.7,0.2,-2.1,0.0,0.0,0.0,0.0,-0.3,0.4,-3.7,-1.4,-0.2,0.0,6.8,-0.9,0.0,0.0,0.7,0.7,14.2,17.1,1.3,0.0,1.3,-11.5,0.0,0.0,-0.3,1.7,-30.9,-13.9,1.7,0.0,2.3,-17.5,0.0,0.0,-0.6,4.5,-46.3,-1.3,1.1,0.0,3.7,-11.0,0.0,0.0,-0.5,4.1,-19.8,21.2,0.1,0.0,7.7,-6.4,0.0,0.0,0.4,1.9,1.4,15.4,0.0,0.0,7.4,-2.5,0.0,0.0,0.4,1.3,9.3,18.9,-0.4,0.0,6.5,0.0,0.0,0.0,0.4,1.0,14.3,20.5,1 49,1,162,54,78,0,376,157,70,67,7,8,51,?,67,0,44,36,0,0,24,0,0,0,0,0,0,0,52,32,0,0,28,0,0,0,0,0,0,0,56,28,0,0,24,0,0,0,0,0,0,48,32,0,0,0,56,0,0,0,0,0,0,52,0,0,0,0,0,0,0,0,0,0,0,0,52,28,0,0,28,0,0,0,0,0,0,0,20,44,0,0,8,0,0,0,0,0,0,0,24,48,0,0,16,0,0,0,0,0,0,0,36,44,0,0,24,0,0,0,0,0,0,0,44,48,0,0,28,0,0,0,0,0,0,0,48,44,0,0,28,0,0,0,0,0,0,0,48,40,0,0,24,0,0,0,0,0,0,-0.3,0.0,4.1,-1.1,0.0,0.0,0.8,1.0,7.1,13.7,-0.3,0.0,8.4,-1.5,0.0,0.0,0.6,0.7,19.4,22.9,0.0,0.0,4.4,-0.8,0.0,0.0,-0.3,-0.6,11.2,6.9,0.1,-6.3,1.3,0.0,0.0,0.0,-0.6,-0.8,-13.1,-17.9,0.1,-0.8,0.0,0.0,0.0,0.0,0.6,0.7,-2.0,2.9,-0.2,0.0,6.3,-1.2,0.0,0.0,0.2,0.3,14.7,16.8,0.7,0.0,0.5,-7.3,0.0,0.0,0.2,-0.1,-15.5,-16.4,0.9,0.0,0.7,-8.9,0.0,0.0,0.6,2.5,-20.5,4.0,0.8,0.0,2.1,-9.0,0.0,0.0,0.6,3.8,-16.1,21.1,0.1,0.0,6.6,-4.1,0.0,0.0,0.3,1.4,4.7,14.2,-0.2,0.0,8.5,-2.7,0.0,0.0,0.1,0.8,14.5,20.9,-0.3,0.0,8.2,-1.9,0.0,0.0,0.1,0.5,15.8,19.8,1 44,0,168,56,84,118,354,160,63,61,69,78,66,84,64,0,40,0,0,0,20,0,0,0,0,0,0,0,44,12,0,0,28,0,0,0,0,0,0,0,36,8,0,0,20,0,0,0,0,0,0,40,12,0,0,0,44,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,36,12,0,0,20,0,0,0,0,0,0,0,20,56,0,0,12,0,0,0,0,0,0,0,24,48,0,0,12,0,0,0,0,0,0,0,28,44,0,0,16,0,0,0,0,0,0,0,44,32,0,0,32,0,0,0,0,0,0,0,44,28,0,0,28,0,0,0,0,0,0,0,40,24,0,0,24,0,0,0,0,0,0,0.1,0.0,2.3,0.0,0.0,0.0,0.4,1.0,4.6,11.6,1.2,0.0,5.4,-0.7,0.0,0.0,1.8,2.8,11.4,31.0,1.1,0.0,3.0,-0.4,0.0,0.0,1.4,1.8,5.3,17.9,-0.7,-3.9,0.5,0.0,0.0,0.0,-1.1,-1.9,-7.5,-20.4,-0.5,0.0,0.0,0.0,0.0,0.0,-0.6,-0.5,0.0,-3.4,1.1,0.0,4.2,-0.5,0.0,0.0,1.6,2.3,7.2,22.8,0.5,0.0,0.9,-5.5,0.0,0.0,-0.7,1.0,-14.5,-5.3,0.7,0.0,1.2,-6.4,0.0,0.0,-0.5,2.6,-13.9,10.0,1.5,0.0,2.4,-10.3,0.0,0.0,0.3,6.8,-19.3,43.2,0.8,0.0,7.9,-7.3,0.0,0.0,0.9,6.5,5.7,62.9,0.1,0.0,9.3,-3.8,0.0,0.0,0.8,3.8,15.1,48.5,0.1,0.0,7.0,-1.3,0.0,0.0,0.6,2.1,12.5,30.9,1 50,1,167,67,89,130,383,156,73,85,34,70,71,?,63,0,44,40,0,0,28,0,0,0,0,0,0,0,56,24,0,0,32,0,0,0,0,0,0,0,72,0,0,0,28,0,0,0,0,0,0,56,28,0,0,0,60,0,0,0,0,0,0,0,28,56,0,0,16,0,0,0,0,0,0,0,60,0,0,0,32,0,0,0,0,0,0,0,24,36,32,0,68,0,0,0,0,0,0,0,36,44,0,0,20,0,0,0,0,0,0,0,40,48,0,0,24,0,0,0,0,0,0,0,56,40,0,0,40,0,0,0,0,0,0,0,52,36,0,0,32,0,0,0,0,0,0,20,44,36,0,0,44,0,0,0,0,0,0,-0.1,0.0,3.5,-2.0,0.0,0.0,0.4,1.3,3.7,13.5,0.0,0.0,9.9,-0.8,0.0,0.0,1.2,1.2,26.8,35.2,0.0,0.0,8.3,0.0,0.0,0.0,0.8,0.3,29.8,32.0,0.1,-6.1,1.1,0.0,0.0,0.0,-0.6,-1.2,-15.5,-24.1,0.0,0.0,0.6,-4.1,0.0,0.0,-0.1,0.8,-10.6,-4.9,-0.2,0.0,8.9,0.0,0.0,0.0,0.8,0.7,26.7,30.2,0.1,0.0,1.3,-5.4,1.9,0.0,0.2,0.8,-5.2,2.1,0.8,0.0,4.4,-8.5,0.0,0.0,0.8,3.9,-10.8,25.0,0.4,0.0,4.3,-7.3,0.0,0.0,1.1,4.0,-8.9,27.9,-0.5,0.0,7.0,-3.2,0.0,0.0,1.1,1.3,13.2,22.3,-0.5,0.0,10.9,-2.5,0.0,0.0,1.0,1.0,23.8,29.6,-0.5,-0.6,10.8,-1.7,0.0,0.0,0.8,0.9,20.1,25.1,10 # check the data import pandas as pd df = pd . read_csv ( 'arrhythmia.data' , header = None ) # since the data has many columns, take just the first few and name them (as per the documentation) data = df [[ 0 , 1 , 2 , 3 , 4 , 5 ]] data . columns = [ 'age' , 'sex' , 'height' , 'weight' , 'QRS duration' , 'P-R interval' ] import matplotlib.pyplot as plt plt . rcParams [ 'figure.figsize' ] = [ 15 , 15 ] # make the plot bigger so the subplots don't overlap data . hist (); # use a semicolon to supress return value from pandas.plotting import scatter_matrix scatter_matrix ( data );","title":"Part 1: Using wget"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/#part-2-using-tfkeras","text":"# use keras get_file to download the auto MPG dataset # source: https://archive.ics.uci.edu/ml/datasets/Auto+MPG #url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data' ### alternate URL url = 'https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/auto-mpg.data' # Install TensorFlow import tensorflow as tf print ( tf . __version__ ) 2.2.0-rc2 # check out the documentation for other arguments tf . keras . utils . get_file ( 'auto-mpg.data' , url ) Downloading data from https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/auto-mpg.data 32768/30286 [================================] - 0s 1us/step '/root/.keras/datasets/auto-mpg.data' ! head / root /. keras / datasets / auto - mpg . data 18.0 8 307.0 130.0 3504. 12.0 70 1 \"chevrolet chevelle malibu\" 15.0 8 350.0 165.0 3693. 11.5 70 1 \"buick skylark 320\" 18.0 8 318.0 150.0 3436. 11.0 70 1 \"plymouth satellite\" 16.0 8 304.0 150.0 3433. 12.0 70 1 \"amc rebel sst\" 17.0 8 302.0 140.0 3449. 10.5 70 1 \"ford torino\" 15.0 8 429.0 198.0 4341. 10.0 70 1 \"ford galaxie 500\" 14.0 8 454.0 220.0 4354. 9.0 70 1 \"chevrolet impala\" 14.0 8 440.0 215.0 4312. 8.5 70 1 \"plymouth fury iii\" 14.0 8 455.0 225.0 4425. 10.0 70 1 \"pontiac catalina\" 15.0 8 390.0 190.0 3850. 8.5 70 1 \"amc ambassador dpl\" # unless you specify an alternative path, the data will go into /root/.keras/datasets/ df = pd . read_csv ( '/root/.keras/datasets/auto-mpg.data' , header = None , delim_whitespace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 0 18.0 8 307.0 130.0 3504.0 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165.0 3693.0 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150.0 3436.0 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150.0 3433.0 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140.0 3449.0 10.5 70 1 ford torino","title":"Part 2: Using tf.keras"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/#part-3-upload-the-file-yourself","text":"# another method: upload your own file ##### PLEASE NOTE: IT DOES NOT MATTER WHICH FILE YOU UPLOAD ##### YOU CAN UPLOAD ANY FILE YOU WANT ##### IN FACT, YOU ARE ENCOURAGED TO EXPLORE ON YOUR OWN # if you must, then get the file from here: # https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/daily-minimum-temperatures-in-me.csv from google.colab import files uploaded = files . upload () <input type=\"file\" id=\"files-f85a97ca-0bfe-4c55-8369-d830289c8925\" name=\"files[]\" multiple disabled /> <output id=\"result-f85a97ca-0bfe-4c55-8369-d830289c8925\"> Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. </output> <script src=\"/nbextensions/google.colab/files.js\"></script> Saving daily-minimum-temperatures-in-me.csv to daily-minimum-temperatures-in-me.csv uploaded {'daily-minimum-temperatures-in-me.csv': b'\"Date\",\"Daily minimum temperatures in Melbourne, Australia, 1981-1990\"\\r\\n\"1981-01-01\",20.7\\r\\n\"1981-01-02\",17.9\\r\\n\"1981-01-03\",18.8\\r\\n\"1981-01-04\",14.6\\r\\n\"1981-01-05\",15.8\\r\\n\"1981-01-06\",15.8\\r\\n\"1981-01-07\",15.8\\r\\n\"1981-01-08\",17.4\\r\\n\"1981-01-09\",21.8\\r\\n\"1981-01-10\",20.0\\r\\n\"1981-01-11\",16.2\\r\\n\"1981-01-12\",13.3\\r\\n\"1981-01-13\",16.7\\r\\n\"1981-01-14\",21.5\\r\\n\"1981-01-15\",25.0\\r\\n\"1981-01-16\",20.7\\r\\n\"1981-01-17\",20.6\\r\\n\"1981-01-18\",24.8\\r\\n\"1981-01-19\",17.7\\r\\n\"1981-01-20\",15.5\\r\\n\"1981-01-21\",18.2\\r\\n\"1981-01-22\",12.1\\r\\n\"1981-01-23\",14.4\\r\\n\"1981-01-24\",16.0\\r\\n\"1981-01-25\",16.5\\r\\n\"1981-01-26\",18.7\\r\\n\"1981-01-27\",19.4\\r\\n\"1981-01-28\",17.2\\r\\n\"1981-01-29\",15.5\\r\\n\"1981-01-30\",15.1\\r\\n\"1981-01-31\",15.4\\r\\n\"1981-02-01\",15.3\\r\\n\"1981-02-02\",18.8\\r\\n\"1981-02-03\",21.9\\r\\n\"1981-02-04\",19.9\\r\\n\"1981-02-05\",16.6\\r\\n\"1981-02-06\",16.8\\r\\n\"1981-02-07\",14.6\\r\\n\"1981-02-08\",17.1\\r\\n\"1981-02-09\",25.0\\r\\n\"1981-02-10\",15.0\\r\\n\"1981-02-11\",13.7\\r\\n\"1981-02-12\",13.9\\r\\n\"1981-02-13\",18.3\\r\\n\"1981-02-14\",22.0\\r\\n\"1981-02-15\",22.1\\r\\n\"1981-02-16\",21.2\\r\\n\"1981-02-17\",18.4\\r\\n\"1981-02-18\",16.6\\r\\n\"1981-02-19\",16.1\\r\\n\"1981-02-20\",15.7\\r\\n\"1981-02-21\",16.6\\r\\n\"1981-02-22\",16.5\\r\\n\"1981-02-23\",14.4\\r\\n\"1981-02-24\",14.4\\r\\n\"1981-02-25\",18.5\\r\\n\"1981-02-26\",16.9\\r\\n\"1981-02-27\",17.5\\r\\n\"1981-02-28\",21.2\\r\\n\"1981-03-01\",17.8\\r\\n\"1981-03-02\",18.6\\r\\n\"1981-03-03\",17.0\\r\\n\"1981-03-04\",16.0\\r\\n\"1981-03-05\",13.3\\r\\n\"1981-03-06\",14.3\\r\\n\"1981-03-07\",11.4\\r\\n\"1981-03-08\",16.3\\r\\n\"1981-03-09\",16.1\\r\\n\"1981-03-10\",11.8\\r\\n\"1981-03-11\",12.2\\r\\n\"1981-03-12\",14.7\\r\\n\"1981-03-13\",11.8\\r\\n\"1981-03-14\",11.3\\r\\n\"1981-03-15\",10.6\\r\\n\"1981-03-16\",11.7\\r\\n\"1981-03-17\",14.2\\r\\n\"1981-03-18\",11.2\\r\\n\"1981-03-19\",16.9\\r\\n\"1981-03-20\",16.7\\r\\n\"1981-03-21\",8.1\\r\\n\"1981-03-22\",8.0\\r\\n\"1981-03-23\",8.8\\r\\n\"1981-03-24\",13.4\\r\\n\"1981-03-25\",10.9\\r\\n\"1981-03-26\",13.4\\r\\n\"1981-03-27\",11.0\\r\\n\"1981-03-28\",15.0\\r\\n\"1981-03-29\",15.7\\r\\n\"1981-03-30\",14.5\\r\\n\"1981-03-31\",15.8\\r\\n\"1981-04-01\",16.7\\r\\n\"1981-04-02\",16.8\\r\\n\"1981-04-03\",17.5\\r\\n\"1981-04-04\",17.1\\r\\n\"1981-04-05\",18.1\\r\\n\"1981-04-06\",16.6\\r\\n\"1981-04-07\",10.0\\r\\n\"1981-04-08\",14.9\\r\\n\"1981-04-09\",15.9\\r\\n\"1981-04-10\",13.0\\r\\n\"1981-04-11\",7.6\\r\\n\"1981-04-12\",11.5\\r\\n\"1981-04-13\",13.5\\r\\n\"1981-04-14\",13.0\\r\\n\"1981-04-15\",13.3\\r\\n\"1981-04-16\",12.1\\r\\n\"1981-04-17\",12.4\\r\\n\"1981-04-18\",13.2\\r\\n\"1981-04-19\",13.8\\r\\n\"1981-04-20\",10.6\\r\\n\"1981-04-21\",9.0\\r\\n\"1981-04-22\",10.0\\r\\n\"1981-04-23\",9.8\\r\\n\"1981-04-24\",11.5\\r\\n\"1981-04-25\",8.9\\r\\n\"1981-04-26\",7.4\\r\\n\"1981-04-27\",9.9\\r\\n\"1981-04-28\",9.3\\r\\n\"1981-04-29\",9.9\\r\\n\"1981-04-30\",7.4\\r\\n\"1981-05-01\",8.6\\r\\n\"1981-05-02\",11.9\\r\\n\"1981-05-03\",14.0\\r\\n\"1981-05-04\",8.6\\r\\n\"1981-05-05\",10.0\\r\\n\"1981-05-06\",13.5\\r\\n\"1981-05-07\",12.0\\r\\n\"1981-05-08\",10.5\\r\\n\"1981-05-09\",10.7\\r\\n\"1981-05-10\",8.1\\r\\n\"1981-05-11\",10.1\\r\\n\"1981-05-12\",10.6\\r\\n\"1981-05-13\",5.3\\r\\n\"1981-05-14\",6.6\\r\\n\"1981-05-15\",8.5\\r\\n\"1981-05-16\",11.2\\r\\n\"1981-05-17\",9.8\\r\\n\"1981-05-18\",5.9\\r\\n\"1981-05-19\",3.2\\r\\n\"1981-05-20\",2.1\\r\\n\"1981-05-21\",3.4\\r\\n\"1981-05-22\",5.4\\r\\n\"1981-05-23\",9.6\\r\\n\"1981-05-24\",11.5\\r\\n\"1981-05-25\",12.3\\r\\n\"1981-05-26\",12.6\\r\\n\"1981-05-27\",11.0\\r\\n\"1981-05-28\",11.2\\r\\n\"1981-05-29\",11.4\\r\\n\"1981-05-30\",11.8\\r\\n\"1981-05-31\",12.8\\r\\n\"1981-06-01\",11.6\\r\\n\"1981-06-02\",10.6\\r\\n\"1981-06-03\",9.8\\r\\n\"1981-06-04\",11.2\\r\\n\"1981-06-05\",5.7\\r\\n\"1981-06-06\",7.1\\r\\n\"1981-06-07\",2.5\\r\\n\"1981-06-08\",3.5\\r\\n\"1981-06-09\",4.6\\r\\n\"1981-06-10\",11.0\\r\\n\"1981-06-11\",5.7\\r\\n\"1981-06-12\",7.7\\r\\n\"1981-06-13\",10.4\\r\\n\"1981-06-14\",11.4\\r\\n\"1981-06-15\",9.2\\r\\n\"1981-06-16\",6.1\\r\\n\"1981-06-17\",2.7\\r\\n\"1981-06-18\",4.3\\r\\n\"1981-06-19\",6.3\\r\\n\"1981-06-20\",3.8\\r\\n\"1981-06-21\",4.4\\r\\n\"1981-06-22\",7.1\\r\\n\"1981-06-23\",4.8\\r\\n\"1981-06-24\",5.8\\r\\n\"1981-06-25\",6.2\\r\\n\"1981-06-26\",7.3\\r\\n\"1981-06-27\",9.2\\r\\n\"1981-06-28\",10.2\\r\\n\"1981-06-29\",9.5\\r\\n\"1981-06-30\",9.5\\r\\n\"1981-07-01\",10.7\\r\\n\"1981-07-02\",10.0\\r\\n\"1981-07-03\",6.5\\r\\n\"1981-07-04\",7.0\\r\\n\"1981-07-05\",7.4\\r\\n\"1981-07-06\",8.1\\r\\n\"1981-07-07\",6.6\\r\\n\"1981-07-08\",8.3\\r\\n\"1981-07-09\",8.9\\r\\n\"1981-07-10\",4.6\\r\\n\"1981-07-11\",6.8\\r\\n\"1981-07-12\",5.7\\r\\n\"1981-07-13\",6.1\\r\\n\"1981-07-14\",7.0\\r\\n\"1981-07-15\",7.2\\r\\n\"1981-07-16\",6.3\\r\\n\"1981-07-17\",8.8\\r\\n\"1981-07-18\",5.0\\r\\n\"1981-07-19\",7.4\\r\\n\"1981-07-20\",10.1\\r\\n\"1981-07-21\",12.0\\r\\n\"1981-07-22\",9.0\\r\\n\"1981-07-23\",8.9\\r\\n\"1981-07-24\",9.8\\r\\n\"1981-07-25\",9.0\\r\\n\"1981-07-26\",9.2\\r\\n\"1981-07-27\",7.7\\r\\n\"1981-07-28\",8.0\\r\\n\"1981-07-29\",6.1\\r\\n\"1981-07-30\",3.5\\r\\n\"1981-07-31\",3.2\\r\\n\"1981-08-01\",5.7\\r\\n\"1981-08-02\",7.7\\r\\n\"1981-08-03\",9.0\\r\\n\"1981-08-04\",10.0\\r\\n\"1981-08-05\",6.2\\r\\n\"1981-08-06\",6.9\\r\\n\"1981-08-07\",6.5\\r\\n\"1981-08-08\",6.8\\r\\n\"1981-08-09\",7.0\\r\\n\"1981-08-10\",5.2\\r\\n\"1981-08-11\",3.0\\r\\n\"1981-08-12\",5.6\\r\\n\"1981-08-13\",7.9\\r\\n\"1981-08-14\",9.0\\r\\n\"1981-08-15\",8.6\\r\\n\"1981-08-16\",10.3\\r\\n\"1981-08-17\",10.5\\r\\n\"1981-08-18\",7.6\\r\\n\"1981-08-19\",9.7\\r\\n\"1981-08-20\",12.5\\r\\n\"1981-08-21\",7.4\\r\\n\"1981-08-22\",7.9\\r\\n\"1981-08-23\",3.9\\r\\n\"1981-08-24\",6.6\\r\\n\"1981-08-25\",4.6\\r\\n\"1981-08-26\",7.0\\r\\n\"1981-08-27\",6.0\\r\\n\"1981-08-28\",5.5\\r\\n\"1981-08-29\",8.1\\r\\n\"1981-08-30\",5.5\\r\\n\"1981-08-31\",6.2\\r\\n\"1981-09-01\",8.0\\r\\n\"1981-09-02\",10.3\\r\\n\"1981-09-03\",9.8\\r\\n\"1981-09-04\",9.6\\r\\n\"1981-09-05\",8.5\\r\\n\"1981-09-06\",7.5\\r\\n\"1981-09-07\",11.2\\r\\n\"1981-09-08\",14.6\\r\\n\"1981-09-09\",11.7\\r\\n\"1981-09-10\",7.8\\r\\n\"1981-09-11\",12.3\\r\\n\"1981-09-12\",10.1\\r\\n\"1981-09-13\",11.5\\r\\n\"1981-09-14\",7.3\\r\\n\"1981-09-15\",10.9\\r\\n\"1981-09-16\",14.1\\r\\n\"1981-09-17\",10.7\\r\\n\"1981-09-18\",16.9\\r\\n\"1981-09-19\",10.5\\r\\n\"1981-09-20\",6.5\\r\\n\"1981-09-21\",11.0\\r\\n\"1981-09-22\",6.3\\r\\n\"1981-09-23\",10.5\\r\\n\"1981-09-24\",7.2\\r\\n\"1981-09-25\",7.6\\r\\n\"1981-09-26\",10.7\\r\\n\"1981-09-27\",7.8\\r\\n\"1981-09-28\",9.6\\r\\n\"1981-09-29\",11.4\\r\\n\"1981-09-30\",12.4\\r\\n\"1981-10-01\",8.9\\r\\n\"1981-10-02\",13.2\\r\\n\"1981-10-03\",8.6\\r\\n\"1981-10-04\",6.2\\r\\n\"1981-10-05\",11.4\\r\\n\"1981-10-06\",13.2\\r\\n\"1981-10-07\",14.3\\r\\n\"1981-10-08\",7.3\\r\\n\"1981-10-09\",12.9\\r\\n\"1981-10-10\",7.8\\r\\n\"1981-10-11\",6.2\\r\\n\"1981-10-12\",5.6\\r\\n\"1981-10-13\",10.0\\r\\n\"1981-10-14\",13.3\\r\\n\"1981-10-15\",8.3\\r\\n\"1981-10-16\",10.2\\r\\n\"1981-10-17\",8.6\\r\\n\"1981-10-18\",7.3\\r\\n\"1981-10-19\",10.4\\r\\n\"1981-10-20\",11.2\\r\\n\"1981-10-21\",13.2\\r\\n\"1981-10-22\",11.4\\r\\n\"1981-10-23\",9.1\\r\\n\"1981-10-24\",6.6\\r\\n\"1981-10-25\",8.4\\r\\n\"1981-10-26\",9.7\\r\\n\"1981-10-27\",13.2\\r\\n\"1981-10-28\",12.5\\r\\n\"1981-10-29\",11.0\\r\\n\"1981-10-30\",11.0\\r\\n\"1981-10-31\",11.7\\r\\n\"1981-11-01\",9.2\\r\\n\"1981-11-02\",11.5\\r\\n\"1981-11-03\",13.6\\r\\n\"1981-11-04\",13.7\\r\\n\"1981-11-05\",10.4\\r\\n\"1981-11-06\",11.5\\r\\n\"1981-11-07\",7.6\\r\\n\"1981-11-08\",9.6\\r\\n\"1981-11-09\",14.2\\r\\n\"1981-11-10\",15.7\\r\\n\"1981-11-11\",10.5\\r\\n\"1981-11-12\",10.5\\r\\n\"1981-11-13\",9.7\\r\\n\"1981-11-14\",9.5\\r\\n\"1981-11-15\",11.3\\r\\n\"1981-11-16\",8.9\\r\\n\"1981-11-17\",9.4\\r\\n\"1981-11-18\",11.9\\r\\n\"1981-11-19\",11.7\\r\\n\"1981-11-20\",13.4\\r\\n\"1981-11-21\",12.6\\r\\n\"1981-11-22\",10.1\\r\\n\"1981-11-23\",15.8\\r\\n\"1981-11-24\",13.6\\r\\n\"1981-11-25\",11.9\\r\\n\"1981-11-26\",9.9\\r\\n\"1981-11-27\",12.6\\r\\n\"1981-11-28\",17.8\\r\\n\"1981-11-29\",15.0\\r\\n\"1981-11-30\",13.6\\r\\n\"1981-12-01\",13.4\\r\\n\"1981-12-02\",10.5\\r\\n\"1981-12-03\",14.2\\r\\n\"1981-12-04\",11.5\\r\\n\"1981-12-05\",13.0\\r\\n\"1981-12-06\",15.0\\r\\n\"1981-12-07\",14.7\\r\\n\"1981-12-08\",12.6\\r\\n\"1981-12-09\",12.5\\r\\n\"1981-12-10\",13.5\\r\\n\"1981-12-11\",14.8\\r\\n\"1981-12-12\",17.2\\r\\n\"1981-12-13\",9.7\\r\\n\"1981-12-14\",12.1\\r\\n\"1981-12-15\",12.8\\r\\n\"1981-12-16\",11.2\\r\\n\"1981-12-17\",16.4\\r\\n\"1981-12-18\",15.6\\r\\n\"1981-12-19\",13.3\\r\\n\"1981-12-20\",11.0\\r\\n\"1981-12-21\",11.1\\r\\n\"1981-12-22\",15.0\\r\\n\"1981-12-23\",12.8\\r\\n\"1981-12-24\",15.0\\r\\n\"1981-12-25\",14.2\\r\\n\"1981-12-26\",14.0\\r\\n\"1981-12-27\",15.5\\r\\n\"1981-12-28\",13.3\\r\\n\"1981-12-29\",15.6\\r\\n\"1981-12-30\",15.2\\r\\n\"1981-12-31\",17.4\\r\\n\"1982-01-01\",17.0\\r\\n\"1982-01-02\",15.0\\r\\n\"1982-01-03\",13.5\\r\\n\"1982-01-04\",15.2\\r\\n\"1982-01-05\",13.0\\r\\n\"1982-01-06\",12.5\\r\\n\"1982-01-07\",14.1\\r\\n\"1982-01-08\",14.8\\r\\n\"1982-01-09\",16.2\\r\\n\"1982-01-10\",15.8\\r\\n\"1982-01-11\",19.1\\r\\n\"1982-01-12\",22.2\\r\\n\"1982-01-13\",15.9\\r\\n\"1982-01-14\",13.0\\r\\n\"1982-01-15\",14.1\\r\\n\"1982-01-16\",15.8\\r\\n\"1982-01-17\",24.0\\r\\n\"1982-01-18\",18.0\\r\\n\"1982-01-19\",19.7\\r\\n\"1982-01-20\",25.2\\r\\n\"1982-01-21\",20.5\\r\\n\"1982-01-22\",19.3\\r\\n\"1982-01-23\",15.8\\r\\n\"1982-01-24\",17.0\\r\\n\"1982-01-25\",18.4\\r\\n\"1982-01-26\",13.3\\r\\n\"1982-01-27\",14.6\\r\\n\"1982-01-28\",12.5\\r\\n\"1982-01-29\",17.0\\r\\n\"1982-01-30\",17.1\\r\\n\"1982-01-31\",14.0\\r\\n\"1982-02-01\",14.6\\r\\n\"1982-02-02\",13.3\\r\\n\"1982-02-03\",14.8\\r\\n\"1982-02-04\",15.1\\r\\n\"1982-02-05\",13.1\\r\\n\"1982-02-06\",13.6\\r\\n\"1982-02-07\",19.5\\r\\n\"1982-02-08\",22.7\\r\\n\"1982-02-09\",17.2\\r\\n\"1982-02-10\",13.5\\r\\n\"1982-02-11\",15.4\\r\\n\"1982-02-12\",17.0\\r\\n\"1982-02-13\",19.2\\r\\n\"1982-02-14\",22.8\\r\\n\"1982-02-15\",26.3\\r\\n\"1982-02-16\",18.2\\r\\n\"1982-02-17\",17.0\\r\\n\"1982-02-18\",14.8\\r\\n\"1982-02-19\",12.8\\r\\n\"1982-02-20\",15.5\\r\\n\"1982-02-21\",15.6\\r\\n\"1982-02-22\",13.1\\r\\n\"1982-02-23\",15.2\\r\\n\"1982-02-24\",14.1\\r\\n\"1982-02-25\",12.5\\r\\n\"1982-02-26\",14.6\\r\\n\"1982-02-27\",10.4\\r\\n\"1982-02-28\",13.9\\r\\n\"1982-03-01\",11.9\\r\\n\"1982-03-02\",13.5\\r\\n\"1982-03-03\",9.8\\r\\n\"1982-03-04\",14.0\\r\\n\"1982-03-05\",21.5\\r\\n\"1982-03-06\",19.5\\r\\n\"1982-03-07\",16.7\\r\\n\"1982-03-08\",19.1\\r\\n\"1982-03-09\",11.0\\r\\n\"1982-03-10\",9.0\\r\\n\"1982-03-11\",10.0\\r\\n\"1982-03-12\",14.6\\r\\n\"1982-03-13\",12.5\\r\\n\"1982-03-14\",17.2\\r\\n\"1982-03-15\",19.2\\r\\n\"1982-03-16\",22.2\\r\\n\"1982-03-17\",15.7\\r\\n\"1982-03-18\",14.2\\r\\n\"1982-03-19\",9.8\\r\\n\"1982-03-20\",14.0\\r\\n\"1982-03-21\",17.5\\r\\n\"1982-03-22\",20.7\\r\\n\"1982-03-23\",15.6\\r\\n\"1982-03-24\",13.2\\r\\n\"1982-03-25\",14.5\\r\\n\"1982-03-26\",16.8\\r\\n\"1982-03-27\",17.2\\r\\n\"1982-03-28\",13.4\\r\\n\"1982-03-29\",14.2\\r\\n\"1982-03-30\",14.3\\r\\n\"1982-03-31\",10.2\\r\\n\"1982-04-01\",10.4\\r\\n\"1982-04-02\",12.3\\r\\n\"1982-04-03\",11.9\\r\\n\"1982-04-04\",11.2\\r\\n\"1982-04-05\",8.5\\r\\n\"1982-04-06\",12.0\\r\\n\"1982-04-07\",12.4\\r\\n\"1982-04-08\",12.9\\r\\n\"1982-04-09\",10.1\\r\\n\"1982-04-10\",15.0\\r\\n\"1982-04-11\",13.6\\r\\n\"1982-04-12\",12.4\\r\\n\"1982-04-13\",13.6\\r\\n\"1982-04-14\",16.1\\r\\n\"1982-04-15\",19.5\\r\\n\"1982-04-16\",14.2\\r\\n\"1982-04-17\",9.3\\r\\n\"1982-04-18\",10.1\\r\\n\"1982-04-19\",7.4\\r\\n\"1982-04-20\",8.6\\r\\n\"1982-04-21\",7.8\\r\\n\"1982-04-22\",9.1\\r\\n\"1982-04-23\",13.0\\r\\n\"1982-04-24\",16.5\\r\\n\"1982-04-25\",12.9\\r\\n\"1982-04-26\",6.9\\r\\n\"1982-04-27\",6.9\\r\\n\"1982-04-28\",8.7\\r\\n\"1982-04-29\",10.0\\r\\n\"1982-04-30\",10.8\\r\\n\"1982-05-01\",7.5\\r\\n\"1982-05-02\",6.3\\r\\n\"1982-05-03\",11.9\\r\\n\"1982-05-04\",13.8\\r\\n\"1982-05-05\",11.8\\r\\n\"1982-05-06\",11.0\\r\\n\"1982-05-07\",10.1\\r\\n\"1982-05-08\",8.5\\r\\n\"1982-05-09\",5.5\\r\\n\"1982-05-10\",7.6\\r\\n\"1982-05-11\",8.7\\r\\n\"1982-05-12\",10.8\\r\\n\"1982-05-13\",11.2\\r\\n\"1982-05-14\",9.1\\r\\n\"1982-05-15\",3.7\\r\\n\"1982-05-16\",4.6\\r\\n\"1982-05-17\",6.6\\r\\n\"1982-05-18\",13.2\\r\\n\"1982-05-19\",15.2\\r\\n\"1982-05-20\",7.6\\r\\n\"1982-05-21\",8.4\\r\\n\"1982-05-22\",6.0\\r\\n\"1982-05-23\",8.3\\r\\n\"1982-05-24\",8.6\\r\\n\"1982-05-25\",11.1\\r\\n\"1982-05-26\",12.1\\r\\n\"1982-05-27\",12.9\\r\\n\"1982-05-28\",14.0\\r\\n\"1982-05-29\",12.5\\r\\n\"1982-05-30\",11.5\\r\\n\"1982-05-31\",7.0\\r\\n\"1982-06-01\",7.1\\r\\n\"1982-06-02\",9.0\\r\\n\"1982-06-03\",3.1\\r\\n\"1982-06-04\",2.5\\r\\n\"1982-06-05\",0.0\\r\\n\"1982-06-06\",1.6\\r\\n\"1982-06-07\",2.6\\r\\n\"1982-06-08\",5.7\\r\\n\"1982-06-09\",2.3\\r\\n\"1982-06-10\",4.5\\r\\n\"1982-06-11\",8.2\\r\\n\"1982-06-12\",6.9\\r\\n\"1982-06-13\",7.3\\r\\n\"1982-06-14\",6.0\\r\\n\"1982-06-15\",7.3\\r\\n\"1982-06-16\",7.6\\r\\n\"1982-06-17\",8.0\\r\\n\"1982-06-18\",8.0\\r\\n\"1982-06-19\",6.8\\r\\n\"1982-06-20\",7.3\\r\\n\"1982-06-21\",6.2\\r\\n\"1982-06-22\",6.9\\r\\n\"1982-06-23\",8.9\\r\\n\"1982-06-24\",4.0\\r\\n\"1982-06-25\",1.3\\r\\n\"1982-06-26\",0.8\\r\\n\"1982-06-27\",4.3\\r\\n\"1982-06-28\",7.3\\r\\n\"1982-06-29\",7.7\\r\\n\"1982-06-30\",9.0\\r\\n\"1982-07-01\",4.2\\r\\n\"1982-07-02\",1.6\\r\\n\"1982-07-03\",2.6\\r\\n\"1982-07-04\",3.4\\r\\n\"1982-07-05\",3.9\\r\\n\"1982-07-06\",7.0\\r\\n\"1982-07-07\",7.8\\r\\n\"1982-07-08\",5.3\\r\\n\"1982-07-09\",2.4\\r\\n\"1982-07-10\",2.8\\r\\n\"1982-07-11\",4.0\\r\\n\"1982-07-12\",7.5\\r\\n\"1982-07-13\",7.8\\r\\n\"1982-07-14\",5.6\\r\\n\"1982-07-15\",3.3\\r\\n\"1982-07-16\",5.0\\r\\n\"1982-07-17\",3.7\\r\\n\"1982-07-18\",3.9\\r\\n\"1982-07-19\",5.2\\r\\n\"1982-07-20\",?0.2\\r\\n\"1982-07-21\",?0.8\\r\\n\"1982-07-22\",0.9\\r\\n\"1982-07-23\",3.5\\r\\n\"1982-07-24\",6.6\\r\\n\"1982-07-25\",9.5\\r\\n\"1982-07-26\",9.0\\r\\n\"1982-07-27\",3.5\\r\\n\"1982-07-28\",4.5\\r\\n\"1982-07-29\",5.7\\r\\n\"1982-07-30\",5.6\\r\\n\"1982-07-31\",7.1\\r\\n\"1982-08-01\",9.7\\r\\n\"1982-08-02\",8.3\\r\\n\"1982-08-03\",9.1\\r\\n\"1982-08-04\",2.8\\r\\n\"1982-08-05\",2.2\\r\\n\"1982-08-06\",4.5\\r\\n\"1982-08-07\",3.8\\r\\n\"1982-08-08\",3.8\\r\\n\"1982-08-09\",6.2\\r\\n\"1982-08-10\",11.5\\r\\n\"1982-08-11\",10.2\\r\\n\"1982-08-12\",7.9\\r\\n\"1982-08-13\",9.0\\r\\n\"1982-08-14\",9.5\\r\\n\"1982-08-15\",6.0\\r\\n\"1982-08-16\",8.2\\r\\n\"1982-08-17\",9.2\\r\\n\"1982-08-18\",4.3\\r\\n\"1982-08-19\",6.6\\r\\n\"1982-08-20\",9.4\\r\\n\"1982-08-21\",13.2\\r\\n\"1982-08-22\",6.6\\r\\n\"1982-08-23\",5.1\\r\\n\"1982-08-24\",12.1\\r\\n\"1982-08-25\",11.2\\r\\n\"1982-08-26\",8.5\\r\\n\"1982-08-27\",4.6\\r\\n\"1982-08-28\",7.0\\r\\n\"1982-08-29\",14.2\\r\\n\"1982-08-30\",12.7\\r\\n\"1982-08-31\",7.6\\r\\n\"1982-09-01\",4.0\\r\\n\"1982-09-02\",10.0\\r\\n\"1982-09-03\",10.5\\r\\n\"1982-09-04\",5.0\\r\\n\"1982-09-05\",4.5\\r\\n\"1982-09-06\",8.2\\r\\n\"1982-09-07\",4.3\\r\\n\"1982-09-08\",9.8\\r\\n\"1982-09-09\",5.8\\r\\n\"1982-09-10\",5.0\\r\\n\"1982-09-11\",8.5\\r\\n\"1982-09-12\",9.0\\r\\n\"1982-09-13\",3.6\\r\\n\"1982-09-14\",6.7\\r\\n\"1982-09-15\",6.7\\r\\n\"1982-09-16\",10.1\\r\\n\"1982-09-17\",15.0\\r\\n\"1982-09-18\",8.9\\r\\n\"1982-09-19\",5.7\\r\\n\"1982-09-20\",4.2\\r\\n\"1982-09-21\",4.0\\r\\n\"1982-09-22\",5.3\\r\\n\"1982-09-23\",6.3\\r\\n\"1982-09-24\",8.5\\r\\n\"1982-09-25\",11.5\\r\\n\"1982-09-26\",7.7\\r\\n\"1982-09-27\",9.2\\r\\n\"1982-09-28\",7.8\\r\\n\"1982-09-29\",6.3\\r\\n\"1982-09-30\",6.3\\r\\n\"1982-10-01\",8.6\\r\\n\"1982-10-02\",6.1\\r\\n\"1982-10-03\",13.2\\r\\n\"1982-10-04\",9.9\\r\\n\"1982-10-05\",4.7\\r\\n\"1982-10-06\",5.8\\r\\n\"1982-10-07\",14.9\\r\\n\"1982-10-08\",10.7\\r\\n\"1982-10-09\",8.6\\r\\n\"1982-10-10\",9.4\\r\\n\"1982-10-11\",5.7\\r\\n\"1982-10-12\",10.9\\r\\n\"1982-10-13\",13.1\\r\\n\"1982-10-14\",10.4\\r\\n\"1982-10-15\",8.2\\r\\n\"1982-10-16\",9.8\\r\\n\"1982-10-17\",7.5\\r\\n\"1982-10-18\",5.8\\r\\n\"1982-10-19\",9.8\\r\\n\"1982-10-20\",7.9\\r\\n\"1982-10-21\",8.7\\r\\n\"1982-10-22\",10.0\\r\\n\"1982-10-23\",10.6\\r\\n\"1982-10-24\",8.0\\r\\n\"1982-10-25\",10.2\\r\\n\"1982-10-26\",15.1\\r\\n\"1982-10-27\",13.9\\r\\n\"1982-10-28\",9.2\\r\\n\"1982-10-29\",9.0\\r\\n\"1982-10-30\",13.2\\r\\n\"1982-10-31\",7.0\\r\\n\"1982-11-01\",10.6\\r\\n\"1982-11-02\",6.9\\r\\n\"1982-11-03\",9.5\\r\\n\"1982-11-04\",12.5\\r\\n\"1982-11-05\",13.6\\r\\n\"1982-11-06\",17.7\\r\\n\"1982-11-07\",16.0\\r\\n\"1982-11-08\",11.3\\r\\n\"1982-11-09\",10.5\\r\\n\"1982-11-10\",14.4\\r\\n\"1982-11-11\",10.3\\r\\n\"1982-11-12\",9.0\\r\\n\"1982-11-13\",11.1\\r\\n\"1982-11-14\",14.5\\r\\n\"1982-11-15\",18.0\\r\\n\"1982-11-16\",12.8\\r\\n\"1982-11-17\",10.7\\r\\n\"1982-11-18\",9.1\\r\\n\"1982-11-19\",8.7\\r\\n\"1982-11-20\",12.4\\r\\n\"1982-11-21\",12.6\\r\\n\"1982-11-22\",10.3\\r\\n\"1982-11-23\",13.7\\r\\n\"1982-11-24\",16.0\\r\\n\"1982-11-25\",15.8\\r\\n\"1982-11-26\",12.1\\r\\n\"1982-11-27\",12.5\\r\\n\"1982-11-28\",12.2\\r\\n\"1982-11-29\",13.7\\r\\n\"1982-11-30\",16.1\\r\\n\"1982-12-01\",15.5\\r\\n\"1982-12-02\",10.3\\r\\n\"1982-12-03\",10.5\\r\\n\"1982-12-04\",11.0\\r\\n\"1982-12-05\",11.9\\r\\n\"1982-12-06\",13.0\\r\\n\"1982-12-07\",12.2\\r\\n\"1982-12-08\",10.6\\r\\n\"1982-12-09\",13.0\\r\\n\"1982-12-10\",13.0\\r\\n\"1982-12-11\",12.2\\r\\n\"1982-12-12\",12.6\\r\\n\"1982-12-13\",18.7\\r\\n\"1982-12-14\",15.2\\r\\n\"1982-12-15\",15.3\\r\\n\"1982-12-16\",13.9\\r\\n\"1982-12-17\",15.8\\r\\n\"1982-12-18\",13.0\\r\\n\"1982-12-19\",13.0\\r\\n\"1982-12-20\",13.7\\r\\n\"1982-12-21\",12.0\\r\\n\"1982-12-22\",10.8\\r\\n\"1982-12-23\",15.6\\r\\n\"1982-12-24\",15.3\\r\\n\"1982-12-25\",13.9\\r\\n\"1982-12-26\",13.0\\r\\n\"1982-12-27\",15.3\\r\\n\"1982-12-28\",16.3\\r\\n\"1982-12-29\",15.8\\r\\n\"1982-12-30\",17.7\\r\\n\"1982-12-31\",16.3\\r\\n\"1983-01-01\",18.4\\r\\n\"1983-01-02\",15.0\\r\\n\"1983-01-03\",10.9\\r\\n\"1983-01-04\",11.4\\r\\n\"1983-01-05\",14.8\\r\\n\"1983-01-06\",12.1\\r\\n\"1983-01-07\",12.8\\r\\n\"1983-01-08\",16.2\\r\\n\"1983-01-09\",15.5\\r\\n\"1983-01-10\",13.0\\r\\n\"1983-01-11\",10.5\\r\\n\"1983-01-12\",9.1\\r\\n\"1983-01-13\",10.5\\r\\n\"1983-01-14\",11.8\\r\\n\"1983-01-15\",12.7\\r\\n\"1983-01-16\",12.7\\r\\n\"1983-01-17\",11.5\\r\\n\"1983-01-18\",13.8\\r\\n\"1983-01-19\",13.3\\r\\n\"1983-01-20\",11.6\\r\\n\"1983-01-21\",15.4\\r\\n\"1983-01-22\",12.4\\r\\n\"1983-01-23\",16.9\\r\\n\"1983-01-24\",14.7\\r\\n\"1983-01-25\",10.6\\r\\n\"1983-01-26\",15.6\\r\\n\"1983-01-27\",10.7\\r\\n\"1983-01-28\",12.6\\r\\n\"1983-01-29\",13.8\\r\\n\"1983-01-30\",14.3\\r\\n\"1983-01-31\",14.0\\r\\n\"1983-02-01\",18.1\\r\\n\"1983-02-02\",17.3\\r\\n\"1983-02-03\",13.0\\r\\n\"1983-02-04\",16.0\\r\\n\"1983-02-05\",14.9\\r\\n\"1983-02-06\",16.2\\r\\n\"1983-02-07\",20.3\\r\\n\"1983-02-08\",22.5\\r\\n\"1983-02-09\",17.2\\r\\n\"1983-02-10\",15.9\\r\\n\"1983-02-11\",16.8\\r\\n\"1983-02-12\",13.8\\r\\n\"1983-02-13\",12.8\\r\\n\"1983-02-14\",14.0\\r\\n\"1983-02-15\",17.5\\r\\n\"1983-02-16\",21.5\\r\\n\"1983-02-17\",16.8\\r\\n\"1983-02-18\",13.6\\r\\n\"1983-02-19\",14.5\\r\\n\"1983-02-20\",14.2\\r\\n\"1983-02-21\",15.7\\r\\n\"1983-02-22\",19.7\\r\\n\"1983-02-23\",17.4\\r\\n\"1983-02-24\",14.4\\r\\n\"1983-02-25\",16.9\\r\\n\"1983-02-26\",19.1\\r\\n\"1983-02-27\",20.4\\r\\n\"1983-02-28\",20.1\\r\\n\"1983-03-01\",19.9\\r\\n\"1983-03-02\",22.0\\r\\n\"1983-03-03\",20.5\\r\\n\"1983-03-04\",22.1\\r\\n\"1983-03-05\",20.6\\r\\n\"1983-03-06\",15.0\\r\\n\"1983-03-07\",20.6\\r\\n\"1983-03-08\",21.5\\r\\n\"1983-03-09\",16.2\\r\\n\"1983-03-10\",14.1\\r\\n\"1983-03-11\",14.5\\r\\n\"1983-03-12\",21.1\\r\\n\"1983-03-13\",15.9\\r\\n\"1983-03-14\",15.2\\r\\n\"1983-03-15\",13.1\\r\\n\"1983-03-16\",13.2\\r\\n\"1983-03-17\",12.5\\r\\n\"1983-03-18\",15.2\\r\\n\"1983-03-19\",17.6\\r\\n\"1983-03-20\",15.5\\r\\n\"1983-03-21\",16.7\\r\\n\"1983-03-22\",16.3\\r\\n\"1983-03-23\",15.1\\r\\n\"1983-03-24\",12.7\\r\\n\"1983-03-25\",10.0\\r\\n\"1983-03-26\",11.4\\r\\n\"1983-03-27\",12.6\\r\\n\"1983-03-28\",10.7\\r\\n\"1983-03-29\",10.0\\r\\n\"1983-03-30\",13.9\\r\\n\"1983-03-31\",13.4\\r\\n\"1983-04-01\",12.5\\r\\n\"1983-04-02\",12.8\\r\\n\"1983-04-03\",7.8\\r\\n\"1983-04-04\",11.1\\r\\n\"1983-04-05\",10.7\\r\\n\"1983-04-06\",7.1\\r\\n\"1983-04-07\",6.7\\r\\n\"1983-04-08\",5.7\\r\\n\"1983-04-09\",9.1\\r\\n\"1983-04-10\",15.2\\r\\n\"1983-04-11\",15.5\\r\\n\"1983-04-12\",11.1\\r\\n\"1983-04-13\",11.7\\r\\n\"1983-04-14\",11.5\\r\\n\"1983-04-15\",9.8\\r\\n\"1983-04-16\",6.2\\r\\n\"1983-04-17\",6.7\\r\\n\"1983-04-18\",7.5\\r\\n\"1983-04-19\",8.8\\r\\n\"1983-04-20\",8.0\\r\\n\"1983-04-21\",10.4\\r\\n\"1983-04-22\",14.5\\r\\n\"1983-04-23\",16.5\\r\\n\"1983-04-24\",14.1\\r\\n\"1983-04-25\",10.5\\r\\n\"1983-04-26\",12.6\\r\\n\"1983-04-27\",13.0\\r\\n\"1983-04-28\",8.7\\r\\n\"1983-04-29\",10.1\\r\\n\"1983-04-30\",12.0\\r\\n\"1983-05-01\",12.5\\r\\n\"1983-05-02\",13.5\\r\\n\"1983-05-03\",13.7\\r\\n\"1983-05-04\",13.5\\r\\n\"1983-05-05\",10.7\\r\\n\"1983-05-06\",13.0\\r\\n\"1983-05-07\",11.6\\r\\n\"1983-05-08\",13.0\\r\\n\"1983-05-09\",11.2\\r\\n\"1983-05-10\",13.5\\r\\n\"1983-05-11\",12.9\\r\\n\"1983-05-12\",6.8\\r\\n\"1983-05-13\",10.0\\r\\n\"1983-05-14\",14.5\\r\\n\"1983-05-15\",11.7\\r\\n\"1983-05-16\",6.7\\r\\n\"1983-05-17\",4.6\\r\\n\"1983-05-18\",4.9\\r\\n\"1983-05-19\",7.4\\r\\n\"1983-05-20\",8.3\\r\\n\"1983-05-21\",7.5\\r\\n\"1983-05-22\",6.2\\r\\n\"1983-05-23\",7.8\\r\\n\"1983-05-24\",13.2\\r\\n\"1983-05-25\",11.9\\r\\n\"1983-05-26\",6.5\\r\\n\"1983-05-27\",8.3\\r\\n\"1983-05-28\",12.1\\r\\n\"1983-05-29\",9.3\\r\\n\"1983-05-30\",7.5\\r\\n\"1983-05-31\",9.3\\r\\n\"1983-06-01\",11.0\\r\\n\"1983-06-02\",10.8\\r\\n\"1983-06-03\",5.3\\r\\n\"1983-06-04\",7.6\\r\\n\"1983-06-05\",5.6\\r\\n\"1983-06-06\",7.2\\r\\n\"1983-06-07\",9.6\\r\\n\"1983-06-08\",7.0\\r\\n\"1983-06-09\",8.3\\r\\n\"1983-06-10\",7.8\\r\\n\"1983-06-11\",4.7\\r\\n\"1983-06-12\",6.8\\r\\n\"1983-06-13\",7.2\\r\\n\"1983-06-14\",8.3\\r\\n\"1983-06-15\",9.5\\r\\n\"1983-06-16\",4.7\\r\\n\"1983-06-17\",3.0\\r\\n\"1983-06-18\",1.5\\r\\n\"1983-06-19\",2.5\\r\\n\"1983-06-20\",6.2\\r\\n\"1983-06-21\",11.6\\r\\n\"1983-06-22\",6.6\\r\\n\"1983-06-23\",6.6\\r\\n\"1983-06-24\",8.0\\r\\n\"1983-06-25\",7.9\\r\\n\"1983-06-26\",3.3\\r\\n\"1983-06-27\",3.9\\r\\n\"1983-06-28\",6.0\\r\\n\"1983-06-29\",4.0\\r\\n\"1983-06-30\",5.5\\r\\n\"1983-07-01\",8.5\\r\\n\"1983-07-02\",9.8\\r\\n\"1983-07-03\",9.5\\r\\n\"1983-07-04\",7.2\\r\\n\"1983-07-05\",8.1\\r\\n\"1983-07-06\",8.0\\r\\n\"1983-07-07\",8.5\\r\\n\"1983-07-08\",8.8\\r\\n\"1983-07-09\",8.3\\r\\n\"1983-07-10\",2.4\\r\\n\"1983-07-11\",4.9\\r\\n\"1983-07-12\",5.9\\r\\n\"1983-07-13\",6.7\\r\\n\"1983-07-14\",8.4\\r\\n\"1983-07-15\",6.5\\r\\n\"1983-07-16\",7.9\\r\\n\"1983-07-17\",4.1\\r\\n\"1983-07-18\",5.4\\r\\n\"1983-07-19\",7.5\\r\\n\"1983-07-20\",3.9\\r\\n\"1983-07-21\",2.5\\r\\n\"1983-07-22\",5.3\\r\\n\"1983-07-23\",6.6\\r\\n\"1983-07-24\",0.0\\r\\n\"1983-07-25\",0.7\\r\\n\"1983-07-26\",7.6\\r\\n\"1983-07-27\",12.3\\r\\n\"1983-07-28\",9.2\\r\\n\"1983-07-29\",9.6\\r\\n\"1983-07-30\",9.5\\r\\n\"1983-07-31\",10.0\\r\\n\"1983-08-01\",7.7\\r\\n\"1983-08-02\",8.0\\r\\n\"1983-08-03\",8.3\\r\\n\"1983-08-04\",8.3\\r\\n\"1983-08-05\",4.5\\r\\n\"1983-08-06\",6.5\\r\\n\"1983-08-07\",9.4\\r\\n\"1983-08-08\",9.4\\r\\n\"1983-08-09\",10.5\\r\\n\"1983-08-10\",10.7\\r\\n\"1983-08-11\",9.9\\r\\n\"1983-08-12\",7.6\\r\\n\"1983-08-13\",5.8\\r\\n\"1983-08-14\",8.5\\r\\n\"1983-08-15\",13.8\\r\\n\"1983-08-16\",14.3\\r\\n\"1983-08-17\",8.3\\r\\n\"1983-08-18\",5.3\\r\\n\"1983-08-19\",3.0\\r\\n\"1983-08-20\",5.2\\r\\n\"1983-08-21\",10.3\\r\\n\"1983-08-22\",11.1\\r\\n\"1983-08-23\",10.5\\r\\n\"1983-08-24\",9.0\\r\\n\"1983-08-25\",13.0\\r\\n\"1983-08-26\",6.4\\r\\n\"1983-08-27\",8.4\\r\\n\"1983-08-28\",6.7\\r\\n\"1983-08-29\",8.3\\r\\n\"1983-08-30\",11.2\\r\\n\"1983-08-31\",10.0\\r\\n\"1983-09-01\",10.1\\r\\n\"1983-09-02\",10.6\\r\\n\"1983-09-03\",10.9\\r\\n\"1983-09-04\",5.7\\r\\n\"1983-09-05\",9.5\\r\\n\"1983-09-06\",10.4\\r\\n\"1983-09-07\",11.1\\r\\n\"1983-09-08\",12.2\\r\\n\"1983-09-09\",10.6\\r\\n\"1983-09-10\",8.8\\r\\n\"1983-09-11\",9.2\\r\\n\"1983-09-12\",5.5\\r\\n\"1983-09-13\",7.1\\r\\n\"1983-09-14\",6.5\\r\\n\"1983-09-15\",4.3\\r\\n\"1983-09-16\",5.0\\r\\n\"1983-09-17\",11.2\\r\\n\"1983-09-18\",7.5\\r\\n\"1983-09-19\",12.0\\r\\n\"1983-09-20\",13.6\\r\\n\"1983-09-21\",8.3\\r\\n\"1983-09-22\",8.5\\r\\n\"1983-09-23\",12.9\\r\\n\"1983-09-24\",7.7\\r\\n\"1983-09-25\",7.6\\r\\n\"1983-09-26\",3.5\\r\\n\"1983-09-27\",10.4\\r\\n\"1983-09-28\",15.4\\r\\n\"1983-09-29\",10.6\\r\\n\"1983-09-30\",9.6\\r\\n\"1983-10-01\",9.3\\r\\n\"1983-10-02\",13.9\\r\\n\"1983-10-03\",7.7\\r\\n\"1983-10-04\",9.5\\r\\n\"1983-10-05\",7.6\\r\\n\"1983-10-06\",6.9\\r\\n\"1983-10-07\",6.8\\r\\n\"1983-10-08\",5.8\\r\\n\"1983-10-09\",6.0\\r\\n\"1983-10-10\",8.3\\r\\n\"1983-10-11\",9.1\\r\\n\"1983-10-12\",12.5\\r\\n\"1983-10-13\",13.2\\r\\n\"1983-10-14\",16.2\\r\\n\"1983-10-15\",12.5\\r\\n\"1983-10-16\",11.8\\r\\n\"1983-10-17\",10.6\\r\\n\"1983-10-18\",10.0\\r\\n\"1983-10-19\",12.2\\r\\n\"1983-10-20\",8.9\\r\\n\"1983-10-21\",10.3\\r\\n\"1983-10-22\",7.5\\r\\n\"1983-10-23\",11.6\\r\\n\"1983-10-24\",12.6\\r\\n\"1983-10-25\",12.9\\r\\n\"1983-10-26\",11.7\\r\\n\"1983-10-27\",14.0\\r\\n\"1983-10-28\",12.3\\r\\n\"1983-10-29\",9.0\\r\\n\"1983-10-30\",9.2\\r\\n\"1983-10-31\",9.8\\r\\n\"1983-11-01\",11.8\\r\\n\"1983-11-02\",10.6\\r\\n\"1983-11-03\",12.6\\r\\n\"1983-11-04\",11.0\\r\\n\"1983-11-05\",8.2\\r\\n\"1983-11-06\",7.5\\r\\n\"1983-11-07\",13.6\\r\\n\"1983-11-08\",14.8\\r\\n\"1983-11-09\",10.9\\r\\n\"1983-11-10\",7.7\\r\\n\"1983-11-11\",10.2\\r\\n\"1983-11-12\",10.8\\r\\n\"1983-11-13\",10.8\\r\\n\"1983-11-14\",12.5\\r\\n\"1983-11-15\",13.2\\r\\n\"1983-11-16\",8.7\\r\\n\"1983-11-17\",5.7\\r\\n\"1983-11-18\",9.8\\r\\n\"1983-11-19\",7.3\\r\\n\"1983-11-20\",10.8\\r\\n\"1983-11-21\",10.0\\r\\n\"1983-11-22\",16.2\\r\\n\"1983-11-23\",15.0\\r\\n\"1983-11-24\",14.5\\r\\n\"1983-11-25\",15.9\\r\\n\"1983-11-26\",14.9\\r\\n\"1983-11-27\",14.2\\r\\n\"1983-11-28\",15.8\\r\\n\"1983-11-29\",17.2\\r\\n\"1983-11-30\",17.6\\r\\n\"1983-12-01\",12.1\\r\\n\"1983-12-02\",11.4\\r\\n\"1983-12-03\",13.0\\r\\n\"1983-12-04\",13.2\\r\\n\"1983-12-05\",12.0\\r\\n\"1983-12-06\",15.3\\r\\n\"1983-12-07\",12.7\\r\\n\"1983-12-08\",12.1\\r\\n\"1983-12-09\",13.8\\r\\n\"1983-12-10\",10.9\\r\\n\"1983-12-11\",12.0\\r\\n\"1983-12-12\",16.5\\r\\n\"1983-12-13\",15.0\\r\\n\"1983-12-14\",11.2\\r\\n\"1983-12-15\",13.9\\r\\n\"1983-12-16\",15.0\\r\\n\"1983-12-17\",14.8\\r\\n\"1983-12-18\",15.0\\r\\n\"1983-12-19\",13.3\\r\\n\"1983-12-20\",20.4\\r\\n\"1983-12-21\",18.0\\r\\n\"1983-12-22\",12.2\\r\\n\"1983-12-23\",16.7\\r\\n\"1983-12-24\",13.8\\r\\n\"1983-12-25\",17.5\\r\\n\"1983-12-26\",15.0\\r\\n\"1983-12-27\",13.9\\r\\n\"1983-12-28\",11.1\\r\\n\"1983-12-29\",16.1\\r\\n\"1983-12-30\",20.4\\r\\n\"1983-12-31\",18.0\\r\\n\"1984-01-01\",19.5\\r\\n\"1984-01-02\",17.1\\r\\n\"1984-01-03\",17.1\\r\\n\"1984-01-04\",12.0\\r\\n\"1984-01-05\",11.0\\r\\n\"1984-01-06\",16.3\\r\\n\"1984-01-07\",16.1\\r\\n\"1984-01-08\",13.0\\r\\n\"1984-01-09\",13.4\\r\\n\"1984-01-10\",15.2\\r\\n\"1984-01-11\",12.5\\r\\n\"1984-01-12\",14.3\\r\\n\"1984-01-13\",16.5\\r\\n\"1984-01-14\",18.6\\r\\n\"1984-01-15\",18.0\\r\\n\"1984-01-16\",18.2\\r\\n\"1984-01-17\",11.4\\r\\n\"1984-01-18\",11.9\\r\\n\"1984-01-19\",12.2\\r\\n\"1984-01-20\",14.8\\r\\n\"1984-01-21\",13.1\\r\\n\"1984-01-22\",12.7\\r\\n\"1984-01-23\",10.5\\r\\n\"1984-01-24\",13.8\\r\\n\"1984-01-25\",18.8\\r\\n\"1984-01-26\",13.9\\r\\n\"1984-01-27\",11.2\\r\\n\"1984-01-28\",10.6\\r\\n\"1984-01-29\",14.7\\r\\n\"1984-01-30\",13.1\\r\\n\"1984-01-31\",12.1\\r\\n\"1984-02-01\",14.7\\r\\n\"1984-02-02\",11.1\\r\\n\"1984-02-03\",13.0\\r\\n\"1984-02-04\",15.6\\r\\n\"1984-02-05\",14.2\\r\\n\"1984-02-06\",15.5\\r\\n\"1984-02-07\",18.0\\r\\n\"1984-02-08\",15.0\\r\\n\"1984-02-09\",15.9\\r\\n\"1984-02-10\",15.5\\r\\n\"1984-02-11\",15.8\\r\\n\"1984-02-12\",16.6\\r\\n\"1984-02-13\",13.6\\r\\n\"1984-02-14\",13.8\\r\\n\"1984-02-15\",14.6\\r\\n\"1984-02-16\",15.6\\r\\n\"1984-02-17\",16.6\\r\\n\"1984-02-18\",14.3\\r\\n\"1984-02-19\",16.3\\r\\n\"1984-02-20\",18.9\\r\\n\"1984-02-21\",18.7\\r\\n\"1984-02-22\",14.5\\r\\n\"1984-02-23\",16.5\\r\\n\"1984-02-24\",14.1\\r\\n\"1984-02-25\",13.5\\r\\n\"1984-02-26\",11.7\\r\\n\"1984-02-27\",15.1\\r\\n\"1984-02-28\",11.2\\r\\n\"1984-02-29\",13.5\\r\\n\"1984-03-01\",12.6\\r\\n\"1984-03-02\",8.8\\r\\n\"1984-03-03\",10.5\\r\\n\"1984-03-04\",12.1\\r\\n\"1984-03-05\",14.5\\r\\n\"1984-03-06\",19.5\\r\\n\"1984-03-07\",14.0\\r\\n\"1984-03-08\",13.8\\r\\n\"1984-03-09\",10.5\\r\\n\"1984-03-10\",13.8\\r\\n\"1984-03-11\",11.4\\r\\n\"1984-03-12\",15.6\\r\\n\"1984-03-13\",11.1\\r\\n\"1984-03-14\",12.1\\r\\n\"1984-03-15\",14.2\\r\\n\"1984-03-16\",10.9\\r\\n\"1984-03-17\",14.2\\r\\n\"1984-03-18\",13.8\\r\\n\"1984-03-19\",15.1\\r\\n\"1984-03-20\",14.0\\r\\n\"1984-03-21\",12.1\\r\\n\"1984-03-22\",13.8\\r\\n\"1984-03-23\",16.6\\r\\n\"1984-03-24\",17.8\\r\\n\"1984-03-25\",9.4\\r\\n\"1984-03-26\",10.2\\r\\n\"1984-03-27\",7.4\\r\\n\"1984-03-28\",8.7\\r\\n\"1984-03-29\",14.0\\r\\n\"1984-03-30\",15.3\\r\\n\"1984-03-31\",11.1\\r\\n\"1984-04-01\",9.7\\r\\n\"1984-04-02\",10.3\\r\\n\"1984-04-03\",9.2\\r\\n\"1984-04-04\",8.2\\r\\n\"1984-04-05\",9.7\\r\\n\"1984-04-06\",12.4\\r\\n\"1984-04-07\",12.5\\r\\n\"1984-04-08\",9.0\\r\\n\"1984-04-09\",9.7\\r\\n\"1984-04-10\",10.1\\r\\n\"1984-04-11\",11.2\\r\\n\"1984-04-12\",12.0\\r\\n\"1984-04-13\",11.1\\r\\n\"1984-04-14\",10.8\\r\\n\"1984-04-15\",12.8\\r\\n\"1984-04-16\",9.8\\r\\n\"1984-04-17\",13.7\\r\\n\"1984-04-18\",11.0\\r\\n\"1984-04-19\",13.2\\r\\n\"1984-04-20\",13.0\\r\\n\"1984-04-21\",10.2\\r\\n\"1984-04-22\",13.2\\r\\n\"1984-04-23\",9.3\\r\\n\"1984-04-24\",11.1\\r\\n\"1984-04-25\",10.3\\r\\n\"1984-04-26\",8.7\\r\\n\"1984-04-27\",11.7\\r\\n\"1984-04-28\",12.5\\r\\n\"1984-04-29\",6.5\\r\\n\"1984-04-30\",9.6\\r\\n\"1984-05-01\",13.8\\r\\n\"1984-05-02\",14.7\\r\\n\"1984-05-03\",9.1\\r\\n\"1984-05-04\",4.8\\r\\n\"1984-05-05\",3.3\\r\\n\"1984-05-06\",3.5\\r\\n\"1984-05-07\",5.7\\r\\n\"1984-05-08\",5.5\\r\\n\"1984-05-09\",7.0\\r\\n\"1984-05-10\",9.5\\r\\n\"1984-05-11\",9.9\\r\\n\"1984-05-12\",4.9\\r\\n\"1984-05-13\",6.3\\r\\n\"1984-05-14\",4.8\\r\\n\"1984-05-15\",6.2\\r\\n\"1984-05-16\",7.1\\r\\n\"1984-05-17\",7.5\\r\\n\"1984-05-18\",9.4\\r\\n\"1984-05-19\",8.7\\r\\n\"1984-05-20\",9.5\\r\\n\"1984-05-21\",12.1\\r\\n\"1984-05-22\",9.5\\r\\n\"1984-05-23\",9.3\\r\\n\"1984-05-24\",8.5\\r\\n\"1984-05-25\",8.0\\r\\n\"1984-05-26\",9.8\\r\\n\"1984-05-27\",6.2\\r\\n\"1984-05-28\",7.3\\r\\n\"1984-05-29\",10.9\\r\\n\"1984-05-30\",10.0\\r\\n\"1984-05-31\",8.7\\r\\n\"1984-06-01\",9.0\\r\\n\"1984-06-02\",10.8\\r\\n\"1984-06-03\",12.4\\r\\n\"1984-06-04\",7.2\\r\\n\"1984-06-05\",7.2\\r\\n\"1984-06-06\",11.1\\r\\n\"1984-06-07\",9.3\\r\\n\"1984-06-08\",10.1\\r\\n\"1984-06-09\",3.9\\r\\n\"1984-06-10\",5.0\\r\\n\"1984-06-11\",8.2\\r\\n\"1984-06-12\",2.8\\r\\n\"1984-06-13\",4.3\\r\\n\"1984-06-14\",8.1\\r\\n\"1984-06-15\",11.1\\r\\n\"1984-06-16\",4.7\\r\\n\"1984-06-17\",5.3\\r\\n\"1984-06-18\",10.0\\r\\n\"1984-06-19\",5.6\\r\\n\"1984-06-20\",2.2\\r\\n\"1984-06-21\",7.1\\r\\n\"1984-06-22\",8.3\\r\\n\"1984-06-23\",8.6\\r\\n\"1984-06-24\",10.1\\r\\n\"1984-06-25\",8.3\\r\\n\"1984-06-26\",7.2\\r\\n\"1984-06-27\",7.7\\r\\n\"1984-06-28\",7.8\\r\\n\"1984-06-29\",9.1\\r\\n\"1984-06-30\",9.4\\r\\n\"1984-07-01\",7.8\\r\\n\"1984-07-02\",2.6\\r\\n\"1984-07-03\",2.4\\r\\n\"1984-07-04\",3.9\\r\\n\"1984-07-05\",1.3\\r\\n\"1984-07-06\",2.1\\r\\n\"1984-07-07\",7.4\\r\\n\"1984-07-08\",7.2\\r\\n\"1984-07-09\",8.8\\r\\n\"1984-07-10\",8.9\\r\\n\"1984-07-11\",8.8\\r\\n\"1984-07-12\",8.0\\r\\n\"1984-07-13\",0.7\\r\\n\"1984-07-14\",?0.1\\r\\n\"1984-07-15\",0.9\\r\\n\"1984-07-16\",7.8\\r\\n\"1984-07-17\",7.2\\r\\n\"1984-07-18\",8.0\\r\\n\"1984-07-19\",4.6\\r\\n\"1984-07-20\",5.2\\r\\n\"1984-07-21\",5.8\\r\\n\"1984-07-22\",6.8\\r\\n\"1984-07-23\",8.1\\r\\n\"1984-07-24\",7.5\\r\\n\"1984-07-25\",5.4\\r\\n\"1984-07-26\",4.6\\r\\n\"1984-07-27\",6.4\\r\\n\"1984-07-28\",9.7\\r\\n\"1984-07-29\",7.0\\r\\n\"1984-07-30\",10.0\\r\\n\"1984-07-31\",10.6\\r\\n\"1984-08-01\",11.5\\r\\n\"1984-08-02\",10.2\\r\\n\"1984-08-03\",11.1\\r\\n\"1984-08-04\",11.0\\r\\n\"1984-08-05\",8.9\\r\\n\"1984-08-06\",9.9\\r\\n\"1984-08-07\",11.7\\r\\n\"1984-08-08\",11.6\\r\\n\"1984-08-09\",9.0\\r\\n\"1984-08-10\",6.3\\r\\n\"1984-08-11\",8.7\\r\\n\"1984-08-12\",8.5\\r\\n\"1984-08-13\",8.5\\r\\n\"1984-08-14\",8.0\\r\\n\"1984-08-15\",6.0\\r\\n\"1984-08-16\",8.0\\r\\n\"1984-08-17\",8.5\\r\\n\"1984-08-18\",7.7\\r\\n\"1984-08-19\",8.4\\r\\n\"1984-08-20\",9.0\\r\\n\"1984-08-21\",8.3\\r\\n\"1984-08-22\",6.8\\r\\n\"1984-08-23\",9.3\\r\\n\"1984-08-24\",6.7\\r\\n\"1984-08-25\",9.0\\r\\n\"1984-08-26\",7.3\\r\\n\"1984-08-27\",6.3\\r\\n\"1984-08-28\",7.9\\r\\n\"1984-08-29\",5.2\\r\\n\"1984-08-30\",9.0\\r\\n\"1984-08-31\",11.3\\r\\n\"1984-09-01\",9.2\\r\\n\"1984-09-02\",11.3\\r\\n\"1984-09-03\",7.0\\r\\n\"1984-09-04\",8.0\\r\\n\"1984-09-05\",4.6\\r\\n\"1984-09-06\",8.5\\r\\n\"1984-09-07\",9.5\\r\\n\"1984-09-08\",9.4\\r\\n\"1984-09-09\",10.5\\r\\n\"1984-09-10\",9.7\\r\\n\"1984-09-11\",4.9\\r\\n\"1984-09-12\",8.0\\r\\n\"1984-09-13\",5.8\\r\\n\"1984-09-14\",5.5\\r\\n\"1984-09-15\",10.9\\r\\n\"1984-09-16\",11.7\\r\\n\"1984-09-17\",9.2\\r\\n\"1984-09-18\",8.9\\r\\n\"1984-09-19\",11.3\\r\\n\"1984-09-20\",8.6\\r\\n\"1984-09-21\",6.2\\r\\n\"1984-09-22\",6.6\\r\\n\"1984-09-23\",9.1\\r\\n\"1984-09-24\",6.1\\r\\n\"1984-09-25\",7.5\\r\\n\"1984-09-26\",10.7\\r\\n\"1984-09-27\",6.3\\r\\n\"1984-09-28\",5.5\\r\\n\"1984-09-29\",6.7\\r\\n\"1984-09-30\",4.2\\r\\n\"1984-10-01\",11.3\\r\\n\"1984-10-02\",16.3\\r\\n\"1984-10-03\",10.5\\r\\n\"1984-10-04\",10.3\\r\\n\"1984-10-05\",7.9\\r\\n\"1984-10-06\",7.7\\r\\n\"1984-10-07\",16.0\\r\\n\"1984-10-08\",14.6\\r\\n\"1984-10-09\",12.5\\r\\n\"1984-10-10\",8.1\\r\\n\"1984-10-11\",12.2\\r\\n\"1984-10-12\",17.2\\r\\n\"1984-10-13\",9.4\\r\\n\"1984-10-14\",8.7\\r\\n\"1984-10-15\",5.9\\r\\n\"1984-10-16\",4.8\\r\\n\"1984-10-17\",7.4\\r\\n\"1984-10-18\",9.4\\r\\n\"1984-10-19\",9.7\\r\\n\"1984-10-20\",9.9\\r\\n\"1984-10-21\",6.5\\r\\n\"1984-10-22\",9.8\\r\\n\"1984-10-23\",18.2\\r\\n\"1984-10-24\",11.3\\r\\n\"1984-10-25\",9.1\\r\\n\"1984-10-26\",9.6\\r\\n\"1984-10-27\",13.5\\r\\n\"1984-10-28\",10.7\\r\\n\"1984-10-29\",10.0\\r\\n\"1984-10-30\",8.5\\r\\n\"1984-10-31\",12.6\\r\\n\"1984-11-01\",16.6\\r\\n\"1984-11-02\",11.6\\r\\n\"1984-11-03\",12.2\\r\\n\"1984-11-04\",11.2\\r\\n\"1984-11-05\",9.2\\r\\n\"1984-11-06\",9.9\\r\\n\"1984-11-07\",11.9\\r\\n\"1984-11-08\",15.6\\r\\n\"1984-11-09\",19.0\\r\\n\"1984-11-10\",12.8\\r\\n\"1984-11-11\",12.2\\r\\n\"1984-11-12\",12.0\\r\\n\"1984-11-13\",11.1\\r\\n\"1984-11-14\",11.8\\r\\n\"1984-11-15\",7.6\\r\\n\"1984-11-16\",13.0\\r\\n\"1984-11-17\",12.7\\r\\n\"1984-11-18\",16.0\\r\\n\"1984-11-19\",14.8\\r\\n\"1984-11-20\",14.2\\r\\n\"1984-11-21\",10.0\\r\\n\"1984-11-22\",8.8\\r\\n\"1984-11-23\",11.6\\r\\n\"1984-11-24\",8.6\\r\\n\"1984-11-25\",14.6\\r\\n\"1984-11-26\",24.3\\r\\n\"1984-11-27\",11.6\\r\\n\"1984-11-28\",10.8\\r\\n\"1984-11-29\",12.0\\r\\n\"1984-11-30\",11.0\\r\\n\"1984-12-01\",12.6\\r\\n\"1984-12-02\",10.8\\r\\n\"1984-12-03\",9.1\\r\\n\"1984-12-04\",11.0\\r\\n\"1984-12-05\",13.0\\r\\n\"1984-12-06\",12.8\\r\\n\"1984-12-07\",9.9\\r\\n\"1984-12-08\",11.6\\r\\n\"1984-12-09\",10.5\\r\\n\"1984-12-10\",15.9\\r\\n\"1984-12-11\",12.2\\r\\n\"1984-12-12\",13.0\\r\\n\"1984-12-13\",12.5\\r\\n\"1984-12-14\",12.5\\r\\n\"1984-12-15\",11.4\\r\\n\"1984-12-16\",12.1\\r\\n\"1984-12-17\",16.8\\r\\n\"1984-12-18\",12.1\\r\\n\"1984-12-19\",11.3\\r\\n\"1984-12-20\",10.4\\r\\n\"1984-12-21\",14.2\\r\\n\"1984-12-22\",11.4\\r\\n\"1984-12-23\",13.7\\r\\n\"1984-12-24\",16.5\\r\\n\"1984-12-25\",12.8\\r\\n\"1984-12-26\",12.2\\r\\n\"1984-12-27\",12.0\\r\\n\"1984-12-28\",12.6\\r\\n\"1984-12-29\",16.0\\r\\n\"1984-12-30\",16.4\\r\\n\"1985-01-01\",13.3\\r\\n\"1985-01-02\",15.2\\r\\n\"1985-01-03\",13.1\\r\\n\"1985-01-04\",12.7\\r\\n\"1985-01-05\",14.6\\r\\n\"1985-01-06\",11.0\\r\\n\"1985-01-07\",13.2\\r\\n\"1985-01-08\",12.2\\r\\n\"1985-01-09\",14.4\\r\\n\"1985-01-10\",13.7\\r\\n\"1985-01-11\",14.5\\r\\n\"1985-01-12\",14.1\\r\\n\"1985-01-13\",14.4\\r\\n\"1985-01-14\",19.7\\r\\n\"1985-01-15\",16.5\\r\\n\"1985-01-16\",15.9\\r\\n\"1985-01-17\",11.8\\r\\n\"1985-01-18\",12.0\\r\\n\"1985-01-19\",11.4\\r\\n\"1985-01-20\",14.4\\r\\n\"1985-01-21\",12.4\\r\\n\"1985-01-22\",15.1\\r\\n\"1985-01-23\",15.6\\r\\n\"1985-01-24\",15.2\\r\\n\"1985-01-25\",12.8\\r\\n\"1985-01-26\",13.3\\r\\n\"1985-01-27\",17.5\\r\\n\"1985-01-28\",15.4\\r\\n\"1985-01-29\",13.5\\r\\n\"1985-01-30\",16.7\\r\\n\"1985-01-31\",15.2\\r\\n\"1985-02-01\",14.9\\r\\n\"1985-02-02\",10.2\\r\\n\"1985-02-03\",13.6\\r\\n\"1985-02-04\",19.0\\r\\n\"1985-02-05\",15.7\\r\\n\"1985-02-06\",18.0\\r\\n\"1985-02-07\",14.8\\r\\n\"1985-02-08\",13.9\\r\\n\"1985-02-09\",13.0\\r\\n\"1985-02-10\",15.3\\r\\n\"1985-02-11\",14.3\\r\\n\"1985-02-12\",15.6\\r\\n\"1985-02-13\",16.0\\r\\n\"1985-02-14\",14.9\\r\\n\"1985-02-15\",11.1\\r\\n\"1985-02-16\",14.8\\r\\n\"1985-02-17\",13.0\\r\\n\"1985-02-18\",12.2\\r\\n\"1985-02-19\",10.9\\r\\n\"1985-02-20\",14.6\\r\\n\"1985-02-21\",16.6\\r\\n\"1985-02-22\",18.1\\r\\n\"1985-02-23\",13.4\\r\\n\"1985-02-24\",10.3\\r\\n\"1985-02-25\",13.6\\r\\n\"1985-02-26\",13.8\\r\\n\"1985-02-27\",10.3\\r\\n\"1985-02-28\",11.0\\r\\n\"1985-03-01\",14.3\\r\\n\"1985-03-02\",15.5\\r\\n\"1985-03-03\",14.7\\r\\n\"1985-03-04\",12.7\\r\\n\"1985-03-05\",10.7\\r\\n\"1985-03-06\",12.6\\r\\n\"1985-03-07\",9.8\\r\\n\"1985-03-08\",13.2\\r\\n\"1985-03-09\",15.2\\r\\n\"1985-03-10\",16.6\\r\\n\"1985-03-11\",21.0\\r\\n\"1985-03-12\",22.4\\r\\n\"1985-03-13\",17.0\\r\\n\"1985-03-14\",21.7\\r\\n\"1985-03-15\",21.4\\r\\n\"1985-03-16\",18.6\\r\\n\"1985-03-17\",16.2\\r\\n\"1985-03-18\",16.8\\r\\n\"1985-03-19\",17.0\\r\\n\"1985-03-20\",18.4\\r\\n\"1985-03-21\",17.2\\r\\n\"1985-03-22\",18.4\\r\\n\"1985-03-23\",18.8\\r\\n\"1985-03-24\",16.5\\r\\n\"1985-03-25\",13.3\\r\\n\"1985-03-26\",12.2\\r\\n\"1985-03-27\",11.3\\r\\n\"1985-03-28\",13.8\\r\\n\"1985-03-29\",16.6\\r\\n\"1985-03-30\",14.0\\r\\n\"1985-03-31\",14.3\\r\\n\"1985-04-01\",16.4\\r\\n\"1985-04-02\",11.9\\r\\n\"1985-04-03\",15.7\\r\\n\"1985-04-04\",17.6\\r\\n\"1985-04-05\",17.5\\r\\n\"1985-04-06\",15.9\\r\\n\"1985-04-07\",16.2\\r\\n\"1985-04-08\",16.0\\r\\n\"1985-04-09\",15.9\\r\\n\"1985-04-10\",16.2\\r\\n\"1985-04-11\",16.2\\r\\n\"1985-04-12\",19.5\\r\\n\"1985-04-13\",18.2\\r\\n\"1985-04-14\",21.8\\r\\n\"1985-04-15\",15.1\\r\\n\"1985-04-16\",11.0\\r\\n\"1985-04-17\",8.1\\r\\n\"1985-04-18\",9.5\\r\\n\"1985-04-19\",9.3\\r\\n\"1985-04-20\",10.6\\r\\n\"1985-04-21\",6.3\\r\\n\"1985-04-22\",8.6\\r\\n\"1985-04-23\",6.8\\r\\n\"1985-04-24\",8.7\\r\\n\"1985-04-25\",8.4\\r\\n\"1985-04-26\",9.3\\r\\n\"1985-04-27\",10.0\\r\\n\"1985-04-28\",10.5\\r\\n\"1985-04-29\",12.0\\r\\n\"1985-04-30\",10.1\\r\\n\"1985-05-01\",9.4\\r\\n\"1985-05-02\",10.1\\r\\n\"1985-05-03\",8.0\\r\\n\"1985-05-04\",10.6\\r\\n\"1985-05-05\",13.6\\r\\n\"1985-05-06\",15.4\\r\\n\"1985-05-07\",9.0\\r\\n\"1985-05-08\",10.4\\r\\n\"1985-05-09\",11.0\\r\\n\"1985-05-10\",12.1\\r\\n\"1985-05-11\",13.4\\r\\n\"1985-05-12\",11.3\\r\\n\"1985-05-13\",6.7\\r\\n\"1985-05-14\",9.8\\r\\n\"1985-05-15\",10.8\\r\\n\"1985-05-16\",7.8\\r\\n\"1985-05-17\",4.5\\r\\n\"1985-05-18\",7.6\\r\\n\"1985-05-19\",6.9\\r\\n\"1985-05-20\",7.5\\r\\n\"1985-05-21\",8.5\\r\\n\"1985-05-22\",5.5\\r\\n\"1985-05-23\",9.5\\r\\n\"1985-05-24\",7.3\\r\\n\"1985-05-25\",5.4\\r\\n\"1985-05-26\",5.5\\r\\n\"1985-05-27\",8.1\\r\\n\"1985-05-28\",11.2\\r\\n\"1985-05-29\",13.4\\r\\n\"1985-05-30\",11.6\\r\\n\"1985-05-31\",10.1\\r\\n\"1985-06-01\",4.3\\r\\n\"1985-06-02\",5.5\\r\\n\"1985-06-03\",4.4\\r\\n\"1985-06-04\",5.9\\r\\n\"1985-06-05\",5.7\\r\\n\"1985-06-06\",8.2\\r\\n\"1985-06-07\",8.2\\r\\n\"1985-06-08\",4.2\\r\\n\"1985-06-09\",6.5\\r\\n\"1985-06-10\",10.0\\r\\n\"1985-06-11\",8.8\\r\\n\"1985-06-12\",6.6\\r\\n\"1985-06-13\",7.8\\r\\n\"1985-06-14\",10.1\\r\\n\"1985-06-15\",7.1\\r\\n\"1985-06-16\",7.7\\r\\n\"1985-06-17\",8.5\\r\\n\"1985-06-18\",7.3\\r\\n\"1985-06-19\",6.9\\r\\n\"1985-06-20\",8.4\\r\\n\"1985-06-21\",7.1\\r\\n\"1985-06-22\",6.3\\r\\n\"1985-06-23\",0.6\\r\\n\"1985-06-24\",1.6\\r\\n\"1985-06-25\",7.0\\r\\n\"1985-06-26\",8.3\\r\\n\"1985-06-27\",8.0\\r\\n\"1985-06-28\",10.2\\r\\n\"1985-06-29\",10.6\\r\\n\"1985-06-30\",10.4\\r\\n\"1985-07-01\",11.6\\r\\n\"1985-07-02\",11.0\\r\\n\"1985-07-03\",10.7\\r\\n\"1985-07-04\",7.3\\r\\n\"1985-07-05\",4.2\\r\\n\"1985-07-06\",4.7\\r\\n\"1985-07-07\",5.6\\r\\n\"1985-07-08\",7.7\\r\\n\"1985-07-09\",7.5\\r\\n\"1985-07-10\",4.9\\r\\n\"1985-07-11\",5.9\\r\\n\"1985-07-12\",7.8\\r\\n\"1985-07-13\",5.8\\r\\n\"1985-07-14\",7.0\\r\\n\"1985-07-15\",8.4\\r\\n\"1985-07-16\",6.2\\r\\n\"1985-07-17\",7.5\\r\\n\"1985-07-18\",4.8\\r\\n\"1985-07-19\",3.3\\r\\n\"1985-07-20\",3.2\\r\\n\"1985-07-21\",7.0\\r\\n\"1985-07-22\",8.4\\r\\n\"1985-07-23\",0.3\\r\\n\"1985-07-24\",0.3\\r\\n\"1985-07-25\",2.1\\r\\n\"1985-07-26\",8.5\\r\\n\"1985-07-27\",1.4\\r\\n\"1985-07-28\",4.1\\r\\n\"1985-07-29\",10.3\\r\\n\"1985-07-30\",6.6\\r\\n\"1985-07-31\",6.1\\r\\n\"1985-08-01\",7.0\\r\\n\"1985-08-02\",5.1\\r\\n\"1985-08-03\",6.3\\r\\n\"1985-08-04\",6.9\\r\\n\"1985-08-05\",11.4\\r\\n\"1985-08-06\",10.4\\r\\n\"1985-08-07\",10.3\\r\\n\"1985-08-08\",9.2\\r\\n\"1985-08-09\",7.2\\r\\n\"1985-08-10\",7.5\\r\\n\"1985-08-11\",4.0\\r\\n\"1985-08-12\",5.6\\r\\n\"1985-08-13\",6.7\\r\\n\"1985-08-14\",8.4\\r\\n\"1985-08-15\",11.0\\r\\n\"1985-08-16\",8.4\\r\\n\"1985-08-17\",8.8\\r\\n\"1985-08-18\",8.6\\r\\n\"1985-08-19\",8.3\\r\\n\"1985-08-20\",4.0\\r\\n\"1985-08-21\",3.6\\r\\n\"1985-08-22\",5.7\\r\\n\"1985-08-23\",10.6\\r\\n\"1985-08-24\",6.9\\r\\n\"1985-08-25\",10.0\\r\\n\"1985-08-26\",9.8\\r\\n\"1985-08-27\",7.2\\r\\n\"1985-08-28\",10.5\\r\\n\"1985-08-29\",3.6\\r\\n\"1985-08-30\",5.3\\r\\n\"1985-08-31\",8.4\\r\\n\"1985-09-01\",10.3\\r\\n\"1985-09-02\",7.9\\r\\n\"1985-09-03\",8.5\\r\\n\"1985-09-04\",7.9\\r\\n\"1985-09-05\",8.0\\r\\n\"1985-09-06\",9.8\\r\\n\"1985-09-07\",6.7\\r\\n\"1985-09-08\",4.8\\r\\n\"1985-09-09\",9.9\\r\\n\"1985-09-10\",12.8\\r\\n\"1985-09-11\",10.9\\r\\n\"1985-09-12\",11.7\\r\\n\"1985-09-13\",11.7\\r\\n\"1985-09-14\",11.0\\r\\n\"1985-09-15\",8.2\\r\\n\"1985-09-16\",7.5\\r\\n\"1985-09-17\",5.4\\r\\n\"1985-09-18\",7.2\\r\\n\"1985-09-19\",9.7\\r\\n\"1985-09-20\",8.4\\r\\n\"1985-09-21\",9.0\\r\\n\"1985-09-22\",8.7\\r\\n\"1985-09-23\",6.6\\r\\n\"1985-09-24\",11.6\\r\\n\"1985-09-25\",13.1\\r\\n\"1985-09-26\",6.7\\r\\n\"1985-09-27\",6.5\\r\\n\"1985-09-28\",7.7\\r\\n\"1985-09-29\",8.7\\r\\n\"1985-09-30\",7.2\\r\\n\"1985-10-01\",10.5\\r\\n\"1985-10-02\",8.6\\r\\n\"1985-10-03\",7.2\\r\\n\"1985-10-04\",11.4\\r\\n\"1985-10-05\",16.2\\r\\n\"1985-10-06\",6.1\\r\\n\"1985-10-07\",9.6\\r\\n\"1985-10-08\",11.1\\r\\n\"1985-10-09\",13.6\\r\\n\"1985-10-10\",10.7\\r\\n\"1985-10-11\",14.7\\r\\n\"1985-10-12\",11.6\\r\\n\"1985-10-13\",7.3\\r\\n\"1985-10-14\",8.0\\r\\n\"1985-10-15\",9.6\\r\\n\"1985-10-16\",16.0\\r\\n\"1985-10-17\",15.1\\r\\n\"1985-10-18\",12.8\\r\\n\"1985-10-19\",6.2\\r\\n\"1985-10-20\",7.1\\r\\n\"1985-10-21\",8.4\\r\\n\"1985-10-22\",10.0\\r\\n\"1985-10-23\",12.7\\r\\n\"1985-10-24\",10.0\\r\\n\"1985-10-25\",10.2\\r\\n\"1985-10-26\",6.5\\r\\n\"1985-10-27\",9.2\\r\\n\"1985-10-28\",11.9\\r\\n\"1985-10-29\",14.7\\r\\n\"1985-10-30\",11.4\\r\\n\"1985-10-31\",6.8\\r\\n\"1985-11-01\",7.4\\r\\n\"1985-11-02\",11.2\\r\\n\"1985-11-03\",9.2\\r\\n\"1985-11-04\",12.6\\r\\n\"1985-11-05\",16.0\\r\\n\"1985-11-06\",17.1\\r\\n\"1985-11-07\",15.3\\r\\n\"1985-11-08\",13.3\\r\\n\"1985-11-09\",15.4\\r\\n\"1985-11-10\",13.2\\r\\n\"1985-11-11\",14.4\\r\\n\"1985-11-12\",14.0\\r\\n\"1985-11-13\",15.5\\r\\n\"1985-11-14\",21.0\\r\\n\"1985-11-15\",10.0\\r\\n\"1985-11-16\",9.6\\r\\n\"1985-11-17\",12.0\\r\\n\"1985-11-18\",12.2\\r\\n\"1985-11-19\",11.3\\r\\n\"1985-11-20\",13.2\\r\\n\"1985-11-21\",10.5\\r\\n\"1985-11-22\",10.1\\r\\n\"1985-11-23\",8.8\\r\\n\"1985-11-24\",13.7\\r\\n\"1985-11-25\",16.2\\r\\n\"1985-11-26\",16.0\\r\\n\"1985-11-27\",14.0\\r\\n\"1985-11-28\",13.7\\r\\n\"1985-11-29\",12.5\\r\\n\"1985-11-30\",12.8\\r\\n\"1985-12-01\",12.3\\r\\n\"1985-12-02\",15.2\\r\\n\"1985-12-03\",15.0\\r\\n\"1985-12-04\",16.4\\r\\n\"1985-12-05\",16.1\\r\\n\"1985-12-06\",14.6\\r\\n\"1985-12-07\",18.2\\r\\n\"1985-12-08\",16.4\\r\\n\"1985-12-09\",16.6\\r\\n\"1985-12-10\",14.7\\r\\n\"1985-12-11\",15.8\\r\\n\"1985-12-12\",14.1\\r\\n\"1985-12-13\",13.5\\r\\n\"1985-12-14\",13.6\\r\\n\"1985-12-15\",13.7\\r\\n\"1985-12-16\",13.6\\r\\n\"1985-12-17\",12.1\\r\\n\"1985-12-18\",12.7\\r\\n\"1985-12-19\",13.3\\r\\n\"1985-12-20\",14.2\\r\\n\"1985-12-21\",15.0\\r\\n\"1985-12-22\",13.7\\r\\n\"1985-12-23\",12.0\\r\\n\"1985-12-24\",13.1\\r\\n\"1985-12-25\",13.2\\r\\n\"1985-12-26\",13.3\\r\\n\"1985-12-27\",11.5\\r\\n\"1985-12-28\",10.8\\r\\n\"1985-12-29\",12.0\\r\\n\"1985-12-30\",16.3\\r\\n\"1985-12-31\",14.4\\r\\n\"1986-01-01\",12.9\\r\\n\"1986-01-02\",13.8\\r\\n\"1986-01-03\",10.6\\r\\n\"1986-01-04\",12.6\\r\\n\"1986-01-05\",13.7\\r\\n\"1986-01-06\",12.6\\r\\n\"1986-01-07\",13.1\\r\\n\"1986-01-08\",15.4\\r\\n\"1986-01-09\",11.9\\r\\n\"1986-01-10\",13.8\\r\\n\"1986-01-11\",14.4\\r\\n\"1986-01-12\",15.2\\r\\n\"1986-01-13\",12.5\\r\\n\"1986-01-14\",12.2\\r\\n\"1986-01-15\",16.1\\r\\n\"1986-01-16\",14.6\\r\\n\"1986-01-17\",11.6\\r\\n\"1986-01-18\",13.1\\r\\n\"1986-01-19\",12.8\\r\\n\"1986-01-20\",15.2\\r\\n\"1986-01-21\",13.8\\r\\n\"1986-01-22\",15.0\\r\\n\"1986-01-23\",13.5\\r\\n\"1986-01-24\",11.8\\r\\n\"1986-01-25\",15.3\\r\\n\"1986-01-26\",13.5\\r\\n\"1986-01-27\",15.3\\r\\n\"1986-01-28\",13.8\\r\\n\"1986-01-29\",15.8\\r\\n\"1986-01-30\",17.4\\r\\n\"1986-01-31\",15.3\\r\\n\"1986-02-01\",14.6\\r\\n\"1986-02-02\",14.8\\r\\n\"1986-02-03\",10.7\\r\\n\"1986-02-04\",11.6\\r\\n\"1986-02-05\",13.6\\r\\n\"1986-02-06\",14.4\\r\\n\"1986-02-07\",11.8\\r\\n\"1986-02-08\",15.8\\r\\n\"1986-02-09\",16.0\\r\\n\"1986-02-10\",11.8\\r\\n\"1986-02-11\",14.5\\r\\n\"1986-02-12\",10.7\\r\\n\"1986-02-13\",14.2\\r\\n\"1986-02-14\",19.5\\r\\n\"1986-02-15\",21.4\\r\\n\"1986-02-16\",17.9\\r\\n\"1986-02-17\",17.4\\r\\n\"1986-02-18\",12.7\\r\\n\"1986-02-19\",13.8\\r\\n\"1986-02-20\",14.0\\r\\n\"1986-02-21\",15.0\\r\\n\"1986-02-22\",14.5\\r\\n\"1986-02-23\",13.1\\r\\n\"1986-02-24\",11.4\\r\\n\"1986-02-25\",12.5\\r\\n\"1986-02-26\",12.0\\r\\n\"1986-02-27\",13.4\\r\\n\"1986-02-28\",14.4\\r\\n\"1986-03-01\",17.7\\r\\n\"1986-03-02\",13.9\\r\\n\"1986-03-03\",13.3\\r\\n\"1986-03-04\",14.6\\r\\n\"1986-03-05\",16.4\\r\\n\"1986-03-06\",16.8\\r\\n\"1986-03-07\",20.0\\r\\n\"1986-03-08\",12.5\\r\\n\"1986-03-09\",12.7\\r\\n\"1986-03-10\",11.7\\r\\n\"1986-03-11\",12.7\\r\\n\"1986-03-12\",8.6\\r\\n\"1986-03-13\",11.9\\r\\n\"1986-03-14\",16.0\\r\\n\"1986-03-15\",15.2\\r\\n\"1986-03-16\",13.4\\r\\n\"1986-03-17\",11.6\\r\\n\"1986-03-18\",11.1\\r\\n\"1986-03-19\",15.6\\r\\n\"1986-03-20\",17.0\\r\\n\"1986-03-21\",18.5\\r\\n\"1986-03-22\",17.4\\r\\n\"1986-03-23\",16.5\\r\\n\"1986-03-24\",16.2\\r\\n\"1986-03-25\",16.1\\r\\n\"1986-03-26\",13.2\\r\\n\"1986-03-27\",18.0\\r\\n\"1986-03-28\",12.8\\r\\n\"1986-03-29\",11.7\\r\\n\"1986-03-30\",16.7\\r\\n\"1986-03-31\",15.6\\r\\n\"1986-04-01\",10.2\\r\\n\"1986-04-02\",10.3\\r\\n\"1986-04-03\",15.0\\r\\n\"1986-04-04\",18.0\\r\\n\"1986-04-05\",13.8\\r\\n\"1986-04-06\",10.5\\r\\n\"1986-04-07\",11.8\\r\\n\"1986-04-08\",7.2\\r\\n\"1986-04-09\",11.6\\r\\n\"1986-04-10\",7.4\\r\\n\"1986-04-11\",14.2\\r\\n\"1986-04-12\",12.2\\r\\n\"1986-04-13\",9.0\\r\\n\"1986-04-14\",12.3\\r\\n\"1986-04-15\",19.7\\r\\n\"1986-04-16\",12.8\\r\\n\"1986-04-17\",12.4\\r\\n\"1986-04-18\",12.0\\r\\n\"1986-04-19\",12.0\\r\\n\"1986-04-20\",11.1\\r\\n\"1986-04-21\",12.7\\r\\n\"1986-04-22\",14.2\\r\\n\"1986-04-23\",11.6\\r\\n\"1986-04-24\",12.0\\r\\n\"1986-04-25\",11.5\\r\\n\"1986-04-26\",8.3\\r\\n\"1986-04-27\",10.5\\r\\n\"1986-04-28\",9.0\\r\\n\"1986-04-29\",6.9\\r\\n\"1986-04-30\",9.4\\r\\n\"1986-05-01\",11.1\\r\\n\"1986-05-02\",9.1\\r\\n\"1986-05-03\",7.7\\r\\n\"1986-05-04\",10.0\\r\\n\"1986-05-05\",10.4\\r\\n\"1986-05-06\",8.0\\r\\n\"1986-05-07\",9.8\\r\\n\"1986-05-08\",12.4\\r\\n\"1986-05-09\",12.9\\r\\n\"1986-05-10\",12.3\\r\\n\"1986-05-11\",6.9\\r\\n\"1986-05-12\",10.5\\r\\n\"1986-05-13\",11.0\\r\\n\"1986-05-14\",9.7\\r\\n\"1986-05-15\",11.1\\r\\n\"1986-05-16\",11.5\\r\\n\"1986-05-17\",13.4\\r\\n\"1986-05-18\",10.9\\r\\n\"1986-05-19\",12.0\\r\\n\"1986-05-20\",12.1\\r\\n\"1986-05-21\",10.4\\r\\n\"1986-05-22\",10.0\\r\\n\"1986-05-23\",9.6\\r\\n\"1986-05-24\",11.3\\r\\n\"1986-05-25\",8.5\\r\\n\"1986-05-26\",6.3\\r\\n\"1986-05-27\",8.2\\r\\n\"1986-05-28\",10.7\\r\\n\"1986-05-29\",10.3\\r\\n\"1986-05-30\",9.5\\r\\n\"1986-05-31\",10.9\\r\\n\"1986-06-01\",10.9\\r\\n\"1986-06-02\",4.3\\r\\n\"1986-06-03\",5.2\\r\\n\"1986-06-04\",11.0\\r\\n\"1986-06-05\",11.6\\r\\n\"1986-06-06\",10.6\\r\\n\"1986-06-07\",9.4\\r\\n\"1986-06-08\",10.0\\r\\n\"1986-06-09\",9.6\\r\\n\"1986-06-10\",9.5\\r\\n\"1986-06-11\",9.7\\r\\n\"1986-06-12\",9.6\\r\\n\"1986-06-13\",7.0\\r\\n\"1986-06-14\",7.0\\r\\n\"1986-06-15\",6.8\\r\\n\"1986-06-16\",6.9\\r\\n\"1986-06-17\",8.0\\r\\n\"1986-06-18\",7.6\\r\\n\"1986-06-19\",8.6\\r\\n\"1986-06-20\",5.7\\r\\n\"1986-06-21\",5.5\\r\\n\"1986-06-22\",5.7\\r\\n\"1986-06-23\",5.7\\r\\n\"1986-06-24\",6.6\\r\\n\"1986-06-25\",6.0\\r\\n\"1986-06-26\",6.9\\r\\n\"1986-06-27\",7.7\\r\\n\"1986-06-28\",8.0\\r\\n\"1986-06-29\",3.9\\r\\n\"1986-06-30\",0.8\\r\\n\"1986-07-01\",2.8\\r\\n\"1986-07-02\",8.0\\r\\n\"1986-07-03\",9.8\\r\\n\"1986-07-04\",11.4\\r\\n\"1986-07-05\",8.6\\r\\n\"1986-07-06\",5.2\\r\\n\"1986-07-07\",6.6\\r\\n\"1986-07-08\",5.7\\r\\n\"1986-07-09\",4.6\\r\\n\"1986-07-10\",5.8\\r\\n\"1986-07-11\",7.0\\r\\n\"1986-07-12\",4.8\\r\\n\"1986-07-13\",4.4\\r\\n\"1986-07-14\",4.4\\r\\n\"1986-07-15\",7.9\\r\\n\"1986-07-16\",10.6\\r\\n\"1986-07-17\",5.0\\r\\n\"1986-07-18\",7.6\\r\\n\"1986-07-19\",9.2\\r\\n\"1986-07-20\",9.7\\r\\n\"1986-07-21\",8.8\\r\\n\"1986-07-22\",6.8\\r\\n\"1986-07-23\",9.4\\r\\n\"1986-07-24\",11.0\\r\\n\"1986-07-25\",2.5\\r\\n\"1986-07-26\",2.1\\r\\n\"1986-07-27\",5.4\\r\\n\"1986-07-28\",6.2\\r\\n\"1986-07-29\",7.8\\r\\n\"1986-07-30\",7.4\\r\\n\"1986-07-31\",9.3\\r\\n\"1986-08-01\",9.3\\r\\n\"1986-08-02\",9.5\\r\\n\"1986-08-03\",8.5\\r\\n\"1986-08-04\",10.0\\r\\n\"1986-08-05\",7.7\\r\\n\"1986-08-06\",9.3\\r\\n\"1986-08-07\",9.1\\r\\n\"1986-08-08\",3.5\\r\\n\"1986-08-09\",3.6\\r\\n\"1986-08-10\",2.5\\r\\n\"1986-08-11\",1.7\\r\\n\"1986-08-12\",2.7\\r\\n\"1986-08-13\",2.9\\r\\n\"1986-08-14\",5.3\\r\\n\"1986-08-15\",7.7\\r\\n\"1986-08-16\",9.1\\r\\n\"1986-08-17\",9.4\\r\\n\"1986-08-18\",7.3\\r\\n\"1986-08-19\",8.4\\r\\n\"1986-08-20\",9.2\\r\\n\"1986-08-21\",6.6\\r\\n\"1986-08-22\",9.7\\r\\n\"1986-08-23\",12.4\\r\\n\"1986-08-24\",10.2\\r\\n\"1986-08-25\",5.9\\r\\n\"1986-08-26\",7.1\\r\\n\"1986-08-27\",7.5\\r\\n\"1986-08-28\",9.7\\r\\n\"1986-08-29\",12.2\\r\\n\"1986-08-30\",5.6\\r\\n\"1986-08-31\",5.4\\r\\n\"1986-09-01\",8.3\\r\\n\"1986-09-02\",10.6\\r\\n\"1986-09-03\",9.1\\r\\n\"1986-09-04\",11.3\\r\\n\"1986-09-05\",10.9\\r\\n\"1986-09-06\",8.9\\r\\n\"1986-09-07\",6.3\\r\\n\"1986-09-08\",9.0\\r\\n\"1986-09-09\",6.1\\r\\n\"1986-09-10\",9.1\\r\\n\"1986-09-11\",9.6\\r\\n\"1986-09-12\",6.0\\r\\n\"1986-09-13\",10.0\\r\\n\"1986-09-14\",11.0\\r\\n\"1986-09-15\",6.2\\r\\n\"1986-09-16\",8.3\\r\\n\"1986-09-17\",11.3\\r\\n\"1986-09-18\",11.3\\r\\n\"1986-09-19\",6.7\\r\\n\"1986-09-20\",6.6\\r\\n\"1986-09-21\",11.4\\r\\n\"1986-09-22\",6.9\\r\\n\"1986-09-23\",10.6\\r\\n\"1986-09-24\",8.6\\r\\n\"1986-09-25\",11.3\\r\\n\"1986-09-26\",12.5\\r\\n\"1986-09-27\",9.9\\r\\n\"1986-09-28\",6.9\\r\\n\"1986-09-29\",5.5\\r\\n\"1986-09-30\",7.8\\r\\n\"1986-10-01\",11.0\\r\\n\"1986-10-02\",16.2\\r\\n\"1986-10-03\",9.9\\r\\n\"1986-10-04\",8.7\\r\\n\"1986-10-05\",10.5\\r\\n\"1986-10-06\",12.2\\r\\n\"1986-10-07\",10.6\\r\\n\"1986-10-08\",8.3\\r\\n\"1986-10-09\",5.5\\r\\n\"1986-10-10\",9.0\\r\\n\"1986-10-11\",6.4\\r\\n\"1986-10-12\",7.2\\r\\n\"1986-10-13\",12.9\\r\\n\"1986-10-14\",12.0\\r\\n\"1986-10-15\",7.3\\r\\n\"1986-10-16\",9.7\\r\\n\"1986-10-17\",8.4\\r\\n\"1986-10-18\",14.7\\r\\n\"1986-10-19\",9.5\\r\\n\"1986-10-20\",7.9\\r\\n\"1986-10-21\",6.8\\r\\n\"1986-10-22\",12.6\\r\\n\"1986-10-23\",5.2\\r\\n\"1986-10-24\",7.5\\r\\n\"1986-10-25\",8.7\\r\\n\"1986-10-26\",7.6\\r\\n\"1986-10-27\",9.0\\r\\n\"1986-10-28\",7.2\\r\\n\"1986-10-29\",10.7\\r\\n\"1986-10-30\",13.1\\r\\n\"1986-10-31\",13.9\\r\\n\"1986-11-01\",10.8\\r\\n\"1986-11-02\",10.4\\r\\n\"1986-11-03\",9.1\\r\\n\"1986-11-04\",16.0\\r\\n\"1986-11-05\",21.0\\r\\n\"1986-11-06\",16.2\\r\\n\"1986-11-07\",8.6\\r\\n\"1986-11-08\",9.2\\r\\n\"1986-11-09\",12.5\\r\\n\"1986-11-10\",9.7\\r\\n\"1986-11-11\",12.5\\r\\n\"1986-11-12\",10.3\\r\\n\"1986-11-13\",12.0\\r\\n\"1986-11-14\",11.0\\r\\n\"1986-11-15\",14.8\\r\\n\"1986-11-16\",15.0\\r\\n\"1986-11-17\",15.3\\r\\n\"1986-11-18\",10.3\\r\\n\"1986-11-19\",10.7\\r\\n\"1986-11-20\",10.5\\r\\n\"1986-11-21\",8.9\\r\\n\"1986-11-22\",8.1\\r\\n\"1986-11-23\",11.5\\r\\n\"1986-11-24\",12.8\\r\\n\"1986-11-25\",9.1\\r\\n\"1986-11-26\",14.6\\r\\n\"1986-11-27\",11.6\\r\\n\"1986-11-28\",11.2\\r\\n\"1986-11-29\",12.6\\r\\n\"1986-11-30\",7.5\\r\\n\"1986-12-01\",11.0\\r\\n\"1986-12-02\",14.5\\r\\n\"1986-12-03\",18.5\\r\\n\"1986-12-04\",15.4\\r\\n\"1986-12-05\",13.1\\r\\n\"1986-12-06\",16.3\\r\\n\"1986-12-07\",20.2\\r\\n\"1986-12-08\",11.5\\r\\n\"1986-12-09\",12.4\\r\\n\"1986-12-10\",10.9\\r\\n\"1986-12-11\",12.7\\r\\n\"1986-12-12\",12.2\\r\\n\"1986-12-13\",12.4\\r\\n\"1986-12-14\",9.8\\r\\n\"1986-12-15\",8.5\\r\\n\"1986-12-16\",14.7\\r\\n\"1986-12-17\",12.0\\r\\n\"1986-12-18\",10.3\\r\\n\"1986-12-19\",11.0\\r\\n\"1986-12-20\",10.2\\r\\n\"1986-12-21\",12.6\\r\\n\"1986-12-22\",11.6\\r\\n\"1986-12-23\",9.7\\r\\n\"1986-12-24\",13.4\\r\\n\"1986-12-25\",10.5\\r\\n\"1986-12-26\",14.7\\r\\n\"1986-12-27\",14.6\\r\\n\"1986-12-28\",14.2\\r\\n\"1986-12-29\",13.2\\r\\n\"1986-12-30\",11.7\\r\\n\"1986-12-31\",17.2\\r\\n\"1987-01-01\",12.3\\r\\n\"1987-01-02\",13.8\\r\\n\"1987-01-03\",15.3\\r\\n\"1987-01-04\",15.6\\r\\n\"1987-01-05\",16.2\\r\\n\"1987-01-06\",16.3\\r\\n\"1987-01-07\",16.8\\r\\n\"1987-01-08\",11.0\\r\\n\"1987-01-09\",8.5\\r\\n\"1987-01-10\",13.2\\r\\n\"1987-01-11\",13.0\\r\\n\"1987-01-12\",12.4\\r\\n\"1987-01-13\",13.0\\r\\n\"1987-01-14\",16.6\\r\\n\"1987-01-15\",12.0\\r\\n\"1987-01-16\",12.4\\r\\n\"1987-01-17\",15.0\\r\\n\"1987-01-18\",11.8\\r\\n\"1987-01-19\",11.6\\r\\n\"1987-01-20\",12.2\\r\\n\"1987-01-21\",13.7\\r\\n\"1987-01-22\",11.2\\r\\n\"1987-01-23\",12.4\\r\\n\"1987-01-24\",11.5\\r\\n\"1987-01-25\",13.8\\r\\n\"1987-01-26\",15.7\\r\\n\"1987-01-27\",12.9\\r\\n\"1987-01-28\",11.5\\r\\n\"1987-01-29\",11.0\\r\\n\"1987-01-30\",12.7\\r\\n\"1987-01-31\",14.9\\r\\n\"1987-02-01\",16.5\\r\\n\"1987-02-02\",12.8\\r\\n\"1987-02-03\",12.7\\r\\n\"1987-02-04\",12.7\\r\\n\"1987-02-05\",11.6\\r\\n\"1987-02-06\",13.3\\r\\n\"1987-02-07\",15.2\\r\\n\"1987-02-08\",16.4\\r\\n\"1987-02-09\",11.9\\r\\n\"1987-02-10\",15.1\\r\\n\"1987-02-11\",10.6\\r\\n\"1987-02-12\",13.6\\r\\n\"1987-02-13\",12.1\\r\\n\"1987-02-14\",16.0\\r\\n\"1987-02-15\",16.8\\r\\n\"1987-02-16\",16.6\\r\\n\"1987-02-17\",15.6\\r\\n\"1987-02-18\",15.2\\r\\n\"1987-02-19\",17.7\\r\\n\"1987-02-20\",21.0\\r\\n\"1987-02-21\",13.4\\r\\n\"1987-02-22\",10.5\\r\\n\"1987-02-23\",9.5\\r\\n\"1987-02-24\",12.0\\r\\n\"1987-02-25\",10.4\\r\\n\"1987-02-26\",11.5\\r\\n\"1987-02-27\",13.2\\r\\n\"1987-02-28\",15.0\\r\\n\"1987-03-01\",14.1\\r\\n\"1987-03-02\",12.4\\r\\n\"1987-03-03\",13.4\\r\\n\"1987-03-04\",12.5\\r\\n\"1987-03-05\",14.3\\r\\n\"1987-03-06\",17.6\\r\\n\"1987-03-07\",10.4\\r\\n\"1987-03-08\",9.9\\r\\n\"1987-03-09\",10.2\\r\\n\"1987-03-10\",11.3\\r\\n\"1987-03-11\",9.5\\r\\n\"1987-03-12\",11.8\\r\\n\"1987-03-13\",11.5\\r\\n\"1987-03-14\",10.5\\r\\n\"1987-03-15\",10.8\\r\\n\"1987-03-16\",13.0\\r\\n\"1987-03-17\",18.5\\r\\n\"1987-03-18\",18.7\\r\\n\"1987-03-19\",15.0\\r\\n\"1987-03-20\",13.0\\r\\n\"1987-03-21\",11.3\\r\\n\"1987-03-22\",13.0\\r\\n\"1987-03-23\",13.3\\r\\n\"1987-03-24\",11.0\\r\\n\"1987-03-25\",10.3\\r\\n\"1987-03-26\",13.0\\r\\n\"1987-03-27\",12.3\\r\\n\"1987-03-28\",15.6\\r\\n\"1987-03-29\",10.2\\r\\n\"1987-03-30\",10.8\\r\\n\"1987-03-31\",12.0\\r\\n\"1987-04-01\",13.3\\r\\n\"1987-04-02\",11.7\\r\\n\"1987-04-03\",12.5\\r\\n\"1987-04-04\",13.7\\r\\n\"1987-04-05\",14.9\\r\\n\"1987-04-06\",20.2\\r\\n\"1987-04-07\",16.3\\r\\n\"1987-04-08\",13.9\\r\\n\"1987-04-09\",10.1\\r\\n\"1987-04-10\",7.3\\r\\n\"1987-04-11\",14.0\\r\\n\"1987-04-12\",17.7\\r\\n\"1987-04-13\",16.3\\r\\n\"1987-04-14\",10.6\\r\\n\"1987-04-15\",9.7\\r\\n\"1987-04-16\",7.8\\r\\n\"1987-04-17\",10.4\\r\\n\"1987-04-18\",10.4\\r\\n\"1987-04-19\",14.1\\r\\n\"1987-04-20\",7.1\\r\\n\"1987-04-21\",8.1\\r\\n\"1987-04-22\",7.8\\r\\n\"1987-04-23\",10.6\\r\\n\"1987-04-24\",9.1\\r\\n\"1987-04-25\",9.0\\r\\n\"1987-04-26\",11.9\\r\\n\"1987-04-27\",17.1\\r\\n\"1987-04-28\",16.8\\r\\n\"1987-04-29\",13.5\\r\\n\"1987-04-30\",11.6\\r\\n\"1987-05-01\",7.0\\r\\n\"1987-05-02\",9.7\\r\\n\"1987-05-03\",9.9\\r\\n\"1987-05-04\",11.2\\r\\n\"1987-05-05\",11.3\\r\\n\"1987-05-06\",11.8\\r\\n\"1987-05-07\",9.9\\r\\n\"1987-05-08\",7.1\\r\\n\"1987-05-09\",9.6\\r\\n\"1987-05-10\",9.8\\r\\n\"1987-05-11\",10.6\\r\\n\"1987-05-12\",12.8\\r\\n\"1987-05-13\",16.5\\r\\n\"1987-05-14\",11.7\\r\\n\"1987-05-15\",12.3\\r\\n\"1987-05-16\",12.2\\r\\n\"1987-05-17\",11.8\\r\\n\"1987-05-18\",10.7\\r\\n\"1987-05-19\",10.2\\r\\n\"1987-05-20\",10.0\\r\\n\"1987-05-21\",8.3\\r\\n\"1987-05-22\",6.6\\r\\n\"1987-05-23\",9.5\\r\\n\"1987-05-24\",12.3\\r\\n\"1987-05-25\",7.6\\r\\n\"1987-05-26\",9.3\\r\\n\"1987-05-27\",5.0\\r\\n\"1987-05-28\",4.3\\r\\n\"1987-05-29\",6.4\\r\\n\"1987-05-30\",10.8\\r\\n\"1987-05-31\",7.8\\r\\n\"1987-06-01\",8.5\\r\\n\"1987-06-02\",9.7\\r\\n\"1987-06-03\",10.0\\r\\n\"1987-06-04\",11.0\\r\\n\"1987-06-05\",10.2\\r\\n\"1987-06-06\",6.6\\r\\n\"1987-06-07\",6.1\\r\\n\"1987-06-08\",5.9\\r\\n\"1987-06-09\",8.9\\r\\n\"1987-06-10\",13.0\\r\\n\"1987-06-11\",12.6\\r\\n\"1987-06-12\",5.4\\r\\n\"1987-06-13\",6.0\\r\\n\"1987-06-14\",7.8\\r\\n\"1987-06-15\",9.0\\r\\n\"1987-06-16\",4.2\\r\\n\"1987-06-17\",3.0\\r\\n\"1987-06-18\",4.5\\r\\n\"1987-06-19\",6.2\\r\\n\"1987-06-20\",11.9\\r\\n\"1987-06-21\",11.8\\r\\n\"1987-06-22\",9.4\\r\\n\"1987-06-23\",9.6\\r\\n\"1987-06-24\",9.4\\r\\n\"1987-06-25\",7.0\\r\\n\"1987-06-26\",8.9\\r\\n\"1987-06-27\",9.3\\r\\n\"1987-06-28\",6.8\\r\\n\"1987-06-29\",7.5\\r\\n\"1987-06-30\",8.0\\r\\n\"1987-07-01\",8.3\\r\\n\"1987-07-02\",2.7\\r\\n\"1987-07-03\",3.9\\r\\n\"1987-07-04\",4.1\\r\\n\"1987-07-05\",5.0\\r\\n\"1987-07-06\",5.8\\r\\n\"1987-07-07\",4.4\\r\\n\"1987-07-08\",4.1\\r\\n\"1987-07-09\",5.8\\r\\n\"1987-07-10\",9.1\\r\\n\"1987-07-11\",7.9\\r\\n\"1987-07-12\",5.0\\r\\n\"1987-07-13\",2.8\\r\\n\"1987-07-14\",4.7\\r\\n\"1987-07-15\",8.9\\r\\n\"1987-07-16\",5.4\\r\\n\"1987-07-17\",7.1\\r\\n\"1987-07-18\",9.0\\r\\n\"1987-07-19\",9.4\\r\\n\"1987-07-20\",6.3\\r\\n\"1987-07-21\",7.0\\r\\n\"1987-07-22\",6.4\\r\\n\"1987-07-23\",6.7\\r\\n\"1987-07-24\",1.5\\r\\n\"1987-07-25\",2.9\\r\\n\"1987-07-26\",4.8\\r\\n\"1987-07-27\",6.3\\r\\n\"1987-07-28\",5.7\\r\\n\"1987-07-29\",7.0\\r\\n\"1987-07-30\",8.8\\r\\n\"1987-07-31\",8.7\\r\\n\"1987-08-01\",9.0\\r\\n\"1987-08-02\",9.6\\r\\n\"1987-08-03\",8.0\\r\\n\"1987-08-04\",8.4\\r\\n\"1987-08-05\",8.1\\r\\n\"1987-08-06\",9.0\\r\\n\"1987-08-07\",5.3\\r\\n\"1987-08-08\",8.9\\r\\n\"1987-08-09\",8.7\\r\\n\"1987-08-10\",4.9\\r\\n\"1987-08-11\",7.0\\r\\n\"1987-08-12\",7.5\\r\\n\"1987-08-13\",7.0\\r\\n\"1987-08-14\",9.1\\r\\n\"1987-08-15\",11.8\\r\\n\"1987-08-16\",9.9\\r\\n\"1987-08-17\",5.6\\r\\n\"1987-08-18\",4.2\\r\\n\"1987-08-19\",4.3\\r\\n\"1987-08-20\",8.0\\r\\n\"1987-08-21\",5.1\\r\\n\"1987-08-22\",9.4\\r\\n\"1987-08-23\",9.1\\r\\n\"1987-08-24\",9.7\\r\\n\"1987-08-25\",10.6\\r\\n\"1987-08-26\",8.6\\r\\n\"1987-08-27\",10.1\\r\\n\"1987-08-28\",11.0\\r\\n\"1987-08-29\",9.7\\r\\n\"1987-08-30\",5.0\\r\\n\"1987-08-31\",6.1\\r\\n\"1987-09-01\",5.4\\r\\n\"1987-09-02\",5.8\\r\\n\"1987-09-03\",7.3\\r\\n\"1987-09-04\",6.3\\r\\n\"1987-09-05\",4.8\\r\\n\"1987-09-06\",7.6\\r\\n\"1987-09-07\",8.1\\r\\n\"1987-09-08\",9.5\\r\\n\"1987-09-09\",10.3\\r\\n\"1987-09-10\",7.0\\r\\n\"1987-09-11\",9.0\\r\\n\"1987-09-12\",10.2\\r\\n\"1987-09-13\",6.8\\r\\n\"1987-09-14\",9.3\\r\\n\"1987-09-15\",9.8\\r\\n\"1987-09-16\",10.7\\r\\n\"1987-09-17\",7.8\\r\\n\"1987-09-18\",9.2\\r\\n\"1987-09-19\",15.0\\r\\n\"1987-09-20\",7.8\\r\\n\"1987-09-21\",5.3\\r\\n\"1987-09-22\",9.5\\r\\n\"1987-09-23\",7.6\\r\\n\"1987-09-24\",14.0\\r\\n\"1987-09-25\",14.9\\r\\n\"1987-09-26\",14.9\\r\\n\"1987-09-27\",19.2\\r\\n\"1987-09-28\",17.0\\r\\n\"1987-09-29\",13.0\\r\\n\"1987-09-30\",11.2\\r\\n\"1987-10-01\",9.5\\r\\n\"1987-10-02\",10.3\\r\\n\"1987-10-03\",9.3\\r\\n\"1987-10-04\",11.3\\r\\n\"1987-10-05\",6.5\\r\\n\"1987-10-06\",12.0\\r\\n\"1987-10-07\",8.3\\r\\n\"1987-10-08\",8.7\\r\\n\"1987-10-09\",8.7\\r\\n\"1987-10-10\",10.2\\r\\n\"1987-10-11\",6.9\\r\\n\"1987-10-12\",4.9\\r\\n\"1987-10-13\",10.0\\r\\n\"1987-10-14\",7.6\\r\\n\"1987-10-15\",14.5\\r\\n\"1987-10-16\",13.2\\r\\n\"1987-10-17\",9.9\\r\\n\"1987-10-18\",10.1\\r\\n\"1987-10-19\",11.3\\r\\n\"1987-10-20\",10.4\\r\\n\"1987-10-21\",10.9\\r\\n\"1987-10-22\",9.2\\r\\n\"1987-10-23\",10.5\\r\\n\"1987-10-24\",11.4\\r\\n\"1987-10-25\",13.5\\r\\n\"1987-10-26\",9.8\\r\\n\"1987-10-27\",13.1\\r\\n\"1987-10-28\",9.7\\r\\n\"1987-10-29\",11.4\\r\\n\"1987-10-30\",9.9\\r\\n\"1987-10-31\",14.4\\r\\n\"1987-11-01\",19.0\\r\\n\"1987-11-02\",23.0\\r\\n\"1987-11-03\",15.4\\r\\n\"1987-11-04\",9.6\\r\\n\"1987-11-05\",10.8\\r\\n\"1987-11-06\",12.1\\r\\n\"1987-11-07\",11.0\\r\\n\"1987-11-08\",12.6\\r\\n\"1987-11-09\",14.7\\r\\n\"1987-11-10\",11.1\\r\\n\"1987-11-11\",10.1\\r\\n\"1987-11-12\",11.4\\r\\n\"1987-11-13\",13.0\\r\\n\"1987-11-14\",11.9\\r\\n\"1987-11-15\",9.5\\r\\n\"1987-11-16\",13.5\\r\\n\"1987-11-17\",15.2\\r\\n\"1987-11-18\",18.4\\r\\n\"1987-11-19\",24.1\\r\\n\"1987-11-20\",14.1\\r\\n\"1987-11-21\",10.7\\r\\n\"1987-11-22\",8.7\\r\\n\"1987-11-23\",13.3\\r\\n\"1987-11-24\",11.6\\r\\n\"1987-11-25\",9.9\\r\\n\"1987-11-26\",10.8\\r\\n\"1987-11-27\",11.5\\r\\n\"1987-11-28\",10.0\\r\\n\"1987-11-29\",13.9\\r\\n\"1987-11-30\",13.6\\r\\n\"1987-12-01\",11.9\\r\\n\"1987-12-02\",11.1\\r\\n\"1987-12-03\",8.2\\r\\n\"1987-12-04\",9.4\\r\\n\"1987-12-05\",12.7\\r\\n\"1987-12-06\",11.6\\r\\n\"1987-12-07\",11.0\\r\\n\"1987-12-08\",11.3\\r\\n\"1987-12-09\",13.4\\r\\n\"1987-12-10\",14.9\\r\\n\"1987-12-11\",15.2\\r\\n\"1987-12-12\",13.9\\r\\n\"1987-12-13\",15.0\\r\\n\"1987-12-14\",16.2\\r\\n\"1987-12-15\",17.7\\r\\n\"1987-12-16\",20.5\\r\\n\"1987-12-17\",14.7\\r\\n\"1987-12-18\",12.5\\r\\n\"1987-12-19\",10.9\\r\\n\"1987-12-20\",12.8\\r\\n\"1987-12-21\",12.7\\r\\n\"1987-12-22\",11.2\\r\\n\"1987-12-23\",11.4\\r\\n\"1987-12-24\",11.2\\r\\n\"1987-12-25\",12.1\\r\\n\"1987-12-26\",12.7\\r\\n\"1987-12-27\",16.2\\r\\n\"1987-12-28\",14.2\\r\\n\"1987-12-29\",14.3\\r\\n\"1987-12-30\",13.3\\r\\n\"1987-12-31\",16.7\\r\\n\"1988-01-01\",15.3\\r\\n\"1988-01-02\",14.3\\r\\n\"1988-01-03\",13.5\\r\\n\"1988-01-04\",15.0\\r\\n\"1988-01-05\",13.6\\r\\n\"1988-01-06\",15.2\\r\\n\"1988-01-07\",17.0\\r\\n\"1988-01-08\",18.7\\r\\n\"1988-01-09\",16.5\\r\\n\"1988-01-10\",17.4\\r\\n\"1988-01-11\",18.3\\r\\n\"1988-01-12\",18.3\\r\\n\"1988-01-13\",22.4\\r\\n\"1988-01-14\",21.4\\r\\n\"1988-01-15\",20.9\\r\\n\"1988-01-16\",17.6\\r\\n\"1988-01-17\",15.5\\r\\n\"1988-01-18\",16.6\\r\\n\"1988-01-19\",16.2\\r\\n\"1988-01-20\",15.6\\r\\n\"1988-01-21\",14.5\\r\\n\"1988-01-22\",14.0\\r\\n\"1988-01-23\",15.6\\r\\n\"1988-01-24\",12.3\\r\\n\"1988-01-25\",11.6\\r\\n\"1988-01-26\",12.6\\r\\n\"1988-01-27\",14.9\\r\\n\"1988-01-28\",17.3\\r\\n\"1988-01-29\",21.4\\r\\n\"1988-01-30\",23.4\\r\\n\"1988-01-31\",14.4\\r\\n\"1988-02-01\",14.1\\r\\n\"1988-02-02\",15.0\\r\\n\"1988-02-03\",14.5\\r\\n\"1988-02-04\",15.1\\r\\n\"1988-02-05\",13.9\\r\\n\"1988-02-06\",13.4\\r\\n\"1988-02-07\",9.2\\r\\n\"1988-02-08\",12.5\\r\\n\"1988-02-09\",15.1\\r\\n\"1988-02-10\",12.1\\r\\n\"1988-02-11\",14.5\\r\\n\"1988-02-12\",16.3\\r\\n\"1988-02-13\",16.5\\r\\n\"1988-02-14\",14.9\\r\\n\"1988-02-15\",13.2\\r\\n\"1988-02-16\",11.8\\r\\n\"1988-02-17\",13.6\\r\\n\"1988-02-18\",16.2\\r\\n\"1988-02-19\",14.1\\r\\n\"1988-02-20\",13.5\\r\\n\"1988-02-21\",15.0\\r\\n\"1988-02-22\",14.8\\r\\n\"1988-02-23\",16.2\\r\\n\"1988-02-24\",16.2\\r\\n\"1988-02-25\",13.3\\r\\n\"1988-02-26\",15.3\\r\\n\"1988-02-27\",18.4\\r\\n\"1988-02-28\",16.2\\r\\n\"1988-02-29\",16.3\\r\\n\"1988-03-01\",12.4\\r\\n\"1988-03-02\",15.6\\r\\n\"1988-03-03\",14.9\\r\\n\"1988-03-04\",14.8\\r\\n\"1988-03-05\",12.7\\r\\n\"1988-03-06\",14.2\\r\\n\"1988-03-07\",16.8\\r\\n\"1988-03-08\",16.7\\r\\n\"1988-03-09\",16.2\\r\\n\"1988-03-10\",14.5\\r\\n\"1988-03-11\",10.0\\r\\n\"1988-03-12\",12.6\\r\\n\"1988-03-13\",11.9\\r\\n\"1988-03-14\",11.8\\r\\n\"1988-03-15\",13.4\\r\\n\"1988-03-16\",14.5\\r\\n\"1988-03-17\",15.7\\r\\n\"1988-03-18\",15.3\\r\\n\"1988-03-19\",13.9\\r\\n\"1988-03-20\",13.7\\r\\n\"1988-03-21\",15.1\\r\\n\"1988-03-22\",15.6\\r\\n\"1988-03-23\",14.4\\r\\n\"1988-03-24\",13.9\\r\\n\"1988-03-25\",16.2\\r\\n\"1988-03-26\",16.7\\r\\n\"1988-03-27\",15.5\\r\\n\"1988-03-28\",16.4\\r\\n\"1988-03-29\",17.5\\r\\n\"1988-03-30\",18.2\\r\\n\"1988-03-31\",16.1\\r\\n\"1988-04-01\",16.5\\r\\n\"1988-04-02\",14.6\\r\\n\"1988-04-03\",16.4\\r\\n\"1988-04-04\",13.6\\r\\n\"1988-04-05\",15.9\\r\\n\"1988-04-06\",11.9\\r\\n\"1988-04-07\",14.7\\r\\n\"1988-04-08\",9.4\\r\\n\"1988-04-09\",6.6\\r\\n\"1988-04-10\",7.9\\r\\n\"1988-04-11\",11.0\\r\\n\"1988-04-12\",15.7\\r\\n\"1988-04-13\",15.2\\r\\n\"1988-04-14\",15.9\\r\\n\"1988-04-15\",10.6\\r\\n\"1988-04-16\",8.3\\r\\n\"1988-04-17\",8.6\\r\\n\"1988-04-18\",12.7\\r\\n\"1988-04-19\",10.5\\r\\n\"1988-04-20\",12.0\\r\\n\"1988-04-21\",11.1\\r\\n\"1988-04-22\",13.0\\r\\n\"1988-04-23\",12.4\\r\\n\"1988-04-24\",13.3\\r\\n\"1988-04-25\",15.9\\r\\n\"1988-04-26\",12.0\\r\\n\"1988-04-27\",13.7\\r\\n\"1988-04-28\",17.6\\r\\n\"1988-04-29\",14.3\\r\\n\"1988-04-30\",13.7\\r\\n\"1988-05-01\",15.2\\r\\n\"1988-05-02\",14.5\\r\\n\"1988-05-03\",14.9\\r\\n\"1988-05-04\",15.5\\r\\n\"1988-05-05\",16.4\\r\\n\"1988-05-06\",14.5\\r\\n\"1988-05-07\",12.6\\r\\n\"1988-05-08\",13.6\\r\\n\"1988-05-09\",11.2\\r\\n\"1988-05-10\",11.0\\r\\n\"1988-05-11\",12.0\\r\\n\"1988-05-12\",6.8\\r\\n\"1988-05-13\",10.6\\r\\n\"1988-05-14\",13.1\\r\\n\"1988-05-15\",13.5\\r\\n\"1988-05-16\",11.7\\r\\n\"1988-05-17\",13.2\\r\\n\"1988-05-18\",12.0\\r\\n\"1988-05-19\",10.4\\r\\n\"1988-05-20\",10.0\\r\\n\"1988-05-21\",8.2\\r\\n\"1988-05-22\",9.4\\r\\n\"1988-05-23\",10.3\\r\\n\"1988-05-24\",8.1\\r\\n\"1988-05-25\",8.7\\r\\n\"1988-05-26\",12.6\\r\\n\"1988-05-27\",10.9\\r\\n\"1988-05-28\",8.7\\r\\n\"1988-05-29\",9.3\\r\\n\"1988-05-30\",6.3\\r\\n\"1988-05-31\",7.8\\r\\n\"1988-06-01\",10.0\\r\\n\"1988-06-02\",11.0\\r\\n\"1988-06-03\",11.1\\r\\n\"1988-06-04\",12.6\\r\\n\"1988-06-05\",10.2\\r\\n\"1988-06-06\",11.1\\r\\n\"1988-06-07\",8.7\\r\\n\"1988-06-08\",9.5\\r\\n\"1988-06-09\",9.7\\r\\n\"1988-06-10\",8.2\\r\\n\"1988-06-11\",5.0\\r\\n\"1988-06-12\",6.5\\r\\n\"1988-06-13\",12.1\\r\\n\"1988-06-14\",8.9\\r\\n\"1988-06-15\",6.1\\r\\n\"1988-06-16\",2.8\\r\\n\"1988-06-17\",3.7\\r\\n\"1988-06-18\",6.8\\r\\n\"1988-06-19\",6.6\\r\\n\"1988-06-20\",7.0\\r\\n\"1988-06-21\",7.3\\r\\n\"1988-06-22\",7.9\\r\\n\"1988-06-23\",10.6\\r\\n\"1988-06-24\",8.1\\r\\n\"1988-06-25\",6.7\\r\\n\"1988-06-26\",8.0\\r\\n\"1988-06-27\",10.0\\r\\n\"1988-06-28\",6.7\\r\\n\"1988-06-29\",9.4\\r\\n\"1988-06-30\",9.3\\r\\n\"1988-07-01\",6.0\\r\\n\"1988-07-02\",5.8\\r\\n\"1988-07-03\",4.9\\r\\n\"1988-07-04\",5.0\\r\\n\"1988-07-05\",8.4\\r\\n\"1988-07-06\",12.3\\r\\n\"1988-07-07\",13.0\\r\\n\"1988-07-08\",11.4\\r\\n\"1988-07-09\",6.8\\r\\n\"1988-07-10\",7.6\\r\\n\"1988-07-11\",12.4\\r\\n\"1988-07-12\",7.1\\r\\n\"1988-07-13\",7.5\\r\\n\"1988-07-14\",10.0\\r\\n\"1988-07-15\",5.3\\r\\n\"1988-07-16\",6.3\\r\\n\"1988-07-17\",8.0\\r\\n\"1988-07-18\",8.3\\r\\n\"1988-07-19\",9.3\\r\\n\"1988-07-20\",9.5\\r\\n\"1988-07-21\",5.6\\r\\n\"1988-07-22\",7.0\\r\\n\"1988-07-23\",8.5\\r\\n\"1988-07-24\",8.5\\r\\n\"1988-07-25\",8.2\\r\\n\"1988-07-26\",8.5\\r\\n\"1988-07-27\",9.6\\r\\n\"1988-07-28\",9.7\\r\\n\"1988-07-29\",7.1\\r\\n\"1988-07-30\",8.4\\r\\n\"1988-07-31\",9.2\\r\\n\"1988-08-01\",9.8\\r\\n\"1988-08-02\",8.1\\r\\n\"1988-08-03\",9.4\\r\\n\"1988-08-04\",10.0\\r\\n\"1988-08-05\",5.1\\r\\n\"1988-08-06\",6.7\\r\\n\"1988-08-07\",6.9\\r\\n\"1988-08-08\",6.8\\r\\n\"1988-08-09\",8.6\\r\\n\"1988-08-10\",9.1\\r\\n\"1988-08-11\",3.9\\r\\n\"1988-08-12\",4.8\\r\\n\"1988-08-13\",8.4\\r\\n\"1988-08-14\",11.6\\r\\n\"1988-08-15\",12.1\\r\\n\"1988-08-16\",12.4\\r\\n\"1988-08-17\",10.0\\r\\n\"1988-08-18\",10.1\\r\\n\"1988-08-19\",9.7\\r\\n\"1988-08-20\",11.7\\r\\n\"1988-08-21\",7.9\\r\\n\"1988-08-22\",8.6\\r\\n\"1988-08-23\",7.7\\r\\n\"1988-08-24\",5.8\\r\\n\"1988-08-25\",8.7\\r\\n\"1988-08-26\",10.6\\r\\n\"1988-08-27\",6.7\\r\\n\"1988-08-28\",8.8\\r\\n\"1988-08-29\",9.7\\r\\n\"1988-08-30\",9.0\\r\\n\"1988-08-31\",11.8\\r\\n\"1988-09-01\",15.2\\r\\n\"1988-09-02\",10.0\\r\\n\"1988-09-03\",10.5\\r\\n\"1988-09-04\",5.5\\r\\n\"1988-09-05\",9.4\\r\\n\"1988-09-06\",8.8\\r\\n\"1988-09-07\",5.3\\r\\n\"1988-09-08\",13.0\\r\\n\"1988-09-09\",15.2\\r\\n\"1988-09-10\",13.2\\r\\n\"1988-09-11\",11.5\\r\\n\"1988-09-12\",6.8\\r\\n\"1988-09-13\",4.7\\r\\n\"1988-09-14\",5.2\\r\\n\"1988-09-15\",6.8\\r\\n\"1988-09-16\",10.7\\r\\n\"1988-09-17\",10.1\\r\\n\"1988-09-18\",10.0\\r\\n\"1988-09-19\",9.8\\r\\n\"1988-09-20\",5.5\\r\\n\"1988-09-21\",13.5\\r\\n\"1988-09-22\",16.6\\r\\n\"1988-09-23\",8.4\\r\\n\"1988-09-24\",8.2\\r\\n\"1988-09-25\",11.1\\r\\n\"1988-09-26\",10.8\\r\\n\"1988-09-27\",8.8\\r\\n\"1988-09-28\",10.8\\r\\n\"1988-09-29\",8.7\\r\\n\"1988-09-30\",12.4\\r\\n\"1988-10-01\",9.0\\r\\n\"1988-10-02\",13.5\\r\\n\"1988-10-03\",14.7\\r\\n\"1988-10-04\",10.9\\r\\n\"1988-10-05\",8.5\\r\\n\"1988-10-06\",6.0\\r\\n\"1988-10-07\",12.7\\r\\n\"1988-10-08\",11.1\\r\\n\"1988-10-09\",8.7\\r\\n\"1988-10-10\",12.3\\r\\n\"1988-10-11\",13.3\\r\\n\"1988-10-12\",5.6\\r\\n\"1988-10-13\",13.7\\r\\n\"1988-10-14\",8.5\\r\\n\"1988-10-15\",11.2\\r\\n\"1988-10-16\",8.7\\r\\n\"1988-10-17\",11.7\\r\\n\"1988-10-18\",12.5\\r\\n\"1988-10-19\",8.2\\r\\n\"1988-10-20\",15.6\\r\\n\"1988-10-21\",10.3\\r\\n\"1988-10-22\",11.4\\r\\n\"1988-10-23\",9.7\\r\\n\"1988-10-24\",6.3\\r\\n\"1988-10-25\",14.3\\r\\n\"1988-10-26\",11.3\\r\\n\"1988-10-27\",7.3\\r\\n\"1988-10-28\",12.8\\r\\n\"1988-10-29\",11.9\\r\\n\"1988-10-30\",14.3\\r\\n\"1988-10-31\",11.6\\r\\n\"1988-11-01\",13.2\\r\\n\"1988-11-02\",15.5\\r\\n\"1988-11-03\",14.1\\r\\n\"1988-11-04\",9.5\\r\\n\"1988-11-05\",7.2\\r\\n\"1988-11-06\",11.8\\r\\n\"1988-11-07\",16.8\\r\\n\"1988-11-08\",12.5\\r\\n\"1988-11-09\",9.4\\r\\n\"1988-11-10\",11.9\\r\\n\"1988-11-11\",10.3\\r\\n\"1988-11-12\",16.9\\r\\n\"1988-11-13\",17.5\\r\\n\"1988-11-14\",7.5\\r\\n\"1988-11-15\",8.6\\r\\n\"1988-11-16\",11.1\\r\\n\"1988-11-17\",11.5\\r\\n\"1988-11-18\",10.7\\r\\n\"1988-11-19\",15.7\\r\\n\"1988-11-20\",12.8\\r\\n\"1988-11-21\",13.0\\r\\n\"1988-11-22\",12.9\\r\\n\"1988-11-23\",14.3\\r\\n\"1988-11-24\",13.7\\r\\n\"1988-11-25\",12.1\\r\\n\"1988-11-26\",11.9\\r\\n\"1988-11-27\",11.8\\r\\n\"1988-11-28\",11.4\\r\\n\"1988-11-29\",10.3\\r\\n\"1988-11-30\",11.7\\r\\n\"1988-12-01\",12.0\\r\\n\"1988-12-02\",17.4\\r\\n\"1988-12-03\",16.8\\r\\n\"1988-12-04\",16.2\\r\\n\"1988-12-05\",13.0\\r\\n\"1988-12-06\",12.5\\r\\n\"1988-12-07\",12.4\\r\\n\"1988-12-08\",16.1\\r\\n\"1988-12-09\",20.2\\r\\n\"1988-12-10\",14.3\\r\\n\"1988-12-11\",11.0\\r\\n\"1988-12-12\",14.4\\r\\n\"1988-12-13\",15.7\\r\\n\"1988-12-14\",19.7\\r\\n\"1988-12-15\",20.7\\r\\n\"1988-12-16\",23.9\\r\\n\"1988-12-17\",16.6\\r\\n\"1988-12-18\",17.5\\r\\n\"1988-12-19\",14.9\\r\\n\"1988-12-20\",13.6\\r\\n\"1988-12-21\",11.9\\r\\n\"1988-12-22\",15.2\\r\\n\"1988-12-23\",17.3\\r\\n\"1988-12-24\",19.8\\r\\n\"1988-12-25\",15.8\\r\\n\"1988-12-26\",9.5\\r\\n\"1988-12-27\",12.9\\r\\n\"1988-12-28\",12.9\\r\\n\"1988-12-29\",14.8\\r\\n\"1988-12-30\",14.1\\r\\n\"1989-01-01\",14.3\\r\\n\"1989-01-02\",17.4\\r\\n\"1989-01-03\",18.5\\r\\n\"1989-01-04\",16.8\\r\\n\"1989-01-05\",11.5\\r\\n\"1989-01-06\",9.5\\r\\n\"1989-01-07\",12.2\\r\\n\"1989-01-08\",15.7\\r\\n\"1989-01-09\",16.3\\r\\n\"1989-01-10\",13.6\\r\\n\"1989-01-11\",12.6\\r\\n\"1989-01-12\",13.8\\r\\n\"1989-01-13\",12.1\\r\\n\"1989-01-14\",13.4\\r\\n\"1989-01-15\",17.3\\r\\n\"1989-01-16\",19.4\\r\\n\"1989-01-17\",16.6\\r\\n\"1989-01-18\",13.9\\r\\n\"1989-01-19\",13.1\\r\\n\"1989-01-20\",16.0\\r\\n\"1989-01-21\",14.5\\r\\n\"1989-01-22\",15.0\\r\\n\"1989-01-23\",12.6\\r\\n\"1989-01-24\",12.5\\r\\n\"1989-01-25\",15.2\\r\\n\"1989-01-26\",16.2\\r\\n\"1989-01-27\",16.5\\r\\n\"1989-01-28\",20.1\\r\\n\"1989-01-29\",20.6\\r\\n\"1989-01-30\",16.9\\r\\n\"1989-01-31\",16.5\\r\\n\"1989-02-01\",16.1\\r\\n\"1989-02-02\",14.4\\r\\n\"1989-02-03\",16.3\\r\\n\"1989-02-04\",15.7\\r\\n\"1989-02-05\",14.2\\r\\n\"1989-02-06\",13.2\\r\\n\"1989-02-07\",16.8\\r\\n\"1989-02-08\",18.5\\r\\n\"1989-02-09\",16.7\\r\\n\"1989-02-10\",15.3\\r\\n\"1989-02-11\",15.9\\r\\n\"1989-02-12\",15.2\\r\\n\"1989-02-13\",17.5\\r\\n\"1989-02-14\",18.3\\r\\n\"1989-02-15\",19.4\\r\\n\"1989-02-16\",19.4\\r\\n\"1989-02-17\",19.5\\r\\n\"1989-02-18\",20.5\\r\\n\"1989-02-19\",15.7\\r\\n\"1989-02-20\",15.0\\r\\n\"1989-02-21\",16.1\\r\\n\"1989-02-22\",14.3\\r\\n\"1989-02-23\",13.0\\r\\n\"1989-02-24\",16.2\\r\\n\"1989-02-25\",17.7\\r\\n\"1989-02-26\",13.2\\r\\n\"1989-02-27\",15.8\\r\\n\"1989-02-28\",18.5\\r\\n\"1989-03-01\",20.4\\r\\n\"1989-03-02\",22.0\\r\\n\"1989-03-03\",19.7\\r\\n\"1989-03-04\",19.6\\r\\n\"1989-03-05\",20.3\\r\\n\"1989-03-06\",18.3\\r\\n\"1989-03-07\",18.9\\r\\n\"1989-03-08\",20.3\\r\\n\"1989-03-09\",21.4\\r\\n\"1989-03-10\",18.3\\r\\n\"1989-03-11\",17.8\\r\\n\"1989-03-12\",17.7\\r\\n\"1989-03-13\",12.8\\r\\n\"1989-03-14\",15.1\\r\\n\"1989-03-15\",15.0\\r\\n\"1989-03-16\",14.8\\r\\n\"1989-03-17\",12.0\\r\\n\"1989-03-18\",12.5\\r\\n\"1989-03-19\",15.0\\r\\n\"1989-03-20\",17.1\\r\\n\"1989-03-21\",17.3\\r\\n\"1989-03-22\",16.9\\r\\n\"1989-03-23\",16.5\\r\\n\"1989-03-24\",13.6\\r\\n\"1989-03-25\",13.2\\r\\n\"1989-03-26\",9.4\\r\\n\"1989-03-27\",9.5\\r\\n\"1989-03-28\",11.8\\r\\n\"1989-03-29\",10.4\\r\\n\"1989-03-30\",9.7\\r\\n\"1989-03-31\",12.6\\r\\n\"1989-04-01\",13.3\\r\\n\"1989-04-02\",15.1\\r\\n\"1989-04-03\",14.2\\r\\n\"1989-04-04\",14.2\\r\\n\"1989-04-05\",19.2\\r\\n\"1989-04-06\",12.6\\r\\n\"1989-04-07\",14.2\\r\\n\"1989-04-08\",11.9\\r\\n\"1989-04-09\",13.9\\r\\n\"1989-04-10\",13.5\\r\\n\"1989-04-11\",15.3\\r\\n\"1989-04-12\",13.9\\r\\n\"1989-04-13\",14.0\\r\\n\"1989-04-14\",12.9\\r\\n\"1989-04-15\",8.5\\r\\n\"1989-04-16\",11.4\\r\\n\"1989-04-17\",10.9\\r\\n\"1989-04-18\",12.0\\r\\n\"1989-04-19\",8.6\\r\\n\"1989-04-20\",9.0\\r\\n\"1989-04-21\",9.6\\r\\n\"1989-04-22\",10.2\\r\\n\"1989-04-23\",9.8\\r\\n\"1989-04-24\",8.3\\r\\n\"1989-04-25\",11.0\\r\\n\"1989-04-26\",11.9\\r\\n\"1989-04-27\",14.0\\r\\n\"1989-04-28\",15.8\\r\\n\"1989-04-29\",14.5\\r\\n\"1989-04-30\",13.2\\r\\n\"1989-05-01\",14.2\\r\\n\"1989-05-02\",14.6\\r\\n\"1989-05-03\",11.8\\r\\n\"1989-05-04\",14.4\\r\\n\"1989-05-05\",10.4\\r\\n\"1989-05-06\",10.3\\r\\n\"1989-05-07\",10.8\\r\\n\"1989-05-08\",10.5\\r\\n\"1989-05-09\",9.5\\r\\n\"1989-05-10\",12.5\\r\\n\"1989-05-11\",13.7\\r\\n\"1989-05-12\",12.7\\r\\n\"1989-05-13\",11.9\\r\\n\"1989-05-14\",11.4\\r\\n\"1989-05-15\",9.7\\r\\n\"1989-05-16\",8.3\\r\\n\"1989-05-17\",8.1\\r\\n\"1989-05-18\",11.7\\r\\n\"1989-05-19\",11.6\\r\\n\"1989-05-20\",7.4\\r\\n\"1989-05-21\",5.2\\r\\n\"1989-05-22\",11.0\\r\\n\"1989-05-23\",9.5\\r\\n\"1989-05-24\",9.2\\r\\n\"1989-05-25\",10.7\\r\\n\"1989-05-26\",9.0\\r\\n\"1989-05-27\",10.2\\r\\n\"1989-05-28\",10.3\\r\\n\"1989-05-29\",12.1\\r\\n\"1989-05-30\",13.2\\r\\n\"1989-05-31\",6.6\\r\\n\"1989-06-01\",2.3\\r\\n\"1989-06-02\",1.4\\r\\n\"1989-06-03\",2.1\\r\\n\"1989-06-04\",6.6\\r\\n\"1989-06-05\",8.9\\r\\n\"1989-06-06\",7.8\\r\\n\"1989-06-07\",9.0\\r\\n\"1989-06-08\",10.3\\r\\n\"1989-06-09\",7.9\\r\\n\"1989-06-10\",7.2\\r\\n\"1989-06-11\",8.6\\r\\n\"1989-06-12\",8.8\\r\\n\"1989-06-13\",6.2\\r\\n\"1989-06-14\",9.5\\r\\n\"1989-06-15\",10.2\\r\\n\"1989-06-16\",9.7\\r\\n\"1989-06-17\",11.2\\r\\n\"1989-06-18\",10.2\\r\\n\"1989-06-19\",10.1\\r\\n\"1989-06-20\",8.1\\r\\n\"1989-06-21\",6.6\\r\\n\"1989-06-22\",5.0\\r\\n\"1989-06-23\",4.7\\r\\n\"1989-06-24\",5.3\\r\\n\"1989-06-25\",4.5\\r\\n\"1989-06-26\",2.3\\r\\n\"1989-06-27\",1.4\\r\\n\"1989-06-28\",0.5\\r\\n\"1989-06-29\",2.4\\r\\n\"1989-06-30\",8.0\\r\\n\"1989-07-01\",6.0\\r\\n\"1989-07-02\",7.1\\r\\n\"1989-07-03\",9.7\\r\\n\"1989-07-04\",6.9\\r\\n\"1989-07-05\",5.3\\r\\n\"1989-07-06\",7.0\\r\\n\"1989-07-07\",6.2\\r\\n\"1989-07-08\",7.0\\r\\n\"1989-07-09\",9.7\\r\\n\"1989-07-10\",8.0\\r\\n\"1989-07-11\",8.5\\r\\n\"1989-07-12\",7.1\\r\\n\"1989-07-13\",7.5\\r\\n\"1989-07-14\",3.3\\r\\n\"1989-07-15\",1.8\\r\\n\"1989-07-16\",2.6\\r\\n\"1989-07-17\",5.3\\r\\n\"1989-07-18\",5.8\\r\\n\"1989-07-19\",5.8\\r\\n\"1989-07-20\",7.2\\r\\n\"1989-07-21\",5.3\\r\\n\"1989-07-22\",1.6\\r\\n\"1989-07-23\",3.1\\r\\n\"1989-07-24\",5.3\\r\\n\"1989-07-25\",7.7\\r\\n\"1989-07-26\",4.2\\r\\n\"1989-07-27\",5.5\\r\\n\"1989-07-28\",9.0\\r\\n\"1989-07-29\",11.2\\r\\n\"1989-07-30\",8.0\\r\\n\"1989-07-31\",7.6\\r\\n\"1989-08-01\",3.7\\r\\n\"1989-08-02\",7.5\\r\\n\"1989-08-03\",8.1\\r\\n\"1989-08-04\",8.4\\r\\n\"1989-08-05\",7.1\\r\\n\"1989-08-06\",7.6\\r\\n\"1989-08-07\",7.6\\r\\n\"1989-08-08\",5.6\\r\\n\"1989-08-09\",7.0\\r\\n\"1989-08-10\",10.5\\r\\n\"1989-08-11\",7.3\\r\\n\"1989-08-12\",7.8\\r\\n\"1989-08-13\",5.8\\r\\n\"1989-08-14\",3.8\\r\\n\"1989-08-15\",5.8\\r\\n\"1989-08-16\",6.7\\r\\n\"1989-08-17\",6.6\\r\\n\"1989-08-18\",6.6\\r\\n\"1989-08-19\",9.0\\r\\n\"1989-08-20\",8.1\\r\\n\"1989-08-21\",5.1\\r\\n\"1989-08-22\",8.6\\r\\n\"1989-08-23\",7.0\\r\\n\"1989-08-24\",5.5\\r\\n\"1989-08-25\",7.4\\r\\n\"1989-08-26\",6.2\\r\\n\"1989-08-27\",4.2\\r\\n\"1989-08-28\",6.3\\r\\n\"1989-08-29\",7.0\\r\\n\"1989-08-30\",4.0\\r\\n\"1989-08-31\",8.0\\r\\n\"1989-09-01\",8.8\\r\\n\"1989-09-02\",8.8\\r\\n\"1989-09-03\",6.1\\r\\n\"1989-09-04\",8.6\\r\\n\"1989-09-05\",8.9\\r\\n\"1989-09-06\",7.8\\r\\n\"1989-09-07\",5.0\\r\\n\"1989-09-08\",7.0\\r\\n\"1989-09-09\",13.3\\r\\n\"1989-09-10\",7.9\\r\\n\"1989-09-11\",7.5\\r\\n\"1989-09-12\",8.3\\r\\n\"1989-09-13\",7.2\\r\\n\"1989-09-14\",6.5\\r\\n\"1989-09-15\",8.9\\r\\n\"1989-09-16\",7.4\\r\\n\"1989-09-17\",9.9\\r\\n\"1989-09-18\",9.3\\r\\n\"1989-09-19\",10.6\\r\\n\"1989-09-20\",8.6\\r\\n\"1989-09-21\",7.2\\r\\n\"1989-09-22\",12.6\\r\\n\"1989-09-23\",7.8\\r\\n\"1989-09-24\",6.3\\r\\n\"1989-09-25\",9.2\\r\\n\"1989-09-26\",5.8\\r\\n\"1989-09-27\",9.0\\r\\n\"1989-09-28\",5.0\\r\\n\"1989-09-29\",11.9\\r\\n\"1989-09-30\",13.4\\r\\n\"1989-10-01\",10.5\\r\\n\"1989-10-02\",6.2\\r\\n\"1989-10-03\",5.1\\r\\n\"1989-10-04\",9.5\\r\\n\"1989-10-05\",11.7\\r\\n\"1989-10-06\",9.2\\r\\n\"1989-10-07\",7.3\\r\\n\"1989-10-08\",9.7\\r\\n\"1989-10-09\",9.4\\r\\n\"1989-10-10\",10.0\\r\\n\"1989-10-11\",10.9\\r\\n\"1989-10-12\",11.0\\r\\n\"1989-10-13\",10.9\\r\\n\"1989-10-14\",8.0\\r\\n\"1989-10-15\",11.2\\r\\n\"1989-10-16\",7.5\\r\\n\"1989-10-17\",7.2\\r\\n\"1989-10-18\",13.2\\r\\n\"1989-10-19\",12.9\\r\\n\"1989-10-20\",9.4\\r\\n\"1989-10-21\",10.2\\r\\n\"1989-10-22\",9.5\\r\\n\"1989-10-23\",12.4\\r\\n\"1989-10-24\",10.2\\r\\n\"1989-10-25\",13.4\\r\\n\"1989-10-26\",11.6\\r\\n\"1989-10-27\",8.0\\r\\n\"1989-10-28\",9.0\\r\\n\"1989-10-29\",9.3\\r\\n\"1989-10-30\",13.5\\r\\n\"1989-10-31\",8.0\\r\\n\"1989-11-01\",8.1\\r\\n\"1989-11-02\",10.0\\r\\n\"1989-11-03\",8.5\\r\\n\"1989-11-04\",12.5\\r\\n\"1989-11-05\",15.0\\r\\n\"1989-11-06\",13.3\\r\\n\"1989-11-07\",11.0\\r\\n\"1989-11-08\",11.9\\r\\n\"1989-11-09\",8.3\\r\\n\"1989-11-10\",9.7\\r\\n\"1989-11-11\",11.3\\r\\n\"1989-11-12\",12.5\\r\\n\"1989-11-13\",9.4\\r\\n\"1989-11-14\",11.4\\r\\n\"1989-11-15\",13.2\\r\\n\"1989-11-16\",13.8\\r\\n\"1989-11-17\",16.0\\r\\n\"1989-11-18\",10.9\\r\\n\"1989-11-19\",11.9\\r\\n\"1989-11-20\",12.4\\r\\n\"1989-11-21\",13.2\\r\\n\"1989-11-22\",15.5\\r\\n\"1989-11-23\",21.6\\r\\n\"1989-11-24\",14.9\\r\\n\"1989-11-25\",14.4\\r\\n\"1989-11-26\",12.9\\r\\n\"1989-11-27\",13.1\\r\\n\"1989-11-28\",14.0\\r\\n\"1989-11-29\",17.9\\r\\n\"1989-11-30\",17.7\\r\\n\"1989-12-01\",16.3\\r\\n\"1989-12-02\",18.3\\r\\n\"1989-12-03\",13.7\\r\\n\"1989-12-04\",13.3\\r\\n\"1989-12-05\",10.6\\r\\n\"1989-12-06\",14.1\\r\\n\"1989-12-07\",16.0\\r\\n\"1989-12-08\",16.5\\r\\n\"1989-12-09\",14.1\\r\\n\"1989-12-10\",18.7\\r\\n\"1989-12-11\",16.2\\r\\n\"1989-12-12\",14.8\\r\\n\"1989-12-13\",12.6\\r\\n\"1989-12-14\",10.4\\r\\n\"1989-12-15\",12.2\\r\\n\"1989-12-16\",12.6\\r\\n\"1989-12-17\",12.1\\r\\n\"1989-12-18\",17.3\\r\\n\"1989-12-19\",16.4\\r\\n\"1989-12-20\",12.6\\r\\n\"1989-12-21\",12.3\\r\\n\"1989-12-22\",11.8\\r\\n\"1989-12-23\",12.0\\r\\n\"1989-12-24\",12.7\\r\\n\"1989-12-25\",16.4\\r\\n\"1989-12-26\",16.0\\r\\n\"1989-12-27\",13.3\\r\\n\"1989-12-28\",11.7\\r\\n\"1989-12-29\",10.4\\r\\n\"1989-12-30\",14.4\\r\\n\"1989-12-31\",12.7\\r\\n\"1990-01-01\",14.8\\r\\n\"1990-01-02\",13.3\\r\\n\"1990-01-03\",15.6\\r\\n\"1990-01-04\",14.5\\r\\n\"1990-01-05\",14.3\\r\\n\"1990-01-06\",15.3\\r\\n\"1990-01-07\",16.4\\r\\n\"1990-01-08\",14.8\\r\\n\"1990-01-09\",17.4\\r\\n\"1990-01-10\",18.8\\r\\n\"1990-01-11\",22.1\\r\\n\"1990-01-12\",19.0\\r\\n\"1990-01-13\",15.5\\r\\n\"1990-01-14\",15.8\\r\\n\"1990-01-15\",14.7\\r\\n\"1990-01-16\",10.7\\r\\n\"1990-01-17\",11.5\\r\\n\"1990-01-18\",15.0\\r\\n\"1990-01-19\",14.5\\r\\n\"1990-01-20\",14.5\\r\\n\"1990-01-21\",13.3\\r\\n\"1990-01-22\",14.3\\r\\n\"1990-01-23\",14.3\\r\\n\"1990-01-24\",20.5\\r\\n\"1990-01-25\",15.0\\r\\n\"1990-01-26\",17.1\\r\\n\"1990-01-27\",16.9\\r\\n\"1990-01-28\",16.9\\r\\n\"1990-01-29\",13.6\\r\\n\"1990-01-30\",16.4\\r\\n\"1990-01-31\",16.1\\r\\n\"1990-02-01\",12.0\\r\\n\"1990-02-02\",12.2\\r\\n\"1990-02-03\",14.8\\r\\n\"1990-02-04\",14.8\\r\\n\"1990-02-05\",14.4\\r\\n\"1990-02-06\",12.9\\r\\n\"1990-02-07\",13.4\\r\\n\"1990-02-08\",15.9\\r\\n\"1990-02-09\",16.1\\r\\n\"1990-02-10\",17.6\\r\\n\"1990-02-11\",15.6\\r\\n\"1990-02-12\",15.0\\r\\n\"1990-02-13\",13.0\\r\\n\"1990-02-14\",14.1\\r\\n\"1990-02-15\",17.3\\r\\n\"1990-02-16\",15.7\\r\\n\"1990-02-17\",18.6\\r\\n\"1990-02-18\",12.7\\r\\n\"1990-02-19\",14.0\\r\\n\"1990-02-20\",13.7\\r\\n\"1990-02-21\",16.3\\r\\n\"1990-02-22\",20.0\\r\\n\"1990-02-23\",17.0\\r\\n\"1990-02-24\",15.2\\r\\n\"1990-02-25\",16.5\\r\\n\"1990-02-26\",16.5\\r\\n\"1990-02-27\",17.3\\r\\n\"1990-02-28\",19.1\\r\\n\"1990-03-01\",19.3\\r\\n\"1990-03-02\",17.3\\r\\n\"1990-03-03\",19.0\\r\\n\"1990-03-04\",19.8\\r\\n\"1990-03-05\",19.3\\r\\n\"1990-03-06\",17.2\\r\\n\"1990-03-07\",14.2\\r\\n\"1990-03-08\",10.3\\r\\n\"1990-03-09\",13.0\\r\\n\"1990-03-10\",15.3\\r\\n\"1990-03-11\",15.0\\r\\n\"1990-03-12\",12.1\\r\\n\"1990-03-13\",9.2\\r\\n\"1990-03-14\",11.0\\r\\n\"1990-03-15\",15.0\\r\\n\"1990-03-16\",11.6\\r\\n\"1990-03-17\",11.6\\r\\n\"1990-03-18\",15.1\\r\\n\"1990-03-19\",15.0\\r\\n\"1990-03-20\",13.6\\r\\n\"1990-03-21\",12.5\\r\\n\"1990-03-22\",14.3\\r\\n\"1990-03-23\",16.0\\r\\n\"1990-03-24\",17.4\\r\\n\"1990-03-25\",16.9\\r\\n\"1990-03-26\",18.0\\r\\n\"1990-03-27\",20.6\\r\\n\"1990-03-28\",14.2\\r\\n\"1990-03-29\",10.9\\r\\n\"1990-03-30\",11.9\\r\\n\"1990-03-31\",13.3\\r\\n\"1990-04-01\",15.3\\r\\n\"1990-04-02\",14.7\\r\\n\"1990-04-03\",11.0\\r\\n\"1990-04-04\",12.2\\r\\n\"1990-04-05\",14.2\\r\\n\"1990-04-06\",17.0\\r\\n\"1990-04-07\",15.8\\r\\n\"1990-04-08\",15.2\\r\\n\"1990-04-09\",15.1\\r\\n\"1990-04-10\",14.7\\r\\n\"1990-04-11\",18.5\\r\\n\"1990-04-12\",16.4\\r\\n\"1990-04-13\",18.4\\r\\n\"1990-04-14\",15.1\\r\\n\"1990-04-15\",9.9\\r\\n\"1990-04-16\",10.2\\r\\n\"1990-04-17\",12.6\\r\\n\"1990-04-18\",13.2\\r\\n\"1990-04-19\",11.5\\r\\n\"1990-04-20\",13.8\\r\\n\"1990-04-21\",14.5\\r\\n\"1990-04-22\",14.7\\r\\n\"1990-04-23\",11.2\\r\\n\"1990-04-24\",12.7\\r\\n\"1990-04-25\",13.7\\r\\n\"1990-04-26\",11.5\\r\\n\"1990-04-27\",10.4\\r\\n\"1990-04-28\",8.9\\r\\n\"1990-04-29\",11.1\\r\\n\"1990-04-30\",9.5\\r\\n\"1990-05-01\",13.0\\r\\n\"1990-05-02\",13.9\\r\\n\"1990-05-03\",12.6\\r\\n\"1990-05-04\",14.3\\r\\n\"1990-05-05\",16.0\\r\\n\"1990-05-06\",13.3\\r\\n\"1990-05-07\",7.0\\r\\n\"1990-05-08\",4.9\\r\\n\"1990-05-09\",6.9\\r\\n\"1990-05-10\",13.7\\r\\n\"1990-05-11\",10.6\\r\\n\"1990-05-12\",12.3\\r\\n\"1990-05-13\",11.1\\r\\n\"1990-05-14\",10.2\\r\\n\"1990-05-15\",9.5\\r\\n\"1990-05-16\",8.9\\r\\n\"1990-05-17\",13.4\\r\\n\"1990-05-18\",9.1\\r\\n\"1990-05-19\",9.4\\r\\n\"1990-05-20\",8.7\\r\\n\"1990-05-21\",5.8\\r\\n\"1990-05-22\",4.5\\r\\n\"1990-05-23\",7.2\\r\\n\"1990-05-24\",10.0\\r\\n\"1990-05-25\",10.5\\r\\n\"1990-05-26\",10.7\\r\\n\"1990-05-27\",8.2\\r\\n\"1990-05-28\",6.1\\r\\n\"1990-05-29\",4.5\\r\\n\"1990-05-30\",6.1\\r\\n\"1990-05-31\",9.8\\r\\n\"1990-06-01\",9.7\\r\\n\"1990-06-02\",8.2\\r\\n\"1990-06-03\",8.4\\r\\n\"1990-06-04\",8.5\\r\\n\"1990-06-05\",10.4\\r\\n\"1990-06-06\",6.8\\r\\n\"1990-06-07\",6.0\\r\\n\"1990-06-08\",6.6\\r\\n\"1990-06-09\",7.8\\r\\n\"1990-06-10\",10.3\\r\\n\"1990-06-11\",7.2\\r\\n\"1990-06-12\",7.4\\r\\n\"1990-06-13\",11.4\\r\\n\"1990-06-14\",5.4\\r\\n\"1990-06-15\",4.4\\r\\n\"1990-06-16\",6.4\\r\\n\"1990-06-17\",9.3\\r\\n\"1990-06-18\",7.7\\r\\n\"1990-06-19\",8.1\\r\\n\"1990-06-20\",8.3\\r\\n\"1990-06-21\",9.1\\r\\n\"1990-06-22\",7.7\\r\\n\"1990-06-23\",10.6\\r\\n\"1990-06-24\",8.2\\r\\n\"1990-06-25\",7.9\\r\\n\"1990-06-26\",5.2\\r\\n\"1990-06-27\",5.9\\r\\n\"1990-06-28\",3.7\\r\\n\"1990-06-29\",5.6\\r\\n\"1990-06-30\",9.4\\r\\n\"1990-07-01\",7.4\\r\\n\"1990-07-02\",7.3\\r\\n\"1990-07-03\",7.7\\r\\n\"1990-07-04\",7.7\\r\\n\"1990-07-05\",9.3\\r\\n\"1990-07-06\",4.4\\r\\n\"1990-07-07\",5.7\\r\\n\"1990-07-08\",10.2\\r\\n\"1990-07-09\",10.2\\r\\n\"1990-07-10\",9.3\\r\\n\"1990-07-11\",5.4\\r\\n\"1990-07-12\",5.0\\r\\n\"1990-07-13\",7.6\\r\\n\"1990-07-14\",9.6\\r\\n\"1990-07-15\",10.4\\r\\n\"1990-07-16\",11.2\\r\\n\"1990-07-17\",9.1\\r\\n\"1990-07-18\",11.2\\r\\n\"1990-07-19\",6.8\\r\\n\"1990-07-20\",8.3\\r\\n\"1990-07-21\",9.7\\r\\n\"1990-07-22\",9.6\\r\\n\"1990-07-23\",9.8\\r\\n\"1990-07-24\",10.8\\r\\n\"1990-07-25\",9.2\\r\\n\"1990-07-26\",6.5\\r\\n\"1990-07-27\",8.1\\r\\n\"1990-07-28\",7.3\\r\\n\"1990-07-29\",7.9\\r\\n\"1990-07-30\",6.0\\r\\n\"1990-07-31\",5.0\\r\\n\"1990-08-01\",6.8\\r\\n\"1990-08-02\",9.8\\r\\n\"1990-08-03\",5.7\\r\\n\"1990-08-04\",8.6\\r\\n\"1990-08-05\",10.6\\r\\n\"1990-08-06\",7.8\\r\\n\"1990-08-07\",7.7\\r\\n\"1990-08-08\",8.6\\r\\n\"1990-08-09\",6.5\\r\\n\"1990-08-10\",6.9\\r\\n\"1990-08-11\",6.4\\r\\n\"1990-08-12\",8.5\\r\\n\"1990-08-13\",7.8\\r\\n\"1990-08-14\",9.3\\r\\n\"1990-08-15\",8.4\\r\\n\"1990-08-16\",7.8\\r\\n\"1990-08-17\",7.4\\r\\n\"1990-08-18\",7.7\\r\\n\"1990-08-19\",8.9\\r\\n\"1990-08-20\",9.7\\r\\n\"1990-08-21\",9.9\\r\\n\"1990-08-22\",6.1\\r\\n\"1990-08-23\",6.6\\r\\n\"1990-08-24\",7.6\\r\\n\"1990-08-25\",7.4\\r\\n\"1990-08-26\",8.0\\r\\n\"1990-08-27\",2.1\\r\\n\"1990-08-28\",5.9\\r\\n\"1990-08-29\",11.6\\r\\n\"1990-08-30\",8.6\\r\\n\"1990-08-31\",7.9\\r\\n\"1990-09-01\",6.0\\r\\n\"1990-09-02\",9.5\\r\\n\"1990-09-03\",8.6\\r\\n\"1990-09-04\",7.6\\r\\n\"1990-09-05\",10.4\\r\\n\"1990-09-06\",10.3\\r\\n\"1990-09-07\",7.5\\r\\n\"1990-09-08\",3.0\\r\\n\"1990-09-09\",5.3\\r\\n\"1990-09-10\",10.5\\r\\n\"1990-09-11\",14.6\\r\\n\"1990-09-12\",12.6\\r\\n\"1990-09-13\",9.8\\r\\n\"1990-09-14\",7.2\\r\\n\"1990-09-15\",10.1\\r\\n\"1990-09-16\",10.4\\r\\n\"1990-09-17\",3.7\\r\\n\"1990-09-18\",7.3\\r\\n\"1990-09-19\",11.6\\r\\n\"1990-09-20\",16.3\\r\\n\"1990-09-21\",9.6\\r\\n\"1990-09-22\",6.8\\r\\n\"1990-09-23\",5.2\\r\\n\"1990-09-24\",10.6\\r\\n\"1990-09-25\",16.3\\r\\n\"1990-09-26\",9.8\\r\\n\"1990-09-27\",4.6\\r\\n\"1990-09-28\",11.1\\r\\n\"1990-09-29\",8.7\\r\\n\"1990-09-30\",10.0\\r\\n\"1990-10-01\",11.3\\r\\n\"1990-10-02\",10.5\\r\\n\"1990-10-03\",9.9\\r\\n\"1990-10-04\",11.0\\r\\n\"1990-10-05\",14.0\\r\\n\"1990-10-06\",9.2\\r\\n\"1990-10-07\",9.8\\r\\n\"1990-10-08\",6.0\\r\\n\"1990-10-09\",9.8\\r\\n\"1990-10-10\",9.2\\r\\n\"1990-10-11\",11.8\\r\\n\"1990-10-12\",10.3\\r\\n\"1990-10-13\",7.5\\r\\n\"1990-10-14\",7.7\\r\\n\"1990-10-15\",15.8\\r\\n\"1990-10-16\",14.6\\r\\n\"1990-10-17\",10.5\\r\\n\"1990-10-18\",11.3\\r\\n\"1990-10-19\",10.9\\r\\n\"1990-10-20\",6.4\\r\\n\"1990-10-21\",10.9\\r\\n\"1990-10-22\",9.0\\r\\n\"1990-10-23\",10.9\\r\\n\"1990-10-24\",12.4\\r\\n\"1990-10-25\",11.6\\r\\n\"1990-10-26\",13.3\\r\\n\"1990-10-27\",14.4\\r\\n\"1990-10-28\",18.4\\r\\n\"1990-10-29\",13.6\\r\\n\"1990-10-30\",14.9\\r\\n\"1990-10-31\",14.8\\r\\n\"1990-11-01\",15.4\\r\\n\"1990-11-02\",11.8\\r\\n\"1990-11-03\",13.0\\r\\n\"1990-11-04\",11.1\\r\\n\"1990-11-05\",12.5\\r\\n\"1990-11-06\",18.3\\r\\n\"1990-11-07\",19.2\\r\\n\"1990-11-08\",15.4\\r\\n\"1990-11-09\",13.1\\r\\n\"1990-11-10\",11.5\\r\\n\"1990-11-11\",8.6\\r\\n\"1990-11-12\",12.6\\r\\n\"1990-11-13\",13.8\\r\\n\"1990-11-14\",14.6\\r\\n\"1990-11-15\",13.2\\r\\n\"1990-11-16\",12.3\\r\\n\"1990-11-17\",8.8\\r\\n\"1990-11-18\",10.7\\r\\n\"1990-11-19\",9.9\\r\\n\"1990-11-20\",8.3\\r\\n\"1990-11-21\",15.0\\r\\n\"1990-11-22\",12.2\\r\\n\"1990-11-23\",10.5\\r\\n\"1990-11-24\",11.1\\r\\n\"1990-11-25\",13.0\\r\\n\"1990-11-26\",12.9\\r\\n\"1990-11-27\",8.8\\r\\n\"1990-11-28\",14.7\\r\\n\"1990-11-29\",14.7\\r\\n\"1990-11-30\",12.7\\r\\n\"1990-12-01\",13.3\\r\\n\"1990-12-02\",13.2\\r\\n\"1990-12-03\",16.2\\r\\n\"1990-12-04\",17.3\\r\\n\"1990-12-05\",20.5\\r\\n\"1990-12-06\",20.2\\r\\n\"1990-12-07\",19.4\\r\\n\"1990-12-08\",15.5\\r\\n\"1990-12-09\",14.1\\r\\n\"1990-12-10\",11.0\\r\\n\"1990-12-11\",11.1\\r\\n\"1990-12-12\",14.0\\r\\n\"1990-12-13\",11.4\\r\\n\"1990-12-14\",12.5\\r\\n\"1990-12-15\",13.4\\r\\n\"1990-12-16\",13.6\\r\\n\"1990-12-17\",13.9\\r\\n\"1990-12-18\",17.2\\r\\n\"1990-12-19\",14.7\\r\\n\"1990-12-20\",15.4\\r\\n\"1990-12-21\",13.1\\r\\n\"1990-12-22\",13.2\\r\\n\"1990-12-23\",13.9\\r\\n\"1990-12-24\",10.0\\r\\n\"1990-12-25\",12.9\\r\\n\"1990-12-26\",14.6\\r\\n\"1990-12-27\",14.0\\r\\n\"1990-12-28\",13.6\\r\\n\"1990-12-29\",13.5\\r\\n\"1990-12-30\",15.7\\r\\n\"1990-12-31\",13.0\\r\\n\\r\\nDaily minimum temperatures in Melbourne, Australia, 1981-1990\\r\\n\\r\\n'} # file is uploaded to the current directory ! ls arrhythmia.data daily-minimum-temperatures-in-me.csv sample_data # open the file # the last few lines are junk df = pd . read_csv ( 'daily-minimum-temperatures-in-me.csv' , error_bad_lines = False ) df . head () b'Skipping line 3653: expected 2 fields, saw 3\\n' .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Daily minimum temperatures in Melbourne, Australia, 1981-1990 0 1981-01-01 20.7 1 1981-01-02 17.9 2 1981-01-03 18.8 3 1981-01-04 14.6 4 1981-01-05 15.8 # upload a Python file with some useful functions (meant for fake_util.py) from google.colab import files uploaded = files . upload () <input type=\"file\" id=\"files-c590b543-ffcc-4177-91d3-62725b363169\" name=\"files[]\" multiple disabled /> <output id=\"result-c590b543-ffcc-4177-91d3-62725b363169\"> Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. </output> <script src=\"/nbextensions/google.colab/files.js\"></script> Saving fake_util.py to fake_util.py from fake_util import my_useful_function my_useful_function () hello world ! pwd /content","title":"Part 3: Upload the file yourself"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/#part-4-access-files-from-google-drive","text":"# Access files from your Google Drive from google.colab import drive drive . mount ( '/content/gdrive' ) # Check current directory - now gdrive is there ! ls # What's in gdrive? ! ls gdrive # Whoa! Look at all this great VIP content! ! ls '/content/gdrive/My Drive/' Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Part 4: Access files from Google Drive"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/GANs/TF2_0_GAN/","text":"================ by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-rc0 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow is already loaded. Please restart the runtime to change versions. 2.0.0-rc0 # More imports from tensorflow.keras.layers import Input , Dense , LeakyReLU , Dropout , \\ BatchNormalization from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt import sys , os # Load in the data mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # map inputs to (-1, +1) for better training x_train , x_test = x_train / 255.0 * 2 - 1 , x_test / 255.0 * 2 - 1 print ( \"x_train.shape:\" , x_train . shape ) x_train.shape: (60000, 28, 28) # Flatten the data N , H , W = x_train . shape D = H * W x_train = x_train . reshape ( - 1 , D ) x_test = x_test . reshape ( - 1 , D ) # Dimensionality of the latent space latent_dim = 100 # Get the generator model def build_generator ( latent_dim ): i = Input ( shape = ( latent_dim ,)) x = Dense ( 256 , activation = LeakyReLU ( alpha = 0.2 ))( i ) x = BatchNormalization ( momentum = 0.7 )( x ) x = Dense ( 512 , activation = LeakyReLU ( alpha = 0.2 ))( x ) x = BatchNormalization ( momentum = 0.7 )( x ) x = Dense ( 1024 , activation = LeakyReLU ( alpha = 0.2 ))( x ) x = BatchNormalization ( momentum = 0.7 )( x ) x = Dense ( D , activation = 'tanh' )( x ) model = Model ( i , x ) return model # Get the discriminator model def build_discriminator ( img_size ): i = Input ( shape = ( img_size ,)) x = Dense ( 512 , activation = LeakyReLU ( alpha = 0.2 ))( i ) x = Dense ( 256 , activation = LeakyReLU ( alpha = 0.2 ))( x ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) return model # Compile both models in preparation for training # Build and compile the discriminator discriminator = build_discriminator ( D ) discriminator . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( 0.0002 , 0.5 ), metrics = [ 'accuracy' ]) # Build and compile the combined model generator = build_generator ( latent_dim ) # Create an input to represent noise sample from latent space z = Input ( shape = ( latent_dim ,)) # Pass noise through generator to get an image img = generator ( z ) # Make sure only the generator is trained discriminator . trainable = False # The true output is fake, but we label them real! fake_pred = discriminator ( img ) # Create the combined model object combined_model = Model ( z , fake_pred ) # Compile the combined model combined_model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( 0.0002 , 0.5 )) # Train the GAN # Config batch_size = 32 epochs = 30000 sample_period = 200 # every `sample_period` steps generate and save some data # Create batch labels to use when calling train_on_batch ones = np . ones ( batch_size ) zeros = np . zeros ( batch_size ) # Store the losses d_losses = [] g_losses = [] # Create a folder to store generated images if not os . path . exists ( 'gan_images' ): os . makedirs ( 'gan_images' ) # A function to generate a grid of random samples from the generator # and save them to a file def sample_images ( epoch ): rows , cols = 5 , 5 noise = np . random . randn ( rows * cols , latent_dim ) imgs = generator . predict ( noise ) # Rescale images 0 - 1 imgs = 0.5 * imgs + 0.5 fig , axs = plt . subplots ( rows , cols ) idx = 0 for i in range ( rows ): for j in range ( cols ): axs [ i , j ] . imshow ( imgs [ idx ] . reshape ( H , W ), cmap = 'gray' ) axs [ i , j ] . axis ( 'off' ) idx += 1 fig . savefig ( \"gan_images/ %d .png\" % epoch ) plt . close () # Main training loop for epoch in range ( epochs ): ########################### ### Train discriminator ### ########################### # Select a random batch of images idx = np . random . randint ( 0 , x_train . shape [ 0 ], batch_size ) real_imgs = x_train [ idx ] # Generate fake images noise = np . random . randn ( batch_size , latent_dim ) fake_imgs = generator . predict ( noise ) # Train the discriminator # both loss and accuracy are returned d_loss_real , d_acc_real = discriminator . train_on_batch ( real_imgs , ones ) d_loss_fake , d_acc_fake = discriminator . train_on_batch ( fake_imgs , zeros ) d_loss = 0.5 * ( d_loss_real + d_loss_fake ) d_acc = 0.5 * ( d_acc_real + d_acc_fake ) ####################### ### Train generator ### ####################### noise = np . random . randn ( batch_size , latent_dim ) g_loss = combined_model . train_on_batch ( noise , ones ) # do it again! noise = np . random . randn ( batch_size , latent_dim ) g_loss = combined_model . train_on_batch ( noise , ones ) # Save the losses d_losses . append ( d_loss ) g_losses . append ( g_loss ) if epoch % 100 == 0 : print ( f \"epoch: { epoch + 1 } / { epochs } , d_loss: { d_loss : .2f } , \\ d_acc: { d_acc : .2f } , g_loss: { g_loss : .2f } \" ) if epoch % sample_period == 0 : sample_images ( epoch ) epoch: 1/30000, d_loss: 0.74, d_acc: 0.42, g_loss: 0.70 epoch: 101/30000, d_loss: 0.04, d_acc: 1.00, g_loss: 3.91 epoch: 201/30000, d_loss: 0.92, d_acc: 0.41, g_loss: 0.70 epoch: 301/30000, d_loss: 0.71, d_acc: 0.45, g_loss: 0.58 epoch: 401/30000, d_loss: 0.69, d_acc: 0.48, g_loss: 0.61 epoch: 501/30000, d_loss: 0.67, d_acc: 0.52, g_loss: 0.63 epoch: 601/30000, d_loss: 0.69, d_acc: 0.45, g_loss: 0.63 epoch: 701/30000, d_loss: 0.68, d_acc: 0.44, g_loss: 0.68 epoch: 801/30000, d_loss: 0.67, d_acc: 0.64, g_loss: 0.68 epoch: 901/30000, d_loss: 0.65, d_acc: 0.61, g_loss: 0.71 epoch: 1001/30000, d_loss: 0.66, d_acc: 0.58, g_loss: 0.71 epoch: 1101/30000, d_loss: 0.66, d_acc: 0.64, g_loss: 0.74 epoch: 1201/30000, d_loss: 0.63, d_acc: 0.72, g_loss: 0.76 epoch: 1301/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.75 epoch: 1401/30000, d_loss: 0.66, d_acc: 0.69, g_loss: 0.72 epoch: 1501/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.80 epoch: 1601/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.80 epoch: 1701/30000, d_loss: 0.64, d_acc: 0.62, g_loss: 0.80 epoch: 1801/30000, d_loss: 0.64, d_acc: 0.78, g_loss: 0.75 epoch: 1901/30000, d_loss: 0.65, d_acc: 0.62, g_loss: 0.76 epoch: 2001/30000, d_loss: 0.66, d_acc: 0.55, g_loss: 0.76 epoch: 2101/30000, d_loss: 0.65, d_acc: 0.59, g_loss: 0.78 epoch: 2201/30000, d_loss: 0.69, d_acc: 0.48, g_loss: 0.75 epoch: 2301/30000, d_loss: 0.65, d_acc: 0.69, g_loss: 0.78 epoch: 2401/30000, d_loss: 0.64, d_acc: 0.67, g_loss: 0.78 epoch: 2501/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.80 epoch: 2601/30000, d_loss: 0.68, d_acc: 0.62, g_loss: 0.78 epoch: 2701/30000, d_loss: 0.67, d_acc: 0.66, g_loss: 0.76 epoch: 2801/30000, d_loss: 0.68, d_acc: 0.61, g_loss: 0.79 epoch: 2901/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.80 epoch: 3001/30000, d_loss: 0.68, d_acc: 0.52, g_loss: 0.76 epoch: 3101/30000, d_loss: 0.66, d_acc: 0.67, g_loss: 0.77 epoch: 3201/30000, d_loss: 0.71, d_acc: 0.52, g_loss: 0.77 epoch: 3301/30000, d_loss: 0.67, d_acc: 0.62, g_loss: 0.77 epoch: 3401/30000, d_loss: 0.64, d_acc: 0.64, g_loss: 0.78 epoch: 3501/30000, d_loss: 0.72, d_acc: 0.48, g_loss: 0.77 epoch: 3601/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.82 epoch: 3701/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.80 epoch: 3801/30000, d_loss: 0.68, d_acc: 0.52, g_loss: 0.73 epoch: 3901/30000, d_loss: 0.70, d_acc: 0.45, g_loss: 0.74 epoch: 4001/30000, d_loss: 0.67, d_acc: 0.58, g_loss: 0.79 epoch: 4101/30000, d_loss: 0.66, d_acc: 0.59, g_loss: 0.76 epoch: 4201/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.74 epoch: 4301/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.76 epoch: 4401/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.76 epoch: 4501/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.78 epoch: 4601/30000, d_loss: 0.67, d_acc: 0.58, g_loss: 0.78 epoch: 4701/30000, d_loss: 0.66, d_acc: 0.59, g_loss: 0.72 epoch: 4801/30000, d_loss: 0.67, d_acc: 0.58, g_loss: 0.76 epoch: 4901/30000, d_loss: 0.72, d_acc: 0.52, g_loss: 0.73 epoch: 5001/30000, d_loss: 0.73, d_acc: 0.42, g_loss: 0.73 epoch: 5101/30000, d_loss: 0.66, d_acc: 0.67, g_loss: 0.78 epoch: 5201/30000, d_loss: 0.67, d_acc: 0.62, g_loss: 0.75 epoch: 5301/30000, d_loss: 0.67, d_acc: 0.53, g_loss: 0.77 epoch: 5401/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.78 epoch: 5501/30000, d_loss: 0.68, d_acc: 0.61, g_loss: 0.75 epoch: 5601/30000, d_loss: 0.72, d_acc: 0.50, g_loss: 0.75 epoch: 5701/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.76 epoch: 5801/30000, d_loss: 0.72, d_acc: 0.47, g_loss: 0.76 epoch: 5901/30000, d_loss: 0.70, d_acc: 0.47, g_loss: 0.75 epoch: 6001/30000, d_loss: 0.70, d_acc: 0.56, g_loss: 0.77 epoch: 6101/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.80 epoch: 6201/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.76 epoch: 6301/30000, d_loss: 0.73, d_acc: 0.48, g_loss: 0.75 epoch: 6401/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.75 epoch: 6501/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.79 epoch: 6601/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.77 epoch: 6701/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.76 epoch: 6801/30000, d_loss: 0.70, d_acc: 0.53, g_loss: 0.76 epoch: 6901/30000, d_loss: 0.70, d_acc: 0.53, g_loss: 0.77 epoch: 7001/30000, d_loss: 0.71, d_acc: 0.52, g_loss: 0.76 epoch: 7101/30000, d_loss: 0.66, d_acc: 0.59, g_loss: 0.77 epoch: 7201/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.80 epoch: 7301/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.77 epoch: 7401/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.76 epoch: 7501/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.76 epoch: 7601/30000, d_loss: 0.70, d_acc: 0.48, g_loss: 0.77 epoch: 7701/30000, d_loss: 0.70, d_acc: 0.45, g_loss: 0.76 epoch: 7801/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.75 epoch: 7901/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.76 epoch: 8001/30000, d_loss: 0.72, d_acc: 0.44, g_loss: 0.77 epoch: 8101/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.78 epoch: 8201/30000, d_loss: 0.69, d_acc: 0.59, g_loss: 0.76 epoch: 8301/30000, d_loss: 0.71, d_acc: 0.47, g_loss: 0.77 epoch: 8401/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.78 epoch: 8501/30000, d_loss: 0.64, d_acc: 0.69, g_loss: 0.78 epoch: 8601/30000, d_loss: 0.70, d_acc: 0.53, g_loss: 0.79 epoch: 8701/30000, d_loss: 0.67, d_acc: 0.53, g_loss: 0.79 epoch: 8801/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.75 epoch: 8901/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.76 epoch: 9001/30000, d_loss: 0.70, d_acc: 0.48, g_loss: 0.76 epoch: 9101/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.74 epoch: 9201/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.77 epoch: 9301/30000, d_loss: 0.71, d_acc: 0.53, g_loss: 0.77 epoch: 9401/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.78 epoch: 9501/30000, d_loss: 0.68, d_acc: 0.61, g_loss: 0.77 epoch: 9601/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.79 epoch: 9701/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.71 epoch: 9801/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.78 epoch: 9901/30000, d_loss: 0.66, d_acc: 0.64, g_loss: 0.76 epoch: 10001/30000, d_loss: 0.71, d_acc: 0.47, g_loss: 0.75 epoch: 10101/30000, d_loss: 0.71, d_acc: 0.48, g_loss: 0.79 epoch: 10201/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.78 epoch: 10301/30000, d_loss: 0.70, d_acc: 0.58, g_loss: 0.80 epoch: 10401/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.80 epoch: 10501/30000, d_loss: 0.69, d_acc: 0.59, g_loss: 0.76 epoch: 10601/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.73 epoch: 10701/30000, d_loss: 0.70, d_acc: 0.56, g_loss: 0.75 epoch: 10801/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.75 epoch: 10901/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.80 epoch: 11001/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.78 epoch: 11101/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.77 epoch: 11201/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.77 epoch: 11301/30000, d_loss: 0.72, d_acc: 0.52, g_loss: 0.76 epoch: 11401/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.72 epoch: 11501/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.76 epoch: 11601/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.77 epoch: 11701/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.77 epoch: 11801/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.76 epoch: 11901/30000, d_loss: 0.71, d_acc: 0.48, g_loss: 0.79 epoch: 12001/30000, d_loss: 0.71, d_acc: 0.50, g_loss: 0.78 epoch: 12101/30000, d_loss: 0.72, d_acc: 0.52, g_loss: 0.75 epoch: 12201/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.74 epoch: 12301/30000, d_loss: 0.68, d_acc: 0.53, g_loss: 0.72 epoch: 12401/30000, d_loss: 0.65, d_acc: 0.67, g_loss: 0.79 epoch: 12501/30000, d_loss: 0.66, d_acc: 0.62, g_loss: 0.73 epoch: 12601/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.79 epoch: 12701/30000, d_loss: 0.65, d_acc: 0.69, g_loss: 0.73 epoch: 12801/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.80 epoch: 12901/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.78 epoch: 13001/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.81 epoch: 13101/30000, d_loss: 0.67, d_acc: 0.64, g_loss: 0.73 epoch: 13201/30000, d_loss: 0.71, d_acc: 0.56, g_loss: 0.77 epoch: 13301/30000, d_loss: 0.70, d_acc: 0.53, g_loss: 0.80 epoch: 13401/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.82 epoch: 13501/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.75 epoch: 13601/30000, d_loss: 0.72, d_acc: 0.50, g_loss: 0.78 epoch: 13701/30000, d_loss: 0.67, d_acc: 0.53, g_loss: 0.75 epoch: 13801/30000, d_loss: 0.71, d_acc: 0.48, g_loss: 0.75 epoch: 13901/30000, d_loss: 0.72, d_acc: 0.47, g_loss: 0.76 epoch: 14001/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.76 epoch: 14101/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.75 epoch: 14201/30000, d_loss: 0.70, d_acc: 0.58, g_loss: 0.75 epoch: 14301/30000, d_loss: 0.71, d_acc: 0.48, g_loss: 0.74 epoch: 14401/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.71 epoch: 14501/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.81 epoch: 14601/30000, d_loss: 0.71, d_acc: 0.48, g_loss: 0.76 epoch: 14701/30000, d_loss: 0.72, d_acc: 0.50, g_loss: 0.73 epoch: 14801/30000, d_loss: 0.69, d_acc: 0.45, g_loss: 0.78 epoch: 14901/30000, d_loss: 0.70, d_acc: 0.48, g_loss: 0.77 epoch: 15001/30000, d_loss: 0.64, d_acc: 0.67, g_loss: 0.80 epoch: 15101/30000, d_loss: 0.70, d_acc: 0.48, g_loss: 0.75 epoch: 15201/30000, d_loss: 0.65, d_acc: 0.66, g_loss: 0.73 epoch: 15301/30000, d_loss: 0.73, d_acc: 0.44, g_loss: 0.72 epoch: 15401/30000, d_loss: 0.71, d_acc: 0.48, g_loss: 0.76 epoch: 15501/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.75 epoch: 15601/30000, d_loss: 0.65, d_acc: 0.66, g_loss: 0.77 epoch: 15701/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.78 epoch: 15801/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.71 epoch: 15901/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.77 epoch: 16001/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.81 epoch: 16101/30000, d_loss: 0.66, d_acc: 0.66, g_loss: 0.77 epoch: 16201/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.74 epoch: 16301/30000, d_loss: 0.68, d_acc: 0.61, g_loss: 0.74 epoch: 16401/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.79 epoch: 16501/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.80 epoch: 16601/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.80 epoch: 16701/30000, d_loss: 0.71, d_acc: 0.50, g_loss: 0.77 epoch: 16801/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.75 epoch: 16901/30000, d_loss: 0.67, d_acc: 0.58, g_loss: 0.78 epoch: 17001/30000, d_loss: 0.65, d_acc: 0.64, g_loss: 0.76 epoch: 17101/30000, d_loss: 0.72, d_acc: 0.45, g_loss: 0.77 epoch: 17201/30000, d_loss: 0.68, d_acc: 0.53, g_loss: 0.77 epoch: 17301/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.81 epoch: 17401/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.83 epoch: 17501/30000, d_loss: 0.70, d_acc: 0.47, g_loss: 0.78 epoch: 17601/30000, d_loss: 0.67, d_acc: 0.64, g_loss: 0.83 epoch: 17701/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.78 epoch: 17801/30000, d_loss: 0.72, d_acc: 0.48, g_loss: 0.76 epoch: 17901/30000, d_loss: 0.69, d_acc: 0.59, g_loss: 0.81 epoch: 18001/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.74 epoch: 18101/30000, d_loss: 0.67, d_acc: 0.66, g_loss: 0.76 epoch: 18201/30000, d_loss: 0.71, d_acc: 0.48, g_loss: 0.82 epoch: 18301/30000, d_loss: 0.72, d_acc: 0.45, g_loss: 0.77 epoch: 18401/30000, d_loss: 0.70, d_acc: 0.47, g_loss: 0.74 epoch: 18501/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.78 epoch: 18601/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.77 epoch: 18701/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.81 epoch: 18801/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.76 epoch: 18901/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.77 epoch: 19001/30000, d_loss: 0.68, d_acc: 0.64, g_loss: 0.77 epoch: 19101/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.73 epoch: 19201/30000, d_loss: 0.70, d_acc: 0.47, g_loss: 0.78 epoch: 19301/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.71 epoch: 19401/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.77 epoch: 19501/30000, d_loss: 0.67, d_acc: 0.52, g_loss: 0.76 epoch: 19601/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.81 epoch: 19701/30000, d_loss: 0.66, d_acc: 0.55, g_loss: 0.76 epoch: 19801/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.78 epoch: 19901/30000, d_loss: 0.68, d_acc: 0.52, g_loss: 0.79 epoch: 20001/30000, d_loss: 0.71, d_acc: 0.50, g_loss: 0.77 epoch: 20101/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.76 epoch: 20201/30000, d_loss: 0.64, d_acc: 0.67, g_loss: 0.78 epoch: 20301/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.75 epoch: 20401/30000, d_loss: 0.66, d_acc: 0.59, g_loss: 0.77 epoch: 20501/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.75 epoch: 20601/30000, d_loss: 0.68, d_acc: 0.53, g_loss: 0.77 epoch: 20701/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.80 epoch: 20801/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.77 epoch: 20901/30000, d_loss: 0.70, d_acc: 0.47, g_loss: 0.77 epoch: 21001/30000, d_loss: 0.75, d_acc: 0.42, g_loss: 0.76 epoch: 21101/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.74 epoch: 21201/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.79 epoch: 21301/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.78 epoch: 21401/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.81 epoch: 21501/30000, d_loss: 0.69, d_acc: 0.48, g_loss: 0.79 epoch: 21601/30000, d_loss: 0.70, d_acc: 0.53, g_loss: 0.76 epoch: 21701/30000, d_loss: 0.66, d_acc: 0.64, g_loss: 0.76 epoch: 21801/30000, d_loss: 0.66, d_acc: 0.66, g_loss: 0.78 epoch: 21901/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.78 epoch: 22001/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.80 epoch: 22101/30000, d_loss: 0.71, d_acc: 0.39, g_loss: 0.79 epoch: 22201/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.73 epoch: 22301/30000, d_loss: 0.69, d_acc: 0.47, g_loss: 0.75 epoch: 22401/30000, d_loss: 0.67, d_acc: 0.53, g_loss: 0.80 epoch: 22501/30000, d_loss: 0.67, d_acc: 0.53, g_loss: 0.77 epoch: 22601/30000, d_loss: 0.70, d_acc: 0.44, g_loss: 0.77 epoch: 22701/30000, d_loss: 0.66, d_acc: 0.64, g_loss: 0.80 epoch: 22801/30000, d_loss: 0.69, d_acc: 0.48, g_loss: 0.74 epoch: 22901/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.76 epoch: 23001/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.79 epoch: 23101/30000, d_loss: 0.72, d_acc: 0.39, g_loss: 0.81 epoch: 23201/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.76 epoch: 23301/30000, d_loss: 0.71, d_acc: 0.53, g_loss: 0.76 epoch: 23401/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.81 epoch: 23501/30000, d_loss: 0.69, d_acc: 0.62, g_loss: 0.79 epoch: 23601/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.75 epoch: 23701/30000, d_loss: 0.73, d_acc: 0.39, g_loss: 0.82 epoch: 23801/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.76 epoch: 23901/30000, d_loss: 0.68, d_acc: 0.62, g_loss: 0.76 epoch: 24001/30000, d_loss: 0.69, d_acc: 0.42, g_loss: 0.79 epoch: 24101/30000, d_loss: 0.66, d_acc: 0.62, g_loss: 0.79 epoch: 24201/30000, d_loss: 0.66, d_acc: 0.59, g_loss: 0.77 epoch: 24301/30000, d_loss: 0.66, d_acc: 0.64, g_loss: 0.81 epoch: 24401/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.81 epoch: 24501/30000, d_loss: 0.71, d_acc: 0.44, g_loss: 0.78 epoch: 24601/30000, d_loss: 0.65, d_acc: 0.64, g_loss: 0.84 epoch: 24701/30000, d_loss: 0.68, d_acc: 0.50, g_loss: 0.82 epoch: 24801/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.75 epoch: 24901/30000, d_loss: 0.72, d_acc: 0.47, g_loss: 0.75 epoch: 25001/30000, d_loss: 0.67, d_acc: 0.58, g_loss: 0.77 epoch: 25101/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.76 epoch: 25201/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.81 epoch: 25301/30000, d_loss: 0.69, d_acc: 0.42, g_loss: 0.74 epoch: 25401/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.80 epoch: 25501/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.84 epoch: 25601/30000, d_loss: 0.71, d_acc: 0.52, g_loss: 0.74 epoch: 25701/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.79 epoch: 25801/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.79 epoch: 25901/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.74 epoch: 26001/30000, d_loss: 0.67, d_acc: 0.67, g_loss: 0.76 epoch: 26101/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.76 epoch: 26201/30000, d_loss: 0.68, d_acc: 0.50, g_loss: 0.80 epoch: 26301/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.80 epoch: 26401/30000, d_loss: 0.70, d_acc: 0.48, g_loss: 0.76 epoch: 26501/30000, d_loss: 0.70, d_acc: 0.44, g_loss: 0.81 epoch: 26601/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.78 epoch: 26701/30000, d_loss: 0.68, d_acc: 0.53, g_loss: 0.75 epoch: 26801/30000, d_loss: 0.69, d_acc: 0.64, g_loss: 0.78 epoch: 26901/30000, d_loss: 0.67, d_acc: 0.58, g_loss: 0.82 epoch: 27001/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.77 epoch: 27101/30000, d_loss: 0.72, d_acc: 0.53, g_loss: 0.79 epoch: 27201/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.73 epoch: 27301/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.77 epoch: 27401/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.78 epoch: 27501/30000, d_loss: 0.70, d_acc: 0.48, g_loss: 0.73 epoch: 27601/30000, d_loss: 0.65, d_acc: 0.62, g_loss: 0.76 epoch: 27701/30000, d_loss: 0.70, d_acc: 0.56, g_loss: 0.78 epoch: 27801/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.79 epoch: 27901/30000, d_loss: 0.68, d_acc: 0.52, g_loss: 0.74 epoch: 28001/30000, d_loss: 0.66, d_acc: 0.58, g_loss: 0.76 epoch: 28101/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.79 epoch: 28201/30000, d_loss: 0.72, d_acc: 0.44, g_loss: 0.78 epoch: 28301/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.74 epoch: 28401/30000, d_loss: 0.66, d_acc: 0.59, g_loss: 0.75 epoch: 28501/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.78 epoch: 28601/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.77 epoch: 28701/30000, d_loss: 0.68, d_acc: 0.62, g_loss: 0.76 epoch: 28801/30000, d_loss: 0.71, d_acc: 0.53, g_loss: 0.79 epoch: 28901/30000, d_loss: 0.70, d_acc: 0.45, g_loss: 0.79 epoch: 29001/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.76 epoch: 29101/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.81 epoch: 29201/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.74 epoch: 29301/30000, d_loss: 0.69, d_acc: 0.45, g_loss: 0.80 epoch: 29401/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.76 epoch: 29501/30000, d_loss: 0.69, d_acc: 0.61, g_loss: 0.74 epoch: 29601/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.75 epoch: 29701/30000, d_loss: 0.65, d_acc: 0.61, g_loss: 0.78 epoch: 29801/30000, d_loss: 0.66, d_acc: 0.61, g_loss: 0.75 epoch: 29901/30000, d_loss: 0.66, d_acc: 0.64, g_loss: 0.78 plt . plot ( g_losses , label = 'g_losses' ) plt . plot ( d_losses , label = 'd_losses' ) plt . legend () <matplotlib.legend.Legend at 0x7f46a05ca748> ! ls gan_images 0.png 13800.png 17800.png 21600.png 25600.png 29600.png 6600.png 10000.png 14000.png 18000.png 21800.png 25800.png 29800.png 6800.png 1000.png 1400.png 1800.png 22000.png 26000.png 3000.png 7000.png 10200.png 14200.png 18200.png 2200.png 2600.png 3200.png 7200.png 10400.png 14400.png 18400.png 22200.png 26200.png 3400.png 7400.png 10600.png 14600.png 18600.png 22400.png 26400.png 3600.png 7600.png 10800.png 14800.png 18800.png 22600.png 26600.png 3800.png 7800.png 11000.png 15000.png 19000.png 22800.png 26800.png 4000.png 8000.png 11200.png 15200.png 19200.png 23000.png 27000.png 400.png 800.png 11400.png 15400.png 19400.png 23200.png 27200.png 4200.png 8200.png 11600.png 15600.png 19600.png 23400.png 27400.png 4400.png 8400.png 11800.png 15800.png 19800.png 23600.png 27600.png 4600.png 8600.png 12000.png 16000.png 20000.png 23800.png 27800.png 4800.png 8800.png 1200.png 1600.png 2000.png 24000.png 28000.png 5000.png 9000.png 12200.png 16200.png 200.png 2400.png 2800.png 5200.png 9200.png 12400.png 16400.png 20200.png 24200.png 28200.png 5400.png 9400.png 12600.png 16600.png 20400.png 24400.png 28400.png 5600.png 9600.png 12800.png 16800.png 20600.png 24600.png 28600.png 5800.png 9800.png 13000.png 17000.png 20800.png 24800.png 28800.png 6000.png 13200.png 17200.png 21000.png 25000.png 29000.png 600.png 13400.png 17400.png 21200.png 25200.png 29200.png 6200.png 13600.png 17600.png 21400.png 25400.png 29400.png 6400.png from skimage.io import imread a = imread ( 'gan_images/0.png' ) plt . imshow ( a ) <matplotlib.image.AxesImage at 0x7f46a05a69b0> a = imread ( 'gan_images/1000.png' ) plt . imshow ( a ) <matplotlib.image.AxesImage at 0x7f46a0484eb8> a = imread ( 'gan_images/5000.png' ) plt . imshow ( a ) <matplotlib.image.AxesImage at 0x7f46a0467a58> a = imread ( 'gan_images/10000.png' ) plt . imshow ( a ) <matplotlib.image.AxesImage at 0x7f46a03cd588> a = imread ( 'gan_images/20000.png' ) plt . imshow ( a ) <matplotlib.image.AxesImage at 0x7f46a03720f0> a = imread ( 'gan_images/29800.png' ) plt . imshow ( a ) <matplotlib.image.AxesImage at 0x7f46a024ec18> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 GAN"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Basic_Computation/","text":"================ by Jawad Haider Basic Computation Basic Computation \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.2.0-rc2 a = tf . constant ( 3.0 ) b = tf . constant ( 4.0 ) c = tf . sqrt ( a ** 2 + b ** 2 ) print ( \"c:\" , c ) # if you use Python 3 f-strings it will print # the tensor as a float print ( f \"c: { c } \" ) c: tf.Tensor(5.0, shape=(), dtype=float32) c: 5.0 # Get the Numpy version of a Tensor c . numpy () 5.0 type ( c . numpy ()) numpy.float32 a = tf . constant ([ 1 , 2 , 3 ]) b = tf . constant ([ 4 , 5 , 6 ]) print ( f \"b: { b } \" ) c = tf . tensordot ( a , b , axes = [ 0 , 0 ]) print ( f \"c: { c } \" ) b: [4 5 6] c: 32 a . numpy () . dot ( b . numpy ()) import numpy as np A0 = np . random . randn ( 3 , 3 ) b0 = np . random . randn ( 3 , 1 ) c0 = A0 . dot ( b0 ) print ( f \"c0: { c0 } \" ) A = tf . constant ( A0 ) b = tf . constant ( b0 ) c = tf . matmul ( A , b ) print ( f \"c: { c } \" ) c0: [[ 1.13966116] [-0.31443995] [-0.78649886]] c: [[ 1.13966116] [-0.31443995] [-0.78649886]] # Broadcasting A = tf . constant ([[ 1 , 2 ],[ 3 , 4 ]]) b = tf . constant ( 1 ) C = A + b print ( f \"C: { C } \" ) C: [[2 3] [4 5]] # Element-wise multiplication A = tf . constant ([[ 1 , 2 ],[ 3 , 4 ]]) B = tf . constant ([[ 2 , 3 ],[ 4 , 5 ]]) C = A * B print ( f \"C: { C } \" ) C: [[ 2 6] [12 20]] Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Basic Computation"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Basic_Computation/#basic-computation","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.2.0-rc2 a = tf . constant ( 3.0 ) b = tf . constant ( 4.0 ) c = tf . sqrt ( a ** 2 + b ** 2 ) print ( \"c:\" , c ) # if you use Python 3 f-strings it will print # the tensor as a float print ( f \"c: { c } \" ) c: tf.Tensor(5.0, shape=(), dtype=float32) c: 5.0 # Get the Numpy version of a Tensor c . numpy () 5.0 type ( c . numpy ()) numpy.float32 a = tf . constant ([ 1 , 2 , 3 ]) b = tf . constant ([ 4 , 5 , 6 ]) print ( f \"b: { b } \" ) c = tf . tensordot ( a , b , axes = [ 0 , 0 ]) print ( f \"c: { c } \" ) b: [4 5 6] c: 32 a . numpy () . dot ( b . numpy ()) import numpy as np A0 = np . random . randn ( 3 , 3 ) b0 = np . random . randn ( 3 , 1 ) c0 = A0 . dot ( b0 ) print ( f \"c0: { c0 } \" ) A = tf . constant ( A0 ) b = tf . constant ( b0 ) c = tf . matmul ( A , b ) print ( f \"c: { c } \" ) c0: [[ 1.13966116] [-0.31443995] [-0.78649886]] c: [[ 1.13966116] [-0.31443995] [-0.78649886]] # Broadcasting A = tf . constant ([[ 1 , 2 ],[ 3 , 4 ]]) b = tf . constant ( 1 ) C = A + b print ( f \"C: { C } \" ) C: [[2 3] [4 5]] # Element-wise multiplication A = tf . constant ([[ 1 , 2 ],[ 3 , 4 ]]) B = tf . constant ([[ 2 , 3 ],[ 4 , 5 ]]) C = A * B print ( f \"C: { C } \" ) C: [[ 2 6] [12 20]] Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Basic Computation"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Build_Your_Own_Model/","text":"================ by Jawad Haider Build your own model in Tensorflow 2.0 Build your own model in Tensorflow 2.0 \u00b6 This notebook will teach you how to build your own custom model, where you define your own variables and how to compute predictions. # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 # Other imports import numpy as np import matplotlib.pyplot as plt # Define linear regression model class LinearRegression ( tf . keras . Model ): def __init__ ( self , num_inputs , num_outputs ): super ( LinearRegression , self ) . __init__ () self . W = tf . Variable ( tf . random_normal_initializer ()(( num_inputs , num_outputs ))) self . b = tf . Variable ( tf . zeros ( num_outputs )) self . params = [ self . W , self . b ] def call ( self , inputs ): return tf . matmul ( inputs , self . W ) + self . b # Create a dataset N = 100 D = 1 K = 1 X = np . random . random (( N , D )) * 2 - 1 w = np . random . randn ( D , K ) b = np . random . randn () Y = X . dot ( w ) + b + np . random . randn ( N , 1 ) * 0.1 plt . scatter ( X , Y ) <matplotlib.collections.PathCollection at 0x7fc430437e48> # Cast type, otherwise Tensorflow will complain X = X . astype ( np . float32 ) Y = Y . astype ( np . float32 ) # Define the loss def get_loss ( model , inputs , targets ): predictions = model ( inputs ) error = targets - predictions return tf . reduce_mean ( tf . square ( error )) # Gradient function def get_grad ( model , inputs , targets ): with tf . GradientTape () as tape : # calculate the loss loss_value = get_loss ( model , inputs , targets ) # return gradient return tape . gradient ( loss_value , model . params ) # Create and train the model model = LinearRegression ( D , K ) # Print the params before training print ( \"Initial params:\" ) print ( model . W ) print ( model . b ) Initial params: <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[-0.07139343]], dtype=float32)> <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)> # Store the losses here losses = [] # Create an optimizer optimizer = tf . keras . optimizers . SGD ( learning_rate = 0.2 ) # Run the training loop for i in range ( 100 ): # Get gradients grads = get_grad ( model , X , Y ) # Do one step of gradient descent: param <- param - learning_rate * grad optimizer . apply_gradients ( zip ( grads , model . params )) # Store the loss loss = get_loss ( model , X , Y ) losses . append ( loss ) plt . plot ( losses ) x_axis = np . linspace ( X . min (), X . max (), 100 ) y_axis = model . predict ( x_axis . reshape ( - 1 , 1 )) . flatten () plt . scatter ( X , Y ) plt . plot ( x_axis , y_axis ) print ( \"Predicted params:\" ) print ( model . W ) print ( model . b ) Predicted params: <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[1.0059998]], dtype=float32)> <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([-0.3224416], dtype=float32)> print ( \"True params:\" ) w , b True params: (array([[1.01350001]]), -0.3286531216778375) Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Build Your Own Model"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Build_Your_Own_Model/#build-your-own-model-in-tensorflow-20","text":"This notebook will teach you how to build your own custom model, where you define your own variables and how to compute predictions. # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 # Other imports import numpy as np import matplotlib.pyplot as plt # Define linear regression model class LinearRegression ( tf . keras . Model ): def __init__ ( self , num_inputs , num_outputs ): super ( LinearRegression , self ) . __init__ () self . W = tf . Variable ( tf . random_normal_initializer ()(( num_inputs , num_outputs ))) self . b = tf . Variable ( tf . zeros ( num_outputs )) self . params = [ self . W , self . b ] def call ( self , inputs ): return tf . matmul ( inputs , self . W ) + self . b # Create a dataset N = 100 D = 1 K = 1 X = np . random . random (( N , D )) * 2 - 1 w = np . random . randn ( D , K ) b = np . random . randn () Y = X . dot ( w ) + b + np . random . randn ( N , 1 ) * 0.1 plt . scatter ( X , Y ) <matplotlib.collections.PathCollection at 0x7fc430437e48> # Cast type, otherwise Tensorflow will complain X = X . astype ( np . float32 ) Y = Y . astype ( np . float32 ) # Define the loss def get_loss ( model , inputs , targets ): predictions = model ( inputs ) error = targets - predictions return tf . reduce_mean ( tf . square ( error )) # Gradient function def get_grad ( model , inputs , targets ): with tf . GradientTape () as tape : # calculate the loss loss_value = get_loss ( model , inputs , targets ) # return gradient return tape . gradient ( loss_value , model . params ) # Create and train the model model = LinearRegression ( D , K ) # Print the params before training print ( \"Initial params:\" ) print ( model . W ) print ( model . b ) Initial params: <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[-0.07139343]], dtype=float32)> <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)> # Store the losses here losses = [] # Create an optimizer optimizer = tf . keras . optimizers . SGD ( learning_rate = 0.2 ) # Run the training loop for i in range ( 100 ): # Get gradients grads = get_grad ( model , X , Y ) # Do one step of gradient descent: param <- param - learning_rate * grad optimizer . apply_gradients ( zip ( grads , model . params )) # Store the loss loss = get_loss ( model , X , Y ) losses . append ( loss ) plt . plot ( losses ) x_axis = np . linspace ( X . min (), X . max (), 100 ) y_axis = model . predict ( x_axis . reshape ( - 1 , 1 )) . flatten () plt . scatter ( X , Y ) plt . plot ( x_axis , y_axis ) print ( \"Predicted params:\" ) print ( model . W ) print ( model . b ) Predicted params: <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[1.0059998]], dtype=float32)> <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([-0.3224416], dtype=float32)> print ( \"True params:\" ) w , b True params: (array([[1.01350001]]), -0.3286531216778375) Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Build your own model in Tensorflow 2.0"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Variables_and_Gradient_Tape/","text":"================ by Jawad Haider Variables and Gradient Tape Tensorflow 2.0 Variables Variables and Gradient Tape \u00b6 Tensorflow 2.0 Variables \u00b6 This notebook will teach you about variables and how to build a basic computation graph. We will perform gradient descent manually to optimize a simple function. If you\u2019re familiar with Tensorflow 1.x, you will find this useful as an example to demonstrate how we can do the same operations, but without sessions, initializers, etc. # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 # First, what is the difference between mutable and immutable? # A tuple is immutable # This should result in an error a = ( 1 , 2 , 3 ) a [ 0 ] = 5 TypeError: ignored # A list is mutable a = [ 1 , 2 , 3 ] a [ 0 ] = 5 print ( a ) [5, 2, 3] # Now Tensorflow variables a = tf . Variable ( 5. ) b = tf . Variable ( 3. ) print ( a * b ) # Eager execution! No need for session.run() or variable initializer tf.Tensor(15.0, shape=(), dtype=float32) # Because it's a variable, it can be updated a = a + 1 print ( a ) tf.Tensor(6.0, shape=(), dtype=float32) # Variables and constants c = tf . constant ( 4. ) print ( a * b + c ) tf.Tensor(22.0, shape=(), dtype=float32) # Let's demonstrate a simple optimization problem # L(w) = w**2 w = tf . Variable ( 5. ) # Now, let us define a loss function def get_loss ( w ): return w ** 2 # Use \"gradient tape\" to record the gradients def get_grad ( w ): with tf . GradientTape () as tape : L = get_loss ( w ) # Get the gradient g = tape . gradient ( L , w ) return g # Define an optimizer optimizer = tf . keras . optimizers . SGD ( learning_rate = 0.1 ) # Store the losses losses = [] # Perform gradient descent for i in range ( 50 ): g = get_grad ( w ) optimizer . apply_gradients ( zip ([ g ], [ w ])) losses . append ( get_loss ( w )) WARNING: Logging before flag parsing goes to stderr. W0812 20:20:03.417810 140436135712640 deprecation.py:323] From /tensorflow-2.0.0b1/python3.6/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where import matplotlib.pyplot as plt plt . plot ( losses ) print ( f \"Final loss: { get_loss ( w ) } \" ) Final loss: 5.0925916816879635e-09 # Let's do the same thing again, but manually w = tf . Variable ( 5. ) # Store the losses losses2 = [] # Perform gradient descent for i in range ( 50 ): # This is doing: w = w - 0.1 * 2 * w # But we don't want to create a new Tensor w . assign ( w - 0.1 * 2 * w ) losses2 . append ( w ** 2 ) plt . plot ( losses , label = \"losses tf\" ) plt . plot ( losses2 , label = \"losses manual\" ) plt . legend () <matplotlib.legend.Legend at 0x7fb994fd99e8> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Variables and Gradient Tape"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Variables_and_Gradient_Tape/#variables-and-gradient-tape","text":"","title":"Variables and Gradient Tape"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Variables_and_Gradient_Tape/#tensorflow-20-variables","text":"This notebook will teach you about variables and how to build a basic computation graph. We will perform gradient descent manually to optimize a simple function. If you\u2019re familiar with Tensorflow 1.x, you will find this useful as an example to demonstrate how we can do the same operations, but without sessions, initializers, etc. # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 # First, what is the difference between mutable and immutable? # A tuple is immutable # This should result in an error a = ( 1 , 2 , 3 ) a [ 0 ] = 5 TypeError: ignored # A list is mutable a = [ 1 , 2 , 3 ] a [ 0 ] = 5 print ( a ) [5, 2, 3] # Now Tensorflow variables a = tf . Variable ( 5. ) b = tf . Variable ( 3. ) print ( a * b ) # Eager execution! No need for session.run() or variable initializer tf.Tensor(15.0, shape=(), dtype=float32) # Because it's a variable, it can be updated a = a + 1 print ( a ) tf.Tensor(6.0, shape=(), dtype=float32) # Variables and constants c = tf . constant ( 4. ) print ( a * b + c ) tf.Tensor(22.0, shape=(), dtype=float32) # Let's demonstrate a simple optimization problem # L(w) = w**2 w = tf . Variable ( 5. ) # Now, let us define a loss function def get_loss ( w ): return w ** 2 # Use \"gradient tape\" to record the gradients def get_grad ( w ): with tf . GradientTape () as tape : L = get_loss ( w ) # Get the gradient g = tape . gradient ( L , w ) return g # Define an optimizer optimizer = tf . keras . optimizers . SGD ( learning_rate = 0.1 ) # Store the losses losses = [] # Perform gradient descent for i in range ( 50 ): g = get_grad ( w ) optimizer . apply_gradients ( zip ([ g ], [ w ])) losses . append ( get_loss ( w )) WARNING: Logging before flag parsing goes to stderr. W0812 20:20:03.417810 140436135712640 deprecation.py:323] From /tensorflow-2.0.0b1/python3.6/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where import matplotlib.pyplot as plt plt . plot ( losses ) print ( f \"Final loss: { get_loss ( w ) } \" ) Final loss: 5.0925916816879635e-09 # Let's do the same thing again, but manually w = tf . Variable ( 5. ) # Store the losses losses2 = [] # Perform gradient descent for i in range ( 50 ): # This is doing: w = w - 0.1 * 2 * w # But we don't want to create a new Tensor w . assign ( w - 0.1 * 2 * w ) losses2 . append ( w ** 2 ) plt . plot ( losses , label = \"losses tf\" ) plt . plot ( losses2 , label = \"losses manual\" ) plt . legend () <matplotlib.legend.Legend at 0x7fb994fd99e8> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Tensorflow 2.0 Variables"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Classification/","text":"================ by Jawad Haider Linear Classification Part 2: Making Predictions Part 3: Saving and Loading a Model Linear Classification \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.5.0 # Load in the data from sklearn.datasets import load_breast_cancer # load the data data = load_breast_cancer () # check the type of 'data' type ( data ) sklearn.utils.Bunch # note: it is a Bunch object # this basically acts like a dictionary where you can treat the keys like attributes data . keys () dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']) # 'data' (the attribute) means the input data data . data . shape # it has 569 samples, 30 features (569, 30) # 'targets' data . target # note how the targets are just 0s and 1s # normally, when you have K targets, they are labeled 0..K-1 array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]) # their meaning is not lost data . target_names array(['malignant', 'benign'], dtype='<U9') # there are also 569 corresponding targets data . target . shape (569,) # you can also determine the meaning of each feature data . feature_names array(['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension'], dtype='<U23') # normally we would put all of our imports at the top # but this lets us tell a story from sklearn.model_selection import train_test_split # split the data into train and test sets # this lets us simulate how our model will perform in the future X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = 0.33 ) N , D = X_train . shape # Scale the data # you'll learn why scaling is needed in a later course from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X_train = scaler . fit_transform ( X_train ) X_test = scaler . transform ( X_test ) # Now all the fun Tensorflow stuff # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Input ( shape = ( D ,)), tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) ]) # Alternatively, you can do: # model = tf.keras.models.Sequential() # model.add(tf.keras.layers.Dense(1, input_shape=(D,), activation='sigmoid')) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) # Train the model r = model . fit ( X_train , y_train , validation_data = ( X_test , y_test ), epochs = 100 ) # Evaluate the model - evaluate() returns loss and accuracy print ( \"Train score:\" , model . evaluate ( X_train , y_train )) print ( \"Test score:\" , model . evaluate ( X_test , y_test )) WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model. Epoch 1/100 12/12 [==============================] - 3s 17ms/step - loss: 0.7727 - accuracy: 0.4724 - val_loss: 0.6664 - val_accuracy: 0.5479 Epoch 2/100 12/12 [==============================] - 0s 5ms/step - loss: 0.7026 - accuracy: 0.5643 - val_loss: 0.6001 - val_accuracy: 0.6596 Epoch 3/100 12/12 [==============================] - 0s 4ms/step - loss: 0.6389 - accuracy: 0.6378 - val_loss: 0.5451 - val_accuracy: 0.7394 Epoch 4/100 12/12 [==============================] - 0s 5ms/step - loss: 0.5867 - accuracy: 0.6877 - val_loss: 0.4977 - val_accuracy: 0.7979 Epoch 5/100 12/12 [==============================] - 0s 4ms/step - loss: 0.5422 - accuracy: 0.7559 - val_loss: 0.4578 - val_accuracy: 0.8457 Epoch 6/100 12/12 [==============================] - 0s 6ms/step - loss: 0.5047 - accuracy: 0.7848 - val_loss: 0.4241 - val_accuracy: 0.8564 Epoch 7/100 12/12 [==============================] - 0s 5ms/step - loss: 0.4718 - accuracy: 0.8163 - val_loss: 0.3957 - val_accuracy: 0.8617 Epoch 8/100 12/12 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.8451 - val_loss: 0.3710 - val_accuracy: 0.8830 Epoch 9/100 12/12 [==============================] - 0s 6ms/step - loss: 0.4201 - accuracy: 0.8530 - val_loss: 0.3494 - val_accuracy: 0.8989 Epoch 10/100 12/12 [==============================] - 0s 6ms/step - loss: 0.3989 - accuracy: 0.8714 - val_loss: 0.3305 - val_accuracy: 0.9096 Epoch 11/100 12/12 [==============================] - 0s 4ms/step - loss: 0.3797 - accuracy: 0.8819 - val_loss: 0.3139 - val_accuracy: 0.9149 Epoch 12/100 12/12 [==============================] - 0s 4ms/step - loss: 0.3626 - accuracy: 0.8950 - val_loss: 0.2993 - val_accuracy: 0.9202 Epoch 13/100 12/12 [==============================] - 0s 5ms/step - loss: 0.3477 - accuracy: 0.9055 - val_loss: 0.2859 - val_accuracy: 0.9255 Epoch 14/100 12/12 [==============================] - 0s 4ms/step - loss: 0.3341 - accuracy: 0.9055 - val_loss: 0.2739 - val_accuracy: 0.9255 Epoch 15/100 12/12 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.9108 - val_loss: 0.2632 - val_accuracy: 0.9309 Epoch 16/100 12/12 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.9134 - val_loss: 0.2534 - val_accuracy: 0.9309 Epoch 17/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2992 - accuracy: 0.9160 - val_loss: 0.2446 - val_accuracy: 0.9362 Epoch 18/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2896 - accuracy: 0.9160 - val_loss: 0.2363 - val_accuracy: 0.9362 Epoch 19/100 12/12 [==============================] - 0s 5ms/step - loss: 0.2806 - accuracy: 0.9213 - val_loss: 0.2287 - val_accuracy: 0.9362 Epoch 20/100 12/12 [==============================] - 0s 5ms/step - loss: 0.2723 - accuracy: 0.9291 - val_loss: 0.2217 - val_accuracy: 0.9362 Epoch 21/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2645 - accuracy: 0.9291 - val_loss: 0.2151 - val_accuracy: 0.9362 Epoch 22/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2571 - accuracy: 0.9291 - val_loss: 0.2091 - val_accuracy: 0.9415 Epoch 23/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2505 - accuracy: 0.9291 - val_loss: 0.2033 - val_accuracy: 0.9468 Epoch 24/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2440 - accuracy: 0.9318 - val_loss: 0.1980 - val_accuracy: 0.9468 Epoch 25/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2380 - accuracy: 0.9344 - val_loss: 0.1931 - val_accuracy: 0.9468 Epoch 26/100 12/12 [==============================] - 0s 5ms/step - loss: 0.2323 - accuracy: 0.9423 - val_loss: 0.1884 - val_accuracy: 0.9468 Epoch 27/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2270 - accuracy: 0.9449 - val_loss: 0.1841 - val_accuracy: 0.9468 Epoch 28/100 12/12 [==============================] - 0s 6ms/step - loss: 0.2221 - accuracy: 0.9449 - val_loss: 0.1799 - val_accuracy: 0.9468 Epoch 29/100 12/12 [==============================] - 0s 6ms/step - loss: 0.2173 - accuracy: 0.9449 - val_loss: 0.1759 - val_accuracy: 0.9468 Epoch 30/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2127 - accuracy: 0.9475 - val_loss: 0.1722 - val_accuracy: 0.9468 Epoch 31/100 12/12 [==============================] - 0s 5ms/step - loss: 0.2083 - accuracy: 0.9501 - val_loss: 0.1688 - val_accuracy: 0.9468 Epoch 32/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.9528 - val_loss: 0.1655 - val_accuracy: 0.9521 Epoch 33/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2006 - accuracy: 0.9528 - val_loss: 0.1622 - val_accuracy: 0.9521 Epoch 34/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1968 - accuracy: 0.9528 - val_loss: 0.1592 - val_accuracy: 0.9574 Epoch 35/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1932 - accuracy: 0.9528 - val_loss: 0.1564 - val_accuracy: 0.9574 Epoch 36/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1900 - accuracy: 0.9554 - val_loss: 0.1536 - val_accuracy: 0.9628 Epoch 37/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1867 - accuracy: 0.9554 - val_loss: 0.1510 - val_accuracy: 0.9681 Epoch 38/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9554 - val_loss: 0.1486 - val_accuracy: 0.9681 Epoch 39/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1807 - accuracy: 0.9554 - val_loss: 0.1462 - val_accuracy: 0.9681 Epoch 40/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1779 - accuracy: 0.9580 - val_loss: 0.1439 - val_accuracy: 0.9681 Epoch 41/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1752 - accuracy: 0.9580 - val_loss: 0.1418 - val_accuracy: 0.9681 Epoch 42/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1727 - accuracy: 0.9580 - val_loss: 0.1396 - val_accuracy: 0.9681 Epoch 43/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1702 - accuracy: 0.9580 - val_loss: 0.1376 - val_accuracy: 0.9681 Epoch 44/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1678 - accuracy: 0.9580 - val_loss: 0.1357 - val_accuracy: 0.9681 Epoch 45/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1655 - accuracy: 0.9580 - val_loss: 0.1338 - val_accuracy: 0.9681 Epoch 46/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1633 - accuracy: 0.9580 - val_loss: 0.1321 - val_accuracy: 0.9681 Epoch 47/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1613 - accuracy: 0.9580 - val_loss: 0.1304 - val_accuracy: 0.9681 Epoch 48/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1592 - accuracy: 0.9606 - val_loss: 0.1287 - val_accuracy: 0.9681 Epoch 49/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9606 - val_loss: 0.1272 - val_accuracy: 0.9681 Epoch 50/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9606 - val_loss: 0.1256 - val_accuracy: 0.9681 Epoch 51/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.9606 - val_loss: 0.1242 - val_accuracy: 0.9681 Epoch 52/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9606 - val_loss: 0.1227 - val_accuracy: 0.9681 Epoch 53/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9633 - val_loss: 0.1213 - val_accuracy: 0.9681 Epoch 54/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1484 - accuracy: 0.9633 - val_loss: 0.1199 - val_accuracy: 0.9681 Epoch 55/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1467 - accuracy: 0.9633 - val_loss: 0.1186 - val_accuracy: 0.9681 Epoch 56/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1453 - accuracy: 0.9633 - val_loss: 0.1174 - val_accuracy: 0.9734 Epoch 57/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.9633 - val_loss: 0.1162 - val_accuracy: 0.9734 Epoch 58/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1423 - accuracy: 0.9633 - val_loss: 0.1150 - val_accuracy: 0.9734 Epoch 59/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1408 - accuracy: 0.9659 - val_loss: 0.1139 - val_accuracy: 0.9734 Epoch 60/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1395 - accuracy: 0.9659 - val_loss: 0.1127 - val_accuracy: 0.9734 Epoch 61/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9659 - val_loss: 0.1117 - val_accuracy: 0.9734 Epoch 62/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1368 - accuracy: 0.9659 - val_loss: 0.1107 - val_accuracy: 0.9734 Epoch 63/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1356 - accuracy: 0.9659 - val_loss: 0.1096 - val_accuracy: 0.9734 Epoch 64/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 0.9659 - val_loss: 0.1086 - val_accuracy: 0.9734 Epoch 65/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1332 - accuracy: 0.9659 - val_loss: 0.1077 - val_accuracy: 0.9734 Epoch 66/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1320 - accuracy: 0.9659 - val_loss: 0.1067 - val_accuracy: 0.9734 Epoch 67/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1309 - accuracy: 0.9659 - val_loss: 0.1059 - val_accuracy: 0.9734 Epoch 68/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1299 - accuracy: 0.9659 - val_loss: 0.1050 - val_accuracy: 0.9734 Epoch 69/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9659 - val_loss: 0.1041 - val_accuracy: 0.9734 Epoch 70/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1277 - accuracy: 0.9685 - val_loss: 0.1033 - val_accuracy: 0.9734 Epoch 71/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1267 - accuracy: 0.9685 - val_loss: 0.1024 - val_accuracy: 0.9734 Epoch 72/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1258 - accuracy: 0.9711 - val_loss: 0.1016 - val_accuracy: 0.9734 Epoch 73/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1248 - accuracy: 0.9738 - val_loss: 0.1008 - val_accuracy: 0.9734 Epoch 74/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9738 - val_loss: 0.1001 - val_accuracy: 0.9734 Epoch 75/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1230 - accuracy: 0.9738 - val_loss: 0.0994 - val_accuracy: 0.9734 Epoch 76/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9738 - val_loss: 0.0987 - val_accuracy: 0.9734 Epoch 77/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1213 - accuracy: 0.9738 - val_loss: 0.0979 - val_accuracy: 0.9734 Epoch 78/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1204 - accuracy: 0.9738 - val_loss: 0.0972 - val_accuracy: 0.9734 Epoch 79/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1196 - accuracy: 0.9738 - val_loss: 0.0966 - val_accuracy: 0.9734 Epoch 80/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1188 - accuracy: 0.9738 - val_loss: 0.0959 - val_accuracy: 0.9787 Epoch 81/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1180 - accuracy: 0.9738 - val_loss: 0.0953 - val_accuracy: 0.9840 Epoch 82/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1173 - accuracy: 0.9738 - val_loss: 0.0947 - val_accuracy: 0.9840 Epoch 83/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1165 - accuracy: 0.9738 - val_loss: 0.0940 - val_accuracy: 0.9840 Epoch 84/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9764 - val_loss: 0.0934 - val_accuracy: 0.9840 Epoch 85/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1150 - accuracy: 0.9764 - val_loss: 0.0928 - val_accuracy: 0.9840 Epoch 86/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9764 - val_loss: 0.0923 - val_accuracy: 0.9840 Epoch 87/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9764 - val_loss: 0.0917 - val_accuracy: 0.9840 Epoch 88/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1130 - accuracy: 0.9764 - val_loss: 0.0912 - val_accuracy: 0.9840 Epoch 89/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1123 - accuracy: 0.9764 - val_loss: 0.0906 - val_accuracy: 0.9840 Epoch 90/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1117 - accuracy: 0.9764 - val_loss: 0.0900 - val_accuracy: 0.9840 Epoch 91/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9764 - val_loss: 0.0895 - val_accuracy: 0.9840 Epoch 92/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9764 - val_loss: 0.0890 - val_accuracy: 0.9840 Epoch 93/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1098 - accuracy: 0.9764 - val_loss: 0.0886 - val_accuracy: 0.9840 Epoch 94/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1093 - accuracy: 0.9790 - val_loss: 0.0881 - val_accuracy: 0.9840 Epoch 95/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1087 - accuracy: 0.9790 - val_loss: 0.0876 - val_accuracy: 0.9840 Epoch 96/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1081 - accuracy: 0.9790 - val_loss: 0.0871 - val_accuracy: 0.9840 Epoch 97/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9790 - val_loss: 0.0866 - val_accuracy: 0.9840 Epoch 98/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9790 - val_loss: 0.0862 - val_accuracy: 0.9840 Epoch 99/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1065 - accuracy: 0.9790 - val_loss: 0.0858 - val_accuracy: 0.9840 Epoch 100/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.9790 - val_loss: 0.0853 - val_accuracy: 0.9840 12/12 [==============================] - 0s 2ms/step - loss: 0.1056 - accuracy: 0.9790 Train score: [0.10559554398059845, 0.9790025949478149] 6/6 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9840 Test score: [0.08529960364103317, 0.9840425252914429] # Plot what's returned by model.fit() import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f75702b9790> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f75701ec310> Part 2: Making Predictions \u00b6 This goes with the lecture \u201cMaking Predictions\u201d # Make predictions P = model . predict ( X_test ) print ( P ) # they are outputs of the sigmoid, interpreted as probabilities p(y = 1 | x) [[9.35080469e-01] [9.99298692e-01] [9.98439729e-01] [9.77653563e-01] [5.64797297e-02] [1.55710494e-02] [6.10508978e-01] [8.52784812e-01] [9.31189537e-01] [9.98825729e-01] [8.09484124e-01] [9.13891383e-03] [9.96914744e-01] [9.93904889e-01] [9.81451333e-01] [9.55752373e-01] [2.45447876e-03] [4.61153733e-03] [5.73845464e-04] [2.48237103e-02] [9.90265667e-01] [3.78944203e-02] [9.94276583e-01] [9.98708129e-01] [9.61843669e-01] [9.87939239e-01] [9.99808848e-01] [9.96969759e-01] [2.85914261e-02] [9.56589937e-01] [9.60226953e-01] [2.69837608e-03] [9.96669829e-01] [9.84309673e-01] [3.50100487e-01] [8.31614912e-01] [9.95373189e-01] [2.96407286e-03] [9.67374027e-01] [9.73011911e-01] [9.94610310e-01] [4.85418539e-04] [8.88491213e-01] [9.84727025e-01] [8.90673800e-06] [9.98509824e-01] [4.42464858e-01] [1.32060540e-03] [1.17375106e-01] [8.27795267e-01] [3.74988168e-01] [1.91124864e-02] [9.95917022e-01] [9.98196542e-01] [4.74949419e-01] [8.22081923e-01] [8.90725613e-01] [9.98178959e-01] [1.02176724e-04] [1.82664804e-02] [9.50180948e-01] [1.43383248e-02] [9.42624748e-01] [5.96681166e-05] [9.99727428e-01] [9.69099402e-01] [2.24412568e-02] [3.96199212e-05] [3.38991098e-02] [9.91276979e-01] [9.91171777e-01] [9.82203007e-01] [6.71500042e-02] [9.87396777e-01] [5.30279765e-04] [9.57228303e-01] [9.98355210e-01] [9.14368153e-01] [9.52866495e-01] [9.97870207e-01] [1.34615432e-02] [2.44203299e-01] [8.79299402e-01] [9.77223754e-01] [9.28715587e-01] [2.52999127e-01] [2.69606203e-01] [9.99876618e-01] [1.51222537e-03] [9.53690171e-01] [4.03010815e-01] [9.98407781e-01] [6.50674924e-02] [9.99741018e-01] [1.14408717e-01] [8.39701653e-01] [9.96540427e-01] [8.24299932e-01] [1.20999947e-01] [9.98341918e-01] [9.98978138e-01] [9.87704277e-01] [9.43565965e-01] [9.89912093e-01] [9.82849538e-01] [1.31196016e-03] [4.07409249e-03] [6.24320284e-03] [7.39843130e-01] [9.90274429e-01] [9.96425331e-01] [4.27843165e-03] [9.28301930e-01] [9.49783742e-01] [9.80192959e-01] [8.80274863e-04] [9.91823375e-01] [9.76207674e-01] [9.17195439e-01] [9.98508751e-01] [9.27063286e-01] [6.56274676e-01] [9.99204099e-01] [9.82241213e-01] [9.71006870e-01] [9.63477552e-01] [9.98024106e-01] [9.99295473e-01] [9.87832904e-01] [3.22171450e-02] [8.78069401e-01] [4.87438217e-02] [9.59760249e-01] [1.59445778e-01] [4.66189593e-01] [9.82630789e-01] [9.93440092e-01] [2.06552606e-04] [9.97168481e-01] [9.90759015e-01] [9.95073020e-01] [9.30102646e-01] [2.89134146e-03] [9.88580644e-01] [9.95565474e-01] [9.84212875e-01] [2.23457310e-02] [9.83895898e-01] [6.18342310e-04] [9.98992264e-01] [9.94715989e-01] [3.41228060e-02] [2.80370237e-04] [4.87311219e-04] [9.97160077e-01] [8.87251318e-01] [9.65952098e-01] [9.75660741e-01] [9.93650138e-01] [4.88551438e-01] [2.07821256e-03] [7.25426733e-01] [6.17845356e-03] [4.90195416e-05] [8.92993286e-02] [8.22957635e-01] [9.97866690e-01] [2.30895057e-02] [9.76001143e-01] [6.94513857e-01] [6.79192960e-01] [9.77537930e-01] [9.90081012e-01] [9.98470843e-01] [9.93132412e-01] [2.66570807e-03] [9.49018478e-01] [9.90014315e-01] [2.29099810e-01] [9.10406351e-01] [9.98505592e-01] [9.42158282e-01] [9.24770832e-01] [9.11253691e-01] [9.84513938e-01] [9.97072697e-01] [7.51034081e-01] [9.97907519e-01]] # Round to get the actual predictions # Note: has to be flattened since the targets are size (N,) while the predictions are size (N,1) import numpy as np P = np . round ( P ) . flatten () print ( P ) [1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] # Calculate the accuracy, compare it to evaluate() output print ( \"Manually calculated accuracy:\" , np . mean ( P == y_test )) print ( \"Evaluate output:\" , model . evaluate ( X_test , y_test )) Manually calculated accuracy: 0.9840425531914894 6/6 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9840 Evaluate output: [0.08529960364103317, 0.9840425252914429] Part 3: Saving and Loading a Model \u00b6 This goes with the lecture \u201cSaving and Loading a Model\u201d # Let's now save our model to a file model . save ( 'linearclassifier.h5' ) # Check that the model file exists ! ls - lh total 24K -rw-r--r-- 1 root root 19K Jul 18 20:05 linearclassifier.h5 drwxr-xr-x 1 root root 4.0K Jul 15 13:38 sample_data # Let's load the model and confirm that it still works # Note: there is a bug in Keras where load/save only works if you DON'T use the Input() layer explicitly # So, make sure you define the model with ONLY Dense(1, input_shape=(D,)) # At least, until the bug is fixed # https://github.com/keras-team/keras/issues/10417 model = tf . keras . models . load_model ( 'linearclassifier.h5' ) print ( model . layers ) model . evaluate ( X_test , y_test ) [<tensorflow.python.keras.layers.core.Dense object at 0x7f7570102f50>] 6/6 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9840 [0.08529960364103317, 0.9840425252914429] # Download the file - requires Chrome (at this point) from google.colab import files files . download ( 'linearclassifier.h5' ) <IPython.core.display.Javascript object> <IPython.core.display.Javascript object> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ s","title":"TF2 0 Linear Classification"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Classification/#linear-classification","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.5.0 # Load in the data from sklearn.datasets import load_breast_cancer # load the data data = load_breast_cancer () # check the type of 'data' type ( data ) sklearn.utils.Bunch # note: it is a Bunch object # this basically acts like a dictionary where you can treat the keys like attributes data . keys () dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']) # 'data' (the attribute) means the input data data . data . shape # it has 569 samples, 30 features (569, 30) # 'targets' data . target # note how the targets are just 0s and 1s # normally, when you have K targets, they are labeled 0..K-1 array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]) # their meaning is not lost data . target_names array(['malignant', 'benign'], dtype='<U9') # there are also 569 corresponding targets data . target . shape (569,) # you can also determine the meaning of each feature data . feature_names array(['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension'], dtype='<U23') # normally we would put all of our imports at the top # but this lets us tell a story from sklearn.model_selection import train_test_split # split the data into train and test sets # this lets us simulate how our model will perform in the future X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = 0.33 ) N , D = X_train . shape # Scale the data # you'll learn why scaling is needed in a later course from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X_train = scaler . fit_transform ( X_train ) X_test = scaler . transform ( X_test ) # Now all the fun Tensorflow stuff # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Input ( shape = ( D ,)), tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) ]) # Alternatively, you can do: # model = tf.keras.models.Sequential() # model.add(tf.keras.layers.Dense(1, input_shape=(D,), activation='sigmoid')) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) # Train the model r = model . fit ( X_train , y_train , validation_data = ( X_test , y_test ), epochs = 100 ) # Evaluate the model - evaluate() returns loss and accuracy print ( \"Train score:\" , model . evaluate ( X_train , y_train )) print ( \"Test score:\" , model . evaluate ( X_test , y_test )) WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model. Epoch 1/100 12/12 [==============================] - 3s 17ms/step - loss: 0.7727 - accuracy: 0.4724 - val_loss: 0.6664 - val_accuracy: 0.5479 Epoch 2/100 12/12 [==============================] - 0s 5ms/step - loss: 0.7026 - accuracy: 0.5643 - val_loss: 0.6001 - val_accuracy: 0.6596 Epoch 3/100 12/12 [==============================] - 0s 4ms/step - loss: 0.6389 - accuracy: 0.6378 - val_loss: 0.5451 - val_accuracy: 0.7394 Epoch 4/100 12/12 [==============================] - 0s 5ms/step - loss: 0.5867 - accuracy: 0.6877 - val_loss: 0.4977 - val_accuracy: 0.7979 Epoch 5/100 12/12 [==============================] - 0s 4ms/step - loss: 0.5422 - accuracy: 0.7559 - val_loss: 0.4578 - val_accuracy: 0.8457 Epoch 6/100 12/12 [==============================] - 0s 6ms/step - loss: 0.5047 - accuracy: 0.7848 - val_loss: 0.4241 - val_accuracy: 0.8564 Epoch 7/100 12/12 [==============================] - 0s 5ms/step - loss: 0.4718 - accuracy: 0.8163 - val_loss: 0.3957 - val_accuracy: 0.8617 Epoch 8/100 12/12 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.8451 - val_loss: 0.3710 - val_accuracy: 0.8830 Epoch 9/100 12/12 [==============================] - 0s 6ms/step - loss: 0.4201 - accuracy: 0.8530 - val_loss: 0.3494 - val_accuracy: 0.8989 Epoch 10/100 12/12 [==============================] - 0s 6ms/step - loss: 0.3989 - accuracy: 0.8714 - val_loss: 0.3305 - val_accuracy: 0.9096 Epoch 11/100 12/12 [==============================] - 0s 4ms/step - loss: 0.3797 - accuracy: 0.8819 - val_loss: 0.3139 - val_accuracy: 0.9149 Epoch 12/100 12/12 [==============================] - 0s 4ms/step - loss: 0.3626 - accuracy: 0.8950 - val_loss: 0.2993 - val_accuracy: 0.9202 Epoch 13/100 12/12 [==============================] - 0s 5ms/step - loss: 0.3477 - accuracy: 0.9055 - val_loss: 0.2859 - val_accuracy: 0.9255 Epoch 14/100 12/12 [==============================] - 0s 4ms/step - loss: 0.3341 - accuracy: 0.9055 - val_loss: 0.2739 - val_accuracy: 0.9255 Epoch 15/100 12/12 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.9108 - val_loss: 0.2632 - val_accuracy: 0.9309 Epoch 16/100 12/12 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.9134 - val_loss: 0.2534 - val_accuracy: 0.9309 Epoch 17/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2992 - accuracy: 0.9160 - val_loss: 0.2446 - val_accuracy: 0.9362 Epoch 18/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2896 - accuracy: 0.9160 - val_loss: 0.2363 - val_accuracy: 0.9362 Epoch 19/100 12/12 [==============================] - 0s 5ms/step - loss: 0.2806 - accuracy: 0.9213 - val_loss: 0.2287 - val_accuracy: 0.9362 Epoch 20/100 12/12 [==============================] - 0s 5ms/step - loss: 0.2723 - accuracy: 0.9291 - val_loss: 0.2217 - val_accuracy: 0.9362 Epoch 21/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2645 - accuracy: 0.9291 - val_loss: 0.2151 - val_accuracy: 0.9362 Epoch 22/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2571 - accuracy: 0.9291 - val_loss: 0.2091 - val_accuracy: 0.9415 Epoch 23/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2505 - accuracy: 0.9291 - val_loss: 0.2033 - val_accuracy: 0.9468 Epoch 24/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2440 - accuracy: 0.9318 - val_loss: 0.1980 - val_accuracy: 0.9468 Epoch 25/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2380 - accuracy: 0.9344 - val_loss: 0.1931 - val_accuracy: 0.9468 Epoch 26/100 12/12 [==============================] - 0s 5ms/step - loss: 0.2323 - accuracy: 0.9423 - val_loss: 0.1884 - val_accuracy: 0.9468 Epoch 27/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2270 - accuracy: 0.9449 - val_loss: 0.1841 - val_accuracy: 0.9468 Epoch 28/100 12/12 [==============================] - 0s 6ms/step - loss: 0.2221 - accuracy: 0.9449 - val_loss: 0.1799 - val_accuracy: 0.9468 Epoch 29/100 12/12 [==============================] - 0s 6ms/step - loss: 0.2173 - accuracy: 0.9449 - val_loss: 0.1759 - val_accuracy: 0.9468 Epoch 30/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2127 - accuracy: 0.9475 - val_loss: 0.1722 - val_accuracy: 0.9468 Epoch 31/100 12/12 [==============================] - 0s 5ms/step - loss: 0.2083 - accuracy: 0.9501 - val_loss: 0.1688 - val_accuracy: 0.9468 Epoch 32/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.9528 - val_loss: 0.1655 - val_accuracy: 0.9521 Epoch 33/100 12/12 [==============================] - 0s 4ms/step - loss: 0.2006 - accuracy: 0.9528 - val_loss: 0.1622 - val_accuracy: 0.9521 Epoch 34/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1968 - accuracy: 0.9528 - val_loss: 0.1592 - val_accuracy: 0.9574 Epoch 35/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1932 - accuracy: 0.9528 - val_loss: 0.1564 - val_accuracy: 0.9574 Epoch 36/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1900 - accuracy: 0.9554 - val_loss: 0.1536 - val_accuracy: 0.9628 Epoch 37/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1867 - accuracy: 0.9554 - val_loss: 0.1510 - val_accuracy: 0.9681 Epoch 38/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9554 - val_loss: 0.1486 - val_accuracy: 0.9681 Epoch 39/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1807 - accuracy: 0.9554 - val_loss: 0.1462 - val_accuracy: 0.9681 Epoch 40/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1779 - accuracy: 0.9580 - val_loss: 0.1439 - val_accuracy: 0.9681 Epoch 41/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1752 - accuracy: 0.9580 - val_loss: 0.1418 - val_accuracy: 0.9681 Epoch 42/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1727 - accuracy: 0.9580 - val_loss: 0.1396 - val_accuracy: 0.9681 Epoch 43/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1702 - accuracy: 0.9580 - val_loss: 0.1376 - val_accuracy: 0.9681 Epoch 44/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1678 - accuracy: 0.9580 - val_loss: 0.1357 - val_accuracy: 0.9681 Epoch 45/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1655 - accuracy: 0.9580 - val_loss: 0.1338 - val_accuracy: 0.9681 Epoch 46/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1633 - accuracy: 0.9580 - val_loss: 0.1321 - val_accuracy: 0.9681 Epoch 47/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1613 - accuracy: 0.9580 - val_loss: 0.1304 - val_accuracy: 0.9681 Epoch 48/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1592 - accuracy: 0.9606 - val_loss: 0.1287 - val_accuracy: 0.9681 Epoch 49/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9606 - val_loss: 0.1272 - val_accuracy: 0.9681 Epoch 50/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9606 - val_loss: 0.1256 - val_accuracy: 0.9681 Epoch 51/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.9606 - val_loss: 0.1242 - val_accuracy: 0.9681 Epoch 52/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9606 - val_loss: 0.1227 - val_accuracy: 0.9681 Epoch 53/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9633 - val_loss: 0.1213 - val_accuracy: 0.9681 Epoch 54/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1484 - accuracy: 0.9633 - val_loss: 0.1199 - val_accuracy: 0.9681 Epoch 55/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1467 - accuracy: 0.9633 - val_loss: 0.1186 - val_accuracy: 0.9681 Epoch 56/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1453 - accuracy: 0.9633 - val_loss: 0.1174 - val_accuracy: 0.9734 Epoch 57/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.9633 - val_loss: 0.1162 - val_accuracy: 0.9734 Epoch 58/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1423 - accuracy: 0.9633 - val_loss: 0.1150 - val_accuracy: 0.9734 Epoch 59/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1408 - accuracy: 0.9659 - val_loss: 0.1139 - val_accuracy: 0.9734 Epoch 60/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1395 - accuracy: 0.9659 - val_loss: 0.1127 - val_accuracy: 0.9734 Epoch 61/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9659 - val_loss: 0.1117 - val_accuracy: 0.9734 Epoch 62/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1368 - accuracy: 0.9659 - val_loss: 0.1107 - val_accuracy: 0.9734 Epoch 63/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1356 - accuracy: 0.9659 - val_loss: 0.1096 - val_accuracy: 0.9734 Epoch 64/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 0.9659 - val_loss: 0.1086 - val_accuracy: 0.9734 Epoch 65/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1332 - accuracy: 0.9659 - val_loss: 0.1077 - val_accuracy: 0.9734 Epoch 66/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1320 - accuracy: 0.9659 - val_loss: 0.1067 - val_accuracy: 0.9734 Epoch 67/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1309 - accuracy: 0.9659 - val_loss: 0.1059 - val_accuracy: 0.9734 Epoch 68/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1299 - accuracy: 0.9659 - val_loss: 0.1050 - val_accuracy: 0.9734 Epoch 69/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9659 - val_loss: 0.1041 - val_accuracy: 0.9734 Epoch 70/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1277 - accuracy: 0.9685 - val_loss: 0.1033 - val_accuracy: 0.9734 Epoch 71/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1267 - accuracy: 0.9685 - val_loss: 0.1024 - val_accuracy: 0.9734 Epoch 72/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1258 - accuracy: 0.9711 - val_loss: 0.1016 - val_accuracy: 0.9734 Epoch 73/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1248 - accuracy: 0.9738 - val_loss: 0.1008 - val_accuracy: 0.9734 Epoch 74/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9738 - val_loss: 0.1001 - val_accuracy: 0.9734 Epoch 75/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1230 - accuracy: 0.9738 - val_loss: 0.0994 - val_accuracy: 0.9734 Epoch 76/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9738 - val_loss: 0.0987 - val_accuracy: 0.9734 Epoch 77/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1213 - accuracy: 0.9738 - val_loss: 0.0979 - val_accuracy: 0.9734 Epoch 78/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1204 - accuracy: 0.9738 - val_loss: 0.0972 - val_accuracy: 0.9734 Epoch 79/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1196 - accuracy: 0.9738 - val_loss: 0.0966 - val_accuracy: 0.9734 Epoch 80/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1188 - accuracy: 0.9738 - val_loss: 0.0959 - val_accuracy: 0.9787 Epoch 81/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1180 - accuracy: 0.9738 - val_loss: 0.0953 - val_accuracy: 0.9840 Epoch 82/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1173 - accuracy: 0.9738 - val_loss: 0.0947 - val_accuracy: 0.9840 Epoch 83/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1165 - accuracy: 0.9738 - val_loss: 0.0940 - val_accuracy: 0.9840 Epoch 84/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9764 - val_loss: 0.0934 - val_accuracy: 0.9840 Epoch 85/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1150 - accuracy: 0.9764 - val_loss: 0.0928 - val_accuracy: 0.9840 Epoch 86/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9764 - val_loss: 0.0923 - val_accuracy: 0.9840 Epoch 87/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9764 - val_loss: 0.0917 - val_accuracy: 0.9840 Epoch 88/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1130 - accuracy: 0.9764 - val_loss: 0.0912 - val_accuracy: 0.9840 Epoch 89/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1123 - accuracy: 0.9764 - val_loss: 0.0906 - val_accuracy: 0.9840 Epoch 90/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1117 - accuracy: 0.9764 - val_loss: 0.0900 - val_accuracy: 0.9840 Epoch 91/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9764 - val_loss: 0.0895 - val_accuracy: 0.9840 Epoch 92/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9764 - val_loss: 0.0890 - val_accuracy: 0.9840 Epoch 93/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1098 - accuracy: 0.9764 - val_loss: 0.0886 - val_accuracy: 0.9840 Epoch 94/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1093 - accuracy: 0.9790 - val_loss: 0.0881 - val_accuracy: 0.9840 Epoch 95/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1087 - accuracy: 0.9790 - val_loss: 0.0876 - val_accuracy: 0.9840 Epoch 96/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1081 - accuracy: 0.9790 - val_loss: 0.0871 - val_accuracy: 0.9840 Epoch 97/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9790 - val_loss: 0.0866 - val_accuracy: 0.9840 Epoch 98/100 12/12 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9790 - val_loss: 0.0862 - val_accuracy: 0.9840 Epoch 99/100 12/12 [==============================] - 0s 6ms/step - loss: 0.1065 - accuracy: 0.9790 - val_loss: 0.0858 - val_accuracy: 0.9840 Epoch 100/100 12/12 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.9790 - val_loss: 0.0853 - val_accuracy: 0.9840 12/12 [==============================] - 0s 2ms/step - loss: 0.1056 - accuracy: 0.9790 Train score: [0.10559554398059845, 0.9790025949478149] 6/6 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9840 Test score: [0.08529960364103317, 0.9840425252914429] # Plot what's returned by model.fit() import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f75702b9790> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f75701ec310>","title":"Linear Classification"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Classification/#part-2-making-predictions","text":"This goes with the lecture \u201cMaking Predictions\u201d # Make predictions P = model . predict ( X_test ) print ( P ) # they are outputs of the sigmoid, interpreted as probabilities p(y = 1 | x) [[9.35080469e-01] [9.99298692e-01] [9.98439729e-01] [9.77653563e-01] [5.64797297e-02] [1.55710494e-02] [6.10508978e-01] [8.52784812e-01] [9.31189537e-01] [9.98825729e-01] [8.09484124e-01] [9.13891383e-03] [9.96914744e-01] [9.93904889e-01] [9.81451333e-01] [9.55752373e-01] [2.45447876e-03] [4.61153733e-03] [5.73845464e-04] [2.48237103e-02] [9.90265667e-01] [3.78944203e-02] [9.94276583e-01] [9.98708129e-01] [9.61843669e-01] [9.87939239e-01] [9.99808848e-01] [9.96969759e-01] [2.85914261e-02] [9.56589937e-01] [9.60226953e-01] [2.69837608e-03] [9.96669829e-01] [9.84309673e-01] [3.50100487e-01] [8.31614912e-01] [9.95373189e-01] [2.96407286e-03] [9.67374027e-01] [9.73011911e-01] [9.94610310e-01] [4.85418539e-04] [8.88491213e-01] [9.84727025e-01] [8.90673800e-06] [9.98509824e-01] [4.42464858e-01] [1.32060540e-03] [1.17375106e-01] [8.27795267e-01] [3.74988168e-01] [1.91124864e-02] [9.95917022e-01] [9.98196542e-01] [4.74949419e-01] [8.22081923e-01] [8.90725613e-01] [9.98178959e-01] [1.02176724e-04] [1.82664804e-02] [9.50180948e-01] [1.43383248e-02] [9.42624748e-01] [5.96681166e-05] [9.99727428e-01] [9.69099402e-01] [2.24412568e-02] [3.96199212e-05] [3.38991098e-02] [9.91276979e-01] [9.91171777e-01] [9.82203007e-01] [6.71500042e-02] [9.87396777e-01] [5.30279765e-04] [9.57228303e-01] [9.98355210e-01] [9.14368153e-01] [9.52866495e-01] [9.97870207e-01] [1.34615432e-02] [2.44203299e-01] [8.79299402e-01] [9.77223754e-01] [9.28715587e-01] [2.52999127e-01] [2.69606203e-01] [9.99876618e-01] [1.51222537e-03] [9.53690171e-01] [4.03010815e-01] [9.98407781e-01] [6.50674924e-02] [9.99741018e-01] [1.14408717e-01] [8.39701653e-01] [9.96540427e-01] [8.24299932e-01] [1.20999947e-01] [9.98341918e-01] [9.98978138e-01] [9.87704277e-01] [9.43565965e-01] [9.89912093e-01] [9.82849538e-01] [1.31196016e-03] [4.07409249e-03] [6.24320284e-03] [7.39843130e-01] [9.90274429e-01] [9.96425331e-01] [4.27843165e-03] [9.28301930e-01] [9.49783742e-01] [9.80192959e-01] [8.80274863e-04] [9.91823375e-01] [9.76207674e-01] [9.17195439e-01] [9.98508751e-01] [9.27063286e-01] [6.56274676e-01] [9.99204099e-01] [9.82241213e-01] [9.71006870e-01] [9.63477552e-01] [9.98024106e-01] [9.99295473e-01] [9.87832904e-01] [3.22171450e-02] [8.78069401e-01] [4.87438217e-02] [9.59760249e-01] [1.59445778e-01] [4.66189593e-01] [9.82630789e-01] [9.93440092e-01] [2.06552606e-04] [9.97168481e-01] [9.90759015e-01] [9.95073020e-01] [9.30102646e-01] [2.89134146e-03] [9.88580644e-01] [9.95565474e-01] [9.84212875e-01] [2.23457310e-02] [9.83895898e-01] [6.18342310e-04] [9.98992264e-01] [9.94715989e-01] [3.41228060e-02] [2.80370237e-04] [4.87311219e-04] [9.97160077e-01] [8.87251318e-01] [9.65952098e-01] [9.75660741e-01] [9.93650138e-01] [4.88551438e-01] [2.07821256e-03] [7.25426733e-01] [6.17845356e-03] [4.90195416e-05] [8.92993286e-02] [8.22957635e-01] [9.97866690e-01] [2.30895057e-02] [9.76001143e-01] [6.94513857e-01] [6.79192960e-01] [9.77537930e-01] [9.90081012e-01] [9.98470843e-01] [9.93132412e-01] [2.66570807e-03] [9.49018478e-01] [9.90014315e-01] [2.29099810e-01] [9.10406351e-01] [9.98505592e-01] [9.42158282e-01] [9.24770832e-01] [9.11253691e-01] [9.84513938e-01] [9.97072697e-01] [7.51034081e-01] [9.97907519e-01]] # Round to get the actual predictions # Note: has to be flattened since the targets are size (N,) while the predictions are size (N,1) import numpy as np P = np . round ( P ) . flatten () print ( P ) [1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] # Calculate the accuracy, compare it to evaluate() output print ( \"Manually calculated accuracy:\" , np . mean ( P == y_test )) print ( \"Evaluate output:\" , model . evaluate ( X_test , y_test )) Manually calculated accuracy: 0.9840425531914894 6/6 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9840 Evaluate output: [0.08529960364103317, 0.9840425252914429]","title":"Part 2: Making Predictions"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Classification/#part-3-saving-and-loading-a-model","text":"This goes with the lecture \u201cSaving and Loading a Model\u201d # Let's now save our model to a file model . save ( 'linearclassifier.h5' ) # Check that the model file exists ! ls - lh total 24K -rw-r--r-- 1 root root 19K Jul 18 20:05 linearclassifier.h5 drwxr-xr-x 1 root root 4.0K Jul 15 13:38 sample_data # Let's load the model and confirm that it still works # Note: there is a bug in Keras where load/save only works if you DON'T use the Input() layer explicitly # So, make sure you define the model with ONLY Dense(1, input_shape=(D,)) # At least, until the bug is fixed # https://github.com/keras-team/keras/issues/10417 model = tf . keras . models . load_model ( 'linearclassifier.h5' ) print ( model . layers ) model . evaluate ( X_test , y_test ) [<tensorflow.python.keras.layers.core.Dense object at 0x7f7570102f50>] 6/6 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9840 [0.08529960364103317, 0.9840425252914429] # Download the file - requires Chrome (at this point) from google.colab import files files . download ( 'linearclassifier.h5' ) <IPython.core.display.Javascript object> <IPython.core.display.Javascript object> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ s","title":"Part 3: Saving and Loading a Model"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Regression/","text":"================ by Jawad Haider Linear Regression Part 2: Making Predictions Linear Regression \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.2.0 # Other imports import pandas as pd import numpy as np import matplotlib.pyplot as plt # Get the data ! wget https : // raw . githubusercontent . com / lazyprogrammer / machine_learning_examples / master / tf2 .0 / moore . csv --2020-06-18 16:14:29-- https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2302 (2.2K) [text/plain] Saving to: \u2018moore.csv\u2019 moore.csv 0%[ ] 0 --.-KB/s moore.csv 100%[===================>] 2.25K --.-KB/s in 0s 2020-06-18 16:14:29 (37.1 MB/s) - \u2018moore.csv\u2019 saved [2302/2302] # Load in the data data = pd . read_csv ( 'moore.csv' , header = None ) . values X = data [:, 0 ] . reshape ( - 1 , 1 ) # make it a 2-D array of size N x D where D = 1 Y = data [:, 1 ] # Plot the data - it is exponential! plt . scatter ( X , Y ) <matplotlib.collections.PathCollection at 0x7f0b9e493ac8> # Since we want a linear model, let's take the log Y = np . log ( Y ) plt . scatter ( X , Y ) # that's better <matplotlib.collections.PathCollection at 0x7f0b9b7eac18> # Let's also center the X data so the values are not too large # We could scale it too but then we'd have to reverse the transformation later X = X - X . mean () # Now create our Tensorflow model model = tf . keras . models . Sequential ([ tf . keras . layers . Input ( shape = ( 1 ,)), tf . keras . layers . Dense ( 1 ) ]) model . compile ( optimizer = tf . keras . optimizers . SGD ( 0.001 , 0.9 ), loss = 'mse' ) # model.compile(optimizer='adam', loss='mse') # learning rate scheduler def schedule ( epoch , lr ): if epoch >= 50 : return 0.0001 return 0.001 scheduler = tf . keras . callbacks . LearningRateScheduler ( schedule ) # Train the model r = model . fit ( X , Y , epochs = 200 , callbacks = [ scheduler ]) Epoch 1/200 6/6 [==============================] - 0s 2ms/step - loss: 564.7578 - lr: 0.0010 Epoch 2/200 6/6 [==============================] - 0s 1ms/step - loss: 416.5006 - lr: 0.0010 Epoch 3/200 6/6 [==============================] - 0s 1ms/step - loss: 435.8105 - lr: 0.0010 Epoch 4/200 6/6 [==============================] - 0s 1ms/step - loss: 291.5248 - lr: 0.0010 Epoch 5/200 6/6 [==============================] - 0s 1ms/step - loss: 309.7632 - lr: 0.0010 Epoch 6/200 6/6 [==============================] - 0s 1ms/step - loss: 173.2291 - lr: 0.0010 Epoch 7/200 6/6 [==============================] - 0s 1ms/step - loss: 122.9670 - lr: 0.0010 Epoch 8/200 6/6 [==============================] - 0s 1ms/step - loss: 88.1215 - lr: 0.0010 Epoch 9/200 6/6 [==============================] - 0s 1ms/step - loss: 51.1509 - lr: 0.0010 Epoch 10/200 6/6 [==============================] - 0s 1ms/step - loss: 39.1848 - lr: 0.0010 Epoch 11/200 6/6 [==============================] - 0s 1ms/step - loss: 50.0891 - lr: 0.0010 Epoch 12/200 6/6 [==============================] - 0s 1ms/step - loss: 57.7818 - lr: 0.0010 Epoch 13/200 6/6 [==============================] - 0s 1ms/step - loss: 46.9341 - lr: 0.0010 Epoch 14/200 6/6 [==============================] - 0s 1ms/step - loss: 43.3448 - lr: 0.0010 Epoch 15/200 6/6 [==============================] - 0s 1ms/step - loss: 28.4526 - lr: 0.0010 Epoch 16/200 6/6 [==============================] - 0s 1ms/step - loss: 15.1617 - lr: 0.0010 Epoch 17/200 6/6 [==============================] - 0s 2ms/step - loss: 6.5347 - lr: 0.0010 Epoch 18/200 6/6 [==============================] - 0s 2ms/step - loss: 5.3453 - lr: 0.0010 Epoch 19/200 6/6 [==============================] - 0s 2ms/step - loss: 13.6583 - lr: 0.0010 Epoch 20/200 6/6 [==============================] - 0s 1ms/step - loss: 13.0628 - lr: 0.0010 Epoch 21/200 6/6 [==============================] - 0s 2ms/step - loss: 6.2095 - lr: 0.0010 Epoch 22/200 6/6 [==============================] - 0s 1ms/step - loss: 2.2462 - lr: 0.0010 Epoch 23/200 6/6 [==============================] - 0s 1ms/step - loss: 1.5828 - lr: 0.0010 Epoch 24/200 6/6 [==============================] - 0s 1ms/step - loss: 1.2626 - lr: 0.0010 Epoch 25/200 6/6 [==============================] - 0s 1ms/step - loss: 1.1254 - lr: 0.0010 Epoch 26/200 6/6 [==============================] - 0s 2ms/step - loss: 1.4200 - lr: 0.0010 Epoch 27/200 6/6 [==============================] - 0s 1ms/step - loss: 1.8661 - lr: 0.0010 Epoch 28/200 6/6 [==============================] - 0s 2ms/step - loss: 1.3886 - lr: 0.0010 Epoch 29/200 6/6 [==============================] - 0s 2ms/step - loss: 1.0745 - lr: 0.0010 Epoch 30/200 6/6 [==============================] - 0s 2ms/step - loss: 1.0018 - lr: 0.0010 Epoch 31/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9356 - lr: 0.0010 Epoch 32/200 6/6 [==============================] - 0s 1ms/step - loss: 1.0224 - lr: 0.0010 Epoch 33/200 6/6 [==============================] - 0s 1ms/step - loss: 1.1923 - lr: 0.0010 Epoch 34/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9860 - lr: 0.0010 Epoch 35/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9608 - lr: 0.0010 Epoch 36/200 6/6 [==============================] - 0s 1ms/step - loss: 1.0304 - lr: 0.0010 Epoch 37/200 6/6 [==============================] - 0s 2ms/step - loss: 1.2601 - lr: 0.0010 Epoch 38/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9664 - lr: 0.0010 Epoch 39/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9088 - lr: 0.0010 Epoch 40/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9199 - lr: 0.0010 Epoch 41/200 6/6 [==============================] - 0s 2ms/step - loss: 1.0986 - lr: 0.0010 Epoch 42/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9465 - lr: 0.0010 Epoch 43/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9290 - lr: 0.0010 Epoch 44/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9168 - lr: 0.0010 Epoch 45/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8770 - lr: 0.0010 Epoch 46/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9378 - lr: 0.0010 Epoch 47/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9742 - lr: 0.0010 Epoch 48/200 6/6 [==============================] - 0s 1ms/step - loss: 1.0062 - lr: 0.0010 Epoch 49/200 6/6 [==============================] - 0s 2ms/step - loss: 1.0642 - lr: 0.0010 Epoch 50/200 6/6 [==============================] - 0s 1ms/step - loss: 1.2359 - lr: 0.0010 Epoch 51/200 6/6 [==============================] - 0s 1ms/step - loss: 1.5608 - lr: 1.0000e-04 Epoch 52/200 6/6 [==============================] - 0s 1ms/step - loss: 1.4480 - lr: 1.0000e-04 Epoch 53/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9116 - lr: 1.0000e-04 Epoch 54/200 6/6 [==============================] - 0s 1ms/step - loss: 1.0805 - lr: 1.0000e-04 Epoch 55/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9856 - lr: 1.0000e-04 Epoch 56/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8962 - lr: 1.0000e-04 Epoch 57/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8972 - lr: 1.0000e-04 Epoch 58/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8909 - lr: 1.0000e-04 Epoch 59/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9246 - lr: 1.0000e-04 Epoch 60/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9195 - lr: 1.0000e-04 Epoch 61/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8844 - lr: 1.0000e-04 Epoch 62/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8750 - lr: 1.0000e-04 Epoch 63/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8757 - lr: 1.0000e-04 Epoch 64/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8778 - lr: 1.0000e-04 Epoch 65/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8765 - lr: 1.0000e-04 Epoch 66/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8757 - lr: 1.0000e-04 Epoch 67/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8775 - lr: 1.0000e-04 Epoch 68/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8722 - lr: 1.0000e-04 Epoch 69/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8820 - lr: 1.0000e-04 Epoch 70/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8755 - lr: 1.0000e-04 Epoch 71/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8759 - lr: 1.0000e-04 Epoch 72/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8742 - lr: 1.0000e-04 Epoch 73/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8739 - lr: 1.0000e-04 Epoch 74/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8822 - lr: 1.0000e-04 Epoch 75/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8789 - lr: 1.0000e-04 Epoch 76/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8737 - lr: 1.0000e-04 Epoch 77/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8740 - lr: 1.0000e-04 Epoch 78/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8715 - lr: 1.0000e-04 Epoch 79/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8746 - lr: 1.0000e-04 Epoch 80/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8734 - lr: 1.0000e-04 Epoch 81/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8754 - lr: 1.0000e-04 Epoch 82/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9202 - lr: 1.0000e-04 Epoch 83/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9127 - lr: 1.0000e-04 Epoch 84/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8801 - lr: 1.0000e-04 Epoch 85/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9101 - lr: 1.0000e-04 Epoch 86/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8975 - lr: 1.0000e-04 Epoch 87/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8751 - lr: 1.0000e-04 Epoch 88/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9003 - lr: 1.0000e-04 Epoch 89/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8835 - lr: 1.0000e-04 Epoch 90/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8847 - lr: 1.0000e-04 Epoch 91/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8835 - lr: 1.0000e-04 Epoch 92/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8738 - lr: 1.0000e-04 Epoch 93/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8766 - lr: 1.0000e-04 Epoch 94/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8879 - lr: 1.0000e-04 Epoch 95/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8782 - lr: 1.0000e-04 Epoch 96/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8820 - lr: 1.0000e-04 Epoch 97/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8783 - lr: 1.0000e-04 Epoch 98/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8732 - lr: 1.0000e-04 Epoch 99/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8792 - lr: 1.0000e-04 Epoch 100/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8876 - lr: 1.0000e-04 Epoch 101/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8766 - lr: 1.0000e-04 Epoch 102/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8823 - lr: 1.0000e-04 Epoch 103/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8881 - lr: 1.0000e-04 Epoch 104/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8811 - lr: 1.0000e-04 Epoch 105/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8718 - lr: 1.0000e-04 Epoch 106/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8732 - lr: 1.0000e-04 Epoch 107/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8719 - lr: 1.0000e-04 Epoch 108/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8730 - lr: 1.0000e-04 Epoch 109/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8898 - lr: 1.0000e-04 Epoch 110/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8772 - lr: 1.0000e-04 Epoch 111/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8751 - lr: 1.0000e-04 Epoch 112/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8814 - lr: 1.0000e-04 Epoch 113/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8745 - lr: 1.0000e-04 Epoch 114/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8742 - lr: 1.0000e-04 Epoch 115/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8779 - lr: 1.0000e-04 Epoch 116/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8807 - lr: 1.0000e-04 Epoch 117/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8740 - lr: 1.0000e-04 Epoch 118/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8724 - lr: 1.0000e-04 Epoch 119/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8773 - lr: 1.0000e-04 Epoch 120/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8761 - lr: 1.0000e-04 Epoch 121/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8736 - lr: 1.0000e-04 Epoch 122/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8712 - lr: 1.0000e-04 Epoch 123/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8758 - lr: 1.0000e-04 Epoch 124/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8742 - lr: 1.0000e-04 Epoch 125/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8714 - lr: 1.0000e-04 Epoch 126/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8715 - lr: 1.0000e-04 Epoch 127/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8725 - lr: 1.0000e-04 Epoch 128/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8748 - lr: 1.0000e-04 Epoch 129/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8769 - lr: 1.0000e-04 Epoch 130/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8706 - lr: 1.0000e-04 Epoch 131/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8716 - lr: 1.0000e-04 Epoch 132/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8730 - lr: 1.0000e-04 Epoch 133/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8739 - lr: 1.0000e-04 Epoch 134/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8878 - lr: 1.0000e-04 Epoch 135/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8825 - lr: 1.0000e-04 Epoch 136/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8833 - lr: 1.0000e-04 Epoch 137/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9046 - lr: 1.0000e-04 Epoch 138/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8770 - lr: 1.0000e-04 Epoch 139/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8811 - lr: 1.0000e-04 Epoch 140/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8759 - lr: 1.0000e-04 Epoch 141/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8711 - lr: 1.0000e-04 Epoch 142/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8715 - lr: 1.0000e-04 Epoch 143/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8749 - lr: 1.0000e-04 Epoch 144/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8770 - lr: 1.0000e-04 Epoch 145/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8843 - lr: 1.0000e-04 Epoch 146/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8729 - lr: 1.0000e-04 Epoch 147/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8829 - lr: 1.0000e-04 Epoch 148/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8759 - lr: 1.0000e-04 Epoch 149/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8953 - lr: 1.0000e-04 Epoch 150/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9202 - lr: 1.0000e-04 Epoch 151/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8893 - lr: 1.0000e-04 Epoch 152/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8754 - lr: 1.0000e-04 Epoch 153/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8883 - lr: 1.0000e-04 Epoch 154/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8810 - lr: 1.0000e-04 Epoch 155/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9087 - lr: 1.0000e-04 Epoch 156/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8908 - lr: 1.0000e-04 Epoch 157/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8742 - lr: 1.0000e-04 Epoch 158/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8722 - lr: 1.0000e-04 Epoch 159/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8697 - lr: 1.0000e-04 Epoch 160/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8768 - lr: 1.0000e-04 Epoch 161/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8773 - lr: 1.0000e-04 Epoch 162/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8770 - lr: 1.0000e-04 Epoch 163/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9014 - lr: 1.0000e-04 Epoch 164/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9614 - lr: 1.0000e-04 Epoch 165/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8981 - lr: 1.0000e-04 Epoch 166/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8774 - lr: 1.0000e-04 Epoch 167/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8831 - lr: 1.0000e-04 Epoch 168/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8719 - lr: 1.0000e-04 Epoch 169/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8717 - lr: 1.0000e-04 Epoch 170/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8751 - lr: 1.0000e-04 Epoch 171/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8804 - lr: 1.0000e-04 Epoch 172/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8728 - lr: 1.0000e-04 Epoch 173/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8745 - lr: 1.0000e-04 Epoch 174/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8770 - lr: 1.0000e-04 Epoch 175/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8736 - lr: 1.0000e-04 Epoch 176/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8843 - lr: 1.0000e-04 Epoch 177/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8807 - lr: 1.0000e-04 Epoch 178/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8691 - lr: 1.0000e-04 Epoch 179/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9173 - lr: 1.0000e-04 Epoch 180/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9064 - lr: 1.0000e-04 Epoch 181/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8801 - lr: 1.0000e-04 Epoch 182/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8868 - lr: 1.0000e-04 Epoch 183/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8794 - lr: 1.0000e-04 Epoch 184/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8743 - lr: 1.0000e-04 Epoch 185/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8744 - lr: 1.0000e-04 Epoch 186/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8731 - lr: 1.0000e-04 Epoch 187/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8709 - lr: 1.0000e-04 Epoch 188/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8752 - lr: 1.0000e-04 Epoch 189/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8969 - lr: 1.0000e-04 Epoch 190/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8766 - lr: 1.0000e-04 Epoch 191/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8744 - lr: 1.0000e-04 Epoch 192/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8772 - lr: 1.0000e-04 Epoch 193/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9139 - lr: 1.0000e-04 Epoch 194/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9380 - lr: 1.0000e-04 Epoch 195/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8694 - lr: 1.0000e-04 Epoch 196/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8820 - lr: 1.0000e-04 Epoch 197/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8696 - lr: 1.0000e-04 Epoch 198/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8744 - lr: 1.0000e-04 Epoch 199/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9066 - lr: 1.0000e-04 Epoch 200/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8892 - lr: 1.0000e-04 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) # Get the slope of the line # The slope of the line is related to the doubling rate of transistor count print ( model . layers ) # Note: there is only 1 layer, the \"Input\" layer doesn't count print ( model . layers [ 0 ] . get_weights ()) [<tensorflow.python.keras.layers.core.Dense object at 0x7f0b9e97eda0>] [array([[0.3362535]], dtype=float32), array([17.74322], dtype=float32)] # The slope of the line is: a = model . layers [ 0 ] . get_weights ()[ 0 ][ 0 , 0 ] Our original model for exponential growth is: \\[ C = A_0 r^t \\] Where $ C $ is transistor the count and $ t $ is the year. $ r $ is the rate of growth. For example, when $ t $ goes from 1 to 2, $ C $ increases by a factor of $ r $. When $ t $ goes from 2 to 3, $ C $ increases by a factor of $ r $ again. When we take the log of both sides, we get: \\[ \\log C = \\log r * t + \\log A_0 \\] This is our linear equation: \\[ \\hat{y} = ax + b \\] Where: \\[ \\hat{y} = \\log C $$ $$ a = \\log r $$ $$ x = t $$ $$ b = \\log A_0 \\] We are interested in $ r $, because that\u2019s the rate of growth. Given our regression weights, we know that: \\[ a = 0.34188038 \\] so that: \\[ r = e^{0.34188038} = 1.4076 \\] To find the time it takes for transistor count to double, we simply need to find the amount of time it takes for $ C $ to increase to $ 2C $. Let\u2019s call the original starting time $ t $, to correspond with the initial transistor count $ C $. Let\u2019s call the end time $ t\u2019 $, to correspond with the final transistor count $ 2C $. Then we also have: \\[ 2C = A_0 r ^ {t'} \\] Combine this with our original equation: \\[ C = A_0 r^t \\] We get (by dividing the 2 equations): \\[ 2C/C = (A_0 r ^ {t'}) / A_0 r^t \\] Which simplifies to: \\[ 2 = r^{(t' - t)} \\] Solve for $ t\u2019 - t $: \\[ t' - t = \\frac{\\log 2}{\\log r} = \\frac{\\log2}{a}\\] Important note! We haven\u2019t specified what the starting time $ t $ actually is, and we don\u2019t have to since we just proved that this holds for any $ t $. print ( \"Time to double:\" , np . log ( 2 ) / a ) Time to double: 2.0613828343145832 # If you know the analytical solution X = np . array ( X ) . flatten () Y = np . array ( Y ) denominator = X . dot ( X ) - X . mean () * X . sum () a = ( X . dot ( Y ) - Y . mean () * X . sum () ) / denominator b = ( Y . mean () * X . dot ( X ) - X . mean () * X . dot ( Y ) ) / denominator print ( a , b ) print ( \"Time to double:\" , np . log ( 2 ) / a ) 0.341682487387313 17.764939393631764 Time to double: 2.0286295205239204 Part 2: Making Predictions \u00b6 This goes with the lecture \u201cMaking Predictions\u201d # Make sure the line fits our data Yhat = model . predict ( X ) . flatten () plt . scatter ( X , Y ) plt . plot ( X , Yhat ) # Manual calculation # Get the weights w , b = model . layers [ 0 ] . get_weights () # Reshape X because we flattened it again earlier X = X . reshape ( - 1 , 1 ) # (N x 1) x (1 x 1) + (1) --> (N x 1) Yhat2 = ( X . dot ( w ) + b ) . flatten () # Don't use == for floating points np . allclose ( Yhat , Yhat2 ) True Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Linear Regression"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Regression/#linear-regression","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.2.0 # Other imports import pandas as pd import numpy as np import matplotlib.pyplot as plt # Get the data ! wget https : // raw . githubusercontent . com / lazyprogrammer / machine_learning_examples / master / tf2 .0 / moore . csv --2020-06-18 16:14:29-- https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2302 (2.2K) [text/plain] Saving to: \u2018moore.csv\u2019 moore.csv 0%[ ] 0 --.-KB/s moore.csv 100%[===================>] 2.25K --.-KB/s in 0s 2020-06-18 16:14:29 (37.1 MB/s) - \u2018moore.csv\u2019 saved [2302/2302] # Load in the data data = pd . read_csv ( 'moore.csv' , header = None ) . values X = data [:, 0 ] . reshape ( - 1 , 1 ) # make it a 2-D array of size N x D where D = 1 Y = data [:, 1 ] # Plot the data - it is exponential! plt . scatter ( X , Y ) <matplotlib.collections.PathCollection at 0x7f0b9e493ac8> # Since we want a linear model, let's take the log Y = np . log ( Y ) plt . scatter ( X , Y ) # that's better <matplotlib.collections.PathCollection at 0x7f0b9b7eac18> # Let's also center the X data so the values are not too large # We could scale it too but then we'd have to reverse the transformation later X = X - X . mean () # Now create our Tensorflow model model = tf . keras . models . Sequential ([ tf . keras . layers . Input ( shape = ( 1 ,)), tf . keras . layers . Dense ( 1 ) ]) model . compile ( optimizer = tf . keras . optimizers . SGD ( 0.001 , 0.9 ), loss = 'mse' ) # model.compile(optimizer='adam', loss='mse') # learning rate scheduler def schedule ( epoch , lr ): if epoch >= 50 : return 0.0001 return 0.001 scheduler = tf . keras . callbacks . LearningRateScheduler ( schedule ) # Train the model r = model . fit ( X , Y , epochs = 200 , callbacks = [ scheduler ]) Epoch 1/200 6/6 [==============================] - 0s 2ms/step - loss: 564.7578 - lr: 0.0010 Epoch 2/200 6/6 [==============================] - 0s 1ms/step - loss: 416.5006 - lr: 0.0010 Epoch 3/200 6/6 [==============================] - 0s 1ms/step - loss: 435.8105 - lr: 0.0010 Epoch 4/200 6/6 [==============================] - 0s 1ms/step - loss: 291.5248 - lr: 0.0010 Epoch 5/200 6/6 [==============================] - 0s 1ms/step - loss: 309.7632 - lr: 0.0010 Epoch 6/200 6/6 [==============================] - 0s 1ms/step - loss: 173.2291 - lr: 0.0010 Epoch 7/200 6/6 [==============================] - 0s 1ms/step - loss: 122.9670 - lr: 0.0010 Epoch 8/200 6/6 [==============================] - 0s 1ms/step - loss: 88.1215 - lr: 0.0010 Epoch 9/200 6/6 [==============================] - 0s 1ms/step - loss: 51.1509 - lr: 0.0010 Epoch 10/200 6/6 [==============================] - 0s 1ms/step - loss: 39.1848 - lr: 0.0010 Epoch 11/200 6/6 [==============================] - 0s 1ms/step - loss: 50.0891 - lr: 0.0010 Epoch 12/200 6/6 [==============================] - 0s 1ms/step - loss: 57.7818 - lr: 0.0010 Epoch 13/200 6/6 [==============================] - 0s 1ms/step - loss: 46.9341 - lr: 0.0010 Epoch 14/200 6/6 [==============================] - 0s 1ms/step - loss: 43.3448 - lr: 0.0010 Epoch 15/200 6/6 [==============================] - 0s 1ms/step - loss: 28.4526 - lr: 0.0010 Epoch 16/200 6/6 [==============================] - 0s 1ms/step - loss: 15.1617 - lr: 0.0010 Epoch 17/200 6/6 [==============================] - 0s 2ms/step - loss: 6.5347 - lr: 0.0010 Epoch 18/200 6/6 [==============================] - 0s 2ms/step - loss: 5.3453 - lr: 0.0010 Epoch 19/200 6/6 [==============================] - 0s 2ms/step - loss: 13.6583 - lr: 0.0010 Epoch 20/200 6/6 [==============================] - 0s 1ms/step - loss: 13.0628 - lr: 0.0010 Epoch 21/200 6/6 [==============================] - 0s 2ms/step - loss: 6.2095 - lr: 0.0010 Epoch 22/200 6/6 [==============================] - 0s 1ms/step - loss: 2.2462 - lr: 0.0010 Epoch 23/200 6/6 [==============================] - 0s 1ms/step - loss: 1.5828 - lr: 0.0010 Epoch 24/200 6/6 [==============================] - 0s 1ms/step - loss: 1.2626 - lr: 0.0010 Epoch 25/200 6/6 [==============================] - 0s 1ms/step - loss: 1.1254 - lr: 0.0010 Epoch 26/200 6/6 [==============================] - 0s 2ms/step - loss: 1.4200 - lr: 0.0010 Epoch 27/200 6/6 [==============================] - 0s 1ms/step - loss: 1.8661 - lr: 0.0010 Epoch 28/200 6/6 [==============================] - 0s 2ms/step - loss: 1.3886 - lr: 0.0010 Epoch 29/200 6/6 [==============================] - 0s 2ms/step - loss: 1.0745 - lr: 0.0010 Epoch 30/200 6/6 [==============================] - 0s 2ms/step - loss: 1.0018 - lr: 0.0010 Epoch 31/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9356 - lr: 0.0010 Epoch 32/200 6/6 [==============================] - 0s 1ms/step - loss: 1.0224 - lr: 0.0010 Epoch 33/200 6/6 [==============================] - 0s 1ms/step - loss: 1.1923 - lr: 0.0010 Epoch 34/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9860 - lr: 0.0010 Epoch 35/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9608 - lr: 0.0010 Epoch 36/200 6/6 [==============================] - 0s 1ms/step - loss: 1.0304 - lr: 0.0010 Epoch 37/200 6/6 [==============================] - 0s 2ms/step - loss: 1.2601 - lr: 0.0010 Epoch 38/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9664 - lr: 0.0010 Epoch 39/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9088 - lr: 0.0010 Epoch 40/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9199 - lr: 0.0010 Epoch 41/200 6/6 [==============================] - 0s 2ms/step - loss: 1.0986 - lr: 0.0010 Epoch 42/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9465 - lr: 0.0010 Epoch 43/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9290 - lr: 0.0010 Epoch 44/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9168 - lr: 0.0010 Epoch 45/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8770 - lr: 0.0010 Epoch 46/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9378 - lr: 0.0010 Epoch 47/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9742 - lr: 0.0010 Epoch 48/200 6/6 [==============================] - 0s 1ms/step - loss: 1.0062 - lr: 0.0010 Epoch 49/200 6/6 [==============================] - 0s 2ms/step - loss: 1.0642 - lr: 0.0010 Epoch 50/200 6/6 [==============================] - 0s 1ms/step - loss: 1.2359 - lr: 0.0010 Epoch 51/200 6/6 [==============================] - 0s 1ms/step - loss: 1.5608 - lr: 1.0000e-04 Epoch 52/200 6/6 [==============================] - 0s 1ms/step - loss: 1.4480 - lr: 1.0000e-04 Epoch 53/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9116 - lr: 1.0000e-04 Epoch 54/200 6/6 [==============================] - 0s 1ms/step - loss: 1.0805 - lr: 1.0000e-04 Epoch 55/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9856 - lr: 1.0000e-04 Epoch 56/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8962 - lr: 1.0000e-04 Epoch 57/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8972 - lr: 1.0000e-04 Epoch 58/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8909 - lr: 1.0000e-04 Epoch 59/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9246 - lr: 1.0000e-04 Epoch 60/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9195 - lr: 1.0000e-04 Epoch 61/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8844 - lr: 1.0000e-04 Epoch 62/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8750 - lr: 1.0000e-04 Epoch 63/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8757 - lr: 1.0000e-04 Epoch 64/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8778 - lr: 1.0000e-04 Epoch 65/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8765 - lr: 1.0000e-04 Epoch 66/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8757 - lr: 1.0000e-04 Epoch 67/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8775 - lr: 1.0000e-04 Epoch 68/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8722 - lr: 1.0000e-04 Epoch 69/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8820 - lr: 1.0000e-04 Epoch 70/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8755 - lr: 1.0000e-04 Epoch 71/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8759 - lr: 1.0000e-04 Epoch 72/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8742 - lr: 1.0000e-04 Epoch 73/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8739 - lr: 1.0000e-04 Epoch 74/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8822 - lr: 1.0000e-04 Epoch 75/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8789 - lr: 1.0000e-04 Epoch 76/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8737 - lr: 1.0000e-04 Epoch 77/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8740 - lr: 1.0000e-04 Epoch 78/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8715 - lr: 1.0000e-04 Epoch 79/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8746 - lr: 1.0000e-04 Epoch 80/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8734 - lr: 1.0000e-04 Epoch 81/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8754 - lr: 1.0000e-04 Epoch 82/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9202 - lr: 1.0000e-04 Epoch 83/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9127 - lr: 1.0000e-04 Epoch 84/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8801 - lr: 1.0000e-04 Epoch 85/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9101 - lr: 1.0000e-04 Epoch 86/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8975 - lr: 1.0000e-04 Epoch 87/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8751 - lr: 1.0000e-04 Epoch 88/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9003 - lr: 1.0000e-04 Epoch 89/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8835 - lr: 1.0000e-04 Epoch 90/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8847 - lr: 1.0000e-04 Epoch 91/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8835 - lr: 1.0000e-04 Epoch 92/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8738 - lr: 1.0000e-04 Epoch 93/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8766 - lr: 1.0000e-04 Epoch 94/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8879 - lr: 1.0000e-04 Epoch 95/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8782 - lr: 1.0000e-04 Epoch 96/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8820 - lr: 1.0000e-04 Epoch 97/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8783 - lr: 1.0000e-04 Epoch 98/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8732 - lr: 1.0000e-04 Epoch 99/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8792 - lr: 1.0000e-04 Epoch 100/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8876 - lr: 1.0000e-04 Epoch 101/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8766 - lr: 1.0000e-04 Epoch 102/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8823 - lr: 1.0000e-04 Epoch 103/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8881 - lr: 1.0000e-04 Epoch 104/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8811 - lr: 1.0000e-04 Epoch 105/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8718 - lr: 1.0000e-04 Epoch 106/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8732 - lr: 1.0000e-04 Epoch 107/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8719 - lr: 1.0000e-04 Epoch 108/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8730 - lr: 1.0000e-04 Epoch 109/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8898 - lr: 1.0000e-04 Epoch 110/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8772 - lr: 1.0000e-04 Epoch 111/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8751 - lr: 1.0000e-04 Epoch 112/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8814 - lr: 1.0000e-04 Epoch 113/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8745 - lr: 1.0000e-04 Epoch 114/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8742 - lr: 1.0000e-04 Epoch 115/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8779 - lr: 1.0000e-04 Epoch 116/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8807 - lr: 1.0000e-04 Epoch 117/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8740 - lr: 1.0000e-04 Epoch 118/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8724 - lr: 1.0000e-04 Epoch 119/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8773 - lr: 1.0000e-04 Epoch 120/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8761 - lr: 1.0000e-04 Epoch 121/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8736 - lr: 1.0000e-04 Epoch 122/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8712 - lr: 1.0000e-04 Epoch 123/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8758 - lr: 1.0000e-04 Epoch 124/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8742 - lr: 1.0000e-04 Epoch 125/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8714 - lr: 1.0000e-04 Epoch 126/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8715 - lr: 1.0000e-04 Epoch 127/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8725 - lr: 1.0000e-04 Epoch 128/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8748 - lr: 1.0000e-04 Epoch 129/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8769 - lr: 1.0000e-04 Epoch 130/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8706 - lr: 1.0000e-04 Epoch 131/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8716 - lr: 1.0000e-04 Epoch 132/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8730 - lr: 1.0000e-04 Epoch 133/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8739 - lr: 1.0000e-04 Epoch 134/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8878 - lr: 1.0000e-04 Epoch 135/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8825 - lr: 1.0000e-04 Epoch 136/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8833 - lr: 1.0000e-04 Epoch 137/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9046 - lr: 1.0000e-04 Epoch 138/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8770 - lr: 1.0000e-04 Epoch 139/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8811 - lr: 1.0000e-04 Epoch 140/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8759 - lr: 1.0000e-04 Epoch 141/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8711 - lr: 1.0000e-04 Epoch 142/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8715 - lr: 1.0000e-04 Epoch 143/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8749 - lr: 1.0000e-04 Epoch 144/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8770 - lr: 1.0000e-04 Epoch 145/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8843 - lr: 1.0000e-04 Epoch 146/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8729 - lr: 1.0000e-04 Epoch 147/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8829 - lr: 1.0000e-04 Epoch 148/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8759 - lr: 1.0000e-04 Epoch 149/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8953 - lr: 1.0000e-04 Epoch 150/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9202 - lr: 1.0000e-04 Epoch 151/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8893 - lr: 1.0000e-04 Epoch 152/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8754 - lr: 1.0000e-04 Epoch 153/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8883 - lr: 1.0000e-04 Epoch 154/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8810 - lr: 1.0000e-04 Epoch 155/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9087 - lr: 1.0000e-04 Epoch 156/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8908 - lr: 1.0000e-04 Epoch 157/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8742 - lr: 1.0000e-04 Epoch 158/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8722 - lr: 1.0000e-04 Epoch 159/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8697 - lr: 1.0000e-04 Epoch 160/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8768 - lr: 1.0000e-04 Epoch 161/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8773 - lr: 1.0000e-04 Epoch 162/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8770 - lr: 1.0000e-04 Epoch 163/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9014 - lr: 1.0000e-04 Epoch 164/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9614 - lr: 1.0000e-04 Epoch 165/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8981 - lr: 1.0000e-04 Epoch 166/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8774 - lr: 1.0000e-04 Epoch 167/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8831 - lr: 1.0000e-04 Epoch 168/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8719 - lr: 1.0000e-04 Epoch 169/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8717 - lr: 1.0000e-04 Epoch 170/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8751 - lr: 1.0000e-04 Epoch 171/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8804 - lr: 1.0000e-04 Epoch 172/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8728 - lr: 1.0000e-04 Epoch 173/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8745 - lr: 1.0000e-04 Epoch 174/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8770 - lr: 1.0000e-04 Epoch 175/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8736 - lr: 1.0000e-04 Epoch 176/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8843 - lr: 1.0000e-04 Epoch 177/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8807 - lr: 1.0000e-04 Epoch 178/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8691 - lr: 1.0000e-04 Epoch 179/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9173 - lr: 1.0000e-04 Epoch 180/200 6/6 [==============================] - 0s 2ms/step - loss: 0.9064 - lr: 1.0000e-04 Epoch 181/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8801 - lr: 1.0000e-04 Epoch 182/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8868 - lr: 1.0000e-04 Epoch 183/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8794 - lr: 1.0000e-04 Epoch 184/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8743 - lr: 1.0000e-04 Epoch 185/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8744 - lr: 1.0000e-04 Epoch 186/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8731 - lr: 1.0000e-04 Epoch 187/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8709 - lr: 1.0000e-04 Epoch 188/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8752 - lr: 1.0000e-04 Epoch 189/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8969 - lr: 1.0000e-04 Epoch 190/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8766 - lr: 1.0000e-04 Epoch 191/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8744 - lr: 1.0000e-04 Epoch 192/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8772 - lr: 1.0000e-04 Epoch 193/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9139 - lr: 1.0000e-04 Epoch 194/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9380 - lr: 1.0000e-04 Epoch 195/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8694 - lr: 1.0000e-04 Epoch 196/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8820 - lr: 1.0000e-04 Epoch 197/200 6/6 [==============================] - 0s 1ms/step - loss: 0.8696 - lr: 1.0000e-04 Epoch 198/200 6/6 [==============================] - 0s 2ms/step - loss: 0.8744 - lr: 1.0000e-04 Epoch 199/200 6/6 [==============================] - 0s 1ms/step - loss: 0.9066 - lr: 1.0000e-04 Epoch 200/200 6/6 [==============================] - 0s 3ms/step - loss: 0.8892 - lr: 1.0000e-04 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) # Get the slope of the line # The slope of the line is related to the doubling rate of transistor count print ( model . layers ) # Note: there is only 1 layer, the \"Input\" layer doesn't count print ( model . layers [ 0 ] . get_weights ()) [<tensorflow.python.keras.layers.core.Dense object at 0x7f0b9e97eda0>] [array([[0.3362535]], dtype=float32), array([17.74322], dtype=float32)] # The slope of the line is: a = model . layers [ 0 ] . get_weights ()[ 0 ][ 0 , 0 ] Our original model for exponential growth is: \\[ C = A_0 r^t \\] Where $ C $ is transistor the count and $ t $ is the year. $ r $ is the rate of growth. For example, when $ t $ goes from 1 to 2, $ C $ increases by a factor of $ r $. When $ t $ goes from 2 to 3, $ C $ increases by a factor of $ r $ again. When we take the log of both sides, we get: \\[ \\log C = \\log r * t + \\log A_0 \\] This is our linear equation: \\[ \\hat{y} = ax + b \\] Where: \\[ \\hat{y} = \\log C $$ $$ a = \\log r $$ $$ x = t $$ $$ b = \\log A_0 \\] We are interested in $ r $, because that\u2019s the rate of growth. Given our regression weights, we know that: \\[ a = 0.34188038 \\] so that: \\[ r = e^{0.34188038} = 1.4076 \\] To find the time it takes for transistor count to double, we simply need to find the amount of time it takes for $ C $ to increase to $ 2C $. Let\u2019s call the original starting time $ t $, to correspond with the initial transistor count $ C $. Let\u2019s call the end time $ t\u2019 $, to correspond with the final transistor count $ 2C $. Then we also have: \\[ 2C = A_0 r ^ {t'} \\] Combine this with our original equation: \\[ C = A_0 r^t \\] We get (by dividing the 2 equations): \\[ 2C/C = (A_0 r ^ {t'}) / A_0 r^t \\] Which simplifies to: \\[ 2 = r^{(t' - t)} \\] Solve for $ t\u2019 - t $: \\[ t' - t = \\frac{\\log 2}{\\log r} = \\frac{\\log2}{a}\\] Important note! We haven\u2019t specified what the starting time $ t $ actually is, and we don\u2019t have to since we just proved that this holds for any $ t $. print ( \"Time to double:\" , np . log ( 2 ) / a ) Time to double: 2.0613828343145832 # If you know the analytical solution X = np . array ( X ) . flatten () Y = np . array ( Y ) denominator = X . dot ( X ) - X . mean () * X . sum () a = ( X . dot ( Y ) - Y . mean () * X . sum () ) / denominator b = ( Y . mean () * X . dot ( X ) - X . mean () * X . dot ( Y ) ) / denominator print ( a , b ) print ( \"Time to double:\" , np . log ( 2 ) / a ) 0.341682487387313 17.764939393631764 Time to double: 2.0286295205239204","title":"Linear Regression"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Regression/#part-2-making-predictions","text":"This goes with the lecture \u201cMaking Predictions\u201d # Make sure the line fits our data Yhat = model . predict ( X ) . flatten () plt . scatter ( X , Y ) plt . plot ( X , Yhat ) # Manual calculation # Get the weights w , b = model . layers [ 0 ] . get_weights () # Reshape X because we flattened it again earlier X = X . reshape ( - 1 , 1 ) # (N x 1) x (1 x 1) + (1) --> (N x 1) Yhat2 = ( X . dot ( w ) + b ) . flatten () # Don't use == for floating points np . allclose ( Yhat , Yhat2 ) True Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Part 2: Making Predictions"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_CNN/","text":"================ by Jawad Haider Spam Detection CNN Spam Detection CNN \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.layers import Dense , Input , GlobalMaxPooling1D from tensorflow.keras.layers import Conv1D , MaxPooling1D , Embedding from tensorflow.keras.models import Model # Unfortunately this URL doesn't work directly with pd.read_csv ! wget - nc https : // lazyprogrammer . me / course_files / spam . csv File \u2018spam.csv\u2019 already there; not retrieving. df = pd . read_csv ( 'spam.csv' , encoding = 'ISO-8859-1' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1 v2 Unnamed: 2 Unnamed: 3 Unnamed: 4 0 ham Go until jurong point, crazy.. Available only ... NaN NaN NaN 1 ham Ok lar... Joking wif u oni... NaN NaN NaN 2 spam Free entry in 2 a wkly comp to win FA Cup fina... NaN NaN NaN 3 ham U dun say so early hor... U c already then say... NaN NaN NaN 4 ham Nah I don't think he goes to usf, he lives aro... NaN NaN NaN # drop unnecessary columns df = df . drop ([ \"Unnamed: 2\" , \"Unnamed: 3\" , \"Unnamed: 4\" ], axis = 1 ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1 v2 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... # rename columns to something better df . columns = [ 'labels' , 'data' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } labels data 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... # create binary labels df [ 'b_labels' ] = df [ 'labels' ] . map ({ 'ham' : 0 , 'spam' : 1 }) Y = df [ 'b_labels' ] . values # split up the data df_train , df_test , Ytrain , Ytest = train_test_split ( df [ 'data' ], Y , test_size = 0.33 ) # Convert sentences to sequences MAX_VOCAB_SIZE = 20000 tokenizer = Tokenizer ( num_words = MAX_VOCAB_SIZE ) tokenizer . fit_on_texts ( df_train ) sequences_train = tokenizer . texts_to_sequences ( df_train ) sequences_test = tokenizer . texts_to_sequences ( df_test ) # get word -> integer mapping word2idx = tokenizer . word_index V = len ( word2idx ) print ( 'Found %s unique tokens.' % V ) Found 7257 unique tokens. # pad sequences so that we get a N x T matrix data_train = pad_sequences ( sequences_train ) print ( 'Shape of data train tensor:' , data_train . shape ) # get sequence length T = data_train . shape [ 1 ] Shape of data train tensor: (3733, 121) data_test = pad_sequences ( sequences_test , maxlen = T ) print ( 'Shape of data test tensor:' , data_test . shape ) Shape of data test tensor: (1839, 121) # Create the model # We get to choose embedding dimensionality D = 20 # Note: we actually want to the size of the embedding to (V + 1) x D, # because the first index starts from 1 and not 0. # Thus, if the final index of the embedding matrix is V, # then it actually must have size V + 1. i = Input ( shape = ( T ,)) x = Embedding ( V + 1 , D )( i ) x = Conv1D ( 32 , 3 , activation = 'relu' )( x ) x = MaxPooling1D ( 3 )( x ) x = Conv1D ( 64 , 3 , activation = 'relu' )( x ) x = MaxPooling1D ( 3 )( x ) x = Conv1D ( 128 , 3 , activation = 'relu' )( x ) x = GlobalMaxPooling1D ()( x ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) # Compile and fit model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) print ( 'Training model...' ) r = model . fit ( data_train , Ytrain , epochs = 5 , validation_data = ( data_test , Ytest ) ) WARNING: Logging before flag parsing goes to stderr. W0817 17:10:21.218855 140024925730688 deprecation.py:323] From /tensorflow-2.0.0b1/python3.6/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Training model... Train on 3733 samples, validate on 1839 samples Epoch 1/5 3733/3733 [==============================] - 5s 1ms/sample - loss: 0.4038 - accuracy: 0.8647 - val_loss: 0.3462 - val_accuracy: 0.8684 Epoch 2/5 3733/3733 [==============================] - 1s 334us/sample - loss: 0.2440 - accuracy: 0.8947 - val_loss: 0.1272 - val_accuracy: 0.9706 Epoch 3/5 3733/3733 [==============================] - 1s 330us/sample - loss: 0.0454 - accuracy: 0.9874 - val_loss: 0.0939 - val_accuracy: 0.9777 Epoch 4/5 3733/3733 [==============================] - 1s 331us/sample - loss: 0.0223 - accuracy: 0.9941 - val_loss: 0.1070 - val_accuracy: 0.9810 Epoch 5/5 3733/3733 [==============================] - 1s 331us/sample - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.1020 - val_accuracy: 0.9793 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f595ac7a9b0> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f5958430898> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Spam Detection CNN"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_CNN/#spam-detection-cnn","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.layers import Dense , Input , GlobalMaxPooling1D from tensorflow.keras.layers import Conv1D , MaxPooling1D , Embedding from tensorflow.keras.models import Model # Unfortunately this URL doesn't work directly with pd.read_csv ! wget - nc https : // lazyprogrammer . me / course_files / spam . csv File \u2018spam.csv\u2019 already there; not retrieving. df = pd . read_csv ( 'spam.csv' , encoding = 'ISO-8859-1' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1 v2 Unnamed: 2 Unnamed: 3 Unnamed: 4 0 ham Go until jurong point, crazy.. Available only ... NaN NaN NaN 1 ham Ok lar... Joking wif u oni... NaN NaN NaN 2 spam Free entry in 2 a wkly comp to win FA Cup fina... NaN NaN NaN 3 ham U dun say so early hor... U c already then say... NaN NaN NaN 4 ham Nah I don't think he goes to usf, he lives aro... NaN NaN NaN # drop unnecessary columns df = df . drop ([ \"Unnamed: 2\" , \"Unnamed: 3\" , \"Unnamed: 4\" ], axis = 1 ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1 v2 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... # rename columns to something better df . columns = [ 'labels' , 'data' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } labels data 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... # create binary labels df [ 'b_labels' ] = df [ 'labels' ] . map ({ 'ham' : 0 , 'spam' : 1 }) Y = df [ 'b_labels' ] . values # split up the data df_train , df_test , Ytrain , Ytest = train_test_split ( df [ 'data' ], Y , test_size = 0.33 ) # Convert sentences to sequences MAX_VOCAB_SIZE = 20000 tokenizer = Tokenizer ( num_words = MAX_VOCAB_SIZE ) tokenizer . fit_on_texts ( df_train ) sequences_train = tokenizer . texts_to_sequences ( df_train ) sequences_test = tokenizer . texts_to_sequences ( df_test ) # get word -> integer mapping word2idx = tokenizer . word_index V = len ( word2idx ) print ( 'Found %s unique tokens.' % V ) Found 7257 unique tokens. # pad sequences so that we get a N x T matrix data_train = pad_sequences ( sequences_train ) print ( 'Shape of data train tensor:' , data_train . shape ) # get sequence length T = data_train . shape [ 1 ] Shape of data train tensor: (3733, 121) data_test = pad_sequences ( sequences_test , maxlen = T ) print ( 'Shape of data test tensor:' , data_test . shape ) Shape of data test tensor: (1839, 121) # Create the model # We get to choose embedding dimensionality D = 20 # Note: we actually want to the size of the embedding to (V + 1) x D, # because the first index starts from 1 and not 0. # Thus, if the final index of the embedding matrix is V, # then it actually must have size V + 1. i = Input ( shape = ( T ,)) x = Embedding ( V + 1 , D )( i ) x = Conv1D ( 32 , 3 , activation = 'relu' )( x ) x = MaxPooling1D ( 3 )( x ) x = Conv1D ( 64 , 3 , activation = 'relu' )( x ) x = MaxPooling1D ( 3 )( x ) x = Conv1D ( 128 , 3 , activation = 'relu' )( x ) x = GlobalMaxPooling1D ()( x ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) # Compile and fit model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) print ( 'Training model...' ) r = model . fit ( data_train , Ytrain , epochs = 5 , validation_data = ( data_test , Ytest ) ) WARNING: Logging before flag parsing goes to stderr. W0817 17:10:21.218855 140024925730688 deprecation.py:323] From /tensorflow-2.0.0b1/python3.6/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Training model... Train on 3733 samples, validate on 1839 samples Epoch 1/5 3733/3733 [==============================] - 5s 1ms/sample - loss: 0.4038 - accuracy: 0.8647 - val_loss: 0.3462 - val_accuracy: 0.8684 Epoch 2/5 3733/3733 [==============================] - 1s 334us/sample - loss: 0.2440 - accuracy: 0.8947 - val_loss: 0.1272 - val_accuracy: 0.9706 Epoch 3/5 3733/3733 [==============================] - 1s 330us/sample - loss: 0.0454 - accuracy: 0.9874 - val_loss: 0.0939 - val_accuracy: 0.9777 Epoch 4/5 3733/3733 [==============================] - 1s 331us/sample - loss: 0.0223 - accuracy: 0.9941 - val_loss: 0.1070 - val_accuracy: 0.9810 Epoch 5/5 3733/3733 [==============================] - 1s 331us/sample - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.1020 - val_accuracy: 0.9793 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f595ac7a9b0> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f5958430898> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Spam Detection CNN"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_RNN/","text":"================ by Jawad Haider Spam Detection RNN Spam Detection RNN \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.layers import Dense , Input , GlobalMaxPooling1D from tensorflow.keras.layers import LSTM , Embedding from tensorflow.keras.models import Model # Unfortunately this URL doesn't work directly with pd.read_csv ! wget - nc https : // lazyprogrammer . me / course_files / spam . csv --2019-08-02 21:15:36-- https://lazyprogrammer.me/course_files/spam.csv Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.31.81.48, 104.31.80.48, 2606:4700:30::681f:5130, ... Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.31.81.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 503663 (492K) [text/csv] Saving to: \u2018spam.csv.3\u2019 spam.csv.3 0%[ ] 0 --.-KB/s spam.csv.3 100%[===================>] 491.86K --.-KB/s in 0.02s 2019-08-02 21:15:36 (23.7 MB/s) - \u2018spam.csv.3\u2019 saved [503663/503663] ! head spam . csv UnicodeDecodeError: ignored df = pd . read_csv ( 'spam.csv' , encoding = 'ISO-8859-1' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1 v2 Unnamed: 2 Unnamed: 3 Unnamed: 4 0 ham Go until jurong point, crazy.. Available only ... NaN NaN NaN 1 ham Ok lar... Joking wif u oni... NaN NaN NaN 2 spam Free entry in 2 a wkly comp to win FA Cup fina... NaN NaN NaN 3 ham U dun say so early hor... U c already then say... NaN NaN NaN 4 ham Nah I don't think he goes to usf, he lives aro... NaN NaN NaN # drop unnecessary columns df = df . drop ([ \"Unnamed: 2\" , \"Unnamed: 3\" , \"Unnamed: 4\" ], axis = 1 ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1 v2 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... # rename columns to something better df . columns = [ 'labels' , 'data' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } labels data 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... # create binary labels df [ 'b_labels' ] = df [ 'labels' ] . map ({ 'ham' : 0 , 'spam' : 1 }) Y = df [ 'b_labels' ] . values # split up the data df_train , df_test , Ytrain , Ytest = train_test_split ( df [ 'data' ], Y , test_size = 0.33 ) # Convert sentences to sequences MAX_VOCAB_SIZE = 20000 tokenizer = Tokenizer ( num_words = MAX_VOCAB_SIZE ) tokenizer . fit_on_texts ( df_train ) sequences_train = tokenizer . texts_to_sequences ( df_train ) sequences_test = tokenizer . texts_to_sequences ( df_test ) # get word -> integer mapping word2idx = tokenizer . word_index V = len ( word2idx ) print ( 'Found %s unique tokens.' % V ) Found 7309 unique tokens. # pad sequences so that we get a N x T matrix data_train = pad_sequences ( sequences_train ) print ( 'Shape of data train tensor:' , data_train . shape ) # get sequence length T = data_train . shape [ 1 ] Shape of data train tensor: (3733, 189) data_test = pad_sequences ( sequences_test , maxlen = T ) print ( 'Shape of data test tensor:' , data_test . shape ) Shape of data test tensor: (1839, 189) # Create the model # We get to choose embedding dimensionality D = 20 # Hidden state dimensionality M = 15 # Note: we actually want to the size of the embedding to (V + 1) x D, # because the first index starts from 1 and not 0. # Thus, if the final index of the embedding matrix is V, # then it actually must have size V + 1. i = Input ( shape = ( T ,)) x = Embedding ( V + 1 , D )( i ) x = LSTM ( M , return_sequences = True )( x ) x = GlobalMaxPooling1D ()( x ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) # Compile and fit model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) print ( 'Training model...' ) r = model . fit ( data_train , Ytrain , epochs = 10 , validation_data = ( data_test , Ytest ) ) Training model... Train on 3733 samples, validate on 1839 samples Epoch 1/10 3733/3733 [==============================] - 6s 1ms/sample - loss: 0.5457 - accuracy: 0.8205 - val_loss: 0.3560 - val_accuracy: 0.8613 Epoch 2/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.2561 - accuracy: 0.8679 - val_loss: 0.2299 - val_accuracy: 0.8613 Epoch 3/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.1705 - accuracy: 0.9263 - val_loss: 0.1772 - val_accuracy: 0.9674 Epoch 4/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.1265 - accuracy: 0.9914 - val_loss: 0.1424 - val_accuracy: 0.9766 Epoch 5/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.1004 - accuracy: 0.9933 - val_loss: 0.1227 - val_accuracy: 0.9799 Epoch 6/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0815 - accuracy: 0.9954 - val_loss: 0.1168 - val_accuracy: 0.9804 Epoch 7/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0685 - accuracy: 0.9968 - val_loss: 0.1055 - val_accuracy: 0.9810 Epoch 8/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0586 - accuracy: 0.9976 - val_loss: 0.1006 - val_accuracy: 0.9821 Epoch 9/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0520 - accuracy: 0.9979 - val_loss: 0.0972 - val_accuracy: 0.9810 Epoch 10/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0464 - accuracy: 0.9979 - val_loss: 0.0945 - val_accuracy: 0.9815 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f0cb026bef0> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f0cb0250128> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Spam Detection RNN"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_RNN/#spam-detection-rnn","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.layers import Dense , Input , GlobalMaxPooling1D from tensorflow.keras.layers import LSTM , Embedding from tensorflow.keras.models import Model # Unfortunately this URL doesn't work directly with pd.read_csv ! wget - nc https : // lazyprogrammer . me / course_files / spam . csv --2019-08-02 21:15:36-- https://lazyprogrammer.me/course_files/spam.csv Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.31.81.48, 104.31.80.48, 2606:4700:30::681f:5130, ... Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.31.81.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 503663 (492K) [text/csv] Saving to: \u2018spam.csv.3\u2019 spam.csv.3 0%[ ] 0 --.-KB/s spam.csv.3 100%[===================>] 491.86K --.-KB/s in 0.02s 2019-08-02 21:15:36 (23.7 MB/s) - \u2018spam.csv.3\u2019 saved [503663/503663] ! head spam . csv UnicodeDecodeError: ignored df = pd . read_csv ( 'spam.csv' , encoding = 'ISO-8859-1' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1 v2 Unnamed: 2 Unnamed: 3 Unnamed: 4 0 ham Go until jurong point, crazy.. Available only ... NaN NaN NaN 1 ham Ok lar... Joking wif u oni... NaN NaN NaN 2 spam Free entry in 2 a wkly comp to win FA Cup fina... NaN NaN NaN 3 ham U dun say so early hor... U c already then say... NaN NaN NaN 4 ham Nah I don't think he goes to usf, he lives aro... NaN NaN NaN # drop unnecessary columns df = df . drop ([ \"Unnamed: 2\" , \"Unnamed: 3\" , \"Unnamed: 4\" ], axis = 1 ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1 v2 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... # rename columns to something better df . columns = [ 'labels' , 'data' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } labels data 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... # create binary labels df [ 'b_labels' ] = df [ 'labels' ] . map ({ 'ham' : 0 , 'spam' : 1 }) Y = df [ 'b_labels' ] . values # split up the data df_train , df_test , Ytrain , Ytest = train_test_split ( df [ 'data' ], Y , test_size = 0.33 ) # Convert sentences to sequences MAX_VOCAB_SIZE = 20000 tokenizer = Tokenizer ( num_words = MAX_VOCAB_SIZE ) tokenizer . fit_on_texts ( df_train ) sequences_train = tokenizer . texts_to_sequences ( df_train ) sequences_test = tokenizer . texts_to_sequences ( df_test ) # get word -> integer mapping word2idx = tokenizer . word_index V = len ( word2idx ) print ( 'Found %s unique tokens.' % V ) Found 7309 unique tokens. # pad sequences so that we get a N x T matrix data_train = pad_sequences ( sequences_train ) print ( 'Shape of data train tensor:' , data_train . shape ) # get sequence length T = data_train . shape [ 1 ] Shape of data train tensor: (3733, 189) data_test = pad_sequences ( sequences_test , maxlen = T ) print ( 'Shape of data test tensor:' , data_test . shape ) Shape of data test tensor: (1839, 189) # Create the model # We get to choose embedding dimensionality D = 20 # Hidden state dimensionality M = 15 # Note: we actually want to the size of the embedding to (V + 1) x D, # because the first index starts from 1 and not 0. # Thus, if the final index of the embedding matrix is V, # then it actually must have size V + 1. i = Input ( shape = ( T ,)) x = Embedding ( V + 1 , D )( i ) x = LSTM ( M , return_sequences = True )( x ) x = GlobalMaxPooling1D ()( x ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) # Compile and fit model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) print ( 'Training model...' ) r = model . fit ( data_train , Ytrain , epochs = 10 , validation_data = ( data_test , Ytest ) ) Training model... Train on 3733 samples, validate on 1839 samples Epoch 1/10 3733/3733 [==============================] - 6s 1ms/sample - loss: 0.5457 - accuracy: 0.8205 - val_loss: 0.3560 - val_accuracy: 0.8613 Epoch 2/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.2561 - accuracy: 0.8679 - val_loss: 0.2299 - val_accuracy: 0.8613 Epoch 3/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.1705 - accuracy: 0.9263 - val_loss: 0.1772 - val_accuracy: 0.9674 Epoch 4/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.1265 - accuracy: 0.9914 - val_loss: 0.1424 - val_accuracy: 0.9766 Epoch 5/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.1004 - accuracy: 0.9933 - val_loss: 0.1227 - val_accuracy: 0.9799 Epoch 6/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0815 - accuracy: 0.9954 - val_loss: 0.1168 - val_accuracy: 0.9804 Epoch 7/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0685 - accuracy: 0.9968 - val_loss: 0.1055 - val_accuracy: 0.9810 Epoch 8/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0586 - accuracy: 0.9976 - val_loss: 0.1006 - val_accuracy: 0.9821 Epoch 9/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0520 - accuracy: 0.9979 - val_loss: 0.0972 - val_accuracy: 0.9810 Epoch 10/10 3733/3733 [==============================] - 4s 1ms/sample - loss: 0.0464 - accuracy: 0.9979 - val_loss: 0.0945 - val_accuracy: 0.9815 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f0cb026bef0> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f0cb0250128> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Spam Detection RNN"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Text_Preprocessing/","text":"================ by Jawad Haider Text Preprocessing Text Preprocessing \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences # Just a simple test sentences = [ \"I like eggs and ham.\" , \"I love chocolate and bunnies.\" , \"I hate onions.\" ] MAX_VOCAB_SIZE = 20000 tokenizer = Tokenizer ( num_words = MAX_VOCAB_SIZE ) tokenizer . fit_on_texts ( sentences ) sequences = tokenizer . texts_to_sequences ( sentences ) print ( sequences ) [[1, 3, 4, 2, 5], [1, 6, 7, 2, 8], [1, 9, 10]] # How to get the word to index mapping? tokenizer . word_index {'and': 2, 'bunnies': 8, 'chocolate': 7, 'eggs': 4, 'ham': 5, 'hate': 9, 'i': 1, 'like': 3, 'love': 6, 'onions': 10} # use the defaults data = pad_sequences ( sequences ) print ( data ) [[ 1 3 4 2 5] [ 1 6 7 2 8] [ 0 0 1 9 10]] MAX_SEQUENCE_LENGTH = 5 data = pad_sequences ( sequences , maxlen = MAX_SEQUENCE_LENGTH ) print ( data ) [[ 1 3 4 2 5] [ 1 6 7 2 8] [ 0 0 1 9 10]] data = pad_sequences ( sequences , maxlen = MAX_SEQUENCE_LENGTH , padding = 'post' ) print ( data ) [[ 1 3 4 2 5] [ 1 6 7 2 8] [ 1 9 10 0 0]] # too much padding data = pad_sequences ( sequences , maxlen = 6 ) print ( data ) [[ 0 1 3 4 2 5] [ 0 1 6 7 2 8] [ 0 0 0 1 9 10]] # truncation data = pad_sequences ( sequences , maxlen = 4 ) print ( data ) [[ 3 4 2 5] [ 6 7 2 8] [ 0 1 9 10]] data = pad_sequences ( sequences , maxlen = 4 , truncating = 'post' ) print ( data ) [[ 1 3 4 2] [ 1 6 7 2] [ 0 1 9 10]] Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Text Preprocessing"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Text_Preprocessing/#text-preprocessing","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-beta1 from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences # Just a simple test sentences = [ \"I like eggs and ham.\" , \"I love chocolate and bunnies.\" , \"I hate onions.\" ] MAX_VOCAB_SIZE = 20000 tokenizer = Tokenizer ( num_words = MAX_VOCAB_SIZE ) tokenizer . fit_on_texts ( sentences ) sequences = tokenizer . texts_to_sequences ( sentences ) print ( sequences ) [[1, 3, 4, 2, 5], [1, 6, 7, 2, 8], [1, 9, 10]] # How to get the word to index mapping? tokenizer . word_index {'and': 2, 'bunnies': 8, 'chocolate': 7, 'eggs': 4, 'ham': 5, 'hate': 9, 'i': 1, 'like': 3, 'love': 6, 'onions': 10} # use the defaults data = pad_sequences ( sequences ) print ( data ) [[ 1 3 4 2 5] [ 1 6 7 2 8] [ 0 0 1 9 10]] MAX_SEQUENCE_LENGTH = 5 data = pad_sequences ( sequences , maxlen = MAX_SEQUENCE_LENGTH ) print ( data ) [[ 1 3 4 2 5] [ 1 6 7 2 8] [ 0 0 1 9 10]] data = pad_sequences ( sequences , maxlen = MAX_SEQUENCE_LENGTH , padding = 'post' ) print ( data ) [[ 1 3 4 2 5] [ 1 6 7 2 8] [ 1 9 10 0 0]] # too much padding data = pad_sequences ( sequences , maxlen = 6 ) print ( data ) [[ 0 1 3 4 2 5] [ 0 1 6 7 2 8] [ 0 0 0 1 9 10]] # truncation data = pad_sequences ( sequences , maxlen = 4 ) print ( data ) [[ 3 4 2 5] [ 6 7 2 8] [ 0 1 9 10]] data = pad_sequences ( sequences , maxlen = 4 , truncating = 'post' ) print ( data ) [[ 1 3 4 2] [ 1 6 7 2] [ 0 1 9 10]] Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Text Preprocessing"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Autoregressive_Model/","text":"================ by Jawad Haider Autoregressive Model Autoregressive Model \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.1.0-rc1 from tensorflow.keras.layers import Input , Dense from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt # make the original data series = np . sin ( 0.1 * np . arange ( 200 )) #+ np.random.randn(200)*0.1 # plot it plt . plot ( series ) plt . show () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T ) Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (190, 10) Y.shape (190,) ### try autoregressive linear model i = Input ( shape = ( T ,)) x = Dense ( 1 )( i ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.1 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 95 samples, validate on 95 samples Epoch 1/80 95/95 [==============================] - 1s 6ms/sample - loss: 0.5928 - val_loss: 0.4919 Epoch 2/80 95/95 [==============================] - 0s 164us/sample - loss: 0.3797 - val_loss: 0.1337 Epoch 3/80 95/95 [==============================] - 0s 266us/sample - loss: 0.1297 - val_loss: 0.1125 Epoch 4/80 95/95 [==============================] - 0s 186us/sample - loss: 0.0874 - val_loss: 0.0243 Epoch 5/80 95/95 [==============================] - 0s 180us/sample - loss: 0.0316 - val_loss: 0.0582 Epoch 6/80 95/95 [==============================] - 0s 205us/sample - loss: 0.0388 - val_loss: 0.0208 Epoch 7/80 95/95 [==============================] - 0s 173us/sample - loss: 0.0353 - val_loss: 0.0687 Epoch 8/80 95/95 [==============================] - 0s 186us/sample - loss: 0.0416 - val_loss: 0.0304 Epoch 9/80 95/95 [==============================] - 0s 170us/sample - loss: 0.0305 - val_loss: 0.0277 Epoch 10/80 95/95 [==============================] - 0s 219us/sample - loss: 0.0195 - val_loss: 0.0061 Epoch 11/80 95/95 [==============================] - 0s 187us/sample - loss: 0.0101 - val_loss: 0.0063 Epoch 12/80 95/95 [==============================] - 0s 184us/sample - loss: 0.0045 - val_loss: 0.0042 Epoch 13/80 95/95 [==============================] - 0s 171us/sample - loss: 0.0056 - val_loss: 0.0061 Epoch 14/80 95/95 [==============================] - 0s 180us/sample - loss: 0.0038 - val_loss: 0.0053 Epoch 15/80 95/95 [==============================] - 0s 180us/sample - loss: 0.0058 - val_loss: 0.0037 Epoch 16/80 95/95 [==============================] - 0s 207us/sample - loss: 0.0033 - val_loss: 0.0032 Epoch 17/80 95/95 [==============================] - 0s 182us/sample - loss: 0.0026 - val_loss: 4.2511e-04 Epoch 18/80 95/95 [==============================] - 0s 179us/sample - loss: 7.7209e-04 - val_loss: 7.0560e-04 Epoch 19/80 95/95 [==============================] - 0s 204us/sample - loss: 5.4705e-04 - val_loss: 6.5394e-04 Epoch 20/80 95/95 [==============================] - 0s 181us/sample - loss: 9.3594e-04 - val_loss: 6.7224e-04 Epoch 21/80 95/95 [==============================] - 0s 169us/sample - loss: 5.9015e-04 - val_loss: 0.0013 Epoch 22/80 95/95 [==============================] - 0s 205us/sample - loss: 8.0676e-04 - val_loss: 5.2523e-04 Epoch 23/80 95/95 [==============================] - 0s 224us/sample - loss: 3.5637e-04 - val_loss: 2.0870e-04 Epoch 24/80 95/95 [==============================] - 0s 235us/sample - loss: 1.0215e-04 - val_loss: 5.3742e-05 Epoch 25/80 95/95 [==============================] - 0s 214us/sample - loss: 1.1322e-04 - val_loss: 8.9134e-05 Epoch 26/80 95/95 [==============================] - 0s 174us/sample - loss: 1.1851e-04 - val_loss: 2.4181e-04 Epoch 27/80 95/95 [==============================] - 0s 169us/sample - loss: 1.2587e-04 - val_loss: 9.3724e-05 Epoch 28/80 95/95 [==============================] - 0s 146us/sample - loss: 9.6006e-05 - val_loss: 4.7208e-05 Epoch 29/80 95/95 [==============================] - 0s 186us/sample - loss: 3.7994e-05 - val_loss: 2.2043e-05 Epoch 30/80 95/95 [==============================] - 0s 171us/sample - loss: 1.7351e-05 - val_loss: 8.1480e-06 Epoch 31/80 95/95 [==============================] - 0s 198us/sample - loss: 1.8036e-05 - val_loss: 1.2440e-05 Epoch 32/80 95/95 [==============================] - 0s 180us/sample - loss: 1.9975e-05 - val_loss: 2.8459e-05 Epoch 33/80 95/95 [==============================] - 0s 242us/sample - loss: 1.9230e-05 - val_loss: 1.5293e-05 Epoch 34/80 95/95 [==============================] - 0s 172us/sample - loss: 1.2585e-05 - val_loss: 6.7560e-06 Epoch 35/80 95/95 [==============================] - 0s 219us/sample - loss: 4.1914e-06 - val_loss: 3.5701e-06 Epoch 36/80 95/95 [==============================] - 0s 171us/sample - loss: 3.0705e-06 - val_loss: 3.9625e-06 Epoch 37/80 95/95 [==============================] - 0s 176us/sample - loss: 4.9658e-06 - val_loss: 4.9225e-06 Epoch 38/80 95/95 [==============================] - 0s 194us/sample - loss: 4.6270e-06 - val_loss: 3.9518e-06 Epoch 39/80 95/95 [==============================] - 0s 180us/sample - loss: 2.0241e-06 - val_loss: 1.9492e-06 Epoch 40/80 95/95 [==============================] - 0s 228us/sample - loss: 1.1528e-06 - val_loss: 7.1516e-07 Epoch 41/80 95/95 [==============================] - 0s 195us/sample - loss: 1.2629e-06 - val_loss: 7.8588e-07 Epoch 42/80 95/95 [==============================] - 0s 217us/sample - loss: 1.2023e-06 - val_loss: 8.7311e-07 Epoch 43/80 95/95 [==============================] - 0s 159us/sample - loss: 8.9838e-07 - val_loss: 1.4360e-06 Epoch 44/80 95/95 [==============================] - 0s 191us/sample - loss: 7.9823e-07 - val_loss: 4.8116e-07 Epoch 45/80 95/95 [==============================] - 0s 176us/sample - loss: 4.4251e-07 - val_loss: 4.6078e-08 Epoch 46/80 95/95 [==============================] - 0s 191us/sample - loss: 2.9620e-07 - val_loss: 1.3887e-07 Epoch 47/80 95/95 [==============================] - 0s 227us/sample - loss: 2.5926e-07 - val_loss: 2.5561e-07 Epoch 48/80 95/95 [==============================] - 0s 159us/sample - loss: 2.0303e-07 - val_loss: 3.1984e-07 Epoch 49/80 95/95 [==============================] - 0s 223us/sample - loss: 1.3438e-07 - val_loss: 2.8710e-08 Epoch 50/80 95/95 [==============================] - 0s 180us/sample - loss: 6.0148e-08 - val_loss: 2.4986e-08 Epoch 51/80 95/95 [==============================] - 0s 186us/sample - loss: 6.0143e-08 - val_loss: 4.7983e-08 Epoch 52/80 95/95 [==============================] - 0s 209us/sample - loss: 5.0851e-08 - val_loss: 5.6664e-08 Epoch 53/80 95/95 [==============================] - 0s 208us/sample - loss: 3.9499e-08 - val_loss: 5.2733e-08 Epoch 54/80 95/95 [==============================] - 0s 167us/sample - loss: 2.6937e-08 - val_loss: 8.6147e-09 Epoch 55/80 95/95 [==============================] - 0s 197us/sample - loss: 1.8057e-08 - val_loss: 3.7212e-09 Epoch 56/80 95/95 [==============================] - 0s 196us/sample - loss: 1.7351e-08 - val_loss: 9.4838e-09 Epoch 57/80 95/95 [==============================] - 0s 188us/sample - loss: 1.4201e-08 - val_loss: 1.2931e-08 Epoch 58/80 95/95 [==============================] - 0s 284us/sample - loss: 7.6740e-09 - val_loss: 1.9589e-08 Epoch 59/80 95/95 [==============================] - 0s 215us/sample - loss: 8.0834e-09 - val_loss: 3.3542e-09 Epoch 60/80 95/95 [==============================] - 0s 162us/sample - loss: 4.9637e-09 - val_loss: 2.0916e-09 Epoch 61/80 95/95 [==============================] - 0s 247us/sample - loss: 4.2822e-09 - val_loss: 2.6194e-09 Epoch 62/80 95/95 [==============================] - 0s 216us/sample - loss: 3.0952e-09 - val_loss: 2.4419e-09 Epoch 63/80 95/95 [==============================] - 0s 209us/sample - loss: 1.4149e-09 - val_loss: 3.5520e-09 Epoch 64/80 95/95 [==============================] - 0s 179us/sample - loss: 1.5010e-09 - val_loss: 1.6126e-09 Epoch 65/80 95/95 [==============================] - 0s 182us/sample - loss: 1.5736e-09 - val_loss: 1.0080e-09 Epoch 66/80 95/95 [==============================] - 0s 172us/sample - loss: 1.6310e-09 - val_loss: 5.8007e-10 Epoch 67/80 95/95 [==============================] - 0s 266us/sample - loss: 1.0262e-09 - val_loss: 7.7888e-11 Epoch 68/80 95/95 [==============================] - 0s 203us/sample - loss: 5.2353e-10 - val_loss: 2.4929e-10 Epoch 69/80 95/95 [==============================] - 0s 191us/sample - loss: 2.6591e-10 - val_loss: 3.0532e-10 Epoch 70/80 95/95 [==============================] - 0s 202us/sample - loss: 2.9230e-10 - val_loss: 3.4441e-10 Epoch 71/80 95/95 [==============================] - 0s 171us/sample - loss: 2.6945e-10 - val_loss: 3.9480e-10 Epoch 72/80 95/95 [==============================] - 0s 176us/sample - loss: 1.6047e-10 - val_loss: 2.6156e-10 Epoch 73/80 95/95 [==============================] - 0s 162us/sample - loss: 1.3762e-10 - val_loss: 7.6606e-11 Epoch 74/80 95/95 [==============================] - 0s 189us/sample - loss: 1.1569e-10 - val_loss: 3.6307e-11 Epoch 75/80 95/95 [==============================] - 0s 183us/sample - loss: 8.6744e-11 - val_loss: 2.3215e-11 Epoch 76/80 95/95 [==============================] - 0s 179us/sample - loss: 5.2023e-11 - val_loss: 1.7341e-11 Epoch 77/80 95/95 [==============================] - 0s 196us/sample - loss: 2.9296e-11 - val_loss: 4.5361e-11 Epoch 78/80 95/95 [==============================] - 0s 216us/sample - loss: 2.8942e-11 - val_loss: 3.2065e-11 Epoch 79/80 95/95 [==============================] - 0s 188us/sample - loss: 2.5990e-11 - val_loss: 2.9623e-11 Epoch 80/80 95/95 [==============================] - 0s 213us/sample - loss: 1.8977e-11 - val_loss: 3.0598e-11 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f64d4fa0278> # \"Wrong\" forecast using true targets validation_target = Y [ - N // 2 :] validation_predictions = [] # index of first validation input i = - N // 2 while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( X [ i ] . reshape ( 1 , - 1 ))[ 0 , 0 ] # 1x1 array -> scalar i += 1 # update the predictions list validation_predictions . append ( p ) plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f64d79f8f98> # Forecast future values (use only self-predictions for making future predictions) validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , - 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f64d7c15cf8> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Autoregressive Model"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Autoregressive_Model/#autoregressive-model","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.1.0-rc1 from tensorflow.keras.layers import Input , Dense from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt # make the original data series = np . sin ( 0.1 * np . arange ( 200 )) #+ np.random.randn(200)*0.1 # plot it plt . plot ( series ) plt . show () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T ) Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (190, 10) Y.shape (190,) ### try autoregressive linear model i = Input ( shape = ( T ,)) x = Dense ( 1 )( i ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.1 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 95 samples, validate on 95 samples Epoch 1/80 95/95 [==============================] - 1s 6ms/sample - loss: 0.5928 - val_loss: 0.4919 Epoch 2/80 95/95 [==============================] - 0s 164us/sample - loss: 0.3797 - val_loss: 0.1337 Epoch 3/80 95/95 [==============================] - 0s 266us/sample - loss: 0.1297 - val_loss: 0.1125 Epoch 4/80 95/95 [==============================] - 0s 186us/sample - loss: 0.0874 - val_loss: 0.0243 Epoch 5/80 95/95 [==============================] - 0s 180us/sample - loss: 0.0316 - val_loss: 0.0582 Epoch 6/80 95/95 [==============================] - 0s 205us/sample - loss: 0.0388 - val_loss: 0.0208 Epoch 7/80 95/95 [==============================] - 0s 173us/sample - loss: 0.0353 - val_loss: 0.0687 Epoch 8/80 95/95 [==============================] - 0s 186us/sample - loss: 0.0416 - val_loss: 0.0304 Epoch 9/80 95/95 [==============================] - 0s 170us/sample - loss: 0.0305 - val_loss: 0.0277 Epoch 10/80 95/95 [==============================] - 0s 219us/sample - loss: 0.0195 - val_loss: 0.0061 Epoch 11/80 95/95 [==============================] - 0s 187us/sample - loss: 0.0101 - val_loss: 0.0063 Epoch 12/80 95/95 [==============================] - 0s 184us/sample - loss: 0.0045 - val_loss: 0.0042 Epoch 13/80 95/95 [==============================] - 0s 171us/sample - loss: 0.0056 - val_loss: 0.0061 Epoch 14/80 95/95 [==============================] - 0s 180us/sample - loss: 0.0038 - val_loss: 0.0053 Epoch 15/80 95/95 [==============================] - 0s 180us/sample - loss: 0.0058 - val_loss: 0.0037 Epoch 16/80 95/95 [==============================] - 0s 207us/sample - loss: 0.0033 - val_loss: 0.0032 Epoch 17/80 95/95 [==============================] - 0s 182us/sample - loss: 0.0026 - val_loss: 4.2511e-04 Epoch 18/80 95/95 [==============================] - 0s 179us/sample - loss: 7.7209e-04 - val_loss: 7.0560e-04 Epoch 19/80 95/95 [==============================] - 0s 204us/sample - loss: 5.4705e-04 - val_loss: 6.5394e-04 Epoch 20/80 95/95 [==============================] - 0s 181us/sample - loss: 9.3594e-04 - val_loss: 6.7224e-04 Epoch 21/80 95/95 [==============================] - 0s 169us/sample - loss: 5.9015e-04 - val_loss: 0.0013 Epoch 22/80 95/95 [==============================] - 0s 205us/sample - loss: 8.0676e-04 - val_loss: 5.2523e-04 Epoch 23/80 95/95 [==============================] - 0s 224us/sample - loss: 3.5637e-04 - val_loss: 2.0870e-04 Epoch 24/80 95/95 [==============================] - 0s 235us/sample - loss: 1.0215e-04 - val_loss: 5.3742e-05 Epoch 25/80 95/95 [==============================] - 0s 214us/sample - loss: 1.1322e-04 - val_loss: 8.9134e-05 Epoch 26/80 95/95 [==============================] - 0s 174us/sample - loss: 1.1851e-04 - val_loss: 2.4181e-04 Epoch 27/80 95/95 [==============================] - 0s 169us/sample - loss: 1.2587e-04 - val_loss: 9.3724e-05 Epoch 28/80 95/95 [==============================] - 0s 146us/sample - loss: 9.6006e-05 - val_loss: 4.7208e-05 Epoch 29/80 95/95 [==============================] - 0s 186us/sample - loss: 3.7994e-05 - val_loss: 2.2043e-05 Epoch 30/80 95/95 [==============================] - 0s 171us/sample - loss: 1.7351e-05 - val_loss: 8.1480e-06 Epoch 31/80 95/95 [==============================] - 0s 198us/sample - loss: 1.8036e-05 - val_loss: 1.2440e-05 Epoch 32/80 95/95 [==============================] - 0s 180us/sample - loss: 1.9975e-05 - val_loss: 2.8459e-05 Epoch 33/80 95/95 [==============================] - 0s 242us/sample - loss: 1.9230e-05 - val_loss: 1.5293e-05 Epoch 34/80 95/95 [==============================] - 0s 172us/sample - loss: 1.2585e-05 - val_loss: 6.7560e-06 Epoch 35/80 95/95 [==============================] - 0s 219us/sample - loss: 4.1914e-06 - val_loss: 3.5701e-06 Epoch 36/80 95/95 [==============================] - 0s 171us/sample - loss: 3.0705e-06 - val_loss: 3.9625e-06 Epoch 37/80 95/95 [==============================] - 0s 176us/sample - loss: 4.9658e-06 - val_loss: 4.9225e-06 Epoch 38/80 95/95 [==============================] - 0s 194us/sample - loss: 4.6270e-06 - val_loss: 3.9518e-06 Epoch 39/80 95/95 [==============================] - 0s 180us/sample - loss: 2.0241e-06 - val_loss: 1.9492e-06 Epoch 40/80 95/95 [==============================] - 0s 228us/sample - loss: 1.1528e-06 - val_loss: 7.1516e-07 Epoch 41/80 95/95 [==============================] - 0s 195us/sample - loss: 1.2629e-06 - val_loss: 7.8588e-07 Epoch 42/80 95/95 [==============================] - 0s 217us/sample - loss: 1.2023e-06 - val_loss: 8.7311e-07 Epoch 43/80 95/95 [==============================] - 0s 159us/sample - loss: 8.9838e-07 - val_loss: 1.4360e-06 Epoch 44/80 95/95 [==============================] - 0s 191us/sample - loss: 7.9823e-07 - val_loss: 4.8116e-07 Epoch 45/80 95/95 [==============================] - 0s 176us/sample - loss: 4.4251e-07 - val_loss: 4.6078e-08 Epoch 46/80 95/95 [==============================] - 0s 191us/sample - loss: 2.9620e-07 - val_loss: 1.3887e-07 Epoch 47/80 95/95 [==============================] - 0s 227us/sample - loss: 2.5926e-07 - val_loss: 2.5561e-07 Epoch 48/80 95/95 [==============================] - 0s 159us/sample - loss: 2.0303e-07 - val_loss: 3.1984e-07 Epoch 49/80 95/95 [==============================] - 0s 223us/sample - loss: 1.3438e-07 - val_loss: 2.8710e-08 Epoch 50/80 95/95 [==============================] - 0s 180us/sample - loss: 6.0148e-08 - val_loss: 2.4986e-08 Epoch 51/80 95/95 [==============================] - 0s 186us/sample - loss: 6.0143e-08 - val_loss: 4.7983e-08 Epoch 52/80 95/95 [==============================] - 0s 209us/sample - loss: 5.0851e-08 - val_loss: 5.6664e-08 Epoch 53/80 95/95 [==============================] - 0s 208us/sample - loss: 3.9499e-08 - val_loss: 5.2733e-08 Epoch 54/80 95/95 [==============================] - 0s 167us/sample - loss: 2.6937e-08 - val_loss: 8.6147e-09 Epoch 55/80 95/95 [==============================] - 0s 197us/sample - loss: 1.8057e-08 - val_loss: 3.7212e-09 Epoch 56/80 95/95 [==============================] - 0s 196us/sample - loss: 1.7351e-08 - val_loss: 9.4838e-09 Epoch 57/80 95/95 [==============================] - 0s 188us/sample - loss: 1.4201e-08 - val_loss: 1.2931e-08 Epoch 58/80 95/95 [==============================] - 0s 284us/sample - loss: 7.6740e-09 - val_loss: 1.9589e-08 Epoch 59/80 95/95 [==============================] - 0s 215us/sample - loss: 8.0834e-09 - val_loss: 3.3542e-09 Epoch 60/80 95/95 [==============================] - 0s 162us/sample - loss: 4.9637e-09 - val_loss: 2.0916e-09 Epoch 61/80 95/95 [==============================] - 0s 247us/sample - loss: 4.2822e-09 - val_loss: 2.6194e-09 Epoch 62/80 95/95 [==============================] - 0s 216us/sample - loss: 3.0952e-09 - val_loss: 2.4419e-09 Epoch 63/80 95/95 [==============================] - 0s 209us/sample - loss: 1.4149e-09 - val_loss: 3.5520e-09 Epoch 64/80 95/95 [==============================] - 0s 179us/sample - loss: 1.5010e-09 - val_loss: 1.6126e-09 Epoch 65/80 95/95 [==============================] - 0s 182us/sample - loss: 1.5736e-09 - val_loss: 1.0080e-09 Epoch 66/80 95/95 [==============================] - 0s 172us/sample - loss: 1.6310e-09 - val_loss: 5.8007e-10 Epoch 67/80 95/95 [==============================] - 0s 266us/sample - loss: 1.0262e-09 - val_loss: 7.7888e-11 Epoch 68/80 95/95 [==============================] - 0s 203us/sample - loss: 5.2353e-10 - val_loss: 2.4929e-10 Epoch 69/80 95/95 [==============================] - 0s 191us/sample - loss: 2.6591e-10 - val_loss: 3.0532e-10 Epoch 70/80 95/95 [==============================] - 0s 202us/sample - loss: 2.9230e-10 - val_loss: 3.4441e-10 Epoch 71/80 95/95 [==============================] - 0s 171us/sample - loss: 2.6945e-10 - val_loss: 3.9480e-10 Epoch 72/80 95/95 [==============================] - 0s 176us/sample - loss: 1.6047e-10 - val_loss: 2.6156e-10 Epoch 73/80 95/95 [==============================] - 0s 162us/sample - loss: 1.3762e-10 - val_loss: 7.6606e-11 Epoch 74/80 95/95 [==============================] - 0s 189us/sample - loss: 1.1569e-10 - val_loss: 3.6307e-11 Epoch 75/80 95/95 [==============================] - 0s 183us/sample - loss: 8.6744e-11 - val_loss: 2.3215e-11 Epoch 76/80 95/95 [==============================] - 0s 179us/sample - loss: 5.2023e-11 - val_loss: 1.7341e-11 Epoch 77/80 95/95 [==============================] - 0s 196us/sample - loss: 2.9296e-11 - val_loss: 4.5361e-11 Epoch 78/80 95/95 [==============================] - 0s 216us/sample - loss: 2.8942e-11 - val_loss: 3.2065e-11 Epoch 79/80 95/95 [==============================] - 0s 188us/sample - loss: 2.5990e-11 - val_loss: 2.9623e-11 Epoch 80/80 95/95 [==============================] - 0s 213us/sample - loss: 1.8977e-11 - val_loss: 3.0598e-11 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f64d4fa0278> # \"Wrong\" forecast using true targets validation_target = Y [ - N // 2 :] validation_predictions = [] # index of first validation input i = - N // 2 while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( X [ i ] . reshape ( 1 , - 1 ))[ 0 , 0 ] # 1x1 array -> scalar i += 1 # update the predictions list validation_predictions . append ( p ) plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f64d79f8f98> # Forecast future values (use only self-predictions for making future predictions) validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , - 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f64d7c15cf8> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Autoregressive Model"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_LSTM_Nonlinear/","text":"================ by Jawad Haider LSTM Nonlinear LSTM Nonlinear \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348.9MB 56kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 40.2MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 30.3MB/s 2.0.0-beta1 from tensorflow.keras.layers import Input , SimpleRNN , GRU , LSTM , Dense , Flatten from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt # make the original data series = np . sin (( 0.1 * np . arange ( 400 )) ** 2 ) This is a time series of the form: \\[ x(t) = \\sin(\\omega t^2) \\] # plot it plt . plot ( series ) plt . show () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 D = 1 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T ) # make it N x T Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (390, 10) Y.shape (390,) ### try autoregressive linear model i = Input ( shape = ( T ,)) x = Dense ( 1 )( i ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.01 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 195 samples, validate on 195 samples Epoch 1/80 195/195 [==============================] - 1s 4ms/sample - loss: 1.4840 - val_loss: 1.7948 Epoch 2/80 195/195 [==============================] - 0s 163us/sample - loss: 1.2677 - val_loss: 1.6177 Epoch 3/80 195/195 [==============================] - 0s 133us/sample - loss: 1.1125 - val_loss: 1.4296 Epoch 4/80 195/195 [==============================] - 0s 136us/sample - loss: 0.9785 - val_loss: 1.2705 Epoch 5/80 195/195 [==============================] - 0s 141us/sample - loss: 0.8676 - val_loss: 1.1453 Epoch 6/80 195/195 [==============================] - 0s 148us/sample - loss: 0.7908 - val_loss: 1.0479 Epoch 7/80 195/195 [==============================] - 0s 171us/sample - loss: 0.7244 - val_loss: 0.9804 Epoch 8/80 195/195 [==============================] - 0s 143us/sample - loss: 0.6734 - val_loss: 0.9236 Epoch 9/80 195/195 [==============================] - 0s 138us/sample - loss: 0.6292 - val_loss: 0.8579 Epoch 10/80 195/195 [==============================] - 0s 136us/sample - loss: 0.5930 - val_loss: 0.8104 Epoch 11/80 195/195 [==============================] - 0s 141us/sample - loss: 0.5689 - val_loss: 0.7731 Epoch 12/80 195/195 [==============================] - 0s 134us/sample - loss: 0.5469 - val_loss: 0.7495 Epoch 13/80 195/195 [==============================] - 0s 145us/sample - loss: 0.5344 - val_loss: 0.7297 Epoch 14/80 195/195 [==============================] - 0s 134us/sample - loss: 0.5224 - val_loss: 0.7084 Epoch 15/80 195/195 [==============================] - 0s 133us/sample - loss: 0.5142 - val_loss: 0.6959 Epoch 16/80 195/195 [==============================] - 0s 153us/sample - loss: 0.5071 - val_loss: 0.6815 Epoch 17/80 195/195 [==============================] - 0s 144us/sample - loss: 0.5022 - val_loss: 0.6578 Epoch 18/80 195/195 [==============================] - 0s 132us/sample - loss: 0.4998 - val_loss: 0.6325 Epoch 19/80 195/195 [==============================] - 0s 136us/sample - loss: 0.4957 - val_loss: 0.6174 Epoch 20/80 195/195 [==============================] - 0s 136us/sample - loss: 0.4933 - val_loss: 0.6121 Epoch 21/80 195/195 [==============================] - 0s 137us/sample - loss: 0.4913 - val_loss: 0.6212 Epoch 22/80 195/195 [==============================] - 0s 135us/sample - loss: 0.4910 - val_loss: 0.6309 Epoch 23/80 195/195 [==============================] - 0s 137us/sample - loss: 0.4895 - val_loss: 0.6345 Epoch 24/80 195/195 [==============================] - 0s 135us/sample - loss: 0.4884 - val_loss: 0.6274 Epoch 25/80 195/195 [==============================] - 0s 139us/sample - loss: 0.4869 - val_loss: 0.6192 Epoch 26/80 195/195 [==============================] - 0s 142us/sample - loss: 0.4858 - val_loss: 0.6032 Epoch 27/80 195/195 [==============================] - 0s 170us/sample - loss: 0.4875 - val_loss: 0.5860 Epoch 28/80 195/195 [==============================] - 0s 154us/sample - loss: 0.4873 - val_loss: 0.5760 Epoch 29/80 195/195 [==============================] - 0s 155us/sample - loss: 0.4850 - val_loss: 0.5833 Epoch 30/80 195/195 [==============================] - 0s 146us/sample - loss: 0.4867 - val_loss: 0.5925 Epoch 31/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4855 - val_loss: 0.5933 Epoch 32/80 195/195 [==============================] - 0s 168us/sample - loss: 0.4856 - val_loss: 0.5947 Epoch 33/80 195/195 [==============================] - 0s 170us/sample - loss: 0.4876 - val_loss: 0.5957 Epoch 34/80 195/195 [==============================] - 0s 161us/sample - loss: 0.4874 - val_loss: 0.6007 Epoch 35/80 195/195 [==============================] - 0s 152us/sample - loss: 0.4875 - val_loss: 0.6079 Epoch 36/80 195/195 [==============================] - 0s 162us/sample - loss: 0.4872 - val_loss: 0.6131 Epoch 37/80 195/195 [==============================] - 0s 161us/sample - loss: 0.4878 - val_loss: 0.6098 Epoch 38/80 195/195 [==============================] - 0s 169us/sample - loss: 0.4869 - val_loss: 0.5985 Epoch 39/80 195/195 [==============================] - 0s 177us/sample - loss: 0.4866 - val_loss: 0.5879 Epoch 40/80 195/195 [==============================] - 0s 214us/sample - loss: 0.4858 - val_loss: 0.5799 Epoch 41/80 195/195 [==============================] - 0s 171us/sample - loss: 0.4859 - val_loss: 0.5793 Epoch 42/80 195/195 [==============================] - 0s 137us/sample - loss: 0.4868 - val_loss: 0.5852 Epoch 43/80 195/195 [==============================] - 0s 137us/sample - loss: 0.4843 - val_loss: 0.5864 Epoch 44/80 195/195 [==============================] - 0s 148us/sample - loss: 0.4857 - val_loss: 0.5939 Epoch 45/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4862 - val_loss: 0.6014 Epoch 46/80 195/195 [==============================] - 0s 151us/sample - loss: 0.4865 - val_loss: 0.5984 Epoch 47/80 195/195 [==============================] - 0s 158us/sample - loss: 0.4870 - val_loss: 0.6047 Epoch 48/80 195/195 [==============================] - 0s 153us/sample - loss: 0.4881 - val_loss: 0.6061 Epoch 49/80 195/195 [==============================] - 0s 140us/sample - loss: 0.4895 - val_loss: 0.6022 Epoch 50/80 195/195 [==============================] - 0s 151us/sample - loss: 0.4887 - val_loss: 0.5952 Epoch 51/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4869 - val_loss: 0.5878 Epoch 52/80 195/195 [==============================] - 0s 147us/sample - loss: 0.4877 - val_loss: 0.5812 Epoch 53/80 195/195 [==============================] - 0s 152us/sample - loss: 0.4893 - val_loss: 0.5791 Epoch 54/80 195/195 [==============================] - 0s 142us/sample - loss: 0.4906 - val_loss: 0.5832 Epoch 55/80 195/195 [==============================] - 0s 135us/sample - loss: 0.4912 - val_loss: 0.5884 Epoch 56/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4915 - val_loss: 0.5919 Epoch 57/80 195/195 [==============================] - 0s 155us/sample - loss: 0.4920 - val_loss: 0.6021 Epoch 58/80 195/195 [==============================] - 0s 151us/sample - loss: 0.4902 - val_loss: 0.6089 Epoch 59/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4877 - val_loss: 0.6124 Epoch 60/80 195/195 [==============================] - 0s 165us/sample - loss: 0.4884 - val_loss: 0.6090 Epoch 61/80 195/195 [==============================] - 0s 146us/sample - loss: 0.4873 - val_loss: 0.6023 Epoch 62/80 195/195 [==============================] - 0s 145us/sample - loss: 0.4865 - val_loss: 0.5946 Epoch 63/80 195/195 [==============================] - 0s 166us/sample - loss: 0.4863 - val_loss: 0.5984 Epoch 64/80 195/195 [==============================] - 0s 153us/sample - loss: 0.4855 - val_loss: 0.5958 Epoch 65/80 195/195 [==============================] - 0s 236us/sample - loss: 0.4863 - val_loss: 0.5991 Epoch 66/80 195/195 [==============================] - 0s 205us/sample - loss: 0.4875 - val_loss: 0.6030 Epoch 67/80 195/195 [==============================] - 0s 140us/sample - loss: 0.4869 - val_loss: 0.6059 Epoch 68/80 195/195 [==============================] - 0s 154us/sample - loss: 0.4885 - val_loss: 0.6003 Epoch 69/80 195/195 [==============================] - 0s 148us/sample - loss: 0.4880 - val_loss: 0.5904 Epoch 70/80 195/195 [==============================] - 0s 149us/sample - loss: 0.4882 - val_loss: 0.5855 Epoch 71/80 195/195 [==============================] - 0s 191us/sample - loss: 0.4881 - val_loss: 0.5788 Epoch 72/80 195/195 [==============================] - 0s 140us/sample - loss: 0.4874 - val_loss: 0.5805 Epoch 73/80 195/195 [==============================] - 0s 167us/sample - loss: 0.4872 - val_loss: 0.5830 Epoch 74/80 195/195 [==============================] - 0s 175us/sample - loss: 0.4878 - val_loss: 0.5786 Epoch 75/80 195/195 [==============================] - 0s 157us/sample - loss: 0.4862 - val_loss: 0.5886 Epoch 76/80 195/195 [==============================] - 0s 152us/sample - loss: 0.4882 - val_loss: 0.5914 Epoch 77/80 195/195 [==============================] - 0s 146us/sample - loss: 0.4871 - val_loss: 0.5927 Epoch 78/80 195/195 [==============================] - 0s 164us/sample - loss: 0.4863 - val_loss: 0.5945 Epoch 79/80 195/195 [==============================] - 0s 170us/sample - loss: 0.4868 - val_loss: 0.5849 Epoch 80/80 195/195 [==============================] - 0s 180us/sample - loss: 0.4865 - val_loss: 0.5714 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f6da02ef048> # One-step forecast using true targets # Note: even the one-step forecast fails badly outputs = model . predict ( X ) print ( outputs . shape ) predictions = outputs [:, 0 ] plt . plot ( Y , label = 'targets' ) plt . plot ( predictions , label = 'predictions' ) plt . title ( \"Linear Regression Predictions\" ) plt . legend () plt . show () (390, 1) # This is the code we had before - it does the same thing # One-step forecast using true targets validation_target = Y [ - N // 2 :] validation_predictions = [] # index of first validation input i = - N // 2 while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( X [ i ] . reshape ( 1 , - 1 ))[ 0 , 0 ] # 1x1 array -> scalar i += 1 # update the predictions list validation_predictions . append ( p ) plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f6da028afd0> # Multi-step forecast validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , - 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f6da0203e48> ### Now try RNN/LSTM model X = X . reshape ( - 1 , T , 1 ) # make it N x T x D # make the RNN i = Input ( shape = ( T , D )) x = LSTM ( 10 )( i ) x = Dense ( 1 )( x ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.05 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], batch_size = 32 , epochs = 200 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 195 samples, validate on 195 samples Epoch 1/200 195/195 [==============================] - 3s 15ms/sample - loss: 0.5466 - val_loss: 0.5635 Epoch 2/200 195/195 [==============================] - 0s 287us/sample - loss: 0.5213 - val_loss: 0.5932 Epoch 3/200 195/195 [==============================] - 0s 258us/sample - loss: 0.5075 - val_loss: 0.6299 Epoch 4/200 195/195 [==============================] - 0s 251us/sample - loss: 0.4792 - val_loss: 0.5864 Epoch 5/200 195/195 [==============================] - 0s 263us/sample - loss: 0.4202 - val_loss: 0.5122 Epoch 6/200 195/195 [==============================] - 0s 308us/sample - loss: 0.3570 - val_loss: 0.5067 Epoch 7/200 195/195 [==============================] - 0s 264us/sample - loss: 0.2767 - val_loss: 0.4933 Epoch 8/200 195/195 [==============================] - 0s 259us/sample - loss: 0.1817 - val_loss: 0.1868 Epoch 9/200 195/195 [==============================] - 0s 262us/sample - loss: 0.1132 - val_loss: 0.1706 Epoch 10/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0677 - val_loss: 0.0982 Epoch 11/200 195/195 [==============================] - 0s 286us/sample - loss: 0.0450 - val_loss: 0.0890 Epoch 12/200 195/195 [==============================] - 0s 245us/sample - loss: 0.0354 - val_loss: 0.0755 Epoch 13/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0266 - val_loss: 0.0730 Epoch 14/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0291 - val_loss: 0.0492 Epoch 15/200 195/195 [==============================] - 0s 269us/sample - loss: 0.0190 - val_loss: 0.0670 Epoch 16/200 195/195 [==============================] - 0s 285us/sample - loss: 0.0132 - val_loss: 0.0457 Epoch 17/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0115 - val_loss: 0.0590 Epoch 18/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0093 - val_loss: 0.0442 Epoch 19/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0077 - val_loss: 0.0493 Epoch 20/200 195/195 [==============================] - 0s 277us/sample - loss: 0.0065 - val_loss: 0.0445 Epoch 21/200 195/195 [==============================] - 0s 276us/sample - loss: 0.0069 - val_loss: 0.0404 Epoch 22/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0070 - val_loss: 0.0399 Epoch 23/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0069 - val_loss: 0.0405 Epoch 24/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0109 - val_loss: 0.0425 Epoch 25/200 195/195 [==============================] - 0s 294us/sample - loss: 0.0105 - val_loss: 0.0475 Epoch 26/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0104 - val_loss: 0.0351 Epoch 27/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0079 - val_loss: 0.0397 Epoch 28/200 195/195 [==============================] - 0s 264us/sample - loss: 0.0059 - val_loss: 0.0375 Epoch 29/200 195/195 [==============================] - 0s 280us/sample - loss: 0.0057 - val_loss: 0.0364 Epoch 30/200 195/195 [==============================] - 0s 344us/sample - loss: 0.0066 - val_loss: 0.0338 Epoch 31/200 195/195 [==============================] - 0s 269us/sample - loss: 0.0060 - val_loss: 0.0394 Epoch 32/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0079 - val_loss: 0.0314 Epoch 33/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0100 - val_loss: 0.0491 Epoch 34/200 195/195 [==============================] - 0s 277us/sample - loss: 0.0064 - val_loss: 0.0307 Epoch 35/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0055 - val_loss: 0.0326 Epoch 36/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0053 - val_loss: 0.0374 Epoch 37/200 195/195 [==============================] - 0s 267us/sample - loss: 0.0063 - val_loss: 0.0298 Epoch 38/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0048 - val_loss: 0.0350 Epoch 39/200 195/195 [==============================] - 0s 264us/sample - loss: 0.0049 - val_loss: 0.0235 Epoch 40/200 195/195 [==============================] - 0s 289us/sample - loss: 0.0037 - val_loss: 0.0342 Epoch 41/200 195/195 [==============================] - 0s 247us/sample - loss: 0.0035 - val_loss: 0.0233 Epoch 42/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0025 - val_loss: 0.0334 Epoch 43/200 195/195 [==============================] - 0s 244us/sample - loss: 0.0029 - val_loss: 0.0261 Epoch 44/200 195/195 [==============================] - 0s 297us/sample - loss: 0.0036 - val_loss: 0.0232 Epoch 45/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0093 - val_loss: 0.0380 Epoch 46/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0120 - val_loss: 0.0282 Epoch 47/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0104 - val_loss: 0.0223 Epoch 48/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0085 - val_loss: 0.0288 Epoch 49/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0032 - val_loss: 0.0350 Epoch 50/200 195/195 [==============================] - 0s 248us/sample - loss: 0.0046 - val_loss: 0.0255 Epoch 51/200 195/195 [==============================] - 0s 264us/sample - loss: 0.0033 - val_loss: 0.0297 Epoch 52/200 195/195 [==============================] - 0s 268us/sample - loss: 0.0026 - val_loss: 0.0239 Epoch 53/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0022 - val_loss: 0.0219 Epoch 54/200 195/195 [==============================] - 0s 256us/sample - loss: 0.0031 - val_loss: 0.0269 Epoch 55/200 195/195 [==============================] - 0s 285us/sample - loss: 0.0023 - val_loss: 0.0234 Epoch 56/200 195/195 [==============================] - 0s 256us/sample - loss: 0.0024 - val_loss: 0.0240 Epoch 57/200 195/195 [==============================] - 0s 266us/sample - loss: 0.0027 - val_loss: 0.0305 Epoch 58/200 195/195 [==============================] - 0s 276us/sample - loss: 0.0045 - val_loss: 0.0202 Epoch 59/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0038 - val_loss: 0.0333 Epoch 60/200 195/195 [==============================] - 0s 285us/sample - loss: 0.0041 - val_loss: 0.0185 Epoch 61/200 195/195 [==============================] - 0s 275us/sample - loss: 0.0037 - val_loss: 0.0325 Epoch 62/200 195/195 [==============================] - 0s 243us/sample - loss: 0.0031 - val_loss: 0.0204 Epoch 63/200 195/195 [==============================] - 0s 297us/sample - loss: 0.0027 - val_loss: 0.0254 Epoch 64/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0022 - val_loss: 0.0251 Epoch 65/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0018 - val_loss: 0.0201 Epoch 66/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0021 - val_loss: 0.0301 Epoch 67/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0053 - val_loss: 0.0257 Epoch 68/200 195/195 [==============================] - 0s 249us/sample - loss: 0.0025 - val_loss: 0.0240 Epoch 69/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0022 - val_loss: 0.0246 Epoch 70/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0019 - val_loss: 0.0241 Epoch 71/200 195/195 [==============================] - 0s 267us/sample - loss: 0.0020 - val_loss: 0.0226 Epoch 72/200 195/195 [==============================] - 0s 276us/sample - loss: 0.0017 - val_loss: 0.0208 Epoch 73/200 195/195 [==============================] - 0s 271us/sample - loss: 0.0035 - val_loss: 0.0240 Epoch 74/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0033 - val_loss: 0.0190 Epoch 75/200 195/195 [==============================] - 0s 244us/sample - loss: 0.0028 - val_loss: 0.0230 Epoch 76/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0020 - val_loss: 0.0200 Epoch 77/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0016 - val_loss: 0.0191 Epoch 78/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0017 - val_loss: 0.0220 Epoch 79/200 195/195 [==============================] - 0s 302us/sample - loss: 0.0019 - val_loss: 0.0230 Epoch 80/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0016 - val_loss: 0.0190 Epoch 81/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0023 - val_loss: 0.0172 Epoch 82/200 195/195 [==============================] - 0s 299us/sample - loss: 0.0026 - val_loss: 0.0209 Epoch 83/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0019 - val_loss: 0.0139 Epoch 84/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0025 - val_loss: 0.0149 Epoch 85/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0059 - val_loss: 0.0308 Epoch 86/200 195/195 [==============================] - 0s 248us/sample - loss: 0.0108 - val_loss: 0.0183 Epoch 87/200 195/195 [==============================] - 0s 251us/sample - loss: 0.0058 - val_loss: 0.0196 Epoch 88/200 195/195 [==============================] - 0s 256us/sample - loss: 0.0057 - val_loss: 0.0177 Epoch 89/200 195/195 [==============================] - 0s 264us/sample - loss: 0.0046 - val_loss: 0.0221 Epoch 90/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0085 - val_loss: 0.0187 Epoch 91/200 195/195 [==============================] - 0s 267us/sample - loss: 0.0149 - val_loss: 0.0262 Epoch 92/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0137 - val_loss: 0.0250 Epoch 93/200 195/195 [==============================] - 0s 261us/sample - loss: 0.1677 - val_loss: 0.2507 Epoch 94/200 195/195 [==============================] - 0s 243us/sample - loss: 0.1237 - val_loss: 0.0823 Epoch 95/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0684 - val_loss: 0.0629 Epoch 96/200 195/195 [==============================] - 0s 237us/sample - loss: 0.0475 - val_loss: 0.0795 Epoch 97/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0396 - val_loss: 0.0473 Epoch 98/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0433 - val_loss: 0.0502 Epoch 99/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0344 - val_loss: 0.0381 Epoch 100/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0220 - val_loss: 0.0319 Epoch 101/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0171 - val_loss: 0.0457 Epoch 102/200 195/195 [==============================] - 0s 300us/sample - loss: 0.0191 - val_loss: 0.0282 Epoch 103/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0106 - val_loss: 0.0277 Epoch 104/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0098 - val_loss: 0.0221 Epoch 105/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0093 - val_loss: 0.0189 Epoch 106/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0079 - val_loss: 0.0203 Epoch 107/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0064 - val_loss: 0.0201 Epoch 108/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0044 - val_loss: 0.0166 Epoch 109/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0045 - val_loss: 0.0178 Epoch 110/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0040 - val_loss: 0.0184 Epoch 111/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0036 - val_loss: 0.0145 Epoch 112/200 195/195 [==============================] - 0s 275us/sample - loss: 0.0031 - val_loss: 0.0167 Epoch 113/200 195/195 [==============================] - 0s 246us/sample - loss: 0.0037 - val_loss: 0.0166 Epoch 114/200 195/195 [==============================] - 0s 270us/sample - loss: 0.0044 - val_loss: 0.0155 Epoch 115/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0051 - val_loss: 0.0175 Epoch 116/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0056 - val_loss: 0.0161 Epoch 117/200 195/195 [==============================] - 0s 271us/sample - loss: 0.0037 - val_loss: 0.0161 Epoch 118/200 195/195 [==============================] - 0s 267us/sample - loss: 0.0046 - val_loss: 0.0135 Epoch 119/200 195/195 [==============================] - 0s 265us/sample - loss: 0.0032 - val_loss: 0.0146 Epoch 120/200 195/195 [==============================] - 0s 251us/sample - loss: 0.0028 - val_loss: 0.0155 Epoch 121/200 195/195 [==============================] - 0s 286us/sample - loss: 0.0026 - val_loss: 0.0175 Epoch 122/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0023 - val_loss: 0.0135 Epoch 123/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0017 - val_loss: 0.0138 Epoch 124/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0023 - val_loss: 0.0138 Epoch 125/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0034 - val_loss: 0.0162 Epoch 126/200 195/195 [==============================] - 0s 244us/sample - loss: 0.0059 - val_loss: 0.0118 Epoch 127/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0057 - val_loss: 0.0203 Epoch 128/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0070 - val_loss: 0.0203 Epoch 129/200 195/195 [==============================] - 0s 302us/sample - loss: 0.0115 - val_loss: 0.0151 Epoch 130/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0090 - val_loss: 0.0158 Epoch 131/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0062 - val_loss: 0.0183 Epoch 132/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0047 - val_loss: 0.0147 Epoch 133/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0066 - val_loss: 0.0185 Epoch 134/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0065 - val_loss: 0.0164 Epoch 135/200 195/195 [==============================] - 0s 266us/sample - loss: 0.0058 - val_loss: 0.0187 Epoch 136/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0047 - val_loss: 0.0159 Epoch 137/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0051 - val_loss: 0.0194 Epoch 138/200 195/195 [==============================] - 0s 277us/sample - loss: 0.0029 - val_loss: 0.0148 Epoch 139/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0051 - val_loss: 0.0174 Epoch 140/200 195/195 [==============================] - 0s 290us/sample - loss: 0.0041 - val_loss: 0.0140 Epoch 141/200 195/195 [==============================] - 0s 241us/sample - loss: 0.0030 - val_loss: 0.0176 Epoch 142/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0032 - val_loss: 0.0140 Epoch 143/200 195/195 [==============================] - 0s 269us/sample - loss: 0.0054 - val_loss: 0.0159 Epoch 144/200 195/195 [==============================] - 0s 274us/sample - loss: 0.0031 - val_loss: 0.0171 Epoch 145/200 195/195 [==============================] - 0s 256us/sample - loss: 0.0032 - val_loss: 0.0137 Epoch 146/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0024 - val_loss: 0.0170 Epoch 147/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0020 - val_loss: 0.0172 Epoch 148/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0015 - val_loss: 0.0136 Epoch 149/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0018 - val_loss: 0.0139 Epoch 150/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0019 - val_loss: 0.0147 Epoch 151/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0015 - val_loss: 0.0135 Epoch 152/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0013 - val_loss: 0.0154 Epoch 153/200 195/195 [==============================] - 0s 276us/sample - loss: 0.0014 - val_loss: 0.0140 Epoch 154/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0013 - val_loss: 0.0135 Epoch 155/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0016 - val_loss: 0.0140 Epoch 156/200 195/195 [==============================] - 0s 269us/sample - loss: 0.0017 - val_loss: 0.0133 Epoch 157/200 195/195 [==============================] - 0s 275us/sample - loss: 0.0016 - val_loss: 0.0167 Epoch 158/200 195/195 [==============================] - 0s 241us/sample - loss: 0.0019 - val_loss: 0.0116 Epoch 159/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0026 - val_loss: 0.0164 Epoch 160/200 195/195 [==============================] - 0s 296us/sample - loss: 0.0023 - val_loss: 0.0129 Epoch 161/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0019 - val_loss: 0.0168 Epoch 162/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0016 - val_loss: 0.0135 Epoch 163/200 195/195 [==============================] - 0s 266us/sample - loss: 0.0012 - val_loss: 0.0141 Epoch 164/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0013 - val_loss: 0.0146 Epoch 165/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0019 - val_loss: 0.0119 Epoch 166/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0030 - val_loss: 0.0168 Epoch 167/200 195/195 [==============================] - 0s 251us/sample - loss: 0.0028 - val_loss: 0.0150 Epoch 168/200 195/195 [==============================] - 0s 291us/sample - loss: 0.0027 - val_loss: 0.0142 Epoch 169/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0020 - val_loss: 0.0154 Epoch 170/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0016 - val_loss: 0.0160 Epoch 171/200 195/195 [==============================] - 0s 268us/sample - loss: 0.0013 - val_loss: 0.0143 Epoch 172/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0011 - val_loss: 0.0150 Epoch 173/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0010 - val_loss: 0.0151 Epoch 174/200 195/195 [==============================] - 0s 270us/sample - loss: 0.0011 - val_loss: 0.0157 Epoch 175/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0014 - val_loss: 0.0144 Epoch 176/200 195/195 [==============================] - 0s 258us/sample - loss: 7.5557e-04 - val_loss: 0.0141 Epoch 177/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0010 - val_loss: 0.0142 Epoch 178/200 195/195 [==============================] - 0s 253us/sample - loss: 8.8674e-04 - val_loss: 0.0148 Epoch 179/200 195/195 [==============================] - 0s 305us/sample - loss: 0.0014 - val_loss: 0.0145 Epoch 180/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0017 - val_loss: 0.0150 Epoch 181/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0018 - val_loss: 0.0142 Epoch 182/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0012 - val_loss: 0.0142 Epoch 183/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0022 - val_loss: 0.0156 Epoch 184/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0023 - val_loss: 0.0134 Epoch 185/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0015 - val_loss: 0.0146 Epoch 186/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0016 - val_loss: 0.0133 Epoch 187/200 195/195 [==============================] - 0s 282us/sample - loss: 0.0032 - val_loss: 0.0164 Epoch 188/200 195/195 [==============================] - 0s 244us/sample - loss: 0.0021 - val_loss: 0.0127 Epoch 189/200 195/195 [==============================] - 0s 251us/sample - loss: 0.0019 - val_loss: 0.0127 Epoch 190/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0025 - val_loss: 0.0199 Epoch 191/200 195/195 [==============================] - 0s 249us/sample - loss: 0.0059 - val_loss: 0.0152 Epoch 192/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0067 - val_loss: 0.0126 Epoch 193/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0145 - val_loss: 0.0203 Epoch 194/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0101 - val_loss: 0.0149 Epoch 195/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0060 - val_loss: 0.0205 Epoch 196/200 195/195 [==============================] - 0s 280us/sample - loss: 0.0060 - val_loss: 0.0129 Epoch 197/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0048 - val_loss: 0.0243 Epoch 198/200 195/195 [==============================] - 0s 281us/sample - loss: 0.0042 - val_loss: 0.0199 Epoch 199/200 195/195 [==============================] - 0s 289us/sample - loss: 0.0044 - val_loss: 0.0192 Epoch 200/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0041 - val_loss: 0.0202 # plot some data plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () plt . show () # One-step forecast using true targets outputs = model . predict ( X ) print ( outputs . shape ) predictions = outputs [:, 0 ] plt . plot ( Y , label = 'targets' ) plt . plot ( predictions , label = 'predictions' ) plt . title ( \"many-to-one RNN\" ) plt . legend () plt . show () (390, 1) # Multi-step forecast forecast = [] input_ = X [ - N // 2 ] while len ( forecast ) < len ( Y [ - N // 2 :]): # Reshape the input_ to N x T x D f = model . predict ( input_ . reshape ( 1 , T , 1 ))[ 0 , 0 ] forecast . append ( f ) # make a new input with the latest forecast input_ = np . roll ( input_ , - 1 ) input_ [ - 1 ] = f plt . plot ( Y [ - N // 2 :], label = 'targets' ) plt . plot ( forecast , label = 'forecast' ) plt . title ( \"RNN Forecast\" ) plt . legend () plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 LSTM Nonlinear"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_LSTM_Nonlinear/#lstm-nonlinear","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348.9MB 56kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 40.2MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 30.3MB/s 2.0.0-beta1 from tensorflow.keras.layers import Input , SimpleRNN , GRU , LSTM , Dense , Flatten from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt # make the original data series = np . sin (( 0.1 * np . arange ( 400 )) ** 2 ) This is a time series of the form: \\[ x(t) = \\sin(\\omega t^2) \\] # plot it plt . plot ( series ) plt . show () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 D = 1 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T ) # make it N x T Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (390, 10) Y.shape (390,) ### try autoregressive linear model i = Input ( shape = ( T ,)) x = Dense ( 1 )( i ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.01 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 195 samples, validate on 195 samples Epoch 1/80 195/195 [==============================] - 1s 4ms/sample - loss: 1.4840 - val_loss: 1.7948 Epoch 2/80 195/195 [==============================] - 0s 163us/sample - loss: 1.2677 - val_loss: 1.6177 Epoch 3/80 195/195 [==============================] - 0s 133us/sample - loss: 1.1125 - val_loss: 1.4296 Epoch 4/80 195/195 [==============================] - 0s 136us/sample - loss: 0.9785 - val_loss: 1.2705 Epoch 5/80 195/195 [==============================] - 0s 141us/sample - loss: 0.8676 - val_loss: 1.1453 Epoch 6/80 195/195 [==============================] - 0s 148us/sample - loss: 0.7908 - val_loss: 1.0479 Epoch 7/80 195/195 [==============================] - 0s 171us/sample - loss: 0.7244 - val_loss: 0.9804 Epoch 8/80 195/195 [==============================] - 0s 143us/sample - loss: 0.6734 - val_loss: 0.9236 Epoch 9/80 195/195 [==============================] - 0s 138us/sample - loss: 0.6292 - val_loss: 0.8579 Epoch 10/80 195/195 [==============================] - 0s 136us/sample - loss: 0.5930 - val_loss: 0.8104 Epoch 11/80 195/195 [==============================] - 0s 141us/sample - loss: 0.5689 - val_loss: 0.7731 Epoch 12/80 195/195 [==============================] - 0s 134us/sample - loss: 0.5469 - val_loss: 0.7495 Epoch 13/80 195/195 [==============================] - 0s 145us/sample - loss: 0.5344 - val_loss: 0.7297 Epoch 14/80 195/195 [==============================] - 0s 134us/sample - loss: 0.5224 - val_loss: 0.7084 Epoch 15/80 195/195 [==============================] - 0s 133us/sample - loss: 0.5142 - val_loss: 0.6959 Epoch 16/80 195/195 [==============================] - 0s 153us/sample - loss: 0.5071 - val_loss: 0.6815 Epoch 17/80 195/195 [==============================] - 0s 144us/sample - loss: 0.5022 - val_loss: 0.6578 Epoch 18/80 195/195 [==============================] - 0s 132us/sample - loss: 0.4998 - val_loss: 0.6325 Epoch 19/80 195/195 [==============================] - 0s 136us/sample - loss: 0.4957 - val_loss: 0.6174 Epoch 20/80 195/195 [==============================] - 0s 136us/sample - loss: 0.4933 - val_loss: 0.6121 Epoch 21/80 195/195 [==============================] - 0s 137us/sample - loss: 0.4913 - val_loss: 0.6212 Epoch 22/80 195/195 [==============================] - 0s 135us/sample - loss: 0.4910 - val_loss: 0.6309 Epoch 23/80 195/195 [==============================] - 0s 137us/sample - loss: 0.4895 - val_loss: 0.6345 Epoch 24/80 195/195 [==============================] - 0s 135us/sample - loss: 0.4884 - val_loss: 0.6274 Epoch 25/80 195/195 [==============================] - 0s 139us/sample - loss: 0.4869 - val_loss: 0.6192 Epoch 26/80 195/195 [==============================] - 0s 142us/sample - loss: 0.4858 - val_loss: 0.6032 Epoch 27/80 195/195 [==============================] - 0s 170us/sample - loss: 0.4875 - val_loss: 0.5860 Epoch 28/80 195/195 [==============================] - 0s 154us/sample - loss: 0.4873 - val_loss: 0.5760 Epoch 29/80 195/195 [==============================] - 0s 155us/sample - loss: 0.4850 - val_loss: 0.5833 Epoch 30/80 195/195 [==============================] - 0s 146us/sample - loss: 0.4867 - val_loss: 0.5925 Epoch 31/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4855 - val_loss: 0.5933 Epoch 32/80 195/195 [==============================] - 0s 168us/sample - loss: 0.4856 - val_loss: 0.5947 Epoch 33/80 195/195 [==============================] - 0s 170us/sample - loss: 0.4876 - val_loss: 0.5957 Epoch 34/80 195/195 [==============================] - 0s 161us/sample - loss: 0.4874 - val_loss: 0.6007 Epoch 35/80 195/195 [==============================] - 0s 152us/sample - loss: 0.4875 - val_loss: 0.6079 Epoch 36/80 195/195 [==============================] - 0s 162us/sample - loss: 0.4872 - val_loss: 0.6131 Epoch 37/80 195/195 [==============================] - 0s 161us/sample - loss: 0.4878 - val_loss: 0.6098 Epoch 38/80 195/195 [==============================] - 0s 169us/sample - loss: 0.4869 - val_loss: 0.5985 Epoch 39/80 195/195 [==============================] - 0s 177us/sample - loss: 0.4866 - val_loss: 0.5879 Epoch 40/80 195/195 [==============================] - 0s 214us/sample - loss: 0.4858 - val_loss: 0.5799 Epoch 41/80 195/195 [==============================] - 0s 171us/sample - loss: 0.4859 - val_loss: 0.5793 Epoch 42/80 195/195 [==============================] - 0s 137us/sample - loss: 0.4868 - val_loss: 0.5852 Epoch 43/80 195/195 [==============================] - 0s 137us/sample - loss: 0.4843 - val_loss: 0.5864 Epoch 44/80 195/195 [==============================] - 0s 148us/sample - loss: 0.4857 - val_loss: 0.5939 Epoch 45/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4862 - val_loss: 0.6014 Epoch 46/80 195/195 [==============================] - 0s 151us/sample - loss: 0.4865 - val_loss: 0.5984 Epoch 47/80 195/195 [==============================] - 0s 158us/sample - loss: 0.4870 - val_loss: 0.6047 Epoch 48/80 195/195 [==============================] - 0s 153us/sample - loss: 0.4881 - val_loss: 0.6061 Epoch 49/80 195/195 [==============================] - 0s 140us/sample - loss: 0.4895 - val_loss: 0.6022 Epoch 50/80 195/195 [==============================] - 0s 151us/sample - loss: 0.4887 - val_loss: 0.5952 Epoch 51/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4869 - val_loss: 0.5878 Epoch 52/80 195/195 [==============================] - 0s 147us/sample - loss: 0.4877 - val_loss: 0.5812 Epoch 53/80 195/195 [==============================] - 0s 152us/sample - loss: 0.4893 - val_loss: 0.5791 Epoch 54/80 195/195 [==============================] - 0s 142us/sample - loss: 0.4906 - val_loss: 0.5832 Epoch 55/80 195/195 [==============================] - 0s 135us/sample - loss: 0.4912 - val_loss: 0.5884 Epoch 56/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4915 - val_loss: 0.5919 Epoch 57/80 195/195 [==============================] - 0s 155us/sample - loss: 0.4920 - val_loss: 0.6021 Epoch 58/80 195/195 [==============================] - 0s 151us/sample - loss: 0.4902 - val_loss: 0.6089 Epoch 59/80 195/195 [==============================] - 0s 150us/sample - loss: 0.4877 - val_loss: 0.6124 Epoch 60/80 195/195 [==============================] - 0s 165us/sample - loss: 0.4884 - val_loss: 0.6090 Epoch 61/80 195/195 [==============================] - 0s 146us/sample - loss: 0.4873 - val_loss: 0.6023 Epoch 62/80 195/195 [==============================] - 0s 145us/sample - loss: 0.4865 - val_loss: 0.5946 Epoch 63/80 195/195 [==============================] - 0s 166us/sample - loss: 0.4863 - val_loss: 0.5984 Epoch 64/80 195/195 [==============================] - 0s 153us/sample - loss: 0.4855 - val_loss: 0.5958 Epoch 65/80 195/195 [==============================] - 0s 236us/sample - loss: 0.4863 - val_loss: 0.5991 Epoch 66/80 195/195 [==============================] - 0s 205us/sample - loss: 0.4875 - val_loss: 0.6030 Epoch 67/80 195/195 [==============================] - 0s 140us/sample - loss: 0.4869 - val_loss: 0.6059 Epoch 68/80 195/195 [==============================] - 0s 154us/sample - loss: 0.4885 - val_loss: 0.6003 Epoch 69/80 195/195 [==============================] - 0s 148us/sample - loss: 0.4880 - val_loss: 0.5904 Epoch 70/80 195/195 [==============================] - 0s 149us/sample - loss: 0.4882 - val_loss: 0.5855 Epoch 71/80 195/195 [==============================] - 0s 191us/sample - loss: 0.4881 - val_loss: 0.5788 Epoch 72/80 195/195 [==============================] - 0s 140us/sample - loss: 0.4874 - val_loss: 0.5805 Epoch 73/80 195/195 [==============================] - 0s 167us/sample - loss: 0.4872 - val_loss: 0.5830 Epoch 74/80 195/195 [==============================] - 0s 175us/sample - loss: 0.4878 - val_loss: 0.5786 Epoch 75/80 195/195 [==============================] - 0s 157us/sample - loss: 0.4862 - val_loss: 0.5886 Epoch 76/80 195/195 [==============================] - 0s 152us/sample - loss: 0.4882 - val_loss: 0.5914 Epoch 77/80 195/195 [==============================] - 0s 146us/sample - loss: 0.4871 - val_loss: 0.5927 Epoch 78/80 195/195 [==============================] - 0s 164us/sample - loss: 0.4863 - val_loss: 0.5945 Epoch 79/80 195/195 [==============================] - 0s 170us/sample - loss: 0.4868 - val_loss: 0.5849 Epoch 80/80 195/195 [==============================] - 0s 180us/sample - loss: 0.4865 - val_loss: 0.5714 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f6da02ef048> # One-step forecast using true targets # Note: even the one-step forecast fails badly outputs = model . predict ( X ) print ( outputs . shape ) predictions = outputs [:, 0 ] plt . plot ( Y , label = 'targets' ) plt . plot ( predictions , label = 'predictions' ) plt . title ( \"Linear Regression Predictions\" ) plt . legend () plt . show () (390, 1) # This is the code we had before - it does the same thing # One-step forecast using true targets validation_target = Y [ - N // 2 :] validation_predictions = [] # index of first validation input i = - N // 2 while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( X [ i ] . reshape ( 1 , - 1 ))[ 0 , 0 ] # 1x1 array -> scalar i += 1 # update the predictions list validation_predictions . append ( p ) plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f6da028afd0> # Multi-step forecast validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , - 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f6da0203e48> ### Now try RNN/LSTM model X = X . reshape ( - 1 , T , 1 ) # make it N x T x D # make the RNN i = Input ( shape = ( T , D )) x = LSTM ( 10 )( i ) x = Dense ( 1 )( x ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.05 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], batch_size = 32 , epochs = 200 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 195 samples, validate on 195 samples Epoch 1/200 195/195 [==============================] - 3s 15ms/sample - loss: 0.5466 - val_loss: 0.5635 Epoch 2/200 195/195 [==============================] - 0s 287us/sample - loss: 0.5213 - val_loss: 0.5932 Epoch 3/200 195/195 [==============================] - 0s 258us/sample - loss: 0.5075 - val_loss: 0.6299 Epoch 4/200 195/195 [==============================] - 0s 251us/sample - loss: 0.4792 - val_loss: 0.5864 Epoch 5/200 195/195 [==============================] - 0s 263us/sample - loss: 0.4202 - val_loss: 0.5122 Epoch 6/200 195/195 [==============================] - 0s 308us/sample - loss: 0.3570 - val_loss: 0.5067 Epoch 7/200 195/195 [==============================] - 0s 264us/sample - loss: 0.2767 - val_loss: 0.4933 Epoch 8/200 195/195 [==============================] - 0s 259us/sample - loss: 0.1817 - val_loss: 0.1868 Epoch 9/200 195/195 [==============================] - 0s 262us/sample - loss: 0.1132 - val_loss: 0.1706 Epoch 10/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0677 - val_loss: 0.0982 Epoch 11/200 195/195 [==============================] - 0s 286us/sample - loss: 0.0450 - val_loss: 0.0890 Epoch 12/200 195/195 [==============================] - 0s 245us/sample - loss: 0.0354 - val_loss: 0.0755 Epoch 13/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0266 - val_loss: 0.0730 Epoch 14/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0291 - val_loss: 0.0492 Epoch 15/200 195/195 [==============================] - 0s 269us/sample - loss: 0.0190 - val_loss: 0.0670 Epoch 16/200 195/195 [==============================] - 0s 285us/sample - loss: 0.0132 - val_loss: 0.0457 Epoch 17/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0115 - val_loss: 0.0590 Epoch 18/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0093 - val_loss: 0.0442 Epoch 19/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0077 - val_loss: 0.0493 Epoch 20/200 195/195 [==============================] - 0s 277us/sample - loss: 0.0065 - val_loss: 0.0445 Epoch 21/200 195/195 [==============================] - 0s 276us/sample - loss: 0.0069 - val_loss: 0.0404 Epoch 22/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0070 - val_loss: 0.0399 Epoch 23/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0069 - val_loss: 0.0405 Epoch 24/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0109 - val_loss: 0.0425 Epoch 25/200 195/195 [==============================] - 0s 294us/sample - loss: 0.0105 - val_loss: 0.0475 Epoch 26/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0104 - val_loss: 0.0351 Epoch 27/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0079 - val_loss: 0.0397 Epoch 28/200 195/195 [==============================] - 0s 264us/sample - loss: 0.0059 - val_loss: 0.0375 Epoch 29/200 195/195 [==============================] - 0s 280us/sample - loss: 0.0057 - val_loss: 0.0364 Epoch 30/200 195/195 [==============================] - 0s 344us/sample - loss: 0.0066 - val_loss: 0.0338 Epoch 31/200 195/195 [==============================] - 0s 269us/sample - loss: 0.0060 - val_loss: 0.0394 Epoch 32/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0079 - val_loss: 0.0314 Epoch 33/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0100 - val_loss: 0.0491 Epoch 34/200 195/195 [==============================] - 0s 277us/sample - loss: 0.0064 - val_loss: 0.0307 Epoch 35/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0055 - val_loss: 0.0326 Epoch 36/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0053 - val_loss: 0.0374 Epoch 37/200 195/195 [==============================] - 0s 267us/sample - loss: 0.0063 - val_loss: 0.0298 Epoch 38/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0048 - val_loss: 0.0350 Epoch 39/200 195/195 [==============================] - 0s 264us/sample - loss: 0.0049 - val_loss: 0.0235 Epoch 40/200 195/195 [==============================] - 0s 289us/sample - loss: 0.0037 - val_loss: 0.0342 Epoch 41/200 195/195 [==============================] - 0s 247us/sample - loss: 0.0035 - val_loss: 0.0233 Epoch 42/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0025 - val_loss: 0.0334 Epoch 43/200 195/195 [==============================] - 0s 244us/sample - loss: 0.0029 - val_loss: 0.0261 Epoch 44/200 195/195 [==============================] - 0s 297us/sample - loss: 0.0036 - val_loss: 0.0232 Epoch 45/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0093 - val_loss: 0.0380 Epoch 46/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0120 - val_loss: 0.0282 Epoch 47/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0104 - val_loss: 0.0223 Epoch 48/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0085 - val_loss: 0.0288 Epoch 49/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0032 - val_loss: 0.0350 Epoch 50/200 195/195 [==============================] - 0s 248us/sample - loss: 0.0046 - val_loss: 0.0255 Epoch 51/200 195/195 [==============================] - 0s 264us/sample - loss: 0.0033 - val_loss: 0.0297 Epoch 52/200 195/195 [==============================] - 0s 268us/sample - loss: 0.0026 - val_loss: 0.0239 Epoch 53/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0022 - val_loss: 0.0219 Epoch 54/200 195/195 [==============================] - 0s 256us/sample - loss: 0.0031 - val_loss: 0.0269 Epoch 55/200 195/195 [==============================] - 0s 285us/sample - loss: 0.0023 - val_loss: 0.0234 Epoch 56/200 195/195 [==============================] - 0s 256us/sample - loss: 0.0024 - val_loss: 0.0240 Epoch 57/200 195/195 [==============================] - 0s 266us/sample - loss: 0.0027 - val_loss: 0.0305 Epoch 58/200 195/195 [==============================] - 0s 276us/sample - loss: 0.0045 - val_loss: 0.0202 Epoch 59/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0038 - val_loss: 0.0333 Epoch 60/200 195/195 [==============================] - 0s 285us/sample - loss: 0.0041 - val_loss: 0.0185 Epoch 61/200 195/195 [==============================] - 0s 275us/sample - loss: 0.0037 - val_loss: 0.0325 Epoch 62/200 195/195 [==============================] - 0s 243us/sample - loss: 0.0031 - val_loss: 0.0204 Epoch 63/200 195/195 [==============================] - 0s 297us/sample - loss: 0.0027 - val_loss: 0.0254 Epoch 64/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0022 - val_loss: 0.0251 Epoch 65/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0018 - val_loss: 0.0201 Epoch 66/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0021 - val_loss: 0.0301 Epoch 67/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0053 - val_loss: 0.0257 Epoch 68/200 195/195 [==============================] - 0s 249us/sample - loss: 0.0025 - val_loss: 0.0240 Epoch 69/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0022 - val_loss: 0.0246 Epoch 70/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0019 - val_loss: 0.0241 Epoch 71/200 195/195 [==============================] - 0s 267us/sample - loss: 0.0020 - val_loss: 0.0226 Epoch 72/200 195/195 [==============================] - 0s 276us/sample - loss: 0.0017 - val_loss: 0.0208 Epoch 73/200 195/195 [==============================] - 0s 271us/sample - loss: 0.0035 - val_loss: 0.0240 Epoch 74/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0033 - val_loss: 0.0190 Epoch 75/200 195/195 [==============================] - 0s 244us/sample - loss: 0.0028 - val_loss: 0.0230 Epoch 76/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0020 - val_loss: 0.0200 Epoch 77/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0016 - val_loss: 0.0191 Epoch 78/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0017 - val_loss: 0.0220 Epoch 79/200 195/195 [==============================] - 0s 302us/sample - loss: 0.0019 - val_loss: 0.0230 Epoch 80/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0016 - val_loss: 0.0190 Epoch 81/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0023 - val_loss: 0.0172 Epoch 82/200 195/195 [==============================] - 0s 299us/sample - loss: 0.0026 - val_loss: 0.0209 Epoch 83/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0019 - val_loss: 0.0139 Epoch 84/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0025 - val_loss: 0.0149 Epoch 85/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0059 - val_loss: 0.0308 Epoch 86/200 195/195 [==============================] - 0s 248us/sample - loss: 0.0108 - val_loss: 0.0183 Epoch 87/200 195/195 [==============================] - 0s 251us/sample - loss: 0.0058 - val_loss: 0.0196 Epoch 88/200 195/195 [==============================] - 0s 256us/sample - loss: 0.0057 - val_loss: 0.0177 Epoch 89/200 195/195 [==============================] - 0s 264us/sample - loss: 0.0046 - val_loss: 0.0221 Epoch 90/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0085 - val_loss: 0.0187 Epoch 91/200 195/195 [==============================] - 0s 267us/sample - loss: 0.0149 - val_loss: 0.0262 Epoch 92/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0137 - val_loss: 0.0250 Epoch 93/200 195/195 [==============================] - 0s 261us/sample - loss: 0.1677 - val_loss: 0.2507 Epoch 94/200 195/195 [==============================] - 0s 243us/sample - loss: 0.1237 - val_loss: 0.0823 Epoch 95/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0684 - val_loss: 0.0629 Epoch 96/200 195/195 [==============================] - 0s 237us/sample - loss: 0.0475 - val_loss: 0.0795 Epoch 97/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0396 - val_loss: 0.0473 Epoch 98/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0433 - val_loss: 0.0502 Epoch 99/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0344 - val_loss: 0.0381 Epoch 100/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0220 - val_loss: 0.0319 Epoch 101/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0171 - val_loss: 0.0457 Epoch 102/200 195/195 [==============================] - 0s 300us/sample - loss: 0.0191 - val_loss: 0.0282 Epoch 103/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0106 - val_loss: 0.0277 Epoch 104/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0098 - val_loss: 0.0221 Epoch 105/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0093 - val_loss: 0.0189 Epoch 106/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0079 - val_loss: 0.0203 Epoch 107/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0064 - val_loss: 0.0201 Epoch 108/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0044 - val_loss: 0.0166 Epoch 109/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0045 - val_loss: 0.0178 Epoch 110/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0040 - val_loss: 0.0184 Epoch 111/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0036 - val_loss: 0.0145 Epoch 112/200 195/195 [==============================] - 0s 275us/sample - loss: 0.0031 - val_loss: 0.0167 Epoch 113/200 195/195 [==============================] - 0s 246us/sample - loss: 0.0037 - val_loss: 0.0166 Epoch 114/200 195/195 [==============================] - 0s 270us/sample - loss: 0.0044 - val_loss: 0.0155 Epoch 115/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0051 - val_loss: 0.0175 Epoch 116/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0056 - val_loss: 0.0161 Epoch 117/200 195/195 [==============================] - 0s 271us/sample - loss: 0.0037 - val_loss: 0.0161 Epoch 118/200 195/195 [==============================] - 0s 267us/sample - loss: 0.0046 - val_loss: 0.0135 Epoch 119/200 195/195 [==============================] - 0s 265us/sample - loss: 0.0032 - val_loss: 0.0146 Epoch 120/200 195/195 [==============================] - 0s 251us/sample - loss: 0.0028 - val_loss: 0.0155 Epoch 121/200 195/195 [==============================] - 0s 286us/sample - loss: 0.0026 - val_loss: 0.0175 Epoch 122/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0023 - val_loss: 0.0135 Epoch 123/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0017 - val_loss: 0.0138 Epoch 124/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0023 - val_loss: 0.0138 Epoch 125/200 195/195 [==============================] - 0s 272us/sample - loss: 0.0034 - val_loss: 0.0162 Epoch 126/200 195/195 [==============================] - 0s 244us/sample - loss: 0.0059 - val_loss: 0.0118 Epoch 127/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0057 - val_loss: 0.0203 Epoch 128/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0070 - val_loss: 0.0203 Epoch 129/200 195/195 [==============================] - 0s 302us/sample - loss: 0.0115 - val_loss: 0.0151 Epoch 130/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0090 - val_loss: 0.0158 Epoch 131/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0062 - val_loss: 0.0183 Epoch 132/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0047 - val_loss: 0.0147 Epoch 133/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0066 - val_loss: 0.0185 Epoch 134/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0065 - val_loss: 0.0164 Epoch 135/200 195/195 [==============================] - 0s 266us/sample - loss: 0.0058 - val_loss: 0.0187 Epoch 136/200 195/195 [==============================] - 0s 250us/sample - loss: 0.0047 - val_loss: 0.0159 Epoch 137/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0051 - val_loss: 0.0194 Epoch 138/200 195/195 [==============================] - 0s 277us/sample - loss: 0.0029 - val_loss: 0.0148 Epoch 139/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0051 - val_loss: 0.0174 Epoch 140/200 195/195 [==============================] - 0s 290us/sample - loss: 0.0041 - val_loss: 0.0140 Epoch 141/200 195/195 [==============================] - 0s 241us/sample - loss: 0.0030 - val_loss: 0.0176 Epoch 142/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0032 - val_loss: 0.0140 Epoch 143/200 195/195 [==============================] - 0s 269us/sample - loss: 0.0054 - val_loss: 0.0159 Epoch 144/200 195/195 [==============================] - 0s 274us/sample - loss: 0.0031 - val_loss: 0.0171 Epoch 145/200 195/195 [==============================] - 0s 256us/sample - loss: 0.0032 - val_loss: 0.0137 Epoch 146/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0024 - val_loss: 0.0170 Epoch 147/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0020 - val_loss: 0.0172 Epoch 148/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0015 - val_loss: 0.0136 Epoch 149/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0018 - val_loss: 0.0139 Epoch 150/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0019 - val_loss: 0.0147 Epoch 151/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0015 - val_loss: 0.0135 Epoch 152/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0013 - val_loss: 0.0154 Epoch 153/200 195/195 [==============================] - 0s 276us/sample - loss: 0.0014 - val_loss: 0.0140 Epoch 154/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0013 - val_loss: 0.0135 Epoch 155/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0016 - val_loss: 0.0140 Epoch 156/200 195/195 [==============================] - 0s 269us/sample - loss: 0.0017 - val_loss: 0.0133 Epoch 157/200 195/195 [==============================] - 0s 275us/sample - loss: 0.0016 - val_loss: 0.0167 Epoch 158/200 195/195 [==============================] - 0s 241us/sample - loss: 0.0019 - val_loss: 0.0116 Epoch 159/200 195/195 [==============================] - 0s 262us/sample - loss: 0.0026 - val_loss: 0.0164 Epoch 160/200 195/195 [==============================] - 0s 296us/sample - loss: 0.0023 - val_loss: 0.0129 Epoch 161/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0019 - val_loss: 0.0168 Epoch 162/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0016 - val_loss: 0.0135 Epoch 163/200 195/195 [==============================] - 0s 266us/sample - loss: 0.0012 - val_loss: 0.0141 Epoch 164/200 195/195 [==============================] - 0s 252us/sample - loss: 0.0013 - val_loss: 0.0146 Epoch 165/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0019 - val_loss: 0.0119 Epoch 166/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0030 - val_loss: 0.0168 Epoch 167/200 195/195 [==============================] - 0s 251us/sample - loss: 0.0028 - val_loss: 0.0150 Epoch 168/200 195/195 [==============================] - 0s 291us/sample - loss: 0.0027 - val_loss: 0.0142 Epoch 169/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0020 - val_loss: 0.0154 Epoch 170/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0016 - val_loss: 0.0160 Epoch 171/200 195/195 [==============================] - 0s 268us/sample - loss: 0.0013 - val_loss: 0.0143 Epoch 172/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0011 - val_loss: 0.0150 Epoch 173/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0010 - val_loss: 0.0151 Epoch 174/200 195/195 [==============================] - 0s 270us/sample - loss: 0.0011 - val_loss: 0.0157 Epoch 175/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0014 - val_loss: 0.0144 Epoch 176/200 195/195 [==============================] - 0s 258us/sample - loss: 7.5557e-04 - val_loss: 0.0141 Epoch 177/200 195/195 [==============================] - 0s 257us/sample - loss: 0.0010 - val_loss: 0.0142 Epoch 178/200 195/195 [==============================] - 0s 253us/sample - loss: 8.8674e-04 - val_loss: 0.0148 Epoch 179/200 195/195 [==============================] - 0s 305us/sample - loss: 0.0014 - val_loss: 0.0145 Epoch 180/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0017 - val_loss: 0.0150 Epoch 181/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0018 - val_loss: 0.0142 Epoch 182/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0012 - val_loss: 0.0142 Epoch 183/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0022 - val_loss: 0.0156 Epoch 184/200 195/195 [==============================] - 0s 253us/sample - loss: 0.0023 - val_loss: 0.0134 Epoch 185/200 195/195 [==============================] - 0s 258us/sample - loss: 0.0015 - val_loss: 0.0146 Epoch 186/200 195/195 [==============================] - 0s 254us/sample - loss: 0.0016 - val_loss: 0.0133 Epoch 187/200 195/195 [==============================] - 0s 282us/sample - loss: 0.0032 - val_loss: 0.0164 Epoch 188/200 195/195 [==============================] - 0s 244us/sample - loss: 0.0021 - val_loss: 0.0127 Epoch 189/200 195/195 [==============================] - 0s 251us/sample - loss: 0.0019 - val_loss: 0.0127 Epoch 190/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0025 - val_loss: 0.0199 Epoch 191/200 195/195 [==============================] - 0s 249us/sample - loss: 0.0059 - val_loss: 0.0152 Epoch 192/200 195/195 [==============================] - 0s 261us/sample - loss: 0.0067 - val_loss: 0.0126 Epoch 193/200 195/195 [==============================] - 0s 263us/sample - loss: 0.0145 - val_loss: 0.0203 Epoch 194/200 195/195 [==============================] - 0s 260us/sample - loss: 0.0101 - val_loss: 0.0149 Epoch 195/200 195/195 [==============================] - 0s 259us/sample - loss: 0.0060 - val_loss: 0.0205 Epoch 196/200 195/195 [==============================] - 0s 280us/sample - loss: 0.0060 - val_loss: 0.0129 Epoch 197/200 195/195 [==============================] - 0s 255us/sample - loss: 0.0048 - val_loss: 0.0243 Epoch 198/200 195/195 [==============================] - 0s 281us/sample - loss: 0.0042 - val_loss: 0.0199 Epoch 199/200 195/195 [==============================] - 0s 289us/sample - loss: 0.0044 - val_loss: 0.0192 Epoch 200/200 195/195 [==============================] - 0s 273us/sample - loss: 0.0041 - val_loss: 0.0202 # plot some data plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () plt . show () # One-step forecast using true targets outputs = model . predict ( X ) print ( outputs . shape ) predictions = outputs [:, 0 ] plt . plot ( Y , label = 'targets' ) plt . plot ( predictions , label = 'predictions' ) plt . title ( \"many-to-one RNN\" ) plt . legend () plt . show () (390, 1) # Multi-step forecast forecast = [] input_ = X [ - N // 2 ] while len ( forecast ) < len ( Y [ - N // 2 :]): # Reshape the input_ to N x T x D f = model . predict ( input_ . reshape ( 1 , T , 1 ))[ 0 , 0 ] forecast . append ( f ) # make a new input with the latest forecast input_ = np . roll ( input_ , - 1 ) input_ [ - 1 ] = f plt . plot ( Y [ - N // 2 :], label = 'targets' ) plt . plot ( forecast , label = 'forecast' ) plt . title ( \"RNN Forecast\" ) plt . legend () plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"LSTM Nonlinear"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Long_Distance/","text":"================ by Jawad Haider Long Distance Long Distance \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.2.0-rc2 # More imports from tensorflow.keras.layers import Input , SimpleRNN , GRU , LSTM , Dense , Flatten , GlobalMaxPool1D from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt ### build the dataset # This is a nonlinear AND long-distance dataset # (Actually, we will test long-distance vs. short-distance patterns) # Start with a small T and increase it later T = 10 D = 1 X = [] Y = [] def get_label ( x , i1 , i2 , i3 ): # x = sequence if x [ i1 ] < 0 and x [ i2 ] < 0 and x [ i3 ] < 0 : return 1 if x [ i1 ] < 0 and x [ i2 ] > 0 and x [ i3 ] > 0 : return 1 if x [ i1 ] > 0 and x [ i2 ] < 0 and x [ i3 ] > 0 : return 1 if x [ i1 ] > 0 and x [ i2 ] > 0 and x [ i3 ] < 0 : return 1 return 0 for t in range ( 5000 ): x = np . random . randn ( T ) X . append ( x ) y = get_label ( x , - 1 , - 2 , - 3 ) # short distance # y = get_label(x, 0, 1, 2) # long distance Y . append ( y ) X = np . array ( X ) Y = np . array ( Y ) N = len ( X ) # Try a linear model first - note: it is classification now! i = Input ( shape = ( T ,)) x = Dense ( 1 , activation = 'sigmoid' )( i ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the network r = model . fit ( X , Y , epochs = 100 , validation_split = 0.5 , ) Epoch 1/100 79/79 [==============================] - 0s 5ms/step - loss: 0.7831 - accuracy: 0.5064 - val_loss: 0.7084 - val_accuracy: 0.4972 Epoch 2/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.5164 - val_loss: 0.6992 - val_accuracy: 0.4860 Epoch 3/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5176 - val_loss: 0.7003 - val_accuracy: 0.4976 Epoch 4/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5172 - val_loss: 0.6995 - val_accuracy: 0.4948 Epoch 5/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5176 - val_loss: 0.6989 - val_accuracy: 0.4968 Epoch 6/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5204 - val_loss: 0.7013 - val_accuracy: 0.4920 Epoch 7/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.5172 - val_loss: 0.7019 - val_accuracy: 0.5088 Epoch 8/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6951 - accuracy: 0.5204 - val_loss: 0.7011 - val_accuracy: 0.4964 Epoch 9/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.7002 - val_accuracy: 0.4916 Epoch 10/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5136 - val_loss: 0.7031 - val_accuracy: 0.4936 Epoch 11/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6936 - accuracy: 0.5132 - val_loss: 0.7009 - val_accuracy: 0.4944 Epoch 12/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5256 - val_loss: 0.7014 - val_accuracy: 0.4968 Epoch 13/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5232 - val_loss: 0.6995 - val_accuracy: 0.4952 Epoch 14/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5268 - val_loss: 0.7004 - val_accuracy: 0.4848 Epoch 15/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5276 - val_loss: 0.6979 - val_accuracy: 0.5020 Epoch 16/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.5176 - val_loss: 0.6960 - val_accuracy: 0.5000 Epoch 17/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5184 - val_loss: 0.6990 - val_accuracy: 0.4872 Epoch 18/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5188 - val_loss: 0.6988 - val_accuracy: 0.4976 Epoch 19/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5132 - val_loss: 0.7003 - val_accuracy: 0.5016 Epoch 20/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5252 - val_loss: 0.7000 - val_accuracy: 0.4964 Epoch 21/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5216 - val_loss: 0.7006 - val_accuracy: 0.4952 Epoch 22/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5116 - val_loss: 0.7036 - val_accuracy: 0.4908 Epoch 23/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5136 - val_loss: 0.7015 - val_accuracy: 0.4984 Epoch 24/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5348 - val_loss: 0.7017 - val_accuracy: 0.4928 Epoch 25/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5080 - val_loss: 0.7023 - val_accuracy: 0.4968 Epoch 26/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5120 - val_loss: 0.7033 - val_accuracy: 0.4876 Epoch 27/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5308 - val_loss: 0.6994 - val_accuracy: 0.4924 Epoch 28/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5260 - val_loss: 0.7000 - val_accuracy: 0.4896 Epoch 29/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5240 - val_loss: 0.7005 - val_accuracy: 0.4980 Epoch 30/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5204 - val_loss: 0.6998 - val_accuracy: 0.4880 Epoch 31/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5216 - val_loss: 0.7035 - val_accuracy: 0.4940 Epoch 32/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5168 - val_loss: 0.7004 - val_accuracy: 0.5016 Epoch 33/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5228 - val_loss: 0.6990 - val_accuracy: 0.4928 Epoch 34/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5128 - val_loss: 0.6978 - val_accuracy: 0.4964 Epoch 35/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5136 - val_loss: 0.6975 - val_accuracy: 0.4908 Epoch 36/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5208 - val_loss: 0.6988 - val_accuracy: 0.4892 Epoch 37/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5240 - val_loss: 0.7005 - val_accuracy: 0.5012 Epoch 38/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6949 - accuracy: 0.5268 - val_loss: 0.7008 - val_accuracy: 0.4780 Epoch 39/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5104 - val_loss: 0.6999 - val_accuracy: 0.4956 Epoch 40/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5160 - val_loss: 0.7001 - val_accuracy: 0.4920 Epoch 41/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5220 - val_loss: 0.7002 - val_accuracy: 0.4952 Epoch 42/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5180 - val_loss: 0.6994 - val_accuracy: 0.5044 Epoch 43/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5176 - val_loss: 0.6996 - val_accuracy: 0.4912 Epoch 44/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5172 - val_loss: 0.7004 - val_accuracy: 0.4884 Epoch 45/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5232 - val_loss: 0.6996 - val_accuracy: 0.4980 Epoch 46/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6937 - accuracy: 0.5224 - val_loss: 0.7002 - val_accuracy: 0.4916 Epoch 47/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5192 - val_loss: 0.6982 - val_accuracy: 0.4948 Epoch 48/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5124 - val_loss: 0.7022 - val_accuracy: 0.4932 Epoch 49/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5172 - val_loss: 0.6997 - val_accuracy: 0.5000 Epoch 50/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5204 - val_loss: 0.6997 - val_accuracy: 0.4904 Epoch 51/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5316 - val_loss: 0.6997 - val_accuracy: 0.4896 Epoch 52/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5196 - val_loss: 0.6992 - val_accuracy: 0.5024 Epoch 53/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5188 - val_loss: 0.6993 - val_accuracy: 0.4888 Epoch 54/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5160 - val_loss: 0.7029 - val_accuracy: 0.4912 Epoch 55/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5208 - val_loss: 0.7008 - val_accuracy: 0.4940 Epoch 56/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5292 - val_loss: 0.7020 - val_accuracy: 0.4988 Epoch 57/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5220 - val_loss: 0.7004 - val_accuracy: 0.4916 Epoch 58/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5192 - val_loss: 0.6998 - val_accuracy: 0.4932 Epoch 59/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5184 - val_loss: 0.6988 - val_accuracy: 0.4956 Epoch 60/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5176 - val_loss: 0.6993 - val_accuracy: 0.4988 Epoch 61/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5308 - val_loss: 0.6986 - val_accuracy: 0.4944 Epoch 62/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6936 - accuracy: 0.5248 - val_loss: 0.6982 - val_accuracy: 0.4892 Epoch 63/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6935 - accuracy: 0.5100 - val_loss: 0.7012 - val_accuracy: 0.4868 Epoch 64/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5208 - val_loss: 0.7002 - val_accuracy: 0.4976 Epoch 65/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5244 - val_loss: 0.6988 - val_accuracy: 0.4948 Epoch 66/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.5108 - val_loss: 0.6994 - val_accuracy: 0.4960 Epoch 67/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5136 - val_loss: 0.7010 - val_accuracy: 0.4820 Epoch 68/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5252 - val_loss: 0.7020 - val_accuracy: 0.4888 Epoch 69/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5240 - val_loss: 0.6987 - val_accuracy: 0.4892 Epoch 70/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5232 - val_loss: 0.6988 - val_accuracy: 0.4980 Epoch 71/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6936 - accuracy: 0.5124 - val_loss: 0.7007 - val_accuracy: 0.4984 Epoch 72/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6943 - accuracy: 0.5252 - val_loss: 0.7007 - val_accuracy: 0.4888 Epoch 73/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5180 - val_loss: 0.6979 - val_accuracy: 0.4968 Epoch 74/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5124 - val_loss: 0.6989 - val_accuracy: 0.4928 Epoch 75/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6942 - accuracy: 0.5176 - val_loss: 0.6995 - val_accuracy: 0.4864 Epoch 76/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5344 - val_loss: 0.7003 - val_accuracy: 0.4940 Epoch 77/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5212 - val_loss: 0.6983 - val_accuracy: 0.4912 Epoch 78/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5256 - val_loss: 0.7017 - val_accuracy: 0.4972 Epoch 79/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6935 - accuracy: 0.5220 - val_loss: 0.7010 - val_accuracy: 0.4900 Epoch 80/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5256 - val_loss: 0.7012 - val_accuracy: 0.4932 Epoch 81/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5332 - val_loss: 0.7000 - val_accuracy: 0.4980 Epoch 82/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5216 - val_loss: 0.6996 - val_accuracy: 0.5052 Epoch 83/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.5156 - val_loss: 0.7002 - val_accuracy: 0.5020 Epoch 84/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5164 - val_loss: 0.6986 - val_accuracy: 0.4940 Epoch 85/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5152 - val_loss: 0.7002 - val_accuracy: 0.5008 Epoch 86/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5124 - val_loss: 0.6978 - val_accuracy: 0.4996 Epoch 87/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5268 - val_loss: 0.7010 - val_accuracy: 0.4960 Epoch 88/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.7032 - val_accuracy: 0.4924 Epoch 89/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5140 - val_loss: 0.6984 - val_accuracy: 0.4972 Epoch 90/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5220 - val_loss: 0.7006 - val_accuracy: 0.4844 Epoch 91/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5252 - val_loss: 0.7019 - val_accuracy: 0.4912 Epoch 92/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5192 - val_loss: 0.6996 - val_accuracy: 0.4928 Epoch 93/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5052 - val_loss: 0.6996 - val_accuracy: 0.4900 Epoch 94/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5180 - val_loss: 0.7006 - val_accuracy: 0.4960 Epoch 95/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5216 - val_loss: 0.6973 - val_accuracy: 0.4960 Epoch 96/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.5172 - val_loss: 0.7002 - val_accuracy: 0.4872 Epoch 97/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5200 - val_loss: 0.7001 - val_accuracy: 0.4948 Epoch 98/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5272 - val_loss: 0.7023 - val_accuracy: 0.4972 Epoch 99/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5164 - val_loss: 0.7003 - val_accuracy: 0.4908 Epoch 100/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5124 - val_loss: 0.6988 - val_accuracy: 0.5004 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec38271438> # Plot the accuracy too - should be around 50% plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec382777b8> # Now try a simple RNN inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 # x = LSTM(5)(i) x = SimpleRNN ( 5 )( i ) # x = GRU(5)(i) # method 2 # x = LSTM(5, return_sequences=True)(i) # x = GlobalMaxPool1D()(x) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , # optimizer='rmsprop', # optimizer='adam', optimizer = Adam ( lr = 0.01 ), # optimizer=SGD(lr=0.1, momentum=0.9), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 12ms/step - loss: 0.6980 - accuracy: 0.5232 - val_loss: 0.6856 - val_accuracy: 0.5432 Epoch 2/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6726 - accuracy: 0.5752 - val_loss: 0.6726 - val_accuracy: 0.5756 Epoch 3/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6472 - accuracy: 0.6276 - val_loss: 0.6279 - val_accuracy: 0.6808 Epoch 4/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5874 - accuracy: 0.7200 - val_loss: 0.5298 - val_accuracy: 0.7960 Epoch 5/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4572 - accuracy: 0.8532 - val_loss: 0.3922 - val_accuracy: 0.8800 Epoch 6/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3494 - accuracy: 0.9056 - val_loss: 0.3111 - val_accuracy: 0.9036 Epoch 7/200 79/79 [==============================] - 1s 10ms/step - loss: 0.2770 - accuracy: 0.9252 - val_loss: 0.2596 - val_accuracy: 0.9248 Epoch 8/200 79/79 [==============================] - 1s 11ms/step - loss: 0.2400 - accuracy: 0.9316 - val_loss: 0.2448 - val_accuracy: 0.9172 Epoch 9/200 79/79 [==============================] - 1s 10ms/step - loss: 0.2085 - accuracy: 0.9356 - val_loss: 0.2131 - val_accuracy: 0.9280 Epoch 10/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1890 - accuracy: 0.9444 - val_loss: 0.1918 - val_accuracy: 0.9376 Epoch 11/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1724 - accuracy: 0.9464 - val_loss: 0.1774 - val_accuracy: 0.9452 Epoch 12/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1559 - accuracy: 0.9528 - val_loss: 0.1733 - val_accuracy: 0.9432 Epoch 13/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1501 - accuracy: 0.9556 - val_loss: 0.1589 - val_accuracy: 0.9476 Epoch 14/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1448 - accuracy: 0.9572 - val_loss: 0.1582 - val_accuracy: 0.9480 Epoch 15/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1415 - accuracy: 0.9544 - val_loss: 0.1503 - val_accuracy: 0.9520 Epoch 16/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1274 - accuracy: 0.9612 - val_loss: 0.1420 - val_accuracy: 0.9484 Epoch 17/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1220 - accuracy: 0.9620 - val_loss: 0.1324 - val_accuracy: 0.9552 Epoch 18/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1146 - accuracy: 0.9632 - val_loss: 0.1278 - val_accuracy: 0.9612 Epoch 19/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1102 - accuracy: 0.9708 - val_loss: 0.1237 - val_accuracy: 0.9644 Epoch 20/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1083 - accuracy: 0.9620 - val_loss: 0.1209 - val_accuracy: 0.9552 Epoch 21/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1070 - accuracy: 0.9664 - val_loss: 0.1181 - val_accuracy: 0.9608 Epoch 22/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1024 - accuracy: 0.9688 - val_loss: 0.1194 - val_accuracy: 0.9644 Epoch 23/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0990 - accuracy: 0.9716 - val_loss: 0.1166 - val_accuracy: 0.9628 Epoch 24/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0969 - accuracy: 0.9696 - val_loss: 0.1108 - val_accuracy: 0.9584 Epoch 25/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0918 - accuracy: 0.9700 - val_loss: 0.1077 - val_accuracy: 0.9652 Epoch 26/200 79/79 [==============================] - 1s 12ms/step - loss: 0.0926 - accuracy: 0.9732 - val_loss: 0.1160 - val_accuracy: 0.9560 Epoch 27/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0943 - accuracy: 0.9712 - val_loss: 0.1010 - val_accuracy: 0.9676 Epoch 28/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0866 - accuracy: 0.9736 - val_loss: 0.1106 - val_accuracy: 0.9672 Epoch 29/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0881 - accuracy: 0.9768 - val_loss: 0.1083 - val_accuracy: 0.9664 Epoch 30/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0850 - accuracy: 0.9728 - val_loss: 0.1055 - val_accuracy: 0.9656 Epoch 31/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0824 - accuracy: 0.9736 - val_loss: 0.1030 - val_accuracy: 0.9648 Epoch 32/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0801 - accuracy: 0.9772 - val_loss: 0.1006 - val_accuracy: 0.9660 Epoch 33/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0801 - accuracy: 0.9740 - val_loss: 0.0912 - val_accuracy: 0.9712 Epoch 34/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0767 - accuracy: 0.9768 - val_loss: 0.0899 - val_accuracy: 0.9688 Epoch 35/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0783 - accuracy: 0.9780 - val_loss: 0.0996 - val_accuracy: 0.9656 Epoch 36/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0740 - accuracy: 0.9776 - val_loss: 0.0913 - val_accuracy: 0.9704 Epoch 37/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0731 - accuracy: 0.9796 - val_loss: 0.0913 - val_accuracy: 0.9712 Epoch 38/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0728 - accuracy: 0.9776 - val_loss: 0.0913 - val_accuracy: 0.9684 Epoch 39/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0749 - accuracy: 0.9776 - val_loss: 0.0900 - val_accuracy: 0.9724 Epoch 40/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0743 - accuracy: 0.9780 - val_loss: 0.0903 - val_accuracy: 0.9704 Epoch 41/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0738 - accuracy: 0.9756 - val_loss: 0.0965 - val_accuracy: 0.9696 Epoch 42/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0735 - accuracy: 0.9780 - val_loss: 0.0941 - val_accuracy: 0.9696 Epoch 43/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0684 - accuracy: 0.9768 - val_loss: 0.0841 - val_accuracy: 0.9696 Epoch 44/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0729 - accuracy: 0.9752 - val_loss: 0.0926 - val_accuracy: 0.9696 Epoch 45/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0690 - accuracy: 0.9776 - val_loss: 0.0843 - val_accuracy: 0.9772 Epoch 46/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0671 - accuracy: 0.9800 - val_loss: 0.0897 - val_accuracy: 0.9712 Epoch 47/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0622 - accuracy: 0.9836 - val_loss: 0.0944 - val_accuracy: 0.9680 Epoch 48/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0703 - accuracy: 0.9792 - val_loss: 0.0870 - val_accuracy: 0.9724 Epoch 49/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0728 - accuracy: 0.9796 - val_loss: 0.0820 - val_accuracy: 0.9792 Epoch 50/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0679 - accuracy: 0.9800 - val_loss: 0.0891 - val_accuracy: 0.9752 Epoch 51/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0661 - accuracy: 0.9788 - val_loss: 0.0897 - val_accuracy: 0.9708 Epoch 52/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0649 - accuracy: 0.9792 - val_loss: 0.0850 - val_accuracy: 0.9732 Epoch 53/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0655 - accuracy: 0.9832 - val_loss: 0.0773 - val_accuracy: 0.9740 Epoch 54/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0626 - accuracy: 0.9808 - val_loss: 0.0780 - val_accuracy: 0.9732 Epoch 55/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0593 - accuracy: 0.9832 - val_loss: 0.0804 - val_accuracy: 0.9756 Epoch 56/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0672 - accuracy: 0.9764 - val_loss: 0.0800 - val_accuracy: 0.9728 Epoch 57/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0639 - accuracy: 0.9816 - val_loss: 0.0769 - val_accuracy: 0.9748 Epoch 58/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0602 - accuracy: 0.9844 - val_loss: 0.0737 - val_accuracy: 0.9728 Epoch 59/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0614 - accuracy: 0.9816 - val_loss: 0.0773 - val_accuracy: 0.9768 Epoch 60/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0595 - accuracy: 0.9836 - val_loss: 0.0908 - val_accuracy: 0.9756 Epoch 61/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0625 - accuracy: 0.9832 - val_loss: 0.0692 - val_accuracy: 0.9796 Epoch 62/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0569 - accuracy: 0.9840 - val_loss: 0.0730 - val_accuracy: 0.9768 Epoch 63/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0552 - accuracy: 0.9832 - val_loss: 0.0710 - val_accuracy: 0.9772 Epoch 64/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0561 - accuracy: 0.9836 - val_loss: 0.0686 - val_accuracy: 0.9784 Epoch 65/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0547 - accuracy: 0.9848 - val_loss: 0.0705 - val_accuracy: 0.9800 Epoch 66/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0524 - accuracy: 0.9852 - val_loss: 0.0934 - val_accuracy: 0.9736 Epoch 67/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0548 - accuracy: 0.9840 - val_loss: 0.0762 - val_accuracy: 0.9796 Epoch 68/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0528 - accuracy: 0.9836 - val_loss: 0.0666 - val_accuracy: 0.9816 Epoch 69/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0510 - accuracy: 0.9876 - val_loss: 0.0894 - val_accuracy: 0.9756 Epoch 70/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0501 - accuracy: 0.9856 - val_loss: 0.0691 - val_accuracy: 0.9796 Epoch 71/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0511 - accuracy: 0.9868 - val_loss: 0.0680 - val_accuracy: 0.9792 Epoch 72/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0536 - accuracy: 0.9836 - val_loss: 0.0819 - val_accuracy: 0.9768 Epoch 73/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0553 - accuracy: 0.9816 - val_loss: 0.0653 - val_accuracy: 0.9800 Epoch 74/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0554 - accuracy: 0.9852 - val_loss: 0.0711 - val_accuracy: 0.9812 Epoch 75/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0483 - accuracy: 0.9880 - val_loss: 0.0823 - val_accuracy: 0.9732 Epoch 76/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0512 - accuracy: 0.9852 - val_loss: 0.0901 - val_accuracy: 0.9808 Epoch 77/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0539 - accuracy: 0.9844 - val_loss: 0.0752 - val_accuracy: 0.9792 Epoch 78/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0491 - accuracy: 0.9872 - val_loss: 0.0671 - val_accuracy: 0.9816 Epoch 79/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0471 - accuracy: 0.9876 - val_loss: 0.0702 - val_accuracy: 0.9820 Epoch 80/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0547 - accuracy: 0.9828 - val_loss: 0.0739 - val_accuracy: 0.9812 Epoch 81/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0561 - accuracy: 0.9816 - val_loss: 0.0658 - val_accuracy: 0.9808 Epoch 82/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0646 - accuracy: 0.9856 - val_loss: 0.0728 - val_accuracy: 0.9788 Epoch 83/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0576 - accuracy: 0.9824 - val_loss: 0.0738 - val_accuracy: 0.9772 Epoch 84/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0573 - accuracy: 0.9824 - val_loss: 0.0668 - val_accuracy: 0.9824 Epoch 85/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0428 - accuracy: 0.9896 - val_loss: 0.0827 - val_accuracy: 0.9804 Epoch 86/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0480 - accuracy: 0.9888 - val_loss: 0.0679 - val_accuracy: 0.9804 Epoch 87/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0436 - accuracy: 0.9888 - val_loss: 0.0677 - val_accuracy: 0.9848 Epoch 88/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0465 - accuracy: 0.9884 - val_loss: 0.0693 - val_accuracy: 0.9804 Epoch 89/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0417 - accuracy: 0.9896 - val_loss: 0.0731 - val_accuracy: 0.9784 Epoch 90/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0465 - accuracy: 0.9868 - val_loss: 0.0693 - val_accuracy: 0.9840 Epoch 91/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0423 - accuracy: 0.9904 - val_loss: 0.0667 - val_accuracy: 0.9840 Epoch 92/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0436 - accuracy: 0.9892 - val_loss: 0.0683 - val_accuracy: 0.9824 Epoch 93/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0470 - accuracy: 0.9864 - val_loss: 0.0644 - val_accuracy: 0.9824 Epoch 94/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0460 - accuracy: 0.9868 - val_loss: 0.0711 - val_accuracy: 0.9788 Epoch 95/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0398 - accuracy: 0.9896 - val_loss: 0.0688 - val_accuracy: 0.9784 Epoch 96/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0410 - accuracy: 0.9868 - val_loss: 0.0733 - val_accuracy: 0.9836 Epoch 97/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0455 - accuracy: 0.9860 - val_loss: 0.0647 - val_accuracy: 0.9848 Epoch 98/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0390 - accuracy: 0.9888 - val_loss: 0.0671 - val_accuracy: 0.9804 Epoch 99/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0387 - accuracy: 0.9884 - val_loss: 0.0686 - val_accuracy: 0.9812 Epoch 100/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0396 - accuracy: 0.9888 - val_loss: 0.0648 - val_accuracy: 0.9800 Epoch 101/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0384 - accuracy: 0.9908 - val_loss: 0.0626 - val_accuracy: 0.9832 Epoch 102/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0415 - accuracy: 0.9884 - val_loss: 0.0720 - val_accuracy: 0.9792 Epoch 103/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0401 - accuracy: 0.9852 - val_loss: 0.0676 - val_accuracy: 0.9820 Epoch 104/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0396 - accuracy: 0.9900 - val_loss: 0.0642 - val_accuracy: 0.9832 Epoch 105/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0422 - accuracy: 0.9872 - val_loss: 0.0724 - val_accuracy: 0.9856 Epoch 106/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0429 - accuracy: 0.9884 - val_loss: 0.0810 - val_accuracy: 0.9828 Epoch 107/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0385 - accuracy: 0.9888 - val_loss: 0.0699 - val_accuracy: 0.9812 Epoch 108/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0494 - accuracy: 0.9868 - val_loss: 0.0689 - val_accuracy: 0.9832 Epoch 109/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0383 - accuracy: 0.9872 - val_loss: 0.0628 - val_accuracy: 0.9820 Epoch 110/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0373 - accuracy: 0.9896 - val_loss: 0.0707 - val_accuracy: 0.9768 Epoch 111/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0348 - accuracy: 0.9892 - val_loss: 0.0613 - val_accuracy: 0.9852 Epoch 112/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0375 - accuracy: 0.9872 - val_loss: 0.0828 - val_accuracy: 0.9820 Epoch 113/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0419 - accuracy: 0.9904 - val_loss: 0.0732 - val_accuracy: 0.9828 Epoch 114/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0386 - accuracy: 0.9868 - val_loss: 0.0676 - val_accuracy: 0.9816 Epoch 115/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0357 - accuracy: 0.9904 - val_loss: 0.0647 - val_accuracy: 0.9828 Epoch 116/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0346 - accuracy: 0.9912 - val_loss: 0.0648 - val_accuracy: 0.9816 Epoch 117/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0339 - accuracy: 0.9892 - val_loss: 0.0679 - val_accuracy: 0.9816 Epoch 118/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0320 - accuracy: 0.9920 - val_loss: 0.0682 - val_accuracy: 0.9828 Epoch 119/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0329 - accuracy: 0.9916 - val_loss: 0.0638 - val_accuracy: 0.9840 Epoch 120/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0324 - accuracy: 0.9904 - val_loss: 0.0648 - val_accuracy: 0.9844 Epoch 121/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0312 - accuracy: 0.9924 - val_loss: 0.0639 - val_accuracy: 0.9824 Epoch 122/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0328 - accuracy: 0.9884 - val_loss: 0.0642 - val_accuracy: 0.9836 Epoch 123/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0380 - accuracy: 0.9888 - val_loss: 0.0727 - val_accuracy: 0.9840 Epoch 124/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0513 - accuracy: 0.9864 - val_loss: 0.0668 - val_accuracy: 0.9844 Epoch 125/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0475 - accuracy: 0.9884 - val_loss: 0.0730 - val_accuracy: 0.9808 Epoch 126/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0350 - accuracy: 0.9884 - val_loss: 0.0622 - val_accuracy: 0.9848 Epoch 127/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0326 - accuracy: 0.9896 - val_loss: 0.0797 - val_accuracy: 0.9800 Epoch 128/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0371 - accuracy: 0.9908 - val_loss: 0.0654 - val_accuracy: 0.9812 Epoch 129/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0304 - accuracy: 0.9916 - val_loss: 0.0640 - val_accuracy: 0.9844 Epoch 130/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0319 - accuracy: 0.9908 - val_loss: 0.0760 - val_accuracy: 0.9820 Epoch 131/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0325 - accuracy: 0.9904 - val_loss: 0.0625 - val_accuracy: 0.9840 Epoch 132/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0307 - accuracy: 0.9920 - val_loss: 0.0597 - val_accuracy: 0.9848 Epoch 133/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0355 - accuracy: 0.9904 - val_loss: 0.0628 - val_accuracy: 0.9796 Epoch 134/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0347 - accuracy: 0.9884 - val_loss: 0.0627 - val_accuracy: 0.9848 Epoch 135/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0478 - accuracy: 0.9848 - val_loss: 0.0708 - val_accuracy: 0.9832 Epoch 136/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0347 - accuracy: 0.9880 - val_loss: 0.0623 - val_accuracy: 0.9844 Epoch 137/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0284 - accuracy: 0.9928 - val_loss: 0.0598 - val_accuracy: 0.9852 Epoch 138/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0464 - accuracy: 0.9920 - val_loss: 0.0620 - val_accuracy: 0.9824 Epoch 139/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0402 - accuracy: 0.9852 - val_loss: 0.0611 - val_accuracy: 0.9852 Epoch 140/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0291 - accuracy: 0.9912 - val_loss: 0.0594 - val_accuracy: 0.9836 Epoch 141/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0353 - accuracy: 0.9892 - val_loss: 0.0697 - val_accuracy: 0.9828 Epoch 142/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0354 - accuracy: 0.9912 - val_loss: 0.0577 - val_accuracy: 0.9844 Epoch 143/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0330 - accuracy: 0.9908 - val_loss: 0.0756 - val_accuracy: 0.9800 Epoch 144/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0322 - accuracy: 0.9936 - val_loss: 0.0530 - val_accuracy: 0.9848 Epoch 145/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0274 - accuracy: 0.9924 - val_loss: 0.0588 - val_accuracy: 0.9872 Epoch 146/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0265 - accuracy: 0.9924 - val_loss: 0.0548 - val_accuracy: 0.9848 Epoch 147/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0268 - accuracy: 0.9924 - val_loss: 0.0554 - val_accuracy: 0.9840 Epoch 148/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0255 - accuracy: 0.9936 - val_loss: 0.0527 - val_accuracy: 0.9868 Epoch 149/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0324 - accuracy: 0.9920 - val_loss: 0.0594 - val_accuracy: 0.9856 Epoch 150/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0282 - accuracy: 0.9916 - val_loss: 0.0602 - val_accuracy: 0.9844 Epoch 151/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0265 - accuracy: 0.9924 - val_loss: 0.0558 - val_accuracy: 0.9864 Epoch 152/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0275 - accuracy: 0.9916 - val_loss: 0.0588 - val_accuracy: 0.9860 Epoch 153/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0275 - accuracy: 0.9928 - val_loss: 0.0579 - val_accuracy: 0.9864 Epoch 154/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0264 - accuracy: 0.9904 - val_loss: 0.0617 - val_accuracy: 0.9860 Epoch 155/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0290 - accuracy: 0.9904 - val_loss: 0.0514 - val_accuracy: 0.9864 Epoch 156/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0293 - accuracy: 0.9920 - val_loss: 0.0588 - val_accuracy: 0.9840 Epoch 157/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0265 - accuracy: 0.9940 - val_loss: 0.0579 - val_accuracy: 0.9880 Epoch 158/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0252 - accuracy: 0.9928 - val_loss: 0.0725 - val_accuracy: 0.9812 Epoch 159/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0300 - accuracy: 0.9888 - val_loss: 0.0600 - val_accuracy: 0.9844 Epoch 160/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0244 - accuracy: 0.9920 - val_loss: 0.0549 - val_accuracy: 0.9840 Epoch 161/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0264 - accuracy: 0.9928 - val_loss: 0.0641 - val_accuracy: 0.9840 Epoch 162/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0262 - accuracy: 0.9912 - val_loss: 0.0681 - val_accuracy: 0.9784 Epoch 163/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.0571 - val_accuracy: 0.9852 Epoch 164/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0226 - accuracy: 0.9944 - val_loss: 0.0585 - val_accuracy: 0.9836 Epoch 165/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0294 - accuracy: 0.9904 - val_loss: 0.0614 - val_accuracy: 0.9836 Epoch 166/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0302 - accuracy: 0.9904 - val_loss: 0.0516 - val_accuracy: 0.9868 Epoch 167/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0314 - accuracy: 0.9916 - val_loss: 0.0550 - val_accuracy: 0.9824 Epoch 168/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0274 - accuracy: 0.9896 - val_loss: 0.0558 - val_accuracy: 0.9828 Epoch 169/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0237 - accuracy: 0.9940 - val_loss: 0.0630 - val_accuracy: 0.9836 Epoch 170/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0252 - accuracy: 0.9912 - val_loss: 0.0737 - val_accuracy: 0.9816 Epoch 171/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.0523 - val_accuracy: 0.9872 Epoch 172/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0277 - accuracy: 0.9920 - val_loss: 0.0562 - val_accuracy: 0.9844 Epoch 173/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0292 - accuracy: 0.9924 - val_loss: 0.0613 - val_accuracy: 0.9852 Epoch 174/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0265 - accuracy: 0.9932 - val_loss: 0.0604 - val_accuracy: 0.9864 Epoch 175/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0280 - accuracy: 0.9940 - val_loss: 0.0560 - val_accuracy: 0.9856 Epoch 176/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0383 - accuracy: 0.9912 - val_loss: 0.0514 - val_accuracy: 0.9852 Epoch 177/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0347 - accuracy: 0.9908 - val_loss: 0.0503 - val_accuracy: 0.9864 Epoch 178/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0213 - accuracy: 0.9948 - val_loss: 0.0522 - val_accuracy: 0.9844 Epoch 179/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0241 - accuracy: 0.9932 - val_loss: 0.0493 - val_accuracy: 0.9856 Epoch 180/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.0493 - val_accuracy: 0.9876 Epoch 181/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0222 - accuracy: 0.9948 - val_loss: 0.0476 - val_accuracy: 0.9860 Epoch 182/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0272 - accuracy: 0.9904 - val_loss: 0.0577 - val_accuracy: 0.9868 Epoch 183/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0304 - accuracy: 0.9912 - val_loss: 0.0785 - val_accuracy: 0.9840 Epoch 184/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0376 - accuracy: 0.9900 - val_loss: 0.0596 - val_accuracy: 0.9848 Epoch 185/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0244 - accuracy: 0.9944 - val_loss: 0.0520 - val_accuracy: 0.9872 Epoch 186/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0525 - val_accuracy: 0.9840 Epoch 187/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0189 - accuracy: 0.9952 - val_loss: 0.0493 - val_accuracy: 0.9844 Epoch 188/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0193 - accuracy: 0.9948 - val_loss: 0.0554 - val_accuracy: 0.9852 Epoch 189/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0211 - accuracy: 0.9944 - val_loss: 0.0525 - val_accuracy: 0.9860 Epoch 190/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0201 - accuracy: 0.9948 - val_loss: 0.0558 - val_accuracy: 0.9856 Epoch 191/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0227 - accuracy: 0.9932 - val_loss: 0.0549 - val_accuracy: 0.9888 Epoch 192/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0191 - accuracy: 0.9948 - val_loss: 0.0506 - val_accuracy: 0.9840 Epoch 193/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 0.0536 - val_accuracy: 0.9864 Epoch 194/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0200 - accuracy: 0.9936 - val_loss: 0.0518 - val_accuracy: 0.9888 Epoch 195/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.0470 - val_accuracy: 0.9876 Epoch 196/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0248 - accuracy: 0.9936 - val_loss: 0.0536 - val_accuracy: 0.9868 Epoch 197/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0272 - accuracy: 0.9912 - val_loss: 0.0642 - val_accuracy: 0.9836 Epoch 198/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0264 - accuracy: 0.9912 - val_loss: 0.0448 - val_accuracy: 0.9872 Epoch 199/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0568 - accuracy: 0.9896 - val_loss: 0.0793 - val_accuracy: 0.9816 Epoch 200/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0396 - accuracy: 0.9904 - val_loss: 0.0707 - val_accuracy: 0.9852 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1f781828> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec380f10b8> # Now change to the long distance problem # Start with a small T and increase it later T = 10 D = 1 X = [] Y = [] for t in range ( 5000 ): x = np . random . randn ( T ) X . append ( x ) y = get_label ( x , 0 , 1 , 2 ) # long distance Y . append ( y ) X = np . array ( X ) Y = np . array ( Y ) N = len ( X ) # Now test our Simple RNN again inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = SimpleRNN ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 12ms/step - loss: 0.7009 - accuracy: 0.4996 - val_loss: 0.6948 - val_accuracy: 0.5088 Epoch 2/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6944 - accuracy: 0.5128 - val_loss: 0.6951 - val_accuracy: 0.5076 Epoch 3/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6951 - accuracy: 0.5112 - val_loss: 0.6940 - val_accuracy: 0.5160 Epoch 4/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6933 - accuracy: 0.5112 - val_loss: 0.6935 - val_accuracy: 0.5024 Epoch 5/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6946 - accuracy: 0.5092 - val_loss: 0.6932 - val_accuracy: 0.5140 Epoch 6/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6923 - accuracy: 0.5236 - val_loss: 0.6952 - val_accuracy: 0.5072 Epoch 7/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5248 - val_loss: 0.6935 - val_accuracy: 0.5088 Epoch 8/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6919 - accuracy: 0.5276 - val_loss: 0.6930 - val_accuracy: 0.5184 Epoch 9/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6936 - accuracy: 0.5220 - val_loss: 0.6934 - val_accuracy: 0.5180 Epoch 10/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6914 - accuracy: 0.5356 - val_loss: 0.6923 - val_accuracy: 0.5204 Epoch 11/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6909 - accuracy: 0.5292 - val_loss: 0.6931 - val_accuracy: 0.5232 Epoch 12/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6912 - accuracy: 0.5288 - val_loss: 0.6967 - val_accuracy: 0.5188 Epoch 13/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6921 - accuracy: 0.5184 - val_loss: 0.6934 - val_accuracy: 0.5124 Epoch 14/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6907 - accuracy: 0.5260 - val_loss: 0.6942 - val_accuracy: 0.5188 Epoch 15/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6907 - accuracy: 0.5304 - val_loss: 0.6951 - val_accuracy: 0.5188 Epoch 16/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6905 - accuracy: 0.5292 - val_loss: 0.6921 - val_accuracy: 0.5196 Epoch 17/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6906 - accuracy: 0.5416 - val_loss: 0.6937 - val_accuracy: 0.5296 Epoch 18/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6895 - accuracy: 0.5184 - val_loss: 0.6940 - val_accuracy: 0.5164 Epoch 19/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6903 - accuracy: 0.5304 - val_loss: 0.6949 - val_accuracy: 0.5120 Epoch 20/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6907 - accuracy: 0.5412 - val_loss: 0.6931 - val_accuracy: 0.5184 Epoch 21/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6893 - accuracy: 0.5444 - val_loss: 0.6942 - val_accuracy: 0.5076 Epoch 22/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6913 - accuracy: 0.5328 - val_loss: 0.6936 - val_accuracy: 0.5148 Epoch 23/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6902 - accuracy: 0.5364 - val_loss: 0.6941 - val_accuracy: 0.5260 Epoch 24/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6883 - accuracy: 0.5464 - val_loss: 0.6933 - val_accuracy: 0.5284 Epoch 25/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6887 - accuracy: 0.5392 - val_loss: 0.6921 - val_accuracy: 0.5248 Epoch 26/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6888 - accuracy: 0.5512 - val_loss: 0.6979 - val_accuracy: 0.5088 Epoch 27/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6897 - accuracy: 0.5436 - val_loss: 0.6926 - val_accuracy: 0.5312 Epoch 28/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6899 - accuracy: 0.5512 - val_loss: 0.6925 - val_accuracy: 0.5236 Epoch 29/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6886 - accuracy: 0.5516 - val_loss: 0.6927 - val_accuracy: 0.5208 Epoch 30/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6912 - accuracy: 0.5448 - val_loss: 0.6915 - val_accuracy: 0.5276 Epoch 31/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6860 - accuracy: 0.5572 - val_loss: 0.6932 - val_accuracy: 0.5264 Epoch 32/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6869 - accuracy: 0.5564 - val_loss: 0.6926 - val_accuracy: 0.5232 Epoch 33/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6858 - accuracy: 0.5604 - val_loss: 0.6936 - val_accuracy: 0.5200 Epoch 34/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6862 - accuracy: 0.5580 - val_loss: 0.6934 - val_accuracy: 0.5284 Epoch 35/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6842 - accuracy: 0.5652 - val_loss: 0.6920 - val_accuracy: 0.5384 Epoch 36/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6839 - accuracy: 0.5624 - val_loss: 0.6884 - val_accuracy: 0.5468 Epoch 37/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6796 - accuracy: 0.5712 - val_loss: 0.6856 - val_accuracy: 0.5488 Epoch 38/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6724 - accuracy: 0.5836 - val_loss: 0.6723 - val_accuracy: 0.5784 Epoch 39/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6612 - accuracy: 0.5936 - val_loss: 0.6746 - val_accuracy: 0.5872 Epoch 40/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6565 - accuracy: 0.5996 - val_loss: 0.6659 - val_accuracy: 0.6104 Epoch 41/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6572 - accuracy: 0.6176 - val_loss: 0.6759 - val_accuracy: 0.5988 Epoch 42/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6521 - accuracy: 0.6092 - val_loss: 0.6534 - val_accuracy: 0.6112 Epoch 43/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6309 - accuracy: 0.6384 - val_loss: 0.6267 - val_accuracy: 0.6444 Epoch 44/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6448 - accuracy: 0.6256 - val_loss: 0.6421 - val_accuracy: 0.6244 Epoch 45/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6301 - accuracy: 0.6380 - val_loss: 0.6289 - val_accuracy: 0.6352 Epoch 46/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6370 - accuracy: 0.6360 - val_loss: 0.7009 - val_accuracy: 0.5672 Epoch 47/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6710 - accuracy: 0.5856 - val_loss: 0.6421 - val_accuracy: 0.6224 Epoch 48/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6243 - accuracy: 0.6436 - val_loss: 0.6336 - val_accuracy: 0.6400 Epoch 49/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6238 - accuracy: 0.6516 - val_loss: 0.6313 - val_accuracy: 0.6420 Epoch 50/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6192 - accuracy: 0.6472 - val_loss: 0.6221 - val_accuracy: 0.6492 Epoch 51/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6009 - accuracy: 0.6724 - val_loss: 0.6352 - val_accuracy: 0.6508 Epoch 52/200 79/79 [==============================] - 1s 12ms/step - loss: 0.6152 - accuracy: 0.6468 - val_loss: 0.6018 - val_accuracy: 0.6596 Epoch 53/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5915 - accuracy: 0.6816 - val_loss: 0.5911 - val_accuracy: 0.6592 Epoch 54/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5794 - accuracy: 0.6776 - val_loss: 0.5868 - val_accuracy: 0.6772 Epoch 55/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6113 - accuracy: 0.6520 - val_loss: 0.5924 - val_accuracy: 0.6788 Epoch 56/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5776 - accuracy: 0.6816 - val_loss: 0.6155 - val_accuracy: 0.6468 Epoch 57/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6426 - accuracy: 0.6272 - val_loss: 0.6447 - val_accuracy: 0.6164 Epoch 58/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6070 - accuracy: 0.6544 - val_loss: 0.6043 - val_accuracy: 0.6600 Epoch 59/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5754 - accuracy: 0.6776 - val_loss: 0.5564 - val_accuracy: 0.6996 Epoch 60/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5524 - accuracy: 0.6972 - val_loss: 0.5752 - val_accuracy: 0.6976 Epoch 61/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5632 - accuracy: 0.6916 - val_loss: 0.6242 - val_accuracy: 0.6272 Epoch 62/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6013 - accuracy: 0.6508 - val_loss: 0.5646 - val_accuracy: 0.6940 Epoch 63/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5497 - accuracy: 0.7024 - val_loss: 0.5493 - val_accuracy: 0.6940 Epoch 64/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5718 - accuracy: 0.6868 - val_loss: 0.5542 - val_accuracy: 0.7016 Epoch 65/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5447 - accuracy: 0.7212 - val_loss: 0.5553 - val_accuracy: 0.7104 Epoch 66/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5345 - accuracy: 0.7232 - val_loss: 0.5673 - val_accuracy: 0.7128 Epoch 67/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5677 - accuracy: 0.7200 - val_loss: 0.5253 - val_accuracy: 0.7688 Epoch 68/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5185 - accuracy: 0.7496 - val_loss: 0.5156 - val_accuracy: 0.7620 Epoch 69/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5330 - accuracy: 0.7440 - val_loss: 0.5130 - val_accuracy: 0.7476 Epoch 70/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5412 - accuracy: 0.7452 - val_loss: 0.5197 - val_accuracy: 0.7636 Epoch 71/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4882 - accuracy: 0.7788 - val_loss: 0.5193 - val_accuracy: 0.7600 Epoch 72/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5023 - accuracy: 0.7716 - val_loss: 0.5054 - val_accuracy: 0.7704 Epoch 73/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5104 - accuracy: 0.7764 - val_loss: 0.5132 - val_accuracy: 0.7764 Epoch 74/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4758 - accuracy: 0.8048 - val_loss: 0.5033 - val_accuracy: 0.7928 Epoch 75/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4713 - accuracy: 0.8124 - val_loss: 0.4803 - val_accuracy: 0.8100 Epoch 76/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4762 - accuracy: 0.7996 - val_loss: 0.4966 - val_accuracy: 0.8000 Epoch 77/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4560 - accuracy: 0.8172 - val_loss: 0.4961 - val_accuracy: 0.7932 Epoch 78/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4521 - accuracy: 0.8200 - val_loss: 0.4663 - val_accuracy: 0.8080 Epoch 79/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4624 - accuracy: 0.8080 - val_loss: 0.5134 - val_accuracy: 0.7788 Epoch 80/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5291 - accuracy: 0.7596 - val_loss: 0.5268 - val_accuracy: 0.7668 Epoch 81/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4886 - accuracy: 0.7952 - val_loss: 0.5036 - val_accuracy: 0.7968 Epoch 82/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4483 - accuracy: 0.8192 - val_loss: 0.4857 - val_accuracy: 0.7820 Epoch 83/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4724 - accuracy: 0.7868 - val_loss: 0.5214 - val_accuracy: 0.7624 Epoch 84/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4629 - accuracy: 0.7980 - val_loss: 0.4601 - val_accuracy: 0.8044 Epoch 85/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4590 - accuracy: 0.8020 - val_loss: 0.4799 - val_accuracy: 0.7892 Epoch 86/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4843 - accuracy: 0.7808 - val_loss: 0.5494 - val_accuracy: 0.7452 Epoch 87/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5111 - accuracy: 0.7768 - val_loss: 0.4562 - val_accuracy: 0.8008 Epoch 88/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4392 - accuracy: 0.8204 - val_loss: 0.4450 - val_accuracy: 0.8128 Epoch 89/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4546 - accuracy: 0.8204 - val_loss: 0.4002 - val_accuracy: 0.8484 Epoch 90/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4200 - accuracy: 0.8312 - val_loss: 0.4415 - val_accuracy: 0.8304 Epoch 91/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4022 - accuracy: 0.8484 - val_loss: 0.4678 - val_accuracy: 0.8000 Epoch 92/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4207 - accuracy: 0.8272 - val_loss: 0.4074 - val_accuracy: 0.8504 Epoch 93/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3875 - accuracy: 0.8520 - val_loss: 0.4857 - val_accuracy: 0.8000 Epoch 94/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4068 - accuracy: 0.8356 - val_loss: 0.4879 - val_accuracy: 0.7900 Epoch 95/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4873 - accuracy: 0.8028 - val_loss: 0.4495 - val_accuracy: 0.8416 Epoch 96/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3773 - accuracy: 0.8732 - val_loss: 0.3833 - val_accuracy: 0.8672 Epoch 97/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3640 - accuracy: 0.8776 - val_loss: 0.4010 - val_accuracy: 0.8684 Epoch 98/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3723 - accuracy: 0.8716 - val_loss: 0.3782 - val_accuracy: 0.8744 Epoch 99/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4678 - accuracy: 0.8152 - val_loss: 0.6316 - val_accuracy: 0.7104 Epoch 100/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5038 - accuracy: 0.7836 - val_loss: 0.4439 - val_accuracy: 0.8280 Epoch 101/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4657 - accuracy: 0.8216 - val_loss: 0.4984 - val_accuracy: 0.7936 Epoch 102/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5420 - accuracy: 0.7528 - val_loss: 0.6300 - val_accuracy: 0.6660 Epoch 103/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5793 - accuracy: 0.7136 - val_loss: 0.5262 - val_accuracy: 0.7696 Epoch 104/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5042 - accuracy: 0.7864 - val_loss: 0.4314 - val_accuracy: 0.8424 Epoch 105/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4277 - accuracy: 0.8372 - val_loss: 0.4179 - val_accuracy: 0.8460 Epoch 106/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4170 - accuracy: 0.8296 - val_loss: 0.4903 - val_accuracy: 0.7940 Epoch 107/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3549 - accuracy: 0.8724 - val_loss: 0.3610 - val_accuracy: 0.8752 Epoch 108/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3170 - accuracy: 0.8988 - val_loss: 0.3182 - val_accuracy: 0.9036 Epoch 109/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3382 - accuracy: 0.8820 - val_loss: 0.3694 - val_accuracy: 0.8732 Epoch 110/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4927 - accuracy: 0.7988 - val_loss: 0.5154 - val_accuracy: 0.7928 Epoch 111/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4499 - accuracy: 0.8240 - val_loss: 0.4852 - val_accuracy: 0.7972 Epoch 112/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5167 - accuracy: 0.7772 - val_loss: 0.5273 - val_accuracy: 0.7652 Epoch 113/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5258 - accuracy: 0.7620 - val_loss: 0.4776 - val_accuracy: 0.8088 Epoch 114/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4321 - accuracy: 0.8384 - val_loss: 0.4269 - val_accuracy: 0.8508 Epoch 115/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3960 - accuracy: 0.8632 - val_loss: 0.3897 - val_accuracy: 0.8652 Epoch 116/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3598 - accuracy: 0.8764 - val_loss: 0.3446 - val_accuracy: 0.8836 Epoch 117/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3355 - accuracy: 0.8896 - val_loss: 0.3143 - val_accuracy: 0.9088 Epoch 118/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3117 - accuracy: 0.8960 - val_loss: 0.3016 - val_accuracy: 0.9068 Epoch 119/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4481 - accuracy: 0.8416 - val_loss: 0.4037 - val_accuracy: 0.8580 Epoch 120/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3510 - accuracy: 0.8880 - val_loss: 0.3300 - val_accuracy: 0.8932 Epoch 121/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3791 - accuracy: 0.8748 - val_loss: 0.4971 - val_accuracy: 0.8112 Epoch 122/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4033 - accuracy: 0.8528 - val_loss: 0.3500 - val_accuracy: 0.8864 Epoch 123/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3238 - accuracy: 0.8996 - val_loss: 0.3461 - val_accuracy: 0.8872 Epoch 124/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4098 - accuracy: 0.8520 - val_loss: 0.6394 - val_accuracy: 0.7480 Epoch 125/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5331 - accuracy: 0.7832 - val_loss: 0.4910 - val_accuracy: 0.8080 Epoch 126/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4373 - accuracy: 0.8432 - val_loss: 0.4168 - val_accuracy: 0.8580 Epoch 127/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3445 - accuracy: 0.8852 - val_loss: 0.3493 - val_accuracy: 0.8856 Epoch 128/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3966 - accuracy: 0.8628 - val_loss: 0.3932 - val_accuracy: 0.8676 Epoch 129/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3495 - accuracy: 0.8852 - val_loss: 0.3292 - val_accuracy: 0.8964 Epoch 130/200 79/79 [==============================] - 1s 11ms/step - loss: 0.3341 - accuracy: 0.8888 - val_loss: 0.3204 - val_accuracy: 0.9028 Epoch 131/200 79/79 [==============================] - 1s 11ms/step - loss: 0.3434 - accuracy: 0.8908 - val_loss: 0.4491 - val_accuracy: 0.8332 Epoch 132/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5237 - accuracy: 0.7880 - val_loss: 0.4900 - val_accuracy: 0.7976 Epoch 133/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4198 - accuracy: 0.8484 - val_loss: 0.4675 - val_accuracy: 0.8184 Epoch 134/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3387 - accuracy: 0.8908 - val_loss: 0.3651 - val_accuracy: 0.8804 Epoch 135/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6448 - accuracy: 0.6680 - val_loss: 0.5885 - val_accuracy: 0.6936 Epoch 136/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5542 - accuracy: 0.7360 - val_loss: 0.5517 - val_accuracy: 0.7392 Epoch 137/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5331 - accuracy: 0.7492 - val_loss: 0.5222 - val_accuracy: 0.7568 Epoch 138/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5067 - accuracy: 0.7716 - val_loss: 0.4807 - val_accuracy: 0.7924 Epoch 139/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4294 - accuracy: 0.8052 - val_loss: 0.4049 - val_accuracy: 0.8116 Epoch 140/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6623 - accuracy: 0.6196 - val_loss: 0.6804 - val_accuracy: 0.5624 Epoch 141/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6885 - accuracy: 0.5320 - val_loss: 0.6820 - val_accuracy: 0.5436 Epoch 142/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6836 - accuracy: 0.5268 - val_loss: 0.6772 - val_accuracy: 0.5556 Epoch 143/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6853 - accuracy: 0.5372 - val_loss: 0.6855 - val_accuracy: 0.5480 Epoch 144/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.5328 - val_loss: 0.6866 - val_accuracy: 0.5392 Epoch 145/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6861 - accuracy: 0.5448 - val_loss: 0.6867 - val_accuracy: 0.5400 Epoch 146/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6825 - accuracy: 0.5608 - val_loss: 0.6919 - val_accuracy: 0.5484 Epoch 147/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6876 - accuracy: 0.5624 - val_loss: 0.6843 - val_accuracy: 0.5644 Epoch 148/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6837 - accuracy: 0.5612 - val_loss: 0.6810 - val_accuracy: 0.5648 Epoch 149/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6843 - accuracy: 0.5596 - val_loss: 0.6818 - val_accuracy: 0.5644 Epoch 150/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6839 - accuracy: 0.5616 - val_loss: 0.6844 - val_accuracy: 0.5536 Epoch 151/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6880 - accuracy: 0.5312 - val_loss: 0.6752 - val_accuracy: 0.5876 Epoch 152/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6816 - accuracy: 0.5660 - val_loss: 0.6818 - val_accuracy: 0.5632 Epoch 153/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6932 - accuracy: 0.5348 - val_loss: 0.6877 - val_accuracy: 0.5476 Epoch 154/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6868 - accuracy: 0.5360 - val_loss: 0.6820 - val_accuracy: 0.5600 Epoch 155/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6835 - accuracy: 0.5600 - val_loss: 0.6846 - val_accuracy: 0.5500 Epoch 156/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6807 - accuracy: 0.5616 - val_loss: 0.6772 - val_accuracy: 0.5640 Epoch 157/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6892 - accuracy: 0.5400 - val_loss: 0.6904 - val_accuracy: 0.5100 Epoch 158/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6936 - accuracy: 0.5148 - val_loss: 0.6913 - val_accuracy: 0.5220 Epoch 159/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6904 - accuracy: 0.5180 - val_loss: 0.6910 - val_accuracy: 0.5248 Epoch 160/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6913 - accuracy: 0.5260 - val_loss: 0.6924 - val_accuracy: 0.4888 Epoch 161/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.6907 - val_accuracy: 0.5220 Epoch 162/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6943 - accuracy: 0.5032 - val_loss: 0.6958 - val_accuracy: 0.5120 Epoch 163/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6947 - accuracy: 0.4876 - val_loss: 0.6931 - val_accuracy: 0.4924 Epoch 164/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6940 - accuracy: 0.4972 - val_loss: 0.6920 - val_accuracy: 0.5240 Epoch 165/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6936 - accuracy: 0.5060 - val_loss: 0.6931 - val_accuracy: 0.5024 Epoch 166/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6947 - accuracy: 0.4984 - val_loss: 0.6931 - val_accuracy: 0.5168 Epoch 167/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6928 - accuracy: 0.5196 - val_loss: 0.6936 - val_accuracy: 0.5188 Epoch 168/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6927 - accuracy: 0.5156 - val_loss: 0.6935 - val_accuracy: 0.5124 Epoch 169/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6936 - accuracy: 0.5144 - val_loss: 0.6939 - val_accuracy: 0.5252 Epoch 170/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6935 - accuracy: 0.5164 - val_loss: 0.6926 - val_accuracy: 0.5288 Epoch 171/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6965 - accuracy: 0.5052 - val_loss: 0.6938 - val_accuracy: 0.5284 Epoch 172/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6928 - accuracy: 0.5264 - val_loss: 0.6968 - val_accuracy: 0.4776 Epoch 173/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6948 - accuracy: 0.5060 - val_loss: 0.6954 - val_accuracy: 0.4988 Epoch 174/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6927 - accuracy: 0.5196 - val_loss: 0.6937 - val_accuracy: 0.5264 Epoch 175/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6922 - accuracy: 0.5316 - val_loss: 0.6917 - val_accuracy: 0.5284 Epoch 176/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6921 - accuracy: 0.5304 - val_loss: 0.6923 - val_accuracy: 0.5292 Epoch 177/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6908 - accuracy: 0.5292 - val_loss: 0.6923 - val_accuracy: 0.5248 Epoch 178/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6937 - accuracy: 0.5164 - val_loss: 0.6915 - val_accuracy: 0.5216 Epoch 179/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6929 - accuracy: 0.5084 - val_loss: 0.6931 - val_accuracy: 0.5072 Epoch 180/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6948 - accuracy: 0.4988 - val_loss: 0.6914 - val_accuracy: 0.5260 Epoch 181/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6944 - accuracy: 0.5032 - val_loss: 0.6938 - val_accuracy: 0.5148 Epoch 182/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6930 - accuracy: 0.5136 - val_loss: 0.6941 - val_accuracy: 0.5080 Epoch 183/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6935 - accuracy: 0.5152 - val_loss: 0.6947 - val_accuracy: 0.5024 Epoch 184/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6958 - val_accuracy: 0.4984 Epoch 185/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6940 - accuracy: 0.5092 - val_loss: 0.6992 - val_accuracy: 0.5260 Epoch 186/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5144 - val_loss: 0.6927 - val_accuracy: 0.5252 Epoch 187/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5200 - val_loss: 0.6918 - val_accuracy: 0.5284 Epoch 188/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5236 - val_loss: 0.6922 - val_accuracy: 0.5244 Epoch 189/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6904 - accuracy: 0.5392 - val_loss: 0.6853 - val_accuracy: 0.5540 Epoch 190/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6791 - accuracy: 0.5796 - val_loss: 0.6795 - val_accuracy: 0.5704 Epoch 191/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6787 - accuracy: 0.5696 - val_loss: 0.6742 - val_accuracy: 0.5720 Epoch 192/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6815 - accuracy: 0.5708 - val_loss: 0.6789 - val_accuracy: 0.5636 Epoch 193/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6819 - accuracy: 0.5672 - val_loss: 0.6796 - val_accuracy: 0.5644 Epoch 194/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6843 - accuracy: 0.5496 - val_loss: 0.6893 - val_accuracy: 0.5508 Epoch 195/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6948 - accuracy: 0.5256 - val_loss: 0.6948 - val_accuracy: 0.4884 Epoch 196/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6938 - accuracy: 0.5128 - val_loss: 0.6924 - val_accuracy: 0.5188 Epoch 197/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6939 - accuracy: 0.4996 - val_loss: 0.6933 - val_accuracy: 0.5076 Epoch 198/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6915 - accuracy: 0.5244 - val_loss: 0.6972 - val_accuracy: 0.5020 Epoch 199/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6951 - accuracy: 0.5020 - val_loss: 0.6947 - val_accuracy: 0.4760 Epoch 200/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6943 - accuracy: 0.5044 - val_loss: 0.6935 - val_accuracy: 0.5160 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1f652c88> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1fae4470> # Now test our LSTM inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = LSTM ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 9ms/step - loss: 0.6951 - accuracy: 0.4972 - val_loss: 0.6948 - val_accuracy: 0.5068 Epoch 2/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5140 - val_loss: 0.6940 - val_accuracy: 0.4960 Epoch 3/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5072 - val_loss: 0.6948 - val_accuracy: 0.5036 Epoch 4/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5048 - val_loss: 0.6952 - val_accuracy: 0.5044 Epoch 5/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6940 - val_accuracy: 0.4968 Epoch 6/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5128 - val_loss: 0.6943 - val_accuracy: 0.4984 Epoch 7/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5100 - val_loss: 0.6947 - val_accuracy: 0.5008 Epoch 8/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5092 - val_loss: 0.6947 - val_accuracy: 0.4980 Epoch 9/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6915 - accuracy: 0.5064 - val_loss: 0.6953 - val_accuracy: 0.5012 Epoch 10/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5180 - val_loss: 0.6933 - val_accuracy: 0.5072 Epoch 11/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6891 - accuracy: 0.5340 - val_loss: 0.6916 - val_accuracy: 0.5252 Epoch 12/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.5268 - val_loss: 0.6932 - val_accuracy: 0.5208 Epoch 13/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6907 - accuracy: 0.5136 - val_loss: 0.6832 - val_accuracy: 0.5584 Epoch 14/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6768 - accuracy: 0.5896 - val_loss: 0.6899 - val_accuracy: 0.5616 Epoch 15/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6580 - accuracy: 0.6140 - val_loss: 0.6567 - val_accuracy: 0.6160 Epoch 16/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.5608 - val_loss: 0.6936 - val_accuracy: 0.5384 Epoch 17/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6870 - accuracy: 0.5492 - val_loss: 0.6713 - val_accuracy: 0.5948 Epoch 18/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6445 - accuracy: 0.6264 - val_loss: 0.6664 - val_accuracy: 0.5708 Epoch 19/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6376 - accuracy: 0.5992 - val_loss: 0.6431 - val_accuracy: 0.6356 Epoch 20/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6416 - accuracy: 0.6456 - val_loss: 0.6316 - val_accuracy: 0.6592 Epoch 21/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6230 - accuracy: 0.6392 - val_loss: 0.6345 - val_accuracy: 0.6368 Epoch 22/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6305 - accuracy: 0.6528 - val_loss: 0.6336 - val_accuracy: 0.6588 Epoch 23/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6154 - accuracy: 0.6664 - val_loss: 0.6297 - val_accuracy: 0.6420 Epoch 24/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6160 - accuracy: 0.6576 - val_loss: 0.6263 - val_accuracy: 0.6508 Epoch 25/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6650 - accuracy: 0.6028 - val_loss: 0.6672 - val_accuracy: 0.5772 Epoch 26/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6428 - accuracy: 0.6164 - val_loss: 0.6448 - val_accuracy: 0.6072 Epoch 27/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6322 - accuracy: 0.6248 - val_loss: 0.6388 - val_accuracy: 0.6304 Epoch 28/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6189 - accuracy: 0.6580 - val_loss: 0.6317 - val_accuracy: 0.6568 Epoch 29/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6113 - accuracy: 0.6692 - val_loss: 0.6560 - val_accuracy: 0.5864 Epoch 30/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6224 - accuracy: 0.6264 - val_loss: 0.6117 - val_accuracy: 0.6744 Epoch 31/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6167 - accuracy: 0.6540 - val_loss: 0.6340 - val_accuracy: 0.6412 Epoch 32/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6174 - accuracy: 0.6456 - val_loss: 0.6164 - val_accuracy: 0.6548 Epoch 33/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6041 - accuracy: 0.6556 - val_loss: 0.5988 - val_accuracy: 0.6636 Epoch 34/200 79/79 [==============================] - 0s 6ms/step - loss: 0.5678 - accuracy: 0.7044 - val_loss: 0.5724 - val_accuracy: 0.7308 Epoch 35/200 79/79 [==============================] - 0s 6ms/step - loss: 0.5348 - accuracy: 0.7356 - val_loss: 0.5414 - val_accuracy: 0.7604 Epoch 36/200 79/79 [==============================] - 0s 6ms/step - loss: 0.5063 - accuracy: 0.7720 - val_loss: 0.5096 - val_accuracy: 0.7684 Epoch 37/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4962 - accuracy: 0.7668 - val_loss: 0.4869 - val_accuracy: 0.7852 Epoch 38/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4727 - accuracy: 0.7940 - val_loss: 0.5253 - val_accuracy: 0.7580 Epoch 39/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4697 - accuracy: 0.7928 - val_loss: 0.4571 - val_accuracy: 0.8084 Epoch 40/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4708 - accuracy: 0.7836 - val_loss: 0.4460 - val_accuracy: 0.8120 Epoch 41/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4428 - accuracy: 0.8052 - val_loss: 0.4407 - val_accuracy: 0.8048 Epoch 42/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4210 - accuracy: 0.8120 - val_loss: 0.4297 - val_accuracy: 0.8144 Epoch 43/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4132 - accuracy: 0.8280 - val_loss: 0.4336 - val_accuracy: 0.8144 Epoch 44/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4101 - accuracy: 0.8260 - val_loss: 0.4483 - val_accuracy: 0.8172 Epoch 45/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4504 - accuracy: 0.8136 - val_loss: 0.4487 - val_accuracy: 0.8144 Epoch 46/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4941 - accuracy: 0.7756 - val_loss: 0.5412 - val_accuracy: 0.7388 Epoch 47/200 79/79 [==============================] - 0s 6ms/step - loss: 0.5151 - accuracy: 0.7604 - val_loss: 0.4995 - val_accuracy: 0.7672 Epoch 48/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4766 - accuracy: 0.7840 - val_loss: 0.4901 - val_accuracy: 0.7740 Epoch 49/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4641 - accuracy: 0.7880 - val_loss: 0.5086 - val_accuracy: 0.7652 Epoch 50/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4598 - accuracy: 0.8052 - val_loss: 0.4543 - val_accuracy: 0.8036 Epoch 51/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4084 - accuracy: 0.8344 - val_loss: 0.4255 - val_accuracy: 0.8240 Epoch 52/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3922 - accuracy: 0.8332 - val_loss: 0.4263 - val_accuracy: 0.8188 Epoch 53/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3988 - accuracy: 0.8376 - val_loss: 0.4253 - val_accuracy: 0.8240 Epoch 54/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3971 - accuracy: 0.8416 - val_loss: 0.4041 - val_accuracy: 0.8356 Epoch 55/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3949 - accuracy: 0.8428 - val_loss: 0.4586 - val_accuracy: 0.8116 Epoch 56/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4201 - accuracy: 0.8240 - val_loss: 0.4205 - val_accuracy: 0.8300 Epoch 57/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3922 - accuracy: 0.8360 - val_loss: 0.4084 - val_accuracy: 0.8288 Epoch 58/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4639 - accuracy: 0.8056 - val_loss: 0.4101 - val_accuracy: 0.8352 Epoch 59/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3755 - accuracy: 0.8492 - val_loss: 0.3841 - val_accuracy: 0.8512 Epoch 60/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3641 - accuracy: 0.8568 - val_loss: 0.3904 - val_accuracy: 0.8528 Epoch 61/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3600 - accuracy: 0.8576 - val_loss: 0.4801 - val_accuracy: 0.8112 Epoch 62/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4689 - accuracy: 0.8104 - val_loss: 0.4442 - val_accuracy: 0.8112 Epoch 63/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4199 - accuracy: 0.8260 - val_loss: 0.4236 - val_accuracy: 0.8212 Epoch 64/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4023 - accuracy: 0.8320 - val_loss: 0.4236 - val_accuracy: 0.8184 Epoch 65/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3846 - accuracy: 0.8368 - val_loss: 0.4025 - val_accuracy: 0.8364 Epoch 66/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3789 - accuracy: 0.8452 - val_loss: 0.4051 - val_accuracy: 0.8356 Epoch 67/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3765 - accuracy: 0.8524 - val_loss: 0.3683 - val_accuracy: 0.8592 Epoch 68/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3581 - accuracy: 0.8600 - val_loss: 0.3453 - val_accuracy: 0.8680 Epoch 69/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3633 - accuracy: 0.8544 - val_loss: 0.3584 - val_accuracy: 0.8532 Epoch 70/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3495 - accuracy: 0.8632 - val_loss: 0.3408 - val_accuracy: 0.8676 Epoch 71/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3435 - accuracy: 0.8732 - val_loss: 0.3465 - val_accuracy: 0.8696 Epoch 72/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3607 - accuracy: 0.8652 - val_loss: 0.3660 - val_accuracy: 0.8560 Epoch 73/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3500 - accuracy: 0.8644 - val_loss: 0.3522 - val_accuracy: 0.8636 Epoch 74/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8628 - val_loss: 0.3526 - val_accuracy: 0.8628 Epoch 75/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3459 - accuracy: 0.8696 - val_loss: 0.3460 - val_accuracy: 0.8668 Epoch 76/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3489 - accuracy: 0.8672 - val_loss: 0.4227 - val_accuracy: 0.8204 Epoch 77/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3986 - accuracy: 0.8300 - val_loss: 0.4046 - val_accuracy: 0.8208 Epoch 78/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3767 - accuracy: 0.8408 - val_loss: 0.3822 - val_accuracy: 0.8432 Epoch 79/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3647 - accuracy: 0.8528 - val_loss: 0.3791 - val_accuracy: 0.8408 Epoch 80/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3537 - accuracy: 0.8548 - val_loss: 0.3624 - val_accuracy: 0.8496 Epoch 81/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3432 - accuracy: 0.8604 - val_loss: 0.3626 - val_accuracy: 0.8508 Epoch 82/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3415 - accuracy: 0.8632 - val_loss: 0.3865 - val_accuracy: 0.8412 Epoch 83/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3452 - accuracy: 0.8592 - val_loss: 0.3724 - val_accuracy: 0.8492 Epoch 84/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3417 - accuracy: 0.8600 - val_loss: 0.3677 - val_accuracy: 0.8460 Epoch 85/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3497 - accuracy: 0.8620 - val_loss: 0.4046 - val_accuracy: 0.8524 Epoch 86/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3627 - accuracy: 0.8620 - val_loss: 0.3845 - val_accuracy: 0.8520 Epoch 87/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3576 - accuracy: 0.8672 - val_loss: 0.4617 - val_accuracy: 0.8260 Epoch 88/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3878 - accuracy: 0.8536 - val_loss: 0.3513 - val_accuracy: 0.8640 Epoch 89/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3353 - accuracy: 0.8796 - val_loss: 0.3367 - val_accuracy: 0.8736 Epoch 90/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3328 - accuracy: 0.8844 - val_loss: 0.3293 - val_accuracy: 0.8780 Epoch 91/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3300 - accuracy: 0.8888 - val_loss: 0.3723 - val_accuracy: 0.8684 Epoch 92/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3477 - accuracy: 0.8728 - val_loss: 0.3583 - val_accuracy: 0.8676 Epoch 93/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3552 - accuracy: 0.8668 - val_loss: 0.4290 - val_accuracy: 0.8332 Epoch 94/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3486 - accuracy: 0.8668 - val_loss: 0.3927 - val_accuracy: 0.8504 Epoch 95/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3503 - accuracy: 0.8712 - val_loss: 0.3956 - val_accuracy: 0.8524 Epoch 96/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8708 - val_loss: 0.3803 - val_accuracy: 0.8520 Epoch 97/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3204 - accuracy: 0.8824 - val_loss: 0.3470 - val_accuracy: 0.8724 Epoch 98/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3000 - accuracy: 0.8932 - val_loss: 0.3088 - val_accuracy: 0.8904 Epoch 99/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2753 - accuracy: 0.9064 - val_loss: 0.3043 - val_accuracy: 0.8900 Epoch 100/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2736 - accuracy: 0.9036 - val_loss: 0.2935 - val_accuracy: 0.8992 Epoch 101/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2749 - accuracy: 0.9072 - val_loss: 0.3115 - val_accuracy: 0.8896 Epoch 102/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2615 - accuracy: 0.9104 - val_loss: 0.3126 - val_accuracy: 0.8864 Epoch 103/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2684 - accuracy: 0.9100 - val_loss: 0.2951 - val_accuracy: 0.8984 Epoch 104/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2611 - accuracy: 0.9124 - val_loss: 0.2887 - val_accuracy: 0.8976 Epoch 105/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2725 - accuracy: 0.9096 - val_loss: 0.3431 - val_accuracy: 0.8796 Epoch 106/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2825 - accuracy: 0.9020 - val_loss: 0.2868 - val_accuracy: 0.9024 Epoch 107/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2767 - accuracy: 0.9044 - val_loss: 0.3275 - val_accuracy: 0.8832 Epoch 108/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2985 - accuracy: 0.8952 - val_loss: 0.3151 - val_accuracy: 0.8876 Epoch 109/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2815 - accuracy: 0.9044 - val_loss: 0.3018 - val_accuracy: 0.8976 Epoch 110/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2827 - accuracy: 0.9028 - val_loss: 0.2902 - val_accuracy: 0.9048 Epoch 111/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2605 - accuracy: 0.9140 - val_loss: 0.2997 - val_accuracy: 0.8952 Epoch 112/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2540 - accuracy: 0.9160 - val_loss: 0.2777 - val_accuracy: 0.9080 Epoch 113/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2428 - accuracy: 0.9176 - val_loss: 0.2913 - val_accuracy: 0.8980 Epoch 114/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2654 - accuracy: 0.9080 - val_loss: 0.3008 - val_accuracy: 0.8948 Epoch 115/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2483 - accuracy: 0.9152 - val_loss: 0.2795 - val_accuracy: 0.9028 Epoch 116/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2470 - accuracy: 0.9160 - val_loss: 0.2937 - val_accuracy: 0.8988 Epoch 117/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3590 - accuracy: 0.8600 - val_loss: 0.3673 - val_accuracy: 0.8496 Epoch 118/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3585 - accuracy: 0.8576 - val_loss: 0.3815 - val_accuracy: 0.8432 Epoch 119/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3618 - accuracy: 0.8560 - val_loss: 0.3473 - val_accuracy: 0.8648 Epoch 120/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3483 - accuracy: 0.8572 - val_loss: 0.3464 - val_accuracy: 0.8596 Epoch 121/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8584 - val_loss: 0.3421 - val_accuracy: 0.8616 Epoch 122/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3424 - accuracy: 0.8644 - val_loss: 0.3477 - val_accuracy: 0.8536 Epoch 123/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3443 - accuracy: 0.8648 - val_loss: 0.3426 - val_accuracy: 0.8528 Epoch 124/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3376 - accuracy: 0.8664 - val_loss: 0.3468 - val_accuracy: 0.8520 Epoch 125/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3363 - accuracy: 0.8612 - val_loss: 0.3369 - val_accuracy: 0.8592 Epoch 126/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3074 - accuracy: 0.8848 - val_loss: 0.2864 - val_accuracy: 0.8988 Epoch 127/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2740 - accuracy: 0.9056 - val_loss: 0.2777 - val_accuracy: 0.9044 Epoch 128/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2652 - accuracy: 0.9044 - val_loss: 0.2824 - val_accuracy: 0.9016 Epoch 129/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2626 - accuracy: 0.9064 - val_loss: 0.2775 - val_accuracy: 0.9020 Epoch 130/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2618 - accuracy: 0.9044 - val_loss: 0.2631 - val_accuracy: 0.9076 Epoch 131/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2511 - accuracy: 0.9108 - val_loss: 0.2600 - val_accuracy: 0.9156 Epoch 132/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2433 - accuracy: 0.9108 - val_loss: 0.2587 - val_accuracy: 0.9148 Epoch 133/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2551 - accuracy: 0.9084 - val_loss: 0.2811 - val_accuracy: 0.8996 Epoch 134/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2351 - accuracy: 0.9188 - val_loss: 0.2558 - val_accuracy: 0.9180 Epoch 135/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2722 - accuracy: 0.9136 - val_loss: 0.2929 - val_accuracy: 0.9044 Epoch 136/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3085 - accuracy: 0.9020 - val_loss: 0.2946 - val_accuracy: 0.9008 Epoch 137/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2960 - accuracy: 0.8980 - val_loss: 0.3340 - val_accuracy: 0.8880 Epoch 138/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2954 - accuracy: 0.8984 - val_loss: 0.3190 - val_accuracy: 0.8904 Epoch 139/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2780 - accuracy: 0.9036 - val_loss: 0.2865 - val_accuracy: 0.9020 Epoch 140/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2774 - accuracy: 0.9012 - val_loss: 0.3152 - val_accuracy: 0.8872 Epoch 141/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2738 - accuracy: 0.9024 - val_loss: 0.4013 - val_accuracy: 0.8472 Epoch 142/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2945 - accuracy: 0.8860 - val_loss: 0.3022 - val_accuracy: 0.8928 Epoch 143/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2758 - accuracy: 0.8980 - val_loss: 0.2889 - val_accuracy: 0.8976 Epoch 144/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2514 - accuracy: 0.9108 - val_loss: 0.2634 - val_accuracy: 0.9100 Epoch 145/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2428 - accuracy: 0.9152 - val_loss: 0.3172 - val_accuracy: 0.8828 Epoch 146/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3130 - accuracy: 0.8796 - val_loss: 0.3579 - val_accuracy: 0.8748 Epoch 147/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3286 - accuracy: 0.8768 - val_loss: 0.3573 - val_accuracy: 0.8692 Epoch 148/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3233 - accuracy: 0.8712 - val_loss: 0.3673 - val_accuracy: 0.8608 Epoch 149/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3389 - accuracy: 0.8708 - val_loss: 0.3232 - val_accuracy: 0.8768 Epoch 150/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2951 - accuracy: 0.8892 - val_loss: 0.2972 - val_accuracy: 0.8940 Epoch 151/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2731 - accuracy: 0.9028 - val_loss: 0.2910 - val_accuracy: 0.8988 Epoch 152/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2837 - accuracy: 0.8960 - val_loss: 0.2896 - val_accuracy: 0.9000 Epoch 153/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2645 - accuracy: 0.9060 - val_loss: 0.2656 - val_accuracy: 0.9052 Epoch 154/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2674 - accuracy: 0.9060 - val_loss: 0.2817 - val_accuracy: 0.8948 Epoch 155/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2627 - accuracy: 0.9004 - val_loss: 0.2649 - val_accuracy: 0.9096 Epoch 156/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2335 - accuracy: 0.9136 - val_loss: 0.2634 - val_accuracy: 0.9152 Epoch 157/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2248 - accuracy: 0.9188 - val_loss: 0.2377 - val_accuracy: 0.9204 Epoch 158/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2385 - accuracy: 0.9156 - val_loss: 0.2319 - val_accuracy: 0.9252 Epoch 159/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2186 - accuracy: 0.9264 - val_loss: 0.2438 - val_accuracy: 0.9228 Epoch 160/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2174 - accuracy: 0.9232 - val_loss: 0.2436 - val_accuracy: 0.9128 Epoch 161/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2391 - accuracy: 0.9180 - val_loss: 0.2516 - val_accuracy: 0.9128 Epoch 162/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2556 - accuracy: 0.9036 - val_loss: 0.2540 - val_accuracy: 0.9088 Epoch 163/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2490 - accuracy: 0.9044 - val_loss: 0.2756 - val_accuracy: 0.9000 Epoch 164/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2661 - accuracy: 0.8964 - val_loss: 0.2696 - val_accuracy: 0.9040 Epoch 165/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2556 - accuracy: 0.9008 - val_loss: 0.2776 - val_accuracy: 0.8924 Epoch 166/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2363 - accuracy: 0.9112 - val_loss: 0.2491 - val_accuracy: 0.9100 Epoch 167/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2443 - accuracy: 0.9072 - val_loss: 0.2796 - val_accuracy: 0.9028 Epoch 168/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2390 - accuracy: 0.9080 - val_loss: 0.2637 - val_accuracy: 0.9100 Epoch 169/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2500 - accuracy: 0.9116 - val_loss: 0.2620 - val_accuracy: 0.9072 Epoch 170/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2543 - accuracy: 0.9076 - val_loss: 0.2640 - val_accuracy: 0.9072 Epoch 171/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2407 - accuracy: 0.9176 - val_loss: 0.3020 - val_accuracy: 0.8964 Epoch 172/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2992 - accuracy: 0.8896 - val_loss: 0.2675 - val_accuracy: 0.9076 Epoch 173/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2341 - accuracy: 0.9220 - val_loss: 0.2504 - val_accuracy: 0.9132 Epoch 174/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2303 - accuracy: 0.9220 - val_loss: 0.2278 - val_accuracy: 0.9256 Epoch 175/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2244 - accuracy: 0.9284 - val_loss: 0.2255 - val_accuracy: 0.9248 Epoch 176/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2026 - accuracy: 0.9328 - val_loss: 0.2065 - val_accuracy: 0.9332 Epoch 177/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1963 - accuracy: 0.9372 - val_loss: 0.2063 - val_accuracy: 0.9296 Epoch 178/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2004 - accuracy: 0.9288 - val_loss: 0.2070 - val_accuracy: 0.9320 Epoch 179/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2023 - accuracy: 0.9356 - val_loss: 0.2043 - val_accuracy: 0.9364 Epoch 180/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1972 - accuracy: 0.9364 - val_loss: 0.1977 - val_accuracy: 0.9360 Epoch 181/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2088 - accuracy: 0.9320 - val_loss: 0.2822 - val_accuracy: 0.9024 Epoch 182/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2404 - accuracy: 0.9160 - val_loss: 0.2308 - val_accuracy: 0.9224 Epoch 183/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2220 - accuracy: 0.9260 - val_loss: 0.2359 - val_accuracy: 0.9180 Epoch 184/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2148 - accuracy: 0.9248 - val_loss: 0.2289 - val_accuracy: 0.9208 Epoch 185/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1943 - accuracy: 0.9332 - val_loss: 0.2050 - val_accuracy: 0.9304 Epoch 186/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1820 - accuracy: 0.9384 - val_loss: 0.2460 - val_accuracy: 0.9156 Epoch 187/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2313 - accuracy: 0.9192 - val_loss: 0.2650 - val_accuracy: 0.9128 Epoch 188/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2283 - accuracy: 0.9176 - val_loss: 0.2776 - val_accuracy: 0.9100 Epoch 189/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2688 - accuracy: 0.8980 - val_loss: 0.3051 - val_accuracy: 0.8956 Epoch 190/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2721 - accuracy: 0.9004 - val_loss: 0.2945 - val_accuracy: 0.9008 Epoch 191/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2764 - accuracy: 0.8976 - val_loss: 0.2676 - val_accuracy: 0.9076 Epoch 192/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2493 - accuracy: 0.9128 - val_loss: 0.2671 - val_accuracy: 0.9052 Epoch 193/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2263 - accuracy: 0.9160 - val_loss: 0.2581 - val_accuracy: 0.9108 Epoch 194/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2231 - accuracy: 0.9200 - val_loss: 0.2770 - val_accuracy: 0.9044 Epoch 195/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2348 - accuracy: 0.9184 - val_loss: 0.2392 - val_accuracy: 0.9176 Epoch 196/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1979 - accuracy: 0.9292 - val_loss: 0.2347 - val_accuracy: 0.9224 Epoch 197/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2099 - accuracy: 0.9264 - val_loss: 0.2498 - val_accuracy: 0.9144 Epoch 198/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2169 - accuracy: 0.9180 - val_loss: 0.2435 - val_accuracy: 0.9176 Epoch 199/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2105 - accuracy: 0.9240 - val_loss: 0.3108 - val_accuracy: 0.8824 Epoch 200/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4865 - accuracy: 0.7912 - val_loss: 0.3834 - val_accuracy: 0.8304 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec38311eb8> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1fb28978> # Make the problem harder by making T larger T = 20 D = 1 X = [] Y = [] for t in range ( 5000 ): x = np . random . randn ( T ) X . append ( x ) y = get_label ( x , 0 , 1 , 2 ) # long distance Y . append ( y ) X = np . array ( X ) Y = np . array ( Y ) N = len ( X ) # Now test our Simple RNN again inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = SimpleRNN ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 18ms/step - loss: 0.7027 - accuracy: 0.4816 - val_loss: 0.6930 - val_accuracy: 0.5052 Epoch 2/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6946 - val_accuracy: 0.5068 Epoch 3/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6920 - accuracy: 0.5136 - val_loss: 0.6973 - val_accuracy: 0.5020 Epoch 4/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6942 - accuracy: 0.5048 - val_loss: 0.6945 - val_accuracy: 0.4984 Epoch 5/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5068 - val_loss: 0.6942 - val_accuracy: 0.5088 Epoch 6/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6934 - accuracy: 0.5076 - val_loss: 0.6945 - val_accuracy: 0.5036 Epoch 7/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5204 - val_loss: 0.6951 - val_accuracy: 0.5004 Epoch 8/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5120 - val_loss: 0.6952 - val_accuracy: 0.5104 Epoch 9/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6921 - accuracy: 0.5272 - val_loss: 0.6949 - val_accuracy: 0.4976 Epoch 10/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6916 - accuracy: 0.5252 - val_loss: 0.6957 - val_accuracy: 0.5100 Epoch 11/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5172 - val_loss: 0.6950 - val_accuracy: 0.5052 Epoch 12/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6922 - accuracy: 0.5268 - val_loss: 0.6953 - val_accuracy: 0.4996 Epoch 13/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6919 - accuracy: 0.5252 - val_loss: 0.6955 - val_accuracy: 0.5100 Epoch 14/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6910 - accuracy: 0.5280 - val_loss: 0.6965 - val_accuracy: 0.5016 Epoch 15/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5296 - val_loss: 0.6960 - val_accuracy: 0.5060 Epoch 16/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6916 - accuracy: 0.5276 - val_loss: 0.6968 - val_accuracy: 0.4972 Epoch 17/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5388 - val_loss: 0.6971 - val_accuracy: 0.4988 Epoch 18/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.5200 - val_loss: 0.6969 - val_accuracy: 0.4908 Epoch 19/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6908 - accuracy: 0.5384 - val_loss: 0.6991 - val_accuracy: 0.4932 Epoch 20/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5480 - val_loss: 0.6964 - val_accuracy: 0.5024 Epoch 21/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6912 - accuracy: 0.5296 - val_loss: 0.6976 - val_accuracy: 0.4924 Epoch 22/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5304 - val_loss: 0.6980 - val_accuracy: 0.4976 Epoch 23/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5408 - val_loss: 0.6993 - val_accuracy: 0.5024 Epoch 24/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5368 - val_loss: 0.6974 - val_accuracy: 0.5044 Epoch 25/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6892 - accuracy: 0.5424 - val_loss: 0.6965 - val_accuracy: 0.5112 Epoch 26/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6882 - accuracy: 0.5452 - val_loss: 0.6970 - val_accuracy: 0.4920 Epoch 27/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6896 - accuracy: 0.5436 - val_loss: 0.6984 - val_accuracy: 0.4944 Epoch 28/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6897 - accuracy: 0.5428 - val_loss: 0.6984 - val_accuracy: 0.4944 Epoch 29/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6887 - accuracy: 0.5392 - val_loss: 0.6974 - val_accuracy: 0.5076 Epoch 30/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6901 - accuracy: 0.5380 - val_loss: 0.6968 - val_accuracy: 0.5000 Epoch 31/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6895 - accuracy: 0.5420 - val_loss: 0.6970 - val_accuracy: 0.5016 Epoch 32/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6891 - accuracy: 0.5508 - val_loss: 0.7012 - val_accuracy: 0.4816 Epoch 33/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6901 - accuracy: 0.5316 - val_loss: 0.6957 - val_accuracy: 0.5072 Epoch 34/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6916 - accuracy: 0.5228 - val_loss: 0.6947 - val_accuracy: 0.5016 Epoch 35/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6880 - accuracy: 0.5452 - val_loss: 0.6961 - val_accuracy: 0.5084 Epoch 36/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6933 - accuracy: 0.5080 - val_loss: 0.6973 - val_accuracy: 0.5136 Epoch 37/200 79/79 [==============================] - 1s 18ms/step - loss: 0.6946 - accuracy: 0.5060 - val_loss: 0.6949 - val_accuracy: 0.5072 Epoch 38/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5168 - val_loss: 0.6961 - val_accuracy: 0.5128 Epoch 39/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5176 - val_loss: 0.6916 - val_accuracy: 0.5384 Epoch 40/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5164 - val_loss: 0.6925 - val_accuracy: 0.5152 Epoch 41/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6933 - accuracy: 0.5176 - val_loss: 0.6920 - val_accuracy: 0.5288 Epoch 42/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5304 - val_loss: 0.6919 - val_accuracy: 0.5264 Epoch 43/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6913 - accuracy: 0.5156 - val_loss: 0.6940 - val_accuracy: 0.5264 Epoch 44/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6920 - accuracy: 0.5380 - val_loss: 0.6931 - val_accuracy: 0.5292 Epoch 45/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6918 - accuracy: 0.5336 - val_loss: 0.6919 - val_accuracy: 0.5276 Epoch 46/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6913 - accuracy: 0.5380 - val_loss: 0.6929 - val_accuracy: 0.4956 Epoch 47/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6921 - accuracy: 0.5256 - val_loss: 0.6917 - val_accuracy: 0.5308 Epoch 48/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6906 - accuracy: 0.5348 - val_loss: 0.6926 - val_accuracy: 0.5300 Epoch 49/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6903 - accuracy: 0.5400 - val_loss: 0.6926 - val_accuracy: 0.5184 Epoch 50/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6901 - accuracy: 0.5412 - val_loss: 0.6920 - val_accuracy: 0.5312 Epoch 51/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6902 - accuracy: 0.5416 - val_loss: 0.6933 - val_accuracy: 0.5308 Epoch 52/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6889 - accuracy: 0.5412 - val_loss: 0.6945 - val_accuracy: 0.5316 Epoch 53/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6894 - accuracy: 0.5456 - val_loss: 0.6920 - val_accuracy: 0.5324 Epoch 54/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5420 - val_loss: 0.6942 - val_accuracy: 0.5328 Epoch 55/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6891 - accuracy: 0.5452 - val_loss: 0.6933 - val_accuracy: 0.5324 Epoch 56/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6903 - accuracy: 0.5436 - val_loss: 0.6931 - val_accuracy: 0.5308 Epoch 57/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6894 - accuracy: 0.5420 - val_loss: 0.6925 - val_accuracy: 0.5340 Epoch 58/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6882 - accuracy: 0.5404 - val_loss: 0.6930 - val_accuracy: 0.5332 Epoch 59/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6909 - accuracy: 0.5404 - val_loss: 0.6931 - val_accuracy: 0.5152 Epoch 60/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6908 - accuracy: 0.5328 - val_loss: 0.6930 - val_accuracy: 0.5256 Epoch 61/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6908 - accuracy: 0.5388 - val_loss: 0.6929 - val_accuracy: 0.5324 Epoch 62/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6909 - accuracy: 0.5292 - val_loss: 0.6923 - val_accuracy: 0.5328 Epoch 63/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6899 - accuracy: 0.5396 - val_loss: 0.6932 - val_accuracy: 0.5204 Epoch 64/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6904 - accuracy: 0.5416 - val_loss: 0.6921 - val_accuracy: 0.5244 Epoch 65/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6912 - accuracy: 0.5332 - val_loss: 0.6913 - val_accuracy: 0.5396 Epoch 66/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6938 - accuracy: 0.5248 - val_loss: 0.7026 - val_accuracy: 0.4960 Epoch 67/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6948 - accuracy: 0.5192 - val_loss: 0.6984 - val_accuracy: 0.5024 Epoch 68/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6944 - accuracy: 0.5088 - val_loss: 0.6966 - val_accuracy: 0.4920 Epoch 69/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6936 - accuracy: 0.5156 - val_loss: 0.6955 - val_accuracy: 0.5000 Epoch 70/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6914 - accuracy: 0.5328 - val_loss: 0.6915 - val_accuracy: 0.5332 Epoch 71/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6908 - accuracy: 0.5444 - val_loss: 0.6909 - val_accuracy: 0.5380 Epoch 72/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5408 - val_loss: 0.6921 - val_accuracy: 0.5360 Epoch 73/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6915 - accuracy: 0.5428 - val_loss: 0.6911 - val_accuracy: 0.5356 Epoch 74/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6902 - accuracy: 0.5456 - val_loss: 0.6911 - val_accuracy: 0.5356 Epoch 75/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6911 - accuracy: 0.5392 - val_loss: 0.6921 - val_accuracy: 0.5396 Epoch 76/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6900 - accuracy: 0.5448 - val_loss: 0.6911 - val_accuracy: 0.5352 Epoch 77/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6917 - accuracy: 0.5384 - val_loss: 0.6918 - val_accuracy: 0.5348 Epoch 78/200 79/79 [==============================] - 1s 18ms/step - loss: 0.6886 - accuracy: 0.5484 - val_loss: 0.6914 - val_accuracy: 0.5348 Epoch 79/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5084 - val_loss: 0.6937 - val_accuracy: 0.5024 Epoch 80/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6942 - accuracy: 0.4972 - val_loss: 0.6940 - val_accuracy: 0.5084 Epoch 81/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6947 - accuracy: 0.5064 - val_loss: 0.6944 - val_accuracy: 0.4936 Epoch 82/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5048 - val_loss: 0.6940 - val_accuracy: 0.5016 Epoch 83/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6945 - accuracy: 0.5036 - val_loss: 0.6940 - val_accuracy: 0.5060 Epoch 84/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.4968 - val_loss: 0.6940 - val_accuracy: 0.5052 Epoch 85/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6942 - accuracy: 0.4948 - val_loss: 0.6940 - val_accuracy: 0.4992 Epoch 86/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.5044 - val_loss: 0.6948 - val_accuracy: 0.5064 Epoch 87/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5116 - val_loss: 0.6954 - val_accuracy: 0.5060 Epoch 88/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.5100 - val_loss: 0.6943 - val_accuracy: 0.4996 Epoch 89/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6943 - accuracy: 0.5136 - val_loss: 0.6951 - val_accuracy: 0.4948 Epoch 90/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6958 - accuracy: 0.4892 - val_loss: 0.6947 - val_accuracy: 0.4976 Epoch 91/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5016 - val_loss: 0.6944 - val_accuracy: 0.5016 Epoch 92/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6941 - accuracy: 0.5028 - val_loss: 0.6953 - val_accuracy: 0.4940 Epoch 93/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5112 - val_loss: 0.6955 - val_accuracy: 0.5056 Epoch 94/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6932 - accuracy: 0.5056 - val_loss: 0.6952 - val_accuracy: 0.4956 Epoch 95/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6942 - accuracy: 0.5064 - val_loss: 0.6950 - val_accuracy: 0.4992 Epoch 96/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6947 - accuracy: 0.5012 - val_loss: 0.6952 - val_accuracy: 0.4980 Epoch 97/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5092 - val_loss: 0.7003 - val_accuracy: 0.4932 Epoch 98/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6956 - accuracy: 0.4948 - val_loss: 0.6956 - val_accuracy: 0.4876 Epoch 99/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6935 - accuracy: 0.5076 - val_loss: 0.6955 - val_accuracy: 0.4872 Epoch 100/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5036 - val_loss: 0.6973 - val_accuracy: 0.4996 Epoch 101/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.5088 - val_loss: 0.6957 - val_accuracy: 0.4860 Epoch 102/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.4972 - val_loss: 0.6959 - val_accuracy: 0.4928 Epoch 103/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.4980 - val_loss: 0.6958 - val_accuracy: 0.4940 Epoch 104/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5208 - val_loss: 0.6985 - val_accuracy: 0.5020 Epoch 105/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6942 - accuracy: 0.5188 - val_loss: 0.6963 - val_accuracy: 0.4852 Epoch 106/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5036 - val_loss: 0.6962 - val_accuracy: 0.4908 Epoch 107/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6926 - accuracy: 0.5136 - val_loss: 0.6973 - val_accuracy: 0.4940 Epoch 108/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6933 - accuracy: 0.5100 - val_loss: 0.6961 - val_accuracy: 0.4920 Epoch 109/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.6963 - val_accuracy: 0.4932 Epoch 110/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.5132 - val_loss: 0.6964 - val_accuracy: 0.4916 Epoch 111/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5224 - val_loss: 0.6963 - val_accuracy: 0.4952 Epoch 112/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6935 - accuracy: 0.5080 - val_loss: 0.6976 - val_accuracy: 0.4972 Epoch 113/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6952 - accuracy: 0.5036 - val_loss: 0.6938 - val_accuracy: 0.4956 Epoch 114/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6944 - accuracy: 0.4964 - val_loss: 0.6938 - val_accuracy: 0.4980 Epoch 115/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.4900 - val_loss: 0.6963 - val_accuracy: 0.5040 Epoch 116/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6963 - accuracy: 0.4868 - val_loss: 0.6966 - val_accuracy: 0.5044 Epoch 117/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6966 - accuracy: 0.4960 - val_loss: 0.6944 - val_accuracy: 0.4992 Epoch 118/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6954 - accuracy: 0.4972 - val_loss: 0.6948 - val_accuracy: 0.5048 Epoch 119/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6956 - accuracy: 0.4964 - val_loss: 0.6938 - val_accuracy: 0.5048 Epoch 120/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6961 - accuracy: 0.4980 - val_loss: 0.6965 - val_accuracy: 0.4988 Epoch 121/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.5016 - val_loss: 0.6941 - val_accuracy: 0.5040 Epoch 122/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6946 - accuracy: 0.5012 - val_loss: 0.6941 - val_accuracy: 0.4944 Epoch 123/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.4932 - val_loss: 0.6953 - val_accuracy: 0.5024 Epoch 124/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5020 - val_loss: 0.6942 - val_accuracy: 0.4936 Epoch 125/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.4956 - val_loss: 0.6975 - val_accuracy: 0.4972 Epoch 126/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6956 - accuracy: 0.4936 - val_loss: 0.6952 - val_accuracy: 0.4888 Epoch 127/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6946 - accuracy: 0.5008 - val_loss: 0.6954 - val_accuracy: 0.4856 Epoch 128/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6932 - accuracy: 0.5124 - val_loss: 0.6959 - val_accuracy: 0.4892 Epoch 129/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5072 - val_loss: 0.6971 - val_accuracy: 0.4960 Epoch 130/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5052 - val_loss: 0.6967 - val_accuracy: 0.4984 Epoch 131/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.5012 - val_loss: 0.6962 - val_accuracy: 0.4948 Epoch 132/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6961 - accuracy: 0.5032 - val_loss: 0.6959 - val_accuracy: 0.4908 Epoch 133/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.4920 - val_loss: 0.6958 - val_accuracy: 0.4884 Epoch 134/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6935 - accuracy: 0.4932 - val_loss: 0.6975 - val_accuracy: 0.4980 Epoch 135/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.4996 - val_loss: 0.6961 - val_accuracy: 0.4956 Epoch 136/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.4968 - val_loss: 0.6952 - val_accuracy: 0.5000 Epoch 137/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.5248 - val_loss: 0.6963 - val_accuracy: 0.4888 Epoch 138/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.5112 - val_loss: 0.6959 - val_accuracy: 0.4904 Epoch 139/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.4996 - val_loss: 0.6970 - val_accuracy: 0.4996 Epoch 140/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6949 - accuracy: 0.5024 - val_loss: 0.6956 - val_accuracy: 0.4960 Epoch 141/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6943 - accuracy: 0.5040 - val_loss: 0.6969 - val_accuracy: 0.4856 Epoch 142/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.5048 - val_loss: 0.6966 - val_accuracy: 0.4812 Epoch 143/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6971 - val_accuracy: 0.4892 Epoch 144/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5196 - val_loss: 0.6970 - val_accuracy: 0.4808 Epoch 145/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6934 - accuracy: 0.5040 - val_loss: 0.6967 - val_accuracy: 0.4896 Epoch 146/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6932 - accuracy: 0.5024 - val_loss: 0.6959 - val_accuracy: 0.4872 Epoch 147/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.5028 - val_loss: 0.6958 - val_accuracy: 0.4972 Epoch 148/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5140 - val_loss: 0.6971 - val_accuracy: 0.4856 Epoch 149/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6933 - accuracy: 0.5088 - val_loss: 0.6978 - val_accuracy: 0.5008 Epoch 150/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.5036 - val_loss: 0.6964 - val_accuracy: 0.4940 Epoch 151/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6949 - accuracy: 0.4984 - val_loss: 0.6959 - val_accuracy: 0.4832 Epoch 152/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6925 - accuracy: 0.5108 - val_loss: 0.6976 - val_accuracy: 0.4940 Epoch 153/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6931 - accuracy: 0.5144 - val_loss: 0.6967 - val_accuracy: 0.4924 Epoch 154/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5168 - val_loss: 0.6970 - val_accuracy: 0.4916 Epoch 155/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5156 - val_loss: 0.6972 - val_accuracy: 0.4924 Epoch 156/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5200 - val_loss: 0.6979 - val_accuracy: 0.4812 Epoch 157/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.4996 - val_loss: 0.7001 - val_accuracy: 0.4912 Epoch 158/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6921 - accuracy: 0.5140 - val_loss: 0.7003 - val_accuracy: 0.4912 Epoch 159/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5104 - val_loss: 0.6973 - val_accuracy: 0.4984 Epoch 160/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5108 - val_loss: 0.6975 - val_accuracy: 0.4776 Epoch 161/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6968 - val_accuracy: 0.4948 Epoch 162/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5172 - val_loss: 0.6981 - val_accuracy: 0.4848 Epoch 163/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6944 - accuracy: 0.5112 - val_loss: 0.6986 - val_accuracy: 0.4816 Epoch 164/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6941 - accuracy: 0.5028 - val_loss: 0.6976 - val_accuracy: 0.4792 Epoch 165/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5188 - val_loss: 0.6971 - val_accuracy: 0.4892 Epoch 166/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5132 - val_loss: 0.6994 - val_accuracy: 0.4840 Epoch 167/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6980 - val_accuracy: 0.4964 Epoch 168/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6951 - accuracy: 0.5156 - val_loss: 0.6974 - val_accuracy: 0.4840 Epoch 169/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5172 - val_loss: 0.6975 - val_accuracy: 0.4836 Epoch 170/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5128 - val_loss: 0.6982 - val_accuracy: 0.4820 Epoch 171/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.5176 - val_loss: 0.6980 - val_accuracy: 0.4812 Epoch 172/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.4972 - val_loss: 0.6972 - val_accuracy: 0.4904 Epoch 173/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5036 - val_loss: 0.6981 - val_accuracy: 0.4964 Epoch 174/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6917 - accuracy: 0.5152 - val_loss: 0.6992 - val_accuracy: 0.4964 Epoch 175/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6977 - val_accuracy: 0.4880 Epoch 176/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5144 - val_loss: 0.6969 - val_accuracy: 0.4976 Epoch 177/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6922 - accuracy: 0.5112 - val_loss: 0.6981 - val_accuracy: 0.4848 Epoch 178/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6934 - accuracy: 0.5112 - val_loss: 0.6968 - val_accuracy: 0.4888 Epoch 179/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6918 - accuracy: 0.5084 - val_loss: 0.6981 - val_accuracy: 0.5016 Epoch 180/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6919 - accuracy: 0.5268 - val_loss: 0.7017 - val_accuracy: 0.4892 Epoch 181/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5192 - val_loss: 0.6972 - val_accuracy: 0.4888 Epoch 182/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5052 - val_loss: 0.6991 - val_accuracy: 0.4936 Epoch 183/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5052 - val_loss: 0.6970 - val_accuracy: 0.4960 Epoch 184/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6941 - accuracy: 0.5016 - val_loss: 0.6965 - val_accuracy: 0.4872 Epoch 185/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5092 - val_loss: 0.6961 - val_accuracy: 0.4856 Epoch 186/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6934 - accuracy: 0.5116 - val_loss: 0.6959 - val_accuracy: 0.4956 Epoch 187/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5108 - val_loss: 0.6976 - val_accuracy: 0.4960 Epoch 188/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6932 - accuracy: 0.5152 - val_loss: 0.6958 - val_accuracy: 0.4932 Epoch 189/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5168 - val_loss: 0.6961 - val_accuracy: 0.4928 Epoch 190/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6965 - val_accuracy: 0.4960 Epoch 191/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5140 - val_loss: 0.7008 - val_accuracy: 0.4932 Epoch 192/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6973 - accuracy: 0.4968 - val_loss: 0.6977 - val_accuracy: 0.4908 Epoch 193/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6942 - accuracy: 0.5104 - val_loss: 0.6978 - val_accuracy: 0.4908 Epoch 194/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5052 - val_loss: 0.6965 - val_accuracy: 0.4872 Epoch 195/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.4968 - val_loss: 0.6968 - val_accuracy: 0.4868 Epoch 196/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6923 - accuracy: 0.5156 - val_loss: 0.6972 - val_accuracy: 0.4936 Epoch 197/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5112 - val_loss: 0.6971 - val_accuracy: 0.4964 Epoch 198/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5036 - val_loss: 0.6979 - val_accuracy: 0.4952 Epoch 199/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5272 - val_loss: 0.6964 - val_accuracy: 0.4904 Epoch 200/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6920 - accuracy: 0.5256 - val_loss: 0.6953 - val_accuracy: 0.4952 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec382d6550> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec260c87f0> # Now test our LSTM inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = LSTM ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6950 - accuracy: 0.5128 - val_loss: 0.6957 - val_accuracy: 0.4920 Epoch 2/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5152 - val_loss: 0.6974 - val_accuracy: 0.4920 Epoch 3/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6929 - accuracy: 0.5192 - val_loss: 0.6955 - val_accuracy: 0.4916 Epoch 4/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6924 - accuracy: 0.5248 - val_loss: 0.6950 - val_accuracy: 0.4952 Epoch 5/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6917 - accuracy: 0.5232 - val_loss: 0.6951 - val_accuracy: 0.4948 Epoch 6/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6910 - accuracy: 0.5232 - val_loss: 0.6979 - val_accuracy: 0.4968 Epoch 7/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6915 - accuracy: 0.5240 - val_loss: 0.6976 - val_accuracy: 0.4952 Epoch 8/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6915 - accuracy: 0.5228 - val_loss: 0.6959 - val_accuracy: 0.4944 Epoch 9/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6921 - accuracy: 0.5220 - val_loss: 0.6959 - val_accuracy: 0.4952 Epoch 10/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6917 - accuracy: 0.5212 - val_loss: 0.6945 - val_accuracy: 0.4984 Epoch 11/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6928 - accuracy: 0.5288 - val_loss: 0.6964 - val_accuracy: 0.4948 Epoch 12/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6913 - accuracy: 0.5256 - val_loss: 0.6964 - val_accuracy: 0.4980 Epoch 13/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6908 - accuracy: 0.5212 - val_loss: 0.6970 - val_accuracy: 0.4976 Epoch 14/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6911 - accuracy: 0.5296 - val_loss: 0.6945 - val_accuracy: 0.5048 Epoch 15/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6913 - accuracy: 0.5344 - val_loss: 0.6958 - val_accuracy: 0.4952 Epoch 16/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5260 - val_loss: 0.6966 - val_accuracy: 0.5020 Epoch 17/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6903 - accuracy: 0.5264 - val_loss: 0.6979 - val_accuracy: 0.4964 Epoch 18/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6911 - accuracy: 0.5236 - val_loss: 0.6947 - val_accuracy: 0.5028 Epoch 19/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6910 - accuracy: 0.5280 - val_loss: 0.6941 - val_accuracy: 0.5076 Epoch 20/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6895 - accuracy: 0.5248 - val_loss: 0.6970 - val_accuracy: 0.4944 Epoch 21/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6906 - accuracy: 0.5232 - val_loss: 0.6933 - val_accuracy: 0.5184 Epoch 22/200 79/79 [==============================] - 1s 9ms/step - loss: 0.6915 - accuracy: 0.5300 - val_loss: 0.6946 - val_accuracy: 0.5040 Epoch 23/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6897 - accuracy: 0.5268 - val_loss: 0.6984 - val_accuracy: 0.4928 Epoch 24/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6902 - accuracy: 0.5220 - val_loss: 0.6979 - val_accuracy: 0.4928 Epoch 25/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6906 - accuracy: 0.5356 - val_loss: 0.6967 - val_accuracy: 0.4996 Epoch 26/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6892 - accuracy: 0.5260 - val_loss: 0.6973 - val_accuracy: 0.5056 Epoch 27/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6907 - accuracy: 0.5264 - val_loss: 0.6952 - val_accuracy: 0.5012 Epoch 28/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6888 - accuracy: 0.5312 - val_loss: 0.6958 - val_accuracy: 0.5036 Epoch 29/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6908 - accuracy: 0.5228 - val_loss: 0.6958 - val_accuracy: 0.4976 Epoch 30/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6903 - accuracy: 0.5180 - val_loss: 0.6970 - val_accuracy: 0.5072 Epoch 31/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6903 - accuracy: 0.5264 - val_loss: 0.6950 - val_accuracy: 0.5020 Epoch 32/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6912 - accuracy: 0.5392 - val_loss: 0.6963 - val_accuracy: 0.4960 Epoch 33/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6904 - accuracy: 0.5352 - val_loss: 0.6961 - val_accuracy: 0.4952 Epoch 34/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.5300 - val_loss: 0.6978 - val_accuracy: 0.4980 Epoch 35/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6911 - accuracy: 0.5424 - val_loss: 0.6949 - val_accuracy: 0.4928 Epoch 36/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6895 - accuracy: 0.5336 - val_loss: 0.6994 - val_accuracy: 0.4976 Epoch 37/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6898 - accuracy: 0.5392 - val_loss: 0.7006 - val_accuracy: 0.4932 Epoch 38/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6908 - accuracy: 0.5372 - val_loss: 0.6993 - val_accuracy: 0.4900 Epoch 39/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6898 - accuracy: 0.5380 - val_loss: 0.6987 - val_accuracy: 0.4912 Epoch 40/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6903 - accuracy: 0.5412 - val_loss: 0.6995 - val_accuracy: 0.4960 Epoch 41/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6909 - accuracy: 0.5336 - val_loss: 0.6954 - val_accuracy: 0.5040 Epoch 42/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6917 - accuracy: 0.5352 - val_loss: 0.7014 - val_accuracy: 0.4920 Epoch 43/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6893 - accuracy: 0.5476 - val_loss: 0.6988 - val_accuracy: 0.4920 Epoch 44/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6890 - accuracy: 0.5420 - val_loss: 0.7003 - val_accuracy: 0.4876 Epoch 45/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6883 - accuracy: 0.5484 - val_loss: 0.7000 - val_accuracy: 0.4900 Epoch 46/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6871 - accuracy: 0.5492 - val_loss: 0.7006 - val_accuracy: 0.4912 Epoch 47/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6870 - accuracy: 0.5580 - val_loss: 0.7034 - val_accuracy: 0.4868 Epoch 48/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6869 - accuracy: 0.5492 - val_loss: 0.7030 - val_accuracy: 0.4868 Epoch 49/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6855 - accuracy: 0.5504 - val_loss: 0.7034 - val_accuracy: 0.4988 Epoch 50/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6852 - accuracy: 0.5448 - val_loss: 0.7031 - val_accuracy: 0.4992 Epoch 51/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6874 - accuracy: 0.5544 - val_loss: 0.7023 - val_accuracy: 0.4940 Epoch 52/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6857 - accuracy: 0.5536 - val_loss: 0.7048 - val_accuracy: 0.4868 Epoch 53/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6847 - accuracy: 0.5608 - val_loss: 0.7034 - val_accuracy: 0.4828 Epoch 54/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6851 - accuracy: 0.5548 - val_loss: 0.7062 - val_accuracy: 0.4940 Epoch 55/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6853 - accuracy: 0.5564 - val_loss: 0.7041 - val_accuracy: 0.4964 Epoch 56/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6812 - accuracy: 0.5648 - val_loss: 0.7075 - val_accuracy: 0.4884 Epoch 57/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6835 - accuracy: 0.5684 - val_loss: 0.7108 - val_accuracy: 0.4896 Epoch 58/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6863 - accuracy: 0.5604 - val_loss: 0.7074 - val_accuracy: 0.4996 Epoch 59/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6879 - accuracy: 0.5484 - val_loss: 0.7055 - val_accuracy: 0.4880 Epoch 60/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6865 - accuracy: 0.5476 - val_loss: 0.7043 - val_accuracy: 0.4912 Epoch 61/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6833 - accuracy: 0.5628 - val_loss: 0.7042 - val_accuracy: 0.4972 Epoch 62/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6821 - accuracy: 0.5628 - val_loss: 0.7035 - val_accuracy: 0.4888 Epoch 63/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6825 - accuracy: 0.5600 - val_loss: 0.7084 - val_accuracy: 0.4936 Epoch 64/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6828 - accuracy: 0.5652 - val_loss: 0.7069 - val_accuracy: 0.4880 Epoch 65/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6809 - accuracy: 0.5664 - val_loss: 0.7080 - val_accuracy: 0.4972 Epoch 66/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6824 - accuracy: 0.5536 - val_loss: 0.7075 - val_accuracy: 0.4940 Epoch 67/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6819 - accuracy: 0.5640 - val_loss: 0.7092 - val_accuracy: 0.4956 Epoch 68/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6825 - accuracy: 0.5644 - val_loss: 0.7102 - val_accuracy: 0.5000 Epoch 69/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6798 - accuracy: 0.5692 - val_loss: 0.7109 - val_accuracy: 0.4916 Epoch 70/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6840 - accuracy: 0.5624 - val_loss: 0.7090 - val_accuracy: 0.5016 Epoch 71/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6847 - accuracy: 0.5580 - val_loss: 0.7095 - val_accuracy: 0.4928 Epoch 72/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6828 - accuracy: 0.5640 - val_loss: 0.7064 - val_accuracy: 0.4980 Epoch 73/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6808 - accuracy: 0.5680 - val_loss: 0.7112 - val_accuracy: 0.4912 Epoch 74/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6827 - accuracy: 0.5624 - val_loss: 0.7086 - val_accuracy: 0.4960 Epoch 75/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.5500 - val_loss: 0.7074 - val_accuracy: 0.4996 Epoch 76/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6852 - accuracy: 0.5508 - val_loss: 0.7075 - val_accuracy: 0.4856 Epoch 77/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6807 - accuracy: 0.5648 - val_loss: 0.7109 - val_accuracy: 0.4880 Epoch 78/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6812 - accuracy: 0.5656 - val_loss: 0.7070 - val_accuracy: 0.4960 Epoch 79/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6792 - accuracy: 0.5696 - val_loss: 0.7127 - val_accuracy: 0.4900 Epoch 80/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6843 - accuracy: 0.5616 - val_loss: 0.7108 - val_accuracy: 0.5008 Epoch 81/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6813 - accuracy: 0.5672 - val_loss: 0.7063 - val_accuracy: 0.5052 Epoch 82/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6851 - accuracy: 0.5524 - val_loss: 0.7084 - val_accuracy: 0.4928 Epoch 83/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6839 - accuracy: 0.5564 - val_loss: 0.7019 - val_accuracy: 0.4904 Epoch 84/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6914 - accuracy: 0.5332 - val_loss: 0.6978 - val_accuracy: 0.4976 Epoch 85/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.5312 - val_loss: 0.6962 - val_accuracy: 0.4892 Epoch 86/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6908 - accuracy: 0.5304 - val_loss: 0.6967 - val_accuracy: 0.4964 Epoch 87/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6916 - accuracy: 0.5272 - val_loss: 0.7025 - val_accuracy: 0.4876 Epoch 88/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6898 - accuracy: 0.5372 - val_loss: 0.7023 - val_accuracy: 0.4896 Epoch 89/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6843 - accuracy: 0.5424 - val_loss: 0.7034 - val_accuracy: 0.4896 Epoch 90/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6820 - accuracy: 0.5524 - val_loss: 0.7054 - val_accuracy: 0.4904 Epoch 91/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6838 - accuracy: 0.5628 - val_loss: 0.7118 - val_accuracy: 0.4940 Epoch 92/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6814 - accuracy: 0.5612 - val_loss: 0.7041 - val_accuracy: 0.4948 Epoch 93/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6815 - accuracy: 0.5548 - val_loss: 0.7158 - val_accuracy: 0.4928 Epoch 94/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6828 - accuracy: 0.5540 - val_loss: 0.7145 - val_accuracy: 0.4968 Epoch 95/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6817 - accuracy: 0.5660 - val_loss: 0.7119 - val_accuracy: 0.4988 Epoch 96/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6814 - accuracy: 0.5616 - val_loss: 0.7085 - val_accuracy: 0.4924 Epoch 97/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6783 - accuracy: 0.5660 - val_loss: 0.7101 - val_accuracy: 0.4836 Epoch 98/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6785 - accuracy: 0.5740 - val_loss: 0.7114 - val_accuracy: 0.4920 Epoch 99/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6781 - accuracy: 0.5732 - val_loss: 0.7108 - val_accuracy: 0.5024 Epoch 100/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6798 - accuracy: 0.5660 - val_loss: 0.7075 - val_accuracy: 0.5036 Epoch 101/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6772 - accuracy: 0.5716 - val_loss: 0.7097 - val_accuracy: 0.5076 Epoch 102/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6769 - accuracy: 0.5672 - val_loss: 0.7078 - val_accuracy: 0.4948 Epoch 103/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6770 - accuracy: 0.5756 - val_loss: 0.7110 - val_accuracy: 0.4924 Epoch 104/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6851 - accuracy: 0.5656 - val_loss: 0.7182 - val_accuracy: 0.4900 Epoch 105/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6867 - accuracy: 0.5504 - val_loss: 0.7089 - val_accuracy: 0.4904 Epoch 106/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6828 - accuracy: 0.5616 - val_loss: 0.7127 - val_accuracy: 0.4940 Epoch 107/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6832 - accuracy: 0.5604 - val_loss: 0.7105 - val_accuracy: 0.4936 Epoch 108/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6829 - accuracy: 0.5648 - val_loss: 0.7085 - val_accuracy: 0.5072 Epoch 109/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6809 - accuracy: 0.5544 - val_loss: 0.7111 - val_accuracy: 0.4932 Epoch 110/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6786 - accuracy: 0.5724 - val_loss: 0.7088 - val_accuracy: 0.5012 Epoch 111/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6811 - accuracy: 0.5664 - val_loss: 0.7108 - val_accuracy: 0.5048 Epoch 112/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6837 - accuracy: 0.5660 - val_loss: 0.7079 - val_accuracy: 0.4864 Epoch 113/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6769 - accuracy: 0.5636 - val_loss: 0.7140 - val_accuracy: 0.4952 Epoch 114/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6745 - accuracy: 0.5728 - val_loss: 0.7131 - val_accuracy: 0.4852 Epoch 115/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6784 - accuracy: 0.5720 - val_loss: 0.7142 - val_accuracy: 0.4932 Epoch 116/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6758 - accuracy: 0.5760 - val_loss: 0.7186 - val_accuracy: 0.4848 Epoch 117/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6799 - accuracy: 0.5664 - val_loss: 0.7121 - val_accuracy: 0.5036 Epoch 118/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6773 - accuracy: 0.5712 - val_loss: 0.7136 - val_accuracy: 0.4928 Epoch 119/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6805 - accuracy: 0.5564 - val_loss: 0.7085 - val_accuracy: 0.4932 Epoch 120/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6805 - accuracy: 0.5696 - val_loss: 0.7131 - val_accuracy: 0.4940 Epoch 121/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6741 - accuracy: 0.5768 - val_loss: 0.7176 - val_accuracy: 0.4932 Epoch 122/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6787 - accuracy: 0.5628 - val_loss: 0.7120 - val_accuracy: 0.5040 Epoch 123/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6794 - accuracy: 0.5704 - val_loss: 0.7113 - val_accuracy: 0.4804 Epoch 124/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6757 - accuracy: 0.5660 - val_loss: 0.7138 - val_accuracy: 0.4992 Epoch 125/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6780 - accuracy: 0.5732 - val_loss: 0.7164 - val_accuracy: 0.4828 Epoch 126/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6807 - accuracy: 0.5684 - val_loss: 0.7191 - val_accuracy: 0.5000 Epoch 127/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6875 - accuracy: 0.5444 - val_loss: 0.7013 - val_accuracy: 0.4972 Epoch 128/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6853 - accuracy: 0.5520 - val_loss: 0.7029 - val_accuracy: 0.4948 Epoch 129/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6843 - accuracy: 0.5580 - val_loss: 0.7058 - val_accuracy: 0.4928 Epoch 130/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6816 - accuracy: 0.5588 - val_loss: 0.7081 - val_accuracy: 0.4976 Epoch 131/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6789 - accuracy: 0.5708 - val_loss: 0.7078 - val_accuracy: 0.4968 Epoch 132/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6753 - accuracy: 0.5728 - val_loss: 0.7179 - val_accuracy: 0.5016 Epoch 133/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6739 - accuracy: 0.5764 - val_loss: 0.7149 - val_accuracy: 0.4844 Epoch 134/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6778 - accuracy: 0.5736 - val_loss: 0.7147 - val_accuracy: 0.4948 Epoch 135/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6738 - accuracy: 0.5780 - val_loss: 0.7169 - val_accuracy: 0.4920 Epoch 136/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6772 - accuracy: 0.5724 - val_loss: 0.7114 - val_accuracy: 0.4948 Epoch 137/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6768 - accuracy: 0.5712 - val_loss: 0.7168 - val_accuracy: 0.4920 Epoch 138/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6744 - accuracy: 0.5664 - val_loss: 0.7164 - val_accuracy: 0.4912 Epoch 139/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6806 - accuracy: 0.5696 - val_loss: 0.7174 - val_accuracy: 0.4876 Epoch 140/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6798 - accuracy: 0.5688 - val_loss: 0.7134 - val_accuracy: 0.4912 Epoch 141/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6789 - accuracy: 0.5668 - val_loss: 0.7169 - val_accuracy: 0.4884 Epoch 142/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6832 - accuracy: 0.5728 - val_loss: 0.7132 - val_accuracy: 0.4864 Epoch 143/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6819 - accuracy: 0.5568 - val_loss: 0.7127 - val_accuracy: 0.4908 Epoch 144/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6875 - accuracy: 0.5420 - val_loss: 0.7054 - val_accuracy: 0.4872 Epoch 145/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6829 - accuracy: 0.5488 - val_loss: 0.7082 - val_accuracy: 0.4840 Epoch 146/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6781 - accuracy: 0.5668 - val_loss: 0.7158 - val_accuracy: 0.4892 Epoch 147/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6786 - accuracy: 0.5696 - val_loss: 0.7123 - val_accuracy: 0.4840 Epoch 148/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6804 - accuracy: 0.5632 - val_loss: 0.7165 - val_accuracy: 0.4936 Epoch 149/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6764 - accuracy: 0.5764 - val_loss: 0.7140 - val_accuracy: 0.4896 Epoch 150/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6780 - accuracy: 0.5700 - val_loss: 0.7158 - val_accuracy: 0.4940 Epoch 151/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6779 - accuracy: 0.5720 - val_loss: 0.7150 - val_accuracy: 0.4940 Epoch 152/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6754 - accuracy: 0.5696 - val_loss: 0.7141 - val_accuracy: 0.4920 Epoch 153/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6744 - accuracy: 0.5748 - val_loss: 0.7144 - val_accuracy: 0.4956 Epoch 154/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6754 - accuracy: 0.5728 - val_loss: 0.7138 - val_accuracy: 0.4996 Epoch 155/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6763 - accuracy: 0.5736 - val_loss: 0.7185 - val_accuracy: 0.4948 Epoch 156/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6787 - accuracy: 0.5664 - val_loss: 0.7184 - val_accuracy: 0.4964 Epoch 157/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6756 - accuracy: 0.5700 - val_loss: 0.7147 - val_accuracy: 0.4976 Epoch 158/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6770 - accuracy: 0.5720 - val_loss: 0.7144 - val_accuracy: 0.5036 Epoch 159/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6740 - accuracy: 0.5744 - val_loss: 0.7171 - val_accuracy: 0.4924 Epoch 160/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6726 - accuracy: 0.5796 - val_loss: 0.7143 - val_accuracy: 0.5036 Epoch 161/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6801 - accuracy: 0.5696 - val_loss: 0.7124 - val_accuracy: 0.5004 Epoch 162/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6783 - accuracy: 0.5664 - val_loss: 0.7123 - val_accuracy: 0.4916 Epoch 163/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6739 - accuracy: 0.5740 - val_loss: 0.7151 - val_accuracy: 0.5008 Epoch 164/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6759 - accuracy: 0.5708 - val_loss: 0.7175 - val_accuracy: 0.4952 Epoch 165/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6760 - accuracy: 0.5756 - val_loss: 0.7178 - val_accuracy: 0.4952 Epoch 166/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6767 - accuracy: 0.5712 - val_loss: 0.7180 - val_accuracy: 0.4896 Epoch 167/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6779 - accuracy: 0.5812 - val_loss: 0.7132 - val_accuracy: 0.5048 Epoch 168/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6794 - accuracy: 0.5560 - val_loss: 0.7107 - val_accuracy: 0.4932 Epoch 169/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6743 - accuracy: 0.5752 - val_loss: 0.7113 - val_accuracy: 0.4936 Epoch 170/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6805 - accuracy: 0.5592 - val_loss: 0.7030 - val_accuracy: 0.4968 Epoch 171/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6835 - accuracy: 0.5532 - val_loss: 0.7062 - val_accuracy: 0.4908 Epoch 172/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6796 - accuracy: 0.5536 - val_loss: 0.7065 - val_accuracy: 0.4956 Epoch 173/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6781 - accuracy: 0.5632 - val_loss: 0.7105 - val_accuracy: 0.4988 Epoch 174/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6777 - accuracy: 0.5668 - val_loss: 0.7129 - val_accuracy: 0.4916 Epoch 175/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6781 - accuracy: 0.5560 - val_loss: 0.7157 - val_accuracy: 0.4956 Epoch 176/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6735 - accuracy: 0.5756 - val_loss: 0.7128 - val_accuracy: 0.4848 Epoch 177/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6764 - accuracy: 0.5776 - val_loss: 0.7170 - val_accuracy: 0.4972 Epoch 178/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6800 - accuracy: 0.5668 - val_loss: 0.7169 - val_accuracy: 0.4964 Epoch 179/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6730 - accuracy: 0.5804 - val_loss: 0.7211 - val_accuracy: 0.4944 Epoch 180/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6750 - accuracy: 0.5796 - val_loss: 0.7169 - val_accuracy: 0.4928 Epoch 181/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6766 - accuracy: 0.5712 - val_loss: 0.7121 - val_accuracy: 0.4884 Epoch 182/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6724 - accuracy: 0.5820 - val_loss: 0.7210 - val_accuracy: 0.4832 Epoch 183/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6677 - accuracy: 0.5844 - val_loss: 0.7196 - val_accuracy: 0.4900 Epoch 184/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6702 - accuracy: 0.5776 - val_loss: 0.7210 - val_accuracy: 0.4912 Epoch 185/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6717 - accuracy: 0.5808 - val_loss: 0.7222 - val_accuracy: 0.4828 Epoch 186/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6698 - accuracy: 0.5836 - val_loss: 0.7229 - val_accuracy: 0.5016 Epoch 187/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6737 - accuracy: 0.5796 - val_loss: 0.7133 - val_accuracy: 0.4928 Epoch 188/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6708 - accuracy: 0.5784 - val_loss: 0.7147 - val_accuracy: 0.4884 Epoch 189/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6691 - accuracy: 0.5688 - val_loss: 0.7199 - val_accuracy: 0.4948 Epoch 190/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6674 - accuracy: 0.5800 - val_loss: 0.7200 - val_accuracy: 0.4912 Epoch 191/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6714 - accuracy: 0.5800 - val_loss: 0.7199 - val_accuracy: 0.4880 Epoch 192/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6718 - accuracy: 0.5776 - val_loss: 0.7248 - val_accuracy: 0.4900 Epoch 193/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6680 - accuracy: 0.5828 - val_loss: 0.7194 - val_accuracy: 0.4928 Epoch 194/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6706 - accuracy: 0.5704 - val_loss: 0.7193 - val_accuracy: 0.4980 Epoch 195/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6704 - accuracy: 0.5712 - val_loss: 0.7215 - val_accuracy: 0.4984 Epoch 196/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6731 - accuracy: 0.5708 - val_loss: 0.7238 - val_accuracy: 0.4948 Epoch 197/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6682 - accuracy: 0.5788 - val_loss: 0.7185 - val_accuracy: 0.4968 Epoch 198/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6705 - accuracy: 0.5724 - val_loss: 0.7211 - val_accuracy: 0.4916 Epoch 199/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6686 - accuracy: 0.5800 - val_loss: 0.7203 - val_accuracy: 0.5032 Epoch 200/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6706 - accuracy: 0.5860 - val_loss: 0.7207 - val_accuracy: 0.5036 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1fc08f60> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1f79cc50> # Now test our GRU inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = GRU ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 400 , validation_split = 0.5 , ) Epoch 1/400 79/79 [==============================] - 1s 9ms/step - loss: 0.6950 - accuracy: 0.4860 - val_loss: 0.6950 - val_accuracy: 0.4904 Epoch 2/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6942 - accuracy: 0.5080 - val_loss: 0.6976 - val_accuracy: 0.4940 Epoch 3/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6943 - accuracy: 0.5056 - val_loss: 0.6958 - val_accuracy: 0.4956 Epoch 4/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.4996 - val_loss: 0.6962 - val_accuracy: 0.4996 Epoch 5/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5180 - val_loss: 0.6959 - val_accuracy: 0.4964 Epoch 6/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.4900 - val_loss: 0.6958 - val_accuracy: 0.4916 Epoch 7/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.4936 - val_loss: 0.6952 - val_accuracy: 0.4928 Epoch 8/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.4980 - val_loss: 0.6952 - val_accuracy: 0.4920 Epoch 9/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.4936 - val_loss: 0.6950 - val_accuracy: 0.4868 Epoch 10/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5080 - val_loss: 0.6955 - val_accuracy: 0.4892 Epoch 11/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.4944 - val_loss: 0.6948 - val_accuracy: 0.4908 Epoch 12/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5056 - val_loss: 0.6948 - val_accuracy: 0.4904 Epoch 13/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5032 - val_loss: 0.6946 - val_accuracy: 0.4848 Epoch 14/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5016 - val_loss: 0.6954 - val_accuracy: 0.4940 Epoch 15/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6945 - val_accuracy: 0.4880 Epoch 16/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5044 - val_loss: 0.6948 - val_accuracy: 0.4868 Epoch 17/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.4896 - val_loss: 0.6953 - val_accuracy: 0.4852 Epoch 18/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5164 - val_loss: 0.6951 - val_accuracy: 0.4920 Epoch 19/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5092 - val_loss: 0.6952 - val_accuracy: 0.4888 Epoch 20/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5172 - val_loss: 0.6958 - val_accuracy: 0.4904 Epoch 21/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5132 - val_loss: 0.6953 - val_accuracy: 0.4876 Epoch 22/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5124 - val_loss: 0.6958 - val_accuracy: 0.4812 Epoch 23/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5084 - val_loss: 0.6968 - val_accuracy: 0.4844 Epoch 24/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6979 - val_accuracy: 0.4936 Epoch 25/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5180 - val_loss: 0.6977 - val_accuracy: 0.4884 Epoch 26/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5224 - val_loss: 0.6965 - val_accuracy: 0.5000 Epoch 27/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.5252 - val_loss: 0.6994 - val_accuracy: 0.4940 Epoch 28/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5348 - val_loss: 0.6968 - val_accuracy: 0.4888 Epoch 29/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6899 - accuracy: 0.5288 - val_loss: 0.6997 - val_accuracy: 0.4904 Epoch 30/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6894 - accuracy: 0.5280 - val_loss: 0.7009 - val_accuracy: 0.4960 Epoch 31/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.5288 - val_loss: 0.7015 - val_accuracy: 0.4860 Epoch 32/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6881 - accuracy: 0.5376 - val_loss: 0.6992 - val_accuracy: 0.4892 Epoch 33/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6859 - accuracy: 0.5480 - val_loss: 0.7075 - val_accuracy: 0.4868 Epoch 34/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6895 - accuracy: 0.5420 - val_loss: 0.7045 - val_accuracy: 0.4872 Epoch 35/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.5432 - val_loss: 0.7005 - val_accuracy: 0.5000 Epoch 36/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6863 - accuracy: 0.5384 - val_loss: 0.7020 - val_accuracy: 0.4920 Epoch 37/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6852 - accuracy: 0.5436 - val_loss: 0.7076 - val_accuracy: 0.4964 Epoch 38/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6861 - accuracy: 0.5520 - val_loss: 0.7036 - val_accuracy: 0.4956 Epoch 39/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6876 - accuracy: 0.5412 - val_loss: 0.7031 - val_accuracy: 0.4976 Epoch 40/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6829 - accuracy: 0.5528 - val_loss: 0.7050 - val_accuracy: 0.4852 Epoch 41/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6808 - accuracy: 0.5508 - val_loss: 0.7074 - val_accuracy: 0.4924 Epoch 42/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6825 - accuracy: 0.5540 - val_loss: 0.7150 - val_accuracy: 0.4892 Epoch 43/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6826 - accuracy: 0.5600 - val_loss: 0.7037 - val_accuracy: 0.4964 Epoch 44/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6846 - accuracy: 0.5484 - val_loss: 0.7099 - val_accuracy: 0.4920 Epoch 45/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6820 - accuracy: 0.5512 - val_loss: 0.7057 - val_accuracy: 0.4976 Epoch 46/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6797 - accuracy: 0.5572 - val_loss: 0.7067 - val_accuracy: 0.5048 Epoch 47/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6816 - accuracy: 0.5552 - val_loss: 0.7072 - val_accuracy: 0.4924 Epoch 48/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6789 - accuracy: 0.5652 - val_loss: 0.7108 - val_accuracy: 0.4888 Epoch 49/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5492 - val_loss: 0.7064 - val_accuracy: 0.4904 Epoch 50/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6853 - accuracy: 0.5500 - val_loss: 0.7052 - val_accuracy: 0.4992 Epoch 51/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6806 - accuracy: 0.5544 - val_loss: 0.7067 - val_accuracy: 0.4944 Epoch 52/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6794 - accuracy: 0.5600 - val_loss: 0.7082 - val_accuracy: 0.4916 Epoch 53/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6785 - accuracy: 0.5640 - val_loss: 0.7079 - val_accuracy: 0.4964 Epoch 54/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6822 - accuracy: 0.5588 - val_loss: 0.7126 - val_accuracy: 0.4868 Epoch 55/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6784 - accuracy: 0.5576 - val_loss: 0.7119 - val_accuracy: 0.4916 Epoch 56/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6746 - accuracy: 0.5680 - val_loss: 0.7108 - val_accuracy: 0.4888 Epoch 57/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6762 - accuracy: 0.5708 - val_loss: 0.7140 - val_accuracy: 0.4888 Epoch 58/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6775 - accuracy: 0.5628 - val_loss: 0.7142 - val_accuracy: 0.4888 Epoch 59/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6749 - accuracy: 0.5664 - val_loss: 0.7183 - val_accuracy: 0.4972 Epoch 60/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6804 - accuracy: 0.5608 - val_loss: 0.7118 - val_accuracy: 0.4996 Epoch 61/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6873 - accuracy: 0.5540 - val_loss: 0.7166 - val_accuracy: 0.4948 Epoch 62/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5248 - val_loss: 0.7052 - val_accuracy: 0.4804 Epoch 63/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5168 - val_loss: 0.7017 - val_accuracy: 0.4920 Epoch 64/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.5060 - val_loss: 0.6979 - val_accuracy: 0.4944 Epoch 65/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5208 - val_loss: 0.6999 - val_accuracy: 0.4868 Epoch 66/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6896 - accuracy: 0.5296 - val_loss: 0.6996 - val_accuracy: 0.5008 Epoch 67/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5228 - val_loss: 0.6984 - val_accuracy: 0.4740 Epoch 68/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.5020 - val_loss: 0.6944 - val_accuracy: 0.5024 Epoch 69/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6936 - accuracy: 0.5144 - val_loss: 0.6962 - val_accuracy: 0.4984 Epoch 70/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.5048 - val_loss: 0.6949 - val_accuracy: 0.5056 Epoch 71/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6938 - accuracy: 0.5096 - val_loss: 0.6965 - val_accuracy: 0.5000 Epoch 72/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6953 - accuracy: 0.4960 - val_loss: 0.6938 - val_accuracy: 0.5044 Epoch 73/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6940 - accuracy: 0.5128 - val_loss: 0.6938 - val_accuracy: 0.5092 Epoch 74/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5108 - val_loss: 0.6935 - val_accuracy: 0.5128 Epoch 75/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6938 - accuracy: 0.5036 - val_loss: 0.6929 - val_accuracy: 0.5064 Epoch 76/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6955 - accuracy: 0.4864 - val_loss: 0.6932 - val_accuracy: 0.5016 Epoch 77/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6949 - accuracy: 0.4744 - val_loss: 0.6938 - val_accuracy: 0.4960 Epoch 78/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.4928 - val_loss: 0.6940 - val_accuracy: 0.4960 Epoch 79/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6942 - accuracy: 0.5060 - val_loss: 0.6936 - val_accuracy: 0.5064 Epoch 80/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5132 - val_loss: 0.6932 - val_accuracy: 0.5036 Epoch 81/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5204 - val_loss: 0.6924 - val_accuracy: 0.5108 Epoch 82/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6910 - accuracy: 0.5316 - val_loss: 0.6921 - val_accuracy: 0.5192 Epoch 83/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6917 - accuracy: 0.5284 - val_loss: 0.6917 - val_accuracy: 0.5288 Epoch 84/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6909 - accuracy: 0.5368 - val_loss: 0.6894 - val_accuracy: 0.5384 Epoch 85/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6892 - accuracy: 0.5384 - val_loss: 0.6890 - val_accuracy: 0.5424 Epoch 86/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6922 - accuracy: 0.5216 - val_loss: 0.6940 - val_accuracy: 0.5144 Epoch 87/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6903 - accuracy: 0.5336 - val_loss: 0.6966 - val_accuracy: 0.5168 Epoch 88/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5236 - val_loss: 0.6966 - val_accuracy: 0.5088 Epoch 89/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5020 - val_loss: 0.6943 - val_accuracy: 0.5016 Epoch 90/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5084 - val_loss: 0.6949 - val_accuracy: 0.4908 Epoch 91/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.5104 - val_loss: 0.6941 - val_accuracy: 0.5068 Epoch 92/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5088 - val_loss: 0.6956 - val_accuracy: 0.4920 Epoch 93/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.5028 - val_loss: 0.6952 - val_accuracy: 0.4936 Epoch 94/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5036 - val_loss: 0.6936 - val_accuracy: 0.4920 Epoch 95/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.5124 - val_loss: 0.6935 - val_accuracy: 0.4964 Epoch 96/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5144 - val_loss: 0.6941 - val_accuracy: 0.5132 Epoch 97/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5132 - val_loss: 0.6943 - val_accuracy: 0.5096 Epoch 98/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5148 - val_loss: 0.6938 - val_accuracy: 0.5092 Epoch 99/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5180 - val_loss: 0.6938 - val_accuracy: 0.5140 Epoch 100/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5328 - val_loss: 0.6930 - val_accuracy: 0.5176 Epoch 101/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6908 - accuracy: 0.5344 - val_loss: 0.6937 - val_accuracy: 0.5240 Epoch 102/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.5324 - val_loss: 0.6922 - val_accuracy: 0.5236 Epoch 103/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6894 - accuracy: 0.5380 - val_loss: 0.6928 - val_accuracy: 0.5296 Epoch 104/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6916 - accuracy: 0.5328 - val_loss: 0.6922 - val_accuracy: 0.5356 Epoch 105/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.5488 - val_loss: 0.6903 - val_accuracy: 0.5352 Epoch 106/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6903 - accuracy: 0.5304 - val_loss: 0.6894 - val_accuracy: 0.5328 Epoch 107/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6894 - accuracy: 0.5364 - val_loss: 0.6925 - val_accuracy: 0.5300 Epoch 108/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6891 - accuracy: 0.5404 - val_loss: 0.6910 - val_accuracy: 0.5400 Epoch 109/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5204 - val_loss: 0.6921 - val_accuracy: 0.5204 Epoch 110/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.5324 - val_loss: 0.6913 - val_accuracy: 0.5344 Epoch 111/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6901 - accuracy: 0.5484 - val_loss: 0.6911 - val_accuracy: 0.5384 Epoch 112/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6895 - accuracy: 0.5476 - val_loss: 0.6919 - val_accuracy: 0.5260 Epoch 113/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6875 - accuracy: 0.5532 - val_loss: 0.6918 - val_accuracy: 0.5296 Epoch 114/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5484 - val_loss: 0.6921 - val_accuracy: 0.5380 Epoch 115/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6878 - accuracy: 0.5408 - val_loss: 0.6925 - val_accuracy: 0.5208 Epoch 116/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.5484 - val_loss: 0.6903 - val_accuracy: 0.5364 Epoch 117/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6853 - accuracy: 0.5528 - val_loss: 0.6894 - val_accuracy: 0.5372 Epoch 118/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.5688 - val_loss: 0.6915 - val_accuracy: 0.5312 Epoch 119/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6834 - accuracy: 0.5680 - val_loss: 0.6895 - val_accuracy: 0.5488 Epoch 120/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6807 - accuracy: 0.5732 - val_loss: 0.6923 - val_accuracy: 0.5396 Epoch 121/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6870 - accuracy: 0.5492 - val_loss: 0.6869 - val_accuracy: 0.5532 Epoch 122/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6868 - accuracy: 0.5484 - val_loss: 0.6855 - val_accuracy: 0.5600 Epoch 123/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5564 - val_loss: 0.6847 - val_accuracy: 0.5568 Epoch 124/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6831 - accuracy: 0.5676 - val_loss: 0.6837 - val_accuracy: 0.5624 Epoch 125/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.5664 - val_loss: 0.6830 - val_accuracy: 0.5680 Epoch 126/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6801 - accuracy: 0.5716 - val_loss: 0.6814 - val_accuracy: 0.5740 Epoch 127/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6792 - accuracy: 0.5772 - val_loss: 0.6786 - val_accuracy: 0.5792 Epoch 128/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6772 - accuracy: 0.5888 - val_loss: 0.6780 - val_accuracy: 0.5784 Epoch 129/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6759 - accuracy: 0.5888 - val_loss: 0.6759 - val_accuracy: 0.5772 Epoch 130/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6715 - accuracy: 0.5900 - val_loss: 0.6708 - val_accuracy: 0.5920 Epoch 131/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6653 - accuracy: 0.5992 - val_loss: 0.6661 - val_accuracy: 0.6016 Epoch 132/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6691 - accuracy: 0.5964 - val_loss: 0.6636 - val_accuracy: 0.6092 Epoch 133/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6598 - accuracy: 0.6020 - val_loss: 0.6593 - val_accuracy: 0.6076 Epoch 134/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6520 - accuracy: 0.6056 - val_loss: 0.6446 - val_accuracy: 0.6276 Epoch 135/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6388 - accuracy: 0.6184 - val_loss: 0.6400 - val_accuracy: 0.6132 Epoch 136/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6322 - accuracy: 0.6208 - val_loss: 0.6191 - val_accuracy: 0.6404 Epoch 137/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6021 - accuracy: 0.6452 - val_loss: 0.6042 - val_accuracy: 0.6336 Epoch 138/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5900 - accuracy: 0.6408 - val_loss: 0.5920 - val_accuracy: 0.6360 Epoch 139/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5946 - accuracy: 0.6268 - val_loss: 0.5867 - val_accuracy: 0.6284 Epoch 140/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5686 - accuracy: 0.6468 - val_loss: 0.5758 - val_accuracy: 0.6464 Epoch 141/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5621 - accuracy: 0.6496 - val_loss: 0.5707 - val_accuracy: 0.6364 Epoch 142/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5562 - accuracy: 0.6496 - val_loss: 0.5659 - val_accuracy: 0.6428 Epoch 143/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5560 - accuracy: 0.6516 - val_loss: 0.5618 - val_accuracy: 0.6484 Epoch 144/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5496 - accuracy: 0.6544 - val_loss: 0.5625 - val_accuracy: 0.6468 Epoch 145/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5465 - accuracy: 0.6568 - val_loss: 0.5568 - val_accuracy: 0.6544 Epoch 146/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5427 - accuracy: 0.6592 - val_loss: 0.5825 - val_accuracy: 0.6384 Epoch 147/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5469 - accuracy: 0.6588 - val_loss: 0.5518 - val_accuracy: 0.6524 Epoch 148/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5413 - accuracy: 0.6608 - val_loss: 0.5475 - val_accuracy: 0.6512 Epoch 149/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5380 - accuracy: 0.6660 - val_loss: 0.5513 - val_accuracy: 0.6480 Epoch 150/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5360 - accuracy: 0.6648 - val_loss: 0.5449 - val_accuracy: 0.6592 Epoch 151/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5345 - accuracy: 0.6608 - val_loss: 0.5473 - val_accuracy: 0.6616 Epoch 152/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5334 - accuracy: 0.6676 - val_loss: 0.5457 - val_accuracy: 0.6520 Epoch 153/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5287 - accuracy: 0.6740 - val_loss: 0.5434 - val_accuracy: 0.6572 Epoch 154/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5241 - accuracy: 0.6708 - val_loss: 0.5523 - val_accuracy: 0.6556 Epoch 155/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5290 - accuracy: 0.6728 - val_loss: 0.5411 - val_accuracy: 0.6656 Epoch 156/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5232 - accuracy: 0.6764 - val_loss: 0.5415 - val_accuracy: 0.6664 Epoch 157/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5294 - accuracy: 0.6868 - val_loss: 0.5385 - val_accuracy: 0.6556 Epoch 158/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5276 - accuracy: 0.6740 - val_loss: 0.5327 - val_accuracy: 0.6648 Epoch 159/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5212 - accuracy: 0.6784 - val_loss: 0.5280 - val_accuracy: 0.6612 Epoch 160/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5185 - accuracy: 0.6800 - val_loss: 0.5347 - val_accuracy: 0.6644 Epoch 161/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5164 - accuracy: 0.6816 - val_loss: 0.5133 - val_accuracy: 0.6800 Epoch 162/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5066 - accuracy: 0.6896 - val_loss: 0.5063 - val_accuracy: 0.6920 Epoch 163/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5099 - accuracy: 0.6888 - val_loss: 0.5053 - val_accuracy: 0.6976 Epoch 164/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4994 - accuracy: 0.6952 - val_loss: 0.5000 - val_accuracy: 0.7000 Epoch 165/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5113 - accuracy: 0.7108 - val_loss: 0.4923 - val_accuracy: 0.7084 Epoch 166/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5311 - accuracy: 0.6740 - val_loss: 0.5261 - val_accuracy: 0.6888 Epoch 167/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5154 - accuracy: 0.6836 - val_loss: 0.5097 - val_accuracy: 0.6908 Epoch 168/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5073 - accuracy: 0.6976 - val_loss: 0.5069 - val_accuracy: 0.7012 Epoch 169/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4980 - accuracy: 0.7064 - val_loss: 0.5059 - val_accuracy: 0.7060 Epoch 170/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4964 - accuracy: 0.7020 - val_loss: 0.4987 - val_accuracy: 0.7032 Epoch 171/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4890 - accuracy: 0.7100 - val_loss: 0.4979 - val_accuracy: 0.7080 Epoch 172/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4920 - accuracy: 0.7112 - val_loss: 0.4929 - val_accuracy: 0.7068 Epoch 173/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4945 - accuracy: 0.7172 - val_loss: 0.5323 - val_accuracy: 0.7124 Epoch 174/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5101 - accuracy: 0.7104 - val_loss: 0.4942 - val_accuracy: 0.7080 Epoch 175/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4816 - accuracy: 0.7244 - val_loss: 0.4950 - val_accuracy: 0.7164 Epoch 176/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4822 - accuracy: 0.7224 - val_loss: 0.4951 - val_accuracy: 0.7048 Epoch 177/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4773 - accuracy: 0.7248 - val_loss: 0.4978 - val_accuracy: 0.7100 Epoch 178/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4834 - accuracy: 0.7220 - val_loss: 0.4915 - val_accuracy: 0.7148 Epoch 179/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4780 - accuracy: 0.7192 - val_loss: 0.4916 - val_accuracy: 0.7228 Epoch 180/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4803 - accuracy: 0.7252 - val_loss: 0.4898 - val_accuracy: 0.7164 Epoch 181/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4768 - accuracy: 0.7228 - val_loss: 0.4940 - val_accuracy: 0.7140 Epoch 182/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4748 - accuracy: 0.7256 - val_loss: 0.4916 - val_accuracy: 0.7168 Epoch 183/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4799 - accuracy: 0.7228 - val_loss: 0.4883 - val_accuracy: 0.7144 Epoch 184/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4753 - accuracy: 0.7268 - val_loss: 0.4849 - val_accuracy: 0.7172 Epoch 185/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4737 - accuracy: 0.7248 - val_loss: 0.4904 - val_accuracy: 0.7204 Epoch 186/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4682 - accuracy: 0.7288 - val_loss: 0.4914 - val_accuracy: 0.7136 Epoch 187/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4657 - accuracy: 0.7300 - val_loss: 0.4925 - val_accuracy: 0.7216 Epoch 188/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4694 - accuracy: 0.7284 - val_loss: 0.4925 - val_accuracy: 0.7148 Epoch 189/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4743 - accuracy: 0.7268 - val_loss: 0.4870 - val_accuracy: 0.7216 Epoch 190/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4759 - accuracy: 0.7336 - val_loss: 0.4797 - val_accuracy: 0.7240 Epoch 191/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4677 - accuracy: 0.7336 - val_loss: 0.4749 - val_accuracy: 0.7184 Epoch 192/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4645 - accuracy: 0.7320 - val_loss: 0.4803 - val_accuracy: 0.7260 Epoch 193/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4638 - accuracy: 0.7376 - val_loss: 0.4966 - val_accuracy: 0.7268 Epoch 194/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4659 - accuracy: 0.7340 - val_loss: 0.4818 - val_accuracy: 0.7308 Epoch 195/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4668 - accuracy: 0.7304 - val_loss: 0.4776 - val_accuracy: 0.7312 Epoch 196/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4604 - accuracy: 0.7356 - val_loss: 0.4731 - val_accuracy: 0.7304 Epoch 197/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4588 - accuracy: 0.7420 - val_loss: 0.4618 - val_accuracy: 0.7356 Epoch 198/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4508 - accuracy: 0.7452 - val_loss: 0.4548 - val_accuracy: 0.7432 Epoch 199/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4599 - accuracy: 0.7452 - val_loss: 0.4670 - val_accuracy: 0.7332 Epoch 200/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4463 - accuracy: 0.7540 - val_loss: 0.4577 - val_accuracy: 0.7496 Epoch 201/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4490 - accuracy: 0.7572 - val_loss: 0.4565 - val_accuracy: 0.7564 Epoch 202/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4400 - accuracy: 0.7680 - val_loss: 0.4427 - val_accuracy: 0.7744 Epoch 203/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4340 - accuracy: 0.7816 - val_loss: 0.4418 - val_accuracy: 0.7820 Epoch 204/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4214 - accuracy: 0.7848 - val_loss: 0.4242 - val_accuracy: 0.7752 Epoch 205/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4140 - accuracy: 0.7900 - val_loss: 0.4061 - val_accuracy: 0.7996 Epoch 206/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4110 - accuracy: 0.8032 - val_loss: 0.4196 - val_accuracy: 0.7864 Epoch 207/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4019 - accuracy: 0.8092 - val_loss: 0.4060 - val_accuracy: 0.8024 Epoch 208/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3966 - accuracy: 0.8056 - val_loss: 0.4329 - val_accuracy: 0.7864 Epoch 209/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4044 - accuracy: 0.8108 - val_loss: 0.4098 - val_accuracy: 0.8000 Epoch 210/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3890 - accuracy: 0.8208 - val_loss: 0.3925 - val_accuracy: 0.8220 Epoch 211/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3767 - accuracy: 0.8268 - val_loss: 0.3883 - val_accuracy: 0.8180 Epoch 212/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3725 - accuracy: 0.8320 - val_loss: 0.3958 - val_accuracy: 0.8180 Epoch 213/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3733 - accuracy: 0.8284 - val_loss: 0.3906 - val_accuracy: 0.8224 Epoch 214/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3736 - accuracy: 0.8304 - val_loss: 0.3767 - val_accuracy: 0.8264 Epoch 215/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3653 - accuracy: 0.8404 - val_loss: 0.3746 - val_accuracy: 0.8288 Epoch 216/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3565 - accuracy: 0.8388 - val_loss: 0.3837 - val_accuracy: 0.8244 Epoch 217/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3645 - accuracy: 0.8364 - val_loss: 0.3668 - val_accuracy: 0.8352 Epoch 218/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3478 - accuracy: 0.8392 - val_loss: 0.3597 - val_accuracy: 0.8404 Epoch 219/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3459 - accuracy: 0.8336 - val_loss: 0.3646 - val_accuracy: 0.8356 Epoch 220/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3489 - accuracy: 0.8388 - val_loss: 0.3582 - val_accuracy: 0.8344 Epoch 221/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3411 - accuracy: 0.8456 - val_loss: 0.3456 - val_accuracy: 0.8360 Epoch 222/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3370 - accuracy: 0.8488 - val_loss: 0.3502 - val_accuracy: 0.8384 Epoch 223/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3319 - accuracy: 0.8476 - val_loss: 0.3502 - val_accuracy: 0.8420 Epoch 224/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3363 - accuracy: 0.8472 - val_loss: 0.3430 - val_accuracy: 0.8436 Epoch 225/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3778 - accuracy: 0.8284 - val_loss: 0.4149 - val_accuracy: 0.8112 Epoch 226/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3839 - accuracy: 0.8228 - val_loss: 0.3709 - val_accuracy: 0.8284 Epoch 227/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3425 - accuracy: 0.8468 - val_loss: 0.3601 - val_accuracy: 0.8340 Epoch 228/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3409 - accuracy: 0.8440 - val_loss: 0.3505 - val_accuracy: 0.8432 Epoch 229/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3295 - accuracy: 0.8484 - val_loss: 0.3356 - val_accuracy: 0.8508 Epoch 230/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3588 - accuracy: 0.8436 - val_loss: 0.3536 - val_accuracy: 0.8308 Epoch 231/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3324 - accuracy: 0.8468 - val_loss: 0.3320 - val_accuracy: 0.8432 Epoch 232/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3206 - accuracy: 0.8556 - val_loss: 0.3609 - val_accuracy: 0.8368 Epoch 233/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3172 - accuracy: 0.8536 - val_loss: 0.3313 - val_accuracy: 0.8484 Epoch 234/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3154 - accuracy: 0.8528 - val_loss: 0.3404 - val_accuracy: 0.8504 Epoch 235/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3129 - accuracy: 0.8564 - val_loss: 0.3338 - val_accuracy: 0.8492 Epoch 236/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3169 - accuracy: 0.8620 - val_loss: 0.3350 - val_accuracy: 0.8488 Epoch 237/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3091 - accuracy: 0.8556 - val_loss: 0.3287 - val_accuracy: 0.8496 Epoch 238/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3036 - accuracy: 0.8608 - val_loss: 0.3361 - val_accuracy: 0.8496 Epoch 239/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3015 - accuracy: 0.8616 - val_loss: 0.3231 - val_accuracy: 0.8540 Epoch 240/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3041 - accuracy: 0.8676 - val_loss: 0.3379 - val_accuracy: 0.8528 Epoch 241/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3002 - accuracy: 0.8636 - val_loss: 0.3311 - val_accuracy: 0.8476 Epoch 242/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3036 - accuracy: 0.8636 - val_loss: 0.3258 - val_accuracy: 0.8544 Epoch 243/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3242 - accuracy: 0.8584 - val_loss: 0.3431 - val_accuracy: 0.8444 Epoch 244/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3018 - accuracy: 0.8628 - val_loss: 0.3199 - val_accuracy: 0.8536 Epoch 245/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3061 - accuracy: 0.8636 - val_loss: 0.3290 - val_accuracy: 0.8496 Epoch 246/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3023 - accuracy: 0.8604 - val_loss: 0.3210 - val_accuracy: 0.8520 Epoch 247/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2966 - accuracy: 0.8700 - val_loss: 0.3544 - val_accuracy: 0.8288 Epoch 248/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3162 - accuracy: 0.8492 - val_loss: 0.3154 - val_accuracy: 0.8520 Epoch 249/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3012 - accuracy: 0.8644 - val_loss: 0.3147 - val_accuracy: 0.8584 Epoch 250/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2968 - accuracy: 0.8632 - val_loss: 0.3246 - val_accuracy: 0.8500 Epoch 251/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2947 - accuracy: 0.8676 - val_loss: 0.3142 - val_accuracy: 0.8568 Epoch 252/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2884 - accuracy: 0.8688 - val_loss: 0.3040 - val_accuracy: 0.8576 Epoch 253/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2885 - accuracy: 0.8696 - val_loss: 0.3189 - val_accuracy: 0.8620 Epoch 254/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2943 - accuracy: 0.8696 - val_loss: 0.3055 - val_accuracy: 0.8516 Epoch 255/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2916 - accuracy: 0.8640 - val_loss: 0.3133 - val_accuracy: 0.8532 Epoch 256/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2931 - accuracy: 0.8664 - val_loss: 0.3389 - val_accuracy: 0.8536 Epoch 257/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2836 - accuracy: 0.8752 - val_loss: 0.3070 - val_accuracy: 0.8572 Epoch 258/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2816 - accuracy: 0.8720 - val_loss: 0.3079 - val_accuracy: 0.8592 Epoch 259/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2950 - accuracy: 0.8692 - val_loss: 0.3066 - val_accuracy: 0.8620 Epoch 260/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2838 - accuracy: 0.8736 - val_loss: 0.3033 - val_accuracy: 0.8620 Epoch 261/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2801 - accuracy: 0.8760 - val_loss: 0.3058 - val_accuracy: 0.8568 Epoch 262/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2814 - accuracy: 0.8772 - val_loss: 0.3037 - val_accuracy: 0.8636 Epoch 263/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2987 - accuracy: 0.8668 - val_loss: 0.3168 - val_accuracy: 0.8512 Epoch 264/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2747 - accuracy: 0.8816 - val_loss: 0.2969 - val_accuracy: 0.8680 Epoch 265/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2730 - accuracy: 0.8816 - val_loss: 0.3008 - val_accuracy: 0.8692 Epoch 266/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2710 - accuracy: 0.8812 - val_loss: 0.2961 - val_accuracy: 0.8620 Epoch 267/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2667 - accuracy: 0.8804 - val_loss: 0.2989 - val_accuracy: 0.8628 Epoch 268/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2685 - accuracy: 0.8832 - val_loss: 0.2945 - val_accuracy: 0.8676 Epoch 269/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2664 - accuracy: 0.8848 - val_loss: 0.2975 - val_accuracy: 0.8612 Epoch 270/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2732 - accuracy: 0.8800 - val_loss: 0.3056 - val_accuracy: 0.8532 Epoch 271/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3005 - accuracy: 0.8776 - val_loss: 0.2920 - val_accuracy: 0.8684 Epoch 272/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2830 - accuracy: 0.8696 - val_loss: 0.2983 - val_accuracy: 0.8684 Epoch 273/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2739 - accuracy: 0.8800 - val_loss: 0.2991 - val_accuracy: 0.8580 Epoch 274/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2663 - accuracy: 0.8816 - val_loss: 0.2911 - val_accuracy: 0.8652 Epoch 275/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2614 - accuracy: 0.8856 - val_loss: 0.2876 - val_accuracy: 0.8660 Epoch 276/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2728 - accuracy: 0.8820 - val_loss: 0.3010 - val_accuracy: 0.8620 Epoch 277/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2796 - accuracy: 0.8736 - val_loss: 0.3075 - val_accuracy: 0.8648 Epoch 278/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2701 - accuracy: 0.8836 - val_loss: 0.2892 - val_accuracy: 0.8632 Epoch 279/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2818 - accuracy: 0.8812 - val_loss: 0.2953 - val_accuracy: 0.8656 Epoch 280/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3780 - accuracy: 0.8300 - val_loss: 0.3273 - val_accuracy: 0.8580 Epoch 281/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2866 - accuracy: 0.8720 - val_loss: 0.2984 - val_accuracy: 0.8660 Epoch 282/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2721 - accuracy: 0.8780 - val_loss: 0.2901 - val_accuracy: 0.8684 Epoch 283/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3003 - accuracy: 0.8636 - val_loss: 0.3686 - val_accuracy: 0.8172 Epoch 284/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3474 - accuracy: 0.8428 - val_loss: 0.3232 - val_accuracy: 0.8496 Epoch 285/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3059 - accuracy: 0.8640 - val_loss: 0.3046 - val_accuracy: 0.8572 Epoch 286/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2884 - accuracy: 0.8696 - val_loss: 0.2977 - val_accuracy: 0.8656 Epoch 287/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2780 - accuracy: 0.8772 - val_loss: 0.3672 - val_accuracy: 0.8384 Epoch 288/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2969 - accuracy: 0.8676 - val_loss: 0.2954 - val_accuracy: 0.8624 Epoch 289/400 79/79 [==============================] - 1s 6ms/step - loss: 0.2803 - accuracy: 0.8736 - val_loss: 0.2911 - val_accuracy: 0.8624 Epoch 290/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2810 - accuracy: 0.8760 - val_loss: 0.2936 - val_accuracy: 0.8616 Epoch 291/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2767 - accuracy: 0.8800 - val_loss: 0.2882 - val_accuracy: 0.8660 Epoch 292/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2697 - accuracy: 0.8780 - val_loss: 0.2849 - val_accuracy: 0.8668 Epoch 293/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2753 - accuracy: 0.8748 - val_loss: 0.2933 - val_accuracy: 0.8648 Epoch 294/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2767 - accuracy: 0.8748 - val_loss: 0.2799 - val_accuracy: 0.8668 Epoch 295/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2623 - accuracy: 0.8824 - val_loss: 0.2881 - val_accuracy: 0.8616 Epoch 296/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2602 - accuracy: 0.8816 - val_loss: 0.2886 - val_accuracy: 0.8652 Epoch 297/400 79/79 [==============================] - 1s 6ms/step - loss: 0.2612 - accuracy: 0.8788 - val_loss: 0.2838 - val_accuracy: 0.8652 Epoch 298/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2593 - accuracy: 0.8868 - val_loss: 0.2771 - val_accuracy: 0.8688 Epoch 299/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2582 - accuracy: 0.8800 - val_loss: 0.2817 - val_accuracy: 0.8668 Epoch 300/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2604 - accuracy: 0.8856 - val_loss: 0.2763 - val_accuracy: 0.8716 Epoch 301/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2640 - accuracy: 0.8848 - val_loss: 0.3153 - val_accuracy: 0.8596 Epoch 302/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2773 - accuracy: 0.8804 - val_loss: 0.2828 - val_accuracy: 0.8680 Epoch 303/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2748 - accuracy: 0.8816 - val_loss: 0.2749 - val_accuracy: 0.8716 Epoch 304/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2664 - accuracy: 0.8816 - val_loss: 0.2797 - val_accuracy: 0.8740 Epoch 305/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2570 - accuracy: 0.8840 - val_loss: 0.2722 - val_accuracy: 0.8760 Epoch 306/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2559 - accuracy: 0.8900 - val_loss: 0.2786 - val_accuracy: 0.8700 Epoch 307/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2547 - accuracy: 0.8896 - val_loss: 0.2692 - val_accuracy: 0.8756 Epoch 308/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2573 - accuracy: 0.8868 - val_loss: 0.2748 - val_accuracy: 0.8720 Epoch 309/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2520 - accuracy: 0.8900 - val_loss: 0.2758 - val_accuracy: 0.8728 Epoch 310/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2533 - accuracy: 0.8864 - val_loss: 0.2676 - val_accuracy: 0.8748 Epoch 311/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2516 - accuracy: 0.8948 - val_loss: 0.2699 - val_accuracy: 0.8784 Epoch 312/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2489 - accuracy: 0.8948 - val_loss: 0.2641 - val_accuracy: 0.8796 Epoch 313/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2505 - accuracy: 0.8972 - val_loss: 0.2711 - val_accuracy: 0.8760 Epoch 314/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2379 - accuracy: 0.9004 - val_loss: 0.2585 - val_accuracy: 0.8832 Epoch 315/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2383 - accuracy: 0.9008 - val_loss: 0.2604 - val_accuracy: 0.8856 Epoch 316/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2286 - accuracy: 0.9076 - val_loss: 0.2660 - val_accuracy: 0.8840 Epoch 317/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2282 - accuracy: 0.9100 - val_loss: 0.2567 - val_accuracy: 0.8888 Epoch 318/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2291 - accuracy: 0.9096 - val_loss: 0.2530 - val_accuracy: 0.8948 Epoch 319/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2284 - accuracy: 0.9108 - val_loss: 0.2332 - val_accuracy: 0.9012 Epoch 320/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2151 - accuracy: 0.9176 - val_loss: 0.2388 - val_accuracy: 0.8984 Epoch 321/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2130 - accuracy: 0.9188 - val_loss: 0.2076 - val_accuracy: 0.9264 Epoch 322/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1967 - accuracy: 0.9292 - val_loss: 0.2096 - val_accuracy: 0.9312 Epoch 323/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1843 - accuracy: 0.9388 - val_loss: 0.1982 - val_accuracy: 0.9344 Epoch 324/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1722 - accuracy: 0.9468 - val_loss: 0.1952 - val_accuracy: 0.9396 Epoch 325/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1616 - accuracy: 0.9528 - val_loss: 0.1722 - val_accuracy: 0.9496 Epoch 326/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1605 - accuracy: 0.9532 - val_loss: 0.1745 - val_accuracy: 0.9448 Epoch 327/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1689 - accuracy: 0.9468 - val_loss: 0.1750 - val_accuracy: 0.9468 Epoch 328/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1570 - accuracy: 0.9536 - val_loss: 0.1766 - val_accuracy: 0.9492 Epoch 329/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1527 - accuracy: 0.9616 - val_loss: 0.1581 - val_accuracy: 0.9576 Epoch 330/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1371 - accuracy: 0.9660 - val_loss: 0.1693 - val_accuracy: 0.9536 Epoch 331/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9648 - val_loss: 0.1552 - val_accuracy: 0.9568 Epoch 332/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1453 - accuracy: 0.9612 - val_loss: 0.1445 - val_accuracy: 0.9608 Epoch 333/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1447 - accuracy: 0.9636 - val_loss: 0.1648 - val_accuracy: 0.9560 Epoch 334/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1431 - accuracy: 0.9652 - val_loss: 0.1450 - val_accuracy: 0.9632 Epoch 335/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2000 - accuracy: 0.9468 - val_loss: 0.1380 - val_accuracy: 0.9644 Epoch 336/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9676 - val_loss: 0.1403 - val_accuracy: 0.9620 Epoch 337/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1197 - accuracy: 0.9700 - val_loss: 0.1930 - val_accuracy: 0.9472 Epoch 338/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1395 - accuracy: 0.9640 - val_loss: 0.1366 - val_accuracy: 0.9636 Epoch 339/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1173 - accuracy: 0.9704 - val_loss: 0.1253 - val_accuracy: 0.9668 Epoch 340/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1209 - accuracy: 0.9724 - val_loss: 0.1151 - val_accuracy: 0.9704 Epoch 341/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1031 - accuracy: 0.9772 - val_loss: 0.1122 - val_accuracy: 0.9724 Epoch 342/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1015 - accuracy: 0.9776 - val_loss: 0.1129 - val_accuracy: 0.9720 Epoch 343/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1069 - accuracy: 0.9752 - val_loss: 0.1121 - val_accuracy: 0.9728 Epoch 344/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1031 - accuracy: 0.9752 - val_loss: 0.0995 - val_accuracy: 0.9756 Epoch 345/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1038 - accuracy: 0.9736 - val_loss: 0.0965 - val_accuracy: 0.9756 Epoch 346/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0845 - accuracy: 0.9800 - val_loss: 0.0894 - val_accuracy: 0.9776 Epoch 347/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0977 - accuracy: 0.9764 - val_loss: 0.1004 - val_accuracy: 0.9752 Epoch 348/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0964 - accuracy: 0.9748 - val_loss: 0.1017 - val_accuracy: 0.9728 Epoch 349/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9820 - val_loss: 0.0827 - val_accuracy: 0.9796 Epoch 350/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0864 - accuracy: 0.9792 - val_loss: 0.0825 - val_accuracy: 0.9832 Epoch 351/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.9832 - val_loss: 0.1226 - val_accuracy: 0.9684 Epoch 352/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1088 - accuracy: 0.9716 - val_loss: 0.1066 - val_accuracy: 0.9740 Epoch 353/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1056 - accuracy: 0.9728 - val_loss: 0.0994 - val_accuracy: 0.9776 Epoch 354/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0875 - accuracy: 0.9788 - val_loss: 0.0836 - val_accuracy: 0.9816 Epoch 355/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0811 - accuracy: 0.9824 - val_loss: 0.0956 - val_accuracy: 0.9756 Epoch 356/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0750 - accuracy: 0.9836 - val_loss: 0.0742 - val_accuracy: 0.9828 Epoch 357/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1014 - accuracy: 0.9764 - val_loss: 0.1630 - val_accuracy: 0.9560 Epoch 358/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1405 - accuracy: 0.9652 - val_loss: 0.1010 - val_accuracy: 0.9752 Epoch 359/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1292 - accuracy: 0.9672 - val_loss: 0.1304 - val_accuracy: 0.9672 Epoch 360/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1048 - accuracy: 0.9740 - val_loss: 0.0815 - val_accuracy: 0.9812 Epoch 361/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0782 - accuracy: 0.9816 - val_loss: 0.0712 - val_accuracy: 0.9852 Epoch 362/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0658 - accuracy: 0.9848 - val_loss: 0.0721 - val_accuracy: 0.9812 Epoch 363/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9848 - val_loss: 0.0672 - val_accuracy: 0.9836 Epoch 364/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0647 - accuracy: 0.9844 - val_loss: 0.0713 - val_accuracy: 0.9828 Epoch 365/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0654 - accuracy: 0.9844 - val_loss: 0.0715 - val_accuracy: 0.9820 Epoch 366/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0677 - accuracy: 0.9820 - val_loss: 0.0689 - val_accuracy: 0.9828 Epoch 367/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 0.9860 - val_loss: 0.0653 - val_accuracy: 0.9836 Epoch 368/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0557 - accuracy: 0.9892 - val_loss: 0.0648 - val_accuracy: 0.9820 Epoch 369/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0545 - accuracy: 0.9872 - val_loss: 0.0668 - val_accuracy: 0.9828 Epoch 370/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0600 - accuracy: 0.9864 - val_loss: 0.0649 - val_accuracy: 0.9848 Epoch 371/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0551 - accuracy: 0.9864 - val_loss: 0.0674 - val_accuracy: 0.9832 Epoch 372/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0639 - accuracy: 0.9856 - val_loss: 0.0781 - val_accuracy: 0.9808 Epoch 373/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0665 - accuracy: 0.9812 - val_loss: 0.0711 - val_accuracy: 0.9824 Epoch 374/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9844 - val_loss: 0.0605 - val_accuracy: 0.9864 Epoch 375/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0620 - accuracy: 0.9864 - val_loss: 0.0632 - val_accuracy: 0.9856 Epoch 376/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0673 - accuracy: 0.9828 - val_loss: 0.0635 - val_accuracy: 0.9848 Epoch 377/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9824 - val_loss: 0.0627 - val_accuracy: 0.9860 Epoch 378/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0504 - accuracy: 0.9872 - val_loss: 0.0597 - val_accuracy: 0.9860 Epoch 379/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0549 - accuracy: 0.9868 - val_loss: 0.0649 - val_accuracy: 0.9856 Epoch 380/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0523 - accuracy: 0.9860 - val_loss: 0.1208 - val_accuracy: 0.9768 Epoch 381/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1911 - accuracy: 0.9576 - val_loss: 0.1913 - val_accuracy: 0.9484 Epoch 382/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9720 - val_loss: 0.0887 - val_accuracy: 0.9820 Epoch 383/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.9836 - val_loss: 0.0687 - val_accuracy: 0.9840 Epoch 384/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0636 - accuracy: 0.9844 - val_loss: 0.0674 - val_accuracy: 0.9856 Epoch 385/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0595 - accuracy: 0.9828 - val_loss: 0.0779 - val_accuracy: 0.9832 Epoch 386/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0703 - accuracy: 0.9844 - val_loss: 0.0651 - val_accuracy: 0.9852 Epoch 387/400 79/79 [==============================] - 1s 8ms/step - loss: 0.0707 - accuracy: 0.9832 - val_loss: 0.0796 - val_accuracy: 0.9828 Epoch 388/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2051 - accuracy: 0.9564 - val_loss: 0.4431 - val_accuracy: 0.8876 Epoch 389/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2905 - accuracy: 0.9260 - val_loss: 0.1163 - val_accuracy: 0.9708 Epoch 390/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1224 - accuracy: 0.9688 - val_loss: 0.0957 - val_accuracy: 0.9780 Epoch 391/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1141 - accuracy: 0.9712 - val_loss: 0.1079 - val_accuracy: 0.9764 Epoch 392/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1019 - accuracy: 0.9752 - val_loss: 0.0845 - val_accuracy: 0.9808 Epoch 393/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0949 - accuracy: 0.9772 - val_loss: 0.0862 - val_accuracy: 0.9820 Epoch 394/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0836 - accuracy: 0.9784 - val_loss: 0.0897 - val_accuracy: 0.9816 Epoch 395/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0785 - accuracy: 0.9796 - val_loss: 0.0960 - val_accuracy: 0.9808 Epoch 396/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0785 - accuracy: 0.9804 - val_loss: 0.0907 - val_accuracy: 0.9800 Epoch 397/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9876 - val_loss: 0.0772 - val_accuracy: 0.9844 Epoch 398/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0550 - accuracy: 0.9876 - val_loss: 0.0745 - val_accuracy: 0.9844 Epoch 399/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9872 - val_loss: 0.0661 - val_accuracy: 0.9848 Epoch 400/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.9880 - val_loss: 0.0654 - val_accuracy: 0.9864 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1f98f400> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec262e1d30> # Make the problem harder by making T larger T = 30 D = 1 X = [] Y = [] for t in range ( 5000 ): x = np . random . randn ( T ) X . append ( x ) y = get_label ( x , 0 , 1 , 2 ) # long distance Y . append ( y ) X = np . array ( X ) Y = np . array ( Y ) N = len ( X ) # Now test our LSTM inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = LSTM ( 15 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 400 , validation_split = 0.5 , ) Epoch 1/400 79/79 [==============================] - 1s 10ms/step - loss: 0.6938 - accuracy: 0.5124 - val_loss: 0.6950 - val_accuracy: 0.4940 Epoch 2/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6925 - accuracy: 0.5200 - val_loss: 0.6945 - val_accuracy: 0.4964 Epoch 3/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6927 - accuracy: 0.5204 - val_loss: 0.6955 - val_accuracy: 0.4960 Epoch 4/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6916 - accuracy: 0.5224 - val_loss: 0.6943 - val_accuracy: 0.4956 Epoch 5/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5228 - val_loss: 0.6952 - val_accuracy: 0.4960 Epoch 6/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6920 - accuracy: 0.5232 - val_loss: 0.6945 - val_accuracy: 0.4960 Epoch 7/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5252 - val_loss: 0.6949 - val_accuracy: 0.4912 Epoch 8/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6917 - accuracy: 0.5276 - val_loss: 0.6943 - val_accuracy: 0.4908 Epoch 9/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6908 - accuracy: 0.5244 - val_loss: 0.6962 - val_accuracy: 0.4892 Epoch 10/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6929 - accuracy: 0.5300 - val_loss: 0.6971 - val_accuracy: 0.4964 Epoch 11/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6913 - accuracy: 0.5284 - val_loss: 0.6950 - val_accuracy: 0.4960 Epoch 12/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6909 - accuracy: 0.5248 - val_loss: 0.6954 - val_accuracy: 0.4940 Epoch 13/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6911 - accuracy: 0.5284 - val_loss: 0.6941 - val_accuracy: 0.4964 Epoch 14/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6912 - accuracy: 0.5344 - val_loss: 0.6959 - val_accuracy: 0.4968 Epoch 15/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6904 - accuracy: 0.5332 - val_loss: 0.6952 - val_accuracy: 0.5016 Epoch 16/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6912 - accuracy: 0.5372 - val_loss: 0.6958 - val_accuracy: 0.5020 Epoch 17/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6907 - accuracy: 0.5324 - val_loss: 0.6962 - val_accuracy: 0.5052 Epoch 18/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6924 - accuracy: 0.5300 - val_loss: 0.6953 - val_accuracy: 0.5056 Epoch 19/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6893 - accuracy: 0.5352 - val_loss: 0.6994 - val_accuracy: 0.4976 Epoch 20/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6899 - accuracy: 0.5268 - val_loss: 0.6960 - val_accuracy: 0.4988 Epoch 21/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6898 - accuracy: 0.5432 - val_loss: 0.6958 - val_accuracy: 0.5104 Epoch 22/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6879 - accuracy: 0.5376 - val_loss: 0.6995 - val_accuracy: 0.5052 Epoch 23/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6891 - accuracy: 0.5356 - val_loss: 0.7014 - val_accuracy: 0.5012 Epoch 24/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6875 - accuracy: 0.5416 - val_loss: 0.7036 - val_accuracy: 0.5016 Epoch 25/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6885 - accuracy: 0.5388 - val_loss: 0.6955 - val_accuracy: 0.4996 Epoch 26/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6861 - accuracy: 0.5396 - val_loss: 0.7011 - val_accuracy: 0.5016 Epoch 27/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6859 - accuracy: 0.5472 - val_loss: 0.7008 - val_accuracy: 0.4956 Epoch 28/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6856 - accuracy: 0.5528 - val_loss: 0.7030 - val_accuracy: 0.5012 Epoch 29/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6843 - accuracy: 0.5496 - val_loss: 0.7024 - val_accuracy: 0.4944 Epoch 30/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6826 - accuracy: 0.5508 - val_loss: 0.7150 - val_accuracy: 0.4960 Epoch 31/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6811 - accuracy: 0.5580 - val_loss: 0.7058 - val_accuracy: 0.5004 Epoch 32/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6882 - accuracy: 0.5332 - val_loss: 0.7025 - val_accuracy: 0.4940 Epoch 33/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6927 - accuracy: 0.5188 - val_loss: 0.6988 - val_accuracy: 0.5000 Epoch 34/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6897 - accuracy: 0.5416 - val_loss: 0.7027 - val_accuracy: 0.5052 Epoch 35/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6892 - accuracy: 0.5388 - val_loss: 0.6926 - val_accuracy: 0.5076 Epoch 36/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6703 - accuracy: 0.5680 - val_loss: 0.6594 - val_accuracy: 0.5656 Epoch 37/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6364 - accuracy: 0.6020 - val_loss: 0.6418 - val_accuracy: 0.5920 Epoch 38/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6196 - accuracy: 0.6136 - val_loss: 0.6510 - val_accuracy: 0.5844 Epoch 39/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6257 - accuracy: 0.6184 - val_loss: 0.6371 - val_accuracy: 0.5816 Epoch 40/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6065 - accuracy: 0.6308 - val_loss: 0.6372 - val_accuracy: 0.5924 Epoch 41/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6144 - accuracy: 0.6324 - val_loss: 0.6255 - val_accuracy: 0.5948 Epoch 42/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6017 - accuracy: 0.6336 - val_loss: 0.6380 - val_accuracy: 0.5960 Epoch 43/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6007 - accuracy: 0.6372 - val_loss: 0.6215 - val_accuracy: 0.6032 Epoch 44/400 79/79 [==============================] - 1s 6ms/step - loss: 0.5848 - accuracy: 0.6604 - val_loss: 0.5784 - val_accuracy: 0.6684 Epoch 45/400 79/79 [==============================] - 1s 6ms/step - loss: 0.5135 - accuracy: 0.7432 - val_loss: 0.4908 - val_accuracy: 0.7532 Epoch 46/400 79/79 [==============================] - 1s 7ms/step - loss: 0.4700 - accuracy: 0.7776 - val_loss: 0.5146 - val_accuracy: 0.7540 Epoch 47/400 79/79 [==============================] - 1s 7ms/step - loss: 0.4460 - accuracy: 0.7956 - val_loss: 0.4578 - val_accuracy: 0.7732 Epoch 48/400 79/79 [==============================] - 1s 6ms/step - loss: 0.4209 - accuracy: 0.8076 - val_loss: 0.4344 - val_accuracy: 0.7916 Epoch 49/400 79/79 [==============================] - 1s 6ms/step - loss: 0.3859 - accuracy: 0.8280 - val_loss: 0.3907 - val_accuracy: 0.8176 Epoch 50/400 79/79 [==============================] - 1s 7ms/step - loss: 0.2989 - accuracy: 0.8860 - val_loss: 0.2240 - val_accuracy: 0.9068 Epoch 51/400 79/79 [==============================] - 1s 6ms/step - loss: 0.1341 - accuracy: 0.9544 - val_loss: 0.1162 - val_accuracy: 0.9628 Epoch 52/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0979 - accuracy: 0.9668 - val_loss: 0.0890 - val_accuracy: 0.9664 Epoch 53/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0632 - accuracy: 0.9812 - val_loss: 0.0899 - val_accuracy: 0.9660 Epoch 54/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0643 - accuracy: 0.9800 - val_loss: 0.1283 - val_accuracy: 0.9576 Epoch 55/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0563 - accuracy: 0.9800 - val_loss: 0.0561 - val_accuracy: 0.9788 Epoch 56/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0444 - accuracy: 0.9836 - val_loss: 0.0495 - val_accuracy: 0.9824 Epoch 57/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0373 - accuracy: 0.9892 - val_loss: 0.0891 - val_accuracy: 0.9708 Epoch 58/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0411 - accuracy: 0.9844 - val_loss: 0.0512 - val_accuracy: 0.9828 Epoch 59/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0312 - accuracy: 0.9896 - val_loss: 0.0501 - val_accuracy: 0.9808 Epoch 60/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0293 - accuracy: 0.9916 - val_loss: 0.0605 - val_accuracy: 0.9748 Epoch 61/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0300 - accuracy: 0.9868 - val_loss: 0.0444 - val_accuracy: 0.9836 Epoch 62/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0214 - accuracy: 0.9924 - val_loss: 0.0561 - val_accuracy: 0.9856 Epoch 63/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.0404 - val_accuracy: 0.9868 Epoch 64/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0212 - accuracy: 0.9920 - val_loss: 0.0402 - val_accuracy: 0.9852 Epoch 65/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.0302 - val_accuracy: 0.9876 Epoch 66/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0317 - accuracy: 0.9876 - val_loss: 0.0469 - val_accuracy: 0.9836 Epoch 67/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0213 - accuracy: 0.9900 - val_loss: 0.0292 - val_accuracy: 0.9900 Epoch 68/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.0289 - val_accuracy: 0.9880 Epoch 69/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0165 - accuracy: 0.9940 - val_loss: 0.0329 - val_accuracy: 0.9868 Epoch 70/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0244 - accuracy: 0.9932 - val_loss: 0.0339 - val_accuracy: 0.9852 Epoch 71/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0256 - accuracy: 0.9908 - val_loss: 0.0401 - val_accuracy: 0.9872 Epoch 72/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0263 - accuracy: 0.9912 - val_loss: 0.0332 - val_accuracy: 0.9880 Epoch 73/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0553 - val_accuracy: 0.9832 Epoch 74/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.0300 - val_accuracy: 0.9888 Epoch 75/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.0419 - val_accuracy: 0.9876 Epoch 76/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0178 - accuracy: 0.9952 - val_loss: 0.0442 - val_accuracy: 0.9856 Epoch 77/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.0257 - val_accuracy: 0.9908 Epoch 78/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0352 - val_accuracy: 0.9892 Epoch 79/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0140 - accuracy: 0.9940 - val_loss: 0.0460 - val_accuracy: 0.9856 Epoch 80/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0125 - accuracy: 0.9964 - val_loss: 0.0349 - val_accuracy: 0.9884 Epoch 81/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0070 - accuracy: 0.9972 - val_loss: 0.0329 - val_accuracy: 0.9908 Epoch 82/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0069 - accuracy: 0.9972 - val_loss: 0.0338 - val_accuracy: 0.9904 Epoch 83/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0289 - accuracy: 0.9964 - val_loss: 0.0410 - val_accuracy: 0.9868 Epoch 84/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0840 - accuracy: 0.9744 - val_loss: 0.0600 - val_accuracy: 0.9792 Epoch 85/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0278 - accuracy: 0.9892 - val_loss: 0.0368 - val_accuracy: 0.9872 Epoch 86/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0358 - val_accuracy: 0.9868 Epoch 87/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0401 - val_accuracy: 0.9868 Epoch 88/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0444 - val_accuracy: 0.9856 Epoch 89/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0383 - val_accuracy: 0.9880 Epoch 90/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0131 - accuracy: 0.9952 - val_loss: 0.0456 - val_accuracy: 0.9852 Epoch 91/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0293 - val_accuracy: 0.9884 Epoch 92/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0057 - accuracy: 0.9976 - val_loss: 0.0328 - val_accuracy: 0.9864 Epoch 93/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0243 - val_accuracy: 0.9924 Epoch 94/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0367 - val_accuracy: 0.9872 Epoch 95/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0063 - accuracy: 0.9984 - val_loss: 0.0438 - val_accuracy: 0.9884 Epoch 96/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.0483 - val_accuracy: 0.9864 Epoch 97/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9988 - val_loss: 0.0479 - val_accuracy: 0.9884 Epoch 98/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.0357 - val_accuracy: 0.9884 Epoch 99/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0145 - accuracy: 0.9960 - val_loss: 0.0373 - val_accuracy: 0.9884 Epoch 100/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0154 - accuracy: 0.9948 - val_loss: 0.0376 - val_accuracy: 0.9852 Epoch 101/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.0364 - val_accuracy: 0.9884 Epoch 102/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.0261 - val_accuracy: 0.9920 Epoch 103/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.0284 - val_accuracy: 0.9900 Epoch 104/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 0.0232 - val_accuracy: 0.9908 Epoch 105/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0271 - val_accuracy: 0.9896 Epoch 106/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0290 - val_accuracy: 0.9892 Epoch 107/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 0.0292 - val_accuracy: 0.9896 Epoch 108/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0302 - val_accuracy: 0.9912 Epoch 109/400 79/79 [==============================] - 1s 7ms/step - loss: 9.9011e-04 - accuracy: 0.9996 - val_loss: 0.0255 - val_accuracy: 0.9920 Epoch 110/400 79/79 [==============================] - 1s 7ms/step - loss: 7.9221e-04 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 0.9896 Epoch 111/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0195 - accuracy: 0.9964 - val_loss: 0.0237 - val_accuracy: 0.9916 Epoch 112/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0388 - val_accuracy: 0.9864 Epoch 113/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.0319 - val_accuracy: 0.9892 Epoch 114/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0298 - val_accuracy: 0.9904 Epoch 115/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0315 - val_accuracy: 0.9892 Epoch 116/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0335 - val_accuracy: 0.9900 Epoch 117/400 79/79 [==============================] - 1s 7ms/step - loss: 9.3726e-04 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 0.9908 Epoch 118/400 79/79 [==============================] - 1s 7ms/step - loss: 6.0662e-04 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 0.9908 Epoch 119/400 79/79 [==============================] - 1s 7ms/step - loss: 5.9509e-04 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 0.9904 Epoch 120/400 79/79 [==============================] - 1s 7ms/step - loss: 5.5597e-04 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 0.9912 Epoch 121/400 79/79 [==============================] - 1s 7ms/step - loss: 5.2154e-04 - accuracy: 1.0000 - val_loss: 0.0347 - val_accuracy: 0.9908 Epoch 122/400 79/79 [==============================] - 1s 6ms/step - loss: 3.3155e-04 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9908 Epoch 123/400 79/79 [==============================] - 1s 6ms/step - loss: 2.8362e-04 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9912 Epoch 124/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2577e-04 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 0.9908 Epoch 125/400 79/79 [==============================] - 1s 7ms/step - loss: 1.9810e-04 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 0.9912 Epoch 126/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7869e-04 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 0.9924 Epoch 127/400 79/79 [==============================] - 1s 6ms/step - loss: 1.5033e-04 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 0.9924 Epoch 128/400 79/79 [==============================] - 1s 7ms/step - loss: 1.1684e-04 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9924 Epoch 129/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0989e-04 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9924 Epoch 130/400 79/79 [==============================] - 1s 6ms/step - loss: 9.0361e-05 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9924 Epoch 131/400 79/79 [==============================] - 1s 7ms/step - loss: 7.7742e-05 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.9924 Epoch 132/400 79/79 [==============================] - 1s 6ms/step - loss: 7.2091e-05 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9924 Epoch 133/400 79/79 [==============================] - 1s 7ms/step - loss: 6.4350e-05 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 0.9924 Epoch 134/400 79/79 [==============================] - 1s 7ms/step - loss: 5.8189e-05 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 0.9924 Epoch 135/400 79/79 [==============================] - 1s 6ms/step - loss: 4.9459e-05 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9924 Epoch 136/400 79/79 [==============================] - 1s 6ms/step - loss: 4.5237e-05 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 0.9928 Epoch 137/400 79/79 [==============================] - 1s 7ms/step - loss: 4.1174e-05 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9928 Epoch 138/400 79/79 [==============================] - 1s 7ms/step - loss: 3.6626e-05 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.9924 Epoch 139/400 79/79 [==============================] - 1s 7ms/step - loss: 3.3935e-05 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9928 Epoch 140/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0910e-05 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 0.9928 Epoch 141/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8376e-05 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9928 Epoch 142/400 79/79 [==============================] - 1s 6ms/step - loss: 2.6438e-05 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9928 Epoch 143/400 79/79 [==============================] - 1s 6ms/step - loss: 2.4154e-05 - accuracy: 1.0000 - val_loss: 0.0538 - val_accuracy: 0.9928 Epoch 144/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2308e-05 - accuracy: 1.0000 - val_loss: 0.0543 - val_accuracy: 0.9924 Epoch 145/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1110e-05 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 0.9928 Epoch 146/400 79/79 [==============================] - 1s 6ms/step - loss: 1.9753e-05 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 0.9924 Epoch 147/400 79/79 [==============================] - 1s 6ms/step - loss: 1.9095e-05 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9920 Epoch 148/400 79/79 [==============================] - 1s 6ms/step - loss: 1.7381e-05 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9924 Epoch 149/400 79/79 [==============================] - 1s 7ms/step - loss: 1.6414e-05 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9924 Epoch 150/400 79/79 [==============================] - 1s 6ms/step - loss: 1.5528e-05 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9924 Epoch 151/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4698e-05 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9924 Epoch 152/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4516e-05 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9924 Epoch 153/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3769e-05 - accuracy: 1.0000 - val_loss: 0.0585 - val_accuracy: 0.9920 Epoch 154/400 79/79 [==============================] - 0s 6ms/step - loss: 1.2615e-05 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 0.9920 Epoch 155/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2033e-05 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9920 Epoch 156/400 79/79 [==============================] - 1s 6ms/step - loss: 1.1480e-05 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9920 Epoch 157/400 79/79 [==============================] - 1s 6ms/step - loss: 1.1004e-05 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9920 Epoch 158/400 79/79 [==============================] - 1s 6ms/step - loss: 1.0427e-05 - accuracy: 1.0000 - val_loss: 0.0609 - val_accuracy: 0.9912 Epoch 159/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0029e-05 - accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 0.9912 Epoch 160/400 79/79 [==============================] - 1s 7ms/step - loss: 9.6064e-06 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9912 Epoch 161/400 79/79 [==============================] - 1s 7ms/step - loss: 9.2735e-06 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 0.9912 Epoch 162/400 79/79 [==============================] - 0s 6ms/step - loss: 8.9280e-06 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9908 Epoch 163/400 79/79 [==============================] - 1s 7ms/step - loss: 8.5592e-06 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9908 Epoch 164/400 79/79 [==============================] - 1s 7ms/step - loss: 8.1564e-06 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9908 Epoch 165/400 79/79 [==============================] - 1s 7ms/step - loss: 7.7459e-06 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9908 Epoch 166/400 79/79 [==============================] - 1s 7ms/step - loss: 7.4394e-06 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9908 Epoch 167/400 79/79 [==============================] - 1s 6ms/step - loss: 7.3749e-06 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9908 Epoch 168/400 79/79 [==============================] - 1s 7ms/step - loss: 6.8616e-06 - accuracy: 1.0000 - val_loss: 0.0657 - val_accuracy: 0.9908 Epoch 169/400 79/79 [==============================] - 1s 7ms/step - loss: 6.6022e-06 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9908 Epoch 170/400 79/79 [==============================] - 1s 7ms/step - loss: 6.3705e-06 - accuracy: 1.0000 - val_loss: 0.0667 - val_accuracy: 0.9908 Epoch 171/400 79/79 [==============================] - 1s 7ms/step - loss: 6.0982e-06 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 0.9908 Epoch 172/400 79/79 [==============================] - 1s 7ms/step - loss: 5.8814e-06 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 0.9908 Epoch 173/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6631e-06 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9908 Epoch 174/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6681e-06 - accuracy: 1.0000 - val_loss: 0.0688 - val_accuracy: 0.9908 Epoch 175/400 79/79 [==============================] - 1s 7ms/step - loss: 5.4060e-06 - accuracy: 1.0000 - val_loss: 0.0691 - val_accuracy: 0.9904 Epoch 176/400 79/79 [==============================] - 1s 7ms/step - loss: 5.0593e-06 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 0.9904 Epoch 177/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9132e-06 - accuracy: 1.0000 - val_loss: 0.0703 - val_accuracy: 0.9904 Epoch 178/400 79/79 [==============================] - 1s 7ms/step - loss: 4.8269e-06 - accuracy: 1.0000 - val_loss: 0.0707 - val_accuracy: 0.9904 Epoch 179/400 79/79 [==============================] - 1s 7ms/step - loss: 4.5163e-06 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9908 Epoch 180/400 79/79 [==============================] - 1s 7ms/step - loss: 4.3722e-06 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9908 Epoch 181/400 79/79 [==============================] - 1s 7ms/step - loss: 4.1614e-06 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9904 Epoch 182/400 79/79 [==============================] - 1s 7ms/step - loss: 4.0555e-06 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9904 Epoch 183/400 79/79 [==============================] - 1s 7ms/step - loss: 3.8702e-06 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9904 Epoch 184/400 79/79 [==============================] - 1s 6ms/step - loss: 3.7382e-06 - accuracy: 1.0000 - val_loss: 0.0741 - val_accuracy: 0.9904 Epoch 185/400 79/79 [==============================] - 1s 7ms/step - loss: 3.6140e-06 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 0.9904 Epoch 186/400 79/79 [==============================] - 1s 6ms/step - loss: 3.4893e-06 - accuracy: 1.0000 - val_loss: 0.0750 - val_accuracy: 0.9904 Epoch 187/400 79/79 [==============================] - 1s 7ms/step - loss: 3.4553e-06 - accuracy: 1.0000 - val_loss: 0.0757 - val_accuracy: 0.9900 Epoch 188/400 79/79 [==============================] - 1s 6ms/step - loss: 3.2320e-06 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9900 Epoch 189/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0963e-06 - accuracy: 1.0000 - val_loss: 0.0772 - val_accuracy: 0.9900 Epoch 190/400 79/79 [==============================] - 1s 6ms/step - loss: 2.9725e-06 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9900 Epoch 191/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8801e-06 - accuracy: 1.0000 - val_loss: 0.0783 - val_accuracy: 0.9900 Epoch 192/400 79/79 [==============================] - 0s 6ms/step - loss: 2.7571e-06 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9900 Epoch 193/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6647e-06 - accuracy: 1.0000 - val_loss: 0.0795 - val_accuracy: 0.9896 Epoch 194/400 79/79 [==============================] - 1s 6ms/step - loss: 2.5738e-06 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9896 Epoch 195/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4619e-06 - accuracy: 1.0000 - val_loss: 0.0809 - val_accuracy: 0.9896 Epoch 196/400 79/79 [==============================] - 0s 6ms/step - loss: 2.4338e-06 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9896 Epoch 197/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2797e-06 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9896 Epoch 198/400 79/79 [==============================] - 1s 6ms/step - loss: 2.2387e-06 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9896 Epoch 199/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1369e-06 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9896 Epoch 200/400 79/79 [==============================] - 1s 6ms/step - loss: 2.0409e-06 - accuracy: 1.0000 - val_loss: 0.0842 - val_accuracy: 0.9896 Epoch 201/400 79/79 [==============================] - 1s 7ms/step - loss: 2.0086e-06 - accuracy: 1.0000 - val_loss: 0.0848 - val_accuracy: 0.9896 Epoch 202/400 79/79 [==============================] - 1s 6ms/step - loss: 1.9156e-06 - accuracy: 1.0000 - val_loss: 0.0853 - val_accuracy: 0.9896 Epoch 203/400 79/79 [==============================] - 1s 7ms/step - loss: 1.9042e-06 - accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9896 Epoch 204/400 79/79 [==============================] - 1s 6ms/step - loss: 1.7815e-06 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9896 Epoch 205/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7301e-06 - accuracy: 1.0000 - val_loss: 0.0860 - val_accuracy: 0.9896 Epoch 206/400 79/79 [==============================] - 1s 7ms/step - loss: 1.6366e-06 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9896 Epoch 207/400 79/79 [==============================] - 1s 6ms/step - loss: 1.6629e-06 - accuracy: 1.0000 - val_loss: 0.0875 - val_accuracy: 0.9896 Epoch 208/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4986e-06 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 0.9896 Epoch 209/400 79/79 [==============================] - 0s 6ms/step - loss: 1.4416e-06 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9896 Epoch 210/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4065e-06 - accuracy: 1.0000 - val_loss: 0.0896 - val_accuracy: 0.9896 Epoch 211/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3245e-06 - accuracy: 1.0000 - val_loss: 0.0903 - val_accuracy: 0.9896 Epoch 212/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2749e-06 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9896 Epoch 213/400 79/79 [==============================] - 0s 6ms/step - loss: 1.2286e-06 - accuracy: 1.0000 - val_loss: 0.0915 - val_accuracy: 0.9896 Epoch 214/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2243e-06 - accuracy: 1.0000 - val_loss: 0.0920 - val_accuracy: 0.9896 Epoch 215/400 79/79 [==============================] - 0s 6ms/step - loss: 1.1306e-06 - accuracy: 1.0000 - val_loss: 0.0929 - val_accuracy: 0.9896 Epoch 216/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0883e-06 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9896 Epoch 217/400 79/79 [==============================] - 1s 6ms/step - loss: 1.0679e-06 - accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9896 Epoch 218/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0146e-06 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9896 Epoch 219/400 79/79 [==============================] - 1s 6ms/step - loss: 9.6506e-07 - accuracy: 1.0000 - val_loss: 0.0951 - val_accuracy: 0.9896 Epoch 220/400 79/79 [==============================] - 1s 7ms/step - loss: 9.4242e-07 - accuracy: 1.0000 - val_loss: 0.0955 - val_accuracy: 0.9896 Epoch 221/400 79/79 [==============================] - 1s 7ms/step - loss: 9.3247e-07 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9896 Epoch 222/400 79/79 [==============================] - 1s 7ms/step - loss: 9.1268e-07 - accuracy: 1.0000 - val_loss: 0.0965 - val_accuracy: 0.9896 Epoch 223/400 79/79 [==============================] - 1s 7ms/step - loss: 8.4512e-07 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 0.9896 Epoch 224/400 79/79 [==============================] - 1s 7ms/step - loss: 7.8692e-07 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9896 Epoch 225/400 79/79 [==============================] - 1s 7ms/step - loss: 7.5930e-07 - accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 0.9896 Epoch 226/400 79/79 [==============================] - 1s 11ms/step - loss: 7.3015e-07 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9896 Epoch 227/400 79/79 [==============================] - 1s 7ms/step - loss: 7.3000e-07 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9896 Epoch 228/400 79/79 [==============================] - 1s 7ms/step - loss: 6.6488e-07 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.9896 Epoch 229/400 79/79 [==============================] - 1s 7ms/step - loss: 6.4071e-07 - accuracy: 1.0000 - val_loss: 0.0997 - val_accuracy: 0.9896 Epoch 230/400 79/79 [==============================] - 1s 7ms/step - loss: 6.2024e-07 - accuracy: 1.0000 - val_loss: 0.1001 - val_accuracy: 0.9896 Epoch 231/400 79/79 [==============================] - 1s 7ms/step - loss: 5.8943e-07 - accuracy: 1.0000 - val_loss: 0.1007 - val_accuracy: 0.9896 Epoch 232/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6635e-07 - accuracy: 1.0000 - val_loss: 0.1010 - val_accuracy: 0.9896 Epoch 233/400 79/79 [==============================] - 1s 7ms/step - loss: 5.4311e-07 - accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 0.9896 Epoch 234/400 79/79 [==============================] - 1s 7ms/step - loss: 5.1889e-07 - accuracy: 1.0000 - val_loss: 0.1019 - val_accuracy: 0.9896 Epoch 235/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9914e-07 - accuracy: 1.0000 - val_loss: 0.1025 - val_accuracy: 0.9896 Epoch 236/400 79/79 [==============================] - 1s 7ms/step - loss: 4.8892e-07 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 0.9896 Epoch 237/400 79/79 [==============================] - 1s 6ms/step - loss: 4.5632e-07 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 0.9896 Epoch 238/400 79/79 [==============================] - 1s 7ms/step - loss: 4.3818e-07 - accuracy: 1.0000 - val_loss: 0.1036 - val_accuracy: 0.9900 Epoch 239/400 79/79 [==============================] - 1s 7ms/step - loss: 4.2312e-07 - accuracy: 1.0000 - val_loss: 0.1043 - val_accuracy: 0.9900 Epoch 240/400 79/79 [==============================] - 1s 7ms/step - loss: 4.0240e-07 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 0.9900 Epoch 241/400 79/79 [==============================] - 1s 6ms/step - loss: 3.8764e-07 - accuracy: 1.0000 - val_loss: 0.1052 - val_accuracy: 0.9900 Epoch 242/400 79/79 [==============================] - 1s 7ms/step - loss: 3.7113e-07 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 0.9900 Epoch 243/400 79/79 [==============================] - 1s 6ms/step - loss: 3.5447e-07 - accuracy: 1.0000 - val_loss: 0.1058 - val_accuracy: 0.9900 Epoch 244/400 79/79 [==============================] - 1s 7ms/step - loss: 3.4402e-07 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9900 Epoch 245/400 79/79 [==============================] - 1s 6ms/step - loss: 3.2618e-07 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 0.9900 Epoch 246/400 79/79 [==============================] - 1s 7ms/step - loss: 3.1365e-07 - accuracy: 1.0000 - val_loss: 0.1074 - val_accuracy: 0.9900 Epoch 247/400 79/79 [==============================] - 1s 7ms/step - loss: 2.9951e-07 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9900 Epoch 248/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8696e-07 - accuracy: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.9900 Epoch 249/400 79/79 [==============================] - 1s 7ms/step - loss: 2.7783e-07 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9900 Epoch 250/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6762e-07 - accuracy: 1.0000 - val_loss: 0.1092 - val_accuracy: 0.9900 Epoch 251/400 79/79 [==============================] - 1s 6ms/step - loss: 2.6602e-07 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9900 Epoch 252/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4770e-07 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9896 Epoch 253/400 79/79 [==============================] - 1s 7ms/step - loss: 2.3165e-07 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 0.9892 Epoch 254/400 79/79 [==============================] - 1s 6ms/step - loss: 2.2150e-07 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 0.9896 Epoch 255/400 79/79 [==============================] - 1s 6ms/step - loss: 2.1205e-07 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9896 Epoch 256/400 79/79 [==============================] - 1s 6ms/step - loss: 2.1201e-07 - accuracy: 1.0000 - val_loss: 0.1126 - val_accuracy: 0.9896 Epoch 257/400 79/79 [==============================] - 1s 6ms/step - loss: 1.9501e-07 - accuracy: 1.0000 - val_loss: 0.1123 - val_accuracy: 0.9896 Epoch 258/400 79/79 [==============================] - 1s 7ms/step - loss: 1.8666e-07 - accuracy: 1.0000 - val_loss: 0.1132 - val_accuracy: 0.9896 Epoch 259/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7893e-07 - accuracy: 1.0000 - val_loss: 0.1140 - val_accuracy: 0.9896 Epoch 260/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7109e-07 - accuracy: 1.0000 - val_loss: 0.1146 - val_accuracy: 0.9896 Epoch 261/400 79/79 [==============================] - 1s 6ms/step - loss: 1.6504e-07 - accuracy: 1.0000 - val_loss: 0.1154 - val_accuracy: 0.9896 Epoch 262/400 79/79 [==============================] - 1s 7ms/step - loss: 1.5974e-07 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9896 Epoch 263/400 79/79 [==============================] - 1s 6ms/step - loss: 1.5161e-07 - accuracy: 1.0000 - val_loss: 0.1162 - val_accuracy: 0.9896 Epoch 264/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4414e-07 - accuracy: 1.0000 - val_loss: 0.1169 - val_accuracy: 0.9896 Epoch 265/400 79/79 [==============================] - 1s 6ms/step - loss: 1.3805e-07 - accuracy: 1.0000 - val_loss: 0.1174 - val_accuracy: 0.9896 Epoch 266/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3314e-07 - accuracy: 1.0000 - val_loss: 0.1183 - val_accuracy: 0.9896 Epoch 267/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2723e-07 - accuracy: 1.0000 - val_loss: 0.1189 - val_accuracy: 0.9896 Epoch 268/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2212e-07 - accuracy: 1.0000 - val_loss: 0.1190 - val_accuracy: 0.9896 Epoch 269/400 79/79 [==============================] - 1s 7ms/step - loss: 1.1708e-07 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9892 Epoch 270/400 79/79 [==============================] - 1s 7ms/step - loss: 1.1297e-07 - accuracy: 1.0000 - val_loss: 0.1210 - val_accuracy: 0.9892 Epoch 271/400 79/79 [==============================] - 1s 6ms/step - loss: 1.4267e-07 - accuracy: 1.0000 - val_loss: 0.1218 - val_accuracy: 0.9892 Epoch 272/400 79/79 [==============================] - 1s 6ms/step - loss: 1.0165e-07 - accuracy: 1.0000 - val_loss: 0.1221 - val_accuracy: 0.9892 Epoch 273/400 79/79 [==============================] - 1s 7ms/step - loss: 9.6930e-08 - accuracy: 1.0000 - val_loss: 0.1238 - val_accuracy: 0.9892 Epoch 274/400 79/79 [==============================] - 1s 6ms/step - loss: 9.3161e-08 - accuracy: 1.0000 - val_loss: 0.1245 - val_accuracy: 0.9892 Epoch 275/400 79/79 [==============================] - 1s 7ms/step - loss: 8.9359e-08 - accuracy: 1.0000 - val_loss: 0.1236 - val_accuracy: 0.9892 Epoch 276/400 79/79 [==============================] - 1s 7ms/step - loss: 8.5516e-08 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 0.9892 Epoch 277/400 79/79 [==============================] - 1s 7ms/step - loss: 8.1970e-08 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 0.9892 Epoch 278/400 79/79 [==============================] - 1s 7ms/step - loss: 7.9408e-08 - accuracy: 1.0000 - val_loss: 0.1266 - val_accuracy: 0.9892 Epoch 279/400 79/79 [==============================] - 1s 7ms/step - loss: 7.9157e-08 - accuracy: 1.0000 - val_loss: 0.1276 - val_accuracy: 0.9892 Epoch 280/400 79/79 [==============================] - 1s 7ms/step - loss: 7.2570e-08 - accuracy: 1.0000 - val_loss: 0.1279 - val_accuracy: 0.9892 Epoch 281/400 79/79 [==============================] - 1s 7ms/step - loss: 6.9062e-08 - accuracy: 1.0000 - val_loss: 0.1283 - val_accuracy: 0.9892 Epoch 282/400 79/79 [==============================] - 1s 6ms/step - loss: 6.6571e-08 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9892 Epoch 283/400 79/79 [==============================] - 1s 7ms/step - loss: 6.3675e-08 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9892 Epoch 284/400 79/79 [==============================] - 1s 6ms/step - loss: 6.1242e-08 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9892 Epoch 285/400 79/79 [==============================] - 1s 7ms/step - loss: 5.8799e-08 - accuracy: 1.0000 - val_loss: 0.1310 - val_accuracy: 0.9892 Epoch 286/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6264e-08 - accuracy: 1.0000 - val_loss: 0.1314 - val_accuracy: 0.9892 Epoch 287/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6734e-08 - accuracy: 1.0000 - val_loss: 0.1321 - val_accuracy: 0.9892 Epoch 288/400 79/79 [==============================] - 1s 7ms/step - loss: 5.2411e-08 - accuracy: 1.0000 - val_loss: 0.1322 - val_accuracy: 0.9892 Epoch 289/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9885e-08 - accuracy: 1.0000 - val_loss: 0.1331 - val_accuracy: 0.9892 Epoch 290/400 79/79 [==============================] - 1s 6ms/step - loss: 4.8236e-08 - accuracy: 1.0000 - val_loss: 0.1334 - val_accuracy: 0.9892 Epoch 291/400 79/79 [==============================] - 1s 7ms/step - loss: 4.6550e-08 - accuracy: 1.0000 - val_loss: 0.1341 - val_accuracy: 0.9892 Epoch 292/400 79/79 [==============================] - 1s 7ms/step - loss: 4.4213e-08 - accuracy: 1.0000 - val_loss: 0.1346 - val_accuracy: 0.9892 Epoch 293/400 79/79 [==============================] - 1s 7ms/step - loss: 4.2812e-08 - accuracy: 1.0000 - val_loss: 0.1348 - val_accuracy: 0.9892 Epoch 294/400 79/79 [==============================] - 1s 7ms/step - loss: 4.2498e-08 - accuracy: 1.0000 - val_loss: 0.1352 - val_accuracy: 0.9892 Epoch 295/400 79/79 [==============================] - 1s 7ms/step - loss: 3.9641e-08 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9892 Epoch 296/400 79/79 [==============================] - 1s 7ms/step - loss: 3.8775e-08 - accuracy: 1.0000 - val_loss: 0.1343 - val_accuracy: 0.9896 Epoch 297/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9838e-08 - accuracy: 1.0000 - val_loss: 0.1330 - val_accuracy: 0.9896 Epoch 298/400 79/79 [==============================] - 1s 7ms/step - loss: 3.5308e-08 - accuracy: 1.0000 - val_loss: 0.1376 - val_accuracy: 0.9892 Epoch 299/400 79/79 [==============================] - 1s 7ms/step - loss: 5.0419e-08 - accuracy: 1.0000 - val_loss: 0.1292 - val_accuracy: 0.9896 Epoch 300/400 79/79 [==============================] - 1s 7ms/step - loss: 3.4353e-08 - accuracy: 1.0000 - val_loss: 0.1311 - val_accuracy: 0.9896 Epoch 301/400 79/79 [==============================] - 1s 7ms/step - loss: 3.2750e-08 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9896 Epoch 302/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0650e-08 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9896 Epoch 303/400 79/79 [==============================] - 1s 7ms/step - loss: 2.9290e-08 - accuracy: 1.0000 - val_loss: 0.1348 - val_accuracy: 0.9896 Epoch 304/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8035e-08 - accuracy: 1.0000 - val_loss: 0.1355 - val_accuracy: 0.9896 Epoch 305/400 79/79 [==============================] - 1s 6ms/step - loss: 2.6969e-08 - accuracy: 1.0000 - val_loss: 0.1362 - val_accuracy: 0.9896 Epoch 306/400 79/79 [==============================] - 1s 7ms/step - loss: 2.5546e-08 - accuracy: 1.0000 - val_loss: 0.1367 - val_accuracy: 0.9896 Epoch 307/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4577e-08 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9896 Epoch 308/400 79/79 [==============================] - 1s 7ms/step - loss: 2.3482e-08 - accuracy: 1.0000 - val_loss: 0.1369 - val_accuracy: 0.9896 Epoch 309/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2726e-08 - accuracy: 1.0000 - val_loss: 0.1374 - val_accuracy: 0.9896 Epoch 310/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1791e-08 - accuracy: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.9900 Epoch 311/400 79/79 [==============================] - 1s 7ms/step - loss: 2.0993e-08 - accuracy: 1.0000 - val_loss: 0.1385 - val_accuracy: 0.9900 Epoch 312/400 79/79 [==============================] - 1s 7ms/step - loss: 2.0128e-08 - accuracy: 1.0000 - val_loss: 0.1393 - val_accuracy: 0.9896 Epoch 313/400 79/79 [==============================] - 1s 7ms/step - loss: 1.9871e-08 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9896 Epoch 314/400 79/79 [==============================] - 1s 7ms/step - loss: 1.8920e-08 - accuracy: 1.0000 - val_loss: 0.1411 - val_accuracy: 0.9896 Epoch 315/400 79/79 [==============================] - 1s 7ms/step - loss: 1.8015e-08 - accuracy: 1.0000 - val_loss: 0.1421 - val_accuracy: 0.9896 Epoch 316/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7445e-08 - accuracy: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.9896 Epoch 317/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7241e-08 - accuracy: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.9896 Epoch 318/400 79/79 [==============================] - 1s 6ms/step - loss: 1.6312e-08 - accuracy: 1.0000 - val_loss: 0.1443 - val_accuracy: 0.9896 Epoch 319/400 79/79 [==============================] - 1s 7ms/step - loss: 1.5645e-08 - accuracy: 1.0000 - val_loss: 0.1450 - val_accuracy: 0.9896 Epoch 320/400 79/79 [==============================] - 1s 7ms/step - loss: 1.5163e-08 - accuracy: 1.0000 - val_loss: 0.1456 - val_accuracy: 0.9896 Epoch 321/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4684e-08 - accuracy: 1.0000 - val_loss: 0.1459 - val_accuracy: 0.9896 Epoch 322/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4123e-08 - accuracy: 1.0000 - val_loss: 0.1465 - val_accuracy: 0.9896 Epoch 323/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3666e-08 - accuracy: 1.0000 - val_loss: 0.1471 - val_accuracy: 0.9892 Epoch 324/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3325e-08 - accuracy: 1.0000 - val_loss: 0.1476 - val_accuracy: 0.9892 Epoch 325/400 79/79 [==============================] - 1s 6ms/step - loss: 1.2906e-08 - accuracy: 1.0000 - val_loss: 0.1479 - val_accuracy: 0.9892 Epoch 326/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2509e-08 - accuracy: 1.0000 - val_loss: 0.1483 - val_accuracy: 0.9892 Epoch 327/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2057e-08 - accuracy: 1.0000 - val_loss: 0.1484 - val_accuracy: 0.9892 Epoch 328/400 79/79 [==============================] - 1s 6ms/step - loss: 1.1698e-08 - accuracy: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.9892 Epoch 329/400 79/79 [==============================] - 1s 7ms/step - loss: 1.1402e-08 - accuracy: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.9892 Epoch 330/400 79/79 [==============================] - 1s 6ms/step - loss: 1.1036e-08 - accuracy: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.9892 Epoch 331/400 79/79 [==============================] - 1s 6ms/step - loss: 1.0763e-08 - accuracy: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.9892 Epoch 332/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0315e-08 - accuracy: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.9896 Epoch 333/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0067e-08 - accuracy: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.9896 Epoch 334/400 79/79 [==============================] - 1s 7ms/step - loss: 9.7468e-09 - accuracy: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.9896 Epoch 335/400 79/79 [==============================] - 1s 7ms/step - loss: 9.4660e-09 - accuracy: 1.0000 - val_loss: 0.1500 - val_accuracy: 0.9896 Epoch 336/400 79/79 [==============================] - 1s 6ms/step - loss: 9.1215e-09 - accuracy: 1.0000 - val_loss: 0.1503 - val_accuracy: 0.9896 Epoch 337/400 79/79 [==============================] - 1s 6ms/step - loss: 8.9241e-09 - accuracy: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.9896 Epoch 338/400 79/79 [==============================] - 1s 7ms/step - loss: 8.6136e-09 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9896 Epoch 339/400 79/79 [==============================] - 1s 7ms/step - loss: 8.3407e-09 - accuracy: 1.0000 - val_loss: 0.1507 - val_accuracy: 0.9896 Epoch 340/400 79/79 [==============================] - 1s 6ms/step - loss: 8.1220e-09 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9896 Epoch 341/400 79/79 [==============================] - 1s 7ms/step - loss: 7.9070e-09 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9896 Epoch 342/400 79/79 [==============================] - 1s 6ms/step - loss: 7.6520e-09 - accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9896 Epoch 343/400 79/79 [==============================] - 1s 7ms/step - loss: 7.4509e-09 - accuracy: 1.0000 - val_loss: 0.1514 - val_accuracy: 0.9896 Epoch 344/400 79/79 [==============================] - 1s 7ms/step - loss: 7.2214e-09 - accuracy: 1.0000 - val_loss: 0.1515 - val_accuracy: 0.9896 Epoch 345/400 79/79 [==============================] - 1s 7ms/step - loss: 7.0713e-09 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9896 Epoch 346/400 79/79 [==============================] - 1s 7ms/step - loss: 6.8703e-09 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9900 Epoch 347/400 79/79 [==============================] - 1s 7ms/step - loss: 6.6697e-09 - accuracy: 1.0000 - val_loss: 0.1522 - val_accuracy: 0.9900 Epoch 348/400 79/79 [==============================] - 1s 6ms/step - loss: 6.4546e-09 - accuracy: 1.0000 - val_loss: 0.1524 - val_accuracy: 0.9900 Epoch 349/400 79/79 [==============================] - 1s 7ms/step - loss: 6.2903e-09 - accuracy: 1.0000 - val_loss: 0.1525 - val_accuracy: 0.9900 Epoch 350/400 79/79 [==============================] - 0s 6ms/step - loss: 6.1464e-09 - accuracy: 1.0000 - val_loss: 0.1528 - val_accuracy: 0.9900 Epoch 351/400 79/79 [==============================] - 1s 7ms/step - loss: 6.1079e-09 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9900 Epoch 352/400 79/79 [==============================] - 1s 7ms/step - loss: 5.7918e-09 - accuracy: 1.0000 - val_loss: 0.1534 - val_accuracy: 0.9900 Epoch 353/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6367e-09 - accuracy: 1.0000 - val_loss: 0.1537 - val_accuracy: 0.9900 Epoch 354/400 79/79 [==============================] - 1s 7ms/step - loss: 5.4914e-09 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 0.9900 Epoch 355/400 79/79 [==============================] - 1s 7ms/step - loss: 5.4113e-09 - accuracy: 1.0000 - val_loss: 0.1541 - val_accuracy: 0.9900 Epoch 356/400 79/79 [==============================] - 1s 7ms/step - loss: 5.2781e-09 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.9900 Epoch 357/400 79/79 [==============================] - 1s 7ms/step - loss: 5.0791e-09 - accuracy: 1.0000 - val_loss: 0.1546 - val_accuracy: 0.9900 Epoch 358/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9753e-09 - accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9900 Epoch 359/400 79/79 [==============================] - 1s 7ms/step - loss: 4.8996e-09 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9900 Epoch 360/400 79/79 [==============================] - 1s 7ms/step - loss: 4.7424e-09 - accuracy: 1.0000 - val_loss: 0.1551 - val_accuracy: 0.9900 Epoch 361/400 79/79 [==============================] - 1s 7ms/step - loss: 4.6247e-09 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9900 Epoch 362/400 79/79 [==============================] - 1s 7ms/step - loss: 4.6962e-09 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9900 Epoch 363/400 79/79 [==============================] - 1s 7ms/step - loss: 4.3938e-09 - accuracy: 1.0000 - val_loss: 0.1558 - val_accuracy: 0.9900 Epoch 364/400 79/79 [==============================] - 1s 7ms/step - loss: 4.3013e-09 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9900 Epoch 365/400 79/79 [==============================] - 1s 7ms/step - loss: 4.1931e-09 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9900 Epoch 366/400 79/79 [==============================] - 1s 7ms/step - loss: 4.1789e-09 - accuracy: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.9900 Epoch 367/400 79/79 [==============================] - 1s 7ms/step - loss: 3.9860e-09 - accuracy: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.9900 Epoch 368/400 79/79 [==============================] - 1s 7ms/step - loss: 3.8786e-09 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9900 Epoch 369/400 79/79 [==============================] - 1s 7ms/step - loss: 3.7894e-09 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9900 Epoch 370/400 79/79 [==============================] - 1s 7ms/step - loss: 3.7430e-09 - accuracy: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.9900 Epoch 371/400 79/79 [==============================] - 1s 7ms/step - loss: 3.6241e-09 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9900 Epoch 372/400 79/79 [==============================] - 1s 7ms/step - loss: 3.5539e-09 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9900 Epoch 373/400 79/79 [==============================] - 1s 7ms/step - loss: 3.4580e-09 - accuracy: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.9900 Epoch 374/400 79/79 [==============================] - 1s 7ms/step - loss: 3.3896e-09 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9900 Epoch 375/400 79/79 [==============================] - 1s 7ms/step - loss: 3.3307e-09 - accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9900 Epoch 376/400 79/79 [==============================] - 1s 7ms/step - loss: 3.2459e-09 - accuracy: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.9900 Epoch 377/400 79/79 [==============================] - 1s 7ms/step - loss: 3.2133e-09 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9900 Epoch 378/400 79/79 [==============================] - 1s 7ms/step - loss: 3.1123e-09 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9900 Epoch 379/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0661e-09 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9900 Epoch 380/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0002e-09 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9900 Epoch 381/400 79/79 [==============================] - 1s 7ms/step - loss: 2.9334e-09 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9900 Epoch 382/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8723e-09 - accuracy: 1.0000 - val_loss: 0.1586 - val_accuracy: 0.9900 Epoch 383/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8174e-09 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9900 Epoch 384/400 79/79 [==============================] - 1s 7ms/step - loss: 2.7735e-09 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9900 Epoch 385/400 79/79 [==============================] - 1s 7ms/step - loss: 2.7304e-09 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9900 Epoch 386/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6646e-09 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9900 Epoch 387/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6190e-09 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9900 Epoch 388/400 79/79 [==============================] - 1s 7ms/step - loss: 2.5701e-09 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9900 Epoch 389/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6549e-09 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9900 Epoch 390/400 79/79 [==============================] - 1s 6ms/step - loss: 2.4742e-09 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9900 Epoch 391/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4180e-09 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9900 Epoch 392/400 79/79 [==============================] - 1s 7ms/step - loss: 2.3913e-09 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9900 Epoch 393/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4473e-09 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9900 Epoch 394/400 79/79 [==============================] - 1s 6ms/step - loss: 2.2954e-09 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9900 Epoch 395/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2595e-09 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9900 Epoch 396/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2090e-09 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9900 Epoch 397/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1746e-09 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9900 Epoch 398/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1427e-09 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9900 Epoch 399/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1051e-09 - accuracy: 1.0000 - val_loss: 0.1602 - val_accuracy: 0.9900 Epoch 400/400 79/79 [==============================] - 1s 7ms/step - loss: 2.0643e-09 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9900 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1e9abf60> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec20e9fa20> # Now try a LSTM with Global Max Pooling inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 2 x = LSTM ( 5 , return_sequences = True )( i ) x = GlobalMaxPool1D ()( x ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 100 , validation_split = 0.5 , ) Epoch 1/100 79/79 [==============================] - 1s 10ms/step - loss: 0.6922 - accuracy: 0.5128 - val_loss: 0.6952 - val_accuracy: 0.4960 Epoch 2/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6920 - accuracy: 0.5256 - val_loss: 0.6938 - val_accuracy: 0.4916 Epoch 3/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6920 - accuracy: 0.5256 - val_loss: 0.6938 - val_accuracy: 0.4916 Epoch 4/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6916 - accuracy: 0.5188 - val_loss: 0.6934 - val_accuracy: 0.4920 Epoch 5/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6906 - accuracy: 0.5200 - val_loss: 0.7001 - val_accuracy: 0.4960 Epoch 6/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6898 - accuracy: 0.5216 - val_loss: 0.6995 - val_accuracy: 0.4924 Epoch 7/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6900 - accuracy: 0.5120 - val_loss: 0.7016 - val_accuracy: 0.4916 Epoch 8/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6862 - accuracy: 0.5516 - val_loss: 0.6866 - val_accuracy: 0.5744 Epoch 9/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6788 - accuracy: 0.5668 - val_loss: 0.6845 - val_accuracy: 0.5476 Epoch 10/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6640 - accuracy: 0.6072 - val_loss: 0.6606 - val_accuracy: 0.6100 Epoch 11/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6728 - accuracy: 0.5720 - val_loss: 0.6956 - val_accuracy: 0.5208 Epoch 12/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6911 - accuracy: 0.5244 - val_loss: 0.6947 - val_accuracy: 0.5100 Epoch 13/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6892 - accuracy: 0.5428 - val_loss: 0.6953 - val_accuracy: 0.5160 Epoch 14/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6879 - accuracy: 0.5524 - val_loss: 0.6920 - val_accuracy: 0.5348 Epoch 15/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6851 - accuracy: 0.5516 - val_loss: 0.6950 - val_accuracy: 0.5072 Epoch 16/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6837 - accuracy: 0.5624 - val_loss: 0.6968 - val_accuracy: 0.5120 Epoch 17/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6834 - accuracy: 0.5360 - val_loss: 0.6892 - val_accuracy: 0.5148 Epoch 18/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6794 - accuracy: 0.5340 - val_loss: 0.6846 - val_accuracy: 0.5392 Epoch 19/100 79/79 [==============================] - 1s 6ms/step - loss: 0.6782 - accuracy: 0.5444 - val_loss: 0.6825 - val_accuracy: 0.5424 Epoch 20/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6752 - accuracy: 0.5480 - val_loss: 0.6802 - val_accuracy: 0.5260 Epoch 21/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6711 - accuracy: 0.5364 - val_loss: 0.6748 - val_accuracy: 0.5260 Epoch 22/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6654 - accuracy: 0.5416 - val_loss: 0.6752 - val_accuracy: 0.5176 Epoch 23/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6588 - accuracy: 0.5520 - val_loss: 0.6679 - val_accuracy: 0.5388 Epoch 24/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6531 - accuracy: 0.5648 - val_loss: 0.6652 - val_accuracy: 0.5416 Epoch 25/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6462 - accuracy: 0.5728 - val_loss: 0.6525 - val_accuracy: 0.5784 Epoch 26/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6396 - accuracy: 0.5880 - val_loss: 0.6378 - val_accuracy: 0.6028 Epoch 27/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6258 - accuracy: 0.6152 - val_loss: 0.6318 - val_accuracy: 0.6208 Epoch 28/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6056 - accuracy: 0.6636 - val_loss: 0.5923 - val_accuracy: 0.6812 Epoch 29/100 79/79 [==============================] - 1s 7ms/step - loss: 0.5701 - accuracy: 0.7096 - val_loss: 0.5999 - val_accuracy: 0.6684 Epoch 30/100 79/79 [==============================] - 1s 7ms/step - loss: 0.5357 - accuracy: 0.7500 - val_loss: 0.5204 - val_accuracy: 0.7716 Epoch 31/100 79/79 [==============================] - 1s 7ms/step - loss: 0.5005 - accuracy: 0.7652 - val_loss: 0.5802 - val_accuracy: 0.7012 Epoch 32/100 79/79 [==============================] - 1s 7ms/step - loss: 0.5468 - accuracy: 0.7332 - val_loss: 0.5303 - val_accuracy: 0.7600 Epoch 33/100 79/79 [==============================] - 1s 7ms/step - loss: 0.4894 - accuracy: 0.7704 - val_loss: 0.4789 - val_accuracy: 0.8024 Epoch 34/100 79/79 [==============================] - 1s 6ms/step - loss: 0.4361 - accuracy: 0.8068 - val_loss: 0.4265 - val_accuracy: 0.8304 Epoch 35/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3958 - accuracy: 0.8424 - val_loss: 0.4039 - val_accuracy: 0.8512 Epoch 36/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3591 - accuracy: 0.8604 - val_loss: 0.3483 - val_accuracy: 0.8824 Epoch 37/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3223 - accuracy: 0.8788 - val_loss: 0.3191 - val_accuracy: 0.8828 Epoch 38/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3949 - accuracy: 0.8412 - val_loss: 0.3870 - val_accuracy: 0.8552 Epoch 39/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3276 - accuracy: 0.8852 - val_loss: 0.3033 - val_accuracy: 0.9040 Epoch 40/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2861 - accuracy: 0.8992 - val_loss: 0.2872 - val_accuracy: 0.8984 Epoch 41/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2709 - accuracy: 0.9060 - val_loss: 0.2601 - val_accuracy: 0.9104 Epoch 42/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2476 - accuracy: 0.9088 - val_loss: 0.2502 - val_accuracy: 0.9128 Epoch 43/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2438 - accuracy: 0.9152 - val_loss: 0.2460 - val_accuracy: 0.9124 Epoch 44/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2410 - accuracy: 0.9128 - val_loss: 0.2703 - val_accuracy: 0.9072 Epoch 45/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2333 - accuracy: 0.9188 - val_loss: 0.2288 - val_accuracy: 0.9252 Epoch 46/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2147 - accuracy: 0.9244 - val_loss: 0.2322 - val_accuracy: 0.9192 Epoch 47/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2729 - accuracy: 0.8940 - val_loss: 0.3241 - val_accuracy: 0.8808 Epoch 48/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2782 - accuracy: 0.8868 - val_loss: 0.2404 - val_accuracy: 0.9212 Epoch 49/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2247 - accuracy: 0.9220 - val_loss: 0.2149 - val_accuracy: 0.9364 Epoch 50/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2006 - accuracy: 0.9340 - val_loss: 0.1975 - val_accuracy: 0.9432 Epoch 51/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1819 - accuracy: 0.9424 - val_loss: 0.1847 - val_accuracy: 0.9492 Epoch 52/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1808 - accuracy: 0.9400 - val_loss: 0.1770 - val_accuracy: 0.9460 Epoch 53/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1819 - accuracy: 0.9396 - val_loss: 0.1812 - val_accuracy: 0.9496 Epoch 54/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1610 - accuracy: 0.9484 - val_loss: 0.1729 - val_accuracy: 0.9516 Epoch 55/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1583 - accuracy: 0.9480 - val_loss: 0.1687 - val_accuracy: 0.9524 Epoch 56/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1474 - accuracy: 0.9484 - val_loss: 0.1795 - val_accuracy: 0.9452 Epoch 57/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1531 - accuracy: 0.9440 - val_loss: 0.1821 - val_accuracy: 0.9504 Epoch 58/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1545 - accuracy: 0.9488 - val_loss: 0.1570 - val_accuracy: 0.9532 Epoch 59/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1374 - accuracy: 0.9520 - val_loss: 0.1556 - val_accuracy: 0.9528 Epoch 60/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1329 - accuracy: 0.9516 - val_loss: 0.1441 - val_accuracy: 0.9580 Epoch 61/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1282 - accuracy: 0.9556 - val_loss: 0.1637 - val_accuracy: 0.9556 Epoch 62/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1489 - accuracy: 0.9568 - val_loss: 0.1428 - val_accuracy: 0.9516 Epoch 63/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1164 - accuracy: 0.9588 - val_loss: 0.1803 - val_accuracy: 0.9516 Epoch 64/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2297 - accuracy: 0.9368 - val_loss: 0.1882 - val_accuracy: 0.9500 Epoch 65/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1285 - accuracy: 0.9552 - val_loss: 0.1299 - val_accuracy: 0.9576 Epoch 66/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1113 - accuracy: 0.9624 - val_loss: 0.1390 - val_accuracy: 0.9596 Epoch 67/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1054 - accuracy: 0.9660 - val_loss: 0.1181 - val_accuracy: 0.9628 Epoch 68/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1078 - accuracy: 0.9672 - val_loss: 0.1155 - val_accuracy: 0.9640 Epoch 69/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1340 - accuracy: 0.9636 - val_loss: 0.1945 - val_accuracy: 0.9488 Epoch 70/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1208 - accuracy: 0.9564 - val_loss: 0.1208 - val_accuracy: 0.9636 Epoch 71/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1188 - accuracy: 0.9604 - val_loss: 0.1047 - val_accuracy: 0.9664 Epoch 72/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1027 - accuracy: 0.9660 - val_loss: 0.1066 - val_accuracy: 0.9668 Epoch 73/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0941 - accuracy: 0.9736 - val_loss: 0.0951 - val_accuracy: 0.9708 Epoch 74/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0874 - accuracy: 0.9716 - val_loss: 0.0990 - val_accuracy: 0.9668 Epoch 75/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0855 - accuracy: 0.9720 - val_loss: 0.1011 - val_accuracy: 0.9688 Epoch 76/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1028 - accuracy: 0.9692 - val_loss: 0.1240 - val_accuracy: 0.9628 Epoch 77/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0825 - accuracy: 0.9720 - val_loss: 0.1017 - val_accuracy: 0.9736 Epoch 78/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0786 - accuracy: 0.9748 - val_loss: 0.0949 - val_accuracy: 0.9704 Epoch 79/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0777 - accuracy: 0.9764 - val_loss: 0.1001 - val_accuracy: 0.9728 Epoch 80/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0846 - accuracy: 0.9736 - val_loss: 0.0992 - val_accuracy: 0.9672 Epoch 81/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0718 - accuracy: 0.9804 - val_loss: 0.0954 - val_accuracy: 0.9700 Epoch 82/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0698 - accuracy: 0.9788 - val_loss: 0.0893 - val_accuracy: 0.9728 Epoch 83/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0633 - accuracy: 0.9820 - val_loss: 0.0924 - val_accuracy: 0.9732 Epoch 84/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0630 - accuracy: 0.9816 - val_loss: 0.0772 - val_accuracy: 0.9760 Epoch 85/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1032 - accuracy: 0.9664 - val_loss: 0.0859 - val_accuracy: 0.9744 Epoch 86/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0643 - accuracy: 0.9812 - val_loss: 0.0761 - val_accuracy: 0.9744 Epoch 87/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0599 - accuracy: 0.9828 - val_loss: 0.0722 - val_accuracy: 0.9756 Epoch 88/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0601 - accuracy: 0.9812 - val_loss: 0.0712 - val_accuracy: 0.9788 Epoch 89/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0545 - accuracy: 0.9816 - val_loss: 0.0688 - val_accuracy: 0.9780 Epoch 90/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0505 - accuracy: 0.9848 - val_loss: 0.0739 - val_accuracy: 0.9796 Epoch 91/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0502 - accuracy: 0.9848 - val_loss: 0.0864 - val_accuracy: 0.9764 Epoch 92/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.0678 - val_accuracy: 0.9780 Epoch 93/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0572 - accuracy: 0.9800 - val_loss: 0.0756 - val_accuracy: 0.9772 Epoch 94/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0530 - accuracy: 0.9804 - val_loss: 0.0852 - val_accuracy: 0.9780 Epoch 95/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0543 - accuracy: 0.9796 - val_loss: 0.0818 - val_accuracy: 0.9776 Epoch 96/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0487 - accuracy: 0.9840 - val_loss: 0.0531 - val_accuracy: 0.9792 Epoch 97/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0526 - accuracy: 0.9852 - val_loss: 0.0644 - val_accuracy: 0.9804 Epoch 98/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0445 - accuracy: 0.9856 - val_loss: 0.0567 - val_accuracy: 0.9796 Epoch 99/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1566 - accuracy: 0.9680 - val_loss: 0.4206 - val_accuracy: 0.8924 Epoch 100/100 79/79 [==============================] - 1s 7ms/step - loss: 1.0968 - accuracy: 0.7592 - val_loss: 1.0851 - val_accuracy: 0.6596 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec209687b8> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec20a92780> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Long Distance"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Long_Distance/#long-distance","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.2.0-rc2 # More imports from tensorflow.keras.layers import Input , SimpleRNN , GRU , LSTM , Dense , Flatten , GlobalMaxPool1D from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt ### build the dataset # This is a nonlinear AND long-distance dataset # (Actually, we will test long-distance vs. short-distance patterns) # Start with a small T and increase it later T = 10 D = 1 X = [] Y = [] def get_label ( x , i1 , i2 , i3 ): # x = sequence if x [ i1 ] < 0 and x [ i2 ] < 0 and x [ i3 ] < 0 : return 1 if x [ i1 ] < 0 and x [ i2 ] > 0 and x [ i3 ] > 0 : return 1 if x [ i1 ] > 0 and x [ i2 ] < 0 and x [ i3 ] > 0 : return 1 if x [ i1 ] > 0 and x [ i2 ] > 0 and x [ i3 ] < 0 : return 1 return 0 for t in range ( 5000 ): x = np . random . randn ( T ) X . append ( x ) y = get_label ( x , - 1 , - 2 , - 3 ) # short distance # y = get_label(x, 0, 1, 2) # long distance Y . append ( y ) X = np . array ( X ) Y = np . array ( Y ) N = len ( X ) # Try a linear model first - note: it is classification now! i = Input ( shape = ( T ,)) x = Dense ( 1 , activation = 'sigmoid' )( i ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the network r = model . fit ( X , Y , epochs = 100 , validation_split = 0.5 , ) Epoch 1/100 79/79 [==============================] - 0s 5ms/step - loss: 0.7831 - accuracy: 0.5064 - val_loss: 0.7084 - val_accuracy: 0.4972 Epoch 2/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.5164 - val_loss: 0.6992 - val_accuracy: 0.4860 Epoch 3/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5176 - val_loss: 0.7003 - val_accuracy: 0.4976 Epoch 4/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5172 - val_loss: 0.6995 - val_accuracy: 0.4948 Epoch 5/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5176 - val_loss: 0.6989 - val_accuracy: 0.4968 Epoch 6/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5204 - val_loss: 0.7013 - val_accuracy: 0.4920 Epoch 7/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.5172 - val_loss: 0.7019 - val_accuracy: 0.5088 Epoch 8/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6951 - accuracy: 0.5204 - val_loss: 0.7011 - val_accuracy: 0.4964 Epoch 9/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.7002 - val_accuracy: 0.4916 Epoch 10/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5136 - val_loss: 0.7031 - val_accuracy: 0.4936 Epoch 11/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6936 - accuracy: 0.5132 - val_loss: 0.7009 - val_accuracy: 0.4944 Epoch 12/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5256 - val_loss: 0.7014 - val_accuracy: 0.4968 Epoch 13/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5232 - val_loss: 0.6995 - val_accuracy: 0.4952 Epoch 14/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5268 - val_loss: 0.7004 - val_accuracy: 0.4848 Epoch 15/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5276 - val_loss: 0.6979 - val_accuracy: 0.5020 Epoch 16/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.5176 - val_loss: 0.6960 - val_accuracy: 0.5000 Epoch 17/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5184 - val_loss: 0.6990 - val_accuracy: 0.4872 Epoch 18/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5188 - val_loss: 0.6988 - val_accuracy: 0.4976 Epoch 19/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5132 - val_loss: 0.7003 - val_accuracy: 0.5016 Epoch 20/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5252 - val_loss: 0.7000 - val_accuracy: 0.4964 Epoch 21/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5216 - val_loss: 0.7006 - val_accuracy: 0.4952 Epoch 22/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5116 - val_loss: 0.7036 - val_accuracy: 0.4908 Epoch 23/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5136 - val_loss: 0.7015 - val_accuracy: 0.4984 Epoch 24/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5348 - val_loss: 0.7017 - val_accuracy: 0.4928 Epoch 25/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5080 - val_loss: 0.7023 - val_accuracy: 0.4968 Epoch 26/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5120 - val_loss: 0.7033 - val_accuracy: 0.4876 Epoch 27/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5308 - val_loss: 0.6994 - val_accuracy: 0.4924 Epoch 28/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5260 - val_loss: 0.7000 - val_accuracy: 0.4896 Epoch 29/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5240 - val_loss: 0.7005 - val_accuracy: 0.4980 Epoch 30/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5204 - val_loss: 0.6998 - val_accuracy: 0.4880 Epoch 31/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5216 - val_loss: 0.7035 - val_accuracy: 0.4940 Epoch 32/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5168 - val_loss: 0.7004 - val_accuracy: 0.5016 Epoch 33/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5228 - val_loss: 0.6990 - val_accuracy: 0.4928 Epoch 34/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5128 - val_loss: 0.6978 - val_accuracy: 0.4964 Epoch 35/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5136 - val_loss: 0.6975 - val_accuracy: 0.4908 Epoch 36/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5208 - val_loss: 0.6988 - val_accuracy: 0.4892 Epoch 37/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5240 - val_loss: 0.7005 - val_accuracy: 0.5012 Epoch 38/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6949 - accuracy: 0.5268 - val_loss: 0.7008 - val_accuracy: 0.4780 Epoch 39/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5104 - val_loss: 0.6999 - val_accuracy: 0.4956 Epoch 40/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5160 - val_loss: 0.7001 - val_accuracy: 0.4920 Epoch 41/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5220 - val_loss: 0.7002 - val_accuracy: 0.4952 Epoch 42/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5180 - val_loss: 0.6994 - val_accuracy: 0.5044 Epoch 43/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5176 - val_loss: 0.6996 - val_accuracy: 0.4912 Epoch 44/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5172 - val_loss: 0.7004 - val_accuracy: 0.4884 Epoch 45/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5232 - val_loss: 0.6996 - val_accuracy: 0.4980 Epoch 46/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6937 - accuracy: 0.5224 - val_loss: 0.7002 - val_accuracy: 0.4916 Epoch 47/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5192 - val_loss: 0.6982 - val_accuracy: 0.4948 Epoch 48/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5124 - val_loss: 0.7022 - val_accuracy: 0.4932 Epoch 49/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5172 - val_loss: 0.6997 - val_accuracy: 0.5000 Epoch 50/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5204 - val_loss: 0.6997 - val_accuracy: 0.4904 Epoch 51/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5316 - val_loss: 0.6997 - val_accuracy: 0.4896 Epoch 52/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5196 - val_loss: 0.6992 - val_accuracy: 0.5024 Epoch 53/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5188 - val_loss: 0.6993 - val_accuracy: 0.4888 Epoch 54/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5160 - val_loss: 0.7029 - val_accuracy: 0.4912 Epoch 55/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5208 - val_loss: 0.7008 - val_accuracy: 0.4940 Epoch 56/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5292 - val_loss: 0.7020 - val_accuracy: 0.4988 Epoch 57/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5220 - val_loss: 0.7004 - val_accuracy: 0.4916 Epoch 58/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5192 - val_loss: 0.6998 - val_accuracy: 0.4932 Epoch 59/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5184 - val_loss: 0.6988 - val_accuracy: 0.4956 Epoch 60/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5176 - val_loss: 0.6993 - val_accuracy: 0.4988 Epoch 61/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5308 - val_loss: 0.6986 - val_accuracy: 0.4944 Epoch 62/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6936 - accuracy: 0.5248 - val_loss: 0.6982 - val_accuracy: 0.4892 Epoch 63/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6935 - accuracy: 0.5100 - val_loss: 0.7012 - val_accuracy: 0.4868 Epoch 64/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5208 - val_loss: 0.7002 - val_accuracy: 0.4976 Epoch 65/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5244 - val_loss: 0.6988 - val_accuracy: 0.4948 Epoch 66/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.5108 - val_loss: 0.6994 - val_accuracy: 0.4960 Epoch 67/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5136 - val_loss: 0.7010 - val_accuracy: 0.4820 Epoch 68/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5252 - val_loss: 0.7020 - val_accuracy: 0.4888 Epoch 69/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5240 - val_loss: 0.6987 - val_accuracy: 0.4892 Epoch 70/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5232 - val_loss: 0.6988 - val_accuracy: 0.4980 Epoch 71/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6936 - accuracy: 0.5124 - val_loss: 0.7007 - val_accuracy: 0.4984 Epoch 72/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6943 - accuracy: 0.5252 - val_loss: 0.7007 - val_accuracy: 0.4888 Epoch 73/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5180 - val_loss: 0.6979 - val_accuracy: 0.4968 Epoch 74/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5124 - val_loss: 0.6989 - val_accuracy: 0.4928 Epoch 75/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6942 - accuracy: 0.5176 - val_loss: 0.6995 - val_accuracy: 0.4864 Epoch 76/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5344 - val_loss: 0.7003 - val_accuracy: 0.4940 Epoch 77/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5212 - val_loss: 0.6983 - val_accuracy: 0.4912 Epoch 78/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5256 - val_loss: 0.7017 - val_accuracy: 0.4972 Epoch 79/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6935 - accuracy: 0.5220 - val_loss: 0.7010 - val_accuracy: 0.4900 Epoch 80/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5256 - val_loss: 0.7012 - val_accuracy: 0.4932 Epoch 81/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5332 - val_loss: 0.7000 - val_accuracy: 0.4980 Epoch 82/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5216 - val_loss: 0.6996 - val_accuracy: 0.5052 Epoch 83/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.5156 - val_loss: 0.7002 - val_accuracy: 0.5020 Epoch 84/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5164 - val_loss: 0.6986 - val_accuracy: 0.4940 Epoch 85/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5152 - val_loss: 0.7002 - val_accuracy: 0.5008 Epoch 86/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5124 - val_loss: 0.6978 - val_accuracy: 0.4996 Epoch 87/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5268 - val_loss: 0.7010 - val_accuracy: 0.4960 Epoch 88/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.7032 - val_accuracy: 0.4924 Epoch 89/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5140 - val_loss: 0.6984 - val_accuracy: 0.4972 Epoch 90/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5220 - val_loss: 0.7006 - val_accuracy: 0.4844 Epoch 91/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5252 - val_loss: 0.7019 - val_accuracy: 0.4912 Epoch 92/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5192 - val_loss: 0.6996 - val_accuracy: 0.4928 Epoch 93/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5052 - val_loss: 0.6996 - val_accuracy: 0.4900 Epoch 94/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5180 - val_loss: 0.7006 - val_accuracy: 0.4960 Epoch 95/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5216 - val_loss: 0.6973 - val_accuracy: 0.4960 Epoch 96/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.5172 - val_loss: 0.7002 - val_accuracy: 0.4872 Epoch 97/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5200 - val_loss: 0.7001 - val_accuracy: 0.4948 Epoch 98/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5272 - val_loss: 0.7023 - val_accuracy: 0.4972 Epoch 99/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5164 - val_loss: 0.7003 - val_accuracy: 0.4908 Epoch 100/100 79/79 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5124 - val_loss: 0.6988 - val_accuracy: 0.5004 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec38271438> # Plot the accuracy too - should be around 50% plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec382777b8> # Now try a simple RNN inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 # x = LSTM(5)(i) x = SimpleRNN ( 5 )( i ) # x = GRU(5)(i) # method 2 # x = LSTM(5, return_sequences=True)(i) # x = GlobalMaxPool1D()(x) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , # optimizer='rmsprop', # optimizer='adam', optimizer = Adam ( lr = 0.01 ), # optimizer=SGD(lr=0.1, momentum=0.9), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 12ms/step - loss: 0.6980 - accuracy: 0.5232 - val_loss: 0.6856 - val_accuracy: 0.5432 Epoch 2/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6726 - accuracy: 0.5752 - val_loss: 0.6726 - val_accuracy: 0.5756 Epoch 3/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6472 - accuracy: 0.6276 - val_loss: 0.6279 - val_accuracy: 0.6808 Epoch 4/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5874 - accuracy: 0.7200 - val_loss: 0.5298 - val_accuracy: 0.7960 Epoch 5/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4572 - accuracy: 0.8532 - val_loss: 0.3922 - val_accuracy: 0.8800 Epoch 6/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3494 - accuracy: 0.9056 - val_loss: 0.3111 - val_accuracy: 0.9036 Epoch 7/200 79/79 [==============================] - 1s 10ms/step - loss: 0.2770 - accuracy: 0.9252 - val_loss: 0.2596 - val_accuracy: 0.9248 Epoch 8/200 79/79 [==============================] - 1s 11ms/step - loss: 0.2400 - accuracy: 0.9316 - val_loss: 0.2448 - val_accuracy: 0.9172 Epoch 9/200 79/79 [==============================] - 1s 10ms/step - loss: 0.2085 - accuracy: 0.9356 - val_loss: 0.2131 - val_accuracy: 0.9280 Epoch 10/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1890 - accuracy: 0.9444 - val_loss: 0.1918 - val_accuracy: 0.9376 Epoch 11/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1724 - accuracy: 0.9464 - val_loss: 0.1774 - val_accuracy: 0.9452 Epoch 12/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1559 - accuracy: 0.9528 - val_loss: 0.1733 - val_accuracy: 0.9432 Epoch 13/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1501 - accuracy: 0.9556 - val_loss: 0.1589 - val_accuracy: 0.9476 Epoch 14/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1448 - accuracy: 0.9572 - val_loss: 0.1582 - val_accuracy: 0.9480 Epoch 15/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1415 - accuracy: 0.9544 - val_loss: 0.1503 - val_accuracy: 0.9520 Epoch 16/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1274 - accuracy: 0.9612 - val_loss: 0.1420 - val_accuracy: 0.9484 Epoch 17/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1220 - accuracy: 0.9620 - val_loss: 0.1324 - val_accuracy: 0.9552 Epoch 18/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1146 - accuracy: 0.9632 - val_loss: 0.1278 - val_accuracy: 0.9612 Epoch 19/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1102 - accuracy: 0.9708 - val_loss: 0.1237 - val_accuracy: 0.9644 Epoch 20/200 79/79 [==============================] - 1s 10ms/step - loss: 0.1083 - accuracy: 0.9620 - val_loss: 0.1209 - val_accuracy: 0.9552 Epoch 21/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1070 - accuracy: 0.9664 - val_loss: 0.1181 - val_accuracy: 0.9608 Epoch 22/200 79/79 [==============================] - 1s 11ms/step - loss: 0.1024 - accuracy: 0.9688 - val_loss: 0.1194 - val_accuracy: 0.9644 Epoch 23/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0990 - accuracy: 0.9716 - val_loss: 0.1166 - val_accuracy: 0.9628 Epoch 24/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0969 - accuracy: 0.9696 - val_loss: 0.1108 - val_accuracy: 0.9584 Epoch 25/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0918 - accuracy: 0.9700 - val_loss: 0.1077 - val_accuracy: 0.9652 Epoch 26/200 79/79 [==============================] - 1s 12ms/step - loss: 0.0926 - accuracy: 0.9732 - val_loss: 0.1160 - val_accuracy: 0.9560 Epoch 27/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0943 - accuracy: 0.9712 - val_loss: 0.1010 - val_accuracy: 0.9676 Epoch 28/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0866 - accuracy: 0.9736 - val_loss: 0.1106 - val_accuracy: 0.9672 Epoch 29/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0881 - accuracy: 0.9768 - val_loss: 0.1083 - val_accuracy: 0.9664 Epoch 30/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0850 - accuracy: 0.9728 - val_loss: 0.1055 - val_accuracy: 0.9656 Epoch 31/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0824 - accuracy: 0.9736 - val_loss: 0.1030 - val_accuracy: 0.9648 Epoch 32/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0801 - accuracy: 0.9772 - val_loss: 0.1006 - val_accuracy: 0.9660 Epoch 33/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0801 - accuracy: 0.9740 - val_loss: 0.0912 - val_accuracy: 0.9712 Epoch 34/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0767 - accuracy: 0.9768 - val_loss: 0.0899 - val_accuracy: 0.9688 Epoch 35/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0783 - accuracy: 0.9780 - val_loss: 0.0996 - val_accuracy: 0.9656 Epoch 36/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0740 - accuracy: 0.9776 - val_loss: 0.0913 - val_accuracy: 0.9704 Epoch 37/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0731 - accuracy: 0.9796 - val_loss: 0.0913 - val_accuracy: 0.9712 Epoch 38/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0728 - accuracy: 0.9776 - val_loss: 0.0913 - val_accuracy: 0.9684 Epoch 39/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0749 - accuracy: 0.9776 - val_loss: 0.0900 - val_accuracy: 0.9724 Epoch 40/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0743 - accuracy: 0.9780 - val_loss: 0.0903 - val_accuracy: 0.9704 Epoch 41/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0738 - accuracy: 0.9756 - val_loss: 0.0965 - val_accuracy: 0.9696 Epoch 42/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0735 - accuracy: 0.9780 - val_loss: 0.0941 - val_accuracy: 0.9696 Epoch 43/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0684 - accuracy: 0.9768 - val_loss: 0.0841 - val_accuracy: 0.9696 Epoch 44/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0729 - accuracy: 0.9752 - val_loss: 0.0926 - val_accuracy: 0.9696 Epoch 45/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0690 - accuracy: 0.9776 - val_loss: 0.0843 - val_accuracy: 0.9772 Epoch 46/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0671 - accuracy: 0.9800 - val_loss: 0.0897 - val_accuracy: 0.9712 Epoch 47/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0622 - accuracy: 0.9836 - val_loss: 0.0944 - val_accuracy: 0.9680 Epoch 48/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0703 - accuracy: 0.9792 - val_loss: 0.0870 - val_accuracy: 0.9724 Epoch 49/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0728 - accuracy: 0.9796 - val_loss: 0.0820 - val_accuracy: 0.9792 Epoch 50/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0679 - accuracy: 0.9800 - val_loss: 0.0891 - val_accuracy: 0.9752 Epoch 51/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0661 - accuracy: 0.9788 - val_loss: 0.0897 - val_accuracy: 0.9708 Epoch 52/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0649 - accuracy: 0.9792 - val_loss: 0.0850 - val_accuracy: 0.9732 Epoch 53/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0655 - accuracy: 0.9832 - val_loss: 0.0773 - val_accuracy: 0.9740 Epoch 54/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0626 - accuracy: 0.9808 - val_loss: 0.0780 - val_accuracy: 0.9732 Epoch 55/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0593 - accuracy: 0.9832 - val_loss: 0.0804 - val_accuracy: 0.9756 Epoch 56/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0672 - accuracy: 0.9764 - val_loss: 0.0800 - val_accuracy: 0.9728 Epoch 57/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0639 - accuracy: 0.9816 - val_loss: 0.0769 - val_accuracy: 0.9748 Epoch 58/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0602 - accuracy: 0.9844 - val_loss: 0.0737 - val_accuracy: 0.9728 Epoch 59/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0614 - accuracy: 0.9816 - val_loss: 0.0773 - val_accuracy: 0.9768 Epoch 60/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0595 - accuracy: 0.9836 - val_loss: 0.0908 - val_accuracy: 0.9756 Epoch 61/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0625 - accuracy: 0.9832 - val_loss: 0.0692 - val_accuracy: 0.9796 Epoch 62/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0569 - accuracy: 0.9840 - val_loss: 0.0730 - val_accuracy: 0.9768 Epoch 63/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0552 - accuracy: 0.9832 - val_loss: 0.0710 - val_accuracy: 0.9772 Epoch 64/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0561 - accuracy: 0.9836 - val_loss: 0.0686 - val_accuracy: 0.9784 Epoch 65/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0547 - accuracy: 0.9848 - val_loss: 0.0705 - val_accuracy: 0.9800 Epoch 66/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0524 - accuracy: 0.9852 - val_loss: 0.0934 - val_accuracy: 0.9736 Epoch 67/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0548 - accuracy: 0.9840 - val_loss: 0.0762 - val_accuracy: 0.9796 Epoch 68/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0528 - accuracy: 0.9836 - val_loss: 0.0666 - val_accuracy: 0.9816 Epoch 69/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0510 - accuracy: 0.9876 - val_loss: 0.0894 - val_accuracy: 0.9756 Epoch 70/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0501 - accuracy: 0.9856 - val_loss: 0.0691 - val_accuracy: 0.9796 Epoch 71/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0511 - accuracy: 0.9868 - val_loss: 0.0680 - val_accuracy: 0.9792 Epoch 72/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0536 - accuracy: 0.9836 - val_loss: 0.0819 - val_accuracy: 0.9768 Epoch 73/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0553 - accuracy: 0.9816 - val_loss: 0.0653 - val_accuracy: 0.9800 Epoch 74/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0554 - accuracy: 0.9852 - val_loss: 0.0711 - val_accuracy: 0.9812 Epoch 75/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0483 - accuracy: 0.9880 - val_loss: 0.0823 - val_accuracy: 0.9732 Epoch 76/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0512 - accuracy: 0.9852 - val_loss: 0.0901 - val_accuracy: 0.9808 Epoch 77/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0539 - accuracy: 0.9844 - val_loss: 0.0752 - val_accuracy: 0.9792 Epoch 78/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0491 - accuracy: 0.9872 - val_loss: 0.0671 - val_accuracy: 0.9816 Epoch 79/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0471 - accuracy: 0.9876 - val_loss: 0.0702 - val_accuracy: 0.9820 Epoch 80/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0547 - accuracy: 0.9828 - val_loss: 0.0739 - val_accuracy: 0.9812 Epoch 81/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0561 - accuracy: 0.9816 - val_loss: 0.0658 - val_accuracy: 0.9808 Epoch 82/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0646 - accuracy: 0.9856 - val_loss: 0.0728 - val_accuracy: 0.9788 Epoch 83/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0576 - accuracy: 0.9824 - val_loss: 0.0738 - val_accuracy: 0.9772 Epoch 84/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0573 - accuracy: 0.9824 - val_loss: 0.0668 - val_accuracy: 0.9824 Epoch 85/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0428 - accuracy: 0.9896 - val_loss: 0.0827 - val_accuracy: 0.9804 Epoch 86/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0480 - accuracy: 0.9888 - val_loss: 0.0679 - val_accuracy: 0.9804 Epoch 87/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0436 - accuracy: 0.9888 - val_loss: 0.0677 - val_accuracy: 0.9848 Epoch 88/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0465 - accuracy: 0.9884 - val_loss: 0.0693 - val_accuracy: 0.9804 Epoch 89/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0417 - accuracy: 0.9896 - val_loss: 0.0731 - val_accuracy: 0.9784 Epoch 90/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0465 - accuracy: 0.9868 - val_loss: 0.0693 - val_accuracy: 0.9840 Epoch 91/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0423 - accuracy: 0.9904 - val_loss: 0.0667 - val_accuracy: 0.9840 Epoch 92/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0436 - accuracy: 0.9892 - val_loss: 0.0683 - val_accuracy: 0.9824 Epoch 93/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0470 - accuracy: 0.9864 - val_loss: 0.0644 - val_accuracy: 0.9824 Epoch 94/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0460 - accuracy: 0.9868 - val_loss: 0.0711 - val_accuracy: 0.9788 Epoch 95/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0398 - accuracy: 0.9896 - val_loss: 0.0688 - val_accuracy: 0.9784 Epoch 96/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0410 - accuracy: 0.9868 - val_loss: 0.0733 - val_accuracy: 0.9836 Epoch 97/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0455 - accuracy: 0.9860 - val_loss: 0.0647 - val_accuracy: 0.9848 Epoch 98/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0390 - accuracy: 0.9888 - val_loss: 0.0671 - val_accuracy: 0.9804 Epoch 99/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0387 - accuracy: 0.9884 - val_loss: 0.0686 - val_accuracy: 0.9812 Epoch 100/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0396 - accuracy: 0.9888 - val_loss: 0.0648 - val_accuracy: 0.9800 Epoch 101/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0384 - accuracy: 0.9908 - val_loss: 0.0626 - val_accuracy: 0.9832 Epoch 102/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0415 - accuracy: 0.9884 - val_loss: 0.0720 - val_accuracy: 0.9792 Epoch 103/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0401 - accuracy: 0.9852 - val_loss: 0.0676 - val_accuracy: 0.9820 Epoch 104/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0396 - accuracy: 0.9900 - val_loss: 0.0642 - val_accuracy: 0.9832 Epoch 105/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0422 - accuracy: 0.9872 - val_loss: 0.0724 - val_accuracy: 0.9856 Epoch 106/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0429 - accuracy: 0.9884 - val_loss: 0.0810 - val_accuracy: 0.9828 Epoch 107/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0385 - accuracy: 0.9888 - val_loss: 0.0699 - val_accuracy: 0.9812 Epoch 108/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0494 - accuracy: 0.9868 - val_loss: 0.0689 - val_accuracy: 0.9832 Epoch 109/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0383 - accuracy: 0.9872 - val_loss: 0.0628 - val_accuracy: 0.9820 Epoch 110/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0373 - accuracy: 0.9896 - val_loss: 0.0707 - val_accuracy: 0.9768 Epoch 111/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0348 - accuracy: 0.9892 - val_loss: 0.0613 - val_accuracy: 0.9852 Epoch 112/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0375 - accuracy: 0.9872 - val_loss: 0.0828 - val_accuracy: 0.9820 Epoch 113/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0419 - accuracy: 0.9904 - val_loss: 0.0732 - val_accuracy: 0.9828 Epoch 114/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0386 - accuracy: 0.9868 - val_loss: 0.0676 - val_accuracy: 0.9816 Epoch 115/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0357 - accuracy: 0.9904 - val_loss: 0.0647 - val_accuracy: 0.9828 Epoch 116/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0346 - accuracy: 0.9912 - val_loss: 0.0648 - val_accuracy: 0.9816 Epoch 117/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0339 - accuracy: 0.9892 - val_loss: 0.0679 - val_accuracy: 0.9816 Epoch 118/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0320 - accuracy: 0.9920 - val_loss: 0.0682 - val_accuracy: 0.9828 Epoch 119/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0329 - accuracy: 0.9916 - val_loss: 0.0638 - val_accuracy: 0.9840 Epoch 120/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0324 - accuracy: 0.9904 - val_loss: 0.0648 - val_accuracy: 0.9844 Epoch 121/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0312 - accuracy: 0.9924 - val_loss: 0.0639 - val_accuracy: 0.9824 Epoch 122/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0328 - accuracy: 0.9884 - val_loss: 0.0642 - val_accuracy: 0.9836 Epoch 123/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0380 - accuracy: 0.9888 - val_loss: 0.0727 - val_accuracy: 0.9840 Epoch 124/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0513 - accuracy: 0.9864 - val_loss: 0.0668 - val_accuracy: 0.9844 Epoch 125/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0475 - accuracy: 0.9884 - val_loss: 0.0730 - val_accuracy: 0.9808 Epoch 126/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0350 - accuracy: 0.9884 - val_loss: 0.0622 - val_accuracy: 0.9848 Epoch 127/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0326 - accuracy: 0.9896 - val_loss: 0.0797 - val_accuracy: 0.9800 Epoch 128/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0371 - accuracy: 0.9908 - val_loss: 0.0654 - val_accuracy: 0.9812 Epoch 129/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0304 - accuracy: 0.9916 - val_loss: 0.0640 - val_accuracy: 0.9844 Epoch 130/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0319 - accuracy: 0.9908 - val_loss: 0.0760 - val_accuracy: 0.9820 Epoch 131/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0325 - accuracy: 0.9904 - val_loss: 0.0625 - val_accuracy: 0.9840 Epoch 132/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0307 - accuracy: 0.9920 - val_loss: 0.0597 - val_accuracy: 0.9848 Epoch 133/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0355 - accuracy: 0.9904 - val_loss: 0.0628 - val_accuracy: 0.9796 Epoch 134/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0347 - accuracy: 0.9884 - val_loss: 0.0627 - val_accuracy: 0.9848 Epoch 135/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0478 - accuracy: 0.9848 - val_loss: 0.0708 - val_accuracy: 0.9832 Epoch 136/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0347 - accuracy: 0.9880 - val_loss: 0.0623 - val_accuracy: 0.9844 Epoch 137/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0284 - accuracy: 0.9928 - val_loss: 0.0598 - val_accuracy: 0.9852 Epoch 138/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0464 - accuracy: 0.9920 - val_loss: 0.0620 - val_accuracy: 0.9824 Epoch 139/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0402 - accuracy: 0.9852 - val_loss: 0.0611 - val_accuracy: 0.9852 Epoch 140/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0291 - accuracy: 0.9912 - val_loss: 0.0594 - val_accuracy: 0.9836 Epoch 141/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0353 - accuracy: 0.9892 - val_loss: 0.0697 - val_accuracy: 0.9828 Epoch 142/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0354 - accuracy: 0.9912 - val_loss: 0.0577 - val_accuracy: 0.9844 Epoch 143/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0330 - accuracy: 0.9908 - val_loss: 0.0756 - val_accuracy: 0.9800 Epoch 144/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0322 - accuracy: 0.9936 - val_loss: 0.0530 - val_accuracy: 0.9848 Epoch 145/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0274 - accuracy: 0.9924 - val_loss: 0.0588 - val_accuracy: 0.9872 Epoch 146/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0265 - accuracy: 0.9924 - val_loss: 0.0548 - val_accuracy: 0.9848 Epoch 147/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0268 - accuracy: 0.9924 - val_loss: 0.0554 - val_accuracy: 0.9840 Epoch 148/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0255 - accuracy: 0.9936 - val_loss: 0.0527 - val_accuracy: 0.9868 Epoch 149/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0324 - accuracy: 0.9920 - val_loss: 0.0594 - val_accuracy: 0.9856 Epoch 150/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0282 - accuracy: 0.9916 - val_loss: 0.0602 - val_accuracy: 0.9844 Epoch 151/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0265 - accuracy: 0.9924 - val_loss: 0.0558 - val_accuracy: 0.9864 Epoch 152/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0275 - accuracy: 0.9916 - val_loss: 0.0588 - val_accuracy: 0.9860 Epoch 153/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0275 - accuracy: 0.9928 - val_loss: 0.0579 - val_accuracy: 0.9864 Epoch 154/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0264 - accuracy: 0.9904 - val_loss: 0.0617 - val_accuracy: 0.9860 Epoch 155/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0290 - accuracy: 0.9904 - val_loss: 0.0514 - val_accuracy: 0.9864 Epoch 156/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0293 - accuracy: 0.9920 - val_loss: 0.0588 - val_accuracy: 0.9840 Epoch 157/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0265 - accuracy: 0.9940 - val_loss: 0.0579 - val_accuracy: 0.9880 Epoch 158/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0252 - accuracy: 0.9928 - val_loss: 0.0725 - val_accuracy: 0.9812 Epoch 159/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0300 - accuracy: 0.9888 - val_loss: 0.0600 - val_accuracy: 0.9844 Epoch 160/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0244 - accuracy: 0.9920 - val_loss: 0.0549 - val_accuracy: 0.9840 Epoch 161/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0264 - accuracy: 0.9928 - val_loss: 0.0641 - val_accuracy: 0.9840 Epoch 162/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0262 - accuracy: 0.9912 - val_loss: 0.0681 - val_accuracy: 0.9784 Epoch 163/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.0571 - val_accuracy: 0.9852 Epoch 164/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0226 - accuracy: 0.9944 - val_loss: 0.0585 - val_accuracy: 0.9836 Epoch 165/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0294 - accuracy: 0.9904 - val_loss: 0.0614 - val_accuracy: 0.9836 Epoch 166/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0302 - accuracy: 0.9904 - val_loss: 0.0516 - val_accuracy: 0.9868 Epoch 167/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0314 - accuracy: 0.9916 - val_loss: 0.0550 - val_accuracy: 0.9824 Epoch 168/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0274 - accuracy: 0.9896 - val_loss: 0.0558 - val_accuracy: 0.9828 Epoch 169/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0237 - accuracy: 0.9940 - val_loss: 0.0630 - val_accuracy: 0.9836 Epoch 170/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0252 - accuracy: 0.9912 - val_loss: 0.0737 - val_accuracy: 0.9816 Epoch 171/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.0523 - val_accuracy: 0.9872 Epoch 172/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0277 - accuracy: 0.9920 - val_loss: 0.0562 - val_accuracy: 0.9844 Epoch 173/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0292 - accuracy: 0.9924 - val_loss: 0.0613 - val_accuracy: 0.9852 Epoch 174/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0265 - accuracy: 0.9932 - val_loss: 0.0604 - val_accuracy: 0.9864 Epoch 175/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0280 - accuracy: 0.9940 - val_loss: 0.0560 - val_accuracy: 0.9856 Epoch 176/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0383 - accuracy: 0.9912 - val_loss: 0.0514 - val_accuracy: 0.9852 Epoch 177/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0347 - accuracy: 0.9908 - val_loss: 0.0503 - val_accuracy: 0.9864 Epoch 178/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0213 - accuracy: 0.9948 - val_loss: 0.0522 - val_accuracy: 0.9844 Epoch 179/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0241 - accuracy: 0.9932 - val_loss: 0.0493 - val_accuracy: 0.9856 Epoch 180/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.0493 - val_accuracy: 0.9876 Epoch 181/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0222 - accuracy: 0.9948 - val_loss: 0.0476 - val_accuracy: 0.9860 Epoch 182/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0272 - accuracy: 0.9904 - val_loss: 0.0577 - val_accuracy: 0.9868 Epoch 183/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0304 - accuracy: 0.9912 - val_loss: 0.0785 - val_accuracy: 0.9840 Epoch 184/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0376 - accuracy: 0.9900 - val_loss: 0.0596 - val_accuracy: 0.9848 Epoch 185/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0244 - accuracy: 0.9944 - val_loss: 0.0520 - val_accuracy: 0.9872 Epoch 186/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0525 - val_accuracy: 0.9840 Epoch 187/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0189 - accuracy: 0.9952 - val_loss: 0.0493 - val_accuracy: 0.9844 Epoch 188/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0193 - accuracy: 0.9948 - val_loss: 0.0554 - val_accuracy: 0.9852 Epoch 189/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0211 - accuracy: 0.9944 - val_loss: 0.0525 - val_accuracy: 0.9860 Epoch 190/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0201 - accuracy: 0.9948 - val_loss: 0.0558 - val_accuracy: 0.9856 Epoch 191/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0227 - accuracy: 0.9932 - val_loss: 0.0549 - val_accuracy: 0.9888 Epoch 192/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0191 - accuracy: 0.9948 - val_loss: 0.0506 - val_accuracy: 0.9840 Epoch 193/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 0.0536 - val_accuracy: 0.9864 Epoch 194/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0200 - accuracy: 0.9936 - val_loss: 0.0518 - val_accuracy: 0.9888 Epoch 195/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.0470 - val_accuracy: 0.9876 Epoch 196/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0248 - accuracy: 0.9936 - val_loss: 0.0536 - val_accuracy: 0.9868 Epoch 197/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0272 - accuracy: 0.9912 - val_loss: 0.0642 - val_accuracy: 0.9836 Epoch 198/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0264 - accuracy: 0.9912 - val_loss: 0.0448 - val_accuracy: 0.9872 Epoch 199/200 79/79 [==============================] - 1s 11ms/step - loss: 0.0568 - accuracy: 0.9896 - val_loss: 0.0793 - val_accuracy: 0.9816 Epoch 200/200 79/79 [==============================] - 1s 10ms/step - loss: 0.0396 - accuracy: 0.9904 - val_loss: 0.0707 - val_accuracy: 0.9852 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1f781828> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec380f10b8> # Now change to the long distance problem # Start with a small T and increase it later T = 10 D = 1 X = [] Y = [] for t in range ( 5000 ): x = np . random . randn ( T ) X . append ( x ) y = get_label ( x , 0 , 1 , 2 ) # long distance Y . append ( y ) X = np . array ( X ) Y = np . array ( Y ) N = len ( X ) # Now test our Simple RNN again inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = SimpleRNN ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 12ms/step - loss: 0.7009 - accuracy: 0.4996 - val_loss: 0.6948 - val_accuracy: 0.5088 Epoch 2/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6944 - accuracy: 0.5128 - val_loss: 0.6951 - val_accuracy: 0.5076 Epoch 3/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6951 - accuracy: 0.5112 - val_loss: 0.6940 - val_accuracy: 0.5160 Epoch 4/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6933 - accuracy: 0.5112 - val_loss: 0.6935 - val_accuracy: 0.5024 Epoch 5/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6946 - accuracy: 0.5092 - val_loss: 0.6932 - val_accuracy: 0.5140 Epoch 6/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6923 - accuracy: 0.5236 - val_loss: 0.6952 - val_accuracy: 0.5072 Epoch 7/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5248 - val_loss: 0.6935 - val_accuracy: 0.5088 Epoch 8/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6919 - accuracy: 0.5276 - val_loss: 0.6930 - val_accuracy: 0.5184 Epoch 9/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6936 - accuracy: 0.5220 - val_loss: 0.6934 - val_accuracy: 0.5180 Epoch 10/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6914 - accuracy: 0.5356 - val_loss: 0.6923 - val_accuracy: 0.5204 Epoch 11/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6909 - accuracy: 0.5292 - val_loss: 0.6931 - val_accuracy: 0.5232 Epoch 12/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6912 - accuracy: 0.5288 - val_loss: 0.6967 - val_accuracy: 0.5188 Epoch 13/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6921 - accuracy: 0.5184 - val_loss: 0.6934 - val_accuracy: 0.5124 Epoch 14/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6907 - accuracy: 0.5260 - val_loss: 0.6942 - val_accuracy: 0.5188 Epoch 15/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6907 - accuracy: 0.5304 - val_loss: 0.6951 - val_accuracy: 0.5188 Epoch 16/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6905 - accuracy: 0.5292 - val_loss: 0.6921 - val_accuracy: 0.5196 Epoch 17/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6906 - accuracy: 0.5416 - val_loss: 0.6937 - val_accuracy: 0.5296 Epoch 18/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6895 - accuracy: 0.5184 - val_loss: 0.6940 - val_accuracy: 0.5164 Epoch 19/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6903 - accuracy: 0.5304 - val_loss: 0.6949 - val_accuracy: 0.5120 Epoch 20/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6907 - accuracy: 0.5412 - val_loss: 0.6931 - val_accuracy: 0.5184 Epoch 21/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6893 - accuracy: 0.5444 - val_loss: 0.6942 - val_accuracy: 0.5076 Epoch 22/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6913 - accuracy: 0.5328 - val_loss: 0.6936 - val_accuracy: 0.5148 Epoch 23/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6902 - accuracy: 0.5364 - val_loss: 0.6941 - val_accuracy: 0.5260 Epoch 24/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6883 - accuracy: 0.5464 - val_loss: 0.6933 - val_accuracy: 0.5284 Epoch 25/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6887 - accuracy: 0.5392 - val_loss: 0.6921 - val_accuracy: 0.5248 Epoch 26/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6888 - accuracy: 0.5512 - val_loss: 0.6979 - val_accuracy: 0.5088 Epoch 27/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6897 - accuracy: 0.5436 - val_loss: 0.6926 - val_accuracy: 0.5312 Epoch 28/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6899 - accuracy: 0.5512 - val_loss: 0.6925 - val_accuracy: 0.5236 Epoch 29/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6886 - accuracy: 0.5516 - val_loss: 0.6927 - val_accuracy: 0.5208 Epoch 30/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6912 - accuracy: 0.5448 - val_loss: 0.6915 - val_accuracy: 0.5276 Epoch 31/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6860 - accuracy: 0.5572 - val_loss: 0.6932 - val_accuracy: 0.5264 Epoch 32/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6869 - accuracy: 0.5564 - val_loss: 0.6926 - val_accuracy: 0.5232 Epoch 33/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6858 - accuracy: 0.5604 - val_loss: 0.6936 - val_accuracy: 0.5200 Epoch 34/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6862 - accuracy: 0.5580 - val_loss: 0.6934 - val_accuracy: 0.5284 Epoch 35/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6842 - accuracy: 0.5652 - val_loss: 0.6920 - val_accuracy: 0.5384 Epoch 36/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6839 - accuracy: 0.5624 - val_loss: 0.6884 - val_accuracy: 0.5468 Epoch 37/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6796 - accuracy: 0.5712 - val_loss: 0.6856 - val_accuracy: 0.5488 Epoch 38/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6724 - accuracy: 0.5836 - val_loss: 0.6723 - val_accuracy: 0.5784 Epoch 39/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6612 - accuracy: 0.5936 - val_loss: 0.6746 - val_accuracy: 0.5872 Epoch 40/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6565 - accuracy: 0.5996 - val_loss: 0.6659 - val_accuracy: 0.6104 Epoch 41/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6572 - accuracy: 0.6176 - val_loss: 0.6759 - val_accuracy: 0.5988 Epoch 42/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6521 - accuracy: 0.6092 - val_loss: 0.6534 - val_accuracy: 0.6112 Epoch 43/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6309 - accuracy: 0.6384 - val_loss: 0.6267 - val_accuracy: 0.6444 Epoch 44/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6448 - accuracy: 0.6256 - val_loss: 0.6421 - val_accuracy: 0.6244 Epoch 45/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6301 - accuracy: 0.6380 - val_loss: 0.6289 - val_accuracy: 0.6352 Epoch 46/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6370 - accuracy: 0.6360 - val_loss: 0.7009 - val_accuracy: 0.5672 Epoch 47/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6710 - accuracy: 0.5856 - val_loss: 0.6421 - val_accuracy: 0.6224 Epoch 48/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6243 - accuracy: 0.6436 - val_loss: 0.6336 - val_accuracy: 0.6400 Epoch 49/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6238 - accuracy: 0.6516 - val_loss: 0.6313 - val_accuracy: 0.6420 Epoch 50/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6192 - accuracy: 0.6472 - val_loss: 0.6221 - val_accuracy: 0.6492 Epoch 51/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6009 - accuracy: 0.6724 - val_loss: 0.6352 - val_accuracy: 0.6508 Epoch 52/200 79/79 [==============================] - 1s 12ms/step - loss: 0.6152 - accuracy: 0.6468 - val_loss: 0.6018 - val_accuracy: 0.6596 Epoch 53/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5915 - accuracy: 0.6816 - val_loss: 0.5911 - val_accuracy: 0.6592 Epoch 54/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5794 - accuracy: 0.6776 - val_loss: 0.5868 - val_accuracy: 0.6772 Epoch 55/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6113 - accuracy: 0.6520 - val_loss: 0.5924 - val_accuracy: 0.6788 Epoch 56/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5776 - accuracy: 0.6816 - val_loss: 0.6155 - val_accuracy: 0.6468 Epoch 57/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6426 - accuracy: 0.6272 - val_loss: 0.6447 - val_accuracy: 0.6164 Epoch 58/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6070 - accuracy: 0.6544 - val_loss: 0.6043 - val_accuracy: 0.6600 Epoch 59/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5754 - accuracy: 0.6776 - val_loss: 0.5564 - val_accuracy: 0.6996 Epoch 60/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5524 - accuracy: 0.6972 - val_loss: 0.5752 - val_accuracy: 0.6976 Epoch 61/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5632 - accuracy: 0.6916 - val_loss: 0.6242 - val_accuracy: 0.6272 Epoch 62/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6013 - accuracy: 0.6508 - val_loss: 0.5646 - val_accuracy: 0.6940 Epoch 63/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5497 - accuracy: 0.7024 - val_loss: 0.5493 - val_accuracy: 0.6940 Epoch 64/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5718 - accuracy: 0.6868 - val_loss: 0.5542 - val_accuracy: 0.7016 Epoch 65/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5447 - accuracy: 0.7212 - val_loss: 0.5553 - val_accuracy: 0.7104 Epoch 66/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5345 - accuracy: 0.7232 - val_loss: 0.5673 - val_accuracy: 0.7128 Epoch 67/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5677 - accuracy: 0.7200 - val_loss: 0.5253 - val_accuracy: 0.7688 Epoch 68/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5185 - accuracy: 0.7496 - val_loss: 0.5156 - val_accuracy: 0.7620 Epoch 69/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5330 - accuracy: 0.7440 - val_loss: 0.5130 - val_accuracy: 0.7476 Epoch 70/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5412 - accuracy: 0.7452 - val_loss: 0.5197 - val_accuracy: 0.7636 Epoch 71/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4882 - accuracy: 0.7788 - val_loss: 0.5193 - val_accuracy: 0.7600 Epoch 72/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5023 - accuracy: 0.7716 - val_loss: 0.5054 - val_accuracy: 0.7704 Epoch 73/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5104 - accuracy: 0.7764 - val_loss: 0.5132 - val_accuracy: 0.7764 Epoch 74/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4758 - accuracy: 0.8048 - val_loss: 0.5033 - val_accuracy: 0.7928 Epoch 75/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4713 - accuracy: 0.8124 - val_loss: 0.4803 - val_accuracy: 0.8100 Epoch 76/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4762 - accuracy: 0.7996 - val_loss: 0.4966 - val_accuracy: 0.8000 Epoch 77/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4560 - accuracy: 0.8172 - val_loss: 0.4961 - val_accuracy: 0.7932 Epoch 78/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4521 - accuracy: 0.8200 - val_loss: 0.4663 - val_accuracy: 0.8080 Epoch 79/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4624 - accuracy: 0.8080 - val_loss: 0.5134 - val_accuracy: 0.7788 Epoch 80/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5291 - accuracy: 0.7596 - val_loss: 0.5268 - val_accuracy: 0.7668 Epoch 81/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4886 - accuracy: 0.7952 - val_loss: 0.5036 - val_accuracy: 0.7968 Epoch 82/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4483 - accuracy: 0.8192 - val_loss: 0.4857 - val_accuracy: 0.7820 Epoch 83/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4724 - accuracy: 0.7868 - val_loss: 0.5214 - val_accuracy: 0.7624 Epoch 84/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4629 - accuracy: 0.7980 - val_loss: 0.4601 - val_accuracy: 0.8044 Epoch 85/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4590 - accuracy: 0.8020 - val_loss: 0.4799 - val_accuracy: 0.7892 Epoch 86/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4843 - accuracy: 0.7808 - val_loss: 0.5494 - val_accuracy: 0.7452 Epoch 87/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5111 - accuracy: 0.7768 - val_loss: 0.4562 - val_accuracy: 0.8008 Epoch 88/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4392 - accuracy: 0.8204 - val_loss: 0.4450 - val_accuracy: 0.8128 Epoch 89/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4546 - accuracy: 0.8204 - val_loss: 0.4002 - val_accuracy: 0.8484 Epoch 90/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4200 - accuracy: 0.8312 - val_loss: 0.4415 - val_accuracy: 0.8304 Epoch 91/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4022 - accuracy: 0.8484 - val_loss: 0.4678 - val_accuracy: 0.8000 Epoch 92/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4207 - accuracy: 0.8272 - val_loss: 0.4074 - val_accuracy: 0.8504 Epoch 93/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3875 - accuracy: 0.8520 - val_loss: 0.4857 - val_accuracy: 0.8000 Epoch 94/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4068 - accuracy: 0.8356 - val_loss: 0.4879 - val_accuracy: 0.7900 Epoch 95/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4873 - accuracy: 0.8028 - val_loss: 0.4495 - val_accuracy: 0.8416 Epoch 96/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3773 - accuracy: 0.8732 - val_loss: 0.3833 - val_accuracy: 0.8672 Epoch 97/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3640 - accuracy: 0.8776 - val_loss: 0.4010 - val_accuracy: 0.8684 Epoch 98/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3723 - accuracy: 0.8716 - val_loss: 0.3782 - val_accuracy: 0.8744 Epoch 99/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4678 - accuracy: 0.8152 - val_loss: 0.6316 - val_accuracy: 0.7104 Epoch 100/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5038 - accuracy: 0.7836 - val_loss: 0.4439 - val_accuracy: 0.8280 Epoch 101/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4657 - accuracy: 0.8216 - val_loss: 0.4984 - val_accuracy: 0.7936 Epoch 102/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5420 - accuracy: 0.7528 - val_loss: 0.6300 - val_accuracy: 0.6660 Epoch 103/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5793 - accuracy: 0.7136 - val_loss: 0.5262 - val_accuracy: 0.7696 Epoch 104/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5042 - accuracy: 0.7864 - val_loss: 0.4314 - val_accuracy: 0.8424 Epoch 105/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4277 - accuracy: 0.8372 - val_loss: 0.4179 - val_accuracy: 0.8460 Epoch 106/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4170 - accuracy: 0.8296 - val_loss: 0.4903 - val_accuracy: 0.7940 Epoch 107/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3549 - accuracy: 0.8724 - val_loss: 0.3610 - val_accuracy: 0.8752 Epoch 108/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3170 - accuracy: 0.8988 - val_loss: 0.3182 - val_accuracy: 0.9036 Epoch 109/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3382 - accuracy: 0.8820 - val_loss: 0.3694 - val_accuracy: 0.8732 Epoch 110/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4927 - accuracy: 0.7988 - val_loss: 0.5154 - val_accuracy: 0.7928 Epoch 111/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4499 - accuracy: 0.8240 - val_loss: 0.4852 - val_accuracy: 0.7972 Epoch 112/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5167 - accuracy: 0.7772 - val_loss: 0.5273 - val_accuracy: 0.7652 Epoch 113/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5258 - accuracy: 0.7620 - val_loss: 0.4776 - val_accuracy: 0.8088 Epoch 114/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4321 - accuracy: 0.8384 - val_loss: 0.4269 - val_accuracy: 0.8508 Epoch 115/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3960 - accuracy: 0.8632 - val_loss: 0.3897 - val_accuracy: 0.8652 Epoch 116/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3598 - accuracy: 0.8764 - val_loss: 0.3446 - val_accuracy: 0.8836 Epoch 117/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3355 - accuracy: 0.8896 - val_loss: 0.3143 - val_accuracy: 0.9088 Epoch 118/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3117 - accuracy: 0.8960 - val_loss: 0.3016 - val_accuracy: 0.9068 Epoch 119/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4481 - accuracy: 0.8416 - val_loss: 0.4037 - val_accuracy: 0.8580 Epoch 120/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3510 - accuracy: 0.8880 - val_loss: 0.3300 - val_accuracy: 0.8932 Epoch 121/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3791 - accuracy: 0.8748 - val_loss: 0.4971 - val_accuracy: 0.8112 Epoch 122/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4033 - accuracy: 0.8528 - val_loss: 0.3500 - val_accuracy: 0.8864 Epoch 123/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3238 - accuracy: 0.8996 - val_loss: 0.3461 - val_accuracy: 0.8872 Epoch 124/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4098 - accuracy: 0.8520 - val_loss: 0.6394 - val_accuracy: 0.7480 Epoch 125/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5331 - accuracy: 0.7832 - val_loss: 0.4910 - val_accuracy: 0.8080 Epoch 126/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4373 - accuracy: 0.8432 - val_loss: 0.4168 - val_accuracy: 0.8580 Epoch 127/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3445 - accuracy: 0.8852 - val_loss: 0.3493 - val_accuracy: 0.8856 Epoch 128/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3966 - accuracy: 0.8628 - val_loss: 0.3932 - val_accuracy: 0.8676 Epoch 129/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3495 - accuracy: 0.8852 - val_loss: 0.3292 - val_accuracy: 0.8964 Epoch 130/200 79/79 [==============================] - 1s 11ms/step - loss: 0.3341 - accuracy: 0.8888 - val_loss: 0.3204 - val_accuracy: 0.9028 Epoch 131/200 79/79 [==============================] - 1s 11ms/step - loss: 0.3434 - accuracy: 0.8908 - val_loss: 0.4491 - val_accuracy: 0.8332 Epoch 132/200 79/79 [==============================] - 1s 11ms/step - loss: 0.5237 - accuracy: 0.7880 - val_loss: 0.4900 - val_accuracy: 0.7976 Epoch 133/200 79/79 [==============================] - 1s 11ms/step - loss: 0.4198 - accuracy: 0.8484 - val_loss: 0.4675 - val_accuracy: 0.8184 Epoch 134/200 79/79 [==============================] - 1s 10ms/step - loss: 0.3387 - accuracy: 0.8908 - val_loss: 0.3651 - val_accuracy: 0.8804 Epoch 135/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6448 - accuracy: 0.6680 - val_loss: 0.5885 - val_accuracy: 0.6936 Epoch 136/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5542 - accuracy: 0.7360 - val_loss: 0.5517 - val_accuracy: 0.7392 Epoch 137/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5331 - accuracy: 0.7492 - val_loss: 0.5222 - val_accuracy: 0.7568 Epoch 138/200 79/79 [==============================] - 1s 10ms/step - loss: 0.5067 - accuracy: 0.7716 - val_loss: 0.4807 - val_accuracy: 0.7924 Epoch 139/200 79/79 [==============================] - 1s 10ms/step - loss: 0.4294 - accuracy: 0.8052 - val_loss: 0.4049 - val_accuracy: 0.8116 Epoch 140/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6623 - accuracy: 0.6196 - val_loss: 0.6804 - val_accuracy: 0.5624 Epoch 141/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6885 - accuracy: 0.5320 - val_loss: 0.6820 - val_accuracy: 0.5436 Epoch 142/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6836 - accuracy: 0.5268 - val_loss: 0.6772 - val_accuracy: 0.5556 Epoch 143/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6853 - accuracy: 0.5372 - val_loss: 0.6855 - val_accuracy: 0.5480 Epoch 144/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.5328 - val_loss: 0.6866 - val_accuracy: 0.5392 Epoch 145/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6861 - accuracy: 0.5448 - val_loss: 0.6867 - val_accuracy: 0.5400 Epoch 146/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6825 - accuracy: 0.5608 - val_loss: 0.6919 - val_accuracy: 0.5484 Epoch 147/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6876 - accuracy: 0.5624 - val_loss: 0.6843 - val_accuracy: 0.5644 Epoch 148/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6837 - accuracy: 0.5612 - val_loss: 0.6810 - val_accuracy: 0.5648 Epoch 149/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6843 - accuracy: 0.5596 - val_loss: 0.6818 - val_accuracy: 0.5644 Epoch 150/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6839 - accuracy: 0.5616 - val_loss: 0.6844 - val_accuracy: 0.5536 Epoch 151/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6880 - accuracy: 0.5312 - val_loss: 0.6752 - val_accuracy: 0.5876 Epoch 152/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6816 - accuracy: 0.5660 - val_loss: 0.6818 - val_accuracy: 0.5632 Epoch 153/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6932 - accuracy: 0.5348 - val_loss: 0.6877 - val_accuracy: 0.5476 Epoch 154/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6868 - accuracy: 0.5360 - val_loss: 0.6820 - val_accuracy: 0.5600 Epoch 155/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6835 - accuracy: 0.5600 - val_loss: 0.6846 - val_accuracy: 0.5500 Epoch 156/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6807 - accuracy: 0.5616 - val_loss: 0.6772 - val_accuracy: 0.5640 Epoch 157/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6892 - accuracy: 0.5400 - val_loss: 0.6904 - val_accuracy: 0.5100 Epoch 158/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6936 - accuracy: 0.5148 - val_loss: 0.6913 - val_accuracy: 0.5220 Epoch 159/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6904 - accuracy: 0.5180 - val_loss: 0.6910 - val_accuracy: 0.5248 Epoch 160/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6913 - accuracy: 0.5260 - val_loss: 0.6924 - val_accuracy: 0.4888 Epoch 161/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.6907 - val_accuracy: 0.5220 Epoch 162/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6943 - accuracy: 0.5032 - val_loss: 0.6958 - val_accuracy: 0.5120 Epoch 163/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6947 - accuracy: 0.4876 - val_loss: 0.6931 - val_accuracy: 0.4924 Epoch 164/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6940 - accuracy: 0.4972 - val_loss: 0.6920 - val_accuracy: 0.5240 Epoch 165/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6936 - accuracy: 0.5060 - val_loss: 0.6931 - val_accuracy: 0.5024 Epoch 166/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6947 - accuracy: 0.4984 - val_loss: 0.6931 - val_accuracy: 0.5168 Epoch 167/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6928 - accuracy: 0.5196 - val_loss: 0.6936 - val_accuracy: 0.5188 Epoch 168/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6927 - accuracy: 0.5156 - val_loss: 0.6935 - val_accuracy: 0.5124 Epoch 169/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6936 - accuracy: 0.5144 - val_loss: 0.6939 - val_accuracy: 0.5252 Epoch 170/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6935 - accuracy: 0.5164 - val_loss: 0.6926 - val_accuracy: 0.5288 Epoch 171/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6965 - accuracy: 0.5052 - val_loss: 0.6938 - val_accuracy: 0.5284 Epoch 172/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6928 - accuracy: 0.5264 - val_loss: 0.6968 - val_accuracy: 0.4776 Epoch 173/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6948 - accuracy: 0.5060 - val_loss: 0.6954 - val_accuracy: 0.4988 Epoch 174/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6927 - accuracy: 0.5196 - val_loss: 0.6937 - val_accuracy: 0.5264 Epoch 175/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6922 - accuracy: 0.5316 - val_loss: 0.6917 - val_accuracy: 0.5284 Epoch 176/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6921 - accuracy: 0.5304 - val_loss: 0.6923 - val_accuracy: 0.5292 Epoch 177/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6908 - accuracy: 0.5292 - val_loss: 0.6923 - val_accuracy: 0.5248 Epoch 178/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6937 - accuracy: 0.5164 - val_loss: 0.6915 - val_accuracy: 0.5216 Epoch 179/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6929 - accuracy: 0.5084 - val_loss: 0.6931 - val_accuracy: 0.5072 Epoch 180/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6948 - accuracy: 0.4988 - val_loss: 0.6914 - val_accuracy: 0.5260 Epoch 181/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6944 - accuracy: 0.5032 - val_loss: 0.6938 - val_accuracy: 0.5148 Epoch 182/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6930 - accuracy: 0.5136 - val_loss: 0.6941 - val_accuracy: 0.5080 Epoch 183/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6935 - accuracy: 0.5152 - val_loss: 0.6947 - val_accuracy: 0.5024 Epoch 184/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6958 - val_accuracy: 0.4984 Epoch 185/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6940 - accuracy: 0.5092 - val_loss: 0.6992 - val_accuracy: 0.5260 Epoch 186/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5144 - val_loss: 0.6927 - val_accuracy: 0.5252 Epoch 187/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5200 - val_loss: 0.6918 - val_accuracy: 0.5284 Epoch 188/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6922 - accuracy: 0.5236 - val_loss: 0.6922 - val_accuracy: 0.5244 Epoch 189/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6904 - accuracy: 0.5392 - val_loss: 0.6853 - val_accuracy: 0.5540 Epoch 190/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6791 - accuracy: 0.5796 - val_loss: 0.6795 - val_accuracy: 0.5704 Epoch 191/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6787 - accuracy: 0.5696 - val_loss: 0.6742 - val_accuracy: 0.5720 Epoch 192/200 79/79 [==============================] - 1s 11ms/step - loss: 0.6815 - accuracy: 0.5708 - val_loss: 0.6789 - val_accuracy: 0.5636 Epoch 193/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6819 - accuracy: 0.5672 - val_loss: 0.6796 - val_accuracy: 0.5644 Epoch 194/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6843 - accuracy: 0.5496 - val_loss: 0.6893 - val_accuracy: 0.5508 Epoch 195/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6948 - accuracy: 0.5256 - val_loss: 0.6948 - val_accuracy: 0.4884 Epoch 196/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6938 - accuracy: 0.5128 - val_loss: 0.6924 - val_accuracy: 0.5188 Epoch 197/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6939 - accuracy: 0.4996 - val_loss: 0.6933 - val_accuracy: 0.5076 Epoch 198/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6915 - accuracy: 0.5244 - val_loss: 0.6972 - val_accuracy: 0.5020 Epoch 199/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6951 - accuracy: 0.5020 - val_loss: 0.6947 - val_accuracy: 0.4760 Epoch 200/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6943 - accuracy: 0.5044 - val_loss: 0.6935 - val_accuracy: 0.5160 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1f652c88> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1fae4470> # Now test our LSTM inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = LSTM ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 9ms/step - loss: 0.6951 - accuracy: 0.4972 - val_loss: 0.6948 - val_accuracy: 0.5068 Epoch 2/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5140 - val_loss: 0.6940 - val_accuracy: 0.4960 Epoch 3/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5072 - val_loss: 0.6948 - val_accuracy: 0.5036 Epoch 4/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5048 - val_loss: 0.6952 - val_accuracy: 0.5044 Epoch 5/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6940 - val_accuracy: 0.4968 Epoch 6/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5128 - val_loss: 0.6943 - val_accuracy: 0.4984 Epoch 7/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5100 - val_loss: 0.6947 - val_accuracy: 0.5008 Epoch 8/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5092 - val_loss: 0.6947 - val_accuracy: 0.4980 Epoch 9/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6915 - accuracy: 0.5064 - val_loss: 0.6953 - val_accuracy: 0.5012 Epoch 10/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5180 - val_loss: 0.6933 - val_accuracy: 0.5072 Epoch 11/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6891 - accuracy: 0.5340 - val_loss: 0.6916 - val_accuracy: 0.5252 Epoch 12/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.5268 - val_loss: 0.6932 - val_accuracy: 0.5208 Epoch 13/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6907 - accuracy: 0.5136 - val_loss: 0.6832 - val_accuracy: 0.5584 Epoch 14/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6768 - accuracy: 0.5896 - val_loss: 0.6899 - val_accuracy: 0.5616 Epoch 15/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6580 - accuracy: 0.6140 - val_loss: 0.6567 - val_accuracy: 0.6160 Epoch 16/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.5608 - val_loss: 0.6936 - val_accuracy: 0.5384 Epoch 17/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6870 - accuracy: 0.5492 - val_loss: 0.6713 - val_accuracy: 0.5948 Epoch 18/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6445 - accuracy: 0.6264 - val_loss: 0.6664 - val_accuracy: 0.5708 Epoch 19/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6376 - accuracy: 0.5992 - val_loss: 0.6431 - val_accuracy: 0.6356 Epoch 20/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6416 - accuracy: 0.6456 - val_loss: 0.6316 - val_accuracy: 0.6592 Epoch 21/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6230 - accuracy: 0.6392 - val_loss: 0.6345 - val_accuracy: 0.6368 Epoch 22/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6305 - accuracy: 0.6528 - val_loss: 0.6336 - val_accuracy: 0.6588 Epoch 23/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6154 - accuracy: 0.6664 - val_loss: 0.6297 - val_accuracy: 0.6420 Epoch 24/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6160 - accuracy: 0.6576 - val_loss: 0.6263 - val_accuracy: 0.6508 Epoch 25/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6650 - accuracy: 0.6028 - val_loss: 0.6672 - val_accuracy: 0.5772 Epoch 26/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6428 - accuracy: 0.6164 - val_loss: 0.6448 - val_accuracy: 0.6072 Epoch 27/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6322 - accuracy: 0.6248 - val_loss: 0.6388 - val_accuracy: 0.6304 Epoch 28/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6189 - accuracy: 0.6580 - val_loss: 0.6317 - val_accuracy: 0.6568 Epoch 29/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6113 - accuracy: 0.6692 - val_loss: 0.6560 - val_accuracy: 0.5864 Epoch 30/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6224 - accuracy: 0.6264 - val_loss: 0.6117 - val_accuracy: 0.6744 Epoch 31/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6167 - accuracy: 0.6540 - val_loss: 0.6340 - val_accuracy: 0.6412 Epoch 32/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6174 - accuracy: 0.6456 - val_loss: 0.6164 - val_accuracy: 0.6548 Epoch 33/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6041 - accuracy: 0.6556 - val_loss: 0.5988 - val_accuracy: 0.6636 Epoch 34/200 79/79 [==============================] - 0s 6ms/step - loss: 0.5678 - accuracy: 0.7044 - val_loss: 0.5724 - val_accuracy: 0.7308 Epoch 35/200 79/79 [==============================] - 0s 6ms/step - loss: 0.5348 - accuracy: 0.7356 - val_loss: 0.5414 - val_accuracy: 0.7604 Epoch 36/200 79/79 [==============================] - 0s 6ms/step - loss: 0.5063 - accuracy: 0.7720 - val_loss: 0.5096 - val_accuracy: 0.7684 Epoch 37/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4962 - accuracy: 0.7668 - val_loss: 0.4869 - val_accuracy: 0.7852 Epoch 38/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4727 - accuracy: 0.7940 - val_loss: 0.5253 - val_accuracy: 0.7580 Epoch 39/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4697 - accuracy: 0.7928 - val_loss: 0.4571 - val_accuracy: 0.8084 Epoch 40/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4708 - accuracy: 0.7836 - val_loss: 0.4460 - val_accuracy: 0.8120 Epoch 41/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4428 - accuracy: 0.8052 - val_loss: 0.4407 - val_accuracy: 0.8048 Epoch 42/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4210 - accuracy: 0.8120 - val_loss: 0.4297 - val_accuracy: 0.8144 Epoch 43/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4132 - accuracy: 0.8280 - val_loss: 0.4336 - val_accuracy: 0.8144 Epoch 44/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4101 - accuracy: 0.8260 - val_loss: 0.4483 - val_accuracy: 0.8172 Epoch 45/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4504 - accuracy: 0.8136 - val_loss: 0.4487 - val_accuracy: 0.8144 Epoch 46/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4941 - accuracy: 0.7756 - val_loss: 0.5412 - val_accuracy: 0.7388 Epoch 47/200 79/79 [==============================] - 0s 6ms/step - loss: 0.5151 - accuracy: 0.7604 - val_loss: 0.4995 - val_accuracy: 0.7672 Epoch 48/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4766 - accuracy: 0.7840 - val_loss: 0.4901 - val_accuracy: 0.7740 Epoch 49/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4641 - accuracy: 0.7880 - val_loss: 0.5086 - val_accuracy: 0.7652 Epoch 50/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4598 - accuracy: 0.8052 - val_loss: 0.4543 - val_accuracy: 0.8036 Epoch 51/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4084 - accuracy: 0.8344 - val_loss: 0.4255 - val_accuracy: 0.8240 Epoch 52/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3922 - accuracy: 0.8332 - val_loss: 0.4263 - val_accuracy: 0.8188 Epoch 53/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3988 - accuracy: 0.8376 - val_loss: 0.4253 - val_accuracy: 0.8240 Epoch 54/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3971 - accuracy: 0.8416 - val_loss: 0.4041 - val_accuracy: 0.8356 Epoch 55/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3949 - accuracy: 0.8428 - val_loss: 0.4586 - val_accuracy: 0.8116 Epoch 56/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4201 - accuracy: 0.8240 - val_loss: 0.4205 - val_accuracy: 0.8300 Epoch 57/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3922 - accuracy: 0.8360 - val_loss: 0.4084 - val_accuracy: 0.8288 Epoch 58/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4639 - accuracy: 0.8056 - val_loss: 0.4101 - val_accuracy: 0.8352 Epoch 59/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3755 - accuracy: 0.8492 - val_loss: 0.3841 - val_accuracy: 0.8512 Epoch 60/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3641 - accuracy: 0.8568 - val_loss: 0.3904 - val_accuracy: 0.8528 Epoch 61/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3600 - accuracy: 0.8576 - val_loss: 0.4801 - val_accuracy: 0.8112 Epoch 62/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4689 - accuracy: 0.8104 - val_loss: 0.4442 - val_accuracy: 0.8112 Epoch 63/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4199 - accuracy: 0.8260 - val_loss: 0.4236 - val_accuracy: 0.8212 Epoch 64/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4023 - accuracy: 0.8320 - val_loss: 0.4236 - val_accuracy: 0.8184 Epoch 65/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3846 - accuracy: 0.8368 - val_loss: 0.4025 - val_accuracy: 0.8364 Epoch 66/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3789 - accuracy: 0.8452 - val_loss: 0.4051 - val_accuracy: 0.8356 Epoch 67/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3765 - accuracy: 0.8524 - val_loss: 0.3683 - val_accuracy: 0.8592 Epoch 68/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3581 - accuracy: 0.8600 - val_loss: 0.3453 - val_accuracy: 0.8680 Epoch 69/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3633 - accuracy: 0.8544 - val_loss: 0.3584 - val_accuracy: 0.8532 Epoch 70/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3495 - accuracy: 0.8632 - val_loss: 0.3408 - val_accuracy: 0.8676 Epoch 71/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3435 - accuracy: 0.8732 - val_loss: 0.3465 - val_accuracy: 0.8696 Epoch 72/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3607 - accuracy: 0.8652 - val_loss: 0.3660 - val_accuracy: 0.8560 Epoch 73/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3500 - accuracy: 0.8644 - val_loss: 0.3522 - val_accuracy: 0.8636 Epoch 74/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8628 - val_loss: 0.3526 - val_accuracy: 0.8628 Epoch 75/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3459 - accuracy: 0.8696 - val_loss: 0.3460 - val_accuracy: 0.8668 Epoch 76/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3489 - accuracy: 0.8672 - val_loss: 0.4227 - val_accuracy: 0.8204 Epoch 77/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3986 - accuracy: 0.8300 - val_loss: 0.4046 - val_accuracy: 0.8208 Epoch 78/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3767 - accuracy: 0.8408 - val_loss: 0.3822 - val_accuracy: 0.8432 Epoch 79/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3647 - accuracy: 0.8528 - val_loss: 0.3791 - val_accuracy: 0.8408 Epoch 80/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3537 - accuracy: 0.8548 - val_loss: 0.3624 - val_accuracy: 0.8496 Epoch 81/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3432 - accuracy: 0.8604 - val_loss: 0.3626 - val_accuracy: 0.8508 Epoch 82/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3415 - accuracy: 0.8632 - val_loss: 0.3865 - val_accuracy: 0.8412 Epoch 83/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3452 - accuracy: 0.8592 - val_loss: 0.3724 - val_accuracy: 0.8492 Epoch 84/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3417 - accuracy: 0.8600 - val_loss: 0.3677 - val_accuracy: 0.8460 Epoch 85/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3497 - accuracy: 0.8620 - val_loss: 0.4046 - val_accuracy: 0.8524 Epoch 86/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3627 - accuracy: 0.8620 - val_loss: 0.3845 - val_accuracy: 0.8520 Epoch 87/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3576 - accuracy: 0.8672 - val_loss: 0.4617 - val_accuracy: 0.8260 Epoch 88/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3878 - accuracy: 0.8536 - val_loss: 0.3513 - val_accuracy: 0.8640 Epoch 89/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3353 - accuracy: 0.8796 - val_loss: 0.3367 - val_accuracy: 0.8736 Epoch 90/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3328 - accuracy: 0.8844 - val_loss: 0.3293 - val_accuracy: 0.8780 Epoch 91/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3300 - accuracy: 0.8888 - val_loss: 0.3723 - val_accuracy: 0.8684 Epoch 92/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3477 - accuracy: 0.8728 - val_loss: 0.3583 - val_accuracy: 0.8676 Epoch 93/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3552 - accuracy: 0.8668 - val_loss: 0.4290 - val_accuracy: 0.8332 Epoch 94/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3486 - accuracy: 0.8668 - val_loss: 0.3927 - val_accuracy: 0.8504 Epoch 95/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3503 - accuracy: 0.8712 - val_loss: 0.3956 - val_accuracy: 0.8524 Epoch 96/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8708 - val_loss: 0.3803 - val_accuracy: 0.8520 Epoch 97/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3204 - accuracy: 0.8824 - val_loss: 0.3470 - val_accuracy: 0.8724 Epoch 98/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3000 - accuracy: 0.8932 - val_loss: 0.3088 - val_accuracy: 0.8904 Epoch 99/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2753 - accuracy: 0.9064 - val_loss: 0.3043 - val_accuracy: 0.8900 Epoch 100/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2736 - accuracy: 0.9036 - val_loss: 0.2935 - val_accuracy: 0.8992 Epoch 101/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2749 - accuracy: 0.9072 - val_loss: 0.3115 - val_accuracy: 0.8896 Epoch 102/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2615 - accuracy: 0.9104 - val_loss: 0.3126 - val_accuracy: 0.8864 Epoch 103/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2684 - accuracy: 0.9100 - val_loss: 0.2951 - val_accuracy: 0.8984 Epoch 104/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2611 - accuracy: 0.9124 - val_loss: 0.2887 - val_accuracy: 0.8976 Epoch 105/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2725 - accuracy: 0.9096 - val_loss: 0.3431 - val_accuracy: 0.8796 Epoch 106/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2825 - accuracy: 0.9020 - val_loss: 0.2868 - val_accuracy: 0.9024 Epoch 107/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2767 - accuracy: 0.9044 - val_loss: 0.3275 - val_accuracy: 0.8832 Epoch 108/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2985 - accuracy: 0.8952 - val_loss: 0.3151 - val_accuracy: 0.8876 Epoch 109/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2815 - accuracy: 0.9044 - val_loss: 0.3018 - val_accuracy: 0.8976 Epoch 110/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2827 - accuracy: 0.9028 - val_loss: 0.2902 - val_accuracy: 0.9048 Epoch 111/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2605 - accuracy: 0.9140 - val_loss: 0.2997 - val_accuracy: 0.8952 Epoch 112/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2540 - accuracy: 0.9160 - val_loss: 0.2777 - val_accuracy: 0.9080 Epoch 113/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2428 - accuracy: 0.9176 - val_loss: 0.2913 - val_accuracy: 0.8980 Epoch 114/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2654 - accuracy: 0.9080 - val_loss: 0.3008 - val_accuracy: 0.8948 Epoch 115/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2483 - accuracy: 0.9152 - val_loss: 0.2795 - val_accuracy: 0.9028 Epoch 116/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2470 - accuracy: 0.9160 - val_loss: 0.2937 - val_accuracy: 0.8988 Epoch 117/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3590 - accuracy: 0.8600 - val_loss: 0.3673 - val_accuracy: 0.8496 Epoch 118/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3585 - accuracy: 0.8576 - val_loss: 0.3815 - val_accuracy: 0.8432 Epoch 119/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3618 - accuracy: 0.8560 - val_loss: 0.3473 - val_accuracy: 0.8648 Epoch 120/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3483 - accuracy: 0.8572 - val_loss: 0.3464 - val_accuracy: 0.8596 Epoch 121/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8584 - val_loss: 0.3421 - val_accuracy: 0.8616 Epoch 122/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3424 - accuracy: 0.8644 - val_loss: 0.3477 - val_accuracy: 0.8536 Epoch 123/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3443 - accuracy: 0.8648 - val_loss: 0.3426 - val_accuracy: 0.8528 Epoch 124/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3376 - accuracy: 0.8664 - val_loss: 0.3468 - val_accuracy: 0.8520 Epoch 125/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3363 - accuracy: 0.8612 - val_loss: 0.3369 - val_accuracy: 0.8592 Epoch 126/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3074 - accuracy: 0.8848 - val_loss: 0.2864 - val_accuracy: 0.8988 Epoch 127/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2740 - accuracy: 0.9056 - val_loss: 0.2777 - val_accuracy: 0.9044 Epoch 128/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2652 - accuracy: 0.9044 - val_loss: 0.2824 - val_accuracy: 0.9016 Epoch 129/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2626 - accuracy: 0.9064 - val_loss: 0.2775 - val_accuracy: 0.9020 Epoch 130/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2618 - accuracy: 0.9044 - val_loss: 0.2631 - val_accuracy: 0.9076 Epoch 131/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2511 - accuracy: 0.9108 - val_loss: 0.2600 - val_accuracy: 0.9156 Epoch 132/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2433 - accuracy: 0.9108 - val_loss: 0.2587 - val_accuracy: 0.9148 Epoch 133/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2551 - accuracy: 0.9084 - val_loss: 0.2811 - val_accuracy: 0.8996 Epoch 134/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2351 - accuracy: 0.9188 - val_loss: 0.2558 - val_accuracy: 0.9180 Epoch 135/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2722 - accuracy: 0.9136 - val_loss: 0.2929 - val_accuracy: 0.9044 Epoch 136/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3085 - accuracy: 0.9020 - val_loss: 0.2946 - val_accuracy: 0.9008 Epoch 137/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2960 - accuracy: 0.8980 - val_loss: 0.3340 - val_accuracy: 0.8880 Epoch 138/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2954 - accuracy: 0.8984 - val_loss: 0.3190 - val_accuracy: 0.8904 Epoch 139/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2780 - accuracy: 0.9036 - val_loss: 0.2865 - val_accuracy: 0.9020 Epoch 140/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2774 - accuracy: 0.9012 - val_loss: 0.3152 - val_accuracy: 0.8872 Epoch 141/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2738 - accuracy: 0.9024 - val_loss: 0.4013 - val_accuracy: 0.8472 Epoch 142/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2945 - accuracy: 0.8860 - val_loss: 0.3022 - val_accuracy: 0.8928 Epoch 143/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2758 - accuracy: 0.8980 - val_loss: 0.2889 - val_accuracy: 0.8976 Epoch 144/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2514 - accuracy: 0.9108 - val_loss: 0.2634 - val_accuracy: 0.9100 Epoch 145/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2428 - accuracy: 0.9152 - val_loss: 0.3172 - val_accuracy: 0.8828 Epoch 146/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3130 - accuracy: 0.8796 - val_loss: 0.3579 - val_accuracy: 0.8748 Epoch 147/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3286 - accuracy: 0.8768 - val_loss: 0.3573 - val_accuracy: 0.8692 Epoch 148/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3233 - accuracy: 0.8712 - val_loss: 0.3673 - val_accuracy: 0.8608 Epoch 149/200 79/79 [==============================] - 0s 6ms/step - loss: 0.3389 - accuracy: 0.8708 - val_loss: 0.3232 - val_accuracy: 0.8768 Epoch 150/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2951 - accuracy: 0.8892 - val_loss: 0.2972 - val_accuracy: 0.8940 Epoch 151/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2731 - accuracy: 0.9028 - val_loss: 0.2910 - val_accuracy: 0.8988 Epoch 152/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2837 - accuracy: 0.8960 - val_loss: 0.2896 - val_accuracy: 0.9000 Epoch 153/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2645 - accuracy: 0.9060 - val_loss: 0.2656 - val_accuracy: 0.9052 Epoch 154/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2674 - accuracy: 0.9060 - val_loss: 0.2817 - val_accuracy: 0.8948 Epoch 155/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2627 - accuracy: 0.9004 - val_loss: 0.2649 - val_accuracy: 0.9096 Epoch 156/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2335 - accuracy: 0.9136 - val_loss: 0.2634 - val_accuracy: 0.9152 Epoch 157/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2248 - accuracy: 0.9188 - val_loss: 0.2377 - val_accuracy: 0.9204 Epoch 158/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2385 - accuracy: 0.9156 - val_loss: 0.2319 - val_accuracy: 0.9252 Epoch 159/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2186 - accuracy: 0.9264 - val_loss: 0.2438 - val_accuracy: 0.9228 Epoch 160/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2174 - accuracy: 0.9232 - val_loss: 0.2436 - val_accuracy: 0.9128 Epoch 161/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2391 - accuracy: 0.9180 - val_loss: 0.2516 - val_accuracy: 0.9128 Epoch 162/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2556 - accuracy: 0.9036 - val_loss: 0.2540 - val_accuracy: 0.9088 Epoch 163/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2490 - accuracy: 0.9044 - val_loss: 0.2756 - val_accuracy: 0.9000 Epoch 164/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2661 - accuracy: 0.8964 - val_loss: 0.2696 - val_accuracy: 0.9040 Epoch 165/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2556 - accuracy: 0.9008 - val_loss: 0.2776 - val_accuracy: 0.8924 Epoch 166/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2363 - accuracy: 0.9112 - val_loss: 0.2491 - val_accuracy: 0.9100 Epoch 167/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2443 - accuracy: 0.9072 - val_loss: 0.2796 - val_accuracy: 0.9028 Epoch 168/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2390 - accuracy: 0.9080 - val_loss: 0.2637 - val_accuracy: 0.9100 Epoch 169/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2500 - accuracy: 0.9116 - val_loss: 0.2620 - val_accuracy: 0.9072 Epoch 170/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2543 - accuracy: 0.9076 - val_loss: 0.2640 - val_accuracy: 0.9072 Epoch 171/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2407 - accuracy: 0.9176 - val_loss: 0.3020 - val_accuracy: 0.8964 Epoch 172/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2992 - accuracy: 0.8896 - val_loss: 0.2675 - val_accuracy: 0.9076 Epoch 173/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2341 - accuracy: 0.9220 - val_loss: 0.2504 - val_accuracy: 0.9132 Epoch 174/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2303 - accuracy: 0.9220 - val_loss: 0.2278 - val_accuracy: 0.9256 Epoch 175/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2244 - accuracy: 0.9284 - val_loss: 0.2255 - val_accuracy: 0.9248 Epoch 176/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2026 - accuracy: 0.9328 - val_loss: 0.2065 - val_accuracy: 0.9332 Epoch 177/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1963 - accuracy: 0.9372 - val_loss: 0.2063 - val_accuracy: 0.9296 Epoch 178/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2004 - accuracy: 0.9288 - val_loss: 0.2070 - val_accuracy: 0.9320 Epoch 179/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2023 - accuracy: 0.9356 - val_loss: 0.2043 - val_accuracy: 0.9364 Epoch 180/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1972 - accuracy: 0.9364 - val_loss: 0.1977 - val_accuracy: 0.9360 Epoch 181/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2088 - accuracy: 0.9320 - val_loss: 0.2822 - val_accuracy: 0.9024 Epoch 182/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2404 - accuracy: 0.9160 - val_loss: 0.2308 - val_accuracy: 0.9224 Epoch 183/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2220 - accuracy: 0.9260 - val_loss: 0.2359 - val_accuracy: 0.9180 Epoch 184/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2148 - accuracy: 0.9248 - val_loss: 0.2289 - val_accuracy: 0.9208 Epoch 185/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1943 - accuracy: 0.9332 - val_loss: 0.2050 - val_accuracy: 0.9304 Epoch 186/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1820 - accuracy: 0.9384 - val_loss: 0.2460 - val_accuracy: 0.9156 Epoch 187/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2313 - accuracy: 0.9192 - val_loss: 0.2650 - val_accuracy: 0.9128 Epoch 188/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2283 - accuracy: 0.9176 - val_loss: 0.2776 - val_accuracy: 0.9100 Epoch 189/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2688 - accuracy: 0.8980 - val_loss: 0.3051 - val_accuracy: 0.8956 Epoch 190/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2721 - accuracy: 0.9004 - val_loss: 0.2945 - val_accuracy: 0.9008 Epoch 191/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2764 - accuracy: 0.8976 - val_loss: 0.2676 - val_accuracy: 0.9076 Epoch 192/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2493 - accuracy: 0.9128 - val_loss: 0.2671 - val_accuracy: 0.9052 Epoch 193/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2263 - accuracy: 0.9160 - val_loss: 0.2581 - val_accuracy: 0.9108 Epoch 194/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2231 - accuracy: 0.9200 - val_loss: 0.2770 - val_accuracy: 0.9044 Epoch 195/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2348 - accuracy: 0.9184 - val_loss: 0.2392 - val_accuracy: 0.9176 Epoch 196/200 79/79 [==============================] - 0s 6ms/step - loss: 0.1979 - accuracy: 0.9292 - val_loss: 0.2347 - val_accuracy: 0.9224 Epoch 197/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2099 - accuracy: 0.9264 - val_loss: 0.2498 - val_accuracy: 0.9144 Epoch 198/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2169 - accuracy: 0.9180 - val_loss: 0.2435 - val_accuracy: 0.9176 Epoch 199/200 79/79 [==============================] - 0s 6ms/step - loss: 0.2105 - accuracy: 0.9240 - val_loss: 0.3108 - val_accuracy: 0.8824 Epoch 200/200 79/79 [==============================] - 0s 6ms/step - loss: 0.4865 - accuracy: 0.7912 - val_loss: 0.3834 - val_accuracy: 0.8304 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec38311eb8> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1fb28978> # Make the problem harder by making T larger T = 20 D = 1 X = [] Y = [] for t in range ( 5000 ): x = np . random . randn ( T ) X . append ( x ) y = get_label ( x , 0 , 1 , 2 ) # long distance Y . append ( y ) X = np . array ( X ) Y = np . array ( Y ) N = len ( X ) # Now test our Simple RNN again inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = SimpleRNN ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 18ms/step - loss: 0.7027 - accuracy: 0.4816 - val_loss: 0.6930 - val_accuracy: 0.5052 Epoch 2/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6946 - val_accuracy: 0.5068 Epoch 3/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6920 - accuracy: 0.5136 - val_loss: 0.6973 - val_accuracy: 0.5020 Epoch 4/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6942 - accuracy: 0.5048 - val_loss: 0.6945 - val_accuracy: 0.4984 Epoch 5/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5068 - val_loss: 0.6942 - val_accuracy: 0.5088 Epoch 6/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6934 - accuracy: 0.5076 - val_loss: 0.6945 - val_accuracy: 0.5036 Epoch 7/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5204 - val_loss: 0.6951 - val_accuracy: 0.5004 Epoch 8/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5120 - val_loss: 0.6952 - val_accuracy: 0.5104 Epoch 9/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6921 - accuracy: 0.5272 - val_loss: 0.6949 - val_accuracy: 0.4976 Epoch 10/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6916 - accuracy: 0.5252 - val_loss: 0.6957 - val_accuracy: 0.5100 Epoch 11/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5172 - val_loss: 0.6950 - val_accuracy: 0.5052 Epoch 12/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6922 - accuracy: 0.5268 - val_loss: 0.6953 - val_accuracy: 0.4996 Epoch 13/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6919 - accuracy: 0.5252 - val_loss: 0.6955 - val_accuracy: 0.5100 Epoch 14/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6910 - accuracy: 0.5280 - val_loss: 0.6965 - val_accuracy: 0.5016 Epoch 15/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5296 - val_loss: 0.6960 - val_accuracy: 0.5060 Epoch 16/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6916 - accuracy: 0.5276 - val_loss: 0.6968 - val_accuracy: 0.4972 Epoch 17/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5388 - val_loss: 0.6971 - val_accuracy: 0.4988 Epoch 18/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.5200 - val_loss: 0.6969 - val_accuracy: 0.4908 Epoch 19/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6908 - accuracy: 0.5384 - val_loss: 0.6991 - val_accuracy: 0.4932 Epoch 20/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5480 - val_loss: 0.6964 - val_accuracy: 0.5024 Epoch 21/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6912 - accuracy: 0.5296 - val_loss: 0.6976 - val_accuracy: 0.4924 Epoch 22/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5304 - val_loss: 0.6980 - val_accuracy: 0.4976 Epoch 23/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5408 - val_loss: 0.6993 - val_accuracy: 0.5024 Epoch 24/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5368 - val_loss: 0.6974 - val_accuracy: 0.5044 Epoch 25/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6892 - accuracy: 0.5424 - val_loss: 0.6965 - val_accuracy: 0.5112 Epoch 26/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6882 - accuracy: 0.5452 - val_loss: 0.6970 - val_accuracy: 0.4920 Epoch 27/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6896 - accuracy: 0.5436 - val_loss: 0.6984 - val_accuracy: 0.4944 Epoch 28/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6897 - accuracy: 0.5428 - val_loss: 0.6984 - val_accuracy: 0.4944 Epoch 29/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6887 - accuracy: 0.5392 - val_loss: 0.6974 - val_accuracy: 0.5076 Epoch 30/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6901 - accuracy: 0.5380 - val_loss: 0.6968 - val_accuracy: 0.5000 Epoch 31/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6895 - accuracy: 0.5420 - val_loss: 0.6970 - val_accuracy: 0.5016 Epoch 32/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6891 - accuracy: 0.5508 - val_loss: 0.7012 - val_accuracy: 0.4816 Epoch 33/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6901 - accuracy: 0.5316 - val_loss: 0.6957 - val_accuracy: 0.5072 Epoch 34/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6916 - accuracy: 0.5228 - val_loss: 0.6947 - val_accuracy: 0.5016 Epoch 35/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6880 - accuracy: 0.5452 - val_loss: 0.6961 - val_accuracy: 0.5084 Epoch 36/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6933 - accuracy: 0.5080 - val_loss: 0.6973 - val_accuracy: 0.5136 Epoch 37/200 79/79 [==============================] - 1s 18ms/step - loss: 0.6946 - accuracy: 0.5060 - val_loss: 0.6949 - val_accuracy: 0.5072 Epoch 38/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5168 - val_loss: 0.6961 - val_accuracy: 0.5128 Epoch 39/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5176 - val_loss: 0.6916 - val_accuracy: 0.5384 Epoch 40/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5164 - val_loss: 0.6925 - val_accuracy: 0.5152 Epoch 41/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6933 - accuracy: 0.5176 - val_loss: 0.6920 - val_accuracy: 0.5288 Epoch 42/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5304 - val_loss: 0.6919 - val_accuracy: 0.5264 Epoch 43/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6913 - accuracy: 0.5156 - val_loss: 0.6940 - val_accuracy: 0.5264 Epoch 44/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6920 - accuracy: 0.5380 - val_loss: 0.6931 - val_accuracy: 0.5292 Epoch 45/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6918 - accuracy: 0.5336 - val_loss: 0.6919 - val_accuracy: 0.5276 Epoch 46/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6913 - accuracy: 0.5380 - val_loss: 0.6929 - val_accuracy: 0.4956 Epoch 47/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6921 - accuracy: 0.5256 - val_loss: 0.6917 - val_accuracy: 0.5308 Epoch 48/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6906 - accuracy: 0.5348 - val_loss: 0.6926 - val_accuracy: 0.5300 Epoch 49/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6903 - accuracy: 0.5400 - val_loss: 0.6926 - val_accuracy: 0.5184 Epoch 50/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6901 - accuracy: 0.5412 - val_loss: 0.6920 - val_accuracy: 0.5312 Epoch 51/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6902 - accuracy: 0.5416 - val_loss: 0.6933 - val_accuracy: 0.5308 Epoch 52/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6889 - accuracy: 0.5412 - val_loss: 0.6945 - val_accuracy: 0.5316 Epoch 53/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6894 - accuracy: 0.5456 - val_loss: 0.6920 - val_accuracy: 0.5324 Epoch 54/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5420 - val_loss: 0.6942 - val_accuracy: 0.5328 Epoch 55/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6891 - accuracy: 0.5452 - val_loss: 0.6933 - val_accuracy: 0.5324 Epoch 56/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6903 - accuracy: 0.5436 - val_loss: 0.6931 - val_accuracy: 0.5308 Epoch 57/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6894 - accuracy: 0.5420 - val_loss: 0.6925 - val_accuracy: 0.5340 Epoch 58/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6882 - accuracy: 0.5404 - val_loss: 0.6930 - val_accuracy: 0.5332 Epoch 59/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6909 - accuracy: 0.5404 - val_loss: 0.6931 - val_accuracy: 0.5152 Epoch 60/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6908 - accuracy: 0.5328 - val_loss: 0.6930 - val_accuracy: 0.5256 Epoch 61/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6908 - accuracy: 0.5388 - val_loss: 0.6929 - val_accuracy: 0.5324 Epoch 62/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6909 - accuracy: 0.5292 - val_loss: 0.6923 - val_accuracy: 0.5328 Epoch 63/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6899 - accuracy: 0.5396 - val_loss: 0.6932 - val_accuracy: 0.5204 Epoch 64/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6904 - accuracy: 0.5416 - val_loss: 0.6921 - val_accuracy: 0.5244 Epoch 65/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6912 - accuracy: 0.5332 - val_loss: 0.6913 - val_accuracy: 0.5396 Epoch 66/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6938 - accuracy: 0.5248 - val_loss: 0.7026 - val_accuracy: 0.4960 Epoch 67/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6948 - accuracy: 0.5192 - val_loss: 0.6984 - val_accuracy: 0.5024 Epoch 68/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6944 - accuracy: 0.5088 - val_loss: 0.6966 - val_accuracy: 0.4920 Epoch 69/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6936 - accuracy: 0.5156 - val_loss: 0.6955 - val_accuracy: 0.5000 Epoch 70/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6914 - accuracy: 0.5328 - val_loss: 0.6915 - val_accuracy: 0.5332 Epoch 71/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6908 - accuracy: 0.5444 - val_loss: 0.6909 - val_accuracy: 0.5380 Epoch 72/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5408 - val_loss: 0.6921 - val_accuracy: 0.5360 Epoch 73/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6915 - accuracy: 0.5428 - val_loss: 0.6911 - val_accuracy: 0.5356 Epoch 74/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6902 - accuracy: 0.5456 - val_loss: 0.6911 - val_accuracy: 0.5356 Epoch 75/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6911 - accuracy: 0.5392 - val_loss: 0.6921 - val_accuracy: 0.5396 Epoch 76/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6900 - accuracy: 0.5448 - val_loss: 0.6911 - val_accuracy: 0.5352 Epoch 77/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6917 - accuracy: 0.5384 - val_loss: 0.6918 - val_accuracy: 0.5348 Epoch 78/200 79/79 [==============================] - 1s 18ms/step - loss: 0.6886 - accuracy: 0.5484 - val_loss: 0.6914 - val_accuracy: 0.5348 Epoch 79/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5084 - val_loss: 0.6937 - val_accuracy: 0.5024 Epoch 80/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6942 - accuracy: 0.4972 - val_loss: 0.6940 - val_accuracy: 0.5084 Epoch 81/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6947 - accuracy: 0.5064 - val_loss: 0.6944 - val_accuracy: 0.4936 Epoch 82/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5048 - val_loss: 0.6940 - val_accuracy: 0.5016 Epoch 83/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6945 - accuracy: 0.5036 - val_loss: 0.6940 - val_accuracy: 0.5060 Epoch 84/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.4968 - val_loss: 0.6940 - val_accuracy: 0.5052 Epoch 85/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6942 - accuracy: 0.4948 - val_loss: 0.6940 - val_accuracy: 0.4992 Epoch 86/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.5044 - val_loss: 0.6948 - val_accuracy: 0.5064 Epoch 87/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5116 - val_loss: 0.6954 - val_accuracy: 0.5060 Epoch 88/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.5100 - val_loss: 0.6943 - val_accuracy: 0.4996 Epoch 89/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6943 - accuracy: 0.5136 - val_loss: 0.6951 - val_accuracy: 0.4948 Epoch 90/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6958 - accuracy: 0.4892 - val_loss: 0.6947 - val_accuracy: 0.4976 Epoch 91/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5016 - val_loss: 0.6944 - val_accuracy: 0.5016 Epoch 92/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6941 - accuracy: 0.5028 - val_loss: 0.6953 - val_accuracy: 0.4940 Epoch 93/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5112 - val_loss: 0.6955 - val_accuracy: 0.5056 Epoch 94/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6932 - accuracy: 0.5056 - val_loss: 0.6952 - val_accuracy: 0.4956 Epoch 95/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6942 - accuracy: 0.5064 - val_loss: 0.6950 - val_accuracy: 0.4992 Epoch 96/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6947 - accuracy: 0.5012 - val_loss: 0.6952 - val_accuracy: 0.4980 Epoch 97/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5092 - val_loss: 0.7003 - val_accuracy: 0.4932 Epoch 98/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6956 - accuracy: 0.4948 - val_loss: 0.6956 - val_accuracy: 0.4876 Epoch 99/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6935 - accuracy: 0.5076 - val_loss: 0.6955 - val_accuracy: 0.4872 Epoch 100/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5036 - val_loss: 0.6973 - val_accuracy: 0.4996 Epoch 101/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.5088 - val_loss: 0.6957 - val_accuracy: 0.4860 Epoch 102/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.4972 - val_loss: 0.6959 - val_accuracy: 0.4928 Epoch 103/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.4980 - val_loss: 0.6958 - val_accuracy: 0.4940 Epoch 104/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5208 - val_loss: 0.6985 - val_accuracy: 0.5020 Epoch 105/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6942 - accuracy: 0.5188 - val_loss: 0.6963 - val_accuracy: 0.4852 Epoch 106/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5036 - val_loss: 0.6962 - val_accuracy: 0.4908 Epoch 107/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6926 - accuracy: 0.5136 - val_loss: 0.6973 - val_accuracy: 0.4940 Epoch 108/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6933 - accuracy: 0.5100 - val_loss: 0.6961 - val_accuracy: 0.4920 Epoch 109/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.6963 - val_accuracy: 0.4932 Epoch 110/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.5132 - val_loss: 0.6964 - val_accuracy: 0.4916 Epoch 111/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6927 - accuracy: 0.5224 - val_loss: 0.6963 - val_accuracy: 0.4952 Epoch 112/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6935 - accuracy: 0.5080 - val_loss: 0.6976 - val_accuracy: 0.4972 Epoch 113/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6952 - accuracy: 0.5036 - val_loss: 0.6938 - val_accuracy: 0.4956 Epoch 114/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6944 - accuracy: 0.4964 - val_loss: 0.6938 - val_accuracy: 0.4980 Epoch 115/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.4900 - val_loss: 0.6963 - val_accuracy: 0.5040 Epoch 116/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6963 - accuracy: 0.4868 - val_loss: 0.6966 - val_accuracy: 0.5044 Epoch 117/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6966 - accuracy: 0.4960 - val_loss: 0.6944 - val_accuracy: 0.4992 Epoch 118/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6954 - accuracy: 0.4972 - val_loss: 0.6948 - val_accuracy: 0.5048 Epoch 119/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6956 - accuracy: 0.4964 - val_loss: 0.6938 - val_accuracy: 0.5048 Epoch 120/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6961 - accuracy: 0.4980 - val_loss: 0.6965 - val_accuracy: 0.4988 Epoch 121/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.5016 - val_loss: 0.6941 - val_accuracy: 0.5040 Epoch 122/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6946 - accuracy: 0.5012 - val_loss: 0.6941 - val_accuracy: 0.4944 Epoch 123/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.4932 - val_loss: 0.6953 - val_accuracy: 0.5024 Epoch 124/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5020 - val_loss: 0.6942 - val_accuracy: 0.4936 Epoch 125/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.4956 - val_loss: 0.6975 - val_accuracy: 0.4972 Epoch 126/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6956 - accuracy: 0.4936 - val_loss: 0.6952 - val_accuracy: 0.4888 Epoch 127/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6946 - accuracy: 0.5008 - val_loss: 0.6954 - val_accuracy: 0.4856 Epoch 128/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6932 - accuracy: 0.5124 - val_loss: 0.6959 - val_accuracy: 0.4892 Epoch 129/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5072 - val_loss: 0.6971 - val_accuracy: 0.4960 Epoch 130/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5052 - val_loss: 0.6967 - val_accuracy: 0.4984 Epoch 131/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.5012 - val_loss: 0.6962 - val_accuracy: 0.4948 Epoch 132/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6961 - accuracy: 0.5032 - val_loss: 0.6959 - val_accuracy: 0.4908 Epoch 133/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.4920 - val_loss: 0.6958 - val_accuracy: 0.4884 Epoch 134/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6935 - accuracy: 0.4932 - val_loss: 0.6975 - val_accuracy: 0.4980 Epoch 135/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.4996 - val_loss: 0.6961 - val_accuracy: 0.4956 Epoch 136/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.4968 - val_loss: 0.6952 - val_accuracy: 0.5000 Epoch 137/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.5248 - val_loss: 0.6963 - val_accuracy: 0.4888 Epoch 138/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.5112 - val_loss: 0.6959 - val_accuracy: 0.4904 Epoch 139/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.4996 - val_loss: 0.6970 - val_accuracy: 0.4996 Epoch 140/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6949 - accuracy: 0.5024 - val_loss: 0.6956 - val_accuracy: 0.4960 Epoch 141/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6943 - accuracy: 0.5040 - val_loss: 0.6969 - val_accuracy: 0.4856 Epoch 142/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.5048 - val_loss: 0.6966 - val_accuracy: 0.4812 Epoch 143/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6971 - val_accuracy: 0.4892 Epoch 144/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5196 - val_loss: 0.6970 - val_accuracy: 0.4808 Epoch 145/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6934 - accuracy: 0.5040 - val_loss: 0.6967 - val_accuracy: 0.4896 Epoch 146/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6932 - accuracy: 0.5024 - val_loss: 0.6959 - val_accuracy: 0.4872 Epoch 147/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.5028 - val_loss: 0.6958 - val_accuracy: 0.4972 Epoch 148/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5140 - val_loss: 0.6971 - val_accuracy: 0.4856 Epoch 149/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6933 - accuracy: 0.5088 - val_loss: 0.6978 - val_accuracy: 0.5008 Epoch 150/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.5036 - val_loss: 0.6964 - val_accuracy: 0.4940 Epoch 151/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6949 - accuracy: 0.4984 - val_loss: 0.6959 - val_accuracy: 0.4832 Epoch 152/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6925 - accuracy: 0.5108 - val_loss: 0.6976 - val_accuracy: 0.4940 Epoch 153/200 79/79 [==============================] - 1s 17ms/step - loss: 0.6931 - accuracy: 0.5144 - val_loss: 0.6967 - val_accuracy: 0.4924 Epoch 154/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5168 - val_loss: 0.6970 - val_accuracy: 0.4916 Epoch 155/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5156 - val_loss: 0.6972 - val_accuracy: 0.4924 Epoch 156/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5200 - val_loss: 0.6979 - val_accuracy: 0.4812 Epoch 157/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6928 - accuracy: 0.4996 - val_loss: 0.7001 - val_accuracy: 0.4912 Epoch 158/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6921 - accuracy: 0.5140 - val_loss: 0.7003 - val_accuracy: 0.4912 Epoch 159/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5104 - val_loss: 0.6973 - val_accuracy: 0.4984 Epoch 160/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5108 - val_loss: 0.6975 - val_accuracy: 0.4776 Epoch 161/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6968 - val_accuracy: 0.4948 Epoch 162/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5172 - val_loss: 0.6981 - val_accuracy: 0.4848 Epoch 163/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6944 - accuracy: 0.5112 - val_loss: 0.6986 - val_accuracy: 0.4816 Epoch 164/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6941 - accuracy: 0.5028 - val_loss: 0.6976 - val_accuracy: 0.4792 Epoch 165/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5188 - val_loss: 0.6971 - val_accuracy: 0.4892 Epoch 166/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5132 - val_loss: 0.6994 - val_accuracy: 0.4840 Epoch 167/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6980 - val_accuracy: 0.4964 Epoch 168/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6951 - accuracy: 0.5156 - val_loss: 0.6974 - val_accuracy: 0.4840 Epoch 169/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5172 - val_loss: 0.6975 - val_accuracy: 0.4836 Epoch 170/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5128 - val_loss: 0.6982 - val_accuracy: 0.4820 Epoch 171/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.5176 - val_loss: 0.6980 - val_accuracy: 0.4812 Epoch 172/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6936 - accuracy: 0.4972 - val_loss: 0.6972 - val_accuracy: 0.4904 Epoch 173/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5036 - val_loss: 0.6981 - val_accuracy: 0.4964 Epoch 174/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6917 - accuracy: 0.5152 - val_loss: 0.6992 - val_accuracy: 0.4964 Epoch 175/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6977 - val_accuracy: 0.4880 Epoch 176/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5144 - val_loss: 0.6969 - val_accuracy: 0.4976 Epoch 177/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6922 - accuracy: 0.5112 - val_loss: 0.6981 - val_accuracy: 0.4848 Epoch 178/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6934 - accuracy: 0.5112 - val_loss: 0.6968 - val_accuracy: 0.4888 Epoch 179/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6918 - accuracy: 0.5084 - val_loss: 0.6981 - val_accuracy: 0.5016 Epoch 180/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6919 - accuracy: 0.5268 - val_loss: 0.7017 - val_accuracy: 0.4892 Epoch 181/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5192 - val_loss: 0.6972 - val_accuracy: 0.4888 Epoch 182/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5052 - val_loss: 0.6991 - val_accuracy: 0.4936 Epoch 183/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5052 - val_loss: 0.6970 - val_accuracy: 0.4960 Epoch 184/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6941 - accuracy: 0.5016 - val_loss: 0.6965 - val_accuracy: 0.4872 Epoch 185/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5092 - val_loss: 0.6961 - val_accuracy: 0.4856 Epoch 186/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6934 - accuracy: 0.5116 - val_loss: 0.6959 - val_accuracy: 0.4956 Epoch 187/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5108 - val_loss: 0.6976 - val_accuracy: 0.4960 Epoch 188/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6932 - accuracy: 0.5152 - val_loss: 0.6958 - val_accuracy: 0.4932 Epoch 189/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5168 - val_loss: 0.6961 - val_accuracy: 0.4928 Epoch 190/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6965 - val_accuracy: 0.4960 Epoch 191/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6930 - accuracy: 0.5140 - val_loss: 0.7008 - val_accuracy: 0.4932 Epoch 192/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6973 - accuracy: 0.4968 - val_loss: 0.6977 - val_accuracy: 0.4908 Epoch 193/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6942 - accuracy: 0.5104 - val_loss: 0.6978 - val_accuracy: 0.4908 Epoch 194/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.5052 - val_loss: 0.6965 - val_accuracy: 0.4872 Epoch 195/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6938 - accuracy: 0.4968 - val_loss: 0.6968 - val_accuracy: 0.4868 Epoch 196/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6923 - accuracy: 0.5156 - val_loss: 0.6972 - val_accuracy: 0.4936 Epoch 197/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6940 - accuracy: 0.5112 - val_loss: 0.6971 - val_accuracy: 0.4964 Epoch 198/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5036 - val_loss: 0.6979 - val_accuracy: 0.4952 Epoch 199/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5272 - val_loss: 0.6964 - val_accuracy: 0.4904 Epoch 200/200 79/79 [==============================] - 1s 16ms/step - loss: 0.6920 - accuracy: 0.5256 - val_loss: 0.6953 - val_accuracy: 0.4952 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec382d6550> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec260c87f0> # Now test our LSTM inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = LSTM ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 200 , validation_split = 0.5 , ) Epoch 1/200 79/79 [==============================] - 1s 10ms/step - loss: 0.6950 - accuracy: 0.5128 - val_loss: 0.6957 - val_accuracy: 0.4920 Epoch 2/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5152 - val_loss: 0.6974 - val_accuracy: 0.4920 Epoch 3/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6929 - accuracy: 0.5192 - val_loss: 0.6955 - val_accuracy: 0.4916 Epoch 4/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6924 - accuracy: 0.5248 - val_loss: 0.6950 - val_accuracy: 0.4952 Epoch 5/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6917 - accuracy: 0.5232 - val_loss: 0.6951 - val_accuracy: 0.4948 Epoch 6/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6910 - accuracy: 0.5232 - val_loss: 0.6979 - val_accuracy: 0.4968 Epoch 7/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6915 - accuracy: 0.5240 - val_loss: 0.6976 - val_accuracy: 0.4952 Epoch 8/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6915 - accuracy: 0.5228 - val_loss: 0.6959 - val_accuracy: 0.4944 Epoch 9/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6921 - accuracy: 0.5220 - val_loss: 0.6959 - val_accuracy: 0.4952 Epoch 10/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6917 - accuracy: 0.5212 - val_loss: 0.6945 - val_accuracy: 0.4984 Epoch 11/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6928 - accuracy: 0.5288 - val_loss: 0.6964 - val_accuracy: 0.4948 Epoch 12/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6913 - accuracy: 0.5256 - val_loss: 0.6964 - val_accuracy: 0.4980 Epoch 13/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6908 - accuracy: 0.5212 - val_loss: 0.6970 - val_accuracy: 0.4976 Epoch 14/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6911 - accuracy: 0.5296 - val_loss: 0.6945 - val_accuracy: 0.5048 Epoch 15/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6913 - accuracy: 0.5344 - val_loss: 0.6958 - val_accuracy: 0.4952 Epoch 16/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5260 - val_loss: 0.6966 - val_accuracy: 0.5020 Epoch 17/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6903 - accuracy: 0.5264 - val_loss: 0.6979 - val_accuracy: 0.4964 Epoch 18/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6911 - accuracy: 0.5236 - val_loss: 0.6947 - val_accuracy: 0.5028 Epoch 19/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6910 - accuracy: 0.5280 - val_loss: 0.6941 - val_accuracy: 0.5076 Epoch 20/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6895 - accuracy: 0.5248 - val_loss: 0.6970 - val_accuracy: 0.4944 Epoch 21/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6906 - accuracy: 0.5232 - val_loss: 0.6933 - val_accuracy: 0.5184 Epoch 22/200 79/79 [==============================] - 1s 9ms/step - loss: 0.6915 - accuracy: 0.5300 - val_loss: 0.6946 - val_accuracy: 0.5040 Epoch 23/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6897 - accuracy: 0.5268 - val_loss: 0.6984 - val_accuracy: 0.4928 Epoch 24/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6902 - accuracy: 0.5220 - val_loss: 0.6979 - val_accuracy: 0.4928 Epoch 25/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6906 - accuracy: 0.5356 - val_loss: 0.6967 - val_accuracy: 0.4996 Epoch 26/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6892 - accuracy: 0.5260 - val_loss: 0.6973 - val_accuracy: 0.5056 Epoch 27/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6907 - accuracy: 0.5264 - val_loss: 0.6952 - val_accuracy: 0.5012 Epoch 28/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6888 - accuracy: 0.5312 - val_loss: 0.6958 - val_accuracy: 0.5036 Epoch 29/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6908 - accuracy: 0.5228 - val_loss: 0.6958 - val_accuracy: 0.4976 Epoch 30/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6903 - accuracy: 0.5180 - val_loss: 0.6970 - val_accuracy: 0.5072 Epoch 31/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6903 - accuracy: 0.5264 - val_loss: 0.6950 - val_accuracy: 0.5020 Epoch 32/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6912 - accuracy: 0.5392 - val_loss: 0.6963 - val_accuracy: 0.4960 Epoch 33/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6904 - accuracy: 0.5352 - val_loss: 0.6961 - val_accuracy: 0.4952 Epoch 34/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.5300 - val_loss: 0.6978 - val_accuracy: 0.4980 Epoch 35/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6911 - accuracy: 0.5424 - val_loss: 0.6949 - val_accuracy: 0.4928 Epoch 36/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6895 - accuracy: 0.5336 - val_loss: 0.6994 - val_accuracy: 0.4976 Epoch 37/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6898 - accuracy: 0.5392 - val_loss: 0.7006 - val_accuracy: 0.4932 Epoch 38/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6908 - accuracy: 0.5372 - val_loss: 0.6993 - val_accuracy: 0.4900 Epoch 39/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6898 - accuracy: 0.5380 - val_loss: 0.6987 - val_accuracy: 0.4912 Epoch 40/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6903 - accuracy: 0.5412 - val_loss: 0.6995 - val_accuracy: 0.4960 Epoch 41/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6909 - accuracy: 0.5336 - val_loss: 0.6954 - val_accuracy: 0.5040 Epoch 42/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6917 - accuracy: 0.5352 - val_loss: 0.7014 - val_accuracy: 0.4920 Epoch 43/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6893 - accuracy: 0.5476 - val_loss: 0.6988 - val_accuracy: 0.4920 Epoch 44/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6890 - accuracy: 0.5420 - val_loss: 0.7003 - val_accuracy: 0.4876 Epoch 45/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6883 - accuracy: 0.5484 - val_loss: 0.7000 - val_accuracy: 0.4900 Epoch 46/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6871 - accuracy: 0.5492 - val_loss: 0.7006 - val_accuracy: 0.4912 Epoch 47/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6870 - accuracy: 0.5580 - val_loss: 0.7034 - val_accuracy: 0.4868 Epoch 48/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6869 - accuracy: 0.5492 - val_loss: 0.7030 - val_accuracy: 0.4868 Epoch 49/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6855 - accuracy: 0.5504 - val_loss: 0.7034 - val_accuracy: 0.4988 Epoch 50/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6852 - accuracy: 0.5448 - val_loss: 0.7031 - val_accuracy: 0.4992 Epoch 51/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6874 - accuracy: 0.5544 - val_loss: 0.7023 - val_accuracy: 0.4940 Epoch 52/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6857 - accuracy: 0.5536 - val_loss: 0.7048 - val_accuracy: 0.4868 Epoch 53/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6847 - accuracy: 0.5608 - val_loss: 0.7034 - val_accuracy: 0.4828 Epoch 54/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6851 - accuracy: 0.5548 - val_loss: 0.7062 - val_accuracy: 0.4940 Epoch 55/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6853 - accuracy: 0.5564 - val_loss: 0.7041 - val_accuracy: 0.4964 Epoch 56/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6812 - accuracy: 0.5648 - val_loss: 0.7075 - val_accuracy: 0.4884 Epoch 57/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6835 - accuracy: 0.5684 - val_loss: 0.7108 - val_accuracy: 0.4896 Epoch 58/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6863 - accuracy: 0.5604 - val_loss: 0.7074 - val_accuracy: 0.4996 Epoch 59/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6879 - accuracy: 0.5484 - val_loss: 0.7055 - val_accuracy: 0.4880 Epoch 60/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6865 - accuracy: 0.5476 - val_loss: 0.7043 - val_accuracy: 0.4912 Epoch 61/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6833 - accuracy: 0.5628 - val_loss: 0.7042 - val_accuracy: 0.4972 Epoch 62/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6821 - accuracy: 0.5628 - val_loss: 0.7035 - val_accuracy: 0.4888 Epoch 63/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6825 - accuracy: 0.5600 - val_loss: 0.7084 - val_accuracy: 0.4936 Epoch 64/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6828 - accuracy: 0.5652 - val_loss: 0.7069 - val_accuracy: 0.4880 Epoch 65/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6809 - accuracy: 0.5664 - val_loss: 0.7080 - val_accuracy: 0.4972 Epoch 66/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6824 - accuracy: 0.5536 - val_loss: 0.7075 - val_accuracy: 0.4940 Epoch 67/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6819 - accuracy: 0.5640 - val_loss: 0.7092 - val_accuracy: 0.4956 Epoch 68/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6825 - accuracy: 0.5644 - val_loss: 0.7102 - val_accuracy: 0.5000 Epoch 69/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6798 - accuracy: 0.5692 - val_loss: 0.7109 - val_accuracy: 0.4916 Epoch 70/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6840 - accuracy: 0.5624 - val_loss: 0.7090 - val_accuracy: 0.5016 Epoch 71/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6847 - accuracy: 0.5580 - val_loss: 0.7095 - val_accuracy: 0.4928 Epoch 72/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6828 - accuracy: 0.5640 - val_loss: 0.7064 - val_accuracy: 0.4980 Epoch 73/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6808 - accuracy: 0.5680 - val_loss: 0.7112 - val_accuracy: 0.4912 Epoch 74/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6827 - accuracy: 0.5624 - val_loss: 0.7086 - val_accuracy: 0.4960 Epoch 75/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.5500 - val_loss: 0.7074 - val_accuracy: 0.4996 Epoch 76/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6852 - accuracy: 0.5508 - val_loss: 0.7075 - val_accuracy: 0.4856 Epoch 77/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6807 - accuracy: 0.5648 - val_loss: 0.7109 - val_accuracy: 0.4880 Epoch 78/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6812 - accuracy: 0.5656 - val_loss: 0.7070 - val_accuracy: 0.4960 Epoch 79/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6792 - accuracy: 0.5696 - val_loss: 0.7127 - val_accuracy: 0.4900 Epoch 80/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6843 - accuracy: 0.5616 - val_loss: 0.7108 - val_accuracy: 0.5008 Epoch 81/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6813 - accuracy: 0.5672 - val_loss: 0.7063 - val_accuracy: 0.5052 Epoch 82/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6851 - accuracy: 0.5524 - val_loss: 0.7084 - val_accuracy: 0.4928 Epoch 83/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6839 - accuracy: 0.5564 - val_loss: 0.7019 - val_accuracy: 0.4904 Epoch 84/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6914 - accuracy: 0.5332 - val_loss: 0.6978 - val_accuracy: 0.4976 Epoch 85/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.5312 - val_loss: 0.6962 - val_accuracy: 0.4892 Epoch 86/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6908 - accuracy: 0.5304 - val_loss: 0.6967 - val_accuracy: 0.4964 Epoch 87/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6916 - accuracy: 0.5272 - val_loss: 0.7025 - val_accuracy: 0.4876 Epoch 88/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6898 - accuracy: 0.5372 - val_loss: 0.7023 - val_accuracy: 0.4896 Epoch 89/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6843 - accuracy: 0.5424 - val_loss: 0.7034 - val_accuracy: 0.4896 Epoch 90/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6820 - accuracy: 0.5524 - val_loss: 0.7054 - val_accuracy: 0.4904 Epoch 91/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6838 - accuracy: 0.5628 - val_loss: 0.7118 - val_accuracy: 0.4940 Epoch 92/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6814 - accuracy: 0.5612 - val_loss: 0.7041 - val_accuracy: 0.4948 Epoch 93/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6815 - accuracy: 0.5548 - val_loss: 0.7158 - val_accuracy: 0.4928 Epoch 94/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6828 - accuracy: 0.5540 - val_loss: 0.7145 - val_accuracy: 0.4968 Epoch 95/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6817 - accuracy: 0.5660 - val_loss: 0.7119 - val_accuracy: 0.4988 Epoch 96/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6814 - accuracy: 0.5616 - val_loss: 0.7085 - val_accuracy: 0.4924 Epoch 97/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6783 - accuracy: 0.5660 - val_loss: 0.7101 - val_accuracy: 0.4836 Epoch 98/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6785 - accuracy: 0.5740 - val_loss: 0.7114 - val_accuracy: 0.4920 Epoch 99/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6781 - accuracy: 0.5732 - val_loss: 0.7108 - val_accuracy: 0.5024 Epoch 100/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6798 - accuracy: 0.5660 - val_loss: 0.7075 - val_accuracy: 0.5036 Epoch 101/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6772 - accuracy: 0.5716 - val_loss: 0.7097 - val_accuracy: 0.5076 Epoch 102/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6769 - accuracy: 0.5672 - val_loss: 0.7078 - val_accuracy: 0.4948 Epoch 103/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6770 - accuracy: 0.5756 - val_loss: 0.7110 - val_accuracy: 0.4924 Epoch 104/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6851 - accuracy: 0.5656 - val_loss: 0.7182 - val_accuracy: 0.4900 Epoch 105/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6867 - accuracy: 0.5504 - val_loss: 0.7089 - val_accuracy: 0.4904 Epoch 106/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6828 - accuracy: 0.5616 - val_loss: 0.7127 - val_accuracy: 0.4940 Epoch 107/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6832 - accuracy: 0.5604 - val_loss: 0.7105 - val_accuracy: 0.4936 Epoch 108/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6829 - accuracy: 0.5648 - val_loss: 0.7085 - val_accuracy: 0.5072 Epoch 109/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6809 - accuracy: 0.5544 - val_loss: 0.7111 - val_accuracy: 0.4932 Epoch 110/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6786 - accuracy: 0.5724 - val_loss: 0.7088 - val_accuracy: 0.5012 Epoch 111/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6811 - accuracy: 0.5664 - val_loss: 0.7108 - val_accuracy: 0.5048 Epoch 112/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6837 - accuracy: 0.5660 - val_loss: 0.7079 - val_accuracy: 0.4864 Epoch 113/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6769 - accuracy: 0.5636 - val_loss: 0.7140 - val_accuracy: 0.4952 Epoch 114/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6745 - accuracy: 0.5728 - val_loss: 0.7131 - val_accuracy: 0.4852 Epoch 115/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6784 - accuracy: 0.5720 - val_loss: 0.7142 - val_accuracy: 0.4932 Epoch 116/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6758 - accuracy: 0.5760 - val_loss: 0.7186 - val_accuracy: 0.4848 Epoch 117/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6799 - accuracy: 0.5664 - val_loss: 0.7121 - val_accuracy: 0.5036 Epoch 118/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6773 - accuracy: 0.5712 - val_loss: 0.7136 - val_accuracy: 0.4928 Epoch 119/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6805 - accuracy: 0.5564 - val_loss: 0.7085 - val_accuracy: 0.4932 Epoch 120/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6805 - accuracy: 0.5696 - val_loss: 0.7131 - val_accuracy: 0.4940 Epoch 121/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6741 - accuracy: 0.5768 - val_loss: 0.7176 - val_accuracy: 0.4932 Epoch 122/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6787 - accuracy: 0.5628 - val_loss: 0.7120 - val_accuracy: 0.5040 Epoch 123/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6794 - accuracy: 0.5704 - val_loss: 0.7113 - val_accuracy: 0.4804 Epoch 124/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6757 - accuracy: 0.5660 - val_loss: 0.7138 - val_accuracy: 0.4992 Epoch 125/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6780 - accuracy: 0.5732 - val_loss: 0.7164 - val_accuracy: 0.4828 Epoch 126/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6807 - accuracy: 0.5684 - val_loss: 0.7191 - val_accuracy: 0.5000 Epoch 127/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6875 - accuracy: 0.5444 - val_loss: 0.7013 - val_accuracy: 0.4972 Epoch 128/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6853 - accuracy: 0.5520 - val_loss: 0.7029 - val_accuracy: 0.4948 Epoch 129/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6843 - accuracy: 0.5580 - val_loss: 0.7058 - val_accuracy: 0.4928 Epoch 130/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6816 - accuracy: 0.5588 - val_loss: 0.7081 - val_accuracy: 0.4976 Epoch 131/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6789 - accuracy: 0.5708 - val_loss: 0.7078 - val_accuracy: 0.4968 Epoch 132/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6753 - accuracy: 0.5728 - val_loss: 0.7179 - val_accuracy: 0.5016 Epoch 133/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6739 - accuracy: 0.5764 - val_loss: 0.7149 - val_accuracy: 0.4844 Epoch 134/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6778 - accuracy: 0.5736 - val_loss: 0.7147 - val_accuracy: 0.4948 Epoch 135/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6738 - accuracy: 0.5780 - val_loss: 0.7169 - val_accuracy: 0.4920 Epoch 136/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6772 - accuracy: 0.5724 - val_loss: 0.7114 - val_accuracy: 0.4948 Epoch 137/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6768 - accuracy: 0.5712 - val_loss: 0.7168 - val_accuracy: 0.4920 Epoch 138/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6744 - accuracy: 0.5664 - val_loss: 0.7164 - val_accuracy: 0.4912 Epoch 139/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6806 - accuracy: 0.5696 - val_loss: 0.7174 - val_accuracy: 0.4876 Epoch 140/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6798 - accuracy: 0.5688 - val_loss: 0.7134 - val_accuracy: 0.4912 Epoch 141/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6789 - accuracy: 0.5668 - val_loss: 0.7169 - val_accuracy: 0.4884 Epoch 142/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6832 - accuracy: 0.5728 - val_loss: 0.7132 - val_accuracy: 0.4864 Epoch 143/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6819 - accuracy: 0.5568 - val_loss: 0.7127 - val_accuracy: 0.4908 Epoch 144/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6875 - accuracy: 0.5420 - val_loss: 0.7054 - val_accuracy: 0.4872 Epoch 145/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6829 - accuracy: 0.5488 - val_loss: 0.7082 - val_accuracy: 0.4840 Epoch 146/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6781 - accuracy: 0.5668 - val_loss: 0.7158 - val_accuracy: 0.4892 Epoch 147/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6786 - accuracy: 0.5696 - val_loss: 0.7123 - val_accuracy: 0.4840 Epoch 148/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6804 - accuracy: 0.5632 - val_loss: 0.7165 - val_accuracy: 0.4936 Epoch 149/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6764 - accuracy: 0.5764 - val_loss: 0.7140 - val_accuracy: 0.4896 Epoch 150/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6780 - accuracy: 0.5700 - val_loss: 0.7158 - val_accuracy: 0.4940 Epoch 151/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6779 - accuracy: 0.5720 - val_loss: 0.7150 - val_accuracy: 0.4940 Epoch 152/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6754 - accuracy: 0.5696 - val_loss: 0.7141 - val_accuracy: 0.4920 Epoch 153/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6744 - accuracy: 0.5748 - val_loss: 0.7144 - val_accuracy: 0.4956 Epoch 154/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6754 - accuracy: 0.5728 - val_loss: 0.7138 - val_accuracy: 0.4996 Epoch 155/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6763 - accuracy: 0.5736 - val_loss: 0.7185 - val_accuracy: 0.4948 Epoch 156/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6787 - accuracy: 0.5664 - val_loss: 0.7184 - val_accuracy: 0.4964 Epoch 157/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6756 - accuracy: 0.5700 - val_loss: 0.7147 - val_accuracy: 0.4976 Epoch 158/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6770 - accuracy: 0.5720 - val_loss: 0.7144 - val_accuracy: 0.5036 Epoch 159/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6740 - accuracy: 0.5744 - val_loss: 0.7171 - val_accuracy: 0.4924 Epoch 160/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6726 - accuracy: 0.5796 - val_loss: 0.7143 - val_accuracy: 0.5036 Epoch 161/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6801 - accuracy: 0.5696 - val_loss: 0.7124 - val_accuracy: 0.5004 Epoch 162/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6783 - accuracy: 0.5664 - val_loss: 0.7123 - val_accuracy: 0.4916 Epoch 163/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6739 - accuracy: 0.5740 - val_loss: 0.7151 - val_accuracy: 0.5008 Epoch 164/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6759 - accuracy: 0.5708 - val_loss: 0.7175 - val_accuracy: 0.4952 Epoch 165/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6760 - accuracy: 0.5756 - val_loss: 0.7178 - val_accuracy: 0.4952 Epoch 166/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6767 - accuracy: 0.5712 - val_loss: 0.7180 - val_accuracy: 0.4896 Epoch 167/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6779 - accuracy: 0.5812 - val_loss: 0.7132 - val_accuracy: 0.5048 Epoch 168/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6794 - accuracy: 0.5560 - val_loss: 0.7107 - val_accuracy: 0.4932 Epoch 169/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6743 - accuracy: 0.5752 - val_loss: 0.7113 - val_accuracy: 0.4936 Epoch 170/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6805 - accuracy: 0.5592 - val_loss: 0.7030 - val_accuracy: 0.4968 Epoch 171/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6835 - accuracy: 0.5532 - val_loss: 0.7062 - val_accuracy: 0.4908 Epoch 172/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6796 - accuracy: 0.5536 - val_loss: 0.7065 - val_accuracy: 0.4956 Epoch 173/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6781 - accuracy: 0.5632 - val_loss: 0.7105 - val_accuracy: 0.4988 Epoch 174/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6777 - accuracy: 0.5668 - val_loss: 0.7129 - val_accuracy: 0.4916 Epoch 175/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6781 - accuracy: 0.5560 - val_loss: 0.7157 - val_accuracy: 0.4956 Epoch 176/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6735 - accuracy: 0.5756 - val_loss: 0.7128 - val_accuracy: 0.4848 Epoch 177/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6764 - accuracy: 0.5776 - val_loss: 0.7170 - val_accuracy: 0.4972 Epoch 178/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6800 - accuracy: 0.5668 - val_loss: 0.7169 - val_accuracy: 0.4964 Epoch 179/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6730 - accuracy: 0.5804 - val_loss: 0.7211 - val_accuracy: 0.4944 Epoch 180/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6750 - accuracy: 0.5796 - val_loss: 0.7169 - val_accuracy: 0.4928 Epoch 181/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6766 - accuracy: 0.5712 - val_loss: 0.7121 - val_accuracy: 0.4884 Epoch 182/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6724 - accuracy: 0.5820 - val_loss: 0.7210 - val_accuracy: 0.4832 Epoch 183/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6677 - accuracy: 0.5844 - val_loss: 0.7196 - val_accuracy: 0.4900 Epoch 184/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6702 - accuracy: 0.5776 - val_loss: 0.7210 - val_accuracy: 0.4912 Epoch 185/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6717 - accuracy: 0.5808 - val_loss: 0.7222 - val_accuracy: 0.4828 Epoch 186/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6698 - accuracy: 0.5836 - val_loss: 0.7229 - val_accuracy: 0.5016 Epoch 187/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6737 - accuracy: 0.5796 - val_loss: 0.7133 - val_accuracy: 0.4928 Epoch 188/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6708 - accuracy: 0.5784 - val_loss: 0.7147 - val_accuracy: 0.4884 Epoch 189/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6691 - accuracy: 0.5688 - val_loss: 0.7199 - val_accuracy: 0.4948 Epoch 190/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6674 - accuracy: 0.5800 - val_loss: 0.7200 - val_accuracy: 0.4912 Epoch 191/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6714 - accuracy: 0.5800 - val_loss: 0.7199 - val_accuracy: 0.4880 Epoch 192/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6718 - accuracy: 0.5776 - val_loss: 0.7248 - val_accuracy: 0.4900 Epoch 193/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6680 - accuracy: 0.5828 - val_loss: 0.7194 - val_accuracy: 0.4928 Epoch 194/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6706 - accuracy: 0.5704 - val_loss: 0.7193 - val_accuracy: 0.4980 Epoch 195/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6704 - accuracy: 0.5712 - val_loss: 0.7215 - val_accuracy: 0.4984 Epoch 196/200 79/79 [==============================] - 1s 7ms/step - loss: 0.6731 - accuracy: 0.5708 - val_loss: 0.7238 - val_accuracy: 0.4948 Epoch 197/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6682 - accuracy: 0.5788 - val_loss: 0.7185 - val_accuracy: 0.4968 Epoch 198/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6705 - accuracy: 0.5724 - val_loss: 0.7211 - val_accuracy: 0.4916 Epoch 199/200 79/79 [==============================] - 0s 6ms/step - loss: 0.6686 - accuracy: 0.5800 - val_loss: 0.7203 - val_accuracy: 0.5032 Epoch 200/200 79/79 [==============================] - 1s 6ms/step - loss: 0.6706 - accuracy: 0.5860 - val_loss: 0.7207 - val_accuracy: 0.5036 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1fc08f60> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1f79cc50> # Now test our GRU inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = GRU ( 5 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 400 , validation_split = 0.5 , ) Epoch 1/400 79/79 [==============================] - 1s 9ms/step - loss: 0.6950 - accuracy: 0.4860 - val_loss: 0.6950 - val_accuracy: 0.4904 Epoch 2/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6942 - accuracy: 0.5080 - val_loss: 0.6976 - val_accuracy: 0.4940 Epoch 3/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6943 - accuracy: 0.5056 - val_loss: 0.6958 - val_accuracy: 0.4956 Epoch 4/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.4996 - val_loss: 0.6962 - val_accuracy: 0.4996 Epoch 5/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5180 - val_loss: 0.6959 - val_accuracy: 0.4964 Epoch 6/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.4900 - val_loss: 0.6958 - val_accuracy: 0.4916 Epoch 7/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.4936 - val_loss: 0.6952 - val_accuracy: 0.4928 Epoch 8/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.4980 - val_loss: 0.6952 - val_accuracy: 0.4920 Epoch 9/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.4936 - val_loss: 0.6950 - val_accuracy: 0.4868 Epoch 10/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5080 - val_loss: 0.6955 - val_accuracy: 0.4892 Epoch 11/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.4944 - val_loss: 0.6948 - val_accuracy: 0.4908 Epoch 12/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5056 - val_loss: 0.6948 - val_accuracy: 0.4904 Epoch 13/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5032 - val_loss: 0.6946 - val_accuracy: 0.4848 Epoch 14/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5016 - val_loss: 0.6954 - val_accuracy: 0.4940 Epoch 15/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6945 - val_accuracy: 0.4880 Epoch 16/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5044 - val_loss: 0.6948 - val_accuracy: 0.4868 Epoch 17/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.4896 - val_loss: 0.6953 - val_accuracy: 0.4852 Epoch 18/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5164 - val_loss: 0.6951 - val_accuracy: 0.4920 Epoch 19/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5092 - val_loss: 0.6952 - val_accuracy: 0.4888 Epoch 20/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5172 - val_loss: 0.6958 - val_accuracy: 0.4904 Epoch 21/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5132 - val_loss: 0.6953 - val_accuracy: 0.4876 Epoch 22/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5124 - val_loss: 0.6958 - val_accuracy: 0.4812 Epoch 23/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5084 - val_loss: 0.6968 - val_accuracy: 0.4844 Epoch 24/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6979 - val_accuracy: 0.4936 Epoch 25/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5180 - val_loss: 0.6977 - val_accuracy: 0.4884 Epoch 26/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5224 - val_loss: 0.6965 - val_accuracy: 0.5000 Epoch 27/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.5252 - val_loss: 0.6994 - val_accuracy: 0.4940 Epoch 28/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5348 - val_loss: 0.6968 - val_accuracy: 0.4888 Epoch 29/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6899 - accuracy: 0.5288 - val_loss: 0.6997 - val_accuracy: 0.4904 Epoch 30/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6894 - accuracy: 0.5280 - val_loss: 0.7009 - val_accuracy: 0.4960 Epoch 31/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.5288 - val_loss: 0.7015 - val_accuracy: 0.4860 Epoch 32/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6881 - accuracy: 0.5376 - val_loss: 0.6992 - val_accuracy: 0.4892 Epoch 33/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6859 - accuracy: 0.5480 - val_loss: 0.7075 - val_accuracy: 0.4868 Epoch 34/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6895 - accuracy: 0.5420 - val_loss: 0.7045 - val_accuracy: 0.4872 Epoch 35/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.5432 - val_loss: 0.7005 - val_accuracy: 0.5000 Epoch 36/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6863 - accuracy: 0.5384 - val_loss: 0.7020 - val_accuracy: 0.4920 Epoch 37/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6852 - accuracy: 0.5436 - val_loss: 0.7076 - val_accuracy: 0.4964 Epoch 38/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6861 - accuracy: 0.5520 - val_loss: 0.7036 - val_accuracy: 0.4956 Epoch 39/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6876 - accuracy: 0.5412 - val_loss: 0.7031 - val_accuracy: 0.4976 Epoch 40/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6829 - accuracy: 0.5528 - val_loss: 0.7050 - val_accuracy: 0.4852 Epoch 41/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6808 - accuracy: 0.5508 - val_loss: 0.7074 - val_accuracy: 0.4924 Epoch 42/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6825 - accuracy: 0.5540 - val_loss: 0.7150 - val_accuracy: 0.4892 Epoch 43/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6826 - accuracy: 0.5600 - val_loss: 0.7037 - val_accuracy: 0.4964 Epoch 44/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6846 - accuracy: 0.5484 - val_loss: 0.7099 - val_accuracy: 0.4920 Epoch 45/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6820 - accuracy: 0.5512 - val_loss: 0.7057 - val_accuracy: 0.4976 Epoch 46/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6797 - accuracy: 0.5572 - val_loss: 0.7067 - val_accuracy: 0.5048 Epoch 47/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6816 - accuracy: 0.5552 - val_loss: 0.7072 - val_accuracy: 0.4924 Epoch 48/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6789 - accuracy: 0.5652 - val_loss: 0.7108 - val_accuracy: 0.4888 Epoch 49/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5492 - val_loss: 0.7064 - val_accuracy: 0.4904 Epoch 50/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6853 - accuracy: 0.5500 - val_loss: 0.7052 - val_accuracy: 0.4992 Epoch 51/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6806 - accuracy: 0.5544 - val_loss: 0.7067 - val_accuracy: 0.4944 Epoch 52/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6794 - accuracy: 0.5600 - val_loss: 0.7082 - val_accuracy: 0.4916 Epoch 53/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6785 - accuracy: 0.5640 - val_loss: 0.7079 - val_accuracy: 0.4964 Epoch 54/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6822 - accuracy: 0.5588 - val_loss: 0.7126 - val_accuracy: 0.4868 Epoch 55/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6784 - accuracy: 0.5576 - val_loss: 0.7119 - val_accuracy: 0.4916 Epoch 56/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6746 - accuracy: 0.5680 - val_loss: 0.7108 - val_accuracy: 0.4888 Epoch 57/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6762 - accuracy: 0.5708 - val_loss: 0.7140 - val_accuracy: 0.4888 Epoch 58/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6775 - accuracy: 0.5628 - val_loss: 0.7142 - val_accuracy: 0.4888 Epoch 59/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6749 - accuracy: 0.5664 - val_loss: 0.7183 - val_accuracy: 0.4972 Epoch 60/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6804 - accuracy: 0.5608 - val_loss: 0.7118 - val_accuracy: 0.4996 Epoch 61/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6873 - accuracy: 0.5540 - val_loss: 0.7166 - val_accuracy: 0.4948 Epoch 62/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5248 - val_loss: 0.7052 - val_accuracy: 0.4804 Epoch 63/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5168 - val_loss: 0.7017 - val_accuracy: 0.4920 Epoch 64/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.5060 - val_loss: 0.6979 - val_accuracy: 0.4944 Epoch 65/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5208 - val_loss: 0.6999 - val_accuracy: 0.4868 Epoch 66/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6896 - accuracy: 0.5296 - val_loss: 0.6996 - val_accuracy: 0.5008 Epoch 67/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5228 - val_loss: 0.6984 - val_accuracy: 0.4740 Epoch 68/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.5020 - val_loss: 0.6944 - val_accuracy: 0.5024 Epoch 69/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6936 - accuracy: 0.5144 - val_loss: 0.6962 - val_accuracy: 0.4984 Epoch 70/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.5048 - val_loss: 0.6949 - val_accuracy: 0.5056 Epoch 71/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6938 - accuracy: 0.5096 - val_loss: 0.6965 - val_accuracy: 0.5000 Epoch 72/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6953 - accuracy: 0.4960 - val_loss: 0.6938 - val_accuracy: 0.5044 Epoch 73/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6940 - accuracy: 0.5128 - val_loss: 0.6938 - val_accuracy: 0.5092 Epoch 74/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5108 - val_loss: 0.6935 - val_accuracy: 0.5128 Epoch 75/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6938 - accuracy: 0.5036 - val_loss: 0.6929 - val_accuracy: 0.5064 Epoch 76/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6955 - accuracy: 0.4864 - val_loss: 0.6932 - val_accuracy: 0.5016 Epoch 77/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6949 - accuracy: 0.4744 - val_loss: 0.6938 - val_accuracy: 0.4960 Epoch 78/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.4928 - val_loss: 0.6940 - val_accuracy: 0.4960 Epoch 79/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6942 - accuracy: 0.5060 - val_loss: 0.6936 - val_accuracy: 0.5064 Epoch 80/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5132 - val_loss: 0.6932 - val_accuracy: 0.5036 Epoch 81/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5204 - val_loss: 0.6924 - val_accuracy: 0.5108 Epoch 82/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6910 - accuracy: 0.5316 - val_loss: 0.6921 - val_accuracy: 0.5192 Epoch 83/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6917 - accuracy: 0.5284 - val_loss: 0.6917 - val_accuracy: 0.5288 Epoch 84/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6909 - accuracy: 0.5368 - val_loss: 0.6894 - val_accuracy: 0.5384 Epoch 85/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6892 - accuracy: 0.5384 - val_loss: 0.6890 - val_accuracy: 0.5424 Epoch 86/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6922 - accuracy: 0.5216 - val_loss: 0.6940 - val_accuracy: 0.5144 Epoch 87/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6903 - accuracy: 0.5336 - val_loss: 0.6966 - val_accuracy: 0.5168 Epoch 88/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5236 - val_loss: 0.6966 - val_accuracy: 0.5088 Epoch 89/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5020 - val_loss: 0.6943 - val_accuracy: 0.5016 Epoch 90/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5084 - val_loss: 0.6949 - val_accuracy: 0.4908 Epoch 91/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.5104 - val_loss: 0.6941 - val_accuracy: 0.5068 Epoch 92/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5088 - val_loss: 0.6956 - val_accuracy: 0.4920 Epoch 93/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.5028 - val_loss: 0.6952 - val_accuracy: 0.4936 Epoch 94/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5036 - val_loss: 0.6936 - val_accuracy: 0.4920 Epoch 95/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.5124 - val_loss: 0.6935 - val_accuracy: 0.4964 Epoch 96/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5144 - val_loss: 0.6941 - val_accuracy: 0.5132 Epoch 97/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5132 - val_loss: 0.6943 - val_accuracy: 0.5096 Epoch 98/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5148 - val_loss: 0.6938 - val_accuracy: 0.5092 Epoch 99/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5180 - val_loss: 0.6938 - val_accuracy: 0.5140 Epoch 100/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5328 - val_loss: 0.6930 - val_accuracy: 0.5176 Epoch 101/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6908 - accuracy: 0.5344 - val_loss: 0.6937 - val_accuracy: 0.5240 Epoch 102/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.5324 - val_loss: 0.6922 - val_accuracy: 0.5236 Epoch 103/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6894 - accuracy: 0.5380 - val_loss: 0.6928 - val_accuracy: 0.5296 Epoch 104/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6916 - accuracy: 0.5328 - val_loss: 0.6922 - val_accuracy: 0.5356 Epoch 105/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.5488 - val_loss: 0.6903 - val_accuracy: 0.5352 Epoch 106/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6903 - accuracy: 0.5304 - val_loss: 0.6894 - val_accuracy: 0.5328 Epoch 107/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6894 - accuracy: 0.5364 - val_loss: 0.6925 - val_accuracy: 0.5300 Epoch 108/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6891 - accuracy: 0.5404 - val_loss: 0.6910 - val_accuracy: 0.5400 Epoch 109/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5204 - val_loss: 0.6921 - val_accuracy: 0.5204 Epoch 110/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.5324 - val_loss: 0.6913 - val_accuracy: 0.5344 Epoch 111/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6901 - accuracy: 0.5484 - val_loss: 0.6911 - val_accuracy: 0.5384 Epoch 112/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6895 - accuracy: 0.5476 - val_loss: 0.6919 - val_accuracy: 0.5260 Epoch 113/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6875 - accuracy: 0.5532 - val_loss: 0.6918 - val_accuracy: 0.5296 Epoch 114/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5484 - val_loss: 0.6921 - val_accuracy: 0.5380 Epoch 115/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6878 - accuracy: 0.5408 - val_loss: 0.6925 - val_accuracy: 0.5208 Epoch 116/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.5484 - val_loss: 0.6903 - val_accuracy: 0.5364 Epoch 117/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6853 - accuracy: 0.5528 - val_loss: 0.6894 - val_accuracy: 0.5372 Epoch 118/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.5688 - val_loss: 0.6915 - val_accuracy: 0.5312 Epoch 119/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6834 - accuracy: 0.5680 - val_loss: 0.6895 - val_accuracy: 0.5488 Epoch 120/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6807 - accuracy: 0.5732 - val_loss: 0.6923 - val_accuracy: 0.5396 Epoch 121/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6870 - accuracy: 0.5492 - val_loss: 0.6869 - val_accuracy: 0.5532 Epoch 122/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6868 - accuracy: 0.5484 - val_loss: 0.6855 - val_accuracy: 0.5600 Epoch 123/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5564 - val_loss: 0.6847 - val_accuracy: 0.5568 Epoch 124/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6831 - accuracy: 0.5676 - val_loss: 0.6837 - val_accuracy: 0.5624 Epoch 125/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.5664 - val_loss: 0.6830 - val_accuracy: 0.5680 Epoch 126/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6801 - accuracy: 0.5716 - val_loss: 0.6814 - val_accuracy: 0.5740 Epoch 127/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6792 - accuracy: 0.5772 - val_loss: 0.6786 - val_accuracy: 0.5792 Epoch 128/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6772 - accuracy: 0.5888 - val_loss: 0.6780 - val_accuracy: 0.5784 Epoch 129/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6759 - accuracy: 0.5888 - val_loss: 0.6759 - val_accuracy: 0.5772 Epoch 130/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6715 - accuracy: 0.5900 - val_loss: 0.6708 - val_accuracy: 0.5920 Epoch 131/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6653 - accuracy: 0.5992 - val_loss: 0.6661 - val_accuracy: 0.6016 Epoch 132/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6691 - accuracy: 0.5964 - val_loss: 0.6636 - val_accuracy: 0.6092 Epoch 133/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6598 - accuracy: 0.6020 - val_loss: 0.6593 - val_accuracy: 0.6076 Epoch 134/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6520 - accuracy: 0.6056 - val_loss: 0.6446 - val_accuracy: 0.6276 Epoch 135/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6388 - accuracy: 0.6184 - val_loss: 0.6400 - val_accuracy: 0.6132 Epoch 136/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6322 - accuracy: 0.6208 - val_loss: 0.6191 - val_accuracy: 0.6404 Epoch 137/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6021 - accuracy: 0.6452 - val_loss: 0.6042 - val_accuracy: 0.6336 Epoch 138/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5900 - accuracy: 0.6408 - val_loss: 0.5920 - val_accuracy: 0.6360 Epoch 139/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5946 - accuracy: 0.6268 - val_loss: 0.5867 - val_accuracy: 0.6284 Epoch 140/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5686 - accuracy: 0.6468 - val_loss: 0.5758 - val_accuracy: 0.6464 Epoch 141/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5621 - accuracy: 0.6496 - val_loss: 0.5707 - val_accuracy: 0.6364 Epoch 142/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5562 - accuracy: 0.6496 - val_loss: 0.5659 - val_accuracy: 0.6428 Epoch 143/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5560 - accuracy: 0.6516 - val_loss: 0.5618 - val_accuracy: 0.6484 Epoch 144/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5496 - accuracy: 0.6544 - val_loss: 0.5625 - val_accuracy: 0.6468 Epoch 145/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5465 - accuracy: 0.6568 - val_loss: 0.5568 - val_accuracy: 0.6544 Epoch 146/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5427 - accuracy: 0.6592 - val_loss: 0.5825 - val_accuracy: 0.6384 Epoch 147/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5469 - accuracy: 0.6588 - val_loss: 0.5518 - val_accuracy: 0.6524 Epoch 148/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5413 - accuracy: 0.6608 - val_loss: 0.5475 - val_accuracy: 0.6512 Epoch 149/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5380 - accuracy: 0.6660 - val_loss: 0.5513 - val_accuracy: 0.6480 Epoch 150/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5360 - accuracy: 0.6648 - val_loss: 0.5449 - val_accuracy: 0.6592 Epoch 151/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5345 - accuracy: 0.6608 - val_loss: 0.5473 - val_accuracy: 0.6616 Epoch 152/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5334 - accuracy: 0.6676 - val_loss: 0.5457 - val_accuracy: 0.6520 Epoch 153/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5287 - accuracy: 0.6740 - val_loss: 0.5434 - val_accuracy: 0.6572 Epoch 154/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5241 - accuracy: 0.6708 - val_loss: 0.5523 - val_accuracy: 0.6556 Epoch 155/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5290 - accuracy: 0.6728 - val_loss: 0.5411 - val_accuracy: 0.6656 Epoch 156/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5232 - accuracy: 0.6764 - val_loss: 0.5415 - val_accuracy: 0.6664 Epoch 157/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5294 - accuracy: 0.6868 - val_loss: 0.5385 - val_accuracy: 0.6556 Epoch 158/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5276 - accuracy: 0.6740 - val_loss: 0.5327 - val_accuracy: 0.6648 Epoch 159/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5212 - accuracy: 0.6784 - val_loss: 0.5280 - val_accuracy: 0.6612 Epoch 160/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5185 - accuracy: 0.6800 - val_loss: 0.5347 - val_accuracy: 0.6644 Epoch 161/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5164 - accuracy: 0.6816 - val_loss: 0.5133 - val_accuracy: 0.6800 Epoch 162/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5066 - accuracy: 0.6896 - val_loss: 0.5063 - val_accuracy: 0.6920 Epoch 163/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5099 - accuracy: 0.6888 - val_loss: 0.5053 - val_accuracy: 0.6976 Epoch 164/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4994 - accuracy: 0.6952 - val_loss: 0.5000 - val_accuracy: 0.7000 Epoch 165/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5113 - accuracy: 0.7108 - val_loss: 0.4923 - val_accuracy: 0.7084 Epoch 166/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5311 - accuracy: 0.6740 - val_loss: 0.5261 - val_accuracy: 0.6888 Epoch 167/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5154 - accuracy: 0.6836 - val_loss: 0.5097 - val_accuracy: 0.6908 Epoch 168/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5073 - accuracy: 0.6976 - val_loss: 0.5069 - val_accuracy: 0.7012 Epoch 169/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4980 - accuracy: 0.7064 - val_loss: 0.5059 - val_accuracy: 0.7060 Epoch 170/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4964 - accuracy: 0.7020 - val_loss: 0.4987 - val_accuracy: 0.7032 Epoch 171/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4890 - accuracy: 0.7100 - val_loss: 0.4979 - val_accuracy: 0.7080 Epoch 172/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4920 - accuracy: 0.7112 - val_loss: 0.4929 - val_accuracy: 0.7068 Epoch 173/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4945 - accuracy: 0.7172 - val_loss: 0.5323 - val_accuracy: 0.7124 Epoch 174/400 79/79 [==============================] - 0s 6ms/step - loss: 0.5101 - accuracy: 0.7104 - val_loss: 0.4942 - val_accuracy: 0.7080 Epoch 175/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4816 - accuracy: 0.7244 - val_loss: 0.4950 - val_accuracy: 0.7164 Epoch 176/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4822 - accuracy: 0.7224 - val_loss: 0.4951 - val_accuracy: 0.7048 Epoch 177/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4773 - accuracy: 0.7248 - val_loss: 0.4978 - val_accuracy: 0.7100 Epoch 178/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4834 - accuracy: 0.7220 - val_loss: 0.4915 - val_accuracy: 0.7148 Epoch 179/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4780 - accuracy: 0.7192 - val_loss: 0.4916 - val_accuracy: 0.7228 Epoch 180/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4803 - accuracy: 0.7252 - val_loss: 0.4898 - val_accuracy: 0.7164 Epoch 181/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4768 - accuracy: 0.7228 - val_loss: 0.4940 - val_accuracy: 0.7140 Epoch 182/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4748 - accuracy: 0.7256 - val_loss: 0.4916 - val_accuracy: 0.7168 Epoch 183/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4799 - accuracy: 0.7228 - val_loss: 0.4883 - val_accuracy: 0.7144 Epoch 184/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4753 - accuracy: 0.7268 - val_loss: 0.4849 - val_accuracy: 0.7172 Epoch 185/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4737 - accuracy: 0.7248 - val_loss: 0.4904 - val_accuracy: 0.7204 Epoch 186/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4682 - accuracy: 0.7288 - val_loss: 0.4914 - val_accuracy: 0.7136 Epoch 187/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4657 - accuracy: 0.7300 - val_loss: 0.4925 - val_accuracy: 0.7216 Epoch 188/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4694 - accuracy: 0.7284 - val_loss: 0.4925 - val_accuracy: 0.7148 Epoch 189/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4743 - accuracy: 0.7268 - val_loss: 0.4870 - val_accuracy: 0.7216 Epoch 190/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4759 - accuracy: 0.7336 - val_loss: 0.4797 - val_accuracy: 0.7240 Epoch 191/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4677 - accuracy: 0.7336 - val_loss: 0.4749 - val_accuracy: 0.7184 Epoch 192/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4645 - accuracy: 0.7320 - val_loss: 0.4803 - val_accuracy: 0.7260 Epoch 193/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4638 - accuracy: 0.7376 - val_loss: 0.4966 - val_accuracy: 0.7268 Epoch 194/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4659 - accuracy: 0.7340 - val_loss: 0.4818 - val_accuracy: 0.7308 Epoch 195/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4668 - accuracy: 0.7304 - val_loss: 0.4776 - val_accuracy: 0.7312 Epoch 196/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4604 - accuracy: 0.7356 - val_loss: 0.4731 - val_accuracy: 0.7304 Epoch 197/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4588 - accuracy: 0.7420 - val_loss: 0.4618 - val_accuracy: 0.7356 Epoch 198/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4508 - accuracy: 0.7452 - val_loss: 0.4548 - val_accuracy: 0.7432 Epoch 199/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4599 - accuracy: 0.7452 - val_loss: 0.4670 - val_accuracy: 0.7332 Epoch 200/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4463 - accuracy: 0.7540 - val_loss: 0.4577 - val_accuracy: 0.7496 Epoch 201/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4490 - accuracy: 0.7572 - val_loss: 0.4565 - val_accuracy: 0.7564 Epoch 202/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4400 - accuracy: 0.7680 - val_loss: 0.4427 - val_accuracy: 0.7744 Epoch 203/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4340 - accuracy: 0.7816 - val_loss: 0.4418 - val_accuracy: 0.7820 Epoch 204/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4214 - accuracy: 0.7848 - val_loss: 0.4242 - val_accuracy: 0.7752 Epoch 205/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4140 - accuracy: 0.7900 - val_loss: 0.4061 - val_accuracy: 0.7996 Epoch 206/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4110 - accuracy: 0.8032 - val_loss: 0.4196 - val_accuracy: 0.7864 Epoch 207/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4019 - accuracy: 0.8092 - val_loss: 0.4060 - val_accuracy: 0.8024 Epoch 208/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3966 - accuracy: 0.8056 - val_loss: 0.4329 - val_accuracy: 0.7864 Epoch 209/400 79/79 [==============================] - 0s 6ms/step - loss: 0.4044 - accuracy: 0.8108 - val_loss: 0.4098 - val_accuracy: 0.8000 Epoch 210/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3890 - accuracy: 0.8208 - val_loss: 0.3925 - val_accuracy: 0.8220 Epoch 211/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3767 - accuracy: 0.8268 - val_loss: 0.3883 - val_accuracy: 0.8180 Epoch 212/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3725 - accuracy: 0.8320 - val_loss: 0.3958 - val_accuracy: 0.8180 Epoch 213/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3733 - accuracy: 0.8284 - val_loss: 0.3906 - val_accuracy: 0.8224 Epoch 214/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3736 - accuracy: 0.8304 - val_loss: 0.3767 - val_accuracy: 0.8264 Epoch 215/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3653 - accuracy: 0.8404 - val_loss: 0.3746 - val_accuracy: 0.8288 Epoch 216/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3565 - accuracy: 0.8388 - val_loss: 0.3837 - val_accuracy: 0.8244 Epoch 217/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3645 - accuracy: 0.8364 - val_loss: 0.3668 - val_accuracy: 0.8352 Epoch 218/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3478 - accuracy: 0.8392 - val_loss: 0.3597 - val_accuracy: 0.8404 Epoch 219/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3459 - accuracy: 0.8336 - val_loss: 0.3646 - val_accuracy: 0.8356 Epoch 220/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3489 - accuracy: 0.8388 - val_loss: 0.3582 - val_accuracy: 0.8344 Epoch 221/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3411 - accuracy: 0.8456 - val_loss: 0.3456 - val_accuracy: 0.8360 Epoch 222/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3370 - accuracy: 0.8488 - val_loss: 0.3502 - val_accuracy: 0.8384 Epoch 223/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3319 - accuracy: 0.8476 - val_loss: 0.3502 - val_accuracy: 0.8420 Epoch 224/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3363 - accuracy: 0.8472 - val_loss: 0.3430 - val_accuracy: 0.8436 Epoch 225/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3778 - accuracy: 0.8284 - val_loss: 0.4149 - val_accuracy: 0.8112 Epoch 226/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3839 - accuracy: 0.8228 - val_loss: 0.3709 - val_accuracy: 0.8284 Epoch 227/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3425 - accuracy: 0.8468 - val_loss: 0.3601 - val_accuracy: 0.8340 Epoch 228/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3409 - accuracy: 0.8440 - val_loss: 0.3505 - val_accuracy: 0.8432 Epoch 229/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3295 - accuracy: 0.8484 - val_loss: 0.3356 - val_accuracy: 0.8508 Epoch 230/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3588 - accuracy: 0.8436 - val_loss: 0.3536 - val_accuracy: 0.8308 Epoch 231/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3324 - accuracy: 0.8468 - val_loss: 0.3320 - val_accuracy: 0.8432 Epoch 232/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3206 - accuracy: 0.8556 - val_loss: 0.3609 - val_accuracy: 0.8368 Epoch 233/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3172 - accuracy: 0.8536 - val_loss: 0.3313 - val_accuracy: 0.8484 Epoch 234/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3154 - accuracy: 0.8528 - val_loss: 0.3404 - val_accuracy: 0.8504 Epoch 235/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3129 - accuracy: 0.8564 - val_loss: 0.3338 - val_accuracy: 0.8492 Epoch 236/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3169 - accuracy: 0.8620 - val_loss: 0.3350 - val_accuracy: 0.8488 Epoch 237/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3091 - accuracy: 0.8556 - val_loss: 0.3287 - val_accuracy: 0.8496 Epoch 238/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3036 - accuracy: 0.8608 - val_loss: 0.3361 - val_accuracy: 0.8496 Epoch 239/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3015 - accuracy: 0.8616 - val_loss: 0.3231 - val_accuracy: 0.8540 Epoch 240/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3041 - accuracy: 0.8676 - val_loss: 0.3379 - val_accuracy: 0.8528 Epoch 241/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3002 - accuracy: 0.8636 - val_loss: 0.3311 - val_accuracy: 0.8476 Epoch 242/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3036 - accuracy: 0.8636 - val_loss: 0.3258 - val_accuracy: 0.8544 Epoch 243/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3242 - accuracy: 0.8584 - val_loss: 0.3431 - val_accuracy: 0.8444 Epoch 244/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3018 - accuracy: 0.8628 - val_loss: 0.3199 - val_accuracy: 0.8536 Epoch 245/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3061 - accuracy: 0.8636 - val_loss: 0.3290 - val_accuracy: 0.8496 Epoch 246/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3023 - accuracy: 0.8604 - val_loss: 0.3210 - val_accuracy: 0.8520 Epoch 247/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2966 - accuracy: 0.8700 - val_loss: 0.3544 - val_accuracy: 0.8288 Epoch 248/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3162 - accuracy: 0.8492 - val_loss: 0.3154 - val_accuracy: 0.8520 Epoch 249/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3012 - accuracy: 0.8644 - val_loss: 0.3147 - val_accuracy: 0.8584 Epoch 250/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2968 - accuracy: 0.8632 - val_loss: 0.3246 - val_accuracy: 0.8500 Epoch 251/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2947 - accuracy: 0.8676 - val_loss: 0.3142 - val_accuracy: 0.8568 Epoch 252/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2884 - accuracy: 0.8688 - val_loss: 0.3040 - val_accuracy: 0.8576 Epoch 253/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2885 - accuracy: 0.8696 - val_loss: 0.3189 - val_accuracy: 0.8620 Epoch 254/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2943 - accuracy: 0.8696 - val_loss: 0.3055 - val_accuracy: 0.8516 Epoch 255/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2916 - accuracy: 0.8640 - val_loss: 0.3133 - val_accuracy: 0.8532 Epoch 256/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2931 - accuracy: 0.8664 - val_loss: 0.3389 - val_accuracy: 0.8536 Epoch 257/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2836 - accuracy: 0.8752 - val_loss: 0.3070 - val_accuracy: 0.8572 Epoch 258/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2816 - accuracy: 0.8720 - val_loss: 0.3079 - val_accuracy: 0.8592 Epoch 259/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2950 - accuracy: 0.8692 - val_loss: 0.3066 - val_accuracy: 0.8620 Epoch 260/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2838 - accuracy: 0.8736 - val_loss: 0.3033 - val_accuracy: 0.8620 Epoch 261/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2801 - accuracy: 0.8760 - val_loss: 0.3058 - val_accuracy: 0.8568 Epoch 262/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2814 - accuracy: 0.8772 - val_loss: 0.3037 - val_accuracy: 0.8636 Epoch 263/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2987 - accuracy: 0.8668 - val_loss: 0.3168 - val_accuracy: 0.8512 Epoch 264/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2747 - accuracy: 0.8816 - val_loss: 0.2969 - val_accuracy: 0.8680 Epoch 265/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2730 - accuracy: 0.8816 - val_loss: 0.3008 - val_accuracy: 0.8692 Epoch 266/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2710 - accuracy: 0.8812 - val_loss: 0.2961 - val_accuracy: 0.8620 Epoch 267/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2667 - accuracy: 0.8804 - val_loss: 0.2989 - val_accuracy: 0.8628 Epoch 268/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2685 - accuracy: 0.8832 - val_loss: 0.2945 - val_accuracy: 0.8676 Epoch 269/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2664 - accuracy: 0.8848 - val_loss: 0.2975 - val_accuracy: 0.8612 Epoch 270/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2732 - accuracy: 0.8800 - val_loss: 0.3056 - val_accuracy: 0.8532 Epoch 271/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3005 - accuracy: 0.8776 - val_loss: 0.2920 - val_accuracy: 0.8684 Epoch 272/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2830 - accuracy: 0.8696 - val_loss: 0.2983 - val_accuracy: 0.8684 Epoch 273/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2739 - accuracy: 0.8800 - val_loss: 0.2991 - val_accuracy: 0.8580 Epoch 274/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2663 - accuracy: 0.8816 - val_loss: 0.2911 - val_accuracy: 0.8652 Epoch 275/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2614 - accuracy: 0.8856 - val_loss: 0.2876 - val_accuracy: 0.8660 Epoch 276/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2728 - accuracy: 0.8820 - val_loss: 0.3010 - val_accuracy: 0.8620 Epoch 277/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2796 - accuracy: 0.8736 - val_loss: 0.3075 - val_accuracy: 0.8648 Epoch 278/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2701 - accuracy: 0.8836 - val_loss: 0.2892 - val_accuracy: 0.8632 Epoch 279/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2818 - accuracy: 0.8812 - val_loss: 0.2953 - val_accuracy: 0.8656 Epoch 280/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3780 - accuracy: 0.8300 - val_loss: 0.3273 - val_accuracy: 0.8580 Epoch 281/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2866 - accuracy: 0.8720 - val_loss: 0.2984 - val_accuracy: 0.8660 Epoch 282/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2721 - accuracy: 0.8780 - val_loss: 0.2901 - val_accuracy: 0.8684 Epoch 283/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3003 - accuracy: 0.8636 - val_loss: 0.3686 - val_accuracy: 0.8172 Epoch 284/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3474 - accuracy: 0.8428 - val_loss: 0.3232 - val_accuracy: 0.8496 Epoch 285/400 79/79 [==============================] - 0s 6ms/step - loss: 0.3059 - accuracy: 0.8640 - val_loss: 0.3046 - val_accuracy: 0.8572 Epoch 286/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2884 - accuracy: 0.8696 - val_loss: 0.2977 - val_accuracy: 0.8656 Epoch 287/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2780 - accuracy: 0.8772 - val_loss: 0.3672 - val_accuracy: 0.8384 Epoch 288/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2969 - accuracy: 0.8676 - val_loss: 0.2954 - val_accuracy: 0.8624 Epoch 289/400 79/79 [==============================] - 1s 6ms/step - loss: 0.2803 - accuracy: 0.8736 - val_loss: 0.2911 - val_accuracy: 0.8624 Epoch 290/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2810 - accuracy: 0.8760 - val_loss: 0.2936 - val_accuracy: 0.8616 Epoch 291/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2767 - accuracy: 0.8800 - val_loss: 0.2882 - val_accuracy: 0.8660 Epoch 292/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2697 - accuracy: 0.8780 - val_loss: 0.2849 - val_accuracy: 0.8668 Epoch 293/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2753 - accuracy: 0.8748 - val_loss: 0.2933 - val_accuracy: 0.8648 Epoch 294/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2767 - accuracy: 0.8748 - val_loss: 0.2799 - val_accuracy: 0.8668 Epoch 295/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2623 - accuracy: 0.8824 - val_loss: 0.2881 - val_accuracy: 0.8616 Epoch 296/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2602 - accuracy: 0.8816 - val_loss: 0.2886 - val_accuracy: 0.8652 Epoch 297/400 79/79 [==============================] - 1s 6ms/step - loss: 0.2612 - accuracy: 0.8788 - val_loss: 0.2838 - val_accuracy: 0.8652 Epoch 298/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2593 - accuracy: 0.8868 - val_loss: 0.2771 - val_accuracy: 0.8688 Epoch 299/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2582 - accuracy: 0.8800 - val_loss: 0.2817 - val_accuracy: 0.8668 Epoch 300/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2604 - accuracy: 0.8856 - val_loss: 0.2763 - val_accuracy: 0.8716 Epoch 301/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2640 - accuracy: 0.8848 - val_loss: 0.3153 - val_accuracy: 0.8596 Epoch 302/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2773 - accuracy: 0.8804 - val_loss: 0.2828 - val_accuracy: 0.8680 Epoch 303/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2748 - accuracy: 0.8816 - val_loss: 0.2749 - val_accuracy: 0.8716 Epoch 304/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2664 - accuracy: 0.8816 - val_loss: 0.2797 - val_accuracy: 0.8740 Epoch 305/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2570 - accuracy: 0.8840 - val_loss: 0.2722 - val_accuracy: 0.8760 Epoch 306/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2559 - accuracy: 0.8900 - val_loss: 0.2786 - val_accuracy: 0.8700 Epoch 307/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2547 - accuracy: 0.8896 - val_loss: 0.2692 - val_accuracy: 0.8756 Epoch 308/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2573 - accuracy: 0.8868 - val_loss: 0.2748 - val_accuracy: 0.8720 Epoch 309/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2520 - accuracy: 0.8900 - val_loss: 0.2758 - val_accuracy: 0.8728 Epoch 310/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2533 - accuracy: 0.8864 - val_loss: 0.2676 - val_accuracy: 0.8748 Epoch 311/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2516 - accuracy: 0.8948 - val_loss: 0.2699 - val_accuracy: 0.8784 Epoch 312/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2489 - accuracy: 0.8948 - val_loss: 0.2641 - val_accuracy: 0.8796 Epoch 313/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2505 - accuracy: 0.8972 - val_loss: 0.2711 - val_accuracy: 0.8760 Epoch 314/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2379 - accuracy: 0.9004 - val_loss: 0.2585 - val_accuracy: 0.8832 Epoch 315/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2383 - accuracy: 0.9008 - val_loss: 0.2604 - val_accuracy: 0.8856 Epoch 316/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2286 - accuracy: 0.9076 - val_loss: 0.2660 - val_accuracy: 0.8840 Epoch 317/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2282 - accuracy: 0.9100 - val_loss: 0.2567 - val_accuracy: 0.8888 Epoch 318/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2291 - accuracy: 0.9096 - val_loss: 0.2530 - val_accuracy: 0.8948 Epoch 319/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2284 - accuracy: 0.9108 - val_loss: 0.2332 - val_accuracy: 0.9012 Epoch 320/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2151 - accuracy: 0.9176 - val_loss: 0.2388 - val_accuracy: 0.8984 Epoch 321/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2130 - accuracy: 0.9188 - val_loss: 0.2076 - val_accuracy: 0.9264 Epoch 322/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1967 - accuracy: 0.9292 - val_loss: 0.2096 - val_accuracy: 0.9312 Epoch 323/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1843 - accuracy: 0.9388 - val_loss: 0.1982 - val_accuracy: 0.9344 Epoch 324/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1722 - accuracy: 0.9468 - val_loss: 0.1952 - val_accuracy: 0.9396 Epoch 325/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1616 - accuracy: 0.9528 - val_loss: 0.1722 - val_accuracy: 0.9496 Epoch 326/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1605 - accuracy: 0.9532 - val_loss: 0.1745 - val_accuracy: 0.9448 Epoch 327/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1689 - accuracy: 0.9468 - val_loss: 0.1750 - val_accuracy: 0.9468 Epoch 328/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1570 - accuracy: 0.9536 - val_loss: 0.1766 - val_accuracy: 0.9492 Epoch 329/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1527 - accuracy: 0.9616 - val_loss: 0.1581 - val_accuracy: 0.9576 Epoch 330/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1371 - accuracy: 0.9660 - val_loss: 0.1693 - val_accuracy: 0.9536 Epoch 331/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9648 - val_loss: 0.1552 - val_accuracy: 0.9568 Epoch 332/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1453 - accuracy: 0.9612 - val_loss: 0.1445 - val_accuracy: 0.9608 Epoch 333/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1447 - accuracy: 0.9636 - val_loss: 0.1648 - val_accuracy: 0.9560 Epoch 334/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1431 - accuracy: 0.9652 - val_loss: 0.1450 - val_accuracy: 0.9632 Epoch 335/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2000 - accuracy: 0.9468 - val_loss: 0.1380 - val_accuracy: 0.9644 Epoch 336/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9676 - val_loss: 0.1403 - val_accuracy: 0.9620 Epoch 337/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1197 - accuracy: 0.9700 - val_loss: 0.1930 - val_accuracy: 0.9472 Epoch 338/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1395 - accuracy: 0.9640 - val_loss: 0.1366 - val_accuracy: 0.9636 Epoch 339/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1173 - accuracy: 0.9704 - val_loss: 0.1253 - val_accuracy: 0.9668 Epoch 340/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1209 - accuracy: 0.9724 - val_loss: 0.1151 - val_accuracy: 0.9704 Epoch 341/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1031 - accuracy: 0.9772 - val_loss: 0.1122 - val_accuracy: 0.9724 Epoch 342/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1015 - accuracy: 0.9776 - val_loss: 0.1129 - val_accuracy: 0.9720 Epoch 343/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1069 - accuracy: 0.9752 - val_loss: 0.1121 - val_accuracy: 0.9728 Epoch 344/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1031 - accuracy: 0.9752 - val_loss: 0.0995 - val_accuracy: 0.9756 Epoch 345/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1038 - accuracy: 0.9736 - val_loss: 0.0965 - val_accuracy: 0.9756 Epoch 346/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0845 - accuracy: 0.9800 - val_loss: 0.0894 - val_accuracy: 0.9776 Epoch 347/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0977 - accuracy: 0.9764 - val_loss: 0.1004 - val_accuracy: 0.9752 Epoch 348/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0964 - accuracy: 0.9748 - val_loss: 0.1017 - val_accuracy: 0.9728 Epoch 349/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9820 - val_loss: 0.0827 - val_accuracy: 0.9796 Epoch 350/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0864 - accuracy: 0.9792 - val_loss: 0.0825 - val_accuracy: 0.9832 Epoch 351/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.9832 - val_loss: 0.1226 - val_accuracy: 0.9684 Epoch 352/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1088 - accuracy: 0.9716 - val_loss: 0.1066 - val_accuracy: 0.9740 Epoch 353/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1056 - accuracy: 0.9728 - val_loss: 0.0994 - val_accuracy: 0.9776 Epoch 354/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0875 - accuracy: 0.9788 - val_loss: 0.0836 - val_accuracy: 0.9816 Epoch 355/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0811 - accuracy: 0.9824 - val_loss: 0.0956 - val_accuracy: 0.9756 Epoch 356/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0750 - accuracy: 0.9836 - val_loss: 0.0742 - val_accuracy: 0.9828 Epoch 357/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1014 - accuracy: 0.9764 - val_loss: 0.1630 - val_accuracy: 0.9560 Epoch 358/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1405 - accuracy: 0.9652 - val_loss: 0.1010 - val_accuracy: 0.9752 Epoch 359/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1292 - accuracy: 0.9672 - val_loss: 0.1304 - val_accuracy: 0.9672 Epoch 360/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1048 - accuracy: 0.9740 - val_loss: 0.0815 - val_accuracy: 0.9812 Epoch 361/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0782 - accuracy: 0.9816 - val_loss: 0.0712 - val_accuracy: 0.9852 Epoch 362/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0658 - accuracy: 0.9848 - val_loss: 0.0721 - val_accuracy: 0.9812 Epoch 363/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9848 - val_loss: 0.0672 - val_accuracy: 0.9836 Epoch 364/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0647 - accuracy: 0.9844 - val_loss: 0.0713 - val_accuracy: 0.9828 Epoch 365/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0654 - accuracy: 0.9844 - val_loss: 0.0715 - val_accuracy: 0.9820 Epoch 366/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0677 - accuracy: 0.9820 - val_loss: 0.0689 - val_accuracy: 0.9828 Epoch 367/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 0.9860 - val_loss: 0.0653 - val_accuracy: 0.9836 Epoch 368/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0557 - accuracy: 0.9892 - val_loss: 0.0648 - val_accuracy: 0.9820 Epoch 369/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0545 - accuracy: 0.9872 - val_loss: 0.0668 - val_accuracy: 0.9828 Epoch 370/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0600 - accuracy: 0.9864 - val_loss: 0.0649 - val_accuracy: 0.9848 Epoch 371/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0551 - accuracy: 0.9864 - val_loss: 0.0674 - val_accuracy: 0.9832 Epoch 372/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0639 - accuracy: 0.9856 - val_loss: 0.0781 - val_accuracy: 0.9808 Epoch 373/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0665 - accuracy: 0.9812 - val_loss: 0.0711 - val_accuracy: 0.9824 Epoch 374/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9844 - val_loss: 0.0605 - val_accuracy: 0.9864 Epoch 375/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0620 - accuracy: 0.9864 - val_loss: 0.0632 - val_accuracy: 0.9856 Epoch 376/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0673 - accuracy: 0.9828 - val_loss: 0.0635 - val_accuracy: 0.9848 Epoch 377/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9824 - val_loss: 0.0627 - val_accuracy: 0.9860 Epoch 378/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0504 - accuracy: 0.9872 - val_loss: 0.0597 - val_accuracy: 0.9860 Epoch 379/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0549 - accuracy: 0.9868 - val_loss: 0.0649 - val_accuracy: 0.9856 Epoch 380/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0523 - accuracy: 0.9860 - val_loss: 0.1208 - val_accuracy: 0.9768 Epoch 381/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1911 - accuracy: 0.9576 - val_loss: 0.1913 - val_accuracy: 0.9484 Epoch 382/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9720 - val_loss: 0.0887 - val_accuracy: 0.9820 Epoch 383/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.9836 - val_loss: 0.0687 - val_accuracy: 0.9840 Epoch 384/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0636 - accuracy: 0.9844 - val_loss: 0.0674 - val_accuracy: 0.9856 Epoch 385/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0595 - accuracy: 0.9828 - val_loss: 0.0779 - val_accuracy: 0.9832 Epoch 386/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0703 - accuracy: 0.9844 - val_loss: 0.0651 - val_accuracy: 0.9852 Epoch 387/400 79/79 [==============================] - 1s 8ms/step - loss: 0.0707 - accuracy: 0.9832 - val_loss: 0.0796 - val_accuracy: 0.9828 Epoch 388/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2051 - accuracy: 0.9564 - val_loss: 0.4431 - val_accuracy: 0.8876 Epoch 389/400 79/79 [==============================] - 0s 6ms/step - loss: 0.2905 - accuracy: 0.9260 - val_loss: 0.1163 - val_accuracy: 0.9708 Epoch 390/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1224 - accuracy: 0.9688 - val_loss: 0.0957 - val_accuracy: 0.9780 Epoch 391/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1141 - accuracy: 0.9712 - val_loss: 0.1079 - val_accuracy: 0.9764 Epoch 392/400 79/79 [==============================] - 0s 6ms/step - loss: 0.1019 - accuracy: 0.9752 - val_loss: 0.0845 - val_accuracy: 0.9808 Epoch 393/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0949 - accuracy: 0.9772 - val_loss: 0.0862 - val_accuracy: 0.9820 Epoch 394/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0836 - accuracy: 0.9784 - val_loss: 0.0897 - val_accuracy: 0.9816 Epoch 395/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0785 - accuracy: 0.9796 - val_loss: 0.0960 - val_accuracy: 0.9808 Epoch 396/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0785 - accuracy: 0.9804 - val_loss: 0.0907 - val_accuracy: 0.9800 Epoch 397/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9876 - val_loss: 0.0772 - val_accuracy: 0.9844 Epoch 398/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0550 - accuracy: 0.9876 - val_loss: 0.0745 - val_accuracy: 0.9844 Epoch 399/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9872 - val_loss: 0.0661 - val_accuracy: 0.9848 Epoch 400/400 79/79 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.9880 - val_loss: 0.0654 - val_accuracy: 0.9864 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1f98f400> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec262e1d30> # Make the problem harder by making T larger T = 30 D = 1 X = [] Y = [] for t in range ( 5000 ): x = np . random . randn ( T ) X . append ( x ) y = get_label ( x , 0 , 1 , 2 ) # long distance Y . append ( y ) X = np . array ( X ) Y = np . array ( Y ) N = len ( X ) # Now test our LSTM inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 1 x = LSTM ( 15 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 400 , validation_split = 0.5 , ) Epoch 1/400 79/79 [==============================] - 1s 10ms/step - loss: 0.6938 - accuracy: 0.5124 - val_loss: 0.6950 - val_accuracy: 0.4940 Epoch 2/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6925 - accuracy: 0.5200 - val_loss: 0.6945 - val_accuracy: 0.4964 Epoch 3/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6927 - accuracy: 0.5204 - val_loss: 0.6955 - val_accuracy: 0.4960 Epoch 4/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6916 - accuracy: 0.5224 - val_loss: 0.6943 - val_accuracy: 0.4956 Epoch 5/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5228 - val_loss: 0.6952 - val_accuracy: 0.4960 Epoch 6/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6920 - accuracy: 0.5232 - val_loss: 0.6945 - val_accuracy: 0.4960 Epoch 7/400 79/79 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5252 - val_loss: 0.6949 - val_accuracy: 0.4912 Epoch 8/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6917 - accuracy: 0.5276 - val_loss: 0.6943 - val_accuracy: 0.4908 Epoch 9/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6908 - accuracy: 0.5244 - val_loss: 0.6962 - val_accuracy: 0.4892 Epoch 10/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6929 - accuracy: 0.5300 - val_loss: 0.6971 - val_accuracy: 0.4964 Epoch 11/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6913 - accuracy: 0.5284 - val_loss: 0.6950 - val_accuracy: 0.4960 Epoch 12/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6909 - accuracy: 0.5248 - val_loss: 0.6954 - val_accuracy: 0.4940 Epoch 13/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6911 - accuracy: 0.5284 - val_loss: 0.6941 - val_accuracy: 0.4964 Epoch 14/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6912 - accuracy: 0.5344 - val_loss: 0.6959 - val_accuracy: 0.4968 Epoch 15/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6904 - accuracy: 0.5332 - val_loss: 0.6952 - val_accuracy: 0.5016 Epoch 16/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6912 - accuracy: 0.5372 - val_loss: 0.6958 - val_accuracy: 0.5020 Epoch 17/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6907 - accuracy: 0.5324 - val_loss: 0.6962 - val_accuracy: 0.5052 Epoch 18/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6924 - accuracy: 0.5300 - val_loss: 0.6953 - val_accuracy: 0.5056 Epoch 19/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6893 - accuracy: 0.5352 - val_loss: 0.6994 - val_accuracy: 0.4976 Epoch 20/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6899 - accuracy: 0.5268 - val_loss: 0.6960 - val_accuracy: 0.4988 Epoch 21/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6898 - accuracy: 0.5432 - val_loss: 0.6958 - val_accuracy: 0.5104 Epoch 22/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6879 - accuracy: 0.5376 - val_loss: 0.6995 - val_accuracy: 0.5052 Epoch 23/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6891 - accuracy: 0.5356 - val_loss: 0.7014 - val_accuracy: 0.5012 Epoch 24/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6875 - accuracy: 0.5416 - val_loss: 0.7036 - val_accuracy: 0.5016 Epoch 25/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6885 - accuracy: 0.5388 - val_loss: 0.6955 - val_accuracy: 0.4996 Epoch 26/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6861 - accuracy: 0.5396 - val_loss: 0.7011 - val_accuracy: 0.5016 Epoch 27/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6859 - accuracy: 0.5472 - val_loss: 0.7008 - val_accuracy: 0.4956 Epoch 28/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6856 - accuracy: 0.5528 - val_loss: 0.7030 - val_accuracy: 0.5012 Epoch 29/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6843 - accuracy: 0.5496 - val_loss: 0.7024 - val_accuracy: 0.4944 Epoch 30/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6826 - accuracy: 0.5508 - val_loss: 0.7150 - val_accuracy: 0.4960 Epoch 31/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6811 - accuracy: 0.5580 - val_loss: 0.7058 - val_accuracy: 0.5004 Epoch 32/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6882 - accuracy: 0.5332 - val_loss: 0.7025 - val_accuracy: 0.4940 Epoch 33/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6927 - accuracy: 0.5188 - val_loss: 0.6988 - val_accuracy: 0.5000 Epoch 34/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6897 - accuracy: 0.5416 - val_loss: 0.7027 - val_accuracy: 0.5052 Epoch 35/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6892 - accuracy: 0.5388 - val_loss: 0.6926 - val_accuracy: 0.5076 Epoch 36/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6703 - accuracy: 0.5680 - val_loss: 0.6594 - val_accuracy: 0.5656 Epoch 37/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6364 - accuracy: 0.6020 - val_loss: 0.6418 - val_accuracy: 0.5920 Epoch 38/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6196 - accuracy: 0.6136 - val_loss: 0.6510 - val_accuracy: 0.5844 Epoch 39/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6257 - accuracy: 0.6184 - val_loss: 0.6371 - val_accuracy: 0.5816 Epoch 40/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6065 - accuracy: 0.6308 - val_loss: 0.6372 - val_accuracy: 0.5924 Epoch 41/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6144 - accuracy: 0.6324 - val_loss: 0.6255 - val_accuracy: 0.5948 Epoch 42/400 79/79 [==============================] - 1s 7ms/step - loss: 0.6017 - accuracy: 0.6336 - val_loss: 0.6380 - val_accuracy: 0.5960 Epoch 43/400 79/79 [==============================] - 1s 6ms/step - loss: 0.6007 - accuracy: 0.6372 - val_loss: 0.6215 - val_accuracy: 0.6032 Epoch 44/400 79/79 [==============================] - 1s 6ms/step - loss: 0.5848 - accuracy: 0.6604 - val_loss: 0.5784 - val_accuracy: 0.6684 Epoch 45/400 79/79 [==============================] - 1s 6ms/step - loss: 0.5135 - accuracy: 0.7432 - val_loss: 0.4908 - val_accuracy: 0.7532 Epoch 46/400 79/79 [==============================] - 1s 7ms/step - loss: 0.4700 - accuracy: 0.7776 - val_loss: 0.5146 - val_accuracy: 0.7540 Epoch 47/400 79/79 [==============================] - 1s 7ms/step - loss: 0.4460 - accuracy: 0.7956 - val_loss: 0.4578 - val_accuracy: 0.7732 Epoch 48/400 79/79 [==============================] - 1s 6ms/step - loss: 0.4209 - accuracy: 0.8076 - val_loss: 0.4344 - val_accuracy: 0.7916 Epoch 49/400 79/79 [==============================] - 1s 6ms/step - loss: 0.3859 - accuracy: 0.8280 - val_loss: 0.3907 - val_accuracy: 0.8176 Epoch 50/400 79/79 [==============================] - 1s 7ms/step - loss: 0.2989 - accuracy: 0.8860 - val_loss: 0.2240 - val_accuracy: 0.9068 Epoch 51/400 79/79 [==============================] - 1s 6ms/step - loss: 0.1341 - accuracy: 0.9544 - val_loss: 0.1162 - val_accuracy: 0.9628 Epoch 52/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0979 - accuracy: 0.9668 - val_loss: 0.0890 - val_accuracy: 0.9664 Epoch 53/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0632 - accuracy: 0.9812 - val_loss: 0.0899 - val_accuracy: 0.9660 Epoch 54/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0643 - accuracy: 0.9800 - val_loss: 0.1283 - val_accuracy: 0.9576 Epoch 55/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0563 - accuracy: 0.9800 - val_loss: 0.0561 - val_accuracy: 0.9788 Epoch 56/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0444 - accuracy: 0.9836 - val_loss: 0.0495 - val_accuracy: 0.9824 Epoch 57/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0373 - accuracy: 0.9892 - val_loss: 0.0891 - val_accuracy: 0.9708 Epoch 58/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0411 - accuracy: 0.9844 - val_loss: 0.0512 - val_accuracy: 0.9828 Epoch 59/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0312 - accuracy: 0.9896 - val_loss: 0.0501 - val_accuracy: 0.9808 Epoch 60/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0293 - accuracy: 0.9916 - val_loss: 0.0605 - val_accuracy: 0.9748 Epoch 61/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0300 - accuracy: 0.9868 - val_loss: 0.0444 - val_accuracy: 0.9836 Epoch 62/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0214 - accuracy: 0.9924 - val_loss: 0.0561 - val_accuracy: 0.9856 Epoch 63/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.0404 - val_accuracy: 0.9868 Epoch 64/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0212 - accuracy: 0.9920 - val_loss: 0.0402 - val_accuracy: 0.9852 Epoch 65/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.0302 - val_accuracy: 0.9876 Epoch 66/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0317 - accuracy: 0.9876 - val_loss: 0.0469 - val_accuracy: 0.9836 Epoch 67/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0213 - accuracy: 0.9900 - val_loss: 0.0292 - val_accuracy: 0.9900 Epoch 68/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.0289 - val_accuracy: 0.9880 Epoch 69/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0165 - accuracy: 0.9940 - val_loss: 0.0329 - val_accuracy: 0.9868 Epoch 70/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0244 - accuracy: 0.9932 - val_loss: 0.0339 - val_accuracy: 0.9852 Epoch 71/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0256 - accuracy: 0.9908 - val_loss: 0.0401 - val_accuracy: 0.9872 Epoch 72/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0263 - accuracy: 0.9912 - val_loss: 0.0332 - val_accuracy: 0.9880 Epoch 73/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0553 - val_accuracy: 0.9832 Epoch 74/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.0300 - val_accuracy: 0.9888 Epoch 75/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.0419 - val_accuracy: 0.9876 Epoch 76/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0178 - accuracy: 0.9952 - val_loss: 0.0442 - val_accuracy: 0.9856 Epoch 77/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.0257 - val_accuracy: 0.9908 Epoch 78/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0352 - val_accuracy: 0.9892 Epoch 79/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0140 - accuracy: 0.9940 - val_loss: 0.0460 - val_accuracy: 0.9856 Epoch 80/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0125 - accuracy: 0.9964 - val_loss: 0.0349 - val_accuracy: 0.9884 Epoch 81/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0070 - accuracy: 0.9972 - val_loss: 0.0329 - val_accuracy: 0.9908 Epoch 82/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0069 - accuracy: 0.9972 - val_loss: 0.0338 - val_accuracy: 0.9904 Epoch 83/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0289 - accuracy: 0.9964 - val_loss: 0.0410 - val_accuracy: 0.9868 Epoch 84/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0840 - accuracy: 0.9744 - val_loss: 0.0600 - val_accuracy: 0.9792 Epoch 85/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0278 - accuracy: 0.9892 - val_loss: 0.0368 - val_accuracy: 0.9872 Epoch 86/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0358 - val_accuracy: 0.9868 Epoch 87/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0401 - val_accuracy: 0.9868 Epoch 88/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0444 - val_accuracy: 0.9856 Epoch 89/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0383 - val_accuracy: 0.9880 Epoch 90/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0131 - accuracy: 0.9952 - val_loss: 0.0456 - val_accuracy: 0.9852 Epoch 91/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0293 - val_accuracy: 0.9884 Epoch 92/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0057 - accuracy: 0.9976 - val_loss: 0.0328 - val_accuracy: 0.9864 Epoch 93/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0243 - val_accuracy: 0.9924 Epoch 94/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0367 - val_accuracy: 0.9872 Epoch 95/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0063 - accuracy: 0.9984 - val_loss: 0.0438 - val_accuracy: 0.9884 Epoch 96/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.0483 - val_accuracy: 0.9864 Epoch 97/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9988 - val_loss: 0.0479 - val_accuracy: 0.9884 Epoch 98/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.0357 - val_accuracy: 0.9884 Epoch 99/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0145 - accuracy: 0.9960 - val_loss: 0.0373 - val_accuracy: 0.9884 Epoch 100/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0154 - accuracy: 0.9948 - val_loss: 0.0376 - val_accuracy: 0.9852 Epoch 101/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.0364 - val_accuracy: 0.9884 Epoch 102/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.0261 - val_accuracy: 0.9920 Epoch 103/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.0284 - val_accuracy: 0.9900 Epoch 104/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 0.0232 - val_accuracy: 0.9908 Epoch 105/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0271 - val_accuracy: 0.9896 Epoch 106/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0290 - val_accuracy: 0.9892 Epoch 107/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 0.0292 - val_accuracy: 0.9896 Epoch 108/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0302 - val_accuracy: 0.9912 Epoch 109/400 79/79 [==============================] - 1s 7ms/step - loss: 9.9011e-04 - accuracy: 0.9996 - val_loss: 0.0255 - val_accuracy: 0.9920 Epoch 110/400 79/79 [==============================] - 1s 7ms/step - loss: 7.9221e-04 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 0.9896 Epoch 111/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0195 - accuracy: 0.9964 - val_loss: 0.0237 - val_accuracy: 0.9916 Epoch 112/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0388 - val_accuracy: 0.9864 Epoch 113/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.0319 - val_accuracy: 0.9892 Epoch 114/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0298 - val_accuracy: 0.9904 Epoch 115/400 79/79 [==============================] - 1s 6ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0315 - val_accuracy: 0.9892 Epoch 116/400 79/79 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0335 - val_accuracy: 0.9900 Epoch 117/400 79/79 [==============================] - 1s 7ms/step - loss: 9.3726e-04 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 0.9908 Epoch 118/400 79/79 [==============================] - 1s 7ms/step - loss: 6.0662e-04 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 0.9908 Epoch 119/400 79/79 [==============================] - 1s 7ms/step - loss: 5.9509e-04 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 0.9904 Epoch 120/400 79/79 [==============================] - 1s 7ms/step - loss: 5.5597e-04 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 0.9912 Epoch 121/400 79/79 [==============================] - 1s 7ms/step - loss: 5.2154e-04 - accuracy: 1.0000 - val_loss: 0.0347 - val_accuracy: 0.9908 Epoch 122/400 79/79 [==============================] - 1s 6ms/step - loss: 3.3155e-04 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9908 Epoch 123/400 79/79 [==============================] - 1s 6ms/step - loss: 2.8362e-04 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9912 Epoch 124/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2577e-04 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 0.9908 Epoch 125/400 79/79 [==============================] - 1s 7ms/step - loss: 1.9810e-04 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 0.9912 Epoch 126/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7869e-04 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 0.9924 Epoch 127/400 79/79 [==============================] - 1s 6ms/step - loss: 1.5033e-04 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 0.9924 Epoch 128/400 79/79 [==============================] - 1s 7ms/step - loss: 1.1684e-04 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9924 Epoch 129/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0989e-04 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9924 Epoch 130/400 79/79 [==============================] - 1s 6ms/step - loss: 9.0361e-05 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9924 Epoch 131/400 79/79 [==============================] - 1s 7ms/step - loss: 7.7742e-05 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.9924 Epoch 132/400 79/79 [==============================] - 1s 6ms/step - loss: 7.2091e-05 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9924 Epoch 133/400 79/79 [==============================] - 1s 7ms/step - loss: 6.4350e-05 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 0.9924 Epoch 134/400 79/79 [==============================] - 1s 7ms/step - loss: 5.8189e-05 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 0.9924 Epoch 135/400 79/79 [==============================] - 1s 6ms/step - loss: 4.9459e-05 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9924 Epoch 136/400 79/79 [==============================] - 1s 6ms/step - loss: 4.5237e-05 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 0.9928 Epoch 137/400 79/79 [==============================] - 1s 7ms/step - loss: 4.1174e-05 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9928 Epoch 138/400 79/79 [==============================] - 1s 7ms/step - loss: 3.6626e-05 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.9924 Epoch 139/400 79/79 [==============================] - 1s 7ms/step - loss: 3.3935e-05 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9928 Epoch 140/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0910e-05 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 0.9928 Epoch 141/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8376e-05 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9928 Epoch 142/400 79/79 [==============================] - 1s 6ms/step - loss: 2.6438e-05 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9928 Epoch 143/400 79/79 [==============================] - 1s 6ms/step - loss: 2.4154e-05 - accuracy: 1.0000 - val_loss: 0.0538 - val_accuracy: 0.9928 Epoch 144/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2308e-05 - accuracy: 1.0000 - val_loss: 0.0543 - val_accuracy: 0.9924 Epoch 145/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1110e-05 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 0.9928 Epoch 146/400 79/79 [==============================] - 1s 6ms/step - loss: 1.9753e-05 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 0.9924 Epoch 147/400 79/79 [==============================] - 1s 6ms/step - loss: 1.9095e-05 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9920 Epoch 148/400 79/79 [==============================] - 1s 6ms/step - loss: 1.7381e-05 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9924 Epoch 149/400 79/79 [==============================] - 1s 7ms/step - loss: 1.6414e-05 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9924 Epoch 150/400 79/79 [==============================] - 1s 6ms/step - loss: 1.5528e-05 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9924 Epoch 151/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4698e-05 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9924 Epoch 152/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4516e-05 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9924 Epoch 153/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3769e-05 - accuracy: 1.0000 - val_loss: 0.0585 - val_accuracy: 0.9920 Epoch 154/400 79/79 [==============================] - 0s 6ms/step - loss: 1.2615e-05 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 0.9920 Epoch 155/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2033e-05 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9920 Epoch 156/400 79/79 [==============================] - 1s 6ms/step - loss: 1.1480e-05 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9920 Epoch 157/400 79/79 [==============================] - 1s 6ms/step - loss: 1.1004e-05 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9920 Epoch 158/400 79/79 [==============================] - 1s 6ms/step - loss: 1.0427e-05 - accuracy: 1.0000 - val_loss: 0.0609 - val_accuracy: 0.9912 Epoch 159/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0029e-05 - accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 0.9912 Epoch 160/400 79/79 [==============================] - 1s 7ms/step - loss: 9.6064e-06 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9912 Epoch 161/400 79/79 [==============================] - 1s 7ms/step - loss: 9.2735e-06 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 0.9912 Epoch 162/400 79/79 [==============================] - 0s 6ms/step - loss: 8.9280e-06 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9908 Epoch 163/400 79/79 [==============================] - 1s 7ms/step - loss: 8.5592e-06 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9908 Epoch 164/400 79/79 [==============================] - 1s 7ms/step - loss: 8.1564e-06 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9908 Epoch 165/400 79/79 [==============================] - 1s 7ms/step - loss: 7.7459e-06 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9908 Epoch 166/400 79/79 [==============================] - 1s 7ms/step - loss: 7.4394e-06 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9908 Epoch 167/400 79/79 [==============================] - 1s 6ms/step - loss: 7.3749e-06 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9908 Epoch 168/400 79/79 [==============================] - 1s 7ms/step - loss: 6.8616e-06 - accuracy: 1.0000 - val_loss: 0.0657 - val_accuracy: 0.9908 Epoch 169/400 79/79 [==============================] - 1s 7ms/step - loss: 6.6022e-06 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9908 Epoch 170/400 79/79 [==============================] - 1s 7ms/step - loss: 6.3705e-06 - accuracy: 1.0000 - val_loss: 0.0667 - val_accuracy: 0.9908 Epoch 171/400 79/79 [==============================] - 1s 7ms/step - loss: 6.0982e-06 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 0.9908 Epoch 172/400 79/79 [==============================] - 1s 7ms/step - loss: 5.8814e-06 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 0.9908 Epoch 173/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6631e-06 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9908 Epoch 174/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6681e-06 - accuracy: 1.0000 - val_loss: 0.0688 - val_accuracy: 0.9908 Epoch 175/400 79/79 [==============================] - 1s 7ms/step - loss: 5.4060e-06 - accuracy: 1.0000 - val_loss: 0.0691 - val_accuracy: 0.9904 Epoch 176/400 79/79 [==============================] - 1s 7ms/step - loss: 5.0593e-06 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 0.9904 Epoch 177/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9132e-06 - accuracy: 1.0000 - val_loss: 0.0703 - val_accuracy: 0.9904 Epoch 178/400 79/79 [==============================] - 1s 7ms/step - loss: 4.8269e-06 - accuracy: 1.0000 - val_loss: 0.0707 - val_accuracy: 0.9904 Epoch 179/400 79/79 [==============================] - 1s 7ms/step - loss: 4.5163e-06 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9908 Epoch 180/400 79/79 [==============================] - 1s 7ms/step - loss: 4.3722e-06 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9908 Epoch 181/400 79/79 [==============================] - 1s 7ms/step - loss: 4.1614e-06 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9904 Epoch 182/400 79/79 [==============================] - 1s 7ms/step - loss: 4.0555e-06 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9904 Epoch 183/400 79/79 [==============================] - 1s 7ms/step - loss: 3.8702e-06 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9904 Epoch 184/400 79/79 [==============================] - 1s 6ms/step - loss: 3.7382e-06 - accuracy: 1.0000 - val_loss: 0.0741 - val_accuracy: 0.9904 Epoch 185/400 79/79 [==============================] - 1s 7ms/step - loss: 3.6140e-06 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 0.9904 Epoch 186/400 79/79 [==============================] - 1s 6ms/step - loss: 3.4893e-06 - accuracy: 1.0000 - val_loss: 0.0750 - val_accuracy: 0.9904 Epoch 187/400 79/79 [==============================] - 1s 7ms/step - loss: 3.4553e-06 - accuracy: 1.0000 - val_loss: 0.0757 - val_accuracy: 0.9900 Epoch 188/400 79/79 [==============================] - 1s 6ms/step - loss: 3.2320e-06 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9900 Epoch 189/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0963e-06 - accuracy: 1.0000 - val_loss: 0.0772 - val_accuracy: 0.9900 Epoch 190/400 79/79 [==============================] - 1s 6ms/step - loss: 2.9725e-06 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9900 Epoch 191/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8801e-06 - accuracy: 1.0000 - val_loss: 0.0783 - val_accuracy: 0.9900 Epoch 192/400 79/79 [==============================] - 0s 6ms/step - loss: 2.7571e-06 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9900 Epoch 193/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6647e-06 - accuracy: 1.0000 - val_loss: 0.0795 - val_accuracy: 0.9896 Epoch 194/400 79/79 [==============================] - 1s 6ms/step - loss: 2.5738e-06 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9896 Epoch 195/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4619e-06 - accuracy: 1.0000 - val_loss: 0.0809 - val_accuracy: 0.9896 Epoch 196/400 79/79 [==============================] - 0s 6ms/step - loss: 2.4338e-06 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9896 Epoch 197/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2797e-06 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9896 Epoch 198/400 79/79 [==============================] - 1s 6ms/step - loss: 2.2387e-06 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9896 Epoch 199/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1369e-06 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9896 Epoch 200/400 79/79 [==============================] - 1s 6ms/step - loss: 2.0409e-06 - accuracy: 1.0000 - val_loss: 0.0842 - val_accuracy: 0.9896 Epoch 201/400 79/79 [==============================] - 1s 7ms/step - loss: 2.0086e-06 - accuracy: 1.0000 - val_loss: 0.0848 - val_accuracy: 0.9896 Epoch 202/400 79/79 [==============================] - 1s 6ms/step - loss: 1.9156e-06 - accuracy: 1.0000 - val_loss: 0.0853 - val_accuracy: 0.9896 Epoch 203/400 79/79 [==============================] - 1s 7ms/step - loss: 1.9042e-06 - accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9896 Epoch 204/400 79/79 [==============================] - 1s 6ms/step - loss: 1.7815e-06 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9896 Epoch 205/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7301e-06 - accuracy: 1.0000 - val_loss: 0.0860 - val_accuracy: 0.9896 Epoch 206/400 79/79 [==============================] - 1s 7ms/step - loss: 1.6366e-06 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9896 Epoch 207/400 79/79 [==============================] - 1s 6ms/step - loss: 1.6629e-06 - accuracy: 1.0000 - val_loss: 0.0875 - val_accuracy: 0.9896 Epoch 208/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4986e-06 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 0.9896 Epoch 209/400 79/79 [==============================] - 0s 6ms/step - loss: 1.4416e-06 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9896 Epoch 210/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4065e-06 - accuracy: 1.0000 - val_loss: 0.0896 - val_accuracy: 0.9896 Epoch 211/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3245e-06 - accuracy: 1.0000 - val_loss: 0.0903 - val_accuracy: 0.9896 Epoch 212/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2749e-06 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9896 Epoch 213/400 79/79 [==============================] - 0s 6ms/step - loss: 1.2286e-06 - accuracy: 1.0000 - val_loss: 0.0915 - val_accuracy: 0.9896 Epoch 214/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2243e-06 - accuracy: 1.0000 - val_loss: 0.0920 - val_accuracy: 0.9896 Epoch 215/400 79/79 [==============================] - 0s 6ms/step - loss: 1.1306e-06 - accuracy: 1.0000 - val_loss: 0.0929 - val_accuracy: 0.9896 Epoch 216/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0883e-06 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9896 Epoch 217/400 79/79 [==============================] - 1s 6ms/step - loss: 1.0679e-06 - accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9896 Epoch 218/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0146e-06 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9896 Epoch 219/400 79/79 [==============================] - 1s 6ms/step - loss: 9.6506e-07 - accuracy: 1.0000 - val_loss: 0.0951 - val_accuracy: 0.9896 Epoch 220/400 79/79 [==============================] - 1s 7ms/step - loss: 9.4242e-07 - accuracy: 1.0000 - val_loss: 0.0955 - val_accuracy: 0.9896 Epoch 221/400 79/79 [==============================] - 1s 7ms/step - loss: 9.3247e-07 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9896 Epoch 222/400 79/79 [==============================] - 1s 7ms/step - loss: 9.1268e-07 - accuracy: 1.0000 - val_loss: 0.0965 - val_accuracy: 0.9896 Epoch 223/400 79/79 [==============================] - 1s 7ms/step - loss: 8.4512e-07 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 0.9896 Epoch 224/400 79/79 [==============================] - 1s 7ms/step - loss: 7.8692e-07 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9896 Epoch 225/400 79/79 [==============================] - 1s 7ms/step - loss: 7.5930e-07 - accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 0.9896 Epoch 226/400 79/79 [==============================] - 1s 11ms/step - loss: 7.3015e-07 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9896 Epoch 227/400 79/79 [==============================] - 1s 7ms/step - loss: 7.3000e-07 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9896 Epoch 228/400 79/79 [==============================] - 1s 7ms/step - loss: 6.6488e-07 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.9896 Epoch 229/400 79/79 [==============================] - 1s 7ms/step - loss: 6.4071e-07 - accuracy: 1.0000 - val_loss: 0.0997 - val_accuracy: 0.9896 Epoch 230/400 79/79 [==============================] - 1s 7ms/step - loss: 6.2024e-07 - accuracy: 1.0000 - val_loss: 0.1001 - val_accuracy: 0.9896 Epoch 231/400 79/79 [==============================] - 1s 7ms/step - loss: 5.8943e-07 - accuracy: 1.0000 - val_loss: 0.1007 - val_accuracy: 0.9896 Epoch 232/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6635e-07 - accuracy: 1.0000 - val_loss: 0.1010 - val_accuracy: 0.9896 Epoch 233/400 79/79 [==============================] - 1s 7ms/step - loss: 5.4311e-07 - accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 0.9896 Epoch 234/400 79/79 [==============================] - 1s 7ms/step - loss: 5.1889e-07 - accuracy: 1.0000 - val_loss: 0.1019 - val_accuracy: 0.9896 Epoch 235/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9914e-07 - accuracy: 1.0000 - val_loss: 0.1025 - val_accuracy: 0.9896 Epoch 236/400 79/79 [==============================] - 1s 7ms/step - loss: 4.8892e-07 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 0.9896 Epoch 237/400 79/79 [==============================] - 1s 6ms/step - loss: 4.5632e-07 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 0.9896 Epoch 238/400 79/79 [==============================] - 1s 7ms/step - loss: 4.3818e-07 - accuracy: 1.0000 - val_loss: 0.1036 - val_accuracy: 0.9900 Epoch 239/400 79/79 [==============================] - 1s 7ms/step - loss: 4.2312e-07 - accuracy: 1.0000 - val_loss: 0.1043 - val_accuracy: 0.9900 Epoch 240/400 79/79 [==============================] - 1s 7ms/step - loss: 4.0240e-07 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 0.9900 Epoch 241/400 79/79 [==============================] - 1s 6ms/step - loss: 3.8764e-07 - accuracy: 1.0000 - val_loss: 0.1052 - val_accuracy: 0.9900 Epoch 242/400 79/79 [==============================] - 1s 7ms/step - loss: 3.7113e-07 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 0.9900 Epoch 243/400 79/79 [==============================] - 1s 6ms/step - loss: 3.5447e-07 - accuracy: 1.0000 - val_loss: 0.1058 - val_accuracy: 0.9900 Epoch 244/400 79/79 [==============================] - 1s 7ms/step - loss: 3.4402e-07 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9900 Epoch 245/400 79/79 [==============================] - 1s 6ms/step - loss: 3.2618e-07 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 0.9900 Epoch 246/400 79/79 [==============================] - 1s 7ms/step - loss: 3.1365e-07 - accuracy: 1.0000 - val_loss: 0.1074 - val_accuracy: 0.9900 Epoch 247/400 79/79 [==============================] - 1s 7ms/step - loss: 2.9951e-07 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9900 Epoch 248/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8696e-07 - accuracy: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.9900 Epoch 249/400 79/79 [==============================] - 1s 7ms/step - loss: 2.7783e-07 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9900 Epoch 250/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6762e-07 - accuracy: 1.0000 - val_loss: 0.1092 - val_accuracy: 0.9900 Epoch 251/400 79/79 [==============================] - 1s 6ms/step - loss: 2.6602e-07 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9900 Epoch 252/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4770e-07 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9896 Epoch 253/400 79/79 [==============================] - 1s 7ms/step - loss: 2.3165e-07 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 0.9892 Epoch 254/400 79/79 [==============================] - 1s 6ms/step - loss: 2.2150e-07 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 0.9896 Epoch 255/400 79/79 [==============================] - 1s 6ms/step - loss: 2.1205e-07 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9896 Epoch 256/400 79/79 [==============================] - 1s 6ms/step - loss: 2.1201e-07 - accuracy: 1.0000 - val_loss: 0.1126 - val_accuracy: 0.9896 Epoch 257/400 79/79 [==============================] - 1s 6ms/step - loss: 1.9501e-07 - accuracy: 1.0000 - val_loss: 0.1123 - val_accuracy: 0.9896 Epoch 258/400 79/79 [==============================] - 1s 7ms/step - loss: 1.8666e-07 - accuracy: 1.0000 - val_loss: 0.1132 - val_accuracy: 0.9896 Epoch 259/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7893e-07 - accuracy: 1.0000 - val_loss: 0.1140 - val_accuracy: 0.9896 Epoch 260/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7109e-07 - accuracy: 1.0000 - val_loss: 0.1146 - val_accuracy: 0.9896 Epoch 261/400 79/79 [==============================] - 1s 6ms/step - loss: 1.6504e-07 - accuracy: 1.0000 - val_loss: 0.1154 - val_accuracy: 0.9896 Epoch 262/400 79/79 [==============================] - 1s 7ms/step - loss: 1.5974e-07 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9896 Epoch 263/400 79/79 [==============================] - 1s 6ms/step - loss: 1.5161e-07 - accuracy: 1.0000 - val_loss: 0.1162 - val_accuracy: 0.9896 Epoch 264/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4414e-07 - accuracy: 1.0000 - val_loss: 0.1169 - val_accuracy: 0.9896 Epoch 265/400 79/79 [==============================] - 1s 6ms/step - loss: 1.3805e-07 - accuracy: 1.0000 - val_loss: 0.1174 - val_accuracy: 0.9896 Epoch 266/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3314e-07 - accuracy: 1.0000 - val_loss: 0.1183 - val_accuracy: 0.9896 Epoch 267/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2723e-07 - accuracy: 1.0000 - val_loss: 0.1189 - val_accuracy: 0.9896 Epoch 268/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2212e-07 - accuracy: 1.0000 - val_loss: 0.1190 - val_accuracy: 0.9896 Epoch 269/400 79/79 [==============================] - 1s 7ms/step - loss: 1.1708e-07 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9892 Epoch 270/400 79/79 [==============================] - 1s 7ms/step - loss: 1.1297e-07 - accuracy: 1.0000 - val_loss: 0.1210 - val_accuracy: 0.9892 Epoch 271/400 79/79 [==============================] - 1s 6ms/step - loss: 1.4267e-07 - accuracy: 1.0000 - val_loss: 0.1218 - val_accuracy: 0.9892 Epoch 272/400 79/79 [==============================] - 1s 6ms/step - loss: 1.0165e-07 - accuracy: 1.0000 - val_loss: 0.1221 - val_accuracy: 0.9892 Epoch 273/400 79/79 [==============================] - 1s 7ms/step - loss: 9.6930e-08 - accuracy: 1.0000 - val_loss: 0.1238 - val_accuracy: 0.9892 Epoch 274/400 79/79 [==============================] - 1s 6ms/step - loss: 9.3161e-08 - accuracy: 1.0000 - val_loss: 0.1245 - val_accuracy: 0.9892 Epoch 275/400 79/79 [==============================] - 1s 7ms/step - loss: 8.9359e-08 - accuracy: 1.0000 - val_loss: 0.1236 - val_accuracy: 0.9892 Epoch 276/400 79/79 [==============================] - 1s 7ms/step - loss: 8.5516e-08 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 0.9892 Epoch 277/400 79/79 [==============================] - 1s 7ms/step - loss: 8.1970e-08 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 0.9892 Epoch 278/400 79/79 [==============================] - 1s 7ms/step - loss: 7.9408e-08 - accuracy: 1.0000 - val_loss: 0.1266 - val_accuracy: 0.9892 Epoch 279/400 79/79 [==============================] - 1s 7ms/step - loss: 7.9157e-08 - accuracy: 1.0000 - val_loss: 0.1276 - val_accuracy: 0.9892 Epoch 280/400 79/79 [==============================] - 1s 7ms/step - loss: 7.2570e-08 - accuracy: 1.0000 - val_loss: 0.1279 - val_accuracy: 0.9892 Epoch 281/400 79/79 [==============================] - 1s 7ms/step - loss: 6.9062e-08 - accuracy: 1.0000 - val_loss: 0.1283 - val_accuracy: 0.9892 Epoch 282/400 79/79 [==============================] - 1s 6ms/step - loss: 6.6571e-08 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9892 Epoch 283/400 79/79 [==============================] - 1s 7ms/step - loss: 6.3675e-08 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9892 Epoch 284/400 79/79 [==============================] - 1s 6ms/step - loss: 6.1242e-08 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9892 Epoch 285/400 79/79 [==============================] - 1s 7ms/step - loss: 5.8799e-08 - accuracy: 1.0000 - val_loss: 0.1310 - val_accuracy: 0.9892 Epoch 286/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6264e-08 - accuracy: 1.0000 - val_loss: 0.1314 - val_accuracy: 0.9892 Epoch 287/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6734e-08 - accuracy: 1.0000 - val_loss: 0.1321 - val_accuracy: 0.9892 Epoch 288/400 79/79 [==============================] - 1s 7ms/step - loss: 5.2411e-08 - accuracy: 1.0000 - val_loss: 0.1322 - val_accuracy: 0.9892 Epoch 289/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9885e-08 - accuracy: 1.0000 - val_loss: 0.1331 - val_accuracy: 0.9892 Epoch 290/400 79/79 [==============================] - 1s 6ms/step - loss: 4.8236e-08 - accuracy: 1.0000 - val_loss: 0.1334 - val_accuracy: 0.9892 Epoch 291/400 79/79 [==============================] - 1s 7ms/step - loss: 4.6550e-08 - accuracy: 1.0000 - val_loss: 0.1341 - val_accuracy: 0.9892 Epoch 292/400 79/79 [==============================] - 1s 7ms/step - loss: 4.4213e-08 - accuracy: 1.0000 - val_loss: 0.1346 - val_accuracy: 0.9892 Epoch 293/400 79/79 [==============================] - 1s 7ms/step - loss: 4.2812e-08 - accuracy: 1.0000 - val_loss: 0.1348 - val_accuracy: 0.9892 Epoch 294/400 79/79 [==============================] - 1s 7ms/step - loss: 4.2498e-08 - accuracy: 1.0000 - val_loss: 0.1352 - val_accuracy: 0.9892 Epoch 295/400 79/79 [==============================] - 1s 7ms/step - loss: 3.9641e-08 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9892 Epoch 296/400 79/79 [==============================] - 1s 7ms/step - loss: 3.8775e-08 - accuracy: 1.0000 - val_loss: 0.1343 - val_accuracy: 0.9896 Epoch 297/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9838e-08 - accuracy: 1.0000 - val_loss: 0.1330 - val_accuracy: 0.9896 Epoch 298/400 79/79 [==============================] - 1s 7ms/step - loss: 3.5308e-08 - accuracy: 1.0000 - val_loss: 0.1376 - val_accuracy: 0.9892 Epoch 299/400 79/79 [==============================] - 1s 7ms/step - loss: 5.0419e-08 - accuracy: 1.0000 - val_loss: 0.1292 - val_accuracy: 0.9896 Epoch 300/400 79/79 [==============================] - 1s 7ms/step - loss: 3.4353e-08 - accuracy: 1.0000 - val_loss: 0.1311 - val_accuracy: 0.9896 Epoch 301/400 79/79 [==============================] - 1s 7ms/step - loss: 3.2750e-08 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9896 Epoch 302/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0650e-08 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9896 Epoch 303/400 79/79 [==============================] - 1s 7ms/step - loss: 2.9290e-08 - accuracy: 1.0000 - val_loss: 0.1348 - val_accuracy: 0.9896 Epoch 304/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8035e-08 - accuracy: 1.0000 - val_loss: 0.1355 - val_accuracy: 0.9896 Epoch 305/400 79/79 [==============================] - 1s 6ms/step - loss: 2.6969e-08 - accuracy: 1.0000 - val_loss: 0.1362 - val_accuracy: 0.9896 Epoch 306/400 79/79 [==============================] - 1s 7ms/step - loss: 2.5546e-08 - accuracy: 1.0000 - val_loss: 0.1367 - val_accuracy: 0.9896 Epoch 307/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4577e-08 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9896 Epoch 308/400 79/79 [==============================] - 1s 7ms/step - loss: 2.3482e-08 - accuracy: 1.0000 - val_loss: 0.1369 - val_accuracy: 0.9896 Epoch 309/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2726e-08 - accuracy: 1.0000 - val_loss: 0.1374 - val_accuracy: 0.9896 Epoch 310/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1791e-08 - accuracy: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.9900 Epoch 311/400 79/79 [==============================] - 1s 7ms/step - loss: 2.0993e-08 - accuracy: 1.0000 - val_loss: 0.1385 - val_accuracy: 0.9900 Epoch 312/400 79/79 [==============================] - 1s 7ms/step - loss: 2.0128e-08 - accuracy: 1.0000 - val_loss: 0.1393 - val_accuracy: 0.9896 Epoch 313/400 79/79 [==============================] - 1s 7ms/step - loss: 1.9871e-08 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9896 Epoch 314/400 79/79 [==============================] - 1s 7ms/step - loss: 1.8920e-08 - accuracy: 1.0000 - val_loss: 0.1411 - val_accuracy: 0.9896 Epoch 315/400 79/79 [==============================] - 1s 7ms/step - loss: 1.8015e-08 - accuracy: 1.0000 - val_loss: 0.1421 - val_accuracy: 0.9896 Epoch 316/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7445e-08 - accuracy: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.9896 Epoch 317/400 79/79 [==============================] - 1s 7ms/step - loss: 1.7241e-08 - accuracy: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.9896 Epoch 318/400 79/79 [==============================] - 1s 6ms/step - loss: 1.6312e-08 - accuracy: 1.0000 - val_loss: 0.1443 - val_accuracy: 0.9896 Epoch 319/400 79/79 [==============================] - 1s 7ms/step - loss: 1.5645e-08 - accuracy: 1.0000 - val_loss: 0.1450 - val_accuracy: 0.9896 Epoch 320/400 79/79 [==============================] - 1s 7ms/step - loss: 1.5163e-08 - accuracy: 1.0000 - val_loss: 0.1456 - val_accuracy: 0.9896 Epoch 321/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4684e-08 - accuracy: 1.0000 - val_loss: 0.1459 - val_accuracy: 0.9896 Epoch 322/400 79/79 [==============================] - 1s 7ms/step - loss: 1.4123e-08 - accuracy: 1.0000 - val_loss: 0.1465 - val_accuracy: 0.9896 Epoch 323/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3666e-08 - accuracy: 1.0000 - val_loss: 0.1471 - val_accuracy: 0.9892 Epoch 324/400 79/79 [==============================] - 1s 7ms/step - loss: 1.3325e-08 - accuracy: 1.0000 - val_loss: 0.1476 - val_accuracy: 0.9892 Epoch 325/400 79/79 [==============================] - 1s 6ms/step - loss: 1.2906e-08 - accuracy: 1.0000 - val_loss: 0.1479 - val_accuracy: 0.9892 Epoch 326/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2509e-08 - accuracy: 1.0000 - val_loss: 0.1483 - val_accuracy: 0.9892 Epoch 327/400 79/79 [==============================] - 1s 7ms/step - loss: 1.2057e-08 - accuracy: 1.0000 - val_loss: 0.1484 - val_accuracy: 0.9892 Epoch 328/400 79/79 [==============================] - 1s 6ms/step - loss: 1.1698e-08 - accuracy: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.9892 Epoch 329/400 79/79 [==============================] - 1s 7ms/step - loss: 1.1402e-08 - accuracy: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.9892 Epoch 330/400 79/79 [==============================] - 1s 6ms/step - loss: 1.1036e-08 - accuracy: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.9892 Epoch 331/400 79/79 [==============================] - 1s 6ms/step - loss: 1.0763e-08 - accuracy: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.9892 Epoch 332/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0315e-08 - accuracy: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.9896 Epoch 333/400 79/79 [==============================] - 1s 7ms/step - loss: 1.0067e-08 - accuracy: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.9896 Epoch 334/400 79/79 [==============================] - 1s 7ms/step - loss: 9.7468e-09 - accuracy: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.9896 Epoch 335/400 79/79 [==============================] - 1s 7ms/step - loss: 9.4660e-09 - accuracy: 1.0000 - val_loss: 0.1500 - val_accuracy: 0.9896 Epoch 336/400 79/79 [==============================] - 1s 6ms/step - loss: 9.1215e-09 - accuracy: 1.0000 - val_loss: 0.1503 - val_accuracy: 0.9896 Epoch 337/400 79/79 [==============================] - 1s 6ms/step - loss: 8.9241e-09 - accuracy: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.9896 Epoch 338/400 79/79 [==============================] - 1s 7ms/step - loss: 8.6136e-09 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9896 Epoch 339/400 79/79 [==============================] - 1s 7ms/step - loss: 8.3407e-09 - accuracy: 1.0000 - val_loss: 0.1507 - val_accuracy: 0.9896 Epoch 340/400 79/79 [==============================] - 1s 6ms/step - loss: 8.1220e-09 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9896 Epoch 341/400 79/79 [==============================] - 1s 7ms/step - loss: 7.9070e-09 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9896 Epoch 342/400 79/79 [==============================] - 1s 6ms/step - loss: 7.6520e-09 - accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9896 Epoch 343/400 79/79 [==============================] - 1s 7ms/step - loss: 7.4509e-09 - accuracy: 1.0000 - val_loss: 0.1514 - val_accuracy: 0.9896 Epoch 344/400 79/79 [==============================] - 1s 7ms/step - loss: 7.2214e-09 - accuracy: 1.0000 - val_loss: 0.1515 - val_accuracy: 0.9896 Epoch 345/400 79/79 [==============================] - 1s 7ms/step - loss: 7.0713e-09 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9896 Epoch 346/400 79/79 [==============================] - 1s 7ms/step - loss: 6.8703e-09 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9900 Epoch 347/400 79/79 [==============================] - 1s 7ms/step - loss: 6.6697e-09 - accuracy: 1.0000 - val_loss: 0.1522 - val_accuracy: 0.9900 Epoch 348/400 79/79 [==============================] - 1s 6ms/step - loss: 6.4546e-09 - accuracy: 1.0000 - val_loss: 0.1524 - val_accuracy: 0.9900 Epoch 349/400 79/79 [==============================] - 1s 7ms/step - loss: 6.2903e-09 - accuracy: 1.0000 - val_loss: 0.1525 - val_accuracy: 0.9900 Epoch 350/400 79/79 [==============================] - 0s 6ms/step - loss: 6.1464e-09 - accuracy: 1.0000 - val_loss: 0.1528 - val_accuracy: 0.9900 Epoch 351/400 79/79 [==============================] - 1s 7ms/step - loss: 6.1079e-09 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9900 Epoch 352/400 79/79 [==============================] - 1s 7ms/step - loss: 5.7918e-09 - accuracy: 1.0000 - val_loss: 0.1534 - val_accuracy: 0.9900 Epoch 353/400 79/79 [==============================] - 1s 7ms/step - loss: 5.6367e-09 - accuracy: 1.0000 - val_loss: 0.1537 - val_accuracy: 0.9900 Epoch 354/400 79/79 [==============================] - 1s 7ms/step - loss: 5.4914e-09 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 0.9900 Epoch 355/400 79/79 [==============================] - 1s 7ms/step - loss: 5.4113e-09 - accuracy: 1.0000 - val_loss: 0.1541 - val_accuracy: 0.9900 Epoch 356/400 79/79 [==============================] - 1s 7ms/step - loss: 5.2781e-09 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.9900 Epoch 357/400 79/79 [==============================] - 1s 7ms/step - loss: 5.0791e-09 - accuracy: 1.0000 - val_loss: 0.1546 - val_accuracy: 0.9900 Epoch 358/400 79/79 [==============================] - 1s 7ms/step - loss: 4.9753e-09 - accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9900 Epoch 359/400 79/79 [==============================] - 1s 7ms/step - loss: 4.8996e-09 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9900 Epoch 360/400 79/79 [==============================] - 1s 7ms/step - loss: 4.7424e-09 - accuracy: 1.0000 - val_loss: 0.1551 - val_accuracy: 0.9900 Epoch 361/400 79/79 [==============================] - 1s 7ms/step - loss: 4.6247e-09 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9900 Epoch 362/400 79/79 [==============================] - 1s 7ms/step - loss: 4.6962e-09 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9900 Epoch 363/400 79/79 [==============================] - 1s 7ms/step - loss: 4.3938e-09 - accuracy: 1.0000 - val_loss: 0.1558 - val_accuracy: 0.9900 Epoch 364/400 79/79 [==============================] - 1s 7ms/step - loss: 4.3013e-09 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9900 Epoch 365/400 79/79 [==============================] - 1s 7ms/step - loss: 4.1931e-09 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9900 Epoch 366/400 79/79 [==============================] - 1s 7ms/step - loss: 4.1789e-09 - accuracy: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.9900 Epoch 367/400 79/79 [==============================] - 1s 7ms/step - loss: 3.9860e-09 - accuracy: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.9900 Epoch 368/400 79/79 [==============================] - 1s 7ms/step - loss: 3.8786e-09 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9900 Epoch 369/400 79/79 [==============================] - 1s 7ms/step - loss: 3.7894e-09 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9900 Epoch 370/400 79/79 [==============================] - 1s 7ms/step - loss: 3.7430e-09 - accuracy: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.9900 Epoch 371/400 79/79 [==============================] - 1s 7ms/step - loss: 3.6241e-09 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9900 Epoch 372/400 79/79 [==============================] - 1s 7ms/step - loss: 3.5539e-09 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9900 Epoch 373/400 79/79 [==============================] - 1s 7ms/step - loss: 3.4580e-09 - accuracy: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.9900 Epoch 374/400 79/79 [==============================] - 1s 7ms/step - loss: 3.3896e-09 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9900 Epoch 375/400 79/79 [==============================] - 1s 7ms/step - loss: 3.3307e-09 - accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9900 Epoch 376/400 79/79 [==============================] - 1s 7ms/step - loss: 3.2459e-09 - accuracy: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.9900 Epoch 377/400 79/79 [==============================] - 1s 7ms/step - loss: 3.2133e-09 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9900 Epoch 378/400 79/79 [==============================] - 1s 7ms/step - loss: 3.1123e-09 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9900 Epoch 379/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0661e-09 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9900 Epoch 380/400 79/79 [==============================] - 1s 7ms/step - loss: 3.0002e-09 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9900 Epoch 381/400 79/79 [==============================] - 1s 7ms/step - loss: 2.9334e-09 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9900 Epoch 382/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8723e-09 - accuracy: 1.0000 - val_loss: 0.1586 - val_accuracy: 0.9900 Epoch 383/400 79/79 [==============================] - 1s 7ms/step - loss: 2.8174e-09 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9900 Epoch 384/400 79/79 [==============================] - 1s 7ms/step - loss: 2.7735e-09 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9900 Epoch 385/400 79/79 [==============================] - 1s 7ms/step - loss: 2.7304e-09 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9900 Epoch 386/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6646e-09 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9900 Epoch 387/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6190e-09 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9900 Epoch 388/400 79/79 [==============================] - 1s 7ms/step - loss: 2.5701e-09 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9900 Epoch 389/400 79/79 [==============================] - 1s 7ms/step - loss: 2.6549e-09 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9900 Epoch 390/400 79/79 [==============================] - 1s 6ms/step - loss: 2.4742e-09 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9900 Epoch 391/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4180e-09 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9900 Epoch 392/400 79/79 [==============================] - 1s 7ms/step - loss: 2.3913e-09 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9900 Epoch 393/400 79/79 [==============================] - 1s 7ms/step - loss: 2.4473e-09 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9900 Epoch 394/400 79/79 [==============================] - 1s 6ms/step - loss: 2.2954e-09 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9900 Epoch 395/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2595e-09 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9900 Epoch 396/400 79/79 [==============================] - 1s 7ms/step - loss: 2.2090e-09 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9900 Epoch 397/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1746e-09 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9900 Epoch 398/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1427e-09 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9900 Epoch 399/400 79/79 [==============================] - 1s 7ms/step - loss: 2.1051e-09 - accuracy: 1.0000 - val_loss: 0.1602 - val_accuracy: 0.9900 Epoch 400/400 79/79 [==============================] - 1s 7ms/step - loss: 2.0643e-09 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9900 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec1e9abf60> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec20e9fa20> # Now try a LSTM with Global Max Pooling inputs = np . expand_dims ( X , - 1 ) # make the RNN i = Input ( shape = ( T , D )) # method 2 x = LSTM ( 5 , return_sequences = True )( i ) x = GlobalMaxPool1D ()( x ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.01 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( inputs , Y , epochs = 100 , validation_split = 0.5 , ) Epoch 1/100 79/79 [==============================] - 1s 10ms/step - loss: 0.6922 - accuracy: 0.5128 - val_loss: 0.6952 - val_accuracy: 0.4960 Epoch 2/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6920 - accuracy: 0.5256 - val_loss: 0.6938 - val_accuracy: 0.4916 Epoch 3/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6920 - accuracy: 0.5256 - val_loss: 0.6938 - val_accuracy: 0.4916 Epoch 4/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6916 - accuracy: 0.5188 - val_loss: 0.6934 - val_accuracy: 0.4920 Epoch 5/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6906 - accuracy: 0.5200 - val_loss: 0.7001 - val_accuracy: 0.4960 Epoch 6/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6898 - accuracy: 0.5216 - val_loss: 0.6995 - val_accuracy: 0.4924 Epoch 7/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6900 - accuracy: 0.5120 - val_loss: 0.7016 - val_accuracy: 0.4916 Epoch 8/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6862 - accuracy: 0.5516 - val_loss: 0.6866 - val_accuracy: 0.5744 Epoch 9/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6788 - accuracy: 0.5668 - val_loss: 0.6845 - val_accuracy: 0.5476 Epoch 10/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6640 - accuracy: 0.6072 - val_loss: 0.6606 - val_accuracy: 0.6100 Epoch 11/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6728 - accuracy: 0.5720 - val_loss: 0.6956 - val_accuracy: 0.5208 Epoch 12/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6911 - accuracy: 0.5244 - val_loss: 0.6947 - val_accuracy: 0.5100 Epoch 13/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6892 - accuracy: 0.5428 - val_loss: 0.6953 - val_accuracy: 0.5160 Epoch 14/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6879 - accuracy: 0.5524 - val_loss: 0.6920 - val_accuracy: 0.5348 Epoch 15/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6851 - accuracy: 0.5516 - val_loss: 0.6950 - val_accuracy: 0.5072 Epoch 16/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6837 - accuracy: 0.5624 - val_loss: 0.6968 - val_accuracy: 0.5120 Epoch 17/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6834 - accuracy: 0.5360 - val_loss: 0.6892 - val_accuracy: 0.5148 Epoch 18/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6794 - accuracy: 0.5340 - val_loss: 0.6846 - val_accuracy: 0.5392 Epoch 19/100 79/79 [==============================] - 1s 6ms/step - loss: 0.6782 - accuracy: 0.5444 - val_loss: 0.6825 - val_accuracy: 0.5424 Epoch 20/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6752 - accuracy: 0.5480 - val_loss: 0.6802 - val_accuracy: 0.5260 Epoch 21/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6711 - accuracy: 0.5364 - val_loss: 0.6748 - val_accuracy: 0.5260 Epoch 22/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6654 - accuracy: 0.5416 - val_loss: 0.6752 - val_accuracy: 0.5176 Epoch 23/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6588 - accuracy: 0.5520 - val_loss: 0.6679 - val_accuracy: 0.5388 Epoch 24/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6531 - accuracy: 0.5648 - val_loss: 0.6652 - val_accuracy: 0.5416 Epoch 25/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6462 - accuracy: 0.5728 - val_loss: 0.6525 - val_accuracy: 0.5784 Epoch 26/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6396 - accuracy: 0.5880 - val_loss: 0.6378 - val_accuracy: 0.6028 Epoch 27/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6258 - accuracy: 0.6152 - val_loss: 0.6318 - val_accuracy: 0.6208 Epoch 28/100 79/79 [==============================] - 1s 7ms/step - loss: 0.6056 - accuracy: 0.6636 - val_loss: 0.5923 - val_accuracy: 0.6812 Epoch 29/100 79/79 [==============================] - 1s 7ms/step - loss: 0.5701 - accuracy: 0.7096 - val_loss: 0.5999 - val_accuracy: 0.6684 Epoch 30/100 79/79 [==============================] - 1s 7ms/step - loss: 0.5357 - accuracy: 0.7500 - val_loss: 0.5204 - val_accuracy: 0.7716 Epoch 31/100 79/79 [==============================] - 1s 7ms/step - loss: 0.5005 - accuracy: 0.7652 - val_loss: 0.5802 - val_accuracy: 0.7012 Epoch 32/100 79/79 [==============================] - 1s 7ms/step - loss: 0.5468 - accuracy: 0.7332 - val_loss: 0.5303 - val_accuracy: 0.7600 Epoch 33/100 79/79 [==============================] - 1s 7ms/step - loss: 0.4894 - accuracy: 0.7704 - val_loss: 0.4789 - val_accuracy: 0.8024 Epoch 34/100 79/79 [==============================] - 1s 6ms/step - loss: 0.4361 - accuracy: 0.8068 - val_loss: 0.4265 - val_accuracy: 0.8304 Epoch 35/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3958 - accuracy: 0.8424 - val_loss: 0.4039 - val_accuracy: 0.8512 Epoch 36/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3591 - accuracy: 0.8604 - val_loss: 0.3483 - val_accuracy: 0.8824 Epoch 37/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3223 - accuracy: 0.8788 - val_loss: 0.3191 - val_accuracy: 0.8828 Epoch 38/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3949 - accuracy: 0.8412 - val_loss: 0.3870 - val_accuracy: 0.8552 Epoch 39/100 79/79 [==============================] - 1s 7ms/step - loss: 0.3276 - accuracy: 0.8852 - val_loss: 0.3033 - val_accuracy: 0.9040 Epoch 40/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2861 - accuracy: 0.8992 - val_loss: 0.2872 - val_accuracy: 0.8984 Epoch 41/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2709 - accuracy: 0.9060 - val_loss: 0.2601 - val_accuracy: 0.9104 Epoch 42/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2476 - accuracy: 0.9088 - val_loss: 0.2502 - val_accuracy: 0.9128 Epoch 43/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2438 - accuracy: 0.9152 - val_loss: 0.2460 - val_accuracy: 0.9124 Epoch 44/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2410 - accuracy: 0.9128 - val_loss: 0.2703 - val_accuracy: 0.9072 Epoch 45/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2333 - accuracy: 0.9188 - val_loss: 0.2288 - val_accuracy: 0.9252 Epoch 46/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2147 - accuracy: 0.9244 - val_loss: 0.2322 - val_accuracy: 0.9192 Epoch 47/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2729 - accuracy: 0.8940 - val_loss: 0.3241 - val_accuracy: 0.8808 Epoch 48/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2782 - accuracy: 0.8868 - val_loss: 0.2404 - val_accuracy: 0.9212 Epoch 49/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2247 - accuracy: 0.9220 - val_loss: 0.2149 - val_accuracy: 0.9364 Epoch 50/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2006 - accuracy: 0.9340 - val_loss: 0.1975 - val_accuracy: 0.9432 Epoch 51/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1819 - accuracy: 0.9424 - val_loss: 0.1847 - val_accuracy: 0.9492 Epoch 52/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1808 - accuracy: 0.9400 - val_loss: 0.1770 - val_accuracy: 0.9460 Epoch 53/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1819 - accuracy: 0.9396 - val_loss: 0.1812 - val_accuracy: 0.9496 Epoch 54/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1610 - accuracy: 0.9484 - val_loss: 0.1729 - val_accuracy: 0.9516 Epoch 55/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1583 - accuracy: 0.9480 - val_loss: 0.1687 - val_accuracy: 0.9524 Epoch 56/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1474 - accuracy: 0.9484 - val_loss: 0.1795 - val_accuracy: 0.9452 Epoch 57/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1531 - accuracy: 0.9440 - val_loss: 0.1821 - val_accuracy: 0.9504 Epoch 58/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1545 - accuracy: 0.9488 - val_loss: 0.1570 - val_accuracy: 0.9532 Epoch 59/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1374 - accuracy: 0.9520 - val_loss: 0.1556 - val_accuracy: 0.9528 Epoch 60/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1329 - accuracy: 0.9516 - val_loss: 0.1441 - val_accuracy: 0.9580 Epoch 61/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1282 - accuracy: 0.9556 - val_loss: 0.1637 - val_accuracy: 0.9556 Epoch 62/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1489 - accuracy: 0.9568 - val_loss: 0.1428 - val_accuracy: 0.9516 Epoch 63/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1164 - accuracy: 0.9588 - val_loss: 0.1803 - val_accuracy: 0.9516 Epoch 64/100 79/79 [==============================] - 1s 7ms/step - loss: 0.2297 - accuracy: 0.9368 - val_loss: 0.1882 - val_accuracy: 0.9500 Epoch 65/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1285 - accuracy: 0.9552 - val_loss: 0.1299 - val_accuracy: 0.9576 Epoch 66/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1113 - accuracy: 0.9624 - val_loss: 0.1390 - val_accuracy: 0.9596 Epoch 67/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1054 - accuracy: 0.9660 - val_loss: 0.1181 - val_accuracy: 0.9628 Epoch 68/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1078 - accuracy: 0.9672 - val_loss: 0.1155 - val_accuracy: 0.9640 Epoch 69/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1340 - accuracy: 0.9636 - val_loss: 0.1945 - val_accuracy: 0.9488 Epoch 70/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1208 - accuracy: 0.9564 - val_loss: 0.1208 - val_accuracy: 0.9636 Epoch 71/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1188 - accuracy: 0.9604 - val_loss: 0.1047 - val_accuracy: 0.9664 Epoch 72/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1027 - accuracy: 0.9660 - val_loss: 0.1066 - val_accuracy: 0.9668 Epoch 73/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0941 - accuracy: 0.9736 - val_loss: 0.0951 - val_accuracy: 0.9708 Epoch 74/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0874 - accuracy: 0.9716 - val_loss: 0.0990 - val_accuracy: 0.9668 Epoch 75/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0855 - accuracy: 0.9720 - val_loss: 0.1011 - val_accuracy: 0.9688 Epoch 76/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1028 - accuracy: 0.9692 - val_loss: 0.1240 - val_accuracy: 0.9628 Epoch 77/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0825 - accuracy: 0.9720 - val_loss: 0.1017 - val_accuracy: 0.9736 Epoch 78/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0786 - accuracy: 0.9748 - val_loss: 0.0949 - val_accuracy: 0.9704 Epoch 79/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0777 - accuracy: 0.9764 - val_loss: 0.1001 - val_accuracy: 0.9728 Epoch 80/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0846 - accuracy: 0.9736 - val_loss: 0.0992 - val_accuracy: 0.9672 Epoch 81/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0718 - accuracy: 0.9804 - val_loss: 0.0954 - val_accuracy: 0.9700 Epoch 82/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0698 - accuracy: 0.9788 - val_loss: 0.0893 - val_accuracy: 0.9728 Epoch 83/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0633 - accuracy: 0.9820 - val_loss: 0.0924 - val_accuracy: 0.9732 Epoch 84/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0630 - accuracy: 0.9816 - val_loss: 0.0772 - val_accuracy: 0.9760 Epoch 85/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1032 - accuracy: 0.9664 - val_loss: 0.0859 - val_accuracy: 0.9744 Epoch 86/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0643 - accuracy: 0.9812 - val_loss: 0.0761 - val_accuracy: 0.9744 Epoch 87/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0599 - accuracy: 0.9828 - val_loss: 0.0722 - val_accuracy: 0.9756 Epoch 88/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0601 - accuracy: 0.9812 - val_loss: 0.0712 - val_accuracy: 0.9788 Epoch 89/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0545 - accuracy: 0.9816 - val_loss: 0.0688 - val_accuracy: 0.9780 Epoch 90/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0505 - accuracy: 0.9848 - val_loss: 0.0739 - val_accuracy: 0.9796 Epoch 91/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0502 - accuracy: 0.9848 - val_loss: 0.0864 - val_accuracy: 0.9764 Epoch 92/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.0678 - val_accuracy: 0.9780 Epoch 93/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0572 - accuracy: 0.9800 - val_loss: 0.0756 - val_accuracy: 0.9772 Epoch 94/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0530 - accuracy: 0.9804 - val_loss: 0.0852 - val_accuracy: 0.9780 Epoch 95/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0543 - accuracy: 0.9796 - val_loss: 0.0818 - val_accuracy: 0.9776 Epoch 96/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0487 - accuracy: 0.9840 - val_loss: 0.0531 - val_accuracy: 0.9792 Epoch 97/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0526 - accuracy: 0.9852 - val_loss: 0.0644 - val_accuracy: 0.9804 Epoch 98/100 79/79 [==============================] - 1s 7ms/step - loss: 0.0445 - accuracy: 0.9856 - val_loss: 0.0567 - val_accuracy: 0.9796 Epoch 99/100 79/79 [==============================] - 1s 7ms/step - loss: 0.1566 - accuracy: 0.9680 - val_loss: 0.4206 - val_accuracy: 0.8924 Epoch 100/100 79/79 [==============================] - 1s 7ms/step - loss: 1.0968 - accuracy: 0.7592 - val_loss: 1.0851 - val_accuracy: 0.6596 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fec209687b8> # Plot the accuracy too plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fec20a92780> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Long Distance"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_MNIST/","text":"================ by Jawad Haider RNN MNIST RNN MNIST \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348.9MB 70kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 33.3MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 41.2MB/s 2.0.0-beta1 # Load in the data mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # Build the model i = Input ( shape = x_train [ 0 ] . shape ) x = LSTM ( 128 )( i ) x = Dense ( 10 , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile and train model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 10 ) WARNING: Logging before flag parsing goes to stderr. W0803 17:03:38.451040 140542585788288 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 25s 411us/sample - loss: 0.3334 - accuracy: 0.8929 - val_loss: 0.1187 - val_accuracy: 0.9629 Epoch 2/10 60000/60000 [==============================] - 21s 354us/sample - loss: 0.1046 - accuracy: 0.9692 - val_loss: 0.0792 - val_accuracy: 0.9759 Epoch 3/10 60000/60000 [==============================] - 21s 357us/sample - loss: 0.0753 - accuracy: 0.9774 - val_loss: 0.0770 - val_accuracy: 0.9756 Epoch 4/10 60000/60000 [==============================] - 22s 359us/sample - loss: 0.0576 - accuracy: 0.9827 - val_loss: 0.0601 - val_accuracy: 0.9803 Epoch 5/10 60000/60000 [==============================] - 21s 358us/sample - loss: 0.0453 - accuracy: 0.9862 - val_loss: 0.0525 - val_accuracy: 0.9833 Epoch 6/10 60000/60000 [==============================] - 21s 353us/sample - loss: 0.0395 - accuracy: 0.9877 - val_loss: 0.0448 - val_accuracy: 0.9864 Epoch 7/10 60000/60000 [==============================] - 21s 352us/sample - loss: 0.0331 - accuracy: 0.9897 - val_loss: 0.0476 - val_accuracy: 0.9857 Epoch 8/10 60000/60000 [==============================] - 21s 353us/sample - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.0416 - val_accuracy: 0.9884 Epoch 9/10 60000/60000 [==============================] - 21s 357us/sample - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.0397 - val_accuracy: 0.9887 Epoch 10/10 60000/60000 [==============================] - 21s 356us/sample - loss: 0.0228 - accuracy: 0.9927 - val_loss: 0.0409 - val_accuracy: 0.9881 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fd1e8624a58> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fd1e5dd0358> # Plot confusion matrix from sklearn.metrics import confusion_matrix import numpy as np import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) # Do these results make sense? # It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc. Confusion matrix, without normalization [[ 975 0 2 0 0 0 2 1 0 0] [ 0 1129 1 1 0 0 1 3 0 0] [ 0 0 1019 1 1 0 0 8 3 0] [ 0 0 2 1001 0 3 0 2 0 2] [ 0 0 1 0 965 0 2 3 1 10] [ 0 0 0 8 0 881 1 2 0 0] [ 3 2 0 1 2 2 948 0 0 0] [ 0 4 1 1 0 0 0 1019 1 2] [ 2 0 2 5 0 9 0 2 954 0] [ 0 0 0 6 5 3 0 2 3 990]] # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( y_test [ i ], p_test [ i ])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 RNN MNIST"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_MNIST/#rnn-mnist","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348.9MB 70kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 33.3MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 41.2MB/s 2.0.0-beta1 # Load in the data mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # Build the model i = Input ( shape = x_train [ 0 ] . shape ) x = LSTM ( 128 )( i ) x = Dense ( 10 , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile and train model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 10 ) WARNING: Logging before flag parsing goes to stderr. W0803 17:03:38.451040 140542585788288 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 25s 411us/sample - loss: 0.3334 - accuracy: 0.8929 - val_loss: 0.1187 - val_accuracy: 0.9629 Epoch 2/10 60000/60000 [==============================] - 21s 354us/sample - loss: 0.1046 - accuracy: 0.9692 - val_loss: 0.0792 - val_accuracy: 0.9759 Epoch 3/10 60000/60000 [==============================] - 21s 357us/sample - loss: 0.0753 - accuracy: 0.9774 - val_loss: 0.0770 - val_accuracy: 0.9756 Epoch 4/10 60000/60000 [==============================] - 22s 359us/sample - loss: 0.0576 - accuracy: 0.9827 - val_loss: 0.0601 - val_accuracy: 0.9803 Epoch 5/10 60000/60000 [==============================] - 21s 358us/sample - loss: 0.0453 - accuracy: 0.9862 - val_loss: 0.0525 - val_accuracy: 0.9833 Epoch 6/10 60000/60000 [==============================] - 21s 353us/sample - loss: 0.0395 - accuracy: 0.9877 - val_loss: 0.0448 - val_accuracy: 0.9864 Epoch 7/10 60000/60000 [==============================] - 21s 352us/sample - loss: 0.0331 - accuracy: 0.9897 - val_loss: 0.0476 - val_accuracy: 0.9857 Epoch 8/10 60000/60000 [==============================] - 21s 353us/sample - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.0416 - val_accuracy: 0.9884 Epoch 9/10 60000/60000 [==============================] - 21s 357us/sample - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.0397 - val_accuracy: 0.9887 Epoch 10/10 60000/60000 [==============================] - 21s 356us/sample - loss: 0.0228 - accuracy: 0.9927 - val_loss: 0.0409 - val_accuracy: 0.9881 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7fd1e8624a58> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7fd1e5dd0358> # Plot confusion matrix from sklearn.metrics import confusion_matrix import numpy as np import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) # Do these results make sense? # It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc. Confusion matrix, without normalization [[ 975 0 2 0 0 0 2 1 0 0] [ 0 1129 1 1 0 0 1 3 0 0] [ 0 0 1019 1 1 0 0 8 3 0] [ 0 0 2 1001 0 3 0 2 0 2] [ 0 0 1 0 965 0 2 3 1 10] [ 0 0 0 8 0 881 1 2 0 0] [ 3 2 0 1 2 2 948 0 0 0] [ 0 4 1 1 0 0 0 1019 1 2] [ 2 0 2 5 0 9 0 2 954 0] [ 0 0 0 6 5 3 0 2 3 990]] # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( y_test [ i ], p_test [ i ])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"RNN MNIST"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_Shapes/","text":"================ by Jawad Haider RNN Shapes RNN Shapes \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348.9MB 45kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 42.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 43.5MB/s 2.0.0-beta1 from tensorflow.keras.layers import Input , SimpleRNN , Dense , Flatten from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt # Things you should automatically know and have memorized # N = number of samples # T = sequence length # D = number of input features # M = number of hidden units # K = number of output units # Make some data N = 1 T = 10 D = 3 K = 2 X = np . random . randn ( N , T , D ) # Make an RNN M = 5 # number of hidden units i = Input ( shape = ( T , D )) x = SimpleRNN ( M )( i ) x = Dense ( K )( x ) model = Model ( i , x ) # Get the output Yhat = model . predict ( X ) print ( Yhat ) [[-0.7062384 0.45167243]] # See if we can replicate this output # Get the weights first model . summary () Model: \"model_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 10, 3)] 0 _________________________________________________________________ simple_rnn_1 (SimpleRNN) (None, 5) 45 _________________________________________________________________ dense_1 (Dense) (None, 2) 12 ================================================================= Total params: 57 Trainable params: 57 Non-trainable params: 0 _________________________________________________________________ # See what's returned model . layers [ 1 ] . get_weights () [array([[ 0.06160122, 0.16070706, 0.83621055, 0.04993761, -0.36932853], [ 0.4978891 , -0.474034 , 0.55890614, 0.06967717, 0.21268493], [-0.44685632, -0.28297323, -0.17539108, 0.42829865, 0.22275227]], dtype=float32), array([[ 0.00272548, 0.04928541, 0.32022277, 0.3270029 , 0.88774437], [ 0.6996881 , 0.64928424, -0.08133215, -0.27187836, 0.09128988], [-0.22173485, 0.50949985, 0.6649476 , 0.31805265, -0.38461757], [ 0.5346833 , -0.24025255, -0.13355102, 0.7674074 , -0.22280595], [ 0.41877976, -0.50861543, 0.65639263, -0.35927662, -0.07747886]], dtype=float32), array([0., 0., 0., 0., 0.], dtype=float32)] # Check their shapes # Should make sense # First output is input > hidden # Second output is hidden > hidden # Third output is bias term (vector of length M) a , b , c = model . layers [ 1 ] . get_weights () print ( a . shape , b . shape , c . shape ) (3, 5) (5, 5) (5,) Wx , Wh , bh = model . layers [ 1 ] . get_weights () Wo , bo = model . layers [ 2 ] . get_weights () h_last = np . zeros ( M ) # initial hidden state x = X [ 0 ] # the one and only sample Yhats = [] # where we store the outputs for t in range ( T ): h = np . tanh ( x [ t ] . dot ( Wx ) + h_last . dot ( Wh ) + bh ) y = h . dot ( Wo ) + bo # we only care about this value on the last iteration Yhats . append ( y ) # important: assign h to h_last h_last = h # print the final output print ( Yhats [ - 1 ]) [-0.70623848 0.45167215] # Bonus exercise: calculate the output for multiple samples at once (N > 1) Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 RNN Shapes"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_Shapes/#rnn-shapes","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348.9MB 45kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 42.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 43.5MB/s 2.0.0-beta1 from tensorflow.keras.layers import Input , SimpleRNN , Dense , Flatten from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt # Things you should automatically know and have memorized # N = number of samples # T = sequence length # D = number of input features # M = number of hidden units # K = number of output units # Make some data N = 1 T = 10 D = 3 K = 2 X = np . random . randn ( N , T , D ) # Make an RNN M = 5 # number of hidden units i = Input ( shape = ( T , D )) x = SimpleRNN ( M )( i ) x = Dense ( K )( x ) model = Model ( i , x ) # Get the output Yhat = model . predict ( X ) print ( Yhat ) [[-0.7062384 0.45167243]] # See if we can replicate this output # Get the weights first model . summary () Model: \"model_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 10, 3)] 0 _________________________________________________________________ simple_rnn_1 (SimpleRNN) (None, 5) 45 _________________________________________________________________ dense_1 (Dense) (None, 2) 12 ================================================================= Total params: 57 Trainable params: 57 Non-trainable params: 0 _________________________________________________________________ # See what's returned model . layers [ 1 ] . get_weights () [array([[ 0.06160122, 0.16070706, 0.83621055, 0.04993761, -0.36932853], [ 0.4978891 , -0.474034 , 0.55890614, 0.06967717, 0.21268493], [-0.44685632, -0.28297323, -0.17539108, 0.42829865, 0.22275227]], dtype=float32), array([[ 0.00272548, 0.04928541, 0.32022277, 0.3270029 , 0.88774437], [ 0.6996881 , 0.64928424, -0.08133215, -0.27187836, 0.09128988], [-0.22173485, 0.50949985, 0.6649476 , 0.31805265, -0.38461757], [ 0.5346833 , -0.24025255, -0.13355102, 0.7674074 , -0.22280595], [ 0.41877976, -0.50861543, 0.65639263, -0.35927662, -0.07747886]], dtype=float32), array([0., 0., 0., 0., 0.], dtype=float32)] # Check their shapes # Should make sense # First output is input > hidden # Second output is hidden > hidden # Third output is bias term (vector of length M) a , b , c = model . layers [ 1 ] . get_weights () print ( a . shape , b . shape , c . shape ) (3, 5) (5, 5) (5,) Wx , Wh , bh = model . layers [ 1 ] . get_weights () Wo , bo = model . layers [ 2 ] . get_weights () h_last = np . zeros ( M ) # initial hidden state x = X [ 0 ] # the one and only sample Yhats = [] # where we store the outputs for t in range ( T ): h = np . tanh ( x [ t ] . dot ( Wx ) + h_last . dot ( Wh ) + bh ) y = h . dot ( Wo ) + bo # we only care about this value on the last iteration Yhats . append ( y ) # important: assign h to h_last h_last = h # print the final output print ( Yhats [ - 1 ]) [-0.70623848 0.45167215] # Bonus exercise: calculate the output for multiple samples at once (N > 1) Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"RNN Shapes"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_SimpleRNN_Sine/","text":"================ by Jawad Haider Simple RNN (sine) Simple RNN (sine) \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow is already loaded. Please restart the runtime to change versions. 2.2.0-rc2 from tensorflow.keras.layers import Input , SimpleRNN , Dense , Flatten from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt # make the original data series = np . sin ( 0.1 * np . arange ( 1000 )) #+ np.random.randn(200)*0.1 # plot it plt . plot ( series ) plt . show () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 D = 1 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T , 1 ) # Now the data should be N x T x D Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (990, 10, 1) Y.shape (990,) ### try autoregressive RNN model i = Input ( shape = ( T , 1 )) x = SimpleRNN ( 15 , activation = 'relu' )( i ) x = Dense ( 1 )( x ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.001 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Epoch 1/80 16/16 [==============================] - 0s 18ms/step - loss: 1.0602 - val_loss: 0.9011 Epoch 2/80 16/16 [==============================] - 0s 11ms/step - loss: 0.7332 - val_loss: 0.6423 Epoch 3/80 16/16 [==============================] - 0s 11ms/step - loss: 0.5313 - val_loss: 0.4673 Epoch 4/80 16/16 [==============================] - 0s 11ms/step - loss: 0.3822 - val_loss: 0.3390 Epoch 5/80 16/16 [==============================] - 0s 11ms/step - loss: 0.2761 - val_loss: 0.2398 Epoch 6/80 16/16 [==============================] - 0s 11ms/step - loss: 0.1896 - val_loss: 0.1587 Epoch 7/80 16/16 [==============================] - 0s 10ms/step - loss: 0.1180 - val_loss: 0.0952 Epoch 8/80 16/16 [==============================] - 0s 11ms/step - loss: 0.0677 - val_loss: 0.0502 Epoch 9/80 16/16 [==============================] - 0s 11ms/step - loss: 0.0335 - val_loss: 0.0208 Epoch 10/80 16/16 [==============================] - 0s 12ms/step - loss: 0.0134 - val_loss: 0.0081 Epoch 11/80 16/16 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 0.0032 Epoch 12/80 16/16 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0012 Epoch 13/80 16/16 [==============================] - 0s 13ms/step - loss: 0.0010 - val_loss: 7.2874e-04 Epoch 14/80 16/16 [==============================] - 0s 10ms/step - loss: 6.3184e-04 - val_loss: 5.0028e-04 Epoch 15/80 16/16 [==============================] - 0s 11ms/step - loss: 4.2787e-04 - val_loss: 3.4568e-04 Epoch 16/80 16/16 [==============================] - 0s 11ms/step - loss: 3.1279e-04 - val_loss: 2.7649e-04 Epoch 17/80 16/16 [==============================] - 0s 10ms/step - loss: 2.4441e-04 - val_loss: 2.3303e-04 Epoch 18/80 16/16 [==============================] - 0s 12ms/step - loss: 2.2058e-04 - val_loss: 2.0768e-04 Epoch 19/80 16/16 [==============================] - 0s 11ms/step - loss: 1.9065e-04 - val_loss: 1.8392e-04 Epoch 20/80 16/16 [==============================] - 0s 10ms/step - loss: 1.7215e-04 - val_loss: 1.6563e-04 Epoch 21/80 16/16 [==============================] - 0s 11ms/step - loss: 1.5594e-04 - val_loss: 1.5306e-04 Epoch 22/80 16/16 [==============================] - 0s 11ms/step - loss: 1.4406e-04 - val_loss: 1.4254e-04 Epoch 23/80 16/16 [==============================] - 0s 11ms/step - loss: 1.3454e-04 - val_loss: 1.4183e-04 Epoch 24/80 16/16 [==============================] - 0s 11ms/step - loss: 1.3729e-04 - val_loss: 1.2434e-04 Epoch 25/80 16/16 [==============================] - 0s 12ms/step - loss: 1.2109e-04 - val_loss: 1.1460e-04 Epoch 26/80 16/16 [==============================] - 0s 10ms/step - loss: 1.1244e-04 - val_loss: 1.1472e-04 Epoch 27/80 16/16 [==============================] - 0s 10ms/step - loss: 1.0749e-04 - val_loss: 1.1334e-04 Epoch 28/80 16/16 [==============================] - 0s 10ms/step - loss: 1.0260e-04 - val_loss: 9.5891e-05 Epoch 29/80 16/16 [==============================] - 0s 11ms/step - loss: 9.2391e-05 - val_loss: 8.9984e-05 Epoch 30/80 16/16 [==============================] - 0s 11ms/step - loss: 8.6465e-05 - val_loss: 8.5319e-05 Epoch 31/80 16/16 [==============================] - 0s 11ms/step - loss: 8.2860e-05 - val_loss: 8.1269e-05 Epoch 32/80 16/16 [==============================] - 0s 10ms/step - loss: 8.3502e-05 - val_loss: 7.7294e-05 Epoch 33/80 16/16 [==============================] - 0s 11ms/step - loss: 7.8006e-05 - val_loss: 7.7529e-05 Epoch 34/80 16/16 [==============================] - 0s 11ms/step - loss: 7.2876e-05 - val_loss: 7.2808e-05 Epoch 35/80 16/16 [==============================] - 0s 11ms/step - loss: 7.1062e-05 - val_loss: 6.9914e-05 Epoch 36/80 16/16 [==============================] - 0s 11ms/step - loss: 7.2599e-05 - val_loss: 6.8395e-05 Epoch 37/80 16/16 [==============================] - 0s 11ms/step - loss: 6.3187e-05 - val_loss: 6.1742e-05 Epoch 38/80 16/16 [==============================] - 0s 11ms/step - loss: 6.2401e-05 - val_loss: 6.1536e-05 Epoch 39/80 16/16 [==============================] - 0s 11ms/step - loss: 6.0000e-05 - val_loss: 5.6692e-05 Epoch 40/80 16/16 [==============================] - 0s 11ms/step - loss: 5.5932e-05 - val_loss: 5.8413e-05 Epoch 41/80 16/16 [==============================] - 0s 11ms/step - loss: 5.7132e-05 - val_loss: 5.4641e-05 Epoch 42/80 16/16 [==============================] - 0s 11ms/step - loss: 5.2083e-05 - val_loss: 5.3853e-05 Epoch 43/80 16/16 [==============================] - 0s 11ms/step - loss: 4.8307e-05 - val_loss: 4.9899e-05 Epoch 44/80 16/16 [==============================] - 0s 11ms/step - loss: 4.9699e-05 - val_loss: 4.6176e-05 Epoch 45/80 16/16 [==============================] - 0s 11ms/step - loss: 4.7037e-05 - val_loss: 4.7717e-05 Epoch 46/80 16/16 [==============================] - 0s 12ms/step - loss: 4.4639e-05 - val_loss: 4.2385e-05 Epoch 47/80 16/16 [==============================] - 0s 11ms/step - loss: 4.2696e-05 - val_loss: 4.0801e-05 Epoch 48/80 16/16 [==============================] - 0s 11ms/step - loss: 4.3489e-05 - val_loss: 4.0174e-05 Epoch 49/80 16/16 [==============================] - 0s 10ms/step - loss: 4.0366e-05 - val_loss: 3.9151e-05 Epoch 50/80 16/16 [==============================] - 0s 11ms/step - loss: 3.9470e-05 - val_loss: 3.7980e-05 Epoch 51/80 16/16 [==============================] - 0s 11ms/step - loss: 3.7470e-05 - val_loss: 4.0943e-05 Epoch 52/80 16/16 [==============================] - 0s 11ms/step - loss: 3.9431e-05 - val_loss: 3.5197e-05 Epoch 53/80 16/16 [==============================] - 0s 11ms/step - loss: 4.0079e-05 - val_loss: 3.3711e-05 Epoch 54/80 16/16 [==============================] - 0s 10ms/step - loss: 3.7805e-05 - val_loss: 3.7489e-05 Epoch 55/80 16/16 [==============================] - 0s 12ms/step - loss: 3.6745e-05 - val_loss: 3.3163e-05 Epoch 56/80 16/16 [==============================] - 0s 11ms/step - loss: 3.0924e-05 - val_loss: 2.8880e-05 Epoch 57/80 16/16 [==============================] - 0s 11ms/step - loss: 2.9857e-05 - val_loss: 2.8039e-05 Epoch 58/80 16/16 [==============================] - 0s 11ms/step - loss: 2.9824e-05 - val_loss: 3.1579e-05 Epoch 59/80 16/16 [==============================] - 0s 11ms/step - loss: 2.7642e-05 - val_loss: 2.6629e-05 Epoch 60/80 16/16 [==============================] - 0s 11ms/step - loss: 2.7216e-05 - val_loss: 2.5554e-05 Epoch 61/80 16/16 [==============================] - 0s 12ms/step - loss: 2.5181e-05 - val_loss: 2.4444e-05 Epoch 62/80 16/16 [==============================] - 0s 11ms/step - loss: 2.4672e-05 - val_loss: 2.3357e-05 Epoch 63/80 16/16 [==============================] - 0s 13ms/step - loss: 2.4355e-05 - val_loss: 2.2902e-05 Epoch 64/80 16/16 [==============================] - 0s 11ms/step - loss: 2.2696e-05 - val_loss: 2.1753e-05 Epoch 65/80 16/16 [==============================] - 0s 10ms/step - loss: 2.2524e-05 - val_loss: 2.1157e-05 Epoch 66/80 16/16 [==============================] - 0s 11ms/step - loss: 2.1685e-05 - val_loss: 2.1447e-05 Epoch 67/80 16/16 [==============================] - 0s 11ms/step - loss: 2.1650e-05 - val_loss: 2.2276e-05 Epoch 68/80 16/16 [==============================] - 0s 12ms/step - loss: 2.3310e-05 - val_loss: 1.9242e-05 Epoch 69/80 16/16 [==============================] - 0s 10ms/step - loss: 2.1571e-05 - val_loss: 2.1485e-05 Epoch 70/80 16/16 [==============================] - 0s 11ms/step - loss: 1.9481e-05 - val_loss: 1.9370e-05 Epoch 71/80 16/16 [==============================] - 0s 12ms/step - loss: 1.8170e-05 - val_loss: 1.9942e-05 Epoch 72/80 16/16 [==============================] - 0s 11ms/step - loss: 2.0107e-05 - val_loss: 1.9778e-05 Epoch 73/80 16/16 [==============================] - 0s 12ms/step - loss: 1.9879e-05 - val_loss: 2.0639e-05 Epoch 74/80 16/16 [==============================] - 0s 12ms/step - loss: 1.9376e-05 - val_loss: 1.8434e-05 Epoch 75/80 16/16 [==============================] - 0s 11ms/step - loss: 1.8296e-05 - val_loss: 1.7548e-05 Epoch 76/80 16/16 [==============================] - 0s 11ms/step - loss: 1.7476e-05 - val_loss: 1.5592e-05 Epoch 77/80 16/16 [==============================] - 0s 11ms/step - loss: 1.6514e-05 - val_loss: 1.5914e-05 Epoch 78/80 16/16 [==============================] - 0s 11ms/step - loss: 1.6950e-05 - val_loss: 1.4816e-05 Epoch 79/80 16/16 [==============================] - 0s 12ms/step - loss: 1.5287e-05 - val_loss: 1.4795e-05 Epoch 80/80 16/16 [==============================] - 0s 10ms/step - loss: 1.5042e-05 - val_loss: 1.4638e-05 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f31202e5978> # \"Wrong\" forecast using true targets validation_target = Y [ - N // 2 :] validation_predictions = [] # index of first validation input i = - N // 2 while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( X [ i ] . reshape ( 1 , - 1 , 1 ))[ 0 , 0 ] # 1x1 array -> scalar i += 1 # update the predictions list validation_predictions . append ( p ) plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f31200508d0> # Forecast future values (use only self-predictions for making future predictions) validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , - 1 , 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f31065716a0> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 SimpleRNN Sine"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_SimpleRNN_Sine/#simple-rnn-sine","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow is already loaded. Please restart the runtime to change versions. 2.2.0-rc2 from tensorflow.keras.layers import Input , SimpleRNN , Dense , Flatten from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt # make the original data series = np . sin ( 0.1 * np . arange ( 1000 )) #+ np.random.randn(200)*0.1 # plot it plt . plot ( series ) plt . show () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 D = 1 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T , 1 ) # Now the data should be N x T x D Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (990, 10, 1) Y.shape (990,) ### try autoregressive RNN model i = Input ( shape = ( T , 1 )) x = SimpleRNN ( 15 , activation = 'relu' )( i ) x = Dense ( 1 )( x ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.001 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Epoch 1/80 16/16 [==============================] - 0s 18ms/step - loss: 1.0602 - val_loss: 0.9011 Epoch 2/80 16/16 [==============================] - 0s 11ms/step - loss: 0.7332 - val_loss: 0.6423 Epoch 3/80 16/16 [==============================] - 0s 11ms/step - loss: 0.5313 - val_loss: 0.4673 Epoch 4/80 16/16 [==============================] - 0s 11ms/step - loss: 0.3822 - val_loss: 0.3390 Epoch 5/80 16/16 [==============================] - 0s 11ms/step - loss: 0.2761 - val_loss: 0.2398 Epoch 6/80 16/16 [==============================] - 0s 11ms/step - loss: 0.1896 - val_loss: 0.1587 Epoch 7/80 16/16 [==============================] - 0s 10ms/step - loss: 0.1180 - val_loss: 0.0952 Epoch 8/80 16/16 [==============================] - 0s 11ms/step - loss: 0.0677 - val_loss: 0.0502 Epoch 9/80 16/16 [==============================] - 0s 11ms/step - loss: 0.0335 - val_loss: 0.0208 Epoch 10/80 16/16 [==============================] - 0s 12ms/step - loss: 0.0134 - val_loss: 0.0081 Epoch 11/80 16/16 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 0.0032 Epoch 12/80 16/16 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0012 Epoch 13/80 16/16 [==============================] - 0s 13ms/step - loss: 0.0010 - val_loss: 7.2874e-04 Epoch 14/80 16/16 [==============================] - 0s 10ms/step - loss: 6.3184e-04 - val_loss: 5.0028e-04 Epoch 15/80 16/16 [==============================] - 0s 11ms/step - loss: 4.2787e-04 - val_loss: 3.4568e-04 Epoch 16/80 16/16 [==============================] - 0s 11ms/step - loss: 3.1279e-04 - val_loss: 2.7649e-04 Epoch 17/80 16/16 [==============================] - 0s 10ms/step - loss: 2.4441e-04 - val_loss: 2.3303e-04 Epoch 18/80 16/16 [==============================] - 0s 12ms/step - loss: 2.2058e-04 - val_loss: 2.0768e-04 Epoch 19/80 16/16 [==============================] - 0s 11ms/step - loss: 1.9065e-04 - val_loss: 1.8392e-04 Epoch 20/80 16/16 [==============================] - 0s 10ms/step - loss: 1.7215e-04 - val_loss: 1.6563e-04 Epoch 21/80 16/16 [==============================] - 0s 11ms/step - loss: 1.5594e-04 - val_loss: 1.5306e-04 Epoch 22/80 16/16 [==============================] - 0s 11ms/step - loss: 1.4406e-04 - val_loss: 1.4254e-04 Epoch 23/80 16/16 [==============================] - 0s 11ms/step - loss: 1.3454e-04 - val_loss: 1.4183e-04 Epoch 24/80 16/16 [==============================] - 0s 11ms/step - loss: 1.3729e-04 - val_loss: 1.2434e-04 Epoch 25/80 16/16 [==============================] - 0s 12ms/step - loss: 1.2109e-04 - val_loss: 1.1460e-04 Epoch 26/80 16/16 [==============================] - 0s 10ms/step - loss: 1.1244e-04 - val_loss: 1.1472e-04 Epoch 27/80 16/16 [==============================] - 0s 10ms/step - loss: 1.0749e-04 - val_loss: 1.1334e-04 Epoch 28/80 16/16 [==============================] - 0s 10ms/step - loss: 1.0260e-04 - val_loss: 9.5891e-05 Epoch 29/80 16/16 [==============================] - 0s 11ms/step - loss: 9.2391e-05 - val_loss: 8.9984e-05 Epoch 30/80 16/16 [==============================] - 0s 11ms/step - loss: 8.6465e-05 - val_loss: 8.5319e-05 Epoch 31/80 16/16 [==============================] - 0s 11ms/step - loss: 8.2860e-05 - val_loss: 8.1269e-05 Epoch 32/80 16/16 [==============================] - 0s 10ms/step - loss: 8.3502e-05 - val_loss: 7.7294e-05 Epoch 33/80 16/16 [==============================] - 0s 11ms/step - loss: 7.8006e-05 - val_loss: 7.7529e-05 Epoch 34/80 16/16 [==============================] - 0s 11ms/step - loss: 7.2876e-05 - val_loss: 7.2808e-05 Epoch 35/80 16/16 [==============================] - 0s 11ms/step - loss: 7.1062e-05 - val_loss: 6.9914e-05 Epoch 36/80 16/16 [==============================] - 0s 11ms/step - loss: 7.2599e-05 - val_loss: 6.8395e-05 Epoch 37/80 16/16 [==============================] - 0s 11ms/step - loss: 6.3187e-05 - val_loss: 6.1742e-05 Epoch 38/80 16/16 [==============================] - 0s 11ms/step - loss: 6.2401e-05 - val_loss: 6.1536e-05 Epoch 39/80 16/16 [==============================] - 0s 11ms/step - loss: 6.0000e-05 - val_loss: 5.6692e-05 Epoch 40/80 16/16 [==============================] - 0s 11ms/step - loss: 5.5932e-05 - val_loss: 5.8413e-05 Epoch 41/80 16/16 [==============================] - 0s 11ms/step - loss: 5.7132e-05 - val_loss: 5.4641e-05 Epoch 42/80 16/16 [==============================] - 0s 11ms/step - loss: 5.2083e-05 - val_loss: 5.3853e-05 Epoch 43/80 16/16 [==============================] - 0s 11ms/step - loss: 4.8307e-05 - val_loss: 4.9899e-05 Epoch 44/80 16/16 [==============================] - 0s 11ms/step - loss: 4.9699e-05 - val_loss: 4.6176e-05 Epoch 45/80 16/16 [==============================] - 0s 11ms/step - loss: 4.7037e-05 - val_loss: 4.7717e-05 Epoch 46/80 16/16 [==============================] - 0s 12ms/step - loss: 4.4639e-05 - val_loss: 4.2385e-05 Epoch 47/80 16/16 [==============================] - 0s 11ms/step - loss: 4.2696e-05 - val_loss: 4.0801e-05 Epoch 48/80 16/16 [==============================] - 0s 11ms/step - loss: 4.3489e-05 - val_loss: 4.0174e-05 Epoch 49/80 16/16 [==============================] - 0s 10ms/step - loss: 4.0366e-05 - val_loss: 3.9151e-05 Epoch 50/80 16/16 [==============================] - 0s 11ms/step - loss: 3.9470e-05 - val_loss: 3.7980e-05 Epoch 51/80 16/16 [==============================] - 0s 11ms/step - loss: 3.7470e-05 - val_loss: 4.0943e-05 Epoch 52/80 16/16 [==============================] - 0s 11ms/step - loss: 3.9431e-05 - val_loss: 3.5197e-05 Epoch 53/80 16/16 [==============================] - 0s 11ms/step - loss: 4.0079e-05 - val_loss: 3.3711e-05 Epoch 54/80 16/16 [==============================] - 0s 10ms/step - loss: 3.7805e-05 - val_loss: 3.7489e-05 Epoch 55/80 16/16 [==============================] - 0s 12ms/step - loss: 3.6745e-05 - val_loss: 3.3163e-05 Epoch 56/80 16/16 [==============================] - 0s 11ms/step - loss: 3.0924e-05 - val_loss: 2.8880e-05 Epoch 57/80 16/16 [==============================] - 0s 11ms/step - loss: 2.9857e-05 - val_loss: 2.8039e-05 Epoch 58/80 16/16 [==============================] - 0s 11ms/step - loss: 2.9824e-05 - val_loss: 3.1579e-05 Epoch 59/80 16/16 [==============================] - 0s 11ms/step - loss: 2.7642e-05 - val_loss: 2.6629e-05 Epoch 60/80 16/16 [==============================] - 0s 11ms/step - loss: 2.7216e-05 - val_loss: 2.5554e-05 Epoch 61/80 16/16 [==============================] - 0s 12ms/step - loss: 2.5181e-05 - val_loss: 2.4444e-05 Epoch 62/80 16/16 [==============================] - 0s 11ms/step - loss: 2.4672e-05 - val_loss: 2.3357e-05 Epoch 63/80 16/16 [==============================] - 0s 13ms/step - loss: 2.4355e-05 - val_loss: 2.2902e-05 Epoch 64/80 16/16 [==============================] - 0s 11ms/step - loss: 2.2696e-05 - val_loss: 2.1753e-05 Epoch 65/80 16/16 [==============================] - 0s 10ms/step - loss: 2.2524e-05 - val_loss: 2.1157e-05 Epoch 66/80 16/16 [==============================] - 0s 11ms/step - loss: 2.1685e-05 - val_loss: 2.1447e-05 Epoch 67/80 16/16 [==============================] - 0s 11ms/step - loss: 2.1650e-05 - val_loss: 2.2276e-05 Epoch 68/80 16/16 [==============================] - 0s 12ms/step - loss: 2.3310e-05 - val_loss: 1.9242e-05 Epoch 69/80 16/16 [==============================] - 0s 10ms/step - loss: 2.1571e-05 - val_loss: 2.1485e-05 Epoch 70/80 16/16 [==============================] - 0s 11ms/step - loss: 1.9481e-05 - val_loss: 1.9370e-05 Epoch 71/80 16/16 [==============================] - 0s 12ms/step - loss: 1.8170e-05 - val_loss: 1.9942e-05 Epoch 72/80 16/16 [==============================] - 0s 11ms/step - loss: 2.0107e-05 - val_loss: 1.9778e-05 Epoch 73/80 16/16 [==============================] - 0s 12ms/step - loss: 1.9879e-05 - val_loss: 2.0639e-05 Epoch 74/80 16/16 [==============================] - 0s 12ms/step - loss: 1.9376e-05 - val_loss: 1.8434e-05 Epoch 75/80 16/16 [==============================] - 0s 11ms/step - loss: 1.8296e-05 - val_loss: 1.7548e-05 Epoch 76/80 16/16 [==============================] - 0s 11ms/step - loss: 1.7476e-05 - val_loss: 1.5592e-05 Epoch 77/80 16/16 [==============================] - 0s 11ms/step - loss: 1.6514e-05 - val_loss: 1.5914e-05 Epoch 78/80 16/16 [==============================] - 0s 11ms/step - loss: 1.6950e-05 - val_loss: 1.4816e-05 Epoch 79/80 16/16 [==============================] - 0s 12ms/step - loss: 1.5287e-05 - val_loss: 1.4795e-05 Epoch 80/80 16/16 [==============================] - 0s 10ms/step - loss: 1.5042e-05 - val_loss: 1.4638e-05 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f31202e5978> # \"Wrong\" forecast using true targets validation_target = Y [ - N // 2 :] validation_predictions = [] # index of first validation input i = - N // 2 while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( X [ i ] . reshape ( 1 , - 1 , 1 ))[ 0 , 0 ] # 1x1 array -> scalar i += 1 # update the predictions list validation_predictions . append ( p ) plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f31200508d0> # Forecast future values (use only self-predictions for making future predictions) validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , - 1 , 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f31065716a0> Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Simple RNN (sine)"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Stock_Returns/","text":"================ by Jawad Haider Stock Returns Stock Returns \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # More imports from tensorflow.keras.layers import Input , LSTM , GRU , SimpleRNN , Dense , GlobalMaxPool1D from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler # yes, you can read dataframes from URLs! df = pd . read_csv ( 'https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/sbux.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date open high low close volume Name 0 2013-02-08 27.920 28.325 27.920 28.185 7146296 SBUX 1 2013-02-11 28.260 28.260 27.930 28.070 5457354 SBUX 2 2013-02-12 28.000 28.275 27.975 28.130 8665592 SBUX 3 2013-02-13 28.230 28.230 27.750 27.915 7022056 SBUX 4 2013-02-14 27.765 27.905 27.675 27.775 8899188 SBUX df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date open high low close volume Name 1254 2018-02-01 56.280 56.42 55.89 56.00 14690146 SBUX 1255 2018-02-02 55.900 56.32 55.70 55.77 15358909 SBUX 1256 2018-02-05 55.530 56.26 54.57 54.69 16059955 SBUX 1257 2018-02-06 53.685 56.06 53.56 55.61 17415065 SBUX 1258 2018-02-07 55.080 55.43 54.44 54.46 13927022 SBUX # Start by doing the WRONG thing - trying to predict the price itself series = df [ 'close' ] . values . reshape ( - 1 , 1 ) # Normalize the data # Note: I didn't think about where the true boundary is, this is just approx. scaler = StandardScaler () scaler . fit ( series [: len ( series ) // 2 ]) series = scaler . transform ( series ) . flatten () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 D = 1 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T , 1 ) # Now the data should be N x T x D Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (1249, 10, 1) Y.shape (1249,) ### try autoregressive RNN model i = Input ( shape = ( T , 1 )) x = LSTM ( 5 )( i ) x = Dense ( 1 )( x ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.1 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 624 samples, validate on 625 samples Epoch 1/80 624/624 [==============================] - 2s 3ms/sample - loss: 0.2222 - val_loss: 0.1727 Epoch 2/80 624/624 [==============================] - 0s 223us/sample - loss: 0.0128 - val_loss: 0.0407 Epoch 3/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0069 - val_loss: 0.0373 Epoch 4/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0063 - val_loss: 0.0386 Epoch 5/80 624/624 [==============================] - 0s 234us/sample - loss: 0.0062 - val_loss: 0.0375 Epoch 6/80 624/624 [==============================] - 0s 214us/sample - loss: 0.0067 - val_loss: 0.0406 Epoch 7/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0057 - val_loss: 0.0290 Epoch 8/80 624/624 [==============================] - 0s 220us/sample - loss: 0.0061 - val_loss: 0.0248 Epoch 9/80 624/624 [==============================] - 0s 223us/sample - loss: 0.0065 - val_loss: 0.0303 Epoch 10/80 624/624 [==============================] - 0s 211us/sample - loss: 0.0069 - val_loss: 0.0529 Epoch 11/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0070 - val_loss: 0.0734 Epoch 12/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0078 - val_loss: 0.0258 Epoch 13/80 624/624 [==============================] - 0s 232us/sample - loss: 0.0063 - val_loss: 0.0313 Epoch 14/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0058 - val_loss: 0.0239 Epoch 15/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0066 - val_loss: 0.0314 Epoch 16/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0068 - val_loss: 0.0248 Epoch 17/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0058 - val_loss: 0.0245 Epoch 18/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0062 - val_loss: 0.0457 Epoch 19/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0056 - val_loss: 0.0200 Epoch 20/80 624/624 [==============================] - 0s 242us/sample - loss: 0.0057 - val_loss: 0.0207 Epoch 21/80 624/624 [==============================] - 0s 207us/sample - loss: 0.0056 - val_loss: 0.0258 Epoch 22/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0059 - val_loss: 0.0244 Epoch 23/80 624/624 [==============================] - 0s 205us/sample - loss: 0.0073 - val_loss: 0.0426 Epoch 24/80 624/624 [==============================] - 0s 205us/sample - loss: 0.0059 - val_loss: 0.0446 Epoch 25/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0065 - val_loss: 0.0359 Epoch 26/80 624/624 [==============================] - 0s 216us/sample - loss: 0.0064 - val_loss: 0.0198 Epoch 27/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0054 - val_loss: 0.0279 Epoch 28/80 624/624 [==============================] - 0s 225us/sample - loss: 0.0053 - val_loss: 0.0239 Epoch 29/80 624/624 [==============================] - 0s 207us/sample - loss: 0.0056 - val_loss: 0.0189 Epoch 30/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0070 - val_loss: 0.0445 Epoch 31/80 624/624 [==============================] - 0s 215us/sample - loss: 0.0066 - val_loss: 0.0388 Epoch 32/80 624/624 [==============================] - 0s 211us/sample - loss: 0.0065 - val_loss: 0.0193 Epoch 33/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0058 - val_loss: 0.0234 Epoch 34/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0066 - val_loss: 0.0463 Epoch 35/80 624/624 [==============================] - 0s 204us/sample - loss: 0.0056 - val_loss: 0.0161 Epoch 36/80 624/624 [==============================] - 0s 229us/sample - loss: 0.0054 - val_loss: 0.0300 Epoch 37/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0062 - val_loss: 0.0232 Epoch 38/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0055 - val_loss: 0.0207 Epoch 39/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0060 - val_loss: 0.0193 Epoch 40/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0058 - val_loss: 0.0362 Epoch 41/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0072 - val_loss: 0.0290 Epoch 42/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0061 - val_loss: 0.0209 Epoch 43/80 624/624 [==============================] - 0s 227us/sample - loss: 0.0060 - val_loss: 0.0521 Epoch 44/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0071 - val_loss: 0.0147 Epoch 45/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0054 - val_loss: 0.0229 Epoch 46/80 624/624 [==============================] - 0s 220us/sample - loss: 0.0054 - val_loss: 0.0163 Epoch 47/80 624/624 [==============================] - 0s 216us/sample - loss: 0.0050 - val_loss: 0.0249 Epoch 48/80 624/624 [==============================] - 0s 206us/sample - loss: 0.0053 - val_loss: 0.0471 Epoch 49/80 624/624 [==============================] - 0s 203us/sample - loss: 0.0054 - val_loss: 0.0186 Epoch 50/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0055 - val_loss: 0.0399 Epoch 51/80 624/624 [==============================] - 0s 228us/sample - loss: 0.0061 - val_loss: 0.0203 Epoch 52/80 624/624 [==============================] - 0s 211us/sample - loss: 0.0064 - val_loss: 0.0176 Epoch 53/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0058 - val_loss: 0.0332 Epoch 54/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0056 - val_loss: 0.0202 Epoch 55/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0073 - val_loss: 0.0219 Epoch 56/80 624/624 [==============================] - 0s 220us/sample - loss: 0.0068 - val_loss: 0.0156 Epoch 57/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0058 - val_loss: 0.0389 Epoch 58/80 624/624 [==============================] - 0s 227us/sample - loss: 0.0064 - val_loss: 0.0973 Epoch 59/80 624/624 [==============================] - 0s 203us/sample - loss: 0.0089 - val_loss: 0.0399 Epoch 60/80 624/624 [==============================] - 0s 220us/sample - loss: 0.0053 - val_loss: 0.0218 Epoch 61/80 624/624 [==============================] - 0s 214us/sample - loss: 0.0062 - val_loss: 0.0229 Epoch 62/80 624/624 [==============================] - 0s 223us/sample - loss: 0.0058 - val_loss: 0.0254 Epoch 63/80 624/624 [==============================] - 0s 241us/sample - loss: 0.0055 - val_loss: 0.0165 Epoch 64/80 624/624 [==============================] - 0s 216us/sample - loss: 0.0060 - val_loss: 0.0158 Epoch 65/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0063 - val_loss: 0.0142 Epoch 66/80 624/624 [==============================] - 0s 230us/sample - loss: 0.0060 - val_loss: 0.0237 Epoch 67/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0054 - val_loss: 0.0207 Epoch 68/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0055 - val_loss: 0.0157 Epoch 69/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0056 - val_loss: 0.0188 Epoch 70/80 624/624 [==============================] - 0s 206us/sample - loss: 0.0055 - val_loss: 0.0216 Epoch 71/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0059 - val_loss: 0.0145 Epoch 72/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0060 - val_loss: 0.0172 Epoch 73/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0063 - val_loss: 0.0127 Epoch 74/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0057 - val_loss: 0.0326 Epoch 75/80 624/624 [==============================] - 0s 223us/sample - loss: 0.0064 - val_loss: 0.0178 Epoch 76/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0056 - val_loss: 0.0182 Epoch 77/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0059 - val_loss: 0.0158 Epoch 78/80 624/624 [==============================] - 0s 219us/sample - loss: 0.0051 - val_loss: 0.0126 Epoch 79/80 624/624 [==============================] - 0s 204us/sample - loss: 0.0051 - val_loss: 0.0187 Epoch 80/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0056 - val_loss: 0.0157 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f726fa0ca90> # One-step forecast using true targets outputs = model . predict ( X ) print ( outputs . shape ) predictions = outputs [:, 0 ] plt . plot ( Y , label = 'targets' ) plt . plot ( predictions , label = 'predictions' ) plt . legend () plt . show () (1249, 1) # Multi-step forecast validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , T , 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f726987fba8> # calculate returns by first shifting the data df [ 'PrevClose' ] = df [ 'close' ] . shift ( 1 ) # move everything up 1 # so now it's like # close / prev close # x[2] x[1] # x[3] x[2] # x[4] x[3] # ... # x[t] x[t-1] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date open high low close volume Name PrevClose 0 2013-02-08 27.920 28.325 27.920 28.185 7146296 SBUX NaN 1 2013-02-11 28.260 28.260 27.930 28.070 5457354 SBUX 28.185 2 2013-02-12 28.000 28.275 27.975 28.130 8665592 SBUX 28.070 3 2013-02-13 28.230 28.230 27.750 27.915 7022056 SBUX 28.130 4 2013-02-14 27.765 27.905 27.675 27.775 8899188 SBUX 27.915 # then the return is # (x[t] - x[t-1]) / x[t-1] df [ 'Return' ] = ( df [ 'close' ] - df [ 'PrevClose' ]) / df [ 'PrevClose' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date open high low close volume Name PrevClose Return 0 2013-02-08 27.920 28.325 27.920 28.185 7146296 SBUX NaN NaN 1 2013-02-11 28.260 28.260 27.930 28.070 5457354 SBUX 28.185 -0.004080 2 2013-02-12 28.000 28.275 27.975 28.130 8665592 SBUX 28.070 0.002138 3 2013-02-13 28.230 28.230 27.750 27.915 7022056 SBUX 28.130 -0.007643 4 2013-02-14 27.765 27.905 27.675 27.775 8899188 SBUX 27.915 -0.005015 # Now let's try an LSTM to predict returns df [ 'Return' ] . hist () <matplotlib.axes._subplots.AxesSubplot at 0x7f726f9a3518> series = df [ 'Return' ] . values [ 1 :] . reshape ( - 1 , 1 ) # Normalize the data # Note: I didn't think about where the true boundary is, this is just approx. scaler = StandardScaler () scaler . fit ( series [: len ( series ) // 2 ]) series = scaler . transform ( series ) . flatten () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 D = 1 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T , 1 ) # Now the data should be N x T x D Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (1248, 10, 1) Y.shape (1248,) ### try autoregressive RNN model i = Input ( shape = ( T , 1 )) x = LSTM ( 5 )( i ) x = Dense ( 1 )( x ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.01 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 624 samples, validate on 624 samples Epoch 1/80 624/624 [==============================] - 1s 1ms/sample - loss: 0.9940 - val_loss: 1.1571 Epoch 2/80 624/624 [==============================] - 0s 214us/sample - loss: 0.9866 - val_loss: 1.1597 Epoch 3/80 624/624 [==============================] - 0s 214us/sample - loss: 0.9829 - val_loss: 1.1493 Epoch 4/80 624/624 [==============================] - 0s 209us/sample - loss: 0.9836 - val_loss: 1.1523 Epoch 5/80 624/624 [==============================] - 0s 216us/sample - loss: 0.9877 - val_loss: 1.1510 Epoch 6/80 624/624 [==============================] - 0s 216us/sample - loss: 0.9838 - val_loss: 1.1552 Epoch 7/80 624/624 [==============================] - 0s 211us/sample - loss: 0.9853 - val_loss: 1.1602 Epoch 8/80 624/624 [==============================] - 0s 222us/sample - loss: 0.9822 - val_loss: 1.1499 Epoch 9/80 624/624 [==============================] - 0s 215us/sample - loss: 0.9857 - val_loss: 1.1637 Epoch 10/80 624/624 [==============================] - 0s 213us/sample - loss: 0.9815 - val_loss: 1.1354 Epoch 11/80 624/624 [==============================] - 0s 212us/sample - loss: 0.9894 - val_loss: 1.1632 Epoch 12/80 624/624 [==============================] - 0s 213us/sample - loss: 0.9768 - val_loss: 1.1462 Epoch 13/80 624/624 [==============================] - 0s 209us/sample - loss: 0.9813 - val_loss: 1.1539 Epoch 14/80 624/624 [==============================] - 0s 205us/sample - loss: 0.9802 - val_loss: 1.1491 Epoch 15/80 624/624 [==============================] - 0s 222us/sample - loss: 0.9745 - val_loss: 1.1466 Epoch 16/80 624/624 [==============================] - 0s 226us/sample - loss: 0.9766 - val_loss: 1.1588 Epoch 17/80 624/624 [==============================] - 0s 220us/sample - loss: 0.9651 - val_loss: 1.1595 Epoch 18/80 624/624 [==============================] - 0s 207us/sample - loss: 0.9571 - val_loss: 1.1705 Epoch 19/80 624/624 [==============================] - 0s 207us/sample - loss: 0.9463 - val_loss: 1.1902 Epoch 20/80 624/624 [==============================] - 0s 242us/sample - loss: 0.9363 - val_loss: 1.1664 Epoch 21/80 624/624 [==============================] - 0s 209us/sample - loss: 0.9419 - val_loss: 1.1877 Epoch 22/80 624/624 [==============================] - 0s 216us/sample - loss: 0.9314 - val_loss: 1.2258 Epoch 23/80 624/624 [==============================] - 0s 231us/sample - loss: 0.9192 - val_loss: 1.1998 Epoch 24/80 624/624 [==============================] - 0s 206us/sample - loss: 0.9266 - val_loss: 1.2173 Epoch 25/80 624/624 [==============================] - 0s 210us/sample - loss: 0.9174 - val_loss: 1.2386 Epoch 26/80 624/624 [==============================] - 0s 213us/sample - loss: 0.9116 - val_loss: 1.2249 Epoch 27/80 624/624 [==============================] - 0s 215us/sample - loss: 0.9065 - val_loss: 1.2387 Epoch 28/80 624/624 [==============================] - 0s 223us/sample - loss: 0.9102 - val_loss: 1.2492 Epoch 29/80 624/624 [==============================] - 0s 213us/sample - loss: 0.9022 - val_loss: 1.2742 Epoch 30/80 624/624 [==============================] - 0s 211us/sample - loss: 0.8919 - val_loss: 1.2505 Epoch 31/80 624/624 [==============================] - 0s 241us/sample - loss: 0.8865 - val_loss: 1.2967 Epoch 32/80 624/624 [==============================] - 0s 212us/sample - loss: 0.9019 - val_loss: 1.2916 Epoch 33/80 624/624 [==============================] - 0s 217us/sample - loss: 0.8976 - val_loss: 1.2559 Epoch 34/80 624/624 [==============================] - 0s 227us/sample - loss: 0.8814 - val_loss: 1.2702 Epoch 35/80 624/624 [==============================] - 0s 224us/sample - loss: 0.8668 - val_loss: 1.3312 Epoch 36/80 624/624 [==============================] - 0s 212us/sample - loss: 0.8769 - val_loss: 1.2489 Epoch 37/80 624/624 [==============================] - 0s 217us/sample - loss: 0.8774 - val_loss: 1.3559 Epoch 38/80 624/624 [==============================] - 0s 240us/sample - loss: 0.8659 - val_loss: 1.3003 Epoch 39/80 624/624 [==============================] - 0s 219us/sample - loss: 0.8513 - val_loss: 1.3741 Epoch 40/80 624/624 [==============================] - 0s 210us/sample - loss: 0.8468 - val_loss: 1.3747 Epoch 41/80 624/624 [==============================] - 0s 224us/sample - loss: 0.8590 - val_loss: 1.3489 Epoch 42/80 624/624 [==============================] - 0s 211us/sample - loss: 0.8395 - val_loss: 1.3724 Epoch 43/80 624/624 [==============================] - 0s 210us/sample - loss: 0.8469 - val_loss: 1.4338 Epoch 44/80 624/624 [==============================] - 0s 211us/sample - loss: 0.8469 - val_loss: 1.4760 Epoch 45/80 624/624 [==============================] - 0s 225us/sample - loss: 0.8629 - val_loss: 1.3005 Epoch 46/80 624/624 [==============================] - 0s 221us/sample - loss: 0.8444 - val_loss: 1.3844 Epoch 47/80 624/624 [==============================] - 0s 215us/sample - loss: 0.8402 - val_loss: 1.3652 Epoch 48/80 624/624 [==============================] - 0s 213us/sample - loss: 0.8166 - val_loss: 1.4462 Epoch 49/80 624/624 [==============================] - 0s 206us/sample - loss: 0.8166 - val_loss: 1.4201 Epoch 50/80 624/624 [==============================] - 0s 215us/sample - loss: 0.8072 - val_loss: 1.4665 Epoch 51/80 624/624 [==============================] - 0s 215us/sample - loss: 0.8032 - val_loss: 1.4918 Epoch 52/80 624/624 [==============================] - 0s 214us/sample - loss: 0.7988 - val_loss: 1.5627 Epoch 53/80 624/624 [==============================] - 0s 224us/sample - loss: 0.7954 - val_loss: 1.5068 Epoch 54/80 624/624 [==============================] - 0s 211us/sample - loss: 0.7866 - val_loss: 1.5371 Epoch 55/80 624/624 [==============================] - 0s 221us/sample - loss: 0.7843 - val_loss: 1.4661 Epoch 56/80 624/624 [==============================] - 0s 228us/sample - loss: 0.7888 - val_loss: 1.6810 Epoch 57/80 624/624 [==============================] - 0s 213us/sample - loss: 0.8047 - val_loss: 1.5455 Epoch 58/80 624/624 [==============================] - 0s 220us/sample - loss: 0.7924 - val_loss: 1.5155 Epoch 59/80 624/624 [==============================] - 0s 217us/sample - loss: 0.7815 - val_loss: 1.4651 Epoch 60/80 624/624 [==============================] - 0s 209us/sample - loss: 0.7777 - val_loss: 1.4777 Epoch 61/80 624/624 [==============================] - 0s 224us/sample - loss: 0.7991 - val_loss: 1.3920 Epoch 62/80 624/624 [==============================] - 0s 219us/sample - loss: 0.7914 - val_loss: 1.4990 Epoch 63/80 624/624 [==============================] - 0s 213us/sample - loss: 0.7683 - val_loss: 1.4576 Epoch 64/80 624/624 [==============================] - 0s 215us/sample - loss: 0.7609 - val_loss: 1.5357 Epoch 65/80 624/624 [==============================] - 0s 211us/sample - loss: 0.7529 - val_loss: 1.5212 Epoch 66/80 624/624 [==============================] - 0s 213us/sample - loss: 0.7500 - val_loss: 1.5611 Epoch 67/80 624/624 [==============================] - 0s 214us/sample - loss: 0.7521 - val_loss: 1.5443 Epoch 68/80 624/624 [==============================] - 0s 239us/sample - loss: 0.7456 - val_loss: 1.6059 Epoch 69/80 624/624 [==============================] - 0s 222us/sample - loss: 0.7481 - val_loss: 1.5969 Epoch 70/80 624/624 [==============================] - 0s 209us/sample - loss: 0.7860 - val_loss: 1.6076 Epoch 71/80 624/624 [==============================] - 0s 213us/sample - loss: 0.8211 - val_loss: 1.3821 Epoch 72/80 624/624 [==============================] - 0s 207us/sample - loss: 0.8178 - val_loss: 1.5862 Epoch 73/80 624/624 [==============================] - 0s 214us/sample - loss: 0.7538 - val_loss: 1.5646 Epoch 74/80 624/624 [==============================] - 0s 222us/sample - loss: 0.7356 - val_loss: 1.6355 Epoch 75/80 624/624 [==============================] - 0s 215us/sample - loss: 0.7513 - val_loss: 1.4834 Epoch 76/80 624/624 [==============================] - 0s 232us/sample - loss: 0.7505 - val_loss: 1.5441 Epoch 77/80 624/624 [==============================] - 0s 213us/sample - loss: 0.7432 - val_loss: 1.5286 Epoch 78/80 624/624 [==============================] - 0s 203us/sample - loss: 0.7276 - val_loss: 1.5084 Epoch 79/80 624/624 [==============================] - 0s 209us/sample - loss: 0.7282 - val_loss: 1.6215 Epoch 80/80 624/624 [==============================] - 0s 215us/sample - loss: 0.7342 - val_loss: 1.5306 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f7267aa46d8> # One-step forecast using true targets outputs = model . predict ( X ) print ( outputs . shape ) predictions = outputs [:, 0 ] plt . plot ( Y , label = 'targets' ) plt . plot ( predictions , label = 'predictions' ) plt . legend () plt . show () (1248, 1) # Multi-step forecast validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , T , 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f726fab4e10> # Now turn the full data into numpy arrays # Not yet in the final \"X\" format! input_data = df [[ 'open' , 'high' , 'low' , 'close' , 'volume' ]] . values targets = df [ 'Return' ] . values # Now make the actual data which will go into the neural network T = 10 # the number of time steps to look at to make a prediction for the next day D = input_data . shape [ 1 ] N = len ( input_data ) - T # (e.g. if T=10 and you have 11 data points then you'd only have 1 sample) # normalize the inputs Ntrain = len ( input_data ) * 2 // 3 scaler = StandardScaler () scaler . fit ( input_data [: Ntrain + T - 1 ]) input_data = scaler . transform ( input_data ) # Setup X_train and Y_train X_train = np . zeros (( Ntrain , T , D )) Y_train = np . zeros ( Ntrain ) for t in range ( Ntrain ): X_train [ t , :, :] = input_data [ t : t + T ] Y_train [ t ] = ( targets [ t + T ] > 0 ) # Setup X_test and Y_test X_test = np . zeros (( N - Ntrain , T , D )) Y_test = np . zeros ( N - Ntrain ) for u in range ( N - Ntrain ): # u counts from 0...(N - Ntrain) # t counts from Ntrain...N t = u + Ntrain X_test [ u , :, :] = input_data [ t : t + T ] Y_test [ u ] = ( targets [ t + T ] > 0 ) # make the RNN i = Input ( shape = ( T , D )) x = LSTM ( 50 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.001 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( X_train , Y_train , batch_size = 32 , epochs = 300 , validation_data = ( X_test , Y_test ), ) WARNING: Logging before flag parsing goes to stderr. W0803 17:30:48.500098 140132640524160 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 839 samples, validate on 410 samples Epoch 1/300 839/839 [==============================] - 1s 2ms/sample - loss: 0.6995 - accuracy: 0.4923 - val_loss: 0.6965 - val_accuracy: 0.4732 Epoch 2/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6937 - accuracy: 0.5221 - val_loss: 0.6970 - val_accuracy: 0.4805 Epoch 3/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6925 - accuracy: 0.5185 - val_loss: 0.6938 - val_accuracy: 0.4951 Epoch 4/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6914 - accuracy: 0.5125 - val_loss: 0.6922 - val_accuracy: 0.5220 Epoch 5/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6915 - accuracy: 0.5280 - val_loss: 0.6944 - val_accuracy: 0.4927 Epoch 6/300 839/839 [==============================] - 0s 246us/sample - loss: 0.6920 - accuracy: 0.5185 - val_loss: 0.6923 - val_accuracy: 0.4927 Epoch 7/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6929 - accuracy: 0.5352 - val_loss: 0.7042 - val_accuracy: 0.4854 Epoch 8/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6918 - accuracy: 0.5435 - val_loss: 0.6933 - val_accuracy: 0.5000 Epoch 9/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6900 - accuracy: 0.5280 - val_loss: 0.6942 - val_accuracy: 0.5024 Epoch 10/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6905 - accuracy: 0.5185 - val_loss: 0.6937 - val_accuracy: 0.5000 Epoch 11/300 839/839 [==============================] - 0s 237us/sample - loss: 0.6892 - accuracy: 0.5209 - val_loss: 0.6926 - val_accuracy: 0.4902 Epoch 12/300 839/839 [==============================] - 0s 224us/sample - loss: 0.6897 - accuracy: 0.5364 - val_loss: 0.6930 - val_accuracy: 0.4878 Epoch 13/300 839/839 [==============================] - 0s 233us/sample - loss: 0.6888 - accuracy: 0.5352 - val_loss: 0.6965 - val_accuracy: 0.5024 Epoch 14/300 839/839 [==============================] - 0s 237us/sample - loss: 0.6878 - accuracy: 0.5411 - val_loss: 0.6956 - val_accuracy: 0.4976 Epoch 15/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6883 - accuracy: 0.5399 - val_loss: 0.6949 - val_accuracy: 0.4878 Epoch 16/300 839/839 [==============================] - 0s 240us/sample - loss: 0.6882 - accuracy: 0.5447 - val_loss: 0.6910 - val_accuracy: 0.5195 Epoch 17/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6861 - accuracy: 0.5650 - val_loss: 0.6932 - val_accuracy: 0.4976 Epoch 18/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6869 - accuracy: 0.5626 - val_loss: 0.6933 - val_accuracy: 0.4878 Epoch 19/300 839/839 [==============================] - 0s 226us/sample - loss: 0.6861 - accuracy: 0.5578 - val_loss: 0.6946 - val_accuracy: 0.4878 Epoch 20/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6860 - accuracy: 0.5662 - val_loss: 0.6928 - val_accuracy: 0.5171 Epoch 21/300 839/839 [==============================] - 0s 223us/sample - loss: 0.6867 - accuracy: 0.5244 - val_loss: 0.6930 - val_accuracy: 0.5171 Epoch 22/300 839/839 [==============================] - 0s 245us/sample - loss: 0.6864 - accuracy: 0.5530 - val_loss: 0.6916 - val_accuracy: 0.5195 Epoch 23/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6866 - accuracy: 0.5352 - val_loss: 0.6941 - val_accuracy: 0.4927 Epoch 24/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6856 - accuracy: 0.5721 - val_loss: 0.6930 - val_accuracy: 0.5220 Epoch 25/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6857 - accuracy: 0.5387 - val_loss: 0.6930 - val_accuracy: 0.5220 Epoch 26/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6878 - accuracy: 0.5602 - val_loss: 0.6960 - val_accuracy: 0.4927 Epoch 27/300 839/839 [==============================] - 0s 256us/sample - loss: 0.6875 - accuracy: 0.5328 - val_loss: 0.6939 - val_accuracy: 0.4854 Epoch 28/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6857 - accuracy: 0.5518 - val_loss: 0.6920 - val_accuracy: 0.5220 Epoch 29/300 839/839 [==============================] - 0s 226us/sample - loss: 0.6857 - accuracy: 0.5495 - val_loss: 0.6931 - val_accuracy: 0.5098 Epoch 30/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6834 - accuracy: 0.5602 - val_loss: 0.6951 - val_accuracy: 0.5098 Epoch 31/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6825 - accuracy: 0.5638 - val_loss: 0.6936 - val_accuracy: 0.5146 Epoch 32/300 839/839 [==============================] - 0s 244us/sample - loss: 0.6828 - accuracy: 0.5745 - val_loss: 0.6937 - val_accuracy: 0.5171 Epoch 33/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6822 - accuracy: 0.5721 - val_loss: 0.6941 - val_accuracy: 0.5146 Epoch 34/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6829 - accuracy: 0.5685 - val_loss: 0.6959 - val_accuracy: 0.5122 Epoch 35/300 839/839 [==============================] - 0s 226us/sample - loss: 0.6818 - accuracy: 0.5638 - val_loss: 0.6941 - val_accuracy: 0.5171 Epoch 36/300 839/839 [==============================] - 0s 263us/sample - loss: 0.6816 - accuracy: 0.5733 - val_loss: 0.6930 - val_accuracy: 0.5195 Epoch 37/300 839/839 [==============================] - 0s 241us/sample - loss: 0.6817 - accuracy: 0.5650 - val_loss: 0.6951 - val_accuracy: 0.5220 Epoch 38/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6816 - accuracy: 0.5507 - val_loss: 0.6927 - val_accuracy: 0.5341 Epoch 39/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6803 - accuracy: 0.5793 - val_loss: 0.6947 - val_accuracy: 0.5122 Epoch 40/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6795 - accuracy: 0.5626 - val_loss: 0.6942 - val_accuracy: 0.5268 Epoch 41/300 839/839 [==============================] - 0s 224us/sample - loss: 0.6792 - accuracy: 0.5626 - val_loss: 0.6946 - val_accuracy: 0.5220 Epoch 42/300 839/839 [==============================] - 0s 246us/sample - loss: 0.6796 - accuracy: 0.5745 - val_loss: 0.6931 - val_accuracy: 0.5244 Epoch 43/300 839/839 [==============================] - 0s 224us/sample - loss: 0.6795 - accuracy: 0.5542 - val_loss: 0.6962 - val_accuracy: 0.5122 Epoch 44/300 839/839 [==============================] - 0s 237us/sample - loss: 0.6784 - accuracy: 0.5781 - val_loss: 0.6947 - val_accuracy: 0.5122 Epoch 45/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6755 - accuracy: 0.5769 - val_loss: 0.6987 - val_accuracy: 0.5073 Epoch 46/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6799 - accuracy: 0.5578 - val_loss: 0.6950 - val_accuracy: 0.5171 Epoch 47/300 839/839 [==============================] - 0s 251us/sample - loss: 0.6779 - accuracy: 0.5757 - val_loss: 0.6953 - val_accuracy: 0.5098 Epoch 48/300 839/839 [==============================] - 0s 234us/sample - loss: 0.6765 - accuracy: 0.5685 - val_loss: 0.6953 - val_accuracy: 0.5049 Epoch 49/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6738 - accuracy: 0.5733 - val_loss: 0.6964 - val_accuracy: 0.5049 Epoch 50/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6753 - accuracy: 0.5662 - val_loss: 0.6969 - val_accuracy: 0.5098 Epoch 51/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6733 - accuracy: 0.5745 - val_loss: 0.6973 - val_accuracy: 0.5098 Epoch 52/300 839/839 [==============================] - 0s 253us/sample - loss: 0.6744 - accuracy: 0.5757 - val_loss: 0.7016 - val_accuracy: 0.4976 Epoch 53/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6717 - accuracy: 0.5650 - val_loss: 0.6987 - val_accuracy: 0.5122 Epoch 54/300 839/839 [==============================] - 0s 220us/sample - loss: 0.6703 - accuracy: 0.5709 - val_loss: 0.6997 - val_accuracy: 0.4976 Epoch 55/300 839/839 [==============================] - 0s 233us/sample - loss: 0.6695 - accuracy: 0.5864 - val_loss: 0.6986 - val_accuracy: 0.5073 Epoch 56/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6708 - accuracy: 0.5781 - val_loss: 0.7012 - val_accuracy: 0.5024 Epoch 57/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6716 - accuracy: 0.5745 - val_loss: 0.7007 - val_accuracy: 0.5024 Epoch 58/300 839/839 [==============================] - 0s 243us/sample - loss: 0.6699 - accuracy: 0.5828 - val_loss: 0.7017 - val_accuracy: 0.4902 Epoch 59/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6656 - accuracy: 0.5840 - val_loss: 0.7005 - val_accuracy: 0.5146 Epoch 60/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6654 - accuracy: 0.5781 - val_loss: 0.7066 - val_accuracy: 0.5000 Epoch 61/300 839/839 [==============================] - 0s 224us/sample - loss: 0.6689 - accuracy: 0.5685 - val_loss: 0.7009 - val_accuracy: 0.5098 Epoch 62/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6640 - accuracy: 0.6007 - val_loss: 0.7120 - val_accuracy: 0.4951 Epoch 63/300 839/839 [==============================] - 0s 239us/sample - loss: 0.6632 - accuracy: 0.5864 - val_loss: 0.6998 - val_accuracy: 0.5049 Epoch 64/300 839/839 [==============================] - 0s 236us/sample - loss: 0.6655 - accuracy: 0.5959 - val_loss: 0.7051 - val_accuracy: 0.5122 Epoch 65/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6616 - accuracy: 0.5864 - val_loss: 0.7050 - val_accuracy: 0.4951 Epoch 66/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6598 - accuracy: 0.5948 - val_loss: 0.7053 - val_accuracy: 0.4878 Epoch 67/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6596 - accuracy: 0.5912 - val_loss: 0.7044 - val_accuracy: 0.4976 Epoch 68/300 839/839 [==============================] - 0s 238us/sample - loss: 0.6580 - accuracy: 0.5971 - val_loss: 0.7052 - val_accuracy: 0.5049 Epoch 69/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6559 - accuracy: 0.5959 - val_loss: 0.7074 - val_accuracy: 0.5000 Epoch 70/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6579 - accuracy: 0.6019 - val_loss: 0.7053 - val_accuracy: 0.5049 Epoch 71/300 839/839 [==============================] - 0s 236us/sample - loss: 0.6540 - accuracy: 0.5948 - val_loss: 0.7123 - val_accuracy: 0.4829 Epoch 72/300 839/839 [==============================] - 0s 238us/sample - loss: 0.6532 - accuracy: 0.6031 - val_loss: 0.7101 - val_accuracy: 0.5000 Epoch 73/300 839/839 [==============================] - 0s 250us/sample - loss: 0.6518 - accuracy: 0.5995 - val_loss: 0.7110 - val_accuracy: 0.4927 Epoch 74/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6512 - accuracy: 0.5888 - val_loss: 0.7119 - val_accuracy: 0.4902 Epoch 75/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6467 - accuracy: 0.6019 - val_loss: 0.7134 - val_accuracy: 0.5049 Epoch 76/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6500 - accuracy: 0.6031 - val_loss: 0.7183 - val_accuracy: 0.4902 Epoch 77/300 839/839 [==============================] - 0s 234us/sample - loss: 0.6479 - accuracy: 0.6079 - val_loss: 0.7141 - val_accuracy: 0.4854 Epoch 78/300 839/839 [==============================] - 0s 250us/sample - loss: 0.6475 - accuracy: 0.6174 - val_loss: 0.7129 - val_accuracy: 0.4976 Epoch 79/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6503 - accuracy: 0.5995 - val_loss: 0.7134 - val_accuracy: 0.4976 Epoch 80/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6484 - accuracy: 0.5936 - val_loss: 0.7196 - val_accuracy: 0.4878 Epoch 81/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6427 - accuracy: 0.6114 - val_loss: 0.7213 - val_accuracy: 0.4902 Epoch 82/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6402 - accuracy: 0.6079 - val_loss: 0.7244 - val_accuracy: 0.4902 Epoch 83/300 839/839 [==============================] - 0s 239us/sample - loss: 0.6366 - accuracy: 0.6174 - val_loss: 0.7321 - val_accuracy: 0.4902 Epoch 84/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6365 - accuracy: 0.6246 - val_loss: 0.7324 - val_accuracy: 0.4707 Epoch 85/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6364 - accuracy: 0.6103 - val_loss: 0.7287 - val_accuracy: 0.4878 Epoch 86/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6331 - accuracy: 0.6317 - val_loss: 0.7351 - val_accuracy: 0.4780 Epoch 87/300 839/839 [==============================] - 0s 223us/sample - loss: 0.6262 - accuracy: 0.6389 - val_loss: 0.7337 - val_accuracy: 0.4683 Epoch 88/300 839/839 [==============================] - 0s 271us/sample - loss: 0.6332 - accuracy: 0.6210 - val_loss: 0.7299 - val_accuracy: 0.4878 Epoch 89/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6295 - accuracy: 0.6234 - val_loss: 0.7332 - val_accuracy: 0.4659 Epoch 90/300 839/839 [==============================] - 0s 233us/sample - loss: 0.6278 - accuracy: 0.6234 - val_loss: 0.7433 - val_accuracy: 0.4707 Epoch 91/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6265 - accuracy: 0.6210 - val_loss: 0.7453 - val_accuracy: 0.4707 Epoch 92/300 839/839 [==============================] - 0s 234us/sample - loss: 0.6205 - accuracy: 0.6353 - val_loss: 0.7377 - val_accuracy: 0.4561 Epoch 93/300 839/839 [==============================] - 0s 244us/sample - loss: 0.6213 - accuracy: 0.6472 - val_loss: 0.7452 - val_accuracy: 0.4610 Epoch 94/300 839/839 [==============================] - 0s 226us/sample - loss: 0.6187 - accuracy: 0.6353 - val_loss: 0.7305 - val_accuracy: 0.4659 Epoch 95/300 839/839 [==============================] - 0s 234us/sample - loss: 0.6149 - accuracy: 0.6377 - val_loss: 0.7415 - val_accuracy: 0.4756 Epoch 96/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6211 - accuracy: 0.6389 - val_loss: 0.7494 - val_accuracy: 0.4780 Epoch 97/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6182 - accuracy: 0.6412 - val_loss: 0.7506 - val_accuracy: 0.4634 Epoch 98/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6123 - accuracy: 0.6424 - val_loss: 0.7368 - val_accuracy: 0.4805 Epoch 99/300 839/839 [==============================] - 0s 246us/sample - loss: 0.6165 - accuracy: 0.6317 - val_loss: 0.7546 - val_accuracy: 0.4634 Epoch 100/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6076 - accuracy: 0.6555 - val_loss: 0.7510 - val_accuracy: 0.4659 Epoch 101/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6032 - accuracy: 0.6579 - val_loss: 0.7475 - val_accuracy: 0.4780 Epoch 102/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6038 - accuracy: 0.6460 - val_loss: 0.7579 - val_accuracy: 0.4732 Epoch 103/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6036 - accuracy: 0.6544 - val_loss: 0.7605 - val_accuracy: 0.4732 Epoch 104/300 839/839 [==============================] - 0s 257us/sample - loss: 0.6001 - accuracy: 0.6555 - val_loss: 0.7650 - val_accuracy: 0.4854 Epoch 105/300 839/839 [==============================] - 0s 221us/sample - loss: 0.5982 - accuracy: 0.6579 - val_loss: 0.7669 - val_accuracy: 0.4659 Epoch 106/300 839/839 [==============================] - 0s 225us/sample - loss: 0.5946 - accuracy: 0.6532 - val_loss: 0.7702 - val_accuracy: 0.4634 Epoch 107/300 839/839 [==============================] - 0s 227us/sample - loss: 0.5914 - accuracy: 0.6484 - val_loss: 0.7694 - val_accuracy: 0.4537 Epoch 108/300 839/839 [==============================] - 0s 229us/sample - loss: 0.5894 - accuracy: 0.6651 - val_loss: 0.7644 - val_accuracy: 0.4561 Epoch 109/300 839/839 [==============================] - 0s 243us/sample - loss: 0.5855 - accuracy: 0.6698 - val_loss: 0.7878 - val_accuracy: 0.4561 Epoch 110/300 839/839 [==============================] - 0s 225us/sample - loss: 0.5890 - accuracy: 0.6722 - val_loss: 0.7798 - val_accuracy: 0.4659 Epoch 111/300 839/839 [==============================] - 0s 226us/sample - loss: 0.5869 - accuracy: 0.6698 - val_loss: 0.7716 - val_accuracy: 0.4732 Epoch 112/300 839/839 [==============================] - 0s 239us/sample - loss: 0.5867 - accuracy: 0.6627 - val_loss: 0.7694 - val_accuracy: 0.4732 Epoch 113/300 839/839 [==============================] - 0s 222us/sample - loss: 0.5815 - accuracy: 0.6722 - val_loss: 0.7673 - val_accuracy: 0.4805 Epoch 114/300 839/839 [==============================] - 0s 237us/sample - loss: 0.5819 - accuracy: 0.6675 - val_loss: 0.7746 - val_accuracy: 0.4659 Epoch 115/300 839/839 [==============================] - 0s 230us/sample - loss: 0.5795 - accuracy: 0.6794 - val_loss: 0.7947 - val_accuracy: 0.4488 Epoch 116/300 839/839 [==============================] - 0s 236us/sample - loss: 0.5747 - accuracy: 0.6794 - val_loss: 0.7833 - val_accuracy: 0.4463 Epoch 117/300 839/839 [==============================] - 0s 227us/sample - loss: 0.5731 - accuracy: 0.6782 - val_loss: 0.7782 - val_accuracy: 0.4683 Epoch 118/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5749 - accuracy: 0.6579 - val_loss: 0.7906 - val_accuracy: 0.4561 Epoch 119/300 839/839 [==============================] - 0s 251us/sample - loss: 0.5842 - accuracy: 0.6698 - val_loss: 0.7910 - val_accuracy: 0.4683 Epoch 120/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5716 - accuracy: 0.6746 - val_loss: 0.7893 - val_accuracy: 0.4732 Epoch 121/300 839/839 [==============================] - 0s 226us/sample - loss: 0.5704 - accuracy: 0.6806 - val_loss: 0.7887 - val_accuracy: 0.4707 Epoch 122/300 839/839 [==============================] - 0s 232us/sample - loss: 0.5719 - accuracy: 0.6746 - val_loss: 0.7765 - val_accuracy: 0.4683 Epoch 123/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5609 - accuracy: 0.6937 - val_loss: 0.7892 - val_accuracy: 0.4659 Epoch 124/300 839/839 [==============================] - 0s 253us/sample - loss: 0.5650 - accuracy: 0.6853 - val_loss: 0.7970 - val_accuracy: 0.4610 Epoch 125/300 839/839 [==============================] - 0s 222us/sample - loss: 0.5561 - accuracy: 0.6925 - val_loss: 0.7993 - val_accuracy: 0.4659 Epoch 126/300 839/839 [==============================] - 0s 222us/sample - loss: 0.5549 - accuracy: 0.6865 - val_loss: 0.7989 - val_accuracy: 0.4732 Epoch 127/300 839/839 [==============================] - 0s 230us/sample - loss: 0.5527 - accuracy: 0.6841 - val_loss: 0.8133 - val_accuracy: 0.4683 Epoch 128/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5459 - accuracy: 0.6996 - val_loss: 0.7921 - val_accuracy: 0.4732 Epoch 129/300 839/839 [==============================] - 0s 242us/sample - loss: 0.5440 - accuracy: 0.6973 - val_loss: 0.8099 - val_accuracy: 0.4683 Epoch 130/300 839/839 [==============================] - 0s 224us/sample - loss: 0.5440 - accuracy: 0.6913 - val_loss: 0.8107 - val_accuracy: 0.4756 Epoch 131/300 839/839 [==============================] - 0s 230us/sample - loss: 0.5413 - accuracy: 0.6949 - val_loss: 0.8150 - val_accuracy: 0.4512 Epoch 132/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5446 - accuracy: 0.7032 - val_loss: 0.8127 - val_accuracy: 0.4756 Epoch 133/300 839/839 [==============================] - 0s 220us/sample - loss: 0.5357 - accuracy: 0.7199 - val_loss: 0.8074 - val_accuracy: 0.4610 Epoch 134/300 839/839 [==============================] - 0s 229us/sample - loss: 0.5530 - accuracy: 0.6877 - val_loss: 0.8165 - val_accuracy: 0.4634 Epoch 135/300 839/839 [==============================] - 0s 250us/sample - loss: 0.5324 - accuracy: 0.7175 - val_loss: 0.8221 - val_accuracy: 0.4634 Epoch 136/300 839/839 [==============================] - 0s 231us/sample - loss: 0.5403 - accuracy: 0.7068 - val_loss: 0.8180 - val_accuracy: 0.4537 Epoch 137/300 839/839 [==============================] - 0s 228us/sample - loss: 0.5320 - accuracy: 0.7163 - val_loss: 0.8175 - val_accuracy: 0.4732 Epoch 138/300 839/839 [==============================] - 0s 222us/sample - loss: 0.5369 - accuracy: 0.7008 - val_loss: 0.8190 - val_accuracy: 0.4512 Epoch 139/300 839/839 [==============================] - 0s 261us/sample - loss: 0.5270 - accuracy: 0.7128 - val_loss: 0.8141 - val_accuracy: 0.4683 Epoch 140/300 839/839 [==============================] - 0s 240us/sample - loss: 0.5216 - accuracy: 0.7282 - val_loss: 0.8281 - val_accuracy: 0.4463 Epoch 141/300 839/839 [==============================] - 0s 224us/sample - loss: 0.5183 - accuracy: 0.7235 - val_loss: 0.8204 - val_accuracy: 0.4683 Epoch 142/300 839/839 [==============================] - 0s 224us/sample - loss: 0.5174 - accuracy: 0.7259 - val_loss: 0.8253 - val_accuracy: 0.4561 Epoch 143/300 839/839 [==============================] - 0s 225us/sample - loss: 0.5185 - accuracy: 0.7223 - val_loss: 0.8074 - val_accuracy: 0.4732 Epoch 144/300 839/839 [==============================] - 0s 235us/sample - loss: 0.5198 - accuracy: 0.7259 - val_loss: 0.8325 - val_accuracy: 0.4488 Epoch 145/300 839/839 [==============================] - 0s 238us/sample - loss: 0.5041 - accuracy: 0.7366 - val_loss: 0.8289 - val_accuracy: 0.4585 Epoch 146/300 839/839 [==============================] - 0s 231us/sample - loss: 0.5039 - accuracy: 0.7354 - val_loss: 0.8330 - val_accuracy: 0.4634 Epoch 147/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5029 - accuracy: 0.7461 - val_loss: 0.8260 - val_accuracy: 0.4439 Epoch 148/300 839/839 [==============================] - 0s 229us/sample - loss: 0.5089 - accuracy: 0.7294 - val_loss: 0.8477 - val_accuracy: 0.4659 Epoch 149/300 839/839 [==============================] - 0s 226us/sample - loss: 0.4999 - accuracy: 0.7414 - val_loss: 0.8250 - val_accuracy: 0.4439 Epoch 150/300 839/839 [==============================] - 0s 238us/sample - loss: 0.4918 - accuracy: 0.7390 - val_loss: 0.8383 - val_accuracy: 0.4634 Epoch 151/300 839/839 [==============================] - 0s 227us/sample - loss: 0.4952 - accuracy: 0.7366 - val_loss: 0.8418 - val_accuracy: 0.4634 Epoch 152/300 839/839 [==============================] - 0s 236us/sample - loss: 0.4889 - accuracy: 0.7521 - val_loss: 0.8415 - val_accuracy: 0.4585 Epoch 153/300 839/839 [==============================] - 0s 227us/sample - loss: 0.4829 - accuracy: 0.7545 - val_loss: 0.8385 - val_accuracy: 0.4610 Epoch 154/300 839/839 [==============================] - 0s 222us/sample - loss: 0.4856 - accuracy: 0.7414 - val_loss: 0.8599 - val_accuracy: 0.4561 Epoch 155/300 839/839 [==============================] - 0s 235us/sample - loss: 0.4784 - accuracy: 0.7557 - val_loss: 0.8614 - val_accuracy: 0.4512 Epoch 156/300 839/839 [==============================] - 0s 223us/sample - loss: 0.4741 - accuracy: 0.7628 - val_loss: 0.8526 - val_accuracy: 0.4512 Epoch 157/300 839/839 [==============================] - 0s 228us/sample - loss: 0.4781 - accuracy: 0.7688 - val_loss: 0.8476 - val_accuracy: 0.4707 Epoch 158/300 839/839 [==============================] - 0s 229us/sample - loss: 0.4776 - accuracy: 0.7485 - val_loss: 0.8494 - val_accuracy: 0.4537 Epoch 159/300 839/839 [==============================] - 0s 230us/sample - loss: 0.4717 - accuracy: 0.7497 - val_loss: 0.8671 - val_accuracy: 0.4488 Epoch 160/300 839/839 [==============================] - 0s 239us/sample - loss: 0.4660 - accuracy: 0.7664 - val_loss: 0.8559 - val_accuracy: 0.4610 Epoch 161/300 839/839 [==============================] - 0s 225us/sample - loss: 0.4675 - accuracy: 0.7592 - val_loss: 0.8709 - val_accuracy: 0.4634 Epoch 162/300 839/839 [==============================] - 0s 233us/sample - loss: 0.4657 - accuracy: 0.7652 - val_loss: 0.8544 - val_accuracy: 0.4659 Epoch 163/300 839/839 [==============================] - 0s 227us/sample - loss: 0.4662 - accuracy: 0.7592 - val_loss: 0.8584 - val_accuracy: 0.4634 Epoch 164/300 839/839 [==============================] - 0s 239us/sample - loss: 0.4613 - accuracy: 0.7783 - val_loss: 0.8703 - val_accuracy: 0.4634 Epoch 165/300 839/839 [==============================] - 0s 226us/sample - loss: 0.4571 - accuracy: 0.7747 - val_loss: 0.8628 - val_accuracy: 0.4634 Epoch 166/300 839/839 [==============================] - 0s 255us/sample - loss: 0.4465 - accuracy: 0.7747 - val_loss: 0.8725 - val_accuracy: 0.4683 Epoch 167/300 839/839 [==============================] - 0s 232us/sample - loss: 0.4495 - accuracy: 0.7795 - val_loss: 0.8717 - val_accuracy: 0.4805 Epoch 168/300 839/839 [==============================] - 0s 224us/sample - loss: 0.4508 - accuracy: 0.7783 - val_loss: 0.8831 - val_accuracy: 0.4659 Epoch 169/300 839/839 [==============================] - 0s 227us/sample - loss: 0.4540 - accuracy: 0.7712 - val_loss: 0.8799 - val_accuracy: 0.4707 Epoch 170/300 839/839 [==============================] - 0s 235us/sample - loss: 0.4417 - accuracy: 0.7819 - val_loss: 0.8839 - val_accuracy: 0.4707 Epoch 171/300 839/839 [==============================] - 0s 243us/sample - loss: 0.4524 - accuracy: 0.7771 - val_loss: 0.8871 - val_accuracy: 0.4634 Epoch 172/300 839/839 [==============================] - 0s 229us/sample - loss: 0.4376 - accuracy: 0.7855 - val_loss: 0.8819 - val_accuracy: 0.4732 Epoch 173/300 839/839 [==============================] - 0s 225us/sample - loss: 0.4345 - accuracy: 0.7867 - val_loss: 0.9004 - val_accuracy: 0.4707 Epoch 174/300 839/839 [==============================] - 0s 230us/sample - loss: 0.4330 - accuracy: 0.7890 - val_loss: 0.9038 - val_accuracy: 0.4780 Epoch 175/300 839/839 [==============================] - 0s 231us/sample - loss: 0.4293 - accuracy: 0.7974 - val_loss: 0.8944 - val_accuracy: 0.4659 Epoch 176/300 839/839 [==============================] - 0s 245us/sample - loss: 0.4300 - accuracy: 0.7878 - val_loss: 0.9014 - val_accuracy: 0.4780 Epoch 177/300 839/839 [==============================] - 0s 222us/sample - loss: 0.4231 - accuracy: 0.7938 - val_loss: 0.8976 - val_accuracy: 0.4829 Epoch 178/300 839/839 [==============================] - 0s 233us/sample - loss: 0.4206 - accuracy: 0.7926 - val_loss: 0.9104 - val_accuracy: 0.4829 Epoch 179/300 839/839 [==============================] - 0s 233us/sample - loss: 0.4240 - accuracy: 0.7902 - val_loss: 0.9080 - val_accuracy: 0.4854 Epoch 180/300 839/839 [==============================] - 0s 228us/sample - loss: 0.4198 - accuracy: 0.7926 - val_loss: 0.9067 - val_accuracy: 0.4854 Epoch 181/300 839/839 [==============================] - 0s 240us/sample - loss: 0.4153 - accuracy: 0.7938 - val_loss: 0.9102 - val_accuracy: 0.4878 Epoch 182/300 839/839 [==============================] - 0s 235us/sample - loss: 0.4161 - accuracy: 0.7926 - val_loss: 0.9071 - val_accuracy: 0.4854 Epoch 183/300 839/839 [==============================] - 0s 235us/sample - loss: 0.4160 - accuracy: 0.7998 - val_loss: 0.9261 - val_accuracy: 0.4634 Epoch 184/300 839/839 [==============================] - 0s 224us/sample - loss: 0.4153 - accuracy: 0.7986 - val_loss: 0.9151 - val_accuracy: 0.4683 Epoch 185/300 839/839 [==============================] - 0s 225us/sample - loss: 0.4110 - accuracy: 0.7926 - val_loss: 0.9413 - val_accuracy: 0.4707 Epoch 186/300 839/839 [==============================] - 0s 248us/sample - loss: 0.4073 - accuracy: 0.8033 - val_loss: 0.9216 - val_accuracy: 0.4829 Epoch 187/300 839/839 [==============================] - 0s 229us/sample - loss: 0.3984 - accuracy: 0.8093 - val_loss: 0.9198 - val_accuracy: 0.4878 Epoch 188/300 839/839 [==============================] - 0s 236us/sample - loss: 0.3971 - accuracy: 0.8081 - val_loss: 0.9325 - val_accuracy: 0.4780 Epoch 189/300 839/839 [==============================] - 0s 238us/sample - loss: 0.3975 - accuracy: 0.7998 - val_loss: 0.9169 - val_accuracy: 0.4756 Epoch 190/300 839/839 [==============================] - 0s 253us/sample - loss: 0.3974 - accuracy: 0.8033 - val_loss: 0.9268 - val_accuracy: 0.4780 Epoch 191/300 839/839 [==============================] - 0s 243us/sample - loss: 0.4044 - accuracy: 0.8045 - val_loss: 0.9188 - val_accuracy: 0.4902 Epoch 192/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3939 - accuracy: 0.8141 - val_loss: 0.9361 - val_accuracy: 0.4902 Epoch 193/300 839/839 [==============================] - 0s 230us/sample - loss: 0.3997 - accuracy: 0.8069 - val_loss: 0.9397 - val_accuracy: 0.4805 Epoch 194/300 839/839 [==============================] - 0s 227us/sample - loss: 0.3846 - accuracy: 0.8200 - val_loss: 0.9364 - val_accuracy: 0.4756 Epoch 195/300 839/839 [==============================] - 0s 232us/sample - loss: 0.3861 - accuracy: 0.8153 - val_loss: 0.9314 - val_accuracy: 0.4878 Epoch 196/300 839/839 [==============================] - 0s 258us/sample - loss: 0.3818 - accuracy: 0.8129 - val_loss: 0.9423 - val_accuracy: 0.4976 Epoch 197/300 839/839 [==============================] - 0s 225us/sample - loss: 0.3822 - accuracy: 0.8141 - val_loss: 0.9365 - val_accuracy: 0.4756 Epoch 198/300 839/839 [==============================] - 0s 229us/sample - loss: 0.3744 - accuracy: 0.8260 - val_loss: 0.9652 - val_accuracy: 0.4927 Epoch 199/300 839/839 [==============================] - 0s 230us/sample - loss: 0.3773 - accuracy: 0.8272 - val_loss: 0.9589 - val_accuracy: 0.4780 Epoch 200/300 839/839 [==============================] - 0s 236us/sample - loss: 0.3652 - accuracy: 0.8391 - val_loss: 0.9297 - val_accuracy: 0.5122 Epoch 201/300 839/839 [==============================] - 0s 249us/sample - loss: 0.3681 - accuracy: 0.8284 - val_loss: 0.9497 - val_accuracy: 0.4878 Epoch 202/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3716 - accuracy: 0.8188 - val_loss: 0.9587 - val_accuracy: 0.4780 Epoch 203/300 839/839 [==============================] - 0s 232us/sample - loss: 0.3610 - accuracy: 0.8319 - val_loss: 0.9399 - val_accuracy: 0.4902 Epoch 204/300 839/839 [==============================] - 0s 234us/sample - loss: 0.3559 - accuracy: 0.8379 - val_loss: 0.9560 - val_accuracy: 0.5000 Epoch 205/300 839/839 [==============================] - 0s 237us/sample - loss: 0.3600 - accuracy: 0.8343 - val_loss: 0.9606 - val_accuracy: 0.4927 Epoch 206/300 839/839 [==============================] - 0s 238us/sample - loss: 0.3483 - accuracy: 0.8415 - val_loss: 0.9535 - val_accuracy: 0.5000 Epoch 207/300 839/839 [==============================] - 0s 222us/sample - loss: 0.3447 - accuracy: 0.8355 - val_loss: 0.9562 - val_accuracy: 0.4854 Epoch 208/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3439 - accuracy: 0.8391 - val_loss: 0.9458 - val_accuracy: 0.4780 Epoch 209/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3452 - accuracy: 0.8427 - val_loss: 0.9819 - val_accuracy: 0.4805 Epoch 210/300 839/839 [==============================] - 0s 231us/sample - loss: 0.3417 - accuracy: 0.8451 - val_loss: 0.9985 - val_accuracy: 0.4732 Epoch 211/300 839/839 [==============================] - 0s 229us/sample - loss: 0.3533 - accuracy: 0.8355 - val_loss: 0.9619 - val_accuracy: 0.4805 Epoch 212/300 839/839 [==============================] - 0s 243us/sample - loss: 0.3368 - accuracy: 0.8522 - val_loss: 0.9666 - val_accuracy: 0.4976 Epoch 213/300 839/839 [==============================] - 0s 223us/sample - loss: 0.3305 - accuracy: 0.8439 - val_loss: 0.9618 - val_accuracy: 0.4902 Epoch 214/300 839/839 [==============================] - 0s 236us/sample - loss: 0.3266 - accuracy: 0.8474 - val_loss: 0.9895 - val_accuracy: 0.4927 Epoch 215/300 839/839 [==============================] - 0s 227us/sample - loss: 0.3365 - accuracy: 0.8343 - val_loss: 0.9858 - val_accuracy: 0.5000 Epoch 216/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3256 - accuracy: 0.8462 - val_loss: 0.9796 - val_accuracy: 0.4854 Epoch 217/300 839/839 [==============================] - 0s 249us/sample - loss: 0.3292 - accuracy: 0.8546 - val_loss: 1.0061 - val_accuracy: 0.4780 Epoch 218/300 839/839 [==============================] - 0s 222us/sample - loss: 0.3259 - accuracy: 0.8439 - val_loss: 1.0063 - val_accuracy: 0.4805 Epoch 219/300 839/839 [==============================] - 0s 224us/sample - loss: 0.3204 - accuracy: 0.8570 - val_loss: 0.9881 - val_accuracy: 0.4756 Epoch 220/300 839/839 [==============================] - 0s 229us/sample - loss: 0.3316 - accuracy: 0.8403 - val_loss: 0.9789 - val_accuracy: 0.4854 Epoch 221/300 839/839 [==============================] - 0s 225us/sample - loss: 0.3185 - accuracy: 0.8486 - val_loss: 1.0082 - val_accuracy: 0.4756 Epoch 222/300 839/839 [==============================] - 0s 235us/sample - loss: 0.3266 - accuracy: 0.8462 - val_loss: 1.0083 - val_accuracy: 0.4780 Epoch 223/300 839/839 [==============================] - 0s 220us/sample - loss: 0.3113 - accuracy: 0.8462 - val_loss: 1.0257 - val_accuracy: 0.4756 Epoch 224/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3023 - accuracy: 0.8641 - val_loss: 1.0054 - val_accuracy: 0.4854 Epoch 225/300 839/839 [==============================] - 0s 233us/sample - loss: 0.2967 - accuracy: 0.8641 - val_loss: 1.0320 - val_accuracy: 0.4829 Epoch 226/300 839/839 [==============================] - 0s 225us/sample - loss: 0.3007 - accuracy: 0.8582 - val_loss: 1.0144 - val_accuracy: 0.4756 Epoch 227/300 839/839 [==============================] - 0s 233us/sample - loss: 0.3366 - accuracy: 0.8439 - val_loss: 1.0106 - val_accuracy: 0.4805 Epoch 228/300 839/839 [==============================] - 0s 230us/sample - loss: 0.3243 - accuracy: 0.8474 - val_loss: 1.0429 - val_accuracy: 0.4780 Epoch 229/300 839/839 [==============================] - 0s 241us/sample - loss: 0.2990 - accuracy: 0.8629 - val_loss: 1.0181 - val_accuracy: 0.4927 Epoch 230/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2901 - accuracy: 0.8594 - val_loss: 1.0292 - val_accuracy: 0.4902 Epoch 231/300 839/839 [==============================] - 0s 235us/sample - loss: 0.3153 - accuracy: 0.8558 - val_loss: 1.0244 - val_accuracy: 0.4780 Epoch 232/300 839/839 [==============================] - 0s 234us/sample - loss: 0.2941 - accuracy: 0.8653 - val_loss: 1.0348 - val_accuracy: 0.4927 Epoch 233/300 839/839 [==============================] - 0s 239us/sample - loss: 0.2882 - accuracy: 0.8629 - val_loss: 1.0568 - val_accuracy: 0.4732 Epoch 234/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2857 - accuracy: 0.8665 - val_loss: 1.0517 - val_accuracy: 0.4780 Epoch 235/300 839/839 [==============================] - 0s 226us/sample - loss: 0.2800 - accuracy: 0.8701 - val_loss: 1.0463 - val_accuracy: 0.4659 Epoch 236/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2967 - accuracy: 0.8629 - val_loss: 1.0587 - val_accuracy: 0.4854 Epoch 237/300 839/839 [==============================] - 0s 231us/sample - loss: 0.2799 - accuracy: 0.8749 - val_loss: 1.0579 - val_accuracy: 0.4780 Epoch 238/300 839/839 [==============================] - 0s 260us/sample - loss: 0.2790 - accuracy: 0.8760 - val_loss: 1.0519 - val_accuracy: 0.4780 Epoch 239/300 839/839 [==============================] - 0s 220us/sample - loss: 0.2760 - accuracy: 0.8737 - val_loss: 1.0895 - val_accuracy: 0.4756 Epoch 240/300 839/839 [==============================] - 0s 224us/sample - loss: 0.2781 - accuracy: 0.8701 - val_loss: 1.0483 - val_accuracy: 0.4902 Epoch 241/300 839/839 [==============================] - 0s 264us/sample - loss: 0.2709 - accuracy: 0.8760 - val_loss: 1.0658 - val_accuracy: 0.4756 Epoch 242/300 839/839 [==============================] - 0s 232us/sample - loss: 0.2641 - accuracy: 0.8808 - val_loss: 1.0596 - val_accuracy: 0.4780 Epoch 243/300 839/839 [==============================] - 0s 247us/sample - loss: 0.2663 - accuracy: 0.8784 - val_loss: 1.0631 - val_accuracy: 0.4829 Epoch 244/300 839/839 [==============================] - 0s 234us/sample - loss: 0.2679 - accuracy: 0.8749 - val_loss: 1.0996 - val_accuracy: 0.4780 Epoch 245/300 839/839 [==============================] - 0s 234us/sample - loss: 0.2604 - accuracy: 0.8737 - val_loss: 1.0590 - val_accuracy: 0.4854 Epoch 246/300 839/839 [==============================] - 0s 225us/sample - loss: 0.2627 - accuracy: 0.8844 - val_loss: 1.0861 - val_accuracy: 0.4805 Epoch 247/300 839/839 [==============================] - 0s 223us/sample - loss: 0.2566 - accuracy: 0.8844 - val_loss: 1.0796 - val_accuracy: 0.4780 Epoch 248/300 839/839 [==============================] - 0s 253us/sample - loss: 0.2547 - accuracy: 0.8820 - val_loss: 1.0945 - val_accuracy: 0.4707 Epoch 249/300 839/839 [==============================] - 0s 230us/sample - loss: 0.2501 - accuracy: 0.8975 - val_loss: 1.1233 - val_accuracy: 0.4829 Epoch 250/300 839/839 [==============================] - 0s 224us/sample - loss: 0.2443 - accuracy: 0.8963 - val_loss: 1.1275 - val_accuracy: 0.4756 Epoch 251/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2532 - accuracy: 0.8808 - val_loss: 1.0957 - val_accuracy: 0.4659 Epoch 252/300 839/839 [==============================] - 0s 227us/sample - loss: 0.2447 - accuracy: 0.8975 - val_loss: 1.1147 - val_accuracy: 0.4927 Epoch 253/300 839/839 [==============================] - 0s 241us/sample - loss: 0.2465 - accuracy: 0.8856 - val_loss: 1.1121 - val_accuracy: 0.4707 Epoch 254/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2442 - accuracy: 0.8844 - val_loss: 1.1563 - val_accuracy: 0.4780 Epoch 255/300 839/839 [==============================] - 0s 243us/sample - loss: 0.2382 - accuracy: 0.8999 - val_loss: 1.1053 - val_accuracy: 0.4756 Epoch 256/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2340 - accuracy: 0.8963 - val_loss: 1.1087 - val_accuracy: 0.4805 Epoch 257/300 839/839 [==============================] - 0s 233us/sample - loss: 0.2360 - accuracy: 0.8951 - val_loss: 1.1373 - val_accuracy: 0.4756 Epoch 258/300 839/839 [==============================] - 0s 246us/sample - loss: 0.2412 - accuracy: 0.8951 - val_loss: 1.1333 - val_accuracy: 0.4878 Epoch 259/300 839/839 [==============================] - 0s 227us/sample - loss: 0.2410 - accuracy: 0.8915 - val_loss: 1.1303 - val_accuracy: 0.4878 Epoch 260/300 839/839 [==============================] - 0s 231us/sample - loss: 0.2266 - accuracy: 0.8987 - val_loss: 1.1675 - val_accuracy: 0.4927 Epoch 261/300 839/839 [==============================] - 0s 225us/sample - loss: 0.2395 - accuracy: 0.8832 - val_loss: 1.1388 - val_accuracy: 0.4732 Epoch 262/300 839/839 [==============================] - 0s 232us/sample - loss: 0.2390 - accuracy: 0.8987 - val_loss: 1.1354 - val_accuracy: 0.4683 Epoch 263/300 839/839 [==============================] - 0s 243us/sample - loss: 0.2320 - accuracy: 0.8939 - val_loss: 1.1302 - val_accuracy: 0.4854 Epoch 264/300 839/839 [==============================] - 0s 226us/sample - loss: 0.2311 - accuracy: 0.9011 - val_loss: 1.1624 - val_accuracy: 0.4854 Epoch 265/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2252 - accuracy: 0.9011 - val_loss: 1.1641 - val_accuracy: 0.4951 Epoch 266/300 839/839 [==============================] - 0s 238us/sample - loss: 0.2339 - accuracy: 0.8975 - val_loss: 1.1559 - val_accuracy: 0.4951 Epoch 267/300 839/839 [==============================] - 0s 236us/sample - loss: 0.2179 - accuracy: 0.9046 - val_loss: 1.1967 - val_accuracy: 0.4878 Epoch 268/300 839/839 [==============================] - 0s 246us/sample - loss: 0.2178 - accuracy: 0.9046 - val_loss: 1.1752 - val_accuracy: 0.5024 Epoch 269/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2176 - accuracy: 0.9070 - val_loss: 1.1572 - val_accuracy: 0.4976 Epoch 270/300 839/839 [==============================] - 0s 230us/sample - loss: 0.2251 - accuracy: 0.8999 - val_loss: 1.2162 - val_accuracy: 0.4878 Epoch 271/300 839/839 [==============================] - 0s 240us/sample - loss: 0.3056 - accuracy: 0.8605 - val_loss: 1.1761 - val_accuracy: 0.4878 Epoch 272/300 839/839 [==============================] - 0s 235us/sample - loss: 0.3435 - accuracy: 0.8403 - val_loss: 1.1900 - val_accuracy: 0.4951 Epoch 273/300 839/839 [==============================] - 0s 241us/sample - loss: 0.2510 - accuracy: 0.8987 - val_loss: 1.1899 - val_accuracy: 0.4878 Epoch 274/300 839/839 [==============================] - 0s 226us/sample - loss: 0.2561 - accuracy: 0.8963 - val_loss: 1.2053 - val_accuracy: 0.4829 Epoch 275/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2505 - accuracy: 0.8987 - val_loss: 1.1747 - val_accuracy: 0.4976 Epoch 276/300 839/839 [==============================] - 0s 237us/sample - loss: 0.2221 - accuracy: 0.9035 - val_loss: 1.1897 - val_accuracy: 0.4976 Epoch 277/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2093 - accuracy: 0.9094 - val_loss: 1.1901 - val_accuracy: 0.5073 Epoch 278/300 839/839 [==============================] - 0s 227us/sample - loss: 0.1988 - accuracy: 0.9118 - val_loss: 1.2047 - val_accuracy: 0.4878 Epoch 279/300 839/839 [==============================] - 0s 255us/sample - loss: 0.1972 - accuracy: 0.9166 - val_loss: 1.2043 - val_accuracy: 0.4902 Epoch 280/300 839/839 [==============================] - 0s 231us/sample - loss: 0.1923 - accuracy: 0.9190 - val_loss: 1.1818 - val_accuracy: 0.5024 Epoch 281/300 839/839 [==============================] - 0s 230us/sample - loss: 0.1976 - accuracy: 0.9142 - val_loss: 1.1958 - val_accuracy: 0.4780 Epoch 282/300 839/839 [==============================] - 0s 231us/sample - loss: 0.1966 - accuracy: 0.9154 - val_loss: 1.2146 - val_accuracy: 0.5024 Epoch 283/300 839/839 [==============================] - 0s 238us/sample - loss: 0.1929 - accuracy: 0.9154 - val_loss: 1.2188 - val_accuracy: 0.4707 Epoch 284/300 839/839 [==============================] - 0s 254us/sample - loss: 0.1893 - accuracy: 0.9213 - val_loss: 1.2239 - val_accuracy: 0.4829 Epoch 285/300 839/839 [==============================] - 0s 228us/sample - loss: 0.1932 - accuracy: 0.9190 - val_loss: 1.1966 - val_accuracy: 0.4927 Epoch 286/300 839/839 [==============================] - 0s 230us/sample - loss: 0.1886 - accuracy: 0.9201 - val_loss: 1.2821 - val_accuracy: 0.4756 Epoch 287/300 839/839 [==============================] - 0s 226us/sample - loss: 0.1865 - accuracy: 0.9190 - val_loss: 1.2937 - val_accuracy: 0.4756 Epoch 288/300 839/839 [==============================] - 0s 225us/sample - loss: 0.1829 - accuracy: 0.9225 - val_loss: 1.2400 - val_accuracy: 0.5122 Epoch 289/300 839/839 [==============================] - 0s 241us/sample - loss: 0.1895 - accuracy: 0.9142 - val_loss: 1.2456 - val_accuracy: 0.4780 Epoch 290/300 839/839 [==============================] - 0s 226us/sample - loss: 0.1847 - accuracy: 0.9166 - val_loss: 1.2518 - val_accuracy: 0.4951 Epoch 291/300 839/839 [==============================] - 0s 234us/sample - loss: 0.1777 - accuracy: 0.9249 - val_loss: 1.2598 - val_accuracy: 0.4732 Epoch 292/300 839/839 [==============================] - 0s 256us/sample - loss: 0.1771 - accuracy: 0.9237 - val_loss: 1.2527 - val_accuracy: 0.4902 Epoch 293/300 839/839 [==============================] - 0s 237us/sample - loss: 0.1735 - accuracy: 0.9273 - val_loss: 1.2976 - val_accuracy: 0.5024 Epoch 294/300 839/839 [==============================] - 0s 240us/sample - loss: 0.1845 - accuracy: 0.9106 - val_loss: 1.2478 - val_accuracy: 0.4976 Epoch 295/300 839/839 [==============================] - 0s 229us/sample - loss: 0.1820 - accuracy: 0.9225 - val_loss: 1.2710 - val_accuracy: 0.5122 Epoch 296/300 839/839 [==============================] - 0s 231us/sample - loss: 0.1829 - accuracy: 0.9154 - val_loss: 1.3229 - val_accuracy: 0.5000 Epoch 297/300 839/839 [==============================] - 0s 235us/sample - loss: 0.1719 - accuracy: 0.9261 - val_loss: 1.2958 - val_accuracy: 0.4951 Epoch 298/300 839/839 [==============================] - 0s 222us/sample - loss: 0.1739 - accuracy: 0.9249 - val_loss: 1.2766 - val_accuracy: 0.5049 Epoch 299/300 839/839 [==============================] - 0s 238us/sample - loss: 0.1712 - accuracy: 0.9249 - val_loss: 1.2826 - val_accuracy: 0.5122 Epoch 300/300 839/839 [==============================] - 0s 231us/sample - loss: 0.1672 - accuracy: 0.9344 - val_loss: 1.2757 - val_accuracy: 0.5024 # plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () plt . show () # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'accuracy' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_accuracy' ) plt . legend () plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Stock Returns"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Stock_Returns/#stock-returns","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # More imports from tensorflow.keras.layers import Input , LSTM , GRU , SimpleRNN , Dense , GlobalMaxPool1D from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler # yes, you can read dataframes from URLs! df = pd . read_csv ( 'https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/sbux.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date open high low close volume Name 0 2013-02-08 27.920 28.325 27.920 28.185 7146296 SBUX 1 2013-02-11 28.260 28.260 27.930 28.070 5457354 SBUX 2 2013-02-12 28.000 28.275 27.975 28.130 8665592 SBUX 3 2013-02-13 28.230 28.230 27.750 27.915 7022056 SBUX 4 2013-02-14 27.765 27.905 27.675 27.775 8899188 SBUX df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date open high low close volume Name 1254 2018-02-01 56.280 56.42 55.89 56.00 14690146 SBUX 1255 2018-02-02 55.900 56.32 55.70 55.77 15358909 SBUX 1256 2018-02-05 55.530 56.26 54.57 54.69 16059955 SBUX 1257 2018-02-06 53.685 56.06 53.56 55.61 17415065 SBUX 1258 2018-02-07 55.080 55.43 54.44 54.46 13927022 SBUX # Start by doing the WRONG thing - trying to predict the price itself series = df [ 'close' ] . values . reshape ( - 1 , 1 ) # Normalize the data # Note: I didn't think about where the true boundary is, this is just approx. scaler = StandardScaler () scaler . fit ( series [: len ( series ) // 2 ]) series = scaler . transform ( series ) . flatten () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 D = 1 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T , 1 ) # Now the data should be N x T x D Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (1249, 10, 1) Y.shape (1249,) ### try autoregressive RNN model i = Input ( shape = ( T , 1 )) x = LSTM ( 5 )( i ) x = Dense ( 1 )( x ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.1 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 624 samples, validate on 625 samples Epoch 1/80 624/624 [==============================] - 2s 3ms/sample - loss: 0.2222 - val_loss: 0.1727 Epoch 2/80 624/624 [==============================] - 0s 223us/sample - loss: 0.0128 - val_loss: 0.0407 Epoch 3/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0069 - val_loss: 0.0373 Epoch 4/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0063 - val_loss: 0.0386 Epoch 5/80 624/624 [==============================] - 0s 234us/sample - loss: 0.0062 - val_loss: 0.0375 Epoch 6/80 624/624 [==============================] - 0s 214us/sample - loss: 0.0067 - val_loss: 0.0406 Epoch 7/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0057 - val_loss: 0.0290 Epoch 8/80 624/624 [==============================] - 0s 220us/sample - loss: 0.0061 - val_loss: 0.0248 Epoch 9/80 624/624 [==============================] - 0s 223us/sample - loss: 0.0065 - val_loss: 0.0303 Epoch 10/80 624/624 [==============================] - 0s 211us/sample - loss: 0.0069 - val_loss: 0.0529 Epoch 11/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0070 - val_loss: 0.0734 Epoch 12/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0078 - val_loss: 0.0258 Epoch 13/80 624/624 [==============================] - 0s 232us/sample - loss: 0.0063 - val_loss: 0.0313 Epoch 14/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0058 - val_loss: 0.0239 Epoch 15/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0066 - val_loss: 0.0314 Epoch 16/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0068 - val_loss: 0.0248 Epoch 17/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0058 - val_loss: 0.0245 Epoch 18/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0062 - val_loss: 0.0457 Epoch 19/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0056 - val_loss: 0.0200 Epoch 20/80 624/624 [==============================] - 0s 242us/sample - loss: 0.0057 - val_loss: 0.0207 Epoch 21/80 624/624 [==============================] - 0s 207us/sample - loss: 0.0056 - val_loss: 0.0258 Epoch 22/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0059 - val_loss: 0.0244 Epoch 23/80 624/624 [==============================] - 0s 205us/sample - loss: 0.0073 - val_loss: 0.0426 Epoch 24/80 624/624 [==============================] - 0s 205us/sample - loss: 0.0059 - val_loss: 0.0446 Epoch 25/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0065 - val_loss: 0.0359 Epoch 26/80 624/624 [==============================] - 0s 216us/sample - loss: 0.0064 - val_loss: 0.0198 Epoch 27/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0054 - val_loss: 0.0279 Epoch 28/80 624/624 [==============================] - 0s 225us/sample - loss: 0.0053 - val_loss: 0.0239 Epoch 29/80 624/624 [==============================] - 0s 207us/sample - loss: 0.0056 - val_loss: 0.0189 Epoch 30/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0070 - val_loss: 0.0445 Epoch 31/80 624/624 [==============================] - 0s 215us/sample - loss: 0.0066 - val_loss: 0.0388 Epoch 32/80 624/624 [==============================] - 0s 211us/sample - loss: 0.0065 - val_loss: 0.0193 Epoch 33/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0058 - val_loss: 0.0234 Epoch 34/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0066 - val_loss: 0.0463 Epoch 35/80 624/624 [==============================] - 0s 204us/sample - loss: 0.0056 - val_loss: 0.0161 Epoch 36/80 624/624 [==============================] - 0s 229us/sample - loss: 0.0054 - val_loss: 0.0300 Epoch 37/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0062 - val_loss: 0.0232 Epoch 38/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0055 - val_loss: 0.0207 Epoch 39/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0060 - val_loss: 0.0193 Epoch 40/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0058 - val_loss: 0.0362 Epoch 41/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0072 - val_loss: 0.0290 Epoch 42/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0061 - val_loss: 0.0209 Epoch 43/80 624/624 [==============================] - 0s 227us/sample - loss: 0.0060 - val_loss: 0.0521 Epoch 44/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0071 - val_loss: 0.0147 Epoch 45/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0054 - val_loss: 0.0229 Epoch 46/80 624/624 [==============================] - 0s 220us/sample - loss: 0.0054 - val_loss: 0.0163 Epoch 47/80 624/624 [==============================] - 0s 216us/sample - loss: 0.0050 - val_loss: 0.0249 Epoch 48/80 624/624 [==============================] - 0s 206us/sample - loss: 0.0053 - val_loss: 0.0471 Epoch 49/80 624/624 [==============================] - 0s 203us/sample - loss: 0.0054 - val_loss: 0.0186 Epoch 50/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0055 - val_loss: 0.0399 Epoch 51/80 624/624 [==============================] - 0s 228us/sample - loss: 0.0061 - val_loss: 0.0203 Epoch 52/80 624/624 [==============================] - 0s 211us/sample - loss: 0.0064 - val_loss: 0.0176 Epoch 53/80 624/624 [==============================] - 0s 210us/sample - loss: 0.0058 - val_loss: 0.0332 Epoch 54/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0056 - val_loss: 0.0202 Epoch 55/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0073 - val_loss: 0.0219 Epoch 56/80 624/624 [==============================] - 0s 220us/sample - loss: 0.0068 - val_loss: 0.0156 Epoch 57/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0058 - val_loss: 0.0389 Epoch 58/80 624/624 [==============================] - 0s 227us/sample - loss: 0.0064 - val_loss: 0.0973 Epoch 59/80 624/624 [==============================] - 0s 203us/sample - loss: 0.0089 - val_loss: 0.0399 Epoch 60/80 624/624 [==============================] - 0s 220us/sample - loss: 0.0053 - val_loss: 0.0218 Epoch 61/80 624/624 [==============================] - 0s 214us/sample - loss: 0.0062 - val_loss: 0.0229 Epoch 62/80 624/624 [==============================] - 0s 223us/sample - loss: 0.0058 - val_loss: 0.0254 Epoch 63/80 624/624 [==============================] - 0s 241us/sample - loss: 0.0055 - val_loss: 0.0165 Epoch 64/80 624/624 [==============================] - 0s 216us/sample - loss: 0.0060 - val_loss: 0.0158 Epoch 65/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0063 - val_loss: 0.0142 Epoch 66/80 624/624 [==============================] - 0s 230us/sample - loss: 0.0060 - val_loss: 0.0237 Epoch 67/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0054 - val_loss: 0.0207 Epoch 68/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0055 - val_loss: 0.0157 Epoch 69/80 624/624 [==============================] - 0s 217us/sample - loss: 0.0056 - val_loss: 0.0188 Epoch 70/80 624/624 [==============================] - 0s 206us/sample - loss: 0.0055 - val_loss: 0.0216 Epoch 71/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0059 - val_loss: 0.0145 Epoch 72/80 624/624 [==============================] - 0s 213us/sample - loss: 0.0060 - val_loss: 0.0172 Epoch 73/80 624/624 [==============================] - 0s 218us/sample - loss: 0.0063 - val_loss: 0.0127 Epoch 74/80 624/624 [==============================] - 0s 212us/sample - loss: 0.0057 - val_loss: 0.0326 Epoch 75/80 624/624 [==============================] - 0s 223us/sample - loss: 0.0064 - val_loss: 0.0178 Epoch 76/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0056 - val_loss: 0.0182 Epoch 77/80 624/624 [==============================] - 0s 209us/sample - loss: 0.0059 - val_loss: 0.0158 Epoch 78/80 624/624 [==============================] - 0s 219us/sample - loss: 0.0051 - val_loss: 0.0126 Epoch 79/80 624/624 [==============================] - 0s 204us/sample - loss: 0.0051 - val_loss: 0.0187 Epoch 80/80 624/624 [==============================] - 0s 208us/sample - loss: 0.0056 - val_loss: 0.0157 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f726fa0ca90> # One-step forecast using true targets outputs = model . predict ( X ) print ( outputs . shape ) predictions = outputs [:, 0 ] plt . plot ( Y , label = 'targets' ) plt . plot ( predictions , label = 'predictions' ) plt . legend () plt . show () (1249, 1) # Multi-step forecast validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , T , 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f726987fba8> # calculate returns by first shifting the data df [ 'PrevClose' ] = df [ 'close' ] . shift ( 1 ) # move everything up 1 # so now it's like # close / prev close # x[2] x[1] # x[3] x[2] # x[4] x[3] # ... # x[t] x[t-1] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date open high low close volume Name PrevClose 0 2013-02-08 27.920 28.325 27.920 28.185 7146296 SBUX NaN 1 2013-02-11 28.260 28.260 27.930 28.070 5457354 SBUX 28.185 2 2013-02-12 28.000 28.275 27.975 28.130 8665592 SBUX 28.070 3 2013-02-13 28.230 28.230 27.750 27.915 7022056 SBUX 28.130 4 2013-02-14 27.765 27.905 27.675 27.775 8899188 SBUX 27.915 # then the return is # (x[t] - x[t-1]) / x[t-1] df [ 'Return' ] = ( df [ 'close' ] - df [ 'PrevClose' ]) / df [ 'PrevClose' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date open high low close volume Name PrevClose Return 0 2013-02-08 27.920 28.325 27.920 28.185 7146296 SBUX NaN NaN 1 2013-02-11 28.260 28.260 27.930 28.070 5457354 SBUX 28.185 -0.004080 2 2013-02-12 28.000 28.275 27.975 28.130 8665592 SBUX 28.070 0.002138 3 2013-02-13 28.230 28.230 27.750 27.915 7022056 SBUX 28.130 -0.007643 4 2013-02-14 27.765 27.905 27.675 27.775 8899188 SBUX 27.915 -0.005015 # Now let's try an LSTM to predict returns df [ 'Return' ] . hist () <matplotlib.axes._subplots.AxesSubplot at 0x7f726f9a3518> series = df [ 'Return' ] . values [ 1 :] . reshape ( - 1 , 1 ) # Normalize the data # Note: I didn't think about where the true boundary is, this is just approx. scaler = StandardScaler () scaler . fit ( series [: len ( series ) // 2 ]) series = scaler . transform ( series ) . flatten () ### build the dataset # let's see if we can use T past values to predict the next value T = 10 D = 1 X = [] Y = [] for t in range ( len ( series ) - T ): x = series [ t : t + T ] X . append ( x ) y = series [ t + T ] Y . append ( y ) X = np . array ( X ) . reshape ( - 1 , T , 1 ) # Now the data should be N x T x D Y = np . array ( Y ) N = len ( X ) print ( \"X.shape\" , X . shape , \"Y.shape\" , Y . shape ) X.shape (1248, 10, 1) Y.shape (1248,) ### try autoregressive RNN model i = Input ( shape = ( T , 1 )) x = LSTM ( 5 )( i ) x = Dense ( 1 )( x ) model = Model ( i , x ) model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.01 ), ) # train the RNN r = model . fit ( X [: - N // 2 ], Y [: - N // 2 ], epochs = 80 , validation_data = ( X [ - N // 2 :], Y [ - N // 2 :]), ) Train on 624 samples, validate on 624 samples Epoch 1/80 624/624 [==============================] - 1s 1ms/sample - loss: 0.9940 - val_loss: 1.1571 Epoch 2/80 624/624 [==============================] - 0s 214us/sample - loss: 0.9866 - val_loss: 1.1597 Epoch 3/80 624/624 [==============================] - 0s 214us/sample - loss: 0.9829 - val_loss: 1.1493 Epoch 4/80 624/624 [==============================] - 0s 209us/sample - loss: 0.9836 - val_loss: 1.1523 Epoch 5/80 624/624 [==============================] - 0s 216us/sample - loss: 0.9877 - val_loss: 1.1510 Epoch 6/80 624/624 [==============================] - 0s 216us/sample - loss: 0.9838 - val_loss: 1.1552 Epoch 7/80 624/624 [==============================] - 0s 211us/sample - loss: 0.9853 - val_loss: 1.1602 Epoch 8/80 624/624 [==============================] - 0s 222us/sample - loss: 0.9822 - val_loss: 1.1499 Epoch 9/80 624/624 [==============================] - 0s 215us/sample - loss: 0.9857 - val_loss: 1.1637 Epoch 10/80 624/624 [==============================] - 0s 213us/sample - loss: 0.9815 - val_loss: 1.1354 Epoch 11/80 624/624 [==============================] - 0s 212us/sample - loss: 0.9894 - val_loss: 1.1632 Epoch 12/80 624/624 [==============================] - 0s 213us/sample - loss: 0.9768 - val_loss: 1.1462 Epoch 13/80 624/624 [==============================] - 0s 209us/sample - loss: 0.9813 - val_loss: 1.1539 Epoch 14/80 624/624 [==============================] - 0s 205us/sample - loss: 0.9802 - val_loss: 1.1491 Epoch 15/80 624/624 [==============================] - 0s 222us/sample - loss: 0.9745 - val_loss: 1.1466 Epoch 16/80 624/624 [==============================] - 0s 226us/sample - loss: 0.9766 - val_loss: 1.1588 Epoch 17/80 624/624 [==============================] - 0s 220us/sample - loss: 0.9651 - val_loss: 1.1595 Epoch 18/80 624/624 [==============================] - 0s 207us/sample - loss: 0.9571 - val_loss: 1.1705 Epoch 19/80 624/624 [==============================] - 0s 207us/sample - loss: 0.9463 - val_loss: 1.1902 Epoch 20/80 624/624 [==============================] - 0s 242us/sample - loss: 0.9363 - val_loss: 1.1664 Epoch 21/80 624/624 [==============================] - 0s 209us/sample - loss: 0.9419 - val_loss: 1.1877 Epoch 22/80 624/624 [==============================] - 0s 216us/sample - loss: 0.9314 - val_loss: 1.2258 Epoch 23/80 624/624 [==============================] - 0s 231us/sample - loss: 0.9192 - val_loss: 1.1998 Epoch 24/80 624/624 [==============================] - 0s 206us/sample - loss: 0.9266 - val_loss: 1.2173 Epoch 25/80 624/624 [==============================] - 0s 210us/sample - loss: 0.9174 - val_loss: 1.2386 Epoch 26/80 624/624 [==============================] - 0s 213us/sample - loss: 0.9116 - val_loss: 1.2249 Epoch 27/80 624/624 [==============================] - 0s 215us/sample - loss: 0.9065 - val_loss: 1.2387 Epoch 28/80 624/624 [==============================] - 0s 223us/sample - loss: 0.9102 - val_loss: 1.2492 Epoch 29/80 624/624 [==============================] - 0s 213us/sample - loss: 0.9022 - val_loss: 1.2742 Epoch 30/80 624/624 [==============================] - 0s 211us/sample - loss: 0.8919 - val_loss: 1.2505 Epoch 31/80 624/624 [==============================] - 0s 241us/sample - loss: 0.8865 - val_loss: 1.2967 Epoch 32/80 624/624 [==============================] - 0s 212us/sample - loss: 0.9019 - val_loss: 1.2916 Epoch 33/80 624/624 [==============================] - 0s 217us/sample - loss: 0.8976 - val_loss: 1.2559 Epoch 34/80 624/624 [==============================] - 0s 227us/sample - loss: 0.8814 - val_loss: 1.2702 Epoch 35/80 624/624 [==============================] - 0s 224us/sample - loss: 0.8668 - val_loss: 1.3312 Epoch 36/80 624/624 [==============================] - 0s 212us/sample - loss: 0.8769 - val_loss: 1.2489 Epoch 37/80 624/624 [==============================] - 0s 217us/sample - loss: 0.8774 - val_loss: 1.3559 Epoch 38/80 624/624 [==============================] - 0s 240us/sample - loss: 0.8659 - val_loss: 1.3003 Epoch 39/80 624/624 [==============================] - 0s 219us/sample - loss: 0.8513 - val_loss: 1.3741 Epoch 40/80 624/624 [==============================] - 0s 210us/sample - loss: 0.8468 - val_loss: 1.3747 Epoch 41/80 624/624 [==============================] - 0s 224us/sample - loss: 0.8590 - val_loss: 1.3489 Epoch 42/80 624/624 [==============================] - 0s 211us/sample - loss: 0.8395 - val_loss: 1.3724 Epoch 43/80 624/624 [==============================] - 0s 210us/sample - loss: 0.8469 - val_loss: 1.4338 Epoch 44/80 624/624 [==============================] - 0s 211us/sample - loss: 0.8469 - val_loss: 1.4760 Epoch 45/80 624/624 [==============================] - 0s 225us/sample - loss: 0.8629 - val_loss: 1.3005 Epoch 46/80 624/624 [==============================] - 0s 221us/sample - loss: 0.8444 - val_loss: 1.3844 Epoch 47/80 624/624 [==============================] - 0s 215us/sample - loss: 0.8402 - val_loss: 1.3652 Epoch 48/80 624/624 [==============================] - 0s 213us/sample - loss: 0.8166 - val_loss: 1.4462 Epoch 49/80 624/624 [==============================] - 0s 206us/sample - loss: 0.8166 - val_loss: 1.4201 Epoch 50/80 624/624 [==============================] - 0s 215us/sample - loss: 0.8072 - val_loss: 1.4665 Epoch 51/80 624/624 [==============================] - 0s 215us/sample - loss: 0.8032 - val_loss: 1.4918 Epoch 52/80 624/624 [==============================] - 0s 214us/sample - loss: 0.7988 - val_loss: 1.5627 Epoch 53/80 624/624 [==============================] - 0s 224us/sample - loss: 0.7954 - val_loss: 1.5068 Epoch 54/80 624/624 [==============================] - 0s 211us/sample - loss: 0.7866 - val_loss: 1.5371 Epoch 55/80 624/624 [==============================] - 0s 221us/sample - loss: 0.7843 - val_loss: 1.4661 Epoch 56/80 624/624 [==============================] - 0s 228us/sample - loss: 0.7888 - val_loss: 1.6810 Epoch 57/80 624/624 [==============================] - 0s 213us/sample - loss: 0.8047 - val_loss: 1.5455 Epoch 58/80 624/624 [==============================] - 0s 220us/sample - loss: 0.7924 - val_loss: 1.5155 Epoch 59/80 624/624 [==============================] - 0s 217us/sample - loss: 0.7815 - val_loss: 1.4651 Epoch 60/80 624/624 [==============================] - 0s 209us/sample - loss: 0.7777 - val_loss: 1.4777 Epoch 61/80 624/624 [==============================] - 0s 224us/sample - loss: 0.7991 - val_loss: 1.3920 Epoch 62/80 624/624 [==============================] - 0s 219us/sample - loss: 0.7914 - val_loss: 1.4990 Epoch 63/80 624/624 [==============================] - 0s 213us/sample - loss: 0.7683 - val_loss: 1.4576 Epoch 64/80 624/624 [==============================] - 0s 215us/sample - loss: 0.7609 - val_loss: 1.5357 Epoch 65/80 624/624 [==============================] - 0s 211us/sample - loss: 0.7529 - val_loss: 1.5212 Epoch 66/80 624/624 [==============================] - 0s 213us/sample - loss: 0.7500 - val_loss: 1.5611 Epoch 67/80 624/624 [==============================] - 0s 214us/sample - loss: 0.7521 - val_loss: 1.5443 Epoch 68/80 624/624 [==============================] - 0s 239us/sample - loss: 0.7456 - val_loss: 1.6059 Epoch 69/80 624/624 [==============================] - 0s 222us/sample - loss: 0.7481 - val_loss: 1.5969 Epoch 70/80 624/624 [==============================] - 0s 209us/sample - loss: 0.7860 - val_loss: 1.6076 Epoch 71/80 624/624 [==============================] - 0s 213us/sample - loss: 0.8211 - val_loss: 1.3821 Epoch 72/80 624/624 [==============================] - 0s 207us/sample - loss: 0.8178 - val_loss: 1.5862 Epoch 73/80 624/624 [==============================] - 0s 214us/sample - loss: 0.7538 - val_loss: 1.5646 Epoch 74/80 624/624 [==============================] - 0s 222us/sample - loss: 0.7356 - val_loss: 1.6355 Epoch 75/80 624/624 [==============================] - 0s 215us/sample - loss: 0.7513 - val_loss: 1.4834 Epoch 76/80 624/624 [==============================] - 0s 232us/sample - loss: 0.7505 - val_loss: 1.5441 Epoch 77/80 624/624 [==============================] - 0s 213us/sample - loss: 0.7432 - val_loss: 1.5286 Epoch 78/80 624/624 [==============================] - 0s 203us/sample - loss: 0.7276 - val_loss: 1.5084 Epoch 79/80 624/624 [==============================] - 0s 209us/sample - loss: 0.7282 - val_loss: 1.6215 Epoch 80/80 624/624 [==============================] - 0s 215us/sample - loss: 0.7342 - val_loss: 1.5306 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f7267aa46d8> # One-step forecast using true targets outputs = model . predict ( X ) print ( outputs . shape ) predictions = outputs [:, 0 ] plt . plot ( Y , label = 'targets' ) plt . plot ( predictions , label = 'predictions' ) plt . legend () plt . show () (1248, 1) # Multi-step forecast validation_target = Y [ - N // 2 :] validation_predictions = [] # first validation input last_x = X [ - N // 2 ] # 1-D array of length T while len ( validation_predictions ) < len ( validation_target ): p = model . predict ( last_x . reshape ( 1 , T , 1 ))[ 0 , 0 ] # 1x1 array -> scalar # update the predictions list validation_predictions . append ( p ) # make the new input last_x = np . roll ( last_x , - 1 ) last_x [ - 1 ] = p plt . plot ( validation_target , label = 'forecast target' ) plt . plot ( validation_predictions , label = 'forecast prediction' ) plt . legend () <matplotlib.legend.Legend at 0x7f726fab4e10> # Now turn the full data into numpy arrays # Not yet in the final \"X\" format! input_data = df [[ 'open' , 'high' , 'low' , 'close' , 'volume' ]] . values targets = df [ 'Return' ] . values # Now make the actual data which will go into the neural network T = 10 # the number of time steps to look at to make a prediction for the next day D = input_data . shape [ 1 ] N = len ( input_data ) - T # (e.g. if T=10 and you have 11 data points then you'd only have 1 sample) # normalize the inputs Ntrain = len ( input_data ) * 2 // 3 scaler = StandardScaler () scaler . fit ( input_data [: Ntrain + T - 1 ]) input_data = scaler . transform ( input_data ) # Setup X_train and Y_train X_train = np . zeros (( Ntrain , T , D )) Y_train = np . zeros ( Ntrain ) for t in range ( Ntrain ): X_train [ t , :, :] = input_data [ t : t + T ] Y_train [ t ] = ( targets [ t + T ] > 0 ) # Setup X_test and Y_test X_test = np . zeros (( N - Ntrain , T , D )) Y_test = np . zeros ( N - Ntrain ) for u in range ( N - Ntrain ): # u counts from 0...(N - Ntrain) # t counts from Ntrain...N t = u + Ntrain X_test [ u , :, :] = input_data [ t : t + T ] Y_test [ u ] = ( targets [ t + T ] > 0 ) # make the RNN i = Input ( shape = ( T , D )) x = LSTM ( 50 )( i ) x = Dense ( 1 , activation = 'sigmoid' )( x ) model = Model ( i , x ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.001 ), metrics = [ 'accuracy' ], ) # train the RNN r = model . fit ( X_train , Y_train , batch_size = 32 , epochs = 300 , validation_data = ( X_test , Y_test ), ) WARNING: Logging before flag parsing goes to stderr. W0803 17:30:48.500098 140132640524160 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 839 samples, validate on 410 samples Epoch 1/300 839/839 [==============================] - 1s 2ms/sample - loss: 0.6995 - accuracy: 0.4923 - val_loss: 0.6965 - val_accuracy: 0.4732 Epoch 2/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6937 - accuracy: 0.5221 - val_loss: 0.6970 - val_accuracy: 0.4805 Epoch 3/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6925 - accuracy: 0.5185 - val_loss: 0.6938 - val_accuracy: 0.4951 Epoch 4/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6914 - accuracy: 0.5125 - val_loss: 0.6922 - val_accuracy: 0.5220 Epoch 5/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6915 - accuracy: 0.5280 - val_loss: 0.6944 - val_accuracy: 0.4927 Epoch 6/300 839/839 [==============================] - 0s 246us/sample - loss: 0.6920 - accuracy: 0.5185 - val_loss: 0.6923 - val_accuracy: 0.4927 Epoch 7/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6929 - accuracy: 0.5352 - val_loss: 0.7042 - val_accuracy: 0.4854 Epoch 8/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6918 - accuracy: 0.5435 - val_loss: 0.6933 - val_accuracy: 0.5000 Epoch 9/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6900 - accuracy: 0.5280 - val_loss: 0.6942 - val_accuracy: 0.5024 Epoch 10/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6905 - accuracy: 0.5185 - val_loss: 0.6937 - val_accuracy: 0.5000 Epoch 11/300 839/839 [==============================] - 0s 237us/sample - loss: 0.6892 - accuracy: 0.5209 - val_loss: 0.6926 - val_accuracy: 0.4902 Epoch 12/300 839/839 [==============================] - 0s 224us/sample - loss: 0.6897 - accuracy: 0.5364 - val_loss: 0.6930 - val_accuracy: 0.4878 Epoch 13/300 839/839 [==============================] - 0s 233us/sample - loss: 0.6888 - accuracy: 0.5352 - val_loss: 0.6965 - val_accuracy: 0.5024 Epoch 14/300 839/839 [==============================] - 0s 237us/sample - loss: 0.6878 - accuracy: 0.5411 - val_loss: 0.6956 - val_accuracy: 0.4976 Epoch 15/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6883 - accuracy: 0.5399 - val_loss: 0.6949 - val_accuracy: 0.4878 Epoch 16/300 839/839 [==============================] - 0s 240us/sample - loss: 0.6882 - accuracy: 0.5447 - val_loss: 0.6910 - val_accuracy: 0.5195 Epoch 17/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6861 - accuracy: 0.5650 - val_loss: 0.6932 - val_accuracy: 0.4976 Epoch 18/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6869 - accuracy: 0.5626 - val_loss: 0.6933 - val_accuracy: 0.4878 Epoch 19/300 839/839 [==============================] - 0s 226us/sample - loss: 0.6861 - accuracy: 0.5578 - val_loss: 0.6946 - val_accuracy: 0.4878 Epoch 20/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6860 - accuracy: 0.5662 - val_loss: 0.6928 - val_accuracy: 0.5171 Epoch 21/300 839/839 [==============================] - 0s 223us/sample - loss: 0.6867 - accuracy: 0.5244 - val_loss: 0.6930 - val_accuracy: 0.5171 Epoch 22/300 839/839 [==============================] - 0s 245us/sample - loss: 0.6864 - accuracy: 0.5530 - val_loss: 0.6916 - val_accuracy: 0.5195 Epoch 23/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6866 - accuracy: 0.5352 - val_loss: 0.6941 - val_accuracy: 0.4927 Epoch 24/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6856 - accuracy: 0.5721 - val_loss: 0.6930 - val_accuracy: 0.5220 Epoch 25/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6857 - accuracy: 0.5387 - val_loss: 0.6930 - val_accuracy: 0.5220 Epoch 26/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6878 - accuracy: 0.5602 - val_loss: 0.6960 - val_accuracy: 0.4927 Epoch 27/300 839/839 [==============================] - 0s 256us/sample - loss: 0.6875 - accuracy: 0.5328 - val_loss: 0.6939 - val_accuracy: 0.4854 Epoch 28/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6857 - accuracy: 0.5518 - val_loss: 0.6920 - val_accuracy: 0.5220 Epoch 29/300 839/839 [==============================] - 0s 226us/sample - loss: 0.6857 - accuracy: 0.5495 - val_loss: 0.6931 - val_accuracy: 0.5098 Epoch 30/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6834 - accuracy: 0.5602 - val_loss: 0.6951 - val_accuracy: 0.5098 Epoch 31/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6825 - accuracy: 0.5638 - val_loss: 0.6936 - val_accuracy: 0.5146 Epoch 32/300 839/839 [==============================] - 0s 244us/sample - loss: 0.6828 - accuracy: 0.5745 - val_loss: 0.6937 - val_accuracy: 0.5171 Epoch 33/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6822 - accuracy: 0.5721 - val_loss: 0.6941 - val_accuracy: 0.5146 Epoch 34/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6829 - accuracy: 0.5685 - val_loss: 0.6959 - val_accuracy: 0.5122 Epoch 35/300 839/839 [==============================] - 0s 226us/sample - loss: 0.6818 - accuracy: 0.5638 - val_loss: 0.6941 - val_accuracy: 0.5171 Epoch 36/300 839/839 [==============================] - 0s 263us/sample - loss: 0.6816 - accuracy: 0.5733 - val_loss: 0.6930 - val_accuracy: 0.5195 Epoch 37/300 839/839 [==============================] - 0s 241us/sample - loss: 0.6817 - accuracy: 0.5650 - val_loss: 0.6951 - val_accuracy: 0.5220 Epoch 38/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6816 - accuracy: 0.5507 - val_loss: 0.6927 - val_accuracy: 0.5341 Epoch 39/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6803 - accuracy: 0.5793 - val_loss: 0.6947 - val_accuracy: 0.5122 Epoch 40/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6795 - accuracy: 0.5626 - val_loss: 0.6942 - val_accuracy: 0.5268 Epoch 41/300 839/839 [==============================] - 0s 224us/sample - loss: 0.6792 - accuracy: 0.5626 - val_loss: 0.6946 - val_accuracy: 0.5220 Epoch 42/300 839/839 [==============================] - 0s 246us/sample - loss: 0.6796 - accuracy: 0.5745 - val_loss: 0.6931 - val_accuracy: 0.5244 Epoch 43/300 839/839 [==============================] - 0s 224us/sample - loss: 0.6795 - accuracy: 0.5542 - val_loss: 0.6962 - val_accuracy: 0.5122 Epoch 44/300 839/839 [==============================] - 0s 237us/sample - loss: 0.6784 - accuracy: 0.5781 - val_loss: 0.6947 - val_accuracy: 0.5122 Epoch 45/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6755 - accuracy: 0.5769 - val_loss: 0.6987 - val_accuracy: 0.5073 Epoch 46/300 839/839 [==============================] - 0s 228us/sample - loss: 0.6799 - accuracy: 0.5578 - val_loss: 0.6950 - val_accuracy: 0.5171 Epoch 47/300 839/839 [==============================] - 0s 251us/sample - loss: 0.6779 - accuracy: 0.5757 - val_loss: 0.6953 - val_accuracy: 0.5098 Epoch 48/300 839/839 [==============================] - 0s 234us/sample - loss: 0.6765 - accuracy: 0.5685 - val_loss: 0.6953 - val_accuracy: 0.5049 Epoch 49/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6738 - accuracy: 0.5733 - val_loss: 0.6964 - val_accuracy: 0.5049 Epoch 50/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6753 - accuracy: 0.5662 - val_loss: 0.6969 - val_accuracy: 0.5098 Epoch 51/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6733 - accuracy: 0.5745 - val_loss: 0.6973 - val_accuracy: 0.5098 Epoch 52/300 839/839 [==============================] - 0s 253us/sample - loss: 0.6744 - accuracy: 0.5757 - val_loss: 0.7016 - val_accuracy: 0.4976 Epoch 53/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6717 - accuracy: 0.5650 - val_loss: 0.6987 - val_accuracy: 0.5122 Epoch 54/300 839/839 [==============================] - 0s 220us/sample - loss: 0.6703 - accuracy: 0.5709 - val_loss: 0.6997 - val_accuracy: 0.4976 Epoch 55/300 839/839 [==============================] - 0s 233us/sample - loss: 0.6695 - accuracy: 0.5864 - val_loss: 0.6986 - val_accuracy: 0.5073 Epoch 56/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6708 - accuracy: 0.5781 - val_loss: 0.7012 - val_accuracy: 0.5024 Epoch 57/300 839/839 [==============================] - 0s 225us/sample - loss: 0.6716 - accuracy: 0.5745 - val_loss: 0.7007 - val_accuracy: 0.5024 Epoch 58/300 839/839 [==============================] - 0s 243us/sample - loss: 0.6699 - accuracy: 0.5828 - val_loss: 0.7017 - val_accuracy: 0.4902 Epoch 59/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6656 - accuracy: 0.5840 - val_loss: 0.7005 - val_accuracy: 0.5146 Epoch 60/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6654 - accuracy: 0.5781 - val_loss: 0.7066 - val_accuracy: 0.5000 Epoch 61/300 839/839 [==============================] - 0s 224us/sample - loss: 0.6689 - accuracy: 0.5685 - val_loss: 0.7009 - val_accuracy: 0.5098 Epoch 62/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6640 - accuracy: 0.6007 - val_loss: 0.7120 - val_accuracy: 0.4951 Epoch 63/300 839/839 [==============================] - 0s 239us/sample - loss: 0.6632 - accuracy: 0.5864 - val_loss: 0.6998 - val_accuracy: 0.5049 Epoch 64/300 839/839 [==============================] - 0s 236us/sample - loss: 0.6655 - accuracy: 0.5959 - val_loss: 0.7051 - val_accuracy: 0.5122 Epoch 65/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6616 - accuracy: 0.5864 - val_loss: 0.7050 - val_accuracy: 0.4951 Epoch 66/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6598 - accuracy: 0.5948 - val_loss: 0.7053 - val_accuracy: 0.4878 Epoch 67/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6596 - accuracy: 0.5912 - val_loss: 0.7044 - val_accuracy: 0.4976 Epoch 68/300 839/839 [==============================] - 0s 238us/sample - loss: 0.6580 - accuracy: 0.5971 - val_loss: 0.7052 - val_accuracy: 0.5049 Epoch 69/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6559 - accuracy: 0.5959 - val_loss: 0.7074 - val_accuracy: 0.5000 Epoch 70/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6579 - accuracy: 0.6019 - val_loss: 0.7053 - val_accuracy: 0.5049 Epoch 71/300 839/839 [==============================] - 0s 236us/sample - loss: 0.6540 - accuracy: 0.5948 - val_loss: 0.7123 - val_accuracy: 0.4829 Epoch 72/300 839/839 [==============================] - 0s 238us/sample - loss: 0.6532 - accuracy: 0.6031 - val_loss: 0.7101 - val_accuracy: 0.5000 Epoch 73/300 839/839 [==============================] - 0s 250us/sample - loss: 0.6518 - accuracy: 0.5995 - val_loss: 0.7110 - val_accuracy: 0.4927 Epoch 74/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6512 - accuracy: 0.5888 - val_loss: 0.7119 - val_accuracy: 0.4902 Epoch 75/300 839/839 [==============================] - 0s 230us/sample - loss: 0.6467 - accuracy: 0.6019 - val_loss: 0.7134 - val_accuracy: 0.5049 Epoch 76/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6500 - accuracy: 0.6031 - val_loss: 0.7183 - val_accuracy: 0.4902 Epoch 77/300 839/839 [==============================] - 0s 234us/sample - loss: 0.6479 - accuracy: 0.6079 - val_loss: 0.7141 - val_accuracy: 0.4854 Epoch 78/300 839/839 [==============================] - 0s 250us/sample - loss: 0.6475 - accuracy: 0.6174 - val_loss: 0.7129 - val_accuracy: 0.4976 Epoch 79/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6503 - accuracy: 0.5995 - val_loss: 0.7134 - val_accuracy: 0.4976 Epoch 80/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6484 - accuracy: 0.5936 - val_loss: 0.7196 - val_accuracy: 0.4878 Epoch 81/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6427 - accuracy: 0.6114 - val_loss: 0.7213 - val_accuracy: 0.4902 Epoch 82/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6402 - accuracy: 0.6079 - val_loss: 0.7244 - val_accuracy: 0.4902 Epoch 83/300 839/839 [==============================] - 0s 239us/sample - loss: 0.6366 - accuracy: 0.6174 - val_loss: 0.7321 - val_accuracy: 0.4902 Epoch 84/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6365 - accuracy: 0.6246 - val_loss: 0.7324 - val_accuracy: 0.4707 Epoch 85/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6364 - accuracy: 0.6103 - val_loss: 0.7287 - val_accuracy: 0.4878 Epoch 86/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6331 - accuracy: 0.6317 - val_loss: 0.7351 - val_accuracy: 0.4780 Epoch 87/300 839/839 [==============================] - 0s 223us/sample - loss: 0.6262 - accuracy: 0.6389 - val_loss: 0.7337 - val_accuracy: 0.4683 Epoch 88/300 839/839 [==============================] - 0s 271us/sample - loss: 0.6332 - accuracy: 0.6210 - val_loss: 0.7299 - val_accuracy: 0.4878 Epoch 89/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6295 - accuracy: 0.6234 - val_loss: 0.7332 - val_accuracy: 0.4659 Epoch 90/300 839/839 [==============================] - 0s 233us/sample - loss: 0.6278 - accuracy: 0.6234 - val_loss: 0.7433 - val_accuracy: 0.4707 Epoch 91/300 839/839 [==============================] - 0s 227us/sample - loss: 0.6265 - accuracy: 0.6210 - val_loss: 0.7453 - val_accuracy: 0.4707 Epoch 92/300 839/839 [==============================] - 0s 234us/sample - loss: 0.6205 - accuracy: 0.6353 - val_loss: 0.7377 - val_accuracy: 0.4561 Epoch 93/300 839/839 [==============================] - 0s 244us/sample - loss: 0.6213 - accuracy: 0.6472 - val_loss: 0.7452 - val_accuracy: 0.4610 Epoch 94/300 839/839 [==============================] - 0s 226us/sample - loss: 0.6187 - accuracy: 0.6353 - val_loss: 0.7305 - val_accuracy: 0.4659 Epoch 95/300 839/839 [==============================] - 0s 234us/sample - loss: 0.6149 - accuracy: 0.6377 - val_loss: 0.7415 - val_accuracy: 0.4756 Epoch 96/300 839/839 [==============================] - 0s 232us/sample - loss: 0.6211 - accuracy: 0.6389 - val_loss: 0.7494 - val_accuracy: 0.4780 Epoch 97/300 839/839 [==============================] - 0s 231us/sample - loss: 0.6182 - accuracy: 0.6412 - val_loss: 0.7506 - val_accuracy: 0.4634 Epoch 98/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6123 - accuracy: 0.6424 - val_loss: 0.7368 - val_accuracy: 0.4805 Epoch 99/300 839/839 [==============================] - 0s 246us/sample - loss: 0.6165 - accuracy: 0.6317 - val_loss: 0.7546 - val_accuracy: 0.4634 Epoch 100/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6076 - accuracy: 0.6555 - val_loss: 0.7510 - val_accuracy: 0.4659 Epoch 101/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6032 - accuracy: 0.6579 - val_loss: 0.7475 - val_accuracy: 0.4780 Epoch 102/300 839/839 [==============================] - 0s 229us/sample - loss: 0.6038 - accuracy: 0.6460 - val_loss: 0.7579 - val_accuracy: 0.4732 Epoch 103/300 839/839 [==============================] - 0s 235us/sample - loss: 0.6036 - accuracy: 0.6544 - val_loss: 0.7605 - val_accuracy: 0.4732 Epoch 104/300 839/839 [==============================] - 0s 257us/sample - loss: 0.6001 - accuracy: 0.6555 - val_loss: 0.7650 - val_accuracy: 0.4854 Epoch 105/300 839/839 [==============================] - 0s 221us/sample - loss: 0.5982 - accuracy: 0.6579 - val_loss: 0.7669 - val_accuracy: 0.4659 Epoch 106/300 839/839 [==============================] - 0s 225us/sample - loss: 0.5946 - accuracy: 0.6532 - val_loss: 0.7702 - val_accuracy: 0.4634 Epoch 107/300 839/839 [==============================] - 0s 227us/sample - loss: 0.5914 - accuracy: 0.6484 - val_loss: 0.7694 - val_accuracy: 0.4537 Epoch 108/300 839/839 [==============================] - 0s 229us/sample - loss: 0.5894 - accuracy: 0.6651 - val_loss: 0.7644 - val_accuracy: 0.4561 Epoch 109/300 839/839 [==============================] - 0s 243us/sample - loss: 0.5855 - accuracy: 0.6698 - val_loss: 0.7878 - val_accuracy: 0.4561 Epoch 110/300 839/839 [==============================] - 0s 225us/sample - loss: 0.5890 - accuracy: 0.6722 - val_loss: 0.7798 - val_accuracy: 0.4659 Epoch 111/300 839/839 [==============================] - 0s 226us/sample - loss: 0.5869 - accuracy: 0.6698 - val_loss: 0.7716 - val_accuracy: 0.4732 Epoch 112/300 839/839 [==============================] - 0s 239us/sample - loss: 0.5867 - accuracy: 0.6627 - val_loss: 0.7694 - val_accuracy: 0.4732 Epoch 113/300 839/839 [==============================] - 0s 222us/sample - loss: 0.5815 - accuracy: 0.6722 - val_loss: 0.7673 - val_accuracy: 0.4805 Epoch 114/300 839/839 [==============================] - 0s 237us/sample - loss: 0.5819 - accuracy: 0.6675 - val_loss: 0.7746 - val_accuracy: 0.4659 Epoch 115/300 839/839 [==============================] - 0s 230us/sample - loss: 0.5795 - accuracy: 0.6794 - val_loss: 0.7947 - val_accuracy: 0.4488 Epoch 116/300 839/839 [==============================] - 0s 236us/sample - loss: 0.5747 - accuracy: 0.6794 - val_loss: 0.7833 - val_accuracy: 0.4463 Epoch 117/300 839/839 [==============================] - 0s 227us/sample - loss: 0.5731 - accuracy: 0.6782 - val_loss: 0.7782 - val_accuracy: 0.4683 Epoch 118/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5749 - accuracy: 0.6579 - val_loss: 0.7906 - val_accuracy: 0.4561 Epoch 119/300 839/839 [==============================] - 0s 251us/sample - loss: 0.5842 - accuracy: 0.6698 - val_loss: 0.7910 - val_accuracy: 0.4683 Epoch 120/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5716 - accuracy: 0.6746 - val_loss: 0.7893 - val_accuracy: 0.4732 Epoch 121/300 839/839 [==============================] - 0s 226us/sample - loss: 0.5704 - accuracy: 0.6806 - val_loss: 0.7887 - val_accuracy: 0.4707 Epoch 122/300 839/839 [==============================] - 0s 232us/sample - loss: 0.5719 - accuracy: 0.6746 - val_loss: 0.7765 - val_accuracy: 0.4683 Epoch 123/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5609 - accuracy: 0.6937 - val_loss: 0.7892 - val_accuracy: 0.4659 Epoch 124/300 839/839 [==============================] - 0s 253us/sample - loss: 0.5650 - accuracy: 0.6853 - val_loss: 0.7970 - val_accuracy: 0.4610 Epoch 125/300 839/839 [==============================] - 0s 222us/sample - loss: 0.5561 - accuracy: 0.6925 - val_loss: 0.7993 - val_accuracy: 0.4659 Epoch 126/300 839/839 [==============================] - 0s 222us/sample - loss: 0.5549 - accuracy: 0.6865 - val_loss: 0.7989 - val_accuracy: 0.4732 Epoch 127/300 839/839 [==============================] - 0s 230us/sample - loss: 0.5527 - accuracy: 0.6841 - val_loss: 0.8133 - val_accuracy: 0.4683 Epoch 128/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5459 - accuracy: 0.6996 - val_loss: 0.7921 - val_accuracy: 0.4732 Epoch 129/300 839/839 [==============================] - 0s 242us/sample - loss: 0.5440 - accuracy: 0.6973 - val_loss: 0.8099 - val_accuracy: 0.4683 Epoch 130/300 839/839 [==============================] - 0s 224us/sample - loss: 0.5440 - accuracy: 0.6913 - val_loss: 0.8107 - val_accuracy: 0.4756 Epoch 131/300 839/839 [==============================] - 0s 230us/sample - loss: 0.5413 - accuracy: 0.6949 - val_loss: 0.8150 - val_accuracy: 0.4512 Epoch 132/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5446 - accuracy: 0.7032 - val_loss: 0.8127 - val_accuracy: 0.4756 Epoch 133/300 839/839 [==============================] - 0s 220us/sample - loss: 0.5357 - accuracy: 0.7199 - val_loss: 0.8074 - val_accuracy: 0.4610 Epoch 134/300 839/839 [==============================] - 0s 229us/sample - loss: 0.5530 - accuracy: 0.6877 - val_loss: 0.8165 - val_accuracy: 0.4634 Epoch 135/300 839/839 [==============================] - 0s 250us/sample - loss: 0.5324 - accuracy: 0.7175 - val_loss: 0.8221 - val_accuracy: 0.4634 Epoch 136/300 839/839 [==============================] - 0s 231us/sample - loss: 0.5403 - accuracy: 0.7068 - val_loss: 0.8180 - val_accuracy: 0.4537 Epoch 137/300 839/839 [==============================] - 0s 228us/sample - loss: 0.5320 - accuracy: 0.7163 - val_loss: 0.8175 - val_accuracy: 0.4732 Epoch 138/300 839/839 [==============================] - 0s 222us/sample - loss: 0.5369 - accuracy: 0.7008 - val_loss: 0.8190 - val_accuracy: 0.4512 Epoch 139/300 839/839 [==============================] - 0s 261us/sample - loss: 0.5270 - accuracy: 0.7128 - val_loss: 0.8141 - val_accuracy: 0.4683 Epoch 140/300 839/839 [==============================] - 0s 240us/sample - loss: 0.5216 - accuracy: 0.7282 - val_loss: 0.8281 - val_accuracy: 0.4463 Epoch 141/300 839/839 [==============================] - 0s 224us/sample - loss: 0.5183 - accuracy: 0.7235 - val_loss: 0.8204 - val_accuracy: 0.4683 Epoch 142/300 839/839 [==============================] - 0s 224us/sample - loss: 0.5174 - accuracy: 0.7259 - val_loss: 0.8253 - val_accuracy: 0.4561 Epoch 143/300 839/839 [==============================] - 0s 225us/sample - loss: 0.5185 - accuracy: 0.7223 - val_loss: 0.8074 - val_accuracy: 0.4732 Epoch 144/300 839/839 [==============================] - 0s 235us/sample - loss: 0.5198 - accuracy: 0.7259 - val_loss: 0.8325 - val_accuracy: 0.4488 Epoch 145/300 839/839 [==============================] - 0s 238us/sample - loss: 0.5041 - accuracy: 0.7366 - val_loss: 0.8289 - val_accuracy: 0.4585 Epoch 146/300 839/839 [==============================] - 0s 231us/sample - loss: 0.5039 - accuracy: 0.7354 - val_loss: 0.8330 - val_accuracy: 0.4634 Epoch 147/300 839/839 [==============================] - 0s 233us/sample - loss: 0.5029 - accuracy: 0.7461 - val_loss: 0.8260 - val_accuracy: 0.4439 Epoch 148/300 839/839 [==============================] - 0s 229us/sample - loss: 0.5089 - accuracy: 0.7294 - val_loss: 0.8477 - val_accuracy: 0.4659 Epoch 149/300 839/839 [==============================] - 0s 226us/sample - loss: 0.4999 - accuracy: 0.7414 - val_loss: 0.8250 - val_accuracy: 0.4439 Epoch 150/300 839/839 [==============================] - 0s 238us/sample - loss: 0.4918 - accuracy: 0.7390 - val_loss: 0.8383 - val_accuracy: 0.4634 Epoch 151/300 839/839 [==============================] - 0s 227us/sample - loss: 0.4952 - accuracy: 0.7366 - val_loss: 0.8418 - val_accuracy: 0.4634 Epoch 152/300 839/839 [==============================] - 0s 236us/sample - loss: 0.4889 - accuracy: 0.7521 - val_loss: 0.8415 - val_accuracy: 0.4585 Epoch 153/300 839/839 [==============================] - 0s 227us/sample - loss: 0.4829 - accuracy: 0.7545 - val_loss: 0.8385 - val_accuracy: 0.4610 Epoch 154/300 839/839 [==============================] - 0s 222us/sample - loss: 0.4856 - accuracy: 0.7414 - val_loss: 0.8599 - val_accuracy: 0.4561 Epoch 155/300 839/839 [==============================] - 0s 235us/sample - loss: 0.4784 - accuracy: 0.7557 - val_loss: 0.8614 - val_accuracy: 0.4512 Epoch 156/300 839/839 [==============================] - 0s 223us/sample - loss: 0.4741 - accuracy: 0.7628 - val_loss: 0.8526 - val_accuracy: 0.4512 Epoch 157/300 839/839 [==============================] - 0s 228us/sample - loss: 0.4781 - accuracy: 0.7688 - val_loss: 0.8476 - val_accuracy: 0.4707 Epoch 158/300 839/839 [==============================] - 0s 229us/sample - loss: 0.4776 - accuracy: 0.7485 - val_loss: 0.8494 - val_accuracy: 0.4537 Epoch 159/300 839/839 [==============================] - 0s 230us/sample - loss: 0.4717 - accuracy: 0.7497 - val_loss: 0.8671 - val_accuracy: 0.4488 Epoch 160/300 839/839 [==============================] - 0s 239us/sample - loss: 0.4660 - accuracy: 0.7664 - val_loss: 0.8559 - val_accuracy: 0.4610 Epoch 161/300 839/839 [==============================] - 0s 225us/sample - loss: 0.4675 - accuracy: 0.7592 - val_loss: 0.8709 - val_accuracy: 0.4634 Epoch 162/300 839/839 [==============================] - 0s 233us/sample - loss: 0.4657 - accuracy: 0.7652 - val_loss: 0.8544 - val_accuracy: 0.4659 Epoch 163/300 839/839 [==============================] - 0s 227us/sample - loss: 0.4662 - accuracy: 0.7592 - val_loss: 0.8584 - val_accuracy: 0.4634 Epoch 164/300 839/839 [==============================] - 0s 239us/sample - loss: 0.4613 - accuracy: 0.7783 - val_loss: 0.8703 - val_accuracy: 0.4634 Epoch 165/300 839/839 [==============================] - 0s 226us/sample - loss: 0.4571 - accuracy: 0.7747 - val_loss: 0.8628 - val_accuracy: 0.4634 Epoch 166/300 839/839 [==============================] - 0s 255us/sample - loss: 0.4465 - accuracy: 0.7747 - val_loss: 0.8725 - val_accuracy: 0.4683 Epoch 167/300 839/839 [==============================] - 0s 232us/sample - loss: 0.4495 - accuracy: 0.7795 - val_loss: 0.8717 - val_accuracy: 0.4805 Epoch 168/300 839/839 [==============================] - 0s 224us/sample - loss: 0.4508 - accuracy: 0.7783 - val_loss: 0.8831 - val_accuracy: 0.4659 Epoch 169/300 839/839 [==============================] - 0s 227us/sample - loss: 0.4540 - accuracy: 0.7712 - val_loss: 0.8799 - val_accuracy: 0.4707 Epoch 170/300 839/839 [==============================] - 0s 235us/sample - loss: 0.4417 - accuracy: 0.7819 - val_loss: 0.8839 - val_accuracy: 0.4707 Epoch 171/300 839/839 [==============================] - 0s 243us/sample - loss: 0.4524 - accuracy: 0.7771 - val_loss: 0.8871 - val_accuracy: 0.4634 Epoch 172/300 839/839 [==============================] - 0s 229us/sample - loss: 0.4376 - accuracy: 0.7855 - val_loss: 0.8819 - val_accuracy: 0.4732 Epoch 173/300 839/839 [==============================] - 0s 225us/sample - loss: 0.4345 - accuracy: 0.7867 - val_loss: 0.9004 - val_accuracy: 0.4707 Epoch 174/300 839/839 [==============================] - 0s 230us/sample - loss: 0.4330 - accuracy: 0.7890 - val_loss: 0.9038 - val_accuracy: 0.4780 Epoch 175/300 839/839 [==============================] - 0s 231us/sample - loss: 0.4293 - accuracy: 0.7974 - val_loss: 0.8944 - val_accuracy: 0.4659 Epoch 176/300 839/839 [==============================] - 0s 245us/sample - loss: 0.4300 - accuracy: 0.7878 - val_loss: 0.9014 - val_accuracy: 0.4780 Epoch 177/300 839/839 [==============================] - 0s 222us/sample - loss: 0.4231 - accuracy: 0.7938 - val_loss: 0.8976 - val_accuracy: 0.4829 Epoch 178/300 839/839 [==============================] - 0s 233us/sample - loss: 0.4206 - accuracy: 0.7926 - val_loss: 0.9104 - val_accuracy: 0.4829 Epoch 179/300 839/839 [==============================] - 0s 233us/sample - loss: 0.4240 - accuracy: 0.7902 - val_loss: 0.9080 - val_accuracy: 0.4854 Epoch 180/300 839/839 [==============================] - 0s 228us/sample - loss: 0.4198 - accuracy: 0.7926 - val_loss: 0.9067 - val_accuracy: 0.4854 Epoch 181/300 839/839 [==============================] - 0s 240us/sample - loss: 0.4153 - accuracy: 0.7938 - val_loss: 0.9102 - val_accuracy: 0.4878 Epoch 182/300 839/839 [==============================] - 0s 235us/sample - loss: 0.4161 - accuracy: 0.7926 - val_loss: 0.9071 - val_accuracy: 0.4854 Epoch 183/300 839/839 [==============================] - 0s 235us/sample - loss: 0.4160 - accuracy: 0.7998 - val_loss: 0.9261 - val_accuracy: 0.4634 Epoch 184/300 839/839 [==============================] - 0s 224us/sample - loss: 0.4153 - accuracy: 0.7986 - val_loss: 0.9151 - val_accuracy: 0.4683 Epoch 185/300 839/839 [==============================] - 0s 225us/sample - loss: 0.4110 - accuracy: 0.7926 - val_loss: 0.9413 - val_accuracy: 0.4707 Epoch 186/300 839/839 [==============================] - 0s 248us/sample - loss: 0.4073 - accuracy: 0.8033 - val_loss: 0.9216 - val_accuracy: 0.4829 Epoch 187/300 839/839 [==============================] - 0s 229us/sample - loss: 0.3984 - accuracy: 0.8093 - val_loss: 0.9198 - val_accuracy: 0.4878 Epoch 188/300 839/839 [==============================] - 0s 236us/sample - loss: 0.3971 - accuracy: 0.8081 - val_loss: 0.9325 - val_accuracy: 0.4780 Epoch 189/300 839/839 [==============================] - 0s 238us/sample - loss: 0.3975 - accuracy: 0.7998 - val_loss: 0.9169 - val_accuracy: 0.4756 Epoch 190/300 839/839 [==============================] - 0s 253us/sample - loss: 0.3974 - accuracy: 0.8033 - val_loss: 0.9268 - val_accuracy: 0.4780 Epoch 191/300 839/839 [==============================] - 0s 243us/sample - loss: 0.4044 - accuracy: 0.8045 - val_loss: 0.9188 - val_accuracy: 0.4902 Epoch 192/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3939 - accuracy: 0.8141 - val_loss: 0.9361 - val_accuracy: 0.4902 Epoch 193/300 839/839 [==============================] - 0s 230us/sample - loss: 0.3997 - accuracy: 0.8069 - val_loss: 0.9397 - val_accuracy: 0.4805 Epoch 194/300 839/839 [==============================] - 0s 227us/sample - loss: 0.3846 - accuracy: 0.8200 - val_loss: 0.9364 - val_accuracy: 0.4756 Epoch 195/300 839/839 [==============================] - 0s 232us/sample - loss: 0.3861 - accuracy: 0.8153 - val_loss: 0.9314 - val_accuracy: 0.4878 Epoch 196/300 839/839 [==============================] - 0s 258us/sample - loss: 0.3818 - accuracy: 0.8129 - val_loss: 0.9423 - val_accuracy: 0.4976 Epoch 197/300 839/839 [==============================] - 0s 225us/sample - loss: 0.3822 - accuracy: 0.8141 - val_loss: 0.9365 - val_accuracy: 0.4756 Epoch 198/300 839/839 [==============================] - 0s 229us/sample - loss: 0.3744 - accuracy: 0.8260 - val_loss: 0.9652 - val_accuracy: 0.4927 Epoch 199/300 839/839 [==============================] - 0s 230us/sample - loss: 0.3773 - accuracy: 0.8272 - val_loss: 0.9589 - val_accuracy: 0.4780 Epoch 200/300 839/839 [==============================] - 0s 236us/sample - loss: 0.3652 - accuracy: 0.8391 - val_loss: 0.9297 - val_accuracy: 0.5122 Epoch 201/300 839/839 [==============================] - 0s 249us/sample - loss: 0.3681 - accuracy: 0.8284 - val_loss: 0.9497 - val_accuracy: 0.4878 Epoch 202/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3716 - accuracy: 0.8188 - val_loss: 0.9587 - val_accuracy: 0.4780 Epoch 203/300 839/839 [==============================] - 0s 232us/sample - loss: 0.3610 - accuracy: 0.8319 - val_loss: 0.9399 - val_accuracy: 0.4902 Epoch 204/300 839/839 [==============================] - 0s 234us/sample - loss: 0.3559 - accuracy: 0.8379 - val_loss: 0.9560 - val_accuracy: 0.5000 Epoch 205/300 839/839 [==============================] - 0s 237us/sample - loss: 0.3600 - accuracy: 0.8343 - val_loss: 0.9606 - val_accuracy: 0.4927 Epoch 206/300 839/839 [==============================] - 0s 238us/sample - loss: 0.3483 - accuracy: 0.8415 - val_loss: 0.9535 - val_accuracy: 0.5000 Epoch 207/300 839/839 [==============================] - 0s 222us/sample - loss: 0.3447 - accuracy: 0.8355 - val_loss: 0.9562 - val_accuracy: 0.4854 Epoch 208/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3439 - accuracy: 0.8391 - val_loss: 0.9458 - val_accuracy: 0.4780 Epoch 209/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3452 - accuracy: 0.8427 - val_loss: 0.9819 - val_accuracy: 0.4805 Epoch 210/300 839/839 [==============================] - 0s 231us/sample - loss: 0.3417 - accuracy: 0.8451 - val_loss: 0.9985 - val_accuracy: 0.4732 Epoch 211/300 839/839 [==============================] - 0s 229us/sample - loss: 0.3533 - accuracy: 0.8355 - val_loss: 0.9619 - val_accuracy: 0.4805 Epoch 212/300 839/839 [==============================] - 0s 243us/sample - loss: 0.3368 - accuracy: 0.8522 - val_loss: 0.9666 - val_accuracy: 0.4976 Epoch 213/300 839/839 [==============================] - 0s 223us/sample - loss: 0.3305 - accuracy: 0.8439 - val_loss: 0.9618 - val_accuracy: 0.4902 Epoch 214/300 839/839 [==============================] - 0s 236us/sample - loss: 0.3266 - accuracy: 0.8474 - val_loss: 0.9895 - val_accuracy: 0.4927 Epoch 215/300 839/839 [==============================] - 0s 227us/sample - loss: 0.3365 - accuracy: 0.8343 - val_loss: 0.9858 - val_accuracy: 0.5000 Epoch 216/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3256 - accuracy: 0.8462 - val_loss: 0.9796 - val_accuracy: 0.4854 Epoch 217/300 839/839 [==============================] - 0s 249us/sample - loss: 0.3292 - accuracy: 0.8546 - val_loss: 1.0061 - val_accuracy: 0.4780 Epoch 218/300 839/839 [==============================] - 0s 222us/sample - loss: 0.3259 - accuracy: 0.8439 - val_loss: 1.0063 - val_accuracy: 0.4805 Epoch 219/300 839/839 [==============================] - 0s 224us/sample - loss: 0.3204 - accuracy: 0.8570 - val_loss: 0.9881 - val_accuracy: 0.4756 Epoch 220/300 839/839 [==============================] - 0s 229us/sample - loss: 0.3316 - accuracy: 0.8403 - val_loss: 0.9789 - val_accuracy: 0.4854 Epoch 221/300 839/839 [==============================] - 0s 225us/sample - loss: 0.3185 - accuracy: 0.8486 - val_loss: 1.0082 - val_accuracy: 0.4756 Epoch 222/300 839/839 [==============================] - 0s 235us/sample - loss: 0.3266 - accuracy: 0.8462 - val_loss: 1.0083 - val_accuracy: 0.4780 Epoch 223/300 839/839 [==============================] - 0s 220us/sample - loss: 0.3113 - accuracy: 0.8462 - val_loss: 1.0257 - val_accuracy: 0.4756 Epoch 224/300 839/839 [==============================] - 0s 228us/sample - loss: 0.3023 - accuracy: 0.8641 - val_loss: 1.0054 - val_accuracy: 0.4854 Epoch 225/300 839/839 [==============================] - 0s 233us/sample - loss: 0.2967 - accuracy: 0.8641 - val_loss: 1.0320 - val_accuracy: 0.4829 Epoch 226/300 839/839 [==============================] - 0s 225us/sample - loss: 0.3007 - accuracy: 0.8582 - val_loss: 1.0144 - val_accuracy: 0.4756 Epoch 227/300 839/839 [==============================] - 0s 233us/sample - loss: 0.3366 - accuracy: 0.8439 - val_loss: 1.0106 - val_accuracy: 0.4805 Epoch 228/300 839/839 [==============================] - 0s 230us/sample - loss: 0.3243 - accuracy: 0.8474 - val_loss: 1.0429 - val_accuracy: 0.4780 Epoch 229/300 839/839 [==============================] - 0s 241us/sample - loss: 0.2990 - accuracy: 0.8629 - val_loss: 1.0181 - val_accuracy: 0.4927 Epoch 230/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2901 - accuracy: 0.8594 - val_loss: 1.0292 - val_accuracy: 0.4902 Epoch 231/300 839/839 [==============================] - 0s 235us/sample - loss: 0.3153 - accuracy: 0.8558 - val_loss: 1.0244 - val_accuracy: 0.4780 Epoch 232/300 839/839 [==============================] - 0s 234us/sample - loss: 0.2941 - accuracy: 0.8653 - val_loss: 1.0348 - val_accuracy: 0.4927 Epoch 233/300 839/839 [==============================] - 0s 239us/sample - loss: 0.2882 - accuracy: 0.8629 - val_loss: 1.0568 - val_accuracy: 0.4732 Epoch 234/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2857 - accuracy: 0.8665 - val_loss: 1.0517 - val_accuracy: 0.4780 Epoch 235/300 839/839 [==============================] - 0s 226us/sample - loss: 0.2800 - accuracy: 0.8701 - val_loss: 1.0463 - val_accuracy: 0.4659 Epoch 236/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2967 - accuracy: 0.8629 - val_loss: 1.0587 - val_accuracy: 0.4854 Epoch 237/300 839/839 [==============================] - 0s 231us/sample - loss: 0.2799 - accuracy: 0.8749 - val_loss: 1.0579 - val_accuracy: 0.4780 Epoch 238/300 839/839 [==============================] - 0s 260us/sample - loss: 0.2790 - accuracy: 0.8760 - val_loss: 1.0519 - val_accuracy: 0.4780 Epoch 239/300 839/839 [==============================] - 0s 220us/sample - loss: 0.2760 - accuracy: 0.8737 - val_loss: 1.0895 - val_accuracy: 0.4756 Epoch 240/300 839/839 [==============================] - 0s 224us/sample - loss: 0.2781 - accuracy: 0.8701 - val_loss: 1.0483 - val_accuracy: 0.4902 Epoch 241/300 839/839 [==============================] - 0s 264us/sample - loss: 0.2709 - accuracy: 0.8760 - val_loss: 1.0658 - val_accuracy: 0.4756 Epoch 242/300 839/839 [==============================] - 0s 232us/sample - loss: 0.2641 - accuracy: 0.8808 - val_loss: 1.0596 - val_accuracy: 0.4780 Epoch 243/300 839/839 [==============================] - 0s 247us/sample - loss: 0.2663 - accuracy: 0.8784 - val_loss: 1.0631 - val_accuracy: 0.4829 Epoch 244/300 839/839 [==============================] - 0s 234us/sample - loss: 0.2679 - accuracy: 0.8749 - val_loss: 1.0996 - val_accuracy: 0.4780 Epoch 245/300 839/839 [==============================] - 0s 234us/sample - loss: 0.2604 - accuracy: 0.8737 - val_loss: 1.0590 - val_accuracy: 0.4854 Epoch 246/300 839/839 [==============================] - 0s 225us/sample - loss: 0.2627 - accuracy: 0.8844 - val_loss: 1.0861 - val_accuracy: 0.4805 Epoch 247/300 839/839 [==============================] - 0s 223us/sample - loss: 0.2566 - accuracy: 0.8844 - val_loss: 1.0796 - val_accuracy: 0.4780 Epoch 248/300 839/839 [==============================] - 0s 253us/sample - loss: 0.2547 - accuracy: 0.8820 - val_loss: 1.0945 - val_accuracy: 0.4707 Epoch 249/300 839/839 [==============================] - 0s 230us/sample - loss: 0.2501 - accuracy: 0.8975 - val_loss: 1.1233 - val_accuracy: 0.4829 Epoch 250/300 839/839 [==============================] - 0s 224us/sample - loss: 0.2443 - accuracy: 0.8963 - val_loss: 1.1275 - val_accuracy: 0.4756 Epoch 251/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2532 - accuracy: 0.8808 - val_loss: 1.0957 - val_accuracy: 0.4659 Epoch 252/300 839/839 [==============================] - 0s 227us/sample - loss: 0.2447 - accuracy: 0.8975 - val_loss: 1.1147 - val_accuracy: 0.4927 Epoch 253/300 839/839 [==============================] - 0s 241us/sample - loss: 0.2465 - accuracy: 0.8856 - val_loss: 1.1121 - val_accuracy: 0.4707 Epoch 254/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2442 - accuracy: 0.8844 - val_loss: 1.1563 - val_accuracy: 0.4780 Epoch 255/300 839/839 [==============================] - 0s 243us/sample - loss: 0.2382 - accuracy: 0.8999 - val_loss: 1.1053 - val_accuracy: 0.4756 Epoch 256/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2340 - accuracy: 0.8963 - val_loss: 1.1087 - val_accuracy: 0.4805 Epoch 257/300 839/839 [==============================] - 0s 233us/sample - loss: 0.2360 - accuracy: 0.8951 - val_loss: 1.1373 - val_accuracy: 0.4756 Epoch 258/300 839/839 [==============================] - 0s 246us/sample - loss: 0.2412 - accuracy: 0.8951 - val_loss: 1.1333 - val_accuracy: 0.4878 Epoch 259/300 839/839 [==============================] - 0s 227us/sample - loss: 0.2410 - accuracy: 0.8915 - val_loss: 1.1303 - val_accuracy: 0.4878 Epoch 260/300 839/839 [==============================] - 0s 231us/sample - loss: 0.2266 - accuracy: 0.8987 - val_loss: 1.1675 - val_accuracy: 0.4927 Epoch 261/300 839/839 [==============================] - 0s 225us/sample - loss: 0.2395 - accuracy: 0.8832 - val_loss: 1.1388 - val_accuracy: 0.4732 Epoch 262/300 839/839 [==============================] - 0s 232us/sample - loss: 0.2390 - accuracy: 0.8987 - val_loss: 1.1354 - val_accuracy: 0.4683 Epoch 263/300 839/839 [==============================] - 0s 243us/sample - loss: 0.2320 - accuracy: 0.8939 - val_loss: 1.1302 - val_accuracy: 0.4854 Epoch 264/300 839/839 [==============================] - 0s 226us/sample - loss: 0.2311 - accuracy: 0.9011 - val_loss: 1.1624 - val_accuracy: 0.4854 Epoch 265/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2252 - accuracy: 0.9011 - val_loss: 1.1641 - val_accuracy: 0.4951 Epoch 266/300 839/839 [==============================] - 0s 238us/sample - loss: 0.2339 - accuracy: 0.8975 - val_loss: 1.1559 - val_accuracy: 0.4951 Epoch 267/300 839/839 [==============================] - 0s 236us/sample - loss: 0.2179 - accuracy: 0.9046 - val_loss: 1.1967 - val_accuracy: 0.4878 Epoch 268/300 839/839 [==============================] - 0s 246us/sample - loss: 0.2178 - accuracy: 0.9046 - val_loss: 1.1752 - val_accuracy: 0.5024 Epoch 269/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2176 - accuracy: 0.9070 - val_loss: 1.1572 - val_accuracy: 0.4976 Epoch 270/300 839/839 [==============================] - 0s 230us/sample - loss: 0.2251 - accuracy: 0.8999 - val_loss: 1.2162 - val_accuracy: 0.4878 Epoch 271/300 839/839 [==============================] - 0s 240us/sample - loss: 0.3056 - accuracy: 0.8605 - val_loss: 1.1761 - val_accuracy: 0.4878 Epoch 272/300 839/839 [==============================] - 0s 235us/sample - loss: 0.3435 - accuracy: 0.8403 - val_loss: 1.1900 - val_accuracy: 0.4951 Epoch 273/300 839/839 [==============================] - 0s 241us/sample - loss: 0.2510 - accuracy: 0.8987 - val_loss: 1.1899 - val_accuracy: 0.4878 Epoch 274/300 839/839 [==============================] - 0s 226us/sample - loss: 0.2561 - accuracy: 0.8963 - val_loss: 1.2053 - val_accuracy: 0.4829 Epoch 275/300 839/839 [==============================] - 0s 228us/sample - loss: 0.2505 - accuracy: 0.8987 - val_loss: 1.1747 - val_accuracy: 0.4976 Epoch 276/300 839/839 [==============================] - 0s 237us/sample - loss: 0.2221 - accuracy: 0.9035 - val_loss: 1.1897 - val_accuracy: 0.4976 Epoch 277/300 839/839 [==============================] - 0s 229us/sample - loss: 0.2093 - accuracy: 0.9094 - val_loss: 1.1901 - val_accuracy: 0.5073 Epoch 278/300 839/839 [==============================] - 0s 227us/sample - loss: 0.1988 - accuracy: 0.9118 - val_loss: 1.2047 - val_accuracy: 0.4878 Epoch 279/300 839/839 [==============================] - 0s 255us/sample - loss: 0.1972 - accuracy: 0.9166 - val_loss: 1.2043 - val_accuracy: 0.4902 Epoch 280/300 839/839 [==============================] - 0s 231us/sample - loss: 0.1923 - accuracy: 0.9190 - val_loss: 1.1818 - val_accuracy: 0.5024 Epoch 281/300 839/839 [==============================] - 0s 230us/sample - loss: 0.1976 - accuracy: 0.9142 - val_loss: 1.1958 - val_accuracy: 0.4780 Epoch 282/300 839/839 [==============================] - 0s 231us/sample - loss: 0.1966 - accuracy: 0.9154 - val_loss: 1.2146 - val_accuracy: 0.5024 Epoch 283/300 839/839 [==============================] - 0s 238us/sample - loss: 0.1929 - accuracy: 0.9154 - val_loss: 1.2188 - val_accuracy: 0.4707 Epoch 284/300 839/839 [==============================] - 0s 254us/sample - loss: 0.1893 - accuracy: 0.9213 - val_loss: 1.2239 - val_accuracy: 0.4829 Epoch 285/300 839/839 [==============================] - 0s 228us/sample - loss: 0.1932 - accuracy: 0.9190 - val_loss: 1.1966 - val_accuracy: 0.4927 Epoch 286/300 839/839 [==============================] - 0s 230us/sample - loss: 0.1886 - accuracy: 0.9201 - val_loss: 1.2821 - val_accuracy: 0.4756 Epoch 287/300 839/839 [==============================] - 0s 226us/sample - loss: 0.1865 - accuracy: 0.9190 - val_loss: 1.2937 - val_accuracy: 0.4756 Epoch 288/300 839/839 [==============================] - 0s 225us/sample - loss: 0.1829 - accuracy: 0.9225 - val_loss: 1.2400 - val_accuracy: 0.5122 Epoch 289/300 839/839 [==============================] - 0s 241us/sample - loss: 0.1895 - accuracy: 0.9142 - val_loss: 1.2456 - val_accuracy: 0.4780 Epoch 290/300 839/839 [==============================] - 0s 226us/sample - loss: 0.1847 - accuracy: 0.9166 - val_loss: 1.2518 - val_accuracy: 0.4951 Epoch 291/300 839/839 [==============================] - 0s 234us/sample - loss: 0.1777 - accuracy: 0.9249 - val_loss: 1.2598 - val_accuracy: 0.4732 Epoch 292/300 839/839 [==============================] - 0s 256us/sample - loss: 0.1771 - accuracy: 0.9237 - val_loss: 1.2527 - val_accuracy: 0.4902 Epoch 293/300 839/839 [==============================] - 0s 237us/sample - loss: 0.1735 - accuracy: 0.9273 - val_loss: 1.2976 - val_accuracy: 0.5024 Epoch 294/300 839/839 [==============================] - 0s 240us/sample - loss: 0.1845 - accuracy: 0.9106 - val_loss: 1.2478 - val_accuracy: 0.4976 Epoch 295/300 839/839 [==============================] - 0s 229us/sample - loss: 0.1820 - accuracy: 0.9225 - val_loss: 1.2710 - val_accuracy: 0.5122 Epoch 296/300 839/839 [==============================] - 0s 231us/sample - loss: 0.1829 - accuracy: 0.9154 - val_loss: 1.3229 - val_accuracy: 0.5000 Epoch 297/300 839/839 [==============================] - 0s 235us/sample - loss: 0.1719 - accuracy: 0.9261 - val_loss: 1.2958 - val_accuracy: 0.4951 Epoch 298/300 839/839 [==============================] - 0s 222us/sample - loss: 0.1739 - accuracy: 0.9249 - val_loss: 1.2766 - val_accuracy: 0.5049 Epoch 299/300 839/839 [==============================] - 0s 238us/sample - loss: 0.1712 - accuracy: 0.9249 - val_loss: 1.2826 - val_accuracy: 0.5122 Epoch 300/300 839/839 [==============================] - 0s 231us/sample - loss: 0.1672 - accuracy: 0.9344 - val_loss: 1.2757 - val_accuracy: 0.5024 # plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () plt . show () # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'accuracy' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_accuracy' ) plt . legend () plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Stock Returns"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RecommenderSystem/TF2_0_Recommender_System/","text":"================ by Jawad Haider Recommendation System Recommendation System \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-rc0 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-rc0 # More imports from tensorflow.keras.layers import Input , Dense , Embedding , Flatten , \\ Concatenate from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam from sklearn.utils import shuffle import numpy as np import pandas as pd import matplotlib.pyplot as plt # data is from: https://grouplens.org/datasets/movielens/ # in case the link changes in the future ! wget - nc http : // files . grouplens . org / datasets / movielens / ml - 20 m . zip --2019-09-16 21:12:57-- http://files.grouplens.org/datasets/movielens/ml-20m.zip Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152 Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 198702078 (189M) [application/zip] Saving to: \u2018ml-20m.zip\u2019 ml-20m.zip 100%[===================>] 189.50M 16.6MB/s in 11s 2019-09-16 21:13:09 (17.5 MB/s) - \u2018ml-20m.zip\u2019 saved [198702078/198702078] ! unzip - n ml - 20 m . zip Archive: ml-20m.zip creating: ml-20m/ inflating: ml-20m/genome-scores.csv inflating: ml-20m/genome-tags.csv inflating: ml-20m/links.csv inflating: ml-20m/movies.csv inflating: ml-20m/ratings.csv inflating: ml-20m/README.txt inflating: ml-20m/tags.csv ! ls ml-20m ml-20m.zip sample_data df = pd . read_csv ( 'ml-20m/ratings.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } userId movieId rating timestamp 0 1 2 3.5 1112486027 1 1 29 3.5 1112484676 2 1 32 3.5 1112484819 3 1 47 3.5 1112484727 4 1 50 3.5 1112484580 # We can't trust the userId and movieId to be numbered 0...N-1 # Let's just set our own ids # current_user_id = 0 # custom_user_map = {} # old user id > new user id # def map_user_id(row): # global current_user_id, custom_user_map # old_user_id = row['userId'] # if old_user_id not in custom_user_map: # custom_user_map[old_user_id] = current_user_id # current_user_id += 1 # return custom_user_map[old_user_id] # df['new_user_id'] = df.apply(map_user_id, axis=1) df . userId = pd . Categorical ( df . userId ) df [ 'new_user_id' ] = df . userId . cat . codes # Now do the same thing for movie ids # current_movie_id = 0 # custom_movie_map = {} # old movie id > new movie id # def map_movie_id(row): # global current_movie_id, custom_movie_map # old_movie_id = row['movieId'] # if old_movie_id not in custom_movie_map: # custom_movie_map[old_movie_id] = current_movie_id # current_movie_id += 1 # return custom_movie_map[old_movie_id] # df['new_movie_id'] = df.apply(map_movie_id, axis=1) df . movieId = pd . Categorical ( df . movieId ) df [ 'new_movie_id' ] = df . movieId . cat . codes # Get user IDs, movie IDs, and ratings as separate arrays user_ids = df [ 'new_user_id' ] . values movie_ids = df [ 'new_movie_id' ] . values ratings = df [ 'rating' ] . values # Get number of users and number of movies N = len ( set ( user_ids )) M = len ( set ( movie_ids )) # Set embedding dimension K = 10 # Make a neural network # User input u = Input ( shape = ( 1 ,)) # Movie input m = Input ( shape = ( 1 ,)) # User embedding u_emb = Embedding ( N , K )( u ) # output is (num_samples, 1, K) # Movie embedding m_emb = Embedding ( M , K )( m ) # output is (num_samples, 1, K) # Flatten both embeddings u_emb = Flatten ()( u_emb ) # now it's (num_samples, K) m_emb = Flatten ()( m_emb ) # now it's (num_samples, K) # Concatenate user-movie embeddings into a feature vector x = Concatenate ()([ u_emb , m_emb ]) # now it's (num_samples, 2K) # Now that we have a feature vector, it's just a regular ANN x = Dense ( 1024 , activation = 'relu' )( x ) # x = Dense(400, activation='relu')(x) # x = Dense(400, activation='relu')(x) x = Dense ( 1 )( x ) # Build the model and compile model = Model ( inputs = [ u , m ], outputs = x ) model . compile ( loss = 'mse' , optimizer = SGD ( lr = 0.08 , momentum = 0.9 ), ) # split the data user_ids , movie_ids , ratings = shuffle ( user_ids , movie_ids , ratings ) Ntrain = int ( 0.8 * len ( ratings )) train_user = user_ids [: Ntrain ] train_movie = movie_ids [: Ntrain ] train_ratings = ratings [: Ntrain ] test_user = user_ids [ Ntrain :] test_movie = movie_ids [ Ntrain :] test_ratings = ratings [ Ntrain :] # center the ratings avg_rating = train_ratings . mean () train_ratings = train_ratings - avg_rating test_ratings = test_ratings - avg_rating r = model . fit ( x = [ train_user , train_movie ], y = train_ratings , epochs = 25 , batch_size = 1024 , verbose = 2 , # goes a little faster when you don't print the progress bar validation_data = ([ test_user , test_movie ], test_ratings ), ) Train on 16000210 samples, validate on 4000053 samples Epoch 1/25 WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fbb23faec80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fbb23faec80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' 16000210/16000210 - 119s - loss: 0.7774 - val_loss: 0.7246 Epoch 2/25 16000210/16000210 - 122s - loss: 0.7018 - val_loss: 0.7025 Epoch 3/25 16000210/16000210 - 124s - loss: 0.6803 - val_loss: 0.6850 Epoch 4/25 16000210/16000210 - 122s - loss: 0.6644 - val_loss: 0.6757 Epoch 5/25 16000210/16000210 - 128s - loss: 0.6531 - val_loss: 0.6699 Epoch 6/25 16000210/16000210 - 125s - loss: 0.6409 - val_loss: 0.6604 Epoch 7/25 16000210/16000210 - 122s - loss: 0.6242 - val_loss: 0.6502 Epoch 8/25 16000210/16000210 - 116s - loss: 0.6111 - val_loss: 0.6473 Epoch 9/25 16000210/16000210 - 116s - loss: 0.6020 - val_loss: 0.6443 Epoch 10/25 16000210/16000210 - 119s - loss: 0.5944 - val_loss: 0.6398 Epoch 11/25 16000210/16000210 - 135s - loss: 0.5871 - val_loss: 0.6370 Epoch 12/25 16000210/16000210 - 125s - loss: 0.5804 - val_loss: 0.6338 Epoch 13/25 16000210/16000210 - 126s - loss: 0.5737 - val_loss: 0.6414 Epoch 14/25 16000210/16000210 - 124s - loss: 0.5668 - val_loss: 0.6294 Epoch 15/25 16000210/16000210 - 123s - loss: 0.5604 - val_loss: 0.6285 Epoch 16/25 16000210/16000210 - 123s - loss: 0.5548 - val_loss: 0.6258 Epoch 17/25 16000210/16000210 - 122s - loss: 0.5503 - val_loss: 0.6261 Epoch 18/25 16000210/16000210 - 122s - loss: 0.5464 - val_loss: 0.6337 Epoch 19/25 16000210/16000210 - 121s - loss: 0.5433 - val_loss: 0.6284 Epoch 20/25 16000210/16000210 - 121s - loss: 0.5406 - val_loss: 0.6274 Epoch 21/25 16000210/16000210 - 122s - loss: 0.5383 - val_loss: 0.6252 Epoch 22/25 16000210/16000210 - 121s - loss: 0.5362 - val_loss: 0.6270 Epoch 23/25 16000210/16000210 - 122s - loss: 0.5343 - val_loss: 0.6256 Epoch 24/25 16000210/16000210 - 124s - loss: 0.5327 - val_loss: 0.6259 Epoch 25/25 16000210/16000210 - 124s - loss: 0.5311 - val_loss: 0.6259 # plot losses plt . plot ( r . history [ 'loss' ], label = \"train loss\" ) plt . plot ( r . history [ 'val_loss' ], label = \"val loss\" ) plt . legend () plt . show () # is this on par with other approaches? # https://datascience.stackexchange.com/questions/29740/benchmark-result-for-movielens-dataset np . sqrt ( 0.6259 ) 0.7911384202527394 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Recommender System"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/RecommenderSystem/TF2_0_Recommender_System/#recommendation-system","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-rc0 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0-rc0 # More imports from tensorflow.keras.layers import Input , Dense , Embedding , Flatten , \\ Concatenate from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam from sklearn.utils import shuffle import numpy as np import pandas as pd import matplotlib.pyplot as plt # data is from: https://grouplens.org/datasets/movielens/ # in case the link changes in the future ! wget - nc http : // files . grouplens . org / datasets / movielens / ml - 20 m . zip --2019-09-16 21:12:57-- http://files.grouplens.org/datasets/movielens/ml-20m.zip Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152 Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 198702078 (189M) [application/zip] Saving to: \u2018ml-20m.zip\u2019 ml-20m.zip 100%[===================>] 189.50M 16.6MB/s in 11s 2019-09-16 21:13:09 (17.5 MB/s) - \u2018ml-20m.zip\u2019 saved [198702078/198702078] ! unzip - n ml - 20 m . zip Archive: ml-20m.zip creating: ml-20m/ inflating: ml-20m/genome-scores.csv inflating: ml-20m/genome-tags.csv inflating: ml-20m/links.csv inflating: ml-20m/movies.csv inflating: ml-20m/ratings.csv inflating: ml-20m/README.txt inflating: ml-20m/tags.csv ! ls ml-20m ml-20m.zip sample_data df = pd . read_csv ( 'ml-20m/ratings.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } userId movieId rating timestamp 0 1 2 3.5 1112486027 1 1 29 3.5 1112484676 2 1 32 3.5 1112484819 3 1 47 3.5 1112484727 4 1 50 3.5 1112484580 # We can't trust the userId and movieId to be numbered 0...N-1 # Let's just set our own ids # current_user_id = 0 # custom_user_map = {} # old user id > new user id # def map_user_id(row): # global current_user_id, custom_user_map # old_user_id = row['userId'] # if old_user_id not in custom_user_map: # custom_user_map[old_user_id] = current_user_id # current_user_id += 1 # return custom_user_map[old_user_id] # df['new_user_id'] = df.apply(map_user_id, axis=1) df . userId = pd . Categorical ( df . userId ) df [ 'new_user_id' ] = df . userId . cat . codes # Now do the same thing for movie ids # current_movie_id = 0 # custom_movie_map = {} # old movie id > new movie id # def map_movie_id(row): # global current_movie_id, custom_movie_map # old_movie_id = row['movieId'] # if old_movie_id not in custom_movie_map: # custom_movie_map[old_movie_id] = current_movie_id # current_movie_id += 1 # return custom_movie_map[old_movie_id] # df['new_movie_id'] = df.apply(map_movie_id, axis=1) df . movieId = pd . Categorical ( df . movieId ) df [ 'new_movie_id' ] = df . movieId . cat . codes # Get user IDs, movie IDs, and ratings as separate arrays user_ids = df [ 'new_user_id' ] . values movie_ids = df [ 'new_movie_id' ] . values ratings = df [ 'rating' ] . values # Get number of users and number of movies N = len ( set ( user_ids )) M = len ( set ( movie_ids )) # Set embedding dimension K = 10 # Make a neural network # User input u = Input ( shape = ( 1 ,)) # Movie input m = Input ( shape = ( 1 ,)) # User embedding u_emb = Embedding ( N , K )( u ) # output is (num_samples, 1, K) # Movie embedding m_emb = Embedding ( M , K )( m ) # output is (num_samples, 1, K) # Flatten both embeddings u_emb = Flatten ()( u_emb ) # now it's (num_samples, K) m_emb = Flatten ()( m_emb ) # now it's (num_samples, K) # Concatenate user-movie embeddings into a feature vector x = Concatenate ()([ u_emb , m_emb ]) # now it's (num_samples, 2K) # Now that we have a feature vector, it's just a regular ANN x = Dense ( 1024 , activation = 'relu' )( x ) # x = Dense(400, activation='relu')(x) # x = Dense(400, activation='relu')(x) x = Dense ( 1 )( x ) # Build the model and compile model = Model ( inputs = [ u , m ], outputs = x ) model . compile ( loss = 'mse' , optimizer = SGD ( lr = 0.08 , momentum = 0.9 ), ) # split the data user_ids , movie_ids , ratings = shuffle ( user_ids , movie_ids , ratings ) Ntrain = int ( 0.8 * len ( ratings )) train_user = user_ids [: Ntrain ] train_movie = movie_ids [: Ntrain ] train_ratings = ratings [: Ntrain ] test_user = user_ids [ Ntrain :] test_movie = movie_ids [ Ntrain :] test_ratings = ratings [ Ntrain :] # center the ratings avg_rating = train_ratings . mean () train_ratings = train_ratings - avg_rating test_ratings = test_ratings - avg_rating r = model . fit ( x = [ train_user , train_movie ], y = train_ratings , epochs = 25 , batch_size = 1024 , verbose = 2 , # goes a little faster when you don't print the progress bar validation_data = ([ test_user , test_movie ], test_ratings ), ) Train on 16000210 samples, validate on 4000053 samples Epoch 1/25 WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fbb23faec80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fbb23faec80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' 16000210/16000210 - 119s - loss: 0.7774 - val_loss: 0.7246 Epoch 2/25 16000210/16000210 - 122s - loss: 0.7018 - val_loss: 0.7025 Epoch 3/25 16000210/16000210 - 124s - loss: 0.6803 - val_loss: 0.6850 Epoch 4/25 16000210/16000210 - 122s - loss: 0.6644 - val_loss: 0.6757 Epoch 5/25 16000210/16000210 - 128s - loss: 0.6531 - val_loss: 0.6699 Epoch 6/25 16000210/16000210 - 125s - loss: 0.6409 - val_loss: 0.6604 Epoch 7/25 16000210/16000210 - 122s - loss: 0.6242 - val_loss: 0.6502 Epoch 8/25 16000210/16000210 - 116s - loss: 0.6111 - val_loss: 0.6473 Epoch 9/25 16000210/16000210 - 116s - loss: 0.6020 - val_loss: 0.6443 Epoch 10/25 16000210/16000210 - 119s - loss: 0.5944 - val_loss: 0.6398 Epoch 11/25 16000210/16000210 - 135s - loss: 0.5871 - val_loss: 0.6370 Epoch 12/25 16000210/16000210 - 125s - loss: 0.5804 - val_loss: 0.6338 Epoch 13/25 16000210/16000210 - 126s - loss: 0.5737 - val_loss: 0.6414 Epoch 14/25 16000210/16000210 - 124s - loss: 0.5668 - val_loss: 0.6294 Epoch 15/25 16000210/16000210 - 123s - loss: 0.5604 - val_loss: 0.6285 Epoch 16/25 16000210/16000210 - 123s - loss: 0.5548 - val_loss: 0.6258 Epoch 17/25 16000210/16000210 - 122s - loss: 0.5503 - val_loss: 0.6261 Epoch 18/25 16000210/16000210 - 122s - loss: 0.5464 - val_loss: 0.6337 Epoch 19/25 16000210/16000210 - 121s - loss: 0.5433 - val_loss: 0.6284 Epoch 20/25 16000210/16000210 - 121s - loss: 0.5406 - val_loss: 0.6274 Epoch 21/25 16000210/16000210 - 122s - loss: 0.5383 - val_loss: 0.6252 Epoch 22/25 16000210/16000210 - 121s - loss: 0.5362 - val_loss: 0.6270 Epoch 23/25 16000210/16000210 - 122s - loss: 0.5343 - val_loss: 0.6256 Epoch 24/25 16000210/16000210 - 124s - loss: 0.5327 - val_loss: 0.6259 Epoch 25/25 16000210/16000210 - 124s - loss: 0.5311 - val_loss: 0.6259 # plot losses plt . plot ( r . history [ 'loss' ], label = \"train loss\" ) plt . plot ( r . history [ 'val_loss' ], label = \"val loss\" ) plt . legend () plt . show () # is this on par with other approaches? # https://datascience.stackexchange.com/questions/29740/benchmark-result-for-movielens-dataset np . sqrt ( 0.6259 ) 0.7911384202527394 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Recommendation System"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning/","text":"================ by Jawad Haider Transfer Learning Transfer Learning \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-rc0 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow is already loaded. Please restart the runtime to change versions. 2.0.0-rc0 # More imports from tensorflow.keras.layers import Input , Dense , Flatten from tensorflow.keras.applications.vgg16 import VGG16 as PretrainedModel , \\ preprocess_input from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam from tensorflow.keras.preprocessing import image from tensorflow.keras.preprocessing.image import ImageDataGenerator from glob import glob import numpy as np import pandas as pd import matplotlib.pyplot as plt import sys , os # Data from: https://mmspg.epfl.ch/downloads/food-image-datasets/ # !wget --passive-ftp --prefer-family=ipv4 --ftp-user FoodImage@grebvm2.epfl.ch \\ # --ftp-password Cahc1moo -nc ftp://tremplin.epfl.ch/Food-5K.zip ! wget - nc https : // lazyprogrammer . me / course_files / Food - 5 K . zip File \u2018Food-5K.zip\u2019 already there; not retrieving. ! unzip - qq - o Food - 5 K . zip ! ls sample_data ! mv Food - 5 K /* . ! ls training 0_0.jpg 0_1387.jpg 0_422.jpg 0_809.jpg 1_1195.jpg 1_230.jpg 1_617.jpg 0_1000.jpg 0_1388.jpg 0_423.jpg 0_80.jpg 1_1196.jpg 1_231.jpg 1_618.jpg 0_1001.jpg 0_1389.jpg 0_424.jpg 0_810.jpg 1_1197.jpg 1_232.jpg 1_619.jpg 0_1002.jpg 0_138.jpg 0_425.jpg 0_811.jpg 1_1198.jpg 1_233.jpg 1_61.jpg 0_1003.jpg 0_1390.jpg 0_426.jpg 0_812.jpg 1_1199.jpg 1_234.jpg 1_620.jpg 0_1004.jpg 0_1391.jpg 0_427.jpg 0_813.jpg 1_119.jpg 1_235.jpg 1_621.jpg 0_1005.jpg 0_1392.jpg 0_428.jpg 0_814.jpg 1_11.jpg 1_236.jpg 1_622.jpg 0_1006.jpg 0_1393.jpg 0_429.jpg 0_815.jpg 1_1200.jpg 1_237.jpg 1_623.jpg 0_1007.jpg 0_1394.jpg 0_42.jpg 0_816.jpg 1_1201.jpg 1_238.jpg 1_624.jpg 0_1008.jpg 0_1395.jpg 0_430.jpg 0_817.jpg 1_1202.jpg 1_239.jpg 1_625.jpg 0_1009.jpg 0_1396.jpg 0_431.jpg 0_818.jpg 1_1203.jpg 1_23.jpg 1_626.jpg 0_100.jpg 0_1397.jpg 0_432.jpg 0_819.jpg 1_1204.jpg 1_240.jpg 1_627.jpg 0_1010.jpg 0_1398.jpg 0_433.jpg 0_81.jpg 1_1205.jpg 1_241.jpg 1_628.jpg 0_1011.jpg 0_1399.jpg 0_434.jpg 0_820.jpg 1_1206.jpg 1_242.jpg 1_629.jpg 0_1012.jpg 0_139.jpg 0_435.jpg 0_821.jpg 1_1207.jpg 1_243.jpg 1_62.jpg 0_1013.jpg 0_13.jpg 0_436.jpg 0_822.jpg 1_1208.jpg 1_244.jpg 1_630.jpg 0_1014.jpg 0_1400.jpg 0_437.jpg 0_823.jpg 1_1209.jpg 1_245.jpg 1_631.jpg 0_1015.jpg 0_1401.jpg 0_438.jpg 0_824.jpg 1_120.jpg 1_246.jpg 1_632.jpg 0_1016.jpg 0_1402.jpg 0_439.jpg 0_825.jpg 1_1210.jpg 1_247.jpg 1_633.jpg 0_1017.jpg 0_1403.jpg 0_43.jpg 0_826.jpg 1_1211.jpg 1_248.jpg 1_634.jpg 0_1018.jpg 0_1404.jpg 0_440.jpg 0_827.jpg 1_1212.jpg 1_249.jpg 1_635.jpg 0_1019.jpg 0_1405.jpg 0_441.jpg 0_828.jpg 1_1213.jpg 1_24.jpg 1_636.jpg 0_101.jpg 0_1406.jpg 0_442.jpg 0_829.jpg 1_1214.jpg 1_250.jpg 1_637.jpg 0_1020.jpg 0_1407.jpg 0_443.jpg 0_82.jpg 1_1215.jpg 1_251.jpg 1_638.jpg 0_1021.jpg 0_1408.jpg 0_444.jpg 0_830.jpg 1_1216.jpg 1_252.jpg 1_639.jpg 0_1022.jpg 0_1409.jpg 0_445.jpg 0_831.jpg 1_1217.jpg 1_253.jpg 1_63.jpg 0_1023.jpg 0_140.jpg 0_446.jpg 0_832.jpg 1_1218.jpg 1_254.jpg 1_640.jpg 0_1024.jpg 0_1410.jpg 0_447.jpg 0_833.jpg 1_1219.jpg 1_255.jpg 1_641.jpg 0_1025.jpg 0_1411.jpg 0_448.jpg 0_834.jpg 1_121.jpg 1_256.jpg 1_642.jpg 0_1026.jpg 0_1412.jpg 0_449.jpg 0_835.jpg 1_1220.jpg 1_257.jpg 1_643.jpg 0_1027.jpg 0_1413.jpg 0_44.jpg 0_836.jpg 1_1221.jpg 1_258.jpg 1_644.jpg 0_1028.jpg 0_1414.jpg 0_450.jpg 0_837.jpg 1_1222.jpg 1_259.jpg 1_645.jpg 0_1029.jpg 0_1415.jpg 0_451.jpg 0_838.jpg 1_1223.jpg 1_25.jpg 1_646.jpg 0_102.jpg 0_1416.jpg 0_452.jpg 0_839.jpg 1_1224.jpg 1_260.jpg 1_647.jpg 0_1030.jpg 0_1417.jpg 0_453.jpg 0_83.jpg 1_1225.jpg 1_261.jpg 1_648.jpg 0_1031.jpg 0_1418.jpg 0_454.jpg 0_840.jpg 1_1226.jpg 1_262.jpg 1_649.jpg 0_1032.jpg 0_1419.jpg 0_455.jpg 0_841.jpg 1_1227.jpg 1_263.jpg 1_64.jpg 0_1033.jpg 0_141.jpg 0_456.jpg 0_842.jpg 1_1228.jpg 1_264.jpg 1_650.jpg 0_1034.jpg 0_1420.jpg 0_457.jpg 0_843.jpg 1_1229.jpg 1_265.jpg 1_651.jpg 0_1035.jpg 0_1421.jpg 0_458.jpg 0_844.jpg 1_122.jpg 1_266.jpg 1_652.jpg 0_1036.jpg 0_1422.jpg 0_459.jpg 0_845.jpg 1_1230.jpg 1_267.jpg 1_653.jpg 0_1037.jpg 0_1423.jpg 0_45.jpg 0_846.jpg 1_1231.jpg 1_268.jpg 1_654.jpg 0_1038.jpg 0_1424.jpg 0_460.jpg 0_847.jpg 1_1232.jpg 1_269.jpg 1_655.jpg 0_1039.jpg 0_1425.jpg 0_461.jpg 0_848.jpg 1_1233.jpg 1_26.jpg 1_656.jpg 0_103.jpg 0_1426.jpg 0_462.jpg 0_849.jpg 1_1234.jpg 1_270.jpg 1_657.jpg 0_1040.jpg 0_1427.jpg 0_463.jpg 0_84.jpg 1_1235.jpg 1_271.jpg 1_658.jpg 0_1041.jpg 0_1428.jpg 0_464.jpg 0_850.jpg 1_1236.jpg 1_272.jpg 1_659.jpg 0_1042.jpg 0_1429.jpg 0_465.jpg 0_851.jpg 1_1237.jpg 1_273.jpg 1_65.jpg 0_1043.jpg 0_142.jpg 0_466.jpg 0_852.jpg 1_1238.jpg 1_274.jpg 1_660.jpg 0_1044.jpg 0_1430.jpg 0_467.jpg 0_853.jpg 1_1239.jpg 1_275.jpg 1_661.jpg 0_1045.jpg 0_1431.jpg 0_468.jpg 0_854.jpg 1_123.jpg 1_276.jpg 1_662.jpg 0_1046.jpg 0_1432.jpg 0_469.jpg 0_855.jpg 1_1240.jpg 1_277.jpg 1_663.jpg 0_1047.jpg 0_1433.jpg 0_46.jpg 0_856.jpg 1_1241.jpg 1_278.jpg 1_664.jpg 0_1048.jpg 0_1434.jpg 0_470.jpg 0_857.jpg 1_1242.jpg 1_279.jpg 1_665.jpg 0_1049.jpg 0_1435.jpg 0_471.jpg 0_858.jpg 1_1243.jpg 1_27.jpg 1_666.jpg 0_104.jpg 0_1436.jpg 0_472.jpg 0_859.jpg 1_1244.jpg 1_280.jpg 1_667.jpg 0_1050.jpg 0_1437.jpg 0_473.jpg 0_85.jpg 1_1245.jpg 1_281.jpg 1_668.jpg 0_1051.jpg 0_1438.jpg 0_474.jpg 0_860.jpg 1_1246.jpg 1_282.jpg 1_669.jpg 0_1052.jpg 0_1439.jpg 0_475.jpg 0_861.jpg 1_1247.jpg 1_283.jpg 1_66.jpg 0_1053.jpg 0_143.jpg 0_476.jpg 0_862.jpg 1_1248.jpg 1_284.jpg 1_670.jpg 0_1054.jpg 0_1440.jpg 0_477.jpg 0_863.jpg 1_1249.jpg 1_285.jpg 1_671.jpg 0_1055.jpg 0_1441.jpg 0_478.jpg 0_864.jpg 1_124.jpg 1_286.jpg 1_672.jpg 0_1056.jpg 0_1442.jpg 0_479.jpg 0_865.jpg 1_1250.jpg 1_287.jpg 1_673.jpg 0_1057.jpg 0_1443.jpg 0_47.jpg 0_866.jpg 1_1251.jpg 1_288.jpg 1_674.jpg 0_1058.jpg 0_1444.jpg 0_480.jpg 0_867.jpg 1_1252.jpg 1_289.jpg 1_675.jpg 0_1059.jpg 0_1445.jpg 0_481.jpg 0_868.jpg 1_1253.jpg 1_28.jpg 1_676.jpg 0_105.jpg 0_1446.jpg 0_482.jpg 0_869.jpg 1_1254.jpg 1_290.jpg 1_677.jpg 0_1060.jpg 0_1447.jpg 0_483.jpg 0_86.jpg 1_1255.jpg 1_291.jpg 1_678.jpg 0_1061.jpg 0_1448.jpg 0_484.jpg 0_870.jpg 1_1256.jpg 1_292.jpg 1_679.jpg 0_1062.jpg 0_1449.jpg 0_485.jpg 0_871.jpg 1_1257.jpg 1_293.jpg 1_67.jpg 0_1063.jpg 0_144.jpg 0_486.jpg 0_872.jpg 1_1258.jpg 1_294.jpg 1_680.jpg 0_1064.jpg 0_1450.jpg 0_487.jpg 0_873.jpg 1_1259.jpg 1_295.jpg 1_681.jpg 0_1065.jpg 0_1451.jpg 0_488.jpg 0_874.jpg 1_125.jpg 1_296.jpg 1_682.jpg 0_1066.jpg 0_1452.jpg 0_489.jpg 0_875.jpg 1_1260.jpg 1_297.jpg 1_683.jpg 0_1067.jpg 0_1453.jpg 0_48.jpg 0_876.jpg 1_1261.jpg 1_298.jpg 1_684.jpg 0_1068.jpg 0_1454.jpg 0_490.jpg 0_877.jpg 1_1262.jpg 1_299.jpg 1_685.jpg 0_1069.jpg 0_1455.jpg 0_491.jpg 0_878.jpg 1_1263.jpg 1_29.jpg 1_686.jpg 0_106.jpg 0_1456.jpg 0_492.jpg 0_879.jpg 1_1264.jpg 1_2.jpg 1_687.jpg 0_1070.jpg 0_1457.jpg 0_493.jpg 0_87.jpg 1_1265.jpg 1_300.jpg 1_688.jpg 0_1071.jpg 0_1458.jpg 0_494.jpg 0_880.jpg 1_1266.jpg 1_301.jpg 1_689.jpg 0_1072.jpg 0_1459.jpg 0_495.jpg 0_881.jpg 1_1267.jpg 1_302.jpg 1_68.jpg 0_1073.jpg 0_145.jpg 0_496.jpg 0_882.jpg 1_1268.jpg 1_303.jpg 1_690.jpg 0_1074.jpg 0_1460.jpg 0_497.jpg 0_883.jpg 1_1269.jpg 1_304.jpg 1_691.jpg 0_1075.jpg 0_1461.jpg 0_498.jpg 0_884.jpg 1_126.jpg 1_305.jpg 1_692.jpg 0_1076.jpg 0_1462.jpg 0_499.jpg 0_885.jpg 1_1270.jpg 1_306.jpg 1_693.jpg 0_1077.jpg 0_1463.jpg 0_49.jpg 0_886.jpg 1_1271.jpg 1_307.jpg 1_694.jpg 0_1078.jpg 0_1464.jpg 0_4.jpg 0_887.jpg 1_1272.jpg 1_308.jpg 1_695.jpg 0_1079.jpg 0_1465.jpg 0_500.jpg 0_888.jpg 1_1273.jpg 1_309.jpg 1_696.jpg 0_107.jpg 0_1466.jpg 0_501.jpg 0_889.jpg 1_1274.jpg 1_30.jpg 1_697.jpg 0_1080.jpg 0_1467.jpg 0_502.jpg 0_88.jpg 1_1275.jpg 1_310.jpg 1_698.jpg 0_1081.jpg 0_1468.jpg 0_503.jpg 0_890.jpg 1_1276.jpg 1_311.jpg 1_699.jpg 0_1082.jpg 0_1469.jpg 0_504.jpg 0_891.jpg 1_1277.jpg 1_312.jpg 1_69.jpg 0_1083.jpg 0_146.jpg 0_505.jpg 0_892.jpg 1_1278.jpg 1_313.jpg 1_6.jpg 0_1084.jpg 0_1470.jpg 0_506.jpg 0_893.jpg 1_1279.jpg 1_314.jpg 1_700.jpg 0_1085.jpg 0_1471.jpg 0_507.jpg 0_894.jpg 1_127.jpg 1_315.jpg 1_701.jpg 0_1086.jpg 0_1472.jpg 0_508.jpg 0_895.jpg 1_1280.jpg 1_316.jpg 1_702.jpg 0_1087.jpg 0_1473.jpg 0_509.jpg 0_896.jpg 1_1281.jpg 1_317.jpg 1_703.jpg 0_1088.jpg 0_1474.jpg 0_50.jpg 0_897.jpg 1_1282.jpg 1_318.jpg 1_704.jpg 0_1089.jpg 0_1475.jpg 0_510.jpg 0_898.jpg 1_1283.jpg 1_319.jpg 1_705.jpg 0_108.jpg 0_1476.jpg 0_511.jpg 0_899.jpg 1_1284.jpg 1_31.jpg 1_706.jpg 0_1090.jpg 0_1477.jpg 0_512.jpg 0_89.jpg 1_1285.jpg 1_320.jpg 1_707.jpg 0_1091.jpg 0_1478.jpg 0_513.jpg 0_8.jpg 1_1286.jpg 1_321.jpg 1_708.jpg 0_1092.jpg 0_1479.jpg 0_514.jpg 0_900.jpg 1_1287.jpg 1_322.jpg 1_709.jpg 0_1093.jpg 0_147.jpg 0_515.jpg 0_901.jpg 1_1288.jpg 1_323.jpg 1_70.jpg 0_1094.jpg 0_1480.jpg 0_516.jpg 0_902.jpg 1_1289.jpg 1_324.jpg 1_710.jpg 0_1095.jpg 0_1481.jpg 0_517.jpg 0_903.jpg 1_128.jpg 1_325.jpg 1_711.jpg 0_1096.jpg 0_1482.jpg 0_518.jpg 0_904.jpg 1_1290.jpg 1_326.jpg 1_712.jpg 0_1097.jpg 0_1483.jpg 0_519.jpg 0_905.jpg 1_1291.jpg 1_327.jpg 1_713.jpg 0_1098.jpg 0_1484.jpg 0_51.jpg 0_906.jpg 1_1292.jpg 1_328.jpg 1_714.jpg 0_1099.jpg 0_1485.jpg 0_520.jpg 0_907.jpg 1_1293.jpg 1_329.jpg 1_715.jpg 0_109.jpg 0_1486.jpg 0_521.jpg 0_908.jpg 1_1294.jpg 1_32.jpg 1_716.jpg 0_10.jpg 0_1487.jpg 0_522.jpg 0_909.jpg 1_1295.jpg 1_330.jpg 1_717.jpg 0_1100.jpg 0_1488.jpg 0_523.jpg 0_90.jpg 1_1296.jpg 1_331.jpg 1_718.jpg 0_1101.jpg 0_1489.jpg 0_524.jpg 0_910.jpg 1_1297.jpg 1_332.jpg 1_719.jpg 0_1102.jpg 0_148.jpg 0_525.jpg 0_911.jpg 1_1298.jpg 1_333.jpg 1_71.jpg 0_1103.jpg 0_1490.jpg 0_526.jpg 0_912.jpg 1_1299.jpg 1_334.jpg 1_720.jpg 0_1104.jpg 0_1491.jpg 0_527.jpg 0_913.jpg 1_129.jpg 1_335.jpg 1_721.jpg 0_1105.jpg 0_1492.jpg 0_528.jpg 0_914.jpg 1_12.jpg 1_336.jpg 1_722.jpg 0_1106.jpg 0_1493.jpg 0_529.jpg 0_915.jpg 1_1300.jpg 1_337.jpg 1_723.jpg 0_1107.jpg 0_1494.jpg 0_52.jpg 0_916.jpg 1_1301.jpg 1_338.jpg 1_724.jpg 0_1108.jpg 0_1495.jpg 0_530.jpg 0_917.jpg 1_1302.jpg 1_339.jpg 1_725.jpg 0_1109.jpg 0_1496.jpg 0_531.jpg 0_918.jpg 1_1303.jpg 1_33.jpg 1_726.jpg 0_110.jpg 0_1497.jpg 0_532.jpg 0_919.jpg 1_1304.jpg 1_340.jpg 1_727.jpg 0_1110.jpg 0_1498.jpg 0_533.jpg 0_91.jpg 1_1305.jpg 1_341.jpg 1_728.jpg 0_1111.jpg 0_1499.jpg 0_534.jpg 0_920.jpg 1_1306.jpg 1_342.jpg 1_729.jpg 0_1112.jpg 0_149.jpg 0_535.jpg 0_921.jpg 1_1307.jpg 1_343.jpg 1_72.jpg 0_1113.jpg 0_14.jpg 0_536.jpg 0_922.jpg 1_1308.jpg 1_344.jpg 1_730.jpg 0_1114.jpg 0_150.jpg 0_537.jpg 0_923.jpg 1_1309.jpg 1_345.jpg 1_731.jpg 0_1115.jpg 0_151.jpg 0_538.jpg 0_924.jpg 1_130.jpg 1_346.jpg 1_732.jpg 0_1116.jpg 0_152.jpg 0_539.jpg 0_925.jpg 1_1310.jpg 1_347.jpg 1_733.jpg 0_1117.jpg 0_153.jpg 0_53.jpg 0_926.jpg 1_1311.jpg 1_348.jpg 1_734.jpg 0_1118.jpg 0_154.jpg 0_540.jpg 0_927.jpg 1_1312.jpg 1_349.jpg 1_735.jpg 0_1119.jpg 0_155.jpg 0_541.jpg 0_928.jpg 1_1313.jpg 1_34.jpg 1_736.jpg 0_111.jpg 0_156.jpg 0_542.jpg 0_929.jpg 1_1314.jpg 1_350.jpg 1_737.jpg 0_1120.jpg 0_157.jpg 0_543.jpg 0_92.jpg 1_1315.jpg 1_351.jpg 1_738.jpg 0_1121.jpg 0_158.jpg 0_544.jpg 0_930.jpg 1_1316.jpg 1_352.jpg 1_739.jpg 0_1122.jpg 0_159.jpg 0_545.jpg 0_931.jpg 1_1317.jpg 1_353.jpg 1_73.jpg 0_1123.jpg 0_15.jpg 0_546.jpg 0_932.jpg 1_1318.jpg 1_354.jpg 1_740.jpg 0_1124.jpg 0_160.jpg 0_547.jpg 0_933.jpg 1_1319.jpg 1_355.jpg 1_741.jpg 0_1125.jpg 0_161.jpg 0_548.jpg 0_934.jpg 1_131.jpg 1_356.jpg 1_742.jpg 0_1126.jpg 0_162.jpg 0_549.jpg 0_935.jpg 1_1320.jpg 1_357.jpg 1_743.jpg 0_1127.jpg 0_163.jpg 0_54.jpg 0_936.jpg 1_1321.jpg 1_358.jpg 1_744.jpg 0_1128.jpg 0_164.jpg 0_550.jpg 0_937.jpg 1_1322.jpg 1_359.jpg 1_745.jpg 0_1129.jpg 0_165.jpg 0_551.jpg 0_938.jpg 1_1323.jpg 1_35.jpg 1_746.jpg 0_112.jpg 0_166.jpg 0_552.jpg 0_939.jpg 1_1324.jpg 1_360.jpg 1_747.jpg 0_1130.jpg 0_167.jpg 0_553.jpg 0_93.jpg 1_1325.jpg 1_361.jpg 1_748.jpg 0_1131.jpg 0_168.jpg 0_554.jpg 0_940.jpg 1_1326.jpg 1_362.jpg 1_749.jpg 0_1132.jpg 0_169.jpg 0_555.jpg 0_941.jpg 1_1327.jpg 1_363.jpg 1_74.jpg 0_1133.jpg 0_16.jpg 0_556.jpg 0_942.jpg 1_1328.jpg 1_364.jpg 1_750.jpg 0_1134.jpg 0_170.jpg 0_557.jpg 0_943.jpg 1_1329.jpg 1_365.jpg 1_751.jpg 0_1135.jpg 0_171.jpg 0_558.jpg 0_944.jpg 1_132.jpg 1_366.jpg 1_752.jpg 0_1136.jpg 0_172.jpg 0_559.jpg 0_945.jpg 1_1330.jpg 1_367.jpg 1_753.jpg 0_1137.jpg 0_173.jpg 0_55.jpg 0_946.jpg 1_1331.jpg 1_368.jpg 1_754.jpg 0_1138.jpg 0_174.jpg 0_560.jpg 0_947.jpg 1_1332.jpg 1_369.jpg 1_755.jpg 0_1139.jpg 0_175.jpg 0_561.jpg 0_948.jpg 1_1333.jpg 1_36.jpg 1_756.jpg 0_113.jpg 0_176.jpg 0_562.jpg 0_949.jpg 1_1334.jpg 1_370.jpg 1_757.jpg 0_1140.jpg 0_177.jpg 0_563.jpg 0_94.jpg 1_1335.jpg 1_371.jpg 1_758.jpg 0_1141.jpg 0_178.jpg 0_564.jpg 0_950.jpg 1_1336.jpg 1_372.jpg 1_759.jpg 0_1142.jpg 0_179.jpg 0_565.jpg 0_951.jpg 1_1337.jpg 1_373.jpg 1_75.jpg 0_1143.jpg 0_17.jpg 0_566.jpg 0_952.jpg 1_1338.jpg 1_374.jpg 1_760.jpg 0_1144.jpg 0_180.jpg 0_567.jpg 0_953.jpg 1_1339.jpg 1_375.jpg 1_761.jpg 0_1145.jpg 0_181.jpg 0_568.jpg 0_954.jpg 1_133.jpg 1_376.jpg 1_762.jpg 0_1146.jpg 0_182.jpg 0_569.jpg 0_955.jpg 1_1340.jpg 1_377.jpg 1_763.jpg 0_1147.jpg 0_183.jpg 0_56.jpg 0_956.jpg 1_1341.jpg 1_378.jpg 1_764.jpg 0_1148.jpg 0_184.jpg 0_570.jpg 0_957.jpg 1_1342.jpg 1_379.jpg 1_765.jpg 0_1149.jpg 0_185.jpg 0_571.jpg 0_958.jpg 1_1343.jpg 1_37.jpg 1_766.jpg 0_114.jpg 0_186.jpg 0_572.jpg 0_959.jpg 1_1344.jpg 1_380.jpg 1_767.jpg 0_1150.jpg 0_187.jpg 0_573.jpg 0_95.jpg 1_1345.jpg 1_381.jpg 1_768.jpg 0_1151.jpg 0_188.jpg 0_574.jpg 0_960.jpg 1_1346.jpg 1_382.jpg 1_769.jpg 0_1152.jpg 0_189.jpg 0_575.jpg 0_961.jpg 1_1347.jpg 1_383.jpg 1_76.jpg 0_1153.jpg 0_18.jpg 0_576.jpg 0_962.jpg 1_1348.jpg 1_384.jpg 1_770.jpg 0_1154.jpg 0_190.jpg 0_577.jpg 0_963.jpg 1_1349.jpg 1_385.jpg 1_771.jpg 0_1155.jpg 0_191.jpg 0_578.jpg 0_964.jpg 1_134.jpg 1_386.jpg 1_772.jpg 0_1156.jpg 0_192.jpg 0_579.jpg 0_965.jpg 1_1350.jpg 1_387.jpg 1_773.jpg 0_1157.jpg 0_193.jpg 0_57.jpg 0_966.jpg 1_1351.jpg 1_388.jpg 1_774.jpg 0_1158.jpg 0_194.jpg 0_580.jpg 0_967.jpg 1_1352.jpg 1_389.jpg 1_775.jpg 0_1159.jpg 0_195.jpg 0_581.jpg 0_968.jpg 1_1353.jpg 1_38.jpg 1_776.jpg 0_115.jpg 0_196.jpg 0_582.jpg 0_969.jpg 1_1354.jpg 1_390.jpg 1_777.jpg 0_1160.jpg 0_197.jpg 0_583.jpg 0_96.jpg 1_1355.jpg 1_391.jpg 1_778.jpg 0_1161.jpg 0_198.jpg 0_584.jpg 0_970.jpg 1_1356.jpg 1_392.jpg 1_779.jpg 0_1162.jpg 0_199.jpg 0_585.jpg 0_971.jpg 1_1357.jpg 1_393.jpg 1_77.jpg 0_1163.jpg 0_19.jpg 0_586.jpg 0_972.jpg 1_1358.jpg 1_394.jpg 1_780.jpg 0_1164.jpg 0_1.jpg 0_587.jpg 0_973.jpg 1_1359.jpg 1_395.jpg 1_781.jpg 0_1165.jpg 0_200.jpg 0_588.jpg 0_974.jpg 1_135.jpg 1_396.jpg 1_782.jpg 0_1166.jpg 0_201.jpg 0_589.jpg 0_975.jpg 1_1360.jpg 1_397.jpg 1_783.jpg 0_1167.jpg 0_202.jpg 0_58.jpg 0_976.jpg 1_1361.jpg 1_398.jpg 1_784.jpg 0_1168.jpg 0_203.jpg 0_590.jpg 0_977.jpg 1_1362.jpg 1_399.jpg 1_785.jpg 0_1169.jpg 0_204.jpg 0_591.jpg 0_978.jpg 1_1363.jpg 1_39.jpg 1_786.jpg 0_116.jpg 0_205.jpg 0_592.jpg 0_979.jpg 1_1364.jpg 1_3.jpg 1_787.jpg 0_1170.jpg 0_206.jpg 0_593.jpg 0_97.jpg 1_1365.jpg 1_400.jpg 1_788.jpg 0_1171.jpg 0_207.jpg 0_594.jpg 0_980.jpg 1_1366.jpg 1_401.jpg 1_789.jpg 0_1172.jpg 0_208.jpg 0_595.jpg 0_981.jpg 1_1367.jpg 1_402.jpg 1_78.jpg 0_1173.jpg 0_209.jpg 0_596.jpg 0_982.jpg 1_1368.jpg 1_403.jpg 1_790.jpg 0_1174.jpg 0_20.jpg 0_597.jpg 0_983.jpg 1_1369.jpg 1_404.jpg 1_791.jpg 0_1175.jpg 0_210.jpg 0_598.jpg 0_984.jpg 1_136.jpg 1_405.jpg 1_792.jpg 0_1176.jpg 0_211.jpg 0_599.jpg 0_985.jpg 1_1370.jpg 1_406.jpg 1_793.jpg 0_1177.jpg 0_212.jpg 0_59.jpg 0_986.jpg 1_1371.jpg 1_407.jpg 1_794.jpg 0_1178.jpg 0_213.jpg 0_5.jpg 0_987.jpg 1_1372.jpg 1_408.jpg 1_795.jpg 0_1179.jpg 0_214.jpg 0_600.jpg 0_988.jpg 1_1373.jpg 1_409.jpg 1_796.jpg 0_117.jpg 0_215.jpg 0_601.jpg 0_989.jpg 1_1374.jpg 1_40.jpg 1_797.jpg 0_1180.jpg 0_216.jpg 0_602.jpg 0_98.jpg 1_1375.jpg 1_410.jpg 1_798.jpg 0_1181.jpg 0_217.jpg 0_603.jpg 0_990.jpg 1_1376.jpg 1_411.jpg 1_799.jpg 0_1182.jpg 0_218.jpg 0_604.jpg 0_991.jpg 1_1377.jpg 1_412.jpg 1_79.jpg 0_1183.jpg 0_219.jpg 0_605.jpg 0_992.jpg 1_1378.jpg 1_413.jpg 1_7.jpg 0_1184.jpg 0_21.jpg 0_606.jpg 0_993.jpg 1_1379.jpg 1_414.jpg 1_800.jpg 0_1185.jpg 0_220.jpg 0_607.jpg 0_994.jpg 1_137.jpg 1_415.jpg 1_801.jpg 0_1186.jpg 0_221.jpg 0_608.jpg 0_995.jpg 1_1380.jpg 1_416.jpg 1_802.jpg 0_1187.jpg 0_222.jpg 0_609.jpg 0_996.jpg 1_1381.jpg 1_417.jpg 1_803.jpg 0_1188.jpg 0_223.jpg 0_60.jpg 0_997.jpg 1_1382.jpg 1_418.jpg 1_804.jpg 0_1189.jpg 0_224.jpg 0_610.jpg 0_998.jpg 1_1383.jpg 1_419.jpg 1_805.jpg 0_118.jpg 0_225.jpg 0_611.jpg 0_999.jpg 1_1384.jpg 1_41.jpg 1_806.jpg 0_1190.jpg 0_226.jpg 0_612.jpg 0_99.jpg 1_1385.jpg 1_420.jpg 1_807.jpg 0_1191.jpg 0_227.jpg 0_613.jpg 0_9.jpg 1_1386.jpg 1_421.jpg 1_808.jpg 0_1192.jpg 0_228.jpg 0_614.jpg 1_0.jpg 1_1387.jpg 1_422.jpg 1_809.jpg 0_1193.jpg 0_229.jpg 0_615.jpg 1_1000.jpg 1_1388.jpg 1_423.jpg 1_80.jpg 0_1194.jpg 0_22.jpg 0_616.jpg 1_1001.jpg 1_1389.jpg 1_424.jpg 1_810.jpg 0_1195.jpg 0_230.jpg 0_617.jpg 1_1002.jpg 1_138.jpg 1_425.jpg 1_811.jpg 0_1196.jpg 0_231.jpg 0_618.jpg 1_1003.jpg 1_1390.jpg 1_426.jpg 1_812.jpg 0_1197.jpg 0_232.jpg 0_619.jpg 1_1004.jpg 1_1391.jpg 1_427.jpg 1_813.jpg 0_1198.jpg 0_233.jpg 0_61.jpg 1_1005.jpg 1_1392.jpg 1_428.jpg 1_814.jpg 0_1199.jpg 0_234.jpg 0_620.jpg 1_1006.jpg 1_1393.jpg 1_429.jpg 1_815.jpg 0_119.jpg 0_235.jpg 0_621.jpg 1_1007.jpg 1_1394.jpg 1_42.jpg 1_816.jpg 0_11.jpg 0_236.jpg 0_622.jpg 1_1008.jpg 1_1395.jpg 1_430.jpg 1_817.jpg 0_1200.jpg 0_237.jpg 0_623.jpg 1_1009.jpg 1_1396.jpg 1_431.jpg 1_818.jpg 0_1201.jpg 0_238.jpg 0_624.jpg 1_100.jpg 1_1397.jpg 1_432.jpg 1_819.jpg 0_1202.jpg 0_239.jpg 0_625.jpg 1_1010.jpg 1_1398.jpg 1_433.jpg 1_81.jpg 0_1203.jpg 0_23.jpg 0_626.jpg 1_1011.jpg 1_1399.jpg 1_434.jpg 1_820.jpg 0_1204.jpg 0_240.jpg 0_627.jpg 1_1012.jpg 1_139.jpg 1_435.jpg 1_821.jpg 0_1205.jpg 0_241.jpg 0_628.jpg 1_1013.jpg 1_13.jpg 1_436.jpg 1_822.jpg 0_1206.jpg 0_242.jpg 0_629.jpg 1_1014.jpg 1_1400.jpg 1_437.jpg 1_823.jpg 0_1207.jpg 0_243.jpg 0_62.jpg 1_1015.jpg 1_1401.jpg 1_438.jpg 1_824.jpg 0_1208.jpg 0_244.jpg 0_630.jpg 1_1016.jpg 1_1402.jpg 1_439.jpg 1_825.jpg 0_1209.jpg 0_245.jpg 0_631.jpg 1_1017.jpg 1_1403.jpg 1_43.jpg 1_826.jpg 0_120.jpg 0_246.jpg 0_632.jpg 1_1018.jpg 1_1404.jpg 1_440.jpg 1_827.jpg 0_1210.jpg 0_247.jpg 0_633.jpg 1_1019.jpg 1_1405.jpg 1_441.jpg 1_828.jpg 0_1211.jpg 0_248.jpg 0_634.jpg 1_101.jpg 1_1406.jpg 1_442.jpg 1_829.jpg 0_1212.jpg 0_249.jpg 0_635.jpg 1_1020.jpg 1_1407.jpg 1_443.jpg 1_82.jpg 0_1213.jpg 0_24.jpg 0_636.jpg 1_1021.jpg 1_1408.jpg 1_444.jpg 1_830.jpg 0_1214.jpg 0_250.jpg 0_637.jpg 1_1022.jpg 1_1409.jpg 1_445.jpg 1_831.jpg 0_1215.jpg 0_251.jpg 0_638.jpg 1_1023.jpg 1_140.jpg 1_446.jpg 1_832.jpg 0_1216.jpg 0_252.jpg 0_639.jpg 1_1024.jpg 1_1410.jpg 1_447.jpg 1_833.jpg 0_1217.jpg 0_253.jpg 0_63.jpg 1_1025.jpg 1_1411.jpg 1_448.jpg 1_834.jpg 0_1218.jpg 0_254.jpg 0_640.jpg 1_1026.jpg 1_1412.jpg 1_449.jpg 1_835.jpg 0_1219.jpg 0_255.jpg 0_641.jpg 1_1027.jpg 1_1413.jpg 1_44.jpg 1_836.jpg 0_121.jpg 0_256.jpg 0_642.jpg 1_1028.jpg 1_1414.jpg 1_450.jpg 1_837.jpg 0_1220.jpg 0_257.jpg 0_643.jpg 1_1029.jpg 1_1415.jpg 1_451.jpg 1_838.jpg 0_1221.jpg 0_258.jpg 0_644.jpg 1_102.jpg 1_1416.jpg 1_452.jpg 1_839.jpg 0_1222.jpg 0_259.jpg 0_645.jpg 1_1030.jpg 1_1417.jpg 1_453.jpg 1_83.jpg 0_1223.jpg 0_25.jpg 0_646.jpg 1_1031.jpg 1_1418.jpg 1_454.jpg 1_840.jpg 0_1224.jpg 0_260.jpg 0_647.jpg 1_1032.jpg 1_1419.jpg 1_455.jpg 1_841.jpg 0_1225.jpg 0_261.jpg 0_648.jpg 1_1033.jpg 1_141.jpg 1_456.jpg 1_842.jpg 0_1226.jpg 0_262.jpg 0_649.jpg 1_1034.jpg 1_1420.jpg 1_457.jpg 1_843.jpg 0_1227.jpg 0_263.jpg 0_64.jpg 1_1035.jpg 1_1421.jpg 1_458.jpg 1_844.jpg 0_1228.jpg 0_264.jpg 0_650.jpg 1_1036.jpg 1_1422.jpg 1_459.jpg 1_845.jpg 0_1229.jpg 0_265.jpg 0_651.jpg 1_1037.jpg 1_1423.jpg 1_45.jpg 1_846.jpg 0_122.jpg 0_266.jpg 0_652.jpg 1_1038.jpg 1_1424.jpg 1_460.jpg 1_847.jpg 0_1230.jpg 0_267.jpg 0_653.jpg 1_1039.jpg 1_1425.jpg 1_461.jpg 1_848.jpg 0_1231.jpg 0_268.jpg 0_654.jpg 1_103.jpg 1_1426.jpg 1_462.jpg 1_849.jpg 0_1232.jpg 0_269.jpg 0_655.jpg 1_1040.jpg 1_1427.jpg 1_463.jpg 1_84.jpg 0_1233.jpg 0_26.jpg 0_656.jpg 1_1041.jpg 1_1428.jpg 1_464.jpg 1_850.jpg 0_1234.jpg 0_270.jpg 0_657.jpg 1_1042.jpg 1_1429.jpg 1_465.jpg 1_851.jpg 0_1235.jpg 0_271.jpg 0_658.jpg 1_1043.jpg 1_142.jpg 1_466.jpg 1_852.jpg 0_1236.jpg 0_272.jpg 0_659.jpg 1_1044.jpg 1_1430.jpg 1_467.jpg 1_853.jpg 0_1237.jpg 0_273.jpg 0_65.jpg 1_1045.jpg 1_1431.jpg 1_468.jpg 1_854.jpg 0_1238.jpg 0_274.jpg 0_660.jpg 1_1046.jpg 1_1432.jpg 1_469.jpg 1_855.jpg 0_1239.jpg 0_275.jpg 0_661.jpg 1_1047.jpg 1_1433.jpg 1_46.jpg 1_856.jpg 0_123.jpg 0_276.jpg 0_662.jpg 1_1048.jpg 1_1434.jpg 1_470.jpg 1_857.jpg 0_1240.jpg 0_277.jpg 0_663.jpg 1_1049.jpg 1_1435.jpg 1_471.jpg 1_858.jpg 0_1241.jpg 0_278.jpg 0_664.jpg 1_104.jpg 1_1436.jpg 1_472.jpg 1_859.jpg 0_1242.jpg 0_279.jpg 0_665.jpg 1_1050.jpg 1_1437.jpg 1_473.jpg 1_85.jpg 0_1243.jpg 0_27.jpg 0_666.jpg 1_1051.jpg 1_1438.jpg 1_474.jpg 1_860.jpg 0_1244.jpg 0_280.jpg 0_667.jpg 1_1052.jpg 1_1439.jpg 1_475.jpg 1_861.jpg 0_1245.jpg 0_281.jpg 0_668.jpg 1_1053.jpg 1_143.jpg 1_476.jpg 1_862.jpg 0_1246.jpg 0_282.jpg 0_669.jpg 1_1054.jpg 1_1440.jpg 1_477.jpg 1_863.jpg 0_1247.jpg 0_283.jpg 0_66.jpg 1_1055.jpg 1_1441.jpg 1_478.jpg 1_864.jpg 0_1248.jpg 0_284.jpg 0_670.jpg 1_1056.jpg 1_1442.jpg 1_479.jpg 1_865.jpg 0_1249.jpg 0_285.jpg 0_671.jpg 1_1057.jpg 1_1443.jpg 1_47.jpg 1_866.jpg 0_124.jpg 0_286.jpg 0_672.jpg 1_1058.jpg 1_1444.jpg 1_480.jpg 1_867.jpg 0_1250.jpg 0_287.jpg 0_673.jpg 1_1059.jpg 1_1445.jpg 1_481.jpg 1_868.jpg 0_1251.jpg 0_288.jpg 0_674.jpg 1_105.jpg 1_1446.jpg 1_482.jpg 1_869.jpg 0_1252.jpg 0_289.jpg 0_675.jpg 1_1060.jpg 1_1447.jpg 1_483.jpg 1_86.jpg 0_1253.jpg 0_28.jpg 0_676.jpg 1_1061.jpg 1_1448.jpg 1_484.jpg 1_870.jpg 0_1254.jpg 0_290.jpg 0_677.jpg 1_1062.jpg 1_1449.jpg 1_485.jpg 1_871.jpg 0_1255.jpg 0_291.jpg 0_678.jpg 1_1063.jpg 1_144.jpg 1_486.jpg 1_872.jpg 0_1256.jpg 0_292.jpg 0_679.jpg 1_1064.jpg 1_1450.jpg 1_487.jpg 1_873.jpg 0_1257.jpg 0_293.jpg 0_67.jpg 1_1065.jpg 1_1451.jpg 1_488.jpg 1_874.jpg 0_1258.jpg 0_294.jpg 0_680.jpg 1_1066.jpg 1_1452.jpg 1_489.jpg 1_875.jpg 0_1259.jpg 0_295.jpg 0_681.jpg 1_1067.jpg 1_1453.jpg 1_48.jpg 1_876.jpg 0_125.jpg 0_296.jpg 0_682.jpg 1_1068.jpg 1_1454.jpg 1_490.jpg 1_877.jpg 0_1260.jpg 0_297.jpg 0_683.jpg 1_1069.jpg 1_1455.jpg 1_491.jpg 1_878.jpg 0_1261.jpg 0_298.jpg 0_684.jpg 1_106.jpg 1_1456.jpg 1_492.jpg 1_879.jpg 0_1262.jpg 0_299.jpg 0_685.jpg 1_1070.jpg 1_1457.jpg 1_493.jpg 1_87.jpg 0_1263.jpg 0_29.jpg 0_686.jpg 1_1071.jpg 1_1458.jpg 1_494.jpg 1_880.jpg 0_1264.jpg 0_2.jpg 0_687.jpg 1_1072.jpg 1_1459.jpg 1_495.jpg 1_881.jpg 0_1265.jpg 0_300.jpg 0_688.jpg 1_1073.jpg 1_145.jpg 1_496.jpg 1_882.jpg 0_1266.jpg 0_301.jpg 0_689.jpg 1_1074.jpg 1_1460.jpg 1_497.jpg 1_883.jpg 0_1267.jpg 0_302.jpg 0_68.jpg 1_1075.jpg 1_1461.jpg 1_498.jpg 1_884.jpg 0_1268.jpg 0_303.jpg 0_690.jpg 1_1076.jpg 1_1462.jpg 1_499.jpg 1_885.jpg 0_1269.jpg 0_304.jpg 0_691.jpg 1_1077.jpg 1_1463.jpg 1_49.jpg 1_886.jpg 0_126.jpg 0_305.jpg 0_692.jpg 1_1078.jpg 1_1464.jpg 1_4.jpg 1_887.jpg 0_1270.jpg 0_306.jpg 0_693.jpg 1_1079.jpg 1_1465.jpg 1_500.jpg 1_888.jpg 0_1271.jpg 0_307.jpg 0_694.jpg 1_107.jpg 1_1466.jpg 1_501.jpg 1_889.jpg 0_1272.jpg 0_308.jpg 0_695.jpg 1_1080.jpg 1_1467.jpg 1_502.jpg 1_88.jpg 0_1273.jpg 0_309.jpg 0_696.jpg 1_1081.jpg 1_1468.jpg 1_503.jpg 1_890.jpg 0_1274.jpg 0_30.jpg 0_697.jpg 1_1082.jpg 1_1469.jpg 1_504.jpg 1_891.jpg 0_1275.jpg 0_310.jpg 0_698.jpg 1_1083.jpg 1_146.jpg 1_505.jpg 1_892.jpg 0_1276.jpg 0_311.jpg 0_699.jpg 1_1084.jpg 1_1470.jpg 1_506.jpg 1_893.jpg 0_1277.jpg 0_312.jpg 0_69.jpg 1_1085.jpg 1_1471.jpg 1_507.jpg 1_894.jpg 0_1278.jpg 0_313.jpg 0_6.jpg 1_1086.jpg 1_1472.jpg 1_508.jpg 1_895.jpg 0_1279.jpg 0_314.jpg 0_700.jpg 1_1087.jpg 1_1473.jpg 1_509.jpg 1_896.jpg 0_127.jpg 0_315.jpg 0_701.jpg 1_1088.jpg 1_1474.jpg 1_50.jpg 1_897.jpg 0_1280.jpg 0_316.jpg 0_702.jpg 1_1089.jpg 1_1475.jpg 1_510.jpg 1_898.jpg 0_1281.jpg 0_317.jpg 0_703.jpg 1_108.jpg 1_1476.jpg 1_511.jpg 1_899.jpg 0_1282.jpg 0_318.jpg 0_704.jpg 1_1090.jpg 1_1477.jpg 1_512.jpg 1_89.jpg 0_1283.jpg 0_319.jpg 0_705.jpg 1_1091.jpg 1_1478.jpg 1_513.jpg 1_8.jpg 0_1284.jpg 0_31.jpg 0_706.jpg 1_1092.jpg 1_1479.jpg 1_514.jpg 1_900.jpg 0_1285.jpg 0_320.jpg 0_707.jpg 1_1093.jpg 1_147.jpg 1_515.jpg 1_901.jpg 0_1286.jpg 0_321.jpg 0_708.jpg 1_1094.jpg 1_1480.jpg 1_516.jpg 1_902.jpg 0_1287.jpg 0_322.jpg 0_709.jpg 1_1095.jpg 1_1481.jpg 1_517.jpg 1_903.jpg 0_1288.jpg 0_323.jpg 0_70.jpg 1_1096.jpg 1_1482.jpg 1_518.jpg 1_904.jpg 0_1289.jpg 0_324.jpg 0_710.jpg 1_1097.jpg 1_1483.jpg 1_519.jpg 1_905.jpg 0_128.jpg 0_325.jpg 0_711.jpg 1_1098.jpg 1_1484.jpg 1_51.jpg 1_906.jpg 0_1290.jpg 0_326.jpg 0_712.jpg 1_1099.jpg 1_1485.jpg 1_520.jpg 1_907.jpg 0_1291.jpg 0_327.jpg 0_713.jpg 1_109.jpg 1_1486.jpg 1_521.jpg 1_908.jpg 0_1292.jpg 0_328.jpg 0_714.jpg 1_10.jpg 1_1487.jpg 1_522.jpg 1_909.jpg 0_1293.jpg 0_329.jpg 0_715.jpg 1_1100.jpg 1_1488.jpg 1_523.jpg 1_90.jpg 0_1294.jpg 0_32.jpg 0_716.jpg 1_1101.jpg 1_1489.jpg 1_524.jpg 1_910.jpg 0_1295.jpg 0_330.jpg 0_717.jpg 1_1102.jpg 1_148.jpg 1_525.jpg 1_911.jpg 0_1296.jpg 0_331.jpg 0_718.jpg 1_1103.jpg 1_1490.jpg 1_526.jpg 1_912.jpg 0_1297.jpg 0_332.jpg 0_719.jpg 1_1104.jpg 1_1491.jpg 1_527.jpg 1_913.jpg 0_1298.jpg 0_333.jpg 0_71.jpg 1_1105.jpg 1_1492.jpg 1_528.jpg 1_914.jpg 0_1299.jpg 0_334.jpg 0_720.jpg 1_1106.jpg 1_1493.jpg 1_529.jpg 1_915.jpg 0_129.jpg 0_335.jpg 0_721.jpg 1_1107.jpg 1_1494.jpg 1_52.jpg 1_916.jpg 0_12.jpg 0_336.jpg 0_722.jpg 1_1108.jpg 1_1495.jpg 1_530.jpg 1_917.jpg 0_1300.jpg 0_337.jpg 0_723.jpg 1_1109.jpg 1_1496.jpg 1_531.jpg 1_918.jpg 0_1301.jpg 0_338.jpg 0_724.jpg 1_110.jpg 1_1497.jpg 1_532.jpg 1_919.jpg 0_1302.jpg 0_339.jpg 0_725.jpg 1_1110.jpg 1_1498.jpg 1_533.jpg 1_91.jpg 0_1303.jpg 0_33.jpg 0_726.jpg 1_1111.jpg 1_1499.jpg 1_534.jpg 1_920.jpg 0_1304.jpg 0_340.jpg 0_727.jpg 1_1112.jpg 1_149.jpg 1_535.jpg 1_921.jpg 0_1305.jpg 0_341.jpg 0_728.jpg 1_1113.jpg 1_14.jpg 1_536.jpg 1_922.jpg 0_1306.jpg 0_342.jpg 0_729.jpg 1_1114.jpg 1_150.jpg 1_537.jpg 1_923.jpg 0_1307.jpg 0_343.jpg 0_72.jpg 1_1115.jpg 1_151.jpg 1_538.jpg 1_924.jpg 0_1308.jpg 0_344.jpg 0_730.jpg 1_1116.jpg 1_152.jpg 1_539.jpg 1_925.jpg 0_1309.jpg 0_345.jpg 0_731.jpg 1_1117.jpg 1_153.jpg 1_53.jpg 1_926.jpg 0_130.jpg 0_346.jpg 0_732.jpg 1_1118.jpg 1_154.jpg 1_540.jpg 1_927.jpg 0_1310.jpg 0_347.jpg 0_733.jpg 1_1119.jpg 1_155.jpg 1_541.jpg 1_928.jpg 0_1311.jpg 0_348.jpg 0_734.jpg 1_111.jpg 1_156.jpg 1_542.jpg 1_929.jpg 0_1312.jpg 0_349.jpg 0_735.jpg 1_1120.jpg 1_157.jpg 1_543.jpg 1_92.jpg 0_1313.jpg 0_34.jpg 0_736.jpg 1_1121.jpg 1_158.jpg 1_544.jpg 1_930.jpg 0_1314.jpg 0_350.jpg 0_737.jpg 1_1122.jpg 1_159.jpg 1_545.jpg 1_931.jpg 0_1315.jpg 0_351.jpg 0_738.jpg 1_1123.jpg 1_15.jpg 1_546.jpg 1_932.jpg 0_1316.jpg 0_352.jpg 0_739.jpg 1_1124.jpg 1_160.jpg 1_547.jpg 1_933.jpg 0_1317.jpg 0_353.jpg 0_73.jpg 1_1125.jpg 1_161.jpg 1_548.jpg 1_934.jpg 0_1318.jpg 0_354.jpg 0_740.jpg 1_1126.jpg 1_162.jpg 1_549.jpg 1_935.jpg 0_1319.jpg 0_355.jpg 0_741.jpg 1_1127.jpg 1_163.jpg 1_54.jpg 1_936.jpg 0_131.jpg 0_356.jpg 0_742.jpg 1_1128.jpg 1_164.jpg 1_550.jpg 1_937.jpg 0_1320.jpg 0_357.jpg 0_743.jpg 1_1129.jpg 1_165.jpg 1_551.jpg 1_938.jpg 0_1321.jpg 0_358.jpg 0_744.jpg 1_112.jpg 1_166.jpg 1_552.jpg 1_939.jpg 0_1322.jpg 0_359.jpg 0_745.jpg 1_1130.jpg 1_167.jpg 1_553.jpg 1_93.jpg 0_1323.jpg 0_35.jpg 0_746.jpg 1_1131.jpg 1_168.jpg 1_554.jpg 1_940.jpg 0_1324.jpg 0_360.jpg 0_747.jpg 1_1132.jpg 1_169.jpg 1_555.jpg 1_941.jpg 0_1325.jpg 0_361.jpg 0_748.jpg 1_1133.jpg 1_16.jpg 1_556.jpg 1_942.jpg 0_1326.jpg 0_362.jpg 0_749.jpg 1_1134.jpg 1_170.jpg 1_557.jpg 1_943.jpg 0_1327.jpg 0_363.jpg 0_74.jpg 1_1135.jpg 1_171.jpg 1_558.jpg 1_944.jpg 0_1328.jpg 0_364.jpg 0_750.jpg 1_1136.jpg 1_172.jpg 1_559.jpg 1_945.jpg 0_1329.jpg 0_365.jpg 0_751.jpg 1_1137.jpg 1_173.jpg 1_55.jpg 1_946.jpg 0_132.jpg 0_366.jpg 0_752.jpg 1_1138.jpg 1_174.jpg 1_560.jpg 1_947.jpg 0_1330.jpg 0_367.jpg 0_753.jpg 1_1139.jpg 1_175.jpg 1_561.jpg 1_948.jpg 0_1331.jpg 0_368.jpg 0_754.jpg 1_113.jpg 1_176.jpg 1_562.jpg 1_949.jpg 0_1332.jpg 0_369.jpg 0_755.jpg 1_1140.jpg 1_177.jpg 1_563.jpg 1_94.jpg 0_1333.jpg 0_36.jpg 0_756.jpg 1_1141.jpg 1_178.jpg 1_564.jpg 1_950.jpg 0_1334.jpg 0_370.jpg 0_757.jpg 1_1142.jpg 1_179.jpg 1_565.jpg 1_951.jpg 0_1335.jpg 0_371.jpg 0_758.jpg 1_1143.jpg 1_17.jpg 1_566.jpg 1_952.jpg 0_1336.jpg 0_372.jpg 0_759.jpg 1_1144.jpg 1_180.jpg 1_567.jpg 1_953.jpg 0_1337.jpg 0_373.jpg 0_75.jpg 1_1145.jpg 1_181.jpg 1_568.jpg 1_954.jpg 0_1338.jpg 0_374.jpg 0_760.jpg 1_1146.jpg 1_182.jpg 1_569.jpg 1_955.jpg 0_1339.jpg 0_375.jpg 0_761.jpg 1_1147.jpg 1_183.jpg 1_56.jpg 1_956.jpg 0_133.jpg 0_376.jpg 0_762.jpg 1_1148.jpg 1_184.jpg 1_570.jpg 1_957.jpg 0_1340.jpg 0_377.jpg 0_763.jpg 1_1149.jpg 1_185.jpg 1_571.jpg 1_958.jpg 0_1341.jpg 0_378.jpg 0_764.jpg 1_114.jpg 1_186.jpg 1_572.jpg 1_959.jpg 0_1342.jpg 0_379.jpg 0_765.jpg 1_1150.jpg 1_187.jpg 1_573.jpg 1_95.jpg 0_1343.jpg 0_37.jpg 0_766.jpg 1_1151.jpg 1_188.jpg 1_574.jpg 1_960.jpg 0_1344.jpg 0_380.jpg 0_767.jpg 1_1152.jpg 1_189.jpg 1_575.jpg 1_961.jpg 0_1345.jpg 0_381.jpg 0_768.jpg 1_1153.jpg 1_18.jpg 1_576.jpg 1_962.jpg 0_1346.jpg 0_382.jpg 0_769.jpg 1_1154.jpg 1_190.jpg 1_577.jpg 1_963.jpg 0_1347.jpg 0_383.jpg 0_76.jpg 1_1155.jpg 1_191.jpg 1_578.jpg 1_964.jpg 0_1348.jpg 0_384.jpg 0_770.jpg 1_1156.jpg 1_192.jpg 1_579.jpg 1_965.jpg 0_1349.jpg 0_385.jpg 0_771.jpg 1_1157.jpg 1_193.jpg 1_57.jpg 1_966.jpg 0_134.jpg 0_386.jpg 0_772.jpg 1_1158.jpg 1_194.jpg 1_580.jpg 1_967.jpg 0_1350.jpg 0_387.jpg 0_773.jpg 1_1159.jpg 1_195.jpg 1_581.jpg 1_968.jpg 0_1351.jpg 0_388.jpg 0_774.jpg 1_115.jpg 1_196.jpg 1_582.jpg 1_969.jpg 0_1352.jpg 0_389.jpg 0_775.jpg 1_1160.jpg 1_197.jpg 1_583.jpg 1_96.jpg 0_1353.jpg 0_38.jpg 0_776.jpg 1_1161.jpg 1_198.jpg 1_584.jpg 1_970.jpg 0_1354.jpg 0_390.jpg 0_777.jpg 1_1162.jpg 1_199.jpg 1_585.jpg 1_971.jpg 0_1355.jpg 0_391.jpg 0_778.jpg 1_1163.jpg 1_19.jpg 1_586.jpg 1_972.jpg 0_1356.jpg 0_392.jpg 0_779.jpg 1_1164.jpg 1_1.jpg 1_587.jpg 1_973.jpg 0_1357.jpg 0_393.jpg 0_77.jpg 1_1165.jpg 1_200.jpg 1_588.jpg 1_974.jpg 0_1358.jpg 0_394.jpg 0_780.jpg 1_1166.jpg 1_201.jpg 1_589.jpg 1_975.jpg 0_1359.jpg 0_395.jpg 0_781.jpg 1_1167.jpg 1_202.jpg 1_58.jpg 1_976.jpg 0_135.jpg 0_396.jpg 0_782.jpg 1_1168.jpg 1_203.jpg 1_590.jpg 1_977.jpg 0_1360.jpg 0_397.jpg 0_783.jpg 1_1169.jpg 1_204.jpg 1_591.jpg 1_978.jpg 0_1361.jpg 0_398.jpg 0_784.jpg 1_116.jpg 1_205.jpg 1_592.jpg 1_979.jpg 0_1362.jpg 0_399.jpg 0_785.jpg 1_1170.jpg 1_206.jpg 1_593.jpg 1_97.jpg 0_1363.jpg 0_39.jpg 0_786.jpg 1_1171.jpg 1_207.jpg 1_594.jpg 1_980.jpg 0_1364.jpg 0_3.jpg 0_787.jpg 1_1172.jpg 1_208.jpg 1_595.jpg 1_981.jpg 0_1365.jpg 0_400.jpg 0_788.jpg 1_1173.jpg 1_209.jpg 1_596.jpg 1_982.jpg 0_1366.jpg 0_401.jpg 0_789.jpg 1_1174.jpg 1_20.jpg 1_597.jpg 1_983.jpg 0_1367.jpg 0_402.jpg 0_78.jpg 1_1175.jpg 1_210.jpg 1_598.jpg 1_984.jpg 0_1368.jpg 0_403.jpg 0_790.jpg 1_1176.jpg 1_211.jpg 1_599.jpg 1_985.jpg 0_1369.jpg 0_404.jpg 0_791.jpg 1_1177.jpg 1_212.jpg 1_59.jpg 1_986.jpg 0_136.jpg 0_405.jpg 0_792.jpg 1_1178.jpg 1_213.jpg 1_5.jpg 1_987.jpg 0_1370.jpg 0_406.jpg 0_793.jpg 1_1179.jpg 1_214.jpg 1_600.jpg 1_988.jpg 0_1371.jpg 0_407.jpg 0_794.jpg 1_117.jpg 1_215.jpg 1_601.jpg 1_989.jpg 0_1372.jpg 0_408.jpg 0_795.jpg 1_1180.jpg 1_216.jpg 1_602.jpg 1_98.jpg 0_1373.jpg 0_409.jpg 0_796.jpg 1_1181.jpg 1_217.jpg 1_603.jpg 1_990.jpg 0_1374.jpg 0_40.jpg 0_797.jpg 1_1182.jpg 1_218.jpg 1_604.jpg 1_991.jpg 0_1375.jpg 0_410.jpg 0_798.jpg 1_1183.jpg 1_219.jpg 1_605.jpg 1_992.jpg 0_1376.jpg 0_411.jpg 0_799.jpg 1_1184.jpg 1_21.jpg 1_606.jpg 1_993.jpg 0_1377.jpg 0_412.jpg 0_79.jpg 1_1185.jpg 1_220.jpg 1_607.jpg 1_994.jpg 0_1378.jpg 0_413.jpg 0_7.jpg 1_1186.jpg 1_221.jpg 1_608.jpg 1_995.jpg 0_1379.jpg 0_414.jpg 0_800.jpg 1_1187.jpg 1_222.jpg 1_609.jpg 1_996.jpg 0_137.jpg 0_415.jpg 0_801.jpg 1_1188.jpg 1_223.jpg 1_60.jpg 1_997.jpg 0_1380.jpg 0_416.jpg 0_802.jpg 1_1189.jpg 1_224.jpg 1_610.jpg 1_998.jpg 0_1381.jpg 0_417.jpg 0_803.jpg 1_118.jpg 1_225.jpg 1_611.jpg 1_999.jpg 0_1382.jpg 0_418.jpg 0_804.jpg 1_1190.jpg 1_226.jpg 1_612.jpg 1_99.jpg 0_1383.jpg 0_419.jpg 0_805.jpg 1_1191.jpg 1_227.jpg 1_613.jpg 1_9.jpg 0_1384.jpg 0_41.jpg 0_806.jpg 1_1192.jpg 1_228.jpg 1_614.jpg 0_1385.jpg 0_420.jpg 0_807.jpg 1_1193.jpg 1_229.jpg 1_615.jpg 0_1386.jpg 0_421.jpg 0_808.jpg 1_1194.jpg 1_22.jpg 1_616.jpg # look at an image for fun plt . imshow ( image . load_img ( 'training/0_808.jpg' )) plt . show () # Food images start with 1, non-food images start with 0 plt . imshow ( image . load_img ( 'training/1_616.jpg' )) plt . show () ! mkdir data # Make directories to store the data Keras-style ! mkdir data / train ! mkdir data / test ! mkdir data / train / nonfood ! mkdir data / train / food ! mkdir data / test / nonfood ! mkdir data / test / food # Move the images # Note: we will consider 'training' to be the train set # 'validation' folder will be the test set # ignore the 'evaluation' set ! mv training / 0 *. jpg data / train / nonfood ! mv training / 1 *. jpg data / train / food ! mv validation / 0 *. jpg data / test / nonfood ! mv validation / 1 *. jpg data / test / food train_path = 'data/train' valid_path = 'data/test' # These images are pretty big and of different sizes # Let's load them all in as the same (smaller) size IMAGE_SIZE = [ 200 , 200 ] # useful for getting number of files image_files = glob ( train_path + '/*/*.jpg' ) valid_image_files = glob ( valid_path + '/*/*.jpg' ) # useful for getting number of classes folders = glob ( train_path + '/*' ) folders # look at an image for fun plt . imshow ( image . load_img ( np . random . choice ( image_files ))) plt . show () ptm = PretrainedModel ( input_shape = IMAGE_SIZE + [ 3 ], weights = 'imagenet' , include_top = False ) # map the data into feature vectors x = Flatten ()( ptm . output ) # create a model object model = Model ( inputs = ptm . input , outputs = x ) # view the structure of the model model . summary () Model: \"model_3\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 200, 200, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 200, 200, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 200, 200, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 100, 100, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 100, 100, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 100, 100, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 50, 50, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 50, 50, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 50, 50, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 50, 50, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 25, 25, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 25, 25, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 25, 25, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 25, 25, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 12, 12, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 6, 6, 512) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 18432) 0 ================================================================= Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________ # create an instance of ImageDataGenerator gen = ImageDataGenerator ( preprocessing_function = preprocess_input ) batch_size = 128 # create generators train_generator = gen . flow_from_directory ( train_path , target_size = IMAGE_SIZE , batch_size = batch_size , class_mode = 'binary' , ) valid_generator = gen . flow_from_directory ( valid_path , target_size = IMAGE_SIZE , batch_size = batch_size , class_mode = 'binary' , ) Found 3000 images belonging to 2 classes. Found 1000 images belonging to 2 classes. Ntrain = len ( image_files ) Nvalid = len ( valid_image_files ) # Figure out the output size feat = model . predict ( np . random . random ([ 1 ] + IMAGE_SIZE + [ 3 ])) D = feat . shape [ 1 ] X_train = np . zeros (( Ntrain , D )) Y_train = np . zeros ( Ntrain ) X_valid = np . zeros (( Nvalid , D )) Y_valid = np . zeros ( Nvalid ) # populate X_train and Y_train i = 0 for x , y in train_generator : # get features features = model . predict ( x ) # size of the batch (may not always be batch_size) sz = len ( y ) # assign to X_train and Ytrain X_train [ i : i + sz ] = features Y_train [ i : i + sz ] = y # increment i i += sz print ( i ) if i >= Ntrain : print ( 'breaking now' ) break print ( i ) 128 256 384 512 640 768 896 1024 1152 1280 1408 1536 1664 1792 1920 2048 2176 2304 2432 2560 2688 2816 2944 3000 breaking now 3000 # populate X_valid and Y_valid i = 0 for x , y in valid_generator : # get features features = model . predict ( x ) # size of the batch (may not always be batch_size) sz = len ( y ) # assign to X_train and Ytrain X_valid [ i : i + sz ] = features Y_valid [ i : i + sz ] = y # increment i i += sz if i >= Nvalid : print ( 'breaking now' ) break print ( i ) breaking now 1000 X_train . max (), X_train . min () (749.7380981445312, 0.0) from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X_train2 = scaler . fit_transform ( X_train ) X_valid2 = scaler . transform ( X_valid ) # Try the built-in logistic regression from sklearn.linear_model import LogisticRegression logr = LogisticRegression () logr . fit ( X_train2 , Y_train ) print ( logr . score ( X_train2 , Y_train )) print ( logr . score ( X_valid2 , Y_valid )) /usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. FutureWarning) 1.0 0.977 # Do logistic regression in Tensorflow i = Input ( shape = ( D ,)) x = Dense ( 1 , activation = 'sigmoid' )( i ) linearmodel = Model ( i , x ) linearmodel . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) # Can try both normalized and unnormalized data r = linearmodel . fit ( X_train , Y_train , batch_size = 128 , epochs = 10 , validation_data = ( X_valid , Y_valid ), ) Train on 3000 samples, validate on 1000 samples Epoch 1/10 WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f7c18dd5598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f7c18dd5598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' 3000/3000 [==============================] - 1s 312us/sample - loss: 0.9217 - accuracy: 0.9293 - val_loss: 0.3271 - val_accuracy: 0.9780 Epoch 2/10 3000/3000 [==============================] - 0s 166us/sample - loss: 0.1115 - accuracy: 0.9893 - val_loss: 0.3457 - val_accuracy: 0.9740 Epoch 3/10 3000/3000 [==============================] - 0s 157us/sample - loss: 0.0210 - accuracy: 0.9977 - val_loss: 0.3130 - val_accuracy: 0.9810 Epoch 4/10 3000/3000 [==============================] - 0s 159us/sample - loss: 8.7803e-04 - accuracy: 0.9993 - val_loss: 0.3244 - val_accuracy: 0.9810 Epoch 5/10 3000/3000 [==============================] - 0s 154us/sample - loss: 2.6010e-05 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.9810 Epoch 6/10 3000/3000 [==============================] - 0s 159us/sample - loss: 8.5458e-06 - accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 0.9800 Epoch 7/10 3000/3000 [==============================] - 0s 155us/sample - loss: 6.6527e-06 - accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 0.9800 Epoch 8/10 3000/3000 [==============================] - 0s 152us/sample - loss: 5.8428e-06 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.9800 Epoch 9/10 3000/3000 [==============================] - 0s 151us/sample - loss: 4.0881e-06 - accuracy: 1.0000 - val_loss: 0.3251 - val_accuracy: 0.9800 Epoch 10/10 3000/3000 [==============================] - 0s 153us/sample - loss: 3.6229e-06 - accuracy: 1.0000 - val_loss: 0.3251 - val_accuracy: 0.9800 # loss plt . plot ( r . history [ 'loss' ], label = 'train loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val loss' ) plt . legend () plt . show () # accuracies plt . plot ( r . history [ 'accuracy' ], label = 'train acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val acc' ) plt . legend () plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Transfer Learning"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning/#transfer-learning","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-rc0 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow is already loaded. Please restart the runtime to change versions. 2.0.0-rc0 # More imports from tensorflow.keras.layers import Input , Dense , Flatten from tensorflow.keras.applications.vgg16 import VGG16 as PretrainedModel , \\ preprocess_input from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam from tensorflow.keras.preprocessing import image from tensorflow.keras.preprocessing.image import ImageDataGenerator from glob import glob import numpy as np import pandas as pd import matplotlib.pyplot as plt import sys , os # Data from: https://mmspg.epfl.ch/downloads/food-image-datasets/ # !wget --passive-ftp --prefer-family=ipv4 --ftp-user FoodImage@grebvm2.epfl.ch \\ # --ftp-password Cahc1moo -nc ftp://tremplin.epfl.ch/Food-5K.zip ! wget - nc https : // lazyprogrammer . me / course_files / Food - 5 K . zip File \u2018Food-5K.zip\u2019 already there; not retrieving. ! unzip - qq - o Food - 5 K . zip ! ls sample_data ! mv Food - 5 K /* . ! ls training 0_0.jpg 0_1387.jpg 0_422.jpg 0_809.jpg 1_1195.jpg 1_230.jpg 1_617.jpg 0_1000.jpg 0_1388.jpg 0_423.jpg 0_80.jpg 1_1196.jpg 1_231.jpg 1_618.jpg 0_1001.jpg 0_1389.jpg 0_424.jpg 0_810.jpg 1_1197.jpg 1_232.jpg 1_619.jpg 0_1002.jpg 0_138.jpg 0_425.jpg 0_811.jpg 1_1198.jpg 1_233.jpg 1_61.jpg 0_1003.jpg 0_1390.jpg 0_426.jpg 0_812.jpg 1_1199.jpg 1_234.jpg 1_620.jpg 0_1004.jpg 0_1391.jpg 0_427.jpg 0_813.jpg 1_119.jpg 1_235.jpg 1_621.jpg 0_1005.jpg 0_1392.jpg 0_428.jpg 0_814.jpg 1_11.jpg 1_236.jpg 1_622.jpg 0_1006.jpg 0_1393.jpg 0_429.jpg 0_815.jpg 1_1200.jpg 1_237.jpg 1_623.jpg 0_1007.jpg 0_1394.jpg 0_42.jpg 0_816.jpg 1_1201.jpg 1_238.jpg 1_624.jpg 0_1008.jpg 0_1395.jpg 0_430.jpg 0_817.jpg 1_1202.jpg 1_239.jpg 1_625.jpg 0_1009.jpg 0_1396.jpg 0_431.jpg 0_818.jpg 1_1203.jpg 1_23.jpg 1_626.jpg 0_100.jpg 0_1397.jpg 0_432.jpg 0_819.jpg 1_1204.jpg 1_240.jpg 1_627.jpg 0_1010.jpg 0_1398.jpg 0_433.jpg 0_81.jpg 1_1205.jpg 1_241.jpg 1_628.jpg 0_1011.jpg 0_1399.jpg 0_434.jpg 0_820.jpg 1_1206.jpg 1_242.jpg 1_629.jpg 0_1012.jpg 0_139.jpg 0_435.jpg 0_821.jpg 1_1207.jpg 1_243.jpg 1_62.jpg 0_1013.jpg 0_13.jpg 0_436.jpg 0_822.jpg 1_1208.jpg 1_244.jpg 1_630.jpg 0_1014.jpg 0_1400.jpg 0_437.jpg 0_823.jpg 1_1209.jpg 1_245.jpg 1_631.jpg 0_1015.jpg 0_1401.jpg 0_438.jpg 0_824.jpg 1_120.jpg 1_246.jpg 1_632.jpg 0_1016.jpg 0_1402.jpg 0_439.jpg 0_825.jpg 1_1210.jpg 1_247.jpg 1_633.jpg 0_1017.jpg 0_1403.jpg 0_43.jpg 0_826.jpg 1_1211.jpg 1_248.jpg 1_634.jpg 0_1018.jpg 0_1404.jpg 0_440.jpg 0_827.jpg 1_1212.jpg 1_249.jpg 1_635.jpg 0_1019.jpg 0_1405.jpg 0_441.jpg 0_828.jpg 1_1213.jpg 1_24.jpg 1_636.jpg 0_101.jpg 0_1406.jpg 0_442.jpg 0_829.jpg 1_1214.jpg 1_250.jpg 1_637.jpg 0_1020.jpg 0_1407.jpg 0_443.jpg 0_82.jpg 1_1215.jpg 1_251.jpg 1_638.jpg 0_1021.jpg 0_1408.jpg 0_444.jpg 0_830.jpg 1_1216.jpg 1_252.jpg 1_639.jpg 0_1022.jpg 0_1409.jpg 0_445.jpg 0_831.jpg 1_1217.jpg 1_253.jpg 1_63.jpg 0_1023.jpg 0_140.jpg 0_446.jpg 0_832.jpg 1_1218.jpg 1_254.jpg 1_640.jpg 0_1024.jpg 0_1410.jpg 0_447.jpg 0_833.jpg 1_1219.jpg 1_255.jpg 1_641.jpg 0_1025.jpg 0_1411.jpg 0_448.jpg 0_834.jpg 1_121.jpg 1_256.jpg 1_642.jpg 0_1026.jpg 0_1412.jpg 0_449.jpg 0_835.jpg 1_1220.jpg 1_257.jpg 1_643.jpg 0_1027.jpg 0_1413.jpg 0_44.jpg 0_836.jpg 1_1221.jpg 1_258.jpg 1_644.jpg 0_1028.jpg 0_1414.jpg 0_450.jpg 0_837.jpg 1_1222.jpg 1_259.jpg 1_645.jpg 0_1029.jpg 0_1415.jpg 0_451.jpg 0_838.jpg 1_1223.jpg 1_25.jpg 1_646.jpg 0_102.jpg 0_1416.jpg 0_452.jpg 0_839.jpg 1_1224.jpg 1_260.jpg 1_647.jpg 0_1030.jpg 0_1417.jpg 0_453.jpg 0_83.jpg 1_1225.jpg 1_261.jpg 1_648.jpg 0_1031.jpg 0_1418.jpg 0_454.jpg 0_840.jpg 1_1226.jpg 1_262.jpg 1_649.jpg 0_1032.jpg 0_1419.jpg 0_455.jpg 0_841.jpg 1_1227.jpg 1_263.jpg 1_64.jpg 0_1033.jpg 0_141.jpg 0_456.jpg 0_842.jpg 1_1228.jpg 1_264.jpg 1_650.jpg 0_1034.jpg 0_1420.jpg 0_457.jpg 0_843.jpg 1_1229.jpg 1_265.jpg 1_651.jpg 0_1035.jpg 0_1421.jpg 0_458.jpg 0_844.jpg 1_122.jpg 1_266.jpg 1_652.jpg 0_1036.jpg 0_1422.jpg 0_459.jpg 0_845.jpg 1_1230.jpg 1_267.jpg 1_653.jpg 0_1037.jpg 0_1423.jpg 0_45.jpg 0_846.jpg 1_1231.jpg 1_268.jpg 1_654.jpg 0_1038.jpg 0_1424.jpg 0_460.jpg 0_847.jpg 1_1232.jpg 1_269.jpg 1_655.jpg 0_1039.jpg 0_1425.jpg 0_461.jpg 0_848.jpg 1_1233.jpg 1_26.jpg 1_656.jpg 0_103.jpg 0_1426.jpg 0_462.jpg 0_849.jpg 1_1234.jpg 1_270.jpg 1_657.jpg 0_1040.jpg 0_1427.jpg 0_463.jpg 0_84.jpg 1_1235.jpg 1_271.jpg 1_658.jpg 0_1041.jpg 0_1428.jpg 0_464.jpg 0_850.jpg 1_1236.jpg 1_272.jpg 1_659.jpg 0_1042.jpg 0_1429.jpg 0_465.jpg 0_851.jpg 1_1237.jpg 1_273.jpg 1_65.jpg 0_1043.jpg 0_142.jpg 0_466.jpg 0_852.jpg 1_1238.jpg 1_274.jpg 1_660.jpg 0_1044.jpg 0_1430.jpg 0_467.jpg 0_853.jpg 1_1239.jpg 1_275.jpg 1_661.jpg 0_1045.jpg 0_1431.jpg 0_468.jpg 0_854.jpg 1_123.jpg 1_276.jpg 1_662.jpg 0_1046.jpg 0_1432.jpg 0_469.jpg 0_855.jpg 1_1240.jpg 1_277.jpg 1_663.jpg 0_1047.jpg 0_1433.jpg 0_46.jpg 0_856.jpg 1_1241.jpg 1_278.jpg 1_664.jpg 0_1048.jpg 0_1434.jpg 0_470.jpg 0_857.jpg 1_1242.jpg 1_279.jpg 1_665.jpg 0_1049.jpg 0_1435.jpg 0_471.jpg 0_858.jpg 1_1243.jpg 1_27.jpg 1_666.jpg 0_104.jpg 0_1436.jpg 0_472.jpg 0_859.jpg 1_1244.jpg 1_280.jpg 1_667.jpg 0_1050.jpg 0_1437.jpg 0_473.jpg 0_85.jpg 1_1245.jpg 1_281.jpg 1_668.jpg 0_1051.jpg 0_1438.jpg 0_474.jpg 0_860.jpg 1_1246.jpg 1_282.jpg 1_669.jpg 0_1052.jpg 0_1439.jpg 0_475.jpg 0_861.jpg 1_1247.jpg 1_283.jpg 1_66.jpg 0_1053.jpg 0_143.jpg 0_476.jpg 0_862.jpg 1_1248.jpg 1_284.jpg 1_670.jpg 0_1054.jpg 0_1440.jpg 0_477.jpg 0_863.jpg 1_1249.jpg 1_285.jpg 1_671.jpg 0_1055.jpg 0_1441.jpg 0_478.jpg 0_864.jpg 1_124.jpg 1_286.jpg 1_672.jpg 0_1056.jpg 0_1442.jpg 0_479.jpg 0_865.jpg 1_1250.jpg 1_287.jpg 1_673.jpg 0_1057.jpg 0_1443.jpg 0_47.jpg 0_866.jpg 1_1251.jpg 1_288.jpg 1_674.jpg 0_1058.jpg 0_1444.jpg 0_480.jpg 0_867.jpg 1_1252.jpg 1_289.jpg 1_675.jpg 0_1059.jpg 0_1445.jpg 0_481.jpg 0_868.jpg 1_1253.jpg 1_28.jpg 1_676.jpg 0_105.jpg 0_1446.jpg 0_482.jpg 0_869.jpg 1_1254.jpg 1_290.jpg 1_677.jpg 0_1060.jpg 0_1447.jpg 0_483.jpg 0_86.jpg 1_1255.jpg 1_291.jpg 1_678.jpg 0_1061.jpg 0_1448.jpg 0_484.jpg 0_870.jpg 1_1256.jpg 1_292.jpg 1_679.jpg 0_1062.jpg 0_1449.jpg 0_485.jpg 0_871.jpg 1_1257.jpg 1_293.jpg 1_67.jpg 0_1063.jpg 0_144.jpg 0_486.jpg 0_872.jpg 1_1258.jpg 1_294.jpg 1_680.jpg 0_1064.jpg 0_1450.jpg 0_487.jpg 0_873.jpg 1_1259.jpg 1_295.jpg 1_681.jpg 0_1065.jpg 0_1451.jpg 0_488.jpg 0_874.jpg 1_125.jpg 1_296.jpg 1_682.jpg 0_1066.jpg 0_1452.jpg 0_489.jpg 0_875.jpg 1_1260.jpg 1_297.jpg 1_683.jpg 0_1067.jpg 0_1453.jpg 0_48.jpg 0_876.jpg 1_1261.jpg 1_298.jpg 1_684.jpg 0_1068.jpg 0_1454.jpg 0_490.jpg 0_877.jpg 1_1262.jpg 1_299.jpg 1_685.jpg 0_1069.jpg 0_1455.jpg 0_491.jpg 0_878.jpg 1_1263.jpg 1_29.jpg 1_686.jpg 0_106.jpg 0_1456.jpg 0_492.jpg 0_879.jpg 1_1264.jpg 1_2.jpg 1_687.jpg 0_1070.jpg 0_1457.jpg 0_493.jpg 0_87.jpg 1_1265.jpg 1_300.jpg 1_688.jpg 0_1071.jpg 0_1458.jpg 0_494.jpg 0_880.jpg 1_1266.jpg 1_301.jpg 1_689.jpg 0_1072.jpg 0_1459.jpg 0_495.jpg 0_881.jpg 1_1267.jpg 1_302.jpg 1_68.jpg 0_1073.jpg 0_145.jpg 0_496.jpg 0_882.jpg 1_1268.jpg 1_303.jpg 1_690.jpg 0_1074.jpg 0_1460.jpg 0_497.jpg 0_883.jpg 1_1269.jpg 1_304.jpg 1_691.jpg 0_1075.jpg 0_1461.jpg 0_498.jpg 0_884.jpg 1_126.jpg 1_305.jpg 1_692.jpg 0_1076.jpg 0_1462.jpg 0_499.jpg 0_885.jpg 1_1270.jpg 1_306.jpg 1_693.jpg 0_1077.jpg 0_1463.jpg 0_49.jpg 0_886.jpg 1_1271.jpg 1_307.jpg 1_694.jpg 0_1078.jpg 0_1464.jpg 0_4.jpg 0_887.jpg 1_1272.jpg 1_308.jpg 1_695.jpg 0_1079.jpg 0_1465.jpg 0_500.jpg 0_888.jpg 1_1273.jpg 1_309.jpg 1_696.jpg 0_107.jpg 0_1466.jpg 0_501.jpg 0_889.jpg 1_1274.jpg 1_30.jpg 1_697.jpg 0_1080.jpg 0_1467.jpg 0_502.jpg 0_88.jpg 1_1275.jpg 1_310.jpg 1_698.jpg 0_1081.jpg 0_1468.jpg 0_503.jpg 0_890.jpg 1_1276.jpg 1_311.jpg 1_699.jpg 0_1082.jpg 0_1469.jpg 0_504.jpg 0_891.jpg 1_1277.jpg 1_312.jpg 1_69.jpg 0_1083.jpg 0_146.jpg 0_505.jpg 0_892.jpg 1_1278.jpg 1_313.jpg 1_6.jpg 0_1084.jpg 0_1470.jpg 0_506.jpg 0_893.jpg 1_1279.jpg 1_314.jpg 1_700.jpg 0_1085.jpg 0_1471.jpg 0_507.jpg 0_894.jpg 1_127.jpg 1_315.jpg 1_701.jpg 0_1086.jpg 0_1472.jpg 0_508.jpg 0_895.jpg 1_1280.jpg 1_316.jpg 1_702.jpg 0_1087.jpg 0_1473.jpg 0_509.jpg 0_896.jpg 1_1281.jpg 1_317.jpg 1_703.jpg 0_1088.jpg 0_1474.jpg 0_50.jpg 0_897.jpg 1_1282.jpg 1_318.jpg 1_704.jpg 0_1089.jpg 0_1475.jpg 0_510.jpg 0_898.jpg 1_1283.jpg 1_319.jpg 1_705.jpg 0_108.jpg 0_1476.jpg 0_511.jpg 0_899.jpg 1_1284.jpg 1_31.jpg 1_706.jpg 0_1090.jpg 0_1477.jpg 0_512.jpg 0_89.jpg 1_1285.jpg 1_320.jpg 1_707.jpg 0_1091.jpg 0_1478.jpg 0_513.jpg 0_8.jpg 1_1286.jpg 1_321.jpg 1_708.jpg 0_1092.jpg 0_1479.jpg 0_514.jpg 0_900.jpg 1_1287.jpg 1_322.jpg 1_709.jpg 0_1093.jpg 0_147.jpg 0_515.jpg 0_901.jpg 1_1288.jpg 1_323.jpg 1_70.jpg 0_1094.jpg 0_1480.jpg 0_516.jpg 0_902.jpg 1_1289.jpg 1_324.jpg 1_710.jpg 0_1095.jpg 0_1481.jpg 0_517.jpg 0_903.jpg 1_128.jpg 1_325.jpg 1_711.jpg 0_1096.jpg 0_1482.jpg 0_518.jpg 0_904.jpg 1_1290.jpg 1_326.jpg 1_712.jpg 0_1097.jpg 0_1483.jpg 0_519.jpg 0_905.jpg 1_1291.jpg 1_327.jpg 1_713.jpg 0_1098.jpg 0_1484.jpg 0_51.jpg 0_906.jpg 1_1292.jpg 1_328.jpg 1_714.jpg 0_1099.jpg 0_1485.jpg 0_520.jpg 0_907.jpg 1_1293.jpg 1_329.jpg 1_715.jpg 0_109.jpg 0_1486.jpg 0_521.jpg 0_908.jpg 1_1294.jpg 1_32.jpg 1_716.jpg 0_10.jpg 0_1487.jpg 0_522.jpg 0_909.jpg 1_1295.jpg 1_330.jpg 1_717.jpg 0_1100.jpg 0_1488.jpg 0_523.jpg 0_90.jpg 1_1296.jpg 1_331.jpg 1_718.jpg 0_1101.jpg 0_1489.jpg 0_524.jpg 0_910.jpg 1_1297.jpg 1_332.jpg 1_719.jpg 0_1102.jpg 0_148.jpg 0_525.jpg 0_911.jpg 1_1298.jpg 1_333.jpg 1_71.jpg 0_1103.jpg 0_1490.jpg 0_526.jpg 0_912.jpg 1_1299.jpg 1_334.jpg 1_720.jpg 0_1104.jpg 0_1491.jpg 0_527.jpg 0_913.jpg 1_129.jpg 1_335.jpg 1_721.jpg 0_1105.jpg 0_1492.jpg 0_528.jpg 0_914.jpg 1_12.jpg 1_336.jpg 1_722.jpg 0_1106.jpg 0_1493.jpg 0_529.jpg 0_915.jpg 1_1300.jpg 1_337.jpg 1_723.jpg 0_1107.jpg 0_1494.jpg 0_52.jpg 0_916.jpg 1_1301.jpg 1_338.jpg 1_724.jpg 0_1108.jpg 0_1495.jpg 0_530.jpg 0_917.jpg 1_1302.jpg 1_339.jpg 1_725.jpg 0_1109.jpg 0_1496.jpg 0_531.jpg 0_918.jpg 1_1303.jpg 1_33.jpg 1_726.jpg 0_110.jpg 0_1497.jpg 0_532.jpg 0_919.jpg 1_1304.jpg 1_340.jpg 1_727.jpg 0_1110.jpg 0_1498.jpg 0_533.jpg 0_91.jpg 1_1305.jpg 1_341.jpg 1_728.jpg 0_1111.jpg 0_1499.jpg 0_534.jpg 0_920.jpg 1_1306.jpg 1_342.jpg 1_729.jpg 0_1112.jpg 0_149.jpg 0_535.jpg 0_921.jpg 1_1307.jpg 1_343.jpg 1_72.jpg 0_1113.jpg 0_14.jpg 0_536.jpg 0_922.jpg 1_1308.jpg 1_344.jpg 1_730.jpg 0_1114.jpg 0_150.jpg 0_537.jpg 0_923.jpg 1_1309.jpg 1_345.jpg 1_731.jpg 0_1115.jpg 0_151.jpg 0_538.jpg 0_924.jpg 1_130.jpg 1_346.jpg 1_732.jpg 0_1116.jpg 0_152.jpg 0_539.jpg 0_925.jpg 1_1310.jpg 1_347.jpg 1_733.jpg 0_1117.jpg 0_153.jpg 0_53.jpg 0_926.jpg 1_1311.jpg 1_348.jpg 1_734.jpg 0_1118.jpg 0_154.jpg 0_540.jpg 0_927.jpg 1_1312.jpg 1_349.jpg 1_735.jpg 0_1119.jpg 0_155.jpg 0_541.jpg 0_928.jpg 1_1313.jpg 1_34.jpg 1_736.jpg 0_111.jpg 0_156.jpg 0_542.jpg 0_929.jpg 1_1314.jpg 1_350.jpg 1_737.jpg 0_1120.jpg 0_157.jpg 0_543.jpg 0_92.jpg 1_1315.jpg 1_351.jpg 1_738.jpg 0_1121.jpg 0_158.jpg 0_544.jpg 0_930.jpg 1_1316.jpg 1_352.jpg 1_739.jpg 0_1122.jpg 0_159.jpg 0_545.jpg 0_931.jpg 1_1317.jpg 1_353.jpg 1_73.jpg 0_1123.jpg 0_15.jpg 0_546.jpg 0_932.jpg 1_1318.jpg 1_354.jpg 1_740.jpg 0_1124.jpg 0_160.jpg 0_547.jpg 0_933.jpg 1_1319.jpg 1_355.jpg 1_741.jpg 0_1125.jpg 0_161.jpg 0_548.jpg 0_934.jpg 1_131.jpg 1_356.jpg 1_742.jpg 0_1126.jpg 0_162.jpg 0_549.jpg 0_935.jpg 1_1320.jpg 1_357.jpg 1_743.jpg 0_1127.jpg 0_163.jpg 0_54.jpg 0_936.jpg 1_1321.jpg 1_358.jpg 1_744.jpg 0_1128.jpg 0_164.jpg 0_550.jpg 0_937.jpg 1_1322.jpg 1_359.jpg 1_745.jpg 0_1129.jpg 0_165.jpg 0_551.jpg 0_938.jpg 1_1323.jpg 1_35.jpg 1_746.jpg 0_112.jpg 0_166.jpg 0_552.jpg 0_939.jpg 1_1324.jpg 1_360.jpg 1_747.jpg 0_1130.jpg 0_167.jpg 0_553.jpg 0_93.jpg 1_1325.jpg 1_361.jpg 1_748.jpg 0_1131.jpg 0_168.jpg 0_554.jpg 0_940.jpg 1_1326.jpg 1_362.jpg 1_749.jpg 0_1132.jpg 0_169.jpg 0_555.jpg 0_941.jpg 1_1327.jpg 1_363.jpg 1_74.jpg 0_1133.jpg 0_16.jpg 0_556.jpg 0_942.jpg 1_1328.jpg 1_364.jpg 1_750.jpg 0_1134.jpg 0_170.jpg 0_557.jpg 0_943.jpg 1_1329.jpg 1_365.jpg 1_751.jpg 0_1135.jpg 0_171.jpg 0_558.jpg 0_944.jpg 1_132.jpg 1_366.jpg 1_752.jpg 0_1136.jpg 0_172.jpg 0_559.jpg 0_945.jpg 1_1330.jpg 1_367.jpg 1_753.jpg 0_1137.jpg 0_173.jpg 0_55.jpg 0_946.jpg 1_1331.jpg 1_368.jpg 1_754.jpg 0_1138.jpg 0_174.jpg 0_560.jpg 0_947.jpg 1_1332.jpg 1_369.jpg 1_755.jpg 0_1139.jpg 0_175.jpg 0_561.jpg 0_948.jpg 1_1333.jpg 1_36.jpg 1_756.jpg 0_113.jpg 0_176.jpg 0_562.jpg 0_949.jpg 1_1334.jpg 1_370.jpg 1_757.jpg 0_1140.jpg 0_177.jpg 0_563.jpg 0_94.jpg 1_1335.jpg 1_371.jpg 1_758.jpg 0_1141.jpg 0_178.jpg 0_564.jpg 0_950.jpg 1_1336.jpg 1_372.jpg 1_759.jpg 0_1142.jpg 0_179.jpg 0_565.jpg 0_951.jpg 1_1337.jpg 1_373.jpg 1_75.jpg 0_1143.jpg 0_17.jpg 0_566.jpg 0_952.jpg 1_1338.jpg 1_374.jpg 1_760.jpg 0_1144.jpg 0_180.jpg 0_567.jpg 0_953.jpg 1_1339.jpg 1_375.jpg 1_761.jpg 0_1145.jpg 0_181.jpg 0_568.jpg 0_954.jpg 1_133.jpg 1_376.jpg 1_762.jpg 0_1146.jpg 0_182.jpg 0_569.jpg 0_955.jpg 1_1340.jpg 1_377.jpg 1_763.jpg 0_1147.jpg 0_183.jpg 0_56.jpg 0_956.jpg 1_1341.jpg 1_378.jpg 1_764.jpg 0_1148.jpg 0_184.jpg 0_570.jpg 0_957.jpg 1_1342.jpg 1_379.jpg 1_765.jpg 0_1149.jpg 0_185.jpg 0_571.jpg 0_958.jpg 1_1343.jpg 1_37.jpg 1_766.jpg 0_114.jpg 0_186.jpg 0_572.jpg 0_959.jpg 1_1344.jpg 1_380.jpg 1_767.jpg 0_1150.jpg 0_187.jpg 0_573.jpg 0_95.jpg 1_1345.jpg 1_381.jpg 1_768.jpg 0_1151.jpg 0_188.jpg 0_574.jpg 0_960.jpg 1_1346.jpg 1_382.jpg 1_769.jpg 0_1152.jpg 0_189.jpg 0_575.jpg 0_961.jpg 1_1347.jpg 1_383.jpg 1_76.jpg 0_1153.jpg 0_18.jpg 0_576.jpg 0_962.jpg 1_1348.jpg 1_384.jpg 1_770.jpg 0_1154.jpg 0_190.jpg 0_577.jpg 0_963.jpg 1_1349.jpg 1_385.jpg 1_771.jpg 0_1155.jpg 0_191.jpg 0_578.jpg 0_964.jpg 1_134.jpg 1_386.jpg 1_772.jpg 0_1156.jpg 0_192.jpg 0_579.jpg 0_965.jpg 1_1350.jpg 1_387.jpg 1_773.jpg 0_1157.jpg 0_193.jpg 0_57.jpg 0_966.jpg 1_1351.jpg 1_388.jpg 1_774.jpg 0_1158.jpg 0_194.jpg 0_580.jpg 0_967.jpg 1_1352.jpg 1_389.jpg 1_775.jpg 0_1159.jpg 0_195.jpg 0_581.jpg 0_968.jpg 1_1353.jpg 1_38.jpg 1_776.jpg 0_115.jpg 0_196.jpg 0_582.jpg 0_969.jpg 1_1354.jpg 1_390.jpg 1_777.jpg 0_1160.jpg 0_197.jpg 0_583.jpg 0_96.jpg 1_1355.jpg 1_391.jpg 1_778.jpg 0_1161.jpg 0_198.jpg 0_584.jpg 0_970.jpg 1_1356.jpg 1_392.jpg 1_779.jpg 0_1162.jpg 0_199.jpg 0_585.jpg 0_971.jpg 1_1357.jpg 1_393.jpg 1_77.jpg 0_1163.jpg 0_19.jpg 0_586.jpg 0_972.jpg 1_1358.jpg 1_394.jpg 1_780.jpg 0_1164.jpg 0_1.jpg 0_587.jpg 0_973.jpg 1_1359.jpg 1_395.jpg 1_781.jpg 0_1165.jpg 0_200.jpg 0_588.jpg 0_974.jpg 1_135.jpg 1_396.jpg 1_782.jpg 0_1166.jpg 0_201.jpg 0_589.jpg 0_975.jpg 1_1360.jpg 1_397.jpg 1_783.jpg 0_1167.jpg 0_202.jpg 0_58.jpg 0_976.jpg 1_1361.jpg 1_398.jpg 1_784.jpg 0_1168.jpg 0_203.jpg 0_590.jpg 0_977.jpg 1_1362.jpg 1_399.jpg 1_785.jpg 0_1169.jpg 0_204.jpg 0_591.jpg 0_978.jpg 1_1363.jpg 1_39.jpg 1_786.jpg 0_116.jpg 0_205.jpg 0_592.jpg 0_979.jpg 1_1364.jpg 1_3.jpg 1_787.jpg 0_1170.jpg 0_206.jpg 0_593.jpg 0_97.jpg 1_1365.jpg 1_400.jpg 1_788.jpg 0_1171.jpg 0_207.jpg 0_594.jpg 0_980.jpg 1_1366.jpg 1_401.jpg 1_789.jpg 0_1172.jpg 0_208.jpg 0_595.jpg 0_981.jpg 1_1367.jpg 1_402.jpg 1_78.jpg 0_1173.jpg 0_209.jpg 0_596.jpg 0_982.jpg 1_1368.jpg 1_403.jpg 1_790.jpg 0_1174.jpg 0_20.jpg 0_597.jpg 0_983.jpg 1_1369.jpg 1_404.jpg 1_791.jpg 0_1175.jpg 0_210.jpg 0_598.jpg 0_984.jpg 1_136.jpg 1_405.jpg 1_792.jpg 0_1176.jpg 0_211.jpg 0_599.jpg 0_985.jpg 1_1370.jpg 1_406.jpg 1_793.jpg 0_1177.jpg 0_212.jpg 0_59.jpg 0_986.jpg 1_1371.jpg 1_407.jpg 1_794.jpg 0_1178.jpg 0_213.jpg 0_5.jpg 0_987.jpg 1_1372.jpg 1_408.jpg 1_795.jpg 0_1179.jpg 0_214.jpg 0_600.jpg 0_988.jpg 1_1373.jpg 1_409.jpg 1_796.jpg 0_117.jpg 0_215.jpg 0_601.jpg 0_989.jpg 1_1374.jpg 1_40.jpg 1_797.jpg 0_1180.jpg 0_216.jpg 0_602.jpg 0_98.jpg 1_1375.jpg 1_410.jpg 1_798.jpg 0_1181.jpg 0_217.jpg 0_603.jpg 0_990.jpg 1_1376.jpg 1_411.jpg 1_799.jpg 0_1182.jpg 0_218.jpg 0_604.jpg 0_991.jpg 1_1377.jpg 1_412.jpg 1_79.jpg 0_1183.jpg 0_219.jpg 0_605.jpg 0_992.jpg 1_1378.jpg 1_413.jpg 1_7.jpg 0_1184.jpg 0_21.jpg 0_606.jpg 0_993.jpg 1_1379.jpg 1_414.jpg 1_800.jpg 0_1185.jpg 0_220.jpg 0_607.jpg 0_994.jpg 1_137.jpg 1_415.jpg 1_801.jpg 0_1186.jpg 0_221.jpg 0_608.jpg 0_995.jpg 1_1380.jpg 1_416.jpg 1_802.jpg 0_1187.jpg 0_222.jpg 0_609.jpg 0_996.jpg 1_1381.jpg 1_417.jpg 1_803.jpg 0_1188.jpg 0_223.jpg 0_60.jpg 0_997.jpg 1_1382.jpg 1_418.jpg 1_804.jpg 0_1189.jpg 0_224.jpg 0_610.jpg 0_998.jpg 1_1383.jpg 1_419.jpg 1_805.jpg 0_118.jpg 0_225.jpg 0_611.jpg 0_999.jpg 1_1384.jpg 1_41.jpg 1_806.jpg 0_1190.jpg 0_226.jpg 0_612.jpg 0_99.jpg 1_1385.jpg 1_420.jpg 1_807.jpg 0_1191.jpg 0_227.jpg 0_613.jpg 0_9.jpg 1_1386.jpg 1_421.jpg 1_808.jpg 0_1192.jpg 0_228.jpg 0_614.jpg 1_0.jpg 1_1387.jpg 1_422.jpg 1_809.jpg 0_1193.jpg 0_229.jpg 0_615.jpg 1_1000.jpg 1_1388.jpg 1_423.jpg 1_80.jpg 0_1194.jpg 0_22.jpg 0_616.jpg 1_1001.jpg 1_1389.jpg 1_424.jpg 1_810.jpg 0_1195.jpg 0_230.jpg 0_617.jpg 1_1002.jpg 1_138.jpg 1_425.jpg 1_811.jpg 0_1196.jpg 0_231.jpg 0_618.jpg 1_1003.jpg 1_1390.jpg 1_426.jpg 1_812.jpg 0_1197.jpg 0_232.jpg 0_619.jpg 1_1004.jpg 1_1391.jpg 1_427.jpg 1_813.jpg 0_1198.jpg 0_233.jpg 0_61.jpg 1_1005.jpg 1_1392.jpg 1_428.jpg 1_814.jpg 0_1199.jpg 0_234.jpg 0_620.jpg 1_1006.jpg 1_1393.jpg 1_429.jpg 1_815.jpg 0_119.jpg 0_235.jpg 0_621.jpg 1_1007.jpg 1_1394.jpg 1_42.jpg 1_816.jpg 0_11.jpg 0_236.jpg 0_622.jpg 1_1008.jpg 1_1395.jpg 1_430.jpg 1_817.jpg 0_1200.jpg 0_237.jpg 0_623.jpg 1_1009.jpg 1_1396.jpg 1_431.jpg 1_818.jpg 0_1201.jpg 0_238.jpg 0_624.jpg 1_100.jpg 1_1397.jpg 1_432.jpg 1_819.jpg 0_1202.jpg 0_239.jpg 0_625.jpg 1_1010.jpg 1_1398.jpg 1_433.jpg 1_81.jpg 0_1203.jpg 0_23.jpg 0_626.jpg 1_1011.jpg 1_1399.jpg 1_434.jpg 1_820.jpg 0_1204.jpg 0_240.jpg 0_627.jpg 1_1012.jpg 1_139.jpg 1_435.jpg 1_821.jpg 0_1205.jpg 0_241.jpg 0_628.jpg 1_1013.jpg 1_13.jpg 1_436.jpg 1_822.jpg 0_1206.jpg 0_242.jpg 0_629.jpg 1_1014.jpg 1_1400.jpg 1_437.jpg 1_823.jpg 0_1207.jpg 0_243.jpg 0_62.jpg 1_1015.jpg 1_1401.jpg 1_438.jpg 1_824.jpg 0_1208.jpg 0_244.jpg 0_630.jpg 1_1016.jpg 1_1402.jpg 1_439.jpg 1_825.jpg 0_1209.jpg 0_245.jpg 0_631.jpg 1_1017.jpg 1_1403.jpg 1_43.jpg 1_826.jpg 0_120.jpg 0_246.jpg 0_632.jpg 1_1018.jpg 1_1404.jpg 1_440.jpg 1_827.jpg 0_1210.jpg 0_247.jpg 0_633.jpg 1_1019.jpg 1_1405.jpg 1_441.jpg 1_828.jpg 0_1211.jpg 0_248.jpg 0_634.jpg 1_101.jpg 1_1406.jpg 1_442.jpg 1_829.jpg 0_1212.jpg 0_249.jpg 0_635.jpg 1_1020.jpg 1_1407.jpg 1_443.jpg 1_82.jpg 0_1213.jpg 0_24.jpg 0_636.jpg 1_1021.jpg 1_1408.jpg 1_444.jpg 1_830.jpg 0_1214.jpg 0_250.jpg 0_637.jpg 1_1022.jpg 1_1409.jpg 1_445.jpg 1_831.jpg 0_1215.jpg 0_251.jpg 0_638.jpg 1_1023.jpg 1_140.jpg 1_446.jpg 1_832.jpg 0_1216.jpg 0_252.jpg 0_639.jpg 1_1024.jpg 1_1410.jpg 1_447.jpg 1_833.jpg 0_1217.jpg 0_253.jpg 0_63.jpg 1_1025.jpg 1_1411.jpg 1_448.jpg 1_834.jpg 0_1218.jpg 0_254.jpg 0_640.jpg 1_1026.jpg 1_1412.jpg 1_449.jpg 1_835.jpg 0_1219.jpg 0_255.jpg 0_641.jpg 1_1027.jpg 1_1413.jpg 1_44.jpg 1_836.jpg 0_121.jpg 0_256.jpg 0_642.jpg 1_1028.jpg 1_1414.jpg 1_450.jpg 1_837.jpg 0_1220.jpg 0_257.jpg 0_643.jpg 1_1029.jpg 1_1415.jpg 1_451.jpg 1_838.jpg 0_1221.jpg 0_258.jpg 0_644.jpg 1_102.jpg 1_1416.jpg 1_452.jpg 1_839.jpg 0_1222.jpg 0_259.jpg 0_645.jpg 1_1030.jpg 1_1417.jpg 1_453.jpg 1_83.jpg 0_1223.jpg 0_25.jpg 0_646.jpg 1_1031.jpg 1_1418.jpg 1_454.jpg 1_840.jpg 0_1224.jpg 0_260.jpg 0_647.jpg 1_1032.jpg 1_1419.jpg 1_455.jpg 1_841.jpg 0_1225.jpg 0_261.jpg 0_648.jpg 1_1033.jpg 1_141.jpg 1_456.jpg 1_842.jpg 0_1226.jpg 0_262.jpg 0_649.jpg 1_1034.jpg 1_1420.jpg 1_457.jpg 1_843.jpg 0_1227.jpg 0_263.jpg 0_64.jpg 1_1035.jpg 1_1421.jpg 1_458.jpg 1_844.jpg 0_1228.jpg 0_264.jpg 0_650.jpg 1_1036.jpg 1_1422.jpg 1_459.jpg 1_845.jpg 0_1229.jpg 0_265.jpg 0_651.jpg 1_1037.jpg 1_1423.jpg 1_45.jpg 1_846.jpg 0_122.jpg 0_266.jpg 0_652.jpg 1_1038.jpg 1_1424.jpg 1_460.jpg 1_847.jpg 0_1230.jpg 0_267.jpg 0_653.jpg 1_1039.jpg 1_1425.jpg 1_461.jpg 1_848.jpg 0_1231.jpg 0_268.jpg 0_654.jpg 1_103.jpg 1_1426.jpg 1_462.jpg 1_849.jpg 0_1232.jpg 0_269.jpg 0_655.jpg 1_1040.jpg 1_1427.jpg 1_463.jpg 1_84.jpg 0_1233.jpg 0_26.jpg 0_656.jpg 1_1041.jpg 1_1428.jpg 1_464.jpg 1_850.jpg 0_1234.jpg 0_270.jpg 0_657.jpg 1_1042.jpg 1_1429.jpg 1_465.jpg 1_851.jpg 0_1235.jpg 0_271.jpg 0_658.jpg 1_1043.jpg 1_142.jpg 1_466.jpg 1_852.jpg 0_1236.jpg 0_272.jpg 0_659.jpg 1_1044.jpg 1_1430.jpg 1_467.jpg 1_853.jpg 0_1237.jpg 0_273.jpg 0_65.jpg 1_1045.jpg 1_1431.jpg 1_468.jpg 1_854.jpg 0_1238.jpg 0_274.jpg 0_660.jpg 1_1046.jpg 1_1432.jpg 1_469.jpg 1_855.jpg 0_1239.jpg 0_275.jpg 0_661.jpg 1_1047.jpg 1_1433.jpg 1_46.jpg 1_856.jpg 0_123.jpg 0_276.jpg 0_662.jpg 1_1048.jpg 1_1434.jpg 1_470.jpg 1_857.jpg 0_1240.jpg 0_277.jpg 0_663.jpg 1_1049.jpg 1_1435.jpg 1_471.jpg 1_858.jpg 0_1241.jpg 0_278.jpg 0_664.jpg 1_104.jpg 1_1436.jpg 1_472.jpg 1_859.jpg 0_1242.jpg 0_279.jpg 0_665.jpg 1_1050.jpg 1_1437.jpg 1_473.jpg 1_85.jpg 0_1243.jpg 0_27.jpg 0_666.jpg 1_1051.jpg 1_1438.jpg 1_474.jpg 1_860.jpg 0_1244.jpg 0_280.jpg 0_667.jpg 1_1052.jpg 1_1439.jpg 1_475.jpg 1_861.jpg 0_1245.jpg 0_281.jpg 0_668.jpg 1_1053.jpg 1_143.jpg 1_476.jpg 1_862.jpg 0_1246.jpg 0_282.jpg 0_669.jpg 1_1054.jpg 1_1440.jpg 1_477.jpg 1_863.jpg 0_1247.jpg 0_283.jpg 0_66.jpg 1_1055.jpg 1_1441.jpg 1_478.jpg 1_864.jpg 0_1248.jpg 0_284.jpg 0_670.jpg 1_1056.jpg 1_1442.jpg 1_479.jpg 1_865.jpg 0_1249.jpg 0_285.jpg 0_671.jpg 1_1057.jpg 1_1443.jpg 1_47.jpg 1_866.jpg 0_124.jpg 0_286.jpg 0_672.jpg 1_1058.jpg 1_1444.jpg 1_480.jpg 1_867.jpg 0_1250.jpg 0_287.jpg 0_673.jpg 1_1059.jpg 1_1445.jpg 1_481.jpg 1_868.jpg 0_1251.jpg 0_288.jpg 0_674.jpg 1_105.jpg 1_1446.jpg 1_482.jpg 1_869.jpg 0_1252.jpg 0_289.jpg 0_675.jpg 1_1060.jpg 1_1447.jpg 1_483.jpg 1_86.jpg 0_1253.jpg 0_28.jpg 0_676.jpg 1_1061.jpg 1_1448.jpg 1_484.jpg 1_870.jpg 0_1254.jpg 0_290.jpg 0_677.jpg 1_1062.jpg 1_1449.jpg 1_485.jpg 1_871.jpg 0_1255.jpg 0_291.jpg 0_678.jpg 1_1063.jpg 1_144.jpg 1_486.jpg 1_872.jpg 0_1256.jpg 0_292.jpg 0_679.jpg 1_1064.jpg 1_1450.jpg 1_487.jpg 1_873.jpg 0_1257.jpg 0_293.jpg 0_67.jpg 1_1065.jpg 1_1451.jpg 1_488.jpg 1_874.jpg 0_1258.jpg 0_294.jpg 0_680.jpg 1_1066.jpg 1_1452.jpg 1_489.jpg 1_875.jpg 0_1259.jpg 0_295.jpg 0_681.jpg 1_1067.jpg 1_1453.jpg 1_48.jpg 1_876.jpg 0_125.jpg 0_296.jpg 0_682.jpg 1_1068.jpg 1_1454.jpg 1_490.jpg 1_877.jpg 0_1260.jpg 0_297.jpg 0_683.jpg 1_1069.jpg 1_1455.jpg 1_491.jpg 1_878.jpg 0_1261.jpg 0_298.jpg 0_684.jpg 1_106.jpg 1_1456.jpg 1_492.jpg 1_879.jpg 0_1262.jpg 0_299.jpg 0_685.jpg 1_1070.jpg 1_1457.jpg 1_493.jpg 1_87.jpg 0_1263.jpg 0_29.jpg 0_686.jpg 1_1071.jpg 1_1458.jpg 1_494.jpg 1_880.jpg 0_1264.jpg 0_2.jpg 0_687.jpg 1_1072.jpg 1_1459.jpg 1_495.jpg 1_881.jpg 0_1265.jpg 0_300.jpg 0_688.jpg 1_1073.jpg 1_145.jpg 1_496.jpg 1_882.jpg 0_1266.jpg 0_301.jpg 0_689.jpg 1_1074.jpg 1_1460.jpg 1_497.jpg 1_883.jpg 0_1267.jpg 0_302.jpg 0_68.jpg 1_1075.jpg 1_1461.jpg 1_498.jpg 1_884.jpg 0_1268.jpg 0_303.jpg 0_690.jpg 1_1076.jpg 1_1462.jpg 1_499.jpg 1_885.jpg 0_1269.jpg 0_304.jpg 0_691.jpg 1_1077.jpg 1_1463.jpg 1_49.jpg 1_886.jpg 0_126.jpg 0_305.jpg 0_692.jpg 1_1078.jpg 1_1464.jpg 1_4.jpg 1_887.jpg 0_1270.jpg 0_306.jpg 0_693.jpg 1_1079.jpg 1_1465.jpg 1_500.jpg 1_888.jpg 0_1271.jpg 0_307.jpg 0_694.jpg 1_107.jpg 1_1466.jpg 1_501.jpg 1_889.jpg 0_1272.jpg 0_308.jpg 0_695.jpg 1_1080.jpg 1_1467.jpg 1_502.jpg 1_88.jpg 0_1273.jpg 0_309.jpg 0_696.jpg 1_1081.jpg 1_1468.jpg 1_503.jpg 1_890.jpg 0_1274.jpg 0_30.jpg 0_697.jpg 1_1082.jpg 1_1469.jpg 1_504.jpg 1_891.jpg 0_1275.jpg 0_310.jpg 0_698.jpg 1_1083.jpg 1_146.jpg 1_505.jpg 1_892.jpg 0_1276.jpg 0_311.jpg 0_699.jpg 1_1084.jpg 1_1470.jpg 1_506.jpg 1_893.jpg 0_1277.jpg 0_312.jpg 0_69.jpg 1_1085.jpg 1_1471.jpg 1_507.jpg 1_894.jpg 0_1278.jpg 0_313.jpg 0_6.jpg 1_1086.jpg 1_1472.jpg 1_508.jpg 1_895.jpg 0_1279.jpg 0_314.jpg 0_700.jpg 1_1087.jpg 1_1473.jpg 1_509.jpg 1_896.jpg 0_127.jpg 0_315.jpg 0_701.jpg 1_1088.jpg 1_1474.jpg 1_50.jpg 1_897.jpg 0_1280.jpg 0_316.jpg 0_702.jpg 1_1089.jpg 1_1475.jpg 1_510.jpg 1_898.jpg 0_1281.jpg 0_317.jpg 0_703.jpg 1_108.jpg 1_1476.jpg 1_511.jpg 1_899.jpg 0_1282.jpg 0_318.jpg 0_704.jpg 1_1090.jpg 1_1477.jpg 1_512.jpg 1_89.jpg 0_1283.jpg 0_319.jpg 0_705.jpg 1_1091.jpg 1_1478.jpg 1_513.jpg 1_8.jpg 0_1284.jpg 0_31.jpg 0_706.jpg 1_1092.jpg 1_1479.jpg 1_514.jpg 1_900.jpg 0_1285.jpg 0_320.jpg 0_707.jpg 1_1093.jpg 1_147.jpg 1_515.jpg 1_901.jpg 0_1286.jpg 0_321.jpg 0_708.jpg 1_1094.jpg 1_1480.jpg 1_516.jpg 1_902.jpg 0_1287.jpg 0_322.jpg 0_709.jpg 1_1095.jpg 1_1481.jpg 1_517.jpg 1_903.jpg 0_1288.jpg 0_323.jpg 0_70.jpg 1_1096.jpg 1_1482.jpg 1_518.jpg 1_904.jpg 0_1289.jpg 0_324.jpg 0_710.jpg 1_1097.jpg 1_1483.jpg 1_519.jpg 1_905.jpg 0_128.jpg 0_325.jpg 0_711.jpg 1_1098.jpg 1_1484.jpg 1_51.jpg 1_906.jpg 0_1290.jpg 0_326.jpg 0_712.jpg 1_1099.jpg 1_1485.jpg 1_520.jpg 1_907.jpg 0_1291.jpg 0_327.jpg 0_713.jpg 1_109.jpg 1_1486.jpg 1_521.jpg 1_908.jpg 0_1292.jpg 0_328.jpg 0_714.jpg 1_10.jpg 1_1487.jpg 1_522.jpg 1_909.jpg 0_1293.jpg 0_329.jpg 0_715.jpg 1_1100.jpg 1_1488.jpg 1_523.jpg 1_90.jpg 0_1294.jpg 0_32.jpg 0_716.jpg 1_1101.jpg 1_1489.jpg 1_524.jpg 1_910.jpg 0_1295.jpg 0_330.jpg 0_717.jpg 1_1102.jpg 1_148.jpg 1_525.jpg 1_911.jpg 0_1296.jpg 0_331.jpg 0_718.jpg 1_1103.jpg 1_1490.jpg 1_526.jpg 1_912.jpg 0_1297.jpg 0_332.jpg 0_719.jpg 1_1104.jpg 1_1491.jpg 1_527.jpg 1_913.jpg 0_1298.jpg 0_333.jpg 0_71.jpg 1_1105.jpg 1_1492.jpg 1_528.jpg 1_914.jpg 0_1299.jpg 0_334.jpg 0_720.jpg 1_1106.jpg 1_1493.jpg 1_529.jpg 1_915.jpg 0_129.jpg 0_335.jpg 0_721.jpg 1_1107.jpg 1_1494.jpg 1_52.jpg 1_916.jpg 0_12.jpg 0_336.jpg 0_722.jpg 1_1108.jpg 1_1495.jpg 1_530.jpg 1_917.jpg 0_1300.jpg 0_337.jpg 0_723.jpg 1_1109.jpg 1_1496.jpg 1_531.jpg 1_918.jpg 0_1301.jpg 0_338.jpg 0_724.jpg 1_110.jpg 1_1497.jpg 1_532.jpg 1_919.jpg 0_1302.jpg 0_339.jpg 0_725.jpg 1_1110.jpg 1_1498.jpg 1_533.jpg 1_91.jpg 0_1303.jpg 0_33.jpg 0_726.jpg 1_1111.jpg 1_1499.jpg 1_534.jpg 1_920.jpg 0_1304.jpg 0_340.jpg 0_727.jpg 1_1112.jpg 1_149.jpg 1_535.jpg 1_921.jpg 0_1305.jpg 0_341.jpg 0_728.jpg 1_1113.jpg 1_14.jpg 1_536.jpg 1_922.jpg 0_1306.jpg 0_342.jpg 0_729.jpg 1_1114.jpg 1_150.jpg 1_537.jpg 1_923.jpg 0_1307.jpg 0_343.jpg 0_72.jpg 1_1115.jpg 1_151.jpg 1_538.jpg 1_924.jpg 0_1308.jpg 0_344.jpg 0_730.jpg 1_1116.jpg 1_152.jpg 1_539.jpg 1_925.jpg 0_1309.jpg 0_345.jpg 0_731.jpg 1_1117.jpg 1_153.jpg 1_53.jpg 1_926.jpg 0_130.jpg 0_346.jpg 0_732.jpg 1_1118.jpg 1_154.jpg 1_540.jpg 1_927.jpg 0_1310.jpg 0_347.jpg 0_733.jpg 1_1119.jpg 1_155.jpg 1_541.jpg 1_928.jpg 0_1311.jpg 0_348.jpg 0_734.jpg 1_111.jpg 1_156.jpg 1_542.jpg 1_929.jpg 0_1312.jpg 0_349.jpg 0_735.jpg 1_1120.jpg 1_157.jpg 1_543.jpg 1_92.jpg 0_1313.jpg 0_34.jpg 0_736.jpg 1_1121.jpg 1_158.jpg 1_544.jpg 1_930.jpg 0_1314.jpg 0_350.jpg 0_737.jpg 1_1122.jpg 1_159.jpg 1_545.jpg 1_931.jpg 0_1315.jpg 0_351.jpg 0_738.jpg 1_1123.jpg 1_15.jpg 1_546.jpg 1_932.jpg 0_1316.jpg 0_352.jpg 0_739.jpg 1_1124.jpg 1_160.jpg 1_547.jpg 1_933.jpg 0_1317.jpg 0_353.jpg 0_73.jpg 1_1125.jpg 1_161.jpg 1_548.jpg 1_934.jpg 0_1318.jpg 0_354.jpg 0_740.jpg 1_1126.jpg 1_162.jpg 1_549.jpg 1_935.jpg 0_1319.jpg 0_355.jpg 0_741.jpg 1_1127.jpg 1_163.jpg 1_54.jpg 1_936.jpg 0_131.jpg 0_356.jpg 0_742.jpg 1_1128.jpg 1_164.jpg 1_550.jpg 1_937.jpg 0_1320.jpg 0_357.jpg 0_743.jpg 1_1129.jpg 1_165.jpg 1_551.jpg 1_938.jpg 0_1321.jpg 0_358.jpg 0_744.jpg 1_112.jpg 1_166.jpg 1_552.jpg 1_939.jpg 0_1322.jpg 0_359.jpg 0_745.jpg 1_1130.jpg 1_167.jpg 1_553.jpg 1_93.jpg 0_1323.jpg 0_35.jpg 0_746.jpg 1_1131.jpg 1_168.jpg 1_554.jpg 1_940.jpg 0_1324.jpg 0_360.jpg 0_747.jpg 1_1132.jpg 1_169.jpg 1_555.jpg 1_941.jpg 0_1325.jpg 0_361.jpg 0_748.jpg 1_1133.jpg 1_16.jpg 1_556.jpg 1_942.jpg 0_1326.jpg 0_362.jpg 0_749.jpg 1_1134.jpg 1_170.jpg 1_557.jpg 1_943.jpg 0_1327.jpg 0_363.jpg 0_74.jpg 1_1135.jpg 1_171.jpg 1_558.jpg 1_944.jpg 0_1328.jpg 0_364.jpg 0_750.jpg 1_1136.jpg 1_172.jpg 1_559.jpg 1_945.jpg 0_1329.jpg 0_365.jpg 0_751.jpg 1_1137.jpg 1_173.jpg 1_55.jpg 1_946.jpg 0_132.jpg 0_366.jpg 0_752.jpg 1_1138.jpg 1_174.jpg 1_560.jpg 1_947.jpg 0_1330.jpg 0_367.jpg 0_753.jpg 1_1139.jpg 1_175.jpg 1_561.jpg 1_948.jpg 0_1331.jpg 0_368.jpg 0_754.jpg 1_113.jpg 1_176.jpg 1_562.jpg 1_949.jpg 0_1332.jpg 0_369.jpg 0_755.jpg 1_1140.jpg 1_177.jpg 1_563.jpg 1_94.jpg 0_1333.jpg 0_36.jpg 0_756.jpg 1_1141.jpg 1_178.jpg 1_564.jpg 1_950.jpg 0_1334.jpg 0_370.jpg 0_757.jpg 1_1142.jpg 1_179.jpg 1_565.jpg 1_951.jpg 0_1335.jpg 0_371.jpg 0_758.jpg 1_1143.jpg 1_17.jpg 1_566.jpg 1_952.jpg 0_1336.jpg 0_372.jpg 0_759.jpg 1_1144.jpg 1_180.jpg 1_567.jpg 1_953.jpg 0_1337.jpg 0_373.jpg 0_75.jpg 1_1145.jpg 1_181.jpg 1_568.jpg 1_954.jpg 0_1338.jpg 0_374.jpg 0_760.jpg 1_1146.jpg 1_182.jpg 1_569.jpg 1_955.jpg 0_1339.jpg 0_375.jpg 0_761.jpg 1_1147.jpg 1_183.jpg 1_56.jpg 1_956.jpg 0_133.jpg 0_376.jpg 0_762.jpg 1_1148.jpg 1_184.jpg 1_570.jpg 1_957.jpg 0_1340.jpg 0_377.jpg 0_763.jpg 1_1149.jpg 1_185.jpg 1_571.jpg 1_958.jpg 0_1341.jpg 0_378.jpg 0_764.jpg 1_114.jpg 1_186.jpg 1_572.jpg 1_959.jpg 0_1342.jpg 0_379.jpg 0_765.jpg 1_1150.jpg 1_187.jpg 1_573.jpg 1_95.jpg 0_1343.jpg 0_37.jpg 0_766.jpg 1_1151.jpg 1_188.jpg 1_574.jpg 1_960.jpg 0_1344.jpg 0_380.jpg 0_767.jpg 1_1152.jpg 1_189.jpg 1_575.jpg 1_961.jpg 0_1345.jpg 0_381.jpg 0_768.jpg 1_1153.jpg 1_18.jpg 1_576.jpg 1_962.jpg 0_1346.jpg 0_382.jpg 0_769.jpg 1_1154.jpg 1_190.jpg 1_577.jpg 1_963.jpg 0_1347.jpg 0_383.jpg 0_76.jpg 1_1155.jpg 1_191.jpg 1_578.jpg 1_964.jpg 0_1348.jpg 0_384.jpg 0_770.jpg 1_1156.jpg 1_192.jpg 1_579.jpg 1_965.jpg 0_1349.jpg 0_385.jpg 0_771.jpg 1_1157.jpg 1_193.jpg 1_57.jpg 1_966.jpg 0_134.jpg 0_386.jpg 0_772.jpg 1_1158.jpg 1_194.jpg 1_580.jpg 1_967.jpg 0_1350.jpg 0_387.jpg 0_773.jpg 1_1159.jpg 1_195.jpg 1_581.jpg 1_968.jpg 0_1351.jpg 0_388.jpg 0_774.jpg 1_115.jpg 1_196.jpg 1_582.jpg 1_969.jpg 0_1352.jpg 0_389.jpg 0_775.jpg 1_1160.jpg 1_197.jpg 1_583.jpg 1_96.jpg 0_1353.jpg 0_38.jpg 0_776.jpg 1_1161.jpg 1_198.jpg 1_584.jpg 1_970.jpg 0_1354.jpg 0_390.jpg 0_777.jpg 1_1162.jpg 1_199.jpg 1_585.jpg 1_971.jpg 0_1355.jpg 0_391.jpg 0_778.jpg 1_1163.jpg 1_19.jpg 1_586.jpg 1_972.jpg 0_1356.jpg 0_392.jpg 0_779.jpg 1_1164.jpg 1_1.jpg 1_587.jpg 1_973.jpg 0_1357.jpg 0_393.jpg 0_77.jpg 1_1165.jpg 1_200.jpg 1_588.jpg 1_974.jpg 0_1358.jpg 0_394.jpg 0_780.jpg 1_1166.jpg 1_201.jpg 1_589.jpg 1_975.jpg 0_1359.jpg 0_395.jpg 0_781.jpg 1_1167.jpg 1_202.jpg 1_58.jpg 1_976.jpg 0_135.jpg 0_396.jpg 0_782.jpg 1_1168.jpg 1_203.jpg 1_590.jpg 1_977.jpg 0_1360.jpg 0_397.jpg 0_783.jpg 1_1169.jpg 1_204.jpg 1_591.jpg 1_978.jpg 0_1361.jpg 0_398.jpg 0_784.jpg 1_116.jpg 1_205.jpg 1_592.jpg 1_979.jpg 0_1362.jpg 0_399.jpg 0_785.jpg 1_1170.jpg 1_206.jpg 1_593.jpg 1_97.jpg 0_1363.jpg 0_39.jpg 0_786.jpg 1_1171.jpg 1_207.jpg 1_594.jpg 1_980.jpg 0_1364.jpg 0_3.jpg 0_787.jpg 1_1172.jpg 1_208.jpg 1_595.jpg 1_981.jpg 0_1365.jpg 0_400.jpg 0_788.jpg 1_1173.jpg 1_209.jpg 1_596.jpg 1_982.jpg 0_1366.jpg 0_401.jpg 0_789.jpg 1_1174.jpg 1_20.jpg 1_597.jpg 1_983.jpg 0_1367.jpg 0_402.jpg 0_78.jpg 1_1175.jpg 1_210.jpg 1_598.jpg 1_984.jpg 0_1368.jpg 0_403.jpg 0_790.jpg 1_1176.jpg 1_211.jpg 1_599.jpg 1_985.jpg 0_1369.jpg 0_404.jpg 0_791.jpg 1_1177.jpg 1_212.jpg 1_59.jpg 1_986.jpg 0_136.jpg 0_405.jpg 0_792.jpg 1_1178.jpg 1_213.jpg 1_5.jpg 1_987.jpg 0_1370.jpg 0_406.jpg 0_793.jpg 1_1179.jpg 1_214.jpg 1_600.jpg 1_988.jpg 0_1371.jpg 0_407.jpg 0_794.jpg 1_117.jpg 1_215.jpg 1_601.jpg 1_989.jpg 0_1372.jpg 0_408.jpg 0_795.jpg 1_1180.jpg 1_216.jpg 1_602.jpg 1_98.jpg 0_1373.jpg 0_409.jpg 0_796.jpg 1_1181.jpg 1_217.jpg 1_603.jpg 1_990.jpg 0_1374.jpg 0_40.jpg 0_797.jpg 1_1182.jpg 1_218.jpg 1_604.jpg 1_991.jpg 0_1375.jpg 0_410.jpg 0_798.jpg 1_1183.jpg 1_219.jpg 1_605.jpg 1_992.jpg 0_1376.jpg 0_411.jpg 0_799.jpg 1_1184.jpg 1_21.jpg 1_606.jpg 1_993.jpg 0_1377.jpg 0_412.jpg 0_79.jpg 1_1185.jpg 1_220.jpg 1_607.jpg 1_994.jpg 0_1378.jpg 0_413.jpg 0_7.jpg 1_1186.jpg 1_221.jpg 1_608.jpg 1_995.jpg 0_1379.jpg 0_414.jpg 0_800.jpg 1_1187.jpg 1_222.jpg 1_609.jpg 1_996.jpg 0_137.jpg 0_415.jpg 0_801.jpg 1_1188.jpg 1_223.jpg 1_60.jpg 1_997.jpg 0_1380.jpg 0_416.jpg 0_802.jpg 1_1189.jpg 1_224.jpg 1_610.jpg 1_998.jpg 0_1381.jpg 0_417.jpg 0_803.jpg 1_118.jpg 1_225.jpg 1_611.jpg 1_999.jpg 0_1382.jpg 0_418.jpg 0_804.jpg 1_1190.jpg 1_226.jpg 1_612.jpg 1_99.jpg 0_1383.jpg 0_419.jpg 0_805.jpg 1_1191.jpg 1_227.jpg 1_613.jpg 1_9.jpg 0_1384.jpg 0_41.jpg 0_806.jpg 1_1192.jpg 1_228.jpg 1_614.jpg 0_1385.jpg 0_420.jpg 0_807.jpg 1_1193.jpg 1_229.jpg 1_615.jpg 0_1386.jpg 0_421.jpg 0_808.jpg 1_1194.jpg 1_22.jpg 1_616.jpg # look at an image for fun plt . imshow ( image . load_img ( 'training/0_808.jpg' )) plt . show () # Food images start with 1, non-food images start with 0 plt . imshow ( image . load_img ( 'training/1_616.jpg' )) plt . show () ! mkdir data # Make directories to store the data Keras-style ! mkdir data / train ! mkdir data / test ! mkdir data / train / nonfood ! mkdir data / train / food ! mkdir data / test / nonfood ! mkdir data / test / food # Move the images # Note: we will consider 'training' to be the train set # 'validation' folder will be the test set # ignore the 'evaluation' set ! mv training / 0 *. jpg data / train / nonfood ! mv training / 1 *. jpg data / train / food ! mv validation / 0 *. jpg data / test / nonfood ! mv validation / 1 *. jpg data / test / food train_path = 'data/train' valid_path = 'data/test' # These images are pretty big and of different sizes # Let's load them all in as the same (smaller) size IMAGE_SIZE = [ 200 , 200 ] # useful for getting number of files image_files = glob ( train_path + '/*/*.jpg' ) valid_image_files = glob ( valid_path + '/*/*.jpg' ) # useful for getting number of classes folders = glob ( train_path + '/*' ) folders # look at an image for fun plt . imshow ( image . load_img ( np . random . choice ( image_files ))) plt . show () ptm = PretrainedModel ( input_shape = IMAGE_SIZE + [ 3 ], weights = 'imagenet' , include_top = False ) # map the data into feature vectors x = Flatten ()( ptm . output ) # create a model object model = Model ( inputs = ptm . input , outputs = x ) # view the structure of the model model . summary () Model: \"model_3\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 200, 200, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 200, 200, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 200, 200, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 100, 100, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 100, 100, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 100, 100, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 50, 50, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 50, 50, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 50, 50, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 50, 50, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 25, 25, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 25, 25, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 25, 25, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 25, 25, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 12, 12, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 6, 6, 512) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 18432) 0 ================================================================= Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________ # create an instance of ImageDataGenerator gen = ImageDataGenerator ( preprocessing_function = preprocess_input ) batch_size = 128 # create generators train_generator = gen . flow_from_directory ( train_path , target_size = IMAGE_SIZE , batch_size = batch_size , class_mode = 'binary' , ) valid_generator = gen . flow_from_directory ( valid_path , target_size = IMAGE_SIZE , batch_size = batch_size , class_mode = 'binary' , ) Found 3000 images belonging to 2 classes. Found 1000 images belonging to 2 classes. Ntrain = len ( image_files ) Nvalid = len ( valid_image_files ) # Figure out the output size feat = model . predict ( np . random . random ([ 1 ] + IMAGE_SIZE + [ 3 ])) D = feat . shape [ 1 ] X_train = np . zeros (( Ntrain , D )) Y_train = np . zeros ( Ntrain ) X_valid = np . zeros (( Nvalid , D )) Y_valid = np . zeros ( Nvalid ) # populate X_train and Y_train i = 0 for x , y in train_generator : # get features features = model . predict ( x ) # size of the batch (may not always be batch_size) sz = len ( y ) # assign to X_train and Ytrain X_train [ i : i + sz ] = features Y_train [ i : i + sz ] = y # increment i i += sz print ( i ) if i >= Ntrain : print ( 'breaking now' ) break print ( i ) 128 256 384 512 640 768 896 1024 1152 1280 1408 1536 1664 1792 1920 2048 2176 2304 2432 2560 2688 2816 2944 3000 breaking now 3000 # populate X_valid and Y_valid i = 0 for x , y in valid_generator : # get features features = model . predict ( x ) # size of the batch (may not always be batch_size) sz = len ( y ) # assign to X_train and Ytrain X_valid [ i : i + sz ] = features Y_valid [ i : i + sz ] = y # increment i i += sz if i >= Nvalid : print ( 'breaking now' ) break print ( i ) breaking now 1000 X_train . max (), X_train . min () (749.7380981445312, 0.0) from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X_train2 = scaler . fit_transform ( X_train ) X_valid2 = scaler . transform ( X_valid ) # Try the built-in logistic regression from sklearn.linear_model import LogisticRegression logr = LogisticRegression () logr . fit ( X_train2 , Y_train ) print ( logr . score ( X_train2 , Y_train )) print ( logr . score ( X_valid2 , Y_valid )) /usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. FutureWarning) 1.0 0.977 # Do logistic regression in Tensorflow i = Input ( shape = ( D ,)) x = Dense ( 1 , activation = 'sigmoid' )( i ) linearmodel = Model ( i , x ) linearmodel . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) # Can try both normalized and unnormalized data r = linearmodel . fit ( X_train , Y_train , batch_size = 128 , epochs = 10 , validation_data = ( X_valid , Y_valid ), ) Train on 3000 samples, validate on 1000 samples Epoch 1/10 WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f7c18dd5598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f7c18dd5598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num' 3000/3000 [==============================] - 1s 312us/sample - loss: 0.9217 - accuracy: 0.9293 - val_loss: 0.3271 - val_accuracy: 0.9780 Epoch 2/10 3000/3000 [==============================] - 0s 166us/sample - loss: 0.1115 - accuracy: 0.9893 - val_loss: 0.3457 - val_accuracy: 0.9740 Epoch 3/10 3000/3000 [==============================] - 0s 157us/sample - loss: 0.0210 - accuracy: 0.9977 - val_loss: 0.3130 - val_accuracy: 0.9810 Epoch 4/10 3000/3000 [==============================] - 0s 159us/sample - loss: 8.7803e-04 - accuracy: 0.9993 - val_loss: 0.3244 - val_accuracy: 0.9810 Epoch 5/10 3000/3000 [==============================] - 0s 154us/sample - loss: 2.6010e-05 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.9810 Epoch 6/10 3000/3000 [==============================] - 0s 159us/sample - loss: 8.5458e-06 - accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 0.9800 Epoch 7/10 3000/3000 [==============================] - 0s 155us/sample - loss: 6.6527e-06 - accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 0.9800 Epoch 8/10 3000/3000 [==============================] - 0s 152us/sample - loss: 5.8428e-06 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.9800 Epoch 9/10 3000/3000 [==============================] - 0s 151us/sample - loss: 4.0881e-06 - accuracy: 1.0000 - val_loss: 0.3251 - val_accuracy: 0.9800 Epoch 10/10 3000/3000 [==============================] - 0s 153us/sample - loss: 3.6229e-06 - accuracy: 1.0000 - val_loss: 0.3251 - val_accuracy: 0.9800 # loss plt . plot ( r . history [ 'loss' ], label = 'train loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val loss' ) plt . legend () plt . show () # accuracies plt . plot ( r . history [ 'accuracy' ], label = 'train acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val acc' ) plt . legend () plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Transfer Learning"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning_with_Data_Augmentation/","text":"================ by Jawad Haider Transfer Learning with Data Augmentation Transfer Learning with Data Augmentation \u00b6 # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-rc0 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0 # More imports from tensorflow.keras.layers import Input , Dense , Flatten from tensorflow.keras.applications.vgg16 import VGG16 as PretrainedModel , \\ preprocess_input from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam from tensorflow.keras.preprocessing import image from tensorflow.keras.preprocessing.image import ImageDataGenerator from glob import glob import numpy as np import pandas as pd import matplotlib.pyplot as plt import sys , os # Data from: https://mmspg.epfl.ch/downloads/food-image-datasets/ # !wget --passive-ftp --prefer-family=ipv4 --ftp-user FoodImage@grebvm2.epfl.ch \\ # --ftp-password Cahc1moo -nc ftp://tremplin.epfl.ch/Food-5K.zip ! wget - nc https : // lazyprogrammer . me / course_files / Food - 5 K . zip --2019-12-04 20:50:28-- https://lazyprogrammer.me/course_files/Food-5K.zip Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.31.81.48, 104.31.80.48, 2606:4700:30::681f:5030, ... Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.31.81.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 447001986 (426M) [application/zip] Saving to: \u2018Food-5K.zip\u2019 Food-5K.zip 100%[===================>] 426.29M 3.94MB/s in 48s 2019-12-04 20:51:17 (8.87 MB/s) - \u2018Food-5K.zip\u2019 saved [447001986/447001986] ! unzip - qq - o Food - 5 K . zip ! ls Food-5K Food-5K.zip __MACOSX sample_data ! ls Food - 5 K / training 0_0.jpg 0_1387.jpg 0_422.jpg 0_809.jpg 1_1195.jpg 1_230.jpg 1_617.jpg 0_1000.jpg 0_1388.jpg 0_423.jpg 0_80.jpg 1_1196.jpg 1_231.jpg 1_618.jpg 0_1001.jpg 0_1389.jpg 0_424.jpg 0_810.jpg 1_1197.jpg 1_232.jpg 1_619.jpg 0_1002.jpg 0_138.jpg 0_425.jpg 0_811.jpg 1_1198.jpg 1_233.jpg 1_61.jpg 0_1003.jpg 0_1390.jpg 0_426.jpg 0_812.jpg 1_1199.jpg 1_234.jpg 1_620.jpg 0_1004.jpg 0_1391.jpg 0_427.jpg 0_813.jpg 1_119.jpg 1_235.jpg 1_621.jpg 0_1005.jpg 0_1392.jpg 0_428.jpg 0_814.jpg 1_11.jpg 1_236.jpg 1_622.jpg 0_1006.jpg 0_1393.jpg 0_429.jpg 0_815.jpg 1_1200.jpg 1_237.jpg 1_623.jpg 0_1007.jpg 0_1394.jpg 0_42.jpg 0_816.jpg 1_1201.jpg 1_238.jpg 1_624.jpg 0_1008.jpg 0_1395.jpg 0_430.jpg 0_817.jpg 1_1202.jpg 1_239.jpg 1_625.jpg 0_1009.jpg 0_1396.jpg 0_431.jpg 0_818.jpg 1_1203.jpg 1_23.jpg 1_626.jpg 0_100.jpg 0_1397.jpg 0_432.jpg 0_819.jpg 1_1204.jpg 1_240.jpg 1_627.jpg 0_1010.jpg 0_1398.jpg 0_433.jpg 0_81.jpg 1_1205.jpg 1_241.jpg 1_628.jpg 0_1011.jpg 0_1399.jpg 0_434.jpg 0_820.jpg 1_1206.jpg 1_242.jpg 1_629.jpg 0_1012.jpg 0_139.jpg 0_435.jpg 0_821.jpg 1_1207.jpg 1_243.jpg 1_62.jpg 0_1013.jpg 0_13.jpg 0_436.jpg 0_822.jpg 1_1208.jpg 1_244.jpg 1_630.jpg 0_1014.jpg 0_1400.jpg 0_437.jpg 0_823.jpg 1_1209.jpg 1_245.jpg 1_631.jpg 0_1015.jpg 0_1401.jpg 0_438.jpg 0_824.jpg 1_120.jpg 1_246.jpg 1_632.jpg 0_1016.jpg 0_1402.jpg 0_439.jpg 0_825.jpg 1_1210.jpg 1_247.jpg 1_633.jpg 0_1017.jpg 0_1403.jpg 0_43.jpg 0_826.jpg 1_1211.jpg 1_248.jpg 1_634.jpg 0_1018.jpg 0_1404.jpg 0_440.jpg 0_827.jpg 1_1212.jpg 1_249.jpg 1_635.jpg 0_1019.jpg 0_1405.jpg 0_441.jpg 0_828.jpg 1_1213.jpg 1_24.jpg 1_636.jpg 0_101.jpg 0_1406.jpg 0_442.jpg 0_829.jpg 1_1214.jpg 1_250.jpg 1_637.jpg 0_1020.jpg 0_1407.jpg 0_443.jpg 0_82.jpg 1_1215.jpg 1_251.jpg 1_638.jpg 0_1021.jpg 0_1408.jpg 0_444.jpg 0_830.jpg 1_1216.jpg 1_252.jpg 1_639.jpg 0_1022.jpg 0_1409.jpg 0_445.jpg 0_831.jpg 1_1217.jpg 1_253.jpg 1_63.jpg 0_1023.jpg 0_140.jpg 0_446.jpg 0_832.jpg 1_1218.jpg 1_254.jpg 1_640.jpg 0_1024.jpg 0_1410.jpg 0_447.jpg 0_833.jpg 1_1219.jpg 1_255.jpg 1_641.jpg 0_1025.jpg 0_1411.jpg 0_448.jpg 0_834.jpg 1_121.jpg 1_256.jpg 1_642.jpg 0_1026.jpg 0_1412.jpg 0_449.jpg 0_835.jpg 1_1220.jpg 1_257.jpg 1_643.jpg 0_1027.jpg 0_1413.jpg 0_44.jpg 0_836.jpg 1_1221.jpg 1_258.jpg 1_644.jpg 0_1028.jpg 0_1414.jpg 0_450.jpg 0_837.jpg 1_1222.jpg 1_259.jpg 1_645.jpg 0_1029.jpg 0_1415.jpg 0_451.jpg 0_838.jpg 1_1223.jpg 1_25.jpg 1_646.jpg 0_102.jpg 0_1416.jpg 0_452.jpg 0_839.jpg 1_1224.jpg 1_260.jpg 1_647.jpg 0_1030.jpg 0_1417.jpg 0_453.jpg 0_83.jpg 1_1225.jpg 1_261.jpg 1_648.jpg 0_1031.jpg 0_1418.jpg 0_454.jpg 0_840.jpg 1_1226.jpg 1_262.jpg 1_649.jpg 0_1032.jpg 0_1419.jpg 0_455.jpg 0_841.jpg 1_1227.jpg 1_263.jpg 1_64.jpg 0_1033.jpg 0_141.jpg 0_456.jpg 0_842.jpg 1_1228.jpg 1_264.jpg 1_650.jpg 0_1034.jpg 0_1420.jpg 0_457.jpg 0_843.jpg 1_1229.jpg 1_265.jpg 1_651.jpg 0_1035.jpg 0_1421.jpg 0_458.jpg 0_844.jpg 1_122.jpg 1_266.jpg 1_652.jpg 0_1036.jpg 0_1422.jpg 0_459.jpg 0_845.jpg 1_1230.jpg 1_267.jpg 1_653.jpg 0_1037.jpg 0_1423.jpg 0_45.jpg 0_846.jpg 1_1231.jpg 1_268.jpg 1_654.jpg 0_1038.jpg 0_1424.jpg 0_460.jpg 0_847.jpg 1_1232.jpg 1_269.jpg 1_655.jpg 0_1039.jpg 0_1425.jpg 0_461.jpg 0_848.jpg 1_1233.jpg 1_26.jpg 1_656.jpg 0_103.jpg 0_1426.jpg 0_462.jpg 0_849.jpg 1_1234.jpg 1_270.jpg 1_657.jpg 0_1040.jpg 0_1427.jpg 0_463.jpg 0_84.jpg 1_1235.jpg 1_271.jpg 1_658.jpg 0_1041.jpg 0_1428.jpg 0_464.jpg 0_850.jpg 1_1236.jpg 1_272.jpg 1_659.jpg 0_1042.jpg 0_1429.jpg 0_465.jpg 0_851.jpg 1_1237.jpg 1_273.jpg 1_65.jpg 0_1043.jpg 0_142.jpg 0_466.jpg 0_852.jpg 1_1238.jpg 1_274.jpg 1_660.jpg 0_1044.jpg 0_1430.jpg 0_467.jpg 0_853.jpg 1_1239.jpg 1_275.jpg 1_661.jpg 0_1045.jpg 0_1431.jpg 0_468.jpg 0_854.jpg 1_123.jpg 1_276.jpg 1_662.jpg 0_1046.jpg 0_1432.jpg 0_469.jpg 0_855.jpg 1_1240.jpg 1_277.jpg 1_663.jpg 0_1047.jpg 0_1433.jpg 0_46.jpg 0_856.jpg 1_1241.jpg 1_278.jpg 1_664.jpg 0_1048.jpg 0_1434.jpg 0_470.jpg 0_857.jpg 1_1242.jpg 1_279.jpg 1_665.jpg 0_1049.jpg 0_1435.jpg 0_471.jpg 0_858.jpg 1_1243.jpg 1_27.jpg 1_666.jpg 0_104.jpg 0_1436.jpg 0_472.jpg 0_859.jpg 1_1244.jpg 1_280.jpg 1_667.jpg 0_1050.jpg 0_1437.jpg 0_473.jpg 0_85.jpg 1_1245.jpg 1_281.jpg 1_668.jpg 0_1051.jpg 0_1438.jpg 0_474.jpg 0_860.jpg 1_1246.jpg 1_282.jpg 1_669.jpg 0_1052.jpg 0_1439.jpg 0_475.jpg 0_861.jpg 1_1247.jpg 1_283.jpg 1_66.jpg 0_1053.jpg 0_143.jpg 0_476.jpg 0_862.jpg 1_1248.jpg 1_284.jpg 1_670.jpg 0_1054.jpg 0_1440.jpg 0_477.jpg 0_863.jpg 1_1249.jpg 1_285.jpg 1_671.jpg 0_1055.jpg 0_1441.jpg 0_478.jpg 0_864.jpg 1_124.jpg 1_286.jpg 1_672.jpg 0_1056.jpg 0_1442.jpg 0_479.jpg 0_865.jpg 1_1250.jpg 1_287.jpg 1_673.jpg 0_1057.jpg 0_1443.jpg 0_47.jpg 0_866.jpg 1_1251.jpg 1_288.jpg 1_674.jpg 0_1058.jpg 0_1444.jpg 0_480.jpg 0_867.jpg 1_1252.jpg 1_289.jpg 1_675.jpg 0_1059.jpg 0_1445.jpg 0_481.jpg 0_868.jpg 1_1253.jpg 1_28.jpg 1_676.jpg 0_105.jpg 0_1446.jpg 0_482.jpg 0_869.jpg 1_1254.jpg 1_290.jpg 1_677.jpg 0_1060.jpg 0_1447.jpg 0_483.jpg 0_86.jpg 1_1255.jpg 1_291.jpg 1_678.jpg 0_1061.jpg 0_1448.jpg 0_484.jpg 0_870.jpg 1_1256.jpg 1_292.jpg 1_679.jpg 0_1062.jpg 0_1449.jpg 0_485.jpg 0_871.jpg 1_1257.jpg 1_293.jpg 1_67.jpg 0_1063.jpg 0_144.jpg 0_486.jpg 0_872.jpg 1_1258.jpg 1_294.jpg 1_680.jpg 0_1064.jpg 0_1450.jpg 0_487.jpg 0_873.jpg 1_1259.jpg 1_295.jpg 1_681.jpg 0_1065.jpg 0_1451.jpg 0_488.jpg 0_874.jpg 1_125.jpg 1_296.jpg 1_682.jpg 0_1066.jpg 0_1452.jpg 0_489.jpg 0_875.jpg 1_1260.jpg 1_297.jpg 1_683.jpg 0_1067.jpg 0_1453.jpg 0_48.jpg 0_876.jpg 1_1261.jpg 1_298.jpg 1_684.jpg 0_1068.jpg 0_1454.jpg 0_490.jpg 0_877.jpg 1_1262.jpg 1_299.jpg 1_685.jpg 0_1069.jpg 0_1455.jpg 0_491.jpg 0_878.jpg 1_1263.jpg 1_29.jpg 1_686.jpg 0_106.jpg 0_1456.jpg 0_492.jpg 0_879.jpg 1_1264.jpg 1_2.jpg 1_687.jpg 0_1070.jpg 0_1457.jpg 0_493.jpg 0_87.jpg 1_1265.jpg 1_300.jpg 1_688.jpg 0_1071.jpg 0_1458.jpg 0_494.jpg 0_880.jpg 1_1266.jpg 1_301.jpg 1_689.jpg 0_1072.jpg 0_1459.jpg 0_495.jpg 0_881.jpg 1_1267.jpg 1_302.jpg 1_68.jpg 0_1073.jpg 0_145.jpg 0_496.jpg 0_882.jpg 1_1268.jpg 1_303.jpg 1_690.jpg 0_1074.jpg 0_1460.jpg 0_497.jpg 0_883.jpg 1_1269.jpg 1_304.jpg 1_691.jpg 0_1075.jpg 0_1461.jpg 0_498.jpg 0_884.jpg 1_126.jpg 1_305.jpg 1_692.jpg 0_1076.jpg 0_1462.jpg 0_499.jpg 0_885.jpg 1_1270.jpg 1_306.jpg 1_693.jpg 0_1077.jpg 0_1463.jpg 0_49.jpg 0_886.jpg 1_1271.jpg 1_307.jpg 1_694.jpg 0_1078.jpg 0_1464.jpg 0_4.jpg 0_887.jpg 1_1272.jpg 1_308.jpg 1_695.jpg 0_1079.jpg 0_1465.jpg 0_500.jpg 0_888.jpg 1_1273.jpg 1_309.jpg 1_696.jpg 0_107.jpg 0_1466.jpg 0_501.jpg 0_889.jpg 1_1274.jpg 1_30.jpg 1_697.jpg 0_1080.jpg 0_1467.jpg 0_502.jpg 0_88.jpg 1_1275.jpg 1_310.jpg 1_698.jpg 0_1081.jpg 0_1468.jpg 0_503.jpg 0_890.jpg 1_1276.jpg 1_311.jpg 1_699.jpg 0_1082.jpg 0_1469.jpg 0_504.jpg 0_891.jpg 1_1277.jpg 1_312.jpg 1_69.jpg 0_1083.jpg 0_146.jpg 0_505.jpg 0_892.jpg 1_1278.jpg 1_313.jpg 1_6.jpg 0_1084.jpg 0_1470.jpg 0_506.jpg 0_893.jpg 1_1279.jpg 1_314.jpg 1_700.jpg 0_1085.jpg 0_1471.jpg 0_507.jpg 0_894.jpg 1_127.jpg 1_315.jpg 1_701.jpg 0_1086.jpg 0_1472.jpg 0_508.jpg 0_895.jpg 1_1280.jpg 1_316.jpg 1_702.jpg 0_1087.jpg 0_1473.jpg 0_509.jpg 0_896.jpg 1_1281.jpg 1_317.jpg 1_703.jpg 0_1088.jpg 0_1474.jpg 0_50.jpg 0_897.jpg 1_1282.jpg 1_318.jpg 1_704.jpg 0_1089.jpg 0_1475.jpg 0_510.jpg 0_898.jpg 1_1283.jpg 1_319.jpg 1_705.jpg 0_108.jpg 0_1476.jpg 0_511.jpg 0_899.jpg 1_1284.jpg 1_31.jpg 1_706.jpg 0_1090.jpg 0_1477.jpg 0_512.jpg 0_89.jpg 1_1285.jpg 1_320.jpg 1_707.jpg 0_1091.jpg 0_1478.jpg 0_513.jpg 0_8.jpg 1_1286.jpg 1_321.jpg 1_708.jpg 0_1092.jpg 0_1479.jpg 0_514.jpg 0_900.jpg 1_1287.jpg 1_322.jpg 1_709.jpg 0_1093.jpg 0_147.jpg 0_515.jpg 0_901.jpg 1_1288.jpg 1_323.jpg 1_70.jpg 0_1094.jpg 0_1480.jpg 0_516.jpg 0_902.jpg 1_1289.jpg 1_324.jpg 1_710.jpg 0_1095.jpg 0_1481.jpg 0_517.jpg 0_903.jpg 1_128.jpg 1_325.jpg 1_711.jpg 0_1096.jpg 0_1482.jpg 0_518.jpg 0_904.jpg 1_1290.jpg 1_326.jpg 1_712.jpg 0_1097.jpg 0_1483.jpg 0_519.jpg 0_905.jpg 1_1291.jpg 1_327.jpg 1_713.jpg 0_1098.jpg 0_1484.jpg 0_51.jpg 0_906.jpg 1_1292.jpg 1_328.jpg 1_714.jpg 0_1099.jpg 0_1485.jpg 0_520.jpg 0_907.jpg 1_1293.jpg 1_329.jpg 1_715.jpg 0_109.jpg 0_1486.jpg 0_521.jpg 0_908.jpg 1_1294.jpg 1_32.jpg 1_716.jpg 0_10.jpg 0_1487.jpg 0_522.jpg 0_909.jpg 1_1295.jpg 1_330.jpg 1_717.jpg 0_1100.jpg 0_1488.jpg 0_523.jpg 0_90.jpg 1_1296.jpg 1_331.jpg 1_718.jpg 0_1101.jpg 0_1489.jpg 0_524.jpg 0_910.jpg 1_1297.jpg 1_332.jpg 1_719.jpg 0_1102.jpg 0_148.jpg 0_525.jpg 0_911.jpg 1_1298.jpg 1_333.jpg 1_71.jpg 0_1103.jpg 0_1490.jpg 0_526.jpg 0_912.jpg 1_1299.jpg 1_334.jpg 1_720.jpg 0_1104.jpg 0_1491.jpg 0_527.jpg 0_913.jpg 1_129.jpg 1_335.jpg 1_721.jpg 0_1105.jpg 0_1492.jpg 0_528.jpg 0_914.jpg 1_12.jpg 1_336.jpg 1_722.jpg 0_1106.jpg 0_1493.jpg 0_529.jpg 0_915.jpg 1_1300.jpg 1_337.jpg 1_723.jpg 0_1107.jpg 0_1494.jpg 0_52.jpg 0_916.jpg 1_1301.jpg 1_338.jpg 1_724.jpg 0_1108.jpg 0_1495.jpg 0_530.jpg 0_917.jpg 1_1302.jpg 1_339.jpg 1_725.jpg 0_1109.jpg 0_1496.jpg 0_531.jpg 0_918.jpg 1_1303.jpg 1_33.jpg 1_726.jpg 0_110.jpg 0_1497.jpg 0_532.jpg 0_919.jpg 1_1304.jpg 1_340.jpg 1_727.jpg 0_1110.jpg 0_1498.jpg 0_533.jpg 0_91.jpg 1_1305.jpg 1_341.jpg 1_728.jpg 0_1111.jpg 0_1499.jpg 0_534.jpg 0_920.jpg 1_1306.jpg 1_342.jpg 1_729.jpg 0_1112.jpg 0_149.jpg 0_535.jpg 0_921.jpg 1_1307.jpg 1_343.jpg 1_72.jpg 0_1113.jpg 0_14.jpg 0_536.jpg 0_922.jpg 1_1308.jpg 1_344.jpg 1_730.jpg 0_1114.jpg 0_150.jpg 0_537.jpg 0_923.jpg 1_1309.jpg 1_345.jpg 1_731.jpg 0_1115.jpg 0_151.jpg 0_538.jpg 0_924.jpg 1_130.jpg 1_346.jpg 1_732.jpg 0_1116.jpg 0_152.jpg 0_539.jpg 0_925.jpg 1_1310.jpg 1_347.jpg 1_733.jpg 0_1117.jpg 0_153.jpg 0_53.jpg 0_926.jpg 1_1311.jpg 1_348.jpg 1_734.jpg 0_1118.jpg 0_154.jpg 0_540.jpg 0_927.jpg 1_1312.jpg 1_349.jpg 1_735.jpg 0_1119.jpg 0_155.jpg 0_541.jpg 0_928.jpg 1_1313.jpg 1_34.jpg 1_736.jpg 0_111.jpg 0_156.jpg 0_542.jpg 0_929.jpg 1_1314.jpg 1_350.jpg 1_737.jpg 0_1120.jpg 0_157.jpg 0_543.jpg 0_92.jpg 1_1315.jpg 1_351.jpg 1_738.jpg 0_1121.jpg 0_158.jpg 0_544.jpg 0_930.jpg 1_1316.jpg 1_352.jpg 1_739.jpg 0_1122.jpg 0_159.jpg 0_545.jpg 0_931.jpg 1_1317.jpg 1_353.jpg 1_73.jpg 0_1123.jpg 0_15.jpg 0_546.jpg 0_932.jpg 1_1318.jpg 1_354.jpg 1_740.jpg 0_1124.jpg 0_160.jpg 0_547.jpg 0_933.jpg 1_1319.jpg 1_355.jpg 1_741.jpg 0_1125.jpg 0_161.jpg 0_548.jpg 0_934.jpg 1_131.jpg 1_356.jpg 1_742.jpg 0_1126.jpg 0_162.jpg 0_549.jpg 0_935.jpg 1_1320.jpg 1_357.jpg 1_743.jpg 0_1127.jpg 0_163.jpg 0_54.jpg 0_936.jpg 1_1321.jpg 1_358.jpg 1_744.jpg 0_1128.jpg 0_164.jpg 0_550.jpg 0_937.jpg 1_1322.jpg 1_359.jpg 1_745.jpg 0_1129.jpg 0_165.jpg 0_551.jpg 0_938.jpg 1_1323.jpg 1_35.jpg 1_746.jpg 0_112.jpg 0_166.jpg 0_552.jpg 0_939.jpg 1_1324.jpg 1_360.jpg 1_747.jpg 0_1130.jpg 0_167.jpg 0_553.jpg 0_93.jpg 1_1325.jpg 1_361.jpg 1_748.jpg 0_1131.jpg 0_168.jpg 0_554.jpg 0_940.jpg 1_1326.jpg 1_362.jpg 1_749.jpg 0_1132.jpg 0_169.jpg 0_555.jpg 0_941.jpg 1_1327.jpg 1_363.jpg 1_74.jpg 0_1133.jpg 0_16.jpg 0_556.jpg 0_942.jpg 1_1328.jpg 1_364.jpg 1_750.jpg 0_1134.jpg 0_170.jpg 0_557.jpg 0_943.jpg 1_1329.jpg 1_365.jpg 1_751.jpg 0_1135.jpg 0_171.jpg 0_558.jpg 0_944.jpg 1_132.jpg 1_366.jpg 1_752.jpg 0_1136.jpg 0_172.jpg 0_559.jpg 0_945.jpg 1_1330.jpg 1_367.jpg 1_753.jpg 0_1137.jpg 0_173.jpg 0_55.jpg 0_946.jpg 1_1331.jpg 1_368.jpg 1_754.jpg 0_1138.jpg 0_174.jpg 0_560.jpg 0_947.jpg 1_1332.jpg 1_369.jpg 1_755.jpg 0_1139.jpg 0_175.jpg 0_561.jpg 0_948.jpg 1_1333.jpg 1_36.jpg 1_756.jpg 0_113.jpg 0_176.jpg 0_562.jpg 0_949.jpg 1_1334.jpg 1_370.jpg 1_757.jpg 0_1140.jpg 0_177.jpg 0_563.jpg 0_94.jpg 1_1335.jpg 1_371.jpg 1_758.jpg 0_1141.jpg 0_178.jpg 0_564.jpg 0_950.jpg 1_1336.jpg 1_372.jpg 1_759.jpg 0_1142.jpg 0_179.jpg 0_565.jpg 0_951.jpg 1_1337.jpg 1_373.jpg 1_75.jpg 0_1143.jpg 0_17.jpg 0_566.jpg 0_952.jpg 1_1338.jpg 1_374.jpg 1_760.jpg 0_1144.jpg 0_180.jpg 0_567.jpg 0_953.jpg 1_1339.jpg 1_375.jpg 1_761.jpg 0_1145.jpg 0_181.jpg 0_568.jpg 0_954.jpg 1_133.jpg 1_376.jpg 1_762.jpg 0_1146.jpg 0_182.jpg 0_569.jpg 0_955.jpg 1_1340.jpg 1_377.jpg 1_763.jpg 0_1147.jpg 0_183.jpg 0_56.jpg 0_956.jpg 1_1341.jpg 1_378.jpg 1_764.jpg 0_1148.jpg 0_184.jpg 0_570.jpg 0_957.jpg 1_1342.jpg 1_379.jpg 1_765.jpg 0_1149.jpg 0_185.jpg 0_571.jpg 0_958.jpg 1_1343.jpg 1_37.jpg 1_766.jpg 0_114.jpg 0_186.jpg 0_572.jpg 0_959.jpg 1_1344.jpg 1_380.jpg 1_767.jpg 0_1150.jpg 0_187.jpg 0_573.jpg 0_95.jpg 1_1345.jpg 1_381.jpg 1_768.jpg 0_1151.jpg 0_188.jpg 0_574.jpg 0_960.jpg 1_1346.jpg 1_382.jpg 1_769.jpg 0_1152.jpg 0_189.jpg 0_575.jpg 0_961.jpg 1_1347.jpg 1_383.jpg 1_76.jpg 0_1153.jpg 0_18.jpg 0_576.jpg 0_962.jpg 1_1348.jpg 1_384.jpg 1_770.jpg 0_1154.jpg 0_190.jpg 0_577.jpg 0_963.jpg 1_1349.jpg 1_385.jpg 1_771.jpg 0_1155.jpg 0_191.jpg 0_578.jpg 0_964.jpg 1_134.jpg 1_386.jpg 1_772.jpg 0_1156.jpg 0_192.jpg 0_579.jpg 0_965.jpg 1_1350.jpg 1_387.jpg 1_773.jpg 0_1157.jpg 0_193.jpg 0_57.jpg 0_966.jpg 1_1351.jpg 1_388.jpg 1_774.jpg 0_1158.jpg 0_194.jpg 0_580.jpg 0_967.jpg 1_1352.jpg 1_389.jpg 1_775.jpg 0_1159.jpg 0_195.jpg 0_581.jpg 0_968.jpg 1_1353.jpg 1_38.jpg 1_776.jpg 0_115.jpg 0_196.jpg 0_582.jpg 0_969.jpg 1_1354.jpg 1_390.jpg 1_777.jpg 0_1160.jpg 0_197.jpg 0_583.jpg 0_96.jpg 1_1355.jpg 1_391.jpg 1_778.jpg 0_1161.jpg 0_198.jpg 0_584.jpg 0_970.jpg 1_1356.jpg 1_392.jpg 1_779.jpg 0_1162.jpg 0_199.jpg 0_585.jpg 0_971.jpg 1_1357.jpg 1_393.jpg 1_77.jpg 0_1163.jpg 0_19.jpg 0_586.jpg 0_972.jpg 1_1358.jpg 1_394.jpg 1_780.jpg 0_1164.jpg 0_1.jpg 0_587.jpg 0_973.jpg 1_1359.jpg 1_395.jpg 1_781.jpg 0_1165.jpg 0_200.jpg 0_588.jpg 0_974.jpg 1_135.jpg 1_396.jpg 1_782.jpg 0_1166.jpg 0_201.jpg 0_589.jpg 0_975.jpg 1_1360.jpg 1_397.jpg 1_783.jpg 0_1167.jpg 0_202.jpg 0_58.jpg 0_976.jpg 1_1361.jpg 1_398.jpg 1_784.jpg 0_1168.jpg 0_203.jpg 0_590.jpg 0_977.jpg 1_1362.jpg 1_399.jpg 1_785.jpg 0_1169.jpg 0_204.jpg 0_591.jpg 0_978.jpg 1_1363.jpg 1_39.jpg 1_786.jpg 0_116.jpg 0_205.jpg 0_592.jpg 0_979.jpg 1_1364.jpg 1_3.jpg 1_787.jpg 0_1170.jpg 0_206.jpg 0_593.jpg 0_97.jpg 1_1365.jpg 1_400.jpg 1_788.jpg 0_1171.jpg 0_207.jpg 0_594.jpg 0_980.jpg 1_1366.jpg 1_401.jpg 1_789.jpg 0_1172.jpg 0_208.jpg 0_595.jpg 0_981.jpg 1_1367.jpg 1_402.jpg 1_78.jpg 0_1173.jpg 0_209.jpg 0_596.jpg 0_982.jpg 1_1368.jpg 1_403.jpg 1_790.jpg 0_1174.jpg 0_20.jpg 0_597.jpg 0_983.jpg 1_1369.jpg 1_404.jpg 1_791.jpg 0_1175.jpg 0_210.jpg 0_598.jpg 0_984.jpg 1_136.jpg 1_405.jpg 1_792.jpg 0_1176.jpg 0_211.jpg 0_599.jpg 0_985.jpg 1_1370.jpg 1_406.jpg 1_793.jpg 0_1177.jpg 0_212.jpg 0_59.jpg 0_986.jpg 1_1371.jpg 1_407.jpg 1_794.jpg 0_1178.jpg 0_213.jpg 0_5.jpg 0_987.jpg 1_1372.jpg 1_408.jpg 1_795.jpg 0_1179.jpg 0_214.jpg 0_600.jpg 0_988.jpg 1_1373.jpg 1_409.jpg 1_796.jpg 0_117.jpg 0_215.jpg 0_601.jpg 0_989.jpg 1_1374.jpg 1_40.jpg 1_797.jpg 0_1180.jpg 0_216.jpg 0_602.jpg 0_98.jpg 1_1375.jpg 1_410.jpg 1_798.jpg 0_1181.jpg 0_217.jpg 0_603.jpg 0_990.jpg 1_1376.jpg 1_411.jpg 1_799.jpg 0_1182.jpg 0_218.jpg 0_604.jpg 0_991.jpg 1_1377.jpg 1_412.jpg 1_79.jpg 0_1183.jpg 0_219.jpg 0_605.jpg 0_992.jpg 1_1378.jpg 1_413.jpg 1_7.jpg 0_1184.jpg 0_21.jpg 0_606.jpg 0_993.jpg 1_1379.jpg 1_414.jpg 1_800.jpg 0_1185.jpg 0_220.jpg 0_607.jpg 0_994.jpg 1_137.jpg 1_415.jpg 1_801.jpg 0_1186.jpg 0_221.jpg 0_608.jpg 0_995.jpg 1_1380.jpg 1_416.jpg 1_802.jpg 0_1187.jpg 0_222.jpg 0_609.jpg 0_996.jpg 1_1381.jpg 1_417.jpg 1_803.jpg 0_1188.jpg 0_223.jpg 0_60.jpg 0_997.jpg 1_1382.jpg 1_418.jpg 1_804.jpg 0_1189.jpg 0_224.jpg 0_610.jpg 0_998.jpg 1_1383.jpg 1_419.jpg 1_805.jpg 0_118.jpg 0_225.jpg 0_611.jpg 0_999.jpg 1_1384.jpg 1_41.jpg 1_806.jpg 0_1190.jpg 0_226.jpg 0_612.jpg 0_99.jpg 1_1385.jpg 1_420.jpg 1_807.jpg 0_1191.jpg 0_227.jpg 0_613.jpg 0_9.jpg 1_1386.jpg 1_421.jpg 1_808.jpg 0_1192.jpg 0_228.jpg 0_614.jpg 1_0.jpg 1_1387.jpg 1_422.jpg 1_809.jpg 0_1193.jpg 0_229.jpg 0_615.jpg 1_1000.jpg 1_1388.jpg 1_423.jpg 1_80.jpg 0_1194.jpg 0_22.jpg 0_616.jpg 1_1001.jpg 1_1389.jpg 1_424.jpg 1_810.jpg 0_1195.jpg 0_230.jpg 0_617.jpg 1_1002.jpg 1_138.jpg 1_425.jpg 1_811.jpg 0_1196.jpg 0_231.jpg 0_618.jpg 1_1003.jpg 1_1390.jpg 1_426.jpg 1_812.jpg 0_1197.jpg 0_232.jpg 0_619.jpg 1_1004.jpg 1_1391.jpg 1_427.jpg 1_813.jpg 0_1198.jpg 0_233.jpg 0_61.jpg 1_1005.jpg 1_1392.jpg 1_428.jpg 1_814.jpg 0_1199.jpg 0_234.jpg 0_620.jpg 1_1006.jpg 1_1393.jpg 1_429.jpg 1_815.jpg 0_119.jpg 0_235.jpg 0_621.jpg 1_1007.jpg 1_1394.jpg 1_42.jpg 1_816.jpg 0_11.jpg 0_236.jpg 0_622.jpg 1_1008.jpg 1_1395.jpg 1_430.jpg 1_817.jpg 0_1200.jpg 0_237.jpg 0_623.jpg 1_1009.jpg 1_1396.jpg 1_431.jpg 1_818.jpg 0_1201.jpg 0_238.jpg 0_624.jpg 1_100.jpg 1_1397.jpg 1_432.jpg 1_819.jpg 0_1202.jpg 0_239.jpg 0_625.jpg 1_1010.jpg 1_1398.jpg 1_433.jpg 1_81.jpg 0_1203.jpg 0_23.jpg 0_626.jpg 1_1011.jpg 1_1399.jpg 1_434.jpg 1_820.jpg 0_1204.jpg 0_240.jpg 0_627.jpg 1_1012.jpg 1_139.jpg 1_435.jpg 1_821.jpg 0_1205.jpg 0_241.jpg 0_628.jpg 1_1013.jpg 1_13.jpg 1_436.jpg 1_822.jpg 0_1206.jpg 0_242.jpg 0_629.jpg 1_1014.jpg 1_1400.jpg 1_437.jpg 1_823.jpg 0_1207.jpg 0_243.jpg 0_62.jpg 1_1015.jpg 1_1401.jpg 1_438.jpg 1_824.jpg 0_1208.jpg 0_244.jpg 0_630.jpg 1_1016.jpg 1_1402.jpg 1_439.jpg 1_825.jpg 0_1209.jpg 0_245.jpg 0_631.jpg 1_1017.jpg 1_1403.jpg 1_43.jpg 1_826.jpg 0_120.jpg 0_246.jpg 0_632.jpg 1_1018.jpg 1_1404.jpg 1_440.jpg 1_827.jpg 0_1210.jpg 0_247.jpg 0_633.jpg 1_1019.jpg 1_1405.jpg 1_441.jpg 1_828.jpg 0_1211.jpg 0_248.jpg 0_634.jpg 1_101.jpg 1_1406.jpg 1_442.jpg 1_829.jpg 0_1212.jpg 0_249.jpg 0_635.jpg 1_1020.jpg 1_1407.jpg 1_443.jpg 1_82.jpg 0_1213.jpg 0_24.jpg 0_636.jpg 1_1021.jpg 1_1408.jpg 1_444.jpg 1_830.jpg 0_1214.jpg 0_250.jpg 0_637.jpg 1_1022.jpg 1_1409.jpg 1_445.jpg 1_831.jpg 0_1215.jpg 0_251.jpg 0_638.jpg 1_1023.jpg 1_140.jpg 1_446.jpg 1_832.jpg 0_1216.jpg 0_252.jpg 0_639.jpg 1_1024.jpg 1_1410.jpg 1_447.jpg 1_833.jpg 0_1217.jpg 0_253.jpg 0_63.jpg 1_1025.jpg 1_1411.jpg 1_448.jpg 1_834.jpg 0_1218.jpg 0_254.jpg 0_640.jpg 1_1026.jpg 1_1412.jpg 1_449.jpg 1_835.jpg 0_1219.jpg 0_255.jpg 0_641.jpg 1_1027.jpg 1_1413.jpg 1_44.jpg 1_836.jpg 0_121.jpg 0_256.jpg 0_642.jpg 1_1028.jpg 1_1414.jpg 1_450.jpg 1_837.jpg 0_1220.jpg 0_257.jpg 0_643.jpg 1_1029.jpg 1_1415.jpg 1_451.jpg 1_838.jpg 0_1221.jpg 0_258.jpg 0_644.jpg 1_102.jpg 1_1416.jpg 1_452.jpg 1_839.jpg 0_1222.jpg 0_259.jpg 0_645.jpg 1_1030.jpg 1_1417.jpg 1_453.jpg 1_83.jpg 0_1223.jpg 0_25.jpg 0_646.jpg 1_1031.jpg 1_1418.jpg 1_454.jpg 1_840.jpg 0_1224.jpg 0_260.jpg 0_647.jpg 1_1032.jpg 1_1419.jpg 1_455.jpg 1_841.jpg 0_1225.jpg 0_261.jpg 0_648.jpg 1_1033.jpg 1_141.jpg 1_456.jpg 1_842.jpg 0_1226.jpg 0_262.jpg 0_649.jpg 1_1034.jpg 1_1420.jpg 1_457.jpg 1_843.jpg 0_1227.jpg 0_263.jpg 0_64.jpg 1_1035.jpg 1_1421.jpg 1_458.jpg 1_844.jpg 0_1228.jpg 0_264.jpg 0_650.jpg 1_1036.jpg 1_1422.jpg 1_459.jpg 1_845.jpg 0_1229.jpg 0_265.jpg 0_651.jpg 1_1037.jpg 1_1423.jpg 1_45.jpg 1_846.jpg 0_122.jpg 0_266.jpg 0_652.jpg 1_1038.jpg 1_1424.jpg 1_460.jpg 1_847.jpg 0_1230.jpg 0_267.jpg 0_653.jpg 1_1039.jpg 1_1425.jpg 1_461.jpg 1_848.jpg 0_1231.jpg 0_268.jpg 0_654.jpg 1_103.jpg 1_1426.jpg 1_462.jpg 1_849.jpg 0_1232.jpg 0_269.jpg 0_655.jpg 1_1040.jpg 1_1427.jpg 1_463.jpg 1_84.jpg 0_1233.jpg 0_26.jpg 0_656.jpg 1_1041.jpg 1_1428.jpg 1_464.jpg 1_850.jpg 0_1234.jpg 0_270.jpg 0_657.jpg 1_1042.jpg 1_1429.jpg 1_465.jpg 1_851.jpg 0_1235.jpg 0_271.jpg 0_658.jpg 1_1043.jpg 1_142.jpg 1_466.jpg 1_852.jpg 0_1236.jpg 0_272.jpg 0_659.jpg 1_1044.jpg 1_1430.jpg 1_467.jpg 1_853.jpg 0_1237.jpg 0_273.jpg 0_65.jpg 1_1045.jpg 1_1431.jpg 1_468.jpg 1_854.jpg 0_1238.jpg 0_274.jpg 0_660.jpg 1_1046.jpg 1_1432.jpg 1_469.jpg 1_855.jpg 0_1239.jpg 0_275.jpg 0_661.jpg 1_1047.jpg 1_1433.jpg 1_46.jpg 1_856.jpg 0_123.jpg 0_276.jpg 0_662.jpg 1_1048.jpg 1_1434.jpg 1_470.jpg 1_857.jpg 0_1240.jpg 0_277.jpg 0_663.jpg 1_1049.jpg 1_1435.jpg 1_471.jpg 1_858.jpg 0_1241.jpg 0_278.jpg 0_664.jpg 1_104.jpg 1_1436.jpg 1_472.jpg 1_859.jpg 0_1242.jpg 0_279.jpg 0_665.jpg 1_1050.jpg 1_1437.jpg 1_473.jpg 1_85.jpg 0_1243.jpg 0_27.jpg 0_666.jpg 1_1051.jpg 1_1438.jpg 1_474.jpg 1_860.jpg 0_1244.jpg 0_280.jpg 0_667.jpg 1_1052.jpg 1_1439.jpg 1_475.jpg 1_861.jpg 0_1245.jpg 0_281.jpg 0_668.jpg 1_1053.jpg 1_143.jpg 1_476.jpg 1_862.jpg 0_1246.jpg 0_282.jpg 0_669.jpg 1_1054.jpg 1_1440.jpg 1_477.jpg 1_863.jpg 0_1247.jpg 0_283.jpg 0_66.jpg 1_1055.jpg 1_1441.jpg 1_478.jpg 1_864.jpg 0_1248.jpg 0_284.jpg 0_670.jpg 1_1056.jpg 1_1442.jpg 1_479.jpg 1_865.jpg 0_1249.jpg 0_285.jpg 0_671.jpg 1_1057.jpg 1_1443.jpg 1_47.jpg 1_866.jpg 0_124.jpg 0_286.jpg 0_672.jpg 1_1058.jpg 1_1444.jpg 1_480.jpg 1_867.jpg 0_1250.jpg 0_287.jpg 0_673.jpg 1_1059.jpg 1_1445.jpg 1_481.jpg 1_868.jpg 0_1251.jpg 0_288.jpg 0_674.jpg 1_105.jpg 1_1446.jpg 1_482.jpg 1_869.jpg 0_1252.jpg 0_289.jpg 0_675.jpg 1_1060.jpg 1_1447.jpg 1_483.jpg 1_86.jpg 0_1253.jpg 0_28.jpg 0_676.jpg 1_1061.jpg 1_1448.jpg 1_484.jpg 1_870.jpg 0_1254.jpg 0_290.jpg 0_677.jpg 1_1062.jpg 1_1449.jpg 1_485.jpg 1_871.jpg 0_1255.jpg 0_291.jpg 0_678.jpg 1_1063.jpg 1_144.jpg 1_486.jpg 1_872.jpg 0_1256.jpg 0_292.jpg 0_679.jpg 1_1064.jpg 1_1450.jpg 1_487.jpg 1_873.jpg 0_1257.jpg 0_293.jpg 0_67.jpg 1_1065.jpg 1_1451.jpg 1_488.jpg 1_874.jpg 0_1258.jpg 0_294.jpg 0_680.jpg 1_1066.jpg 1_1452.jpg 1_489.jpg 1_875.jpg 0_1259.jpg 0_295.jpg 0_681.jpg 1_1067.jpg 1_1453.jpg 1_48.jpg 1_876.jpg 0_125.jpg 0_296.jpg 0_682.jpg 1_1068.jpg 1_1454.jpg 1_490.jpg 1_877.jpg 0_1260.jpg 0_297.jpg 0_683.jpg 1_1069.jpg 1_1455.jpg 1_491.jpg 1_878.jpg 0_1261.jpg 0_298.jpg 0_684.jpg 1_106.jpg 1_1456.jpg 1_492.jpg 1_879.jpg 0_1262.jpg 0_299.jpg 0_685.jpg 1_1070.jpg 1_1457.jpg 1_493.jpg 1_87.jpg 0_1263.jpg 0_29.jpg 0_686.jpg 1_1071.jpg 1_1458.jpg 1_494.jpg 1_880.jpg 0_1264.jpg 0_2.jpg 0_687.jpg 1_1072.jpg 1_1459.jpg 1_495.jpg 1_881.jpg 0_1265.jpg 0_300.jpg 0_688.jpg 1_1073.jpg 1_145.jpg 1_496.jpg 1_882.jpg 0_1266.jpg 0_301.jpg 0_689.jpg 1_1074.jpg 1_1460.jpg 1_497.jpg 1_883.jpg 0_1267.jpg 0_302.jpg 0_68.jpg 1_1075.jpg 1_1461.jpg 1_498.jpg 1_884.jpg 0_1268.jpg 0_303.jpg 0_690.jpg 1_1076.jpg 1_1462.jpg 1_499.jpg 1_885.jpg 0_1269.jpg 0_304.jpg 0_691.jpg 1_1077.jpg 1_1463.jpg 1_49.jpg 1_886.jpg 0_126.jpg 0_305.jpg 0_692.jpg 1_1078.jpg 1_1464.jpg 1_4.jpg 1_887.jpg 0_1270.jpg 0_306.jpg 0_693.jpg 1_1079.jpg 1_1465.jpg 1_500.jpg 1_888.jpg 0_1271.jpg 0_307.jpg 0_694.jpg 1_107.jpg 1_1466.jpg 1_501.jpg 1_889.jpg 0_1272.jpg 0_308.jpg 0_695.jpg 1_1080.jpg 1_1467.jpg 1_502.jpg 1_88.jpg 0_1273.jpg 0_309.jpg 0_696.jpg 1_1081.jpg 1_1468.jpg 1_503.jpg 1_890.jpg 0_1274.jpg 0_30.jpg 0_697.jpg 1_1082.jpg 1_1469.jpg 1_504.jpg 1_891.jpg 0_1275.jpg 0_310.jpg 0_698.jpg 1_1083.jpg 1_146.jpg 1_505.jpg 1_892.jpg 0_1276.jpg 0_311.jpg 0_699.jpg 1_1084.jpg 1_1470.jpg 1_506.jpg 1_893.jpg 0_1277.jpg 0_312.jpg 0_69.jpg 1_1085.jpg 1_1471.jpg 1_507.jpg 1_894.jpg 0_1278.jpg 0_313.jpg 0_6.jpg 1_1086.jpg 1_1472.jpg 1_508.jpg 1_895.jpg 0_1279.jpg 0_314.jpg 0_700.jpg 1_1087.jpg 1_1473.jpg 1_509.jpg 1_896.jpg 0_127.jpg 0_315.jpg 0_701.jpg 1_1088.jpg 1_1474.jpg 1_50.jpg 1_897.jpg 0_1280.jpg 0_316.jpg 0_702.jpg 1_1089.jpg 1_1475.jpg 1_510.jpg 1_898.jpg 0_1281.jpg 0_317.jpg 0_703.jpg 1_108.jpg 1_1476.jpg 1_511.jpg 1_899.jpg 0_1282.jpg 0_318.jpg 0_704.jpg 1_1090.jpg 1_1477.jpg 1_512.jpg 1_89.jpg 0_1283.jpg 0_319.jpg 0_705.jpg 1_1091.jpg 1_1478.jpg 1_513.jpg 1_8.jpg 0_1284.jpg 0_31.jpg 0_706.jpg 1_1092.jpg 1_1479.jpg 1_514.jpg 1_900.jpg 0_1285.jpg 0_320.jpg 0_707.jpg 1_1093.jpg 1_147.jpg 1_515.jpg 1_901.jpg 0_1286.jpg 0_321.jpg 0_708.jpg 1_1094.jpg 1_1480.jpg 1_516.jpg 1_902.jpg 0_1287.jpg 0_322.jpg 0_709.jpg 1_1095.jpg 1_1481.jpg 1_517.jpg 1_903.jpg 0_1288.jpg 0_323.jpg 0_70.jpg 1_1096.jpg 1_1482.jpg 1_518.jpg 1_904.jpg 0_1289.jpg 0_324.jpg 0_710.jpg 1_1097.jpg 1_1483.jpg 1_519.jpg 1_905.jpg 0_128.jpg 0_325.jpg 0_711.jpg 1_1098.jpg 1_1484.jpg 1_51.jpg 1_906.jpg 0_1290.jpg 0_326.jpg 0_712.jpg 1_1099.jpg 1_1485.jpg 1_520.jpg 1_907.jpg 0_1291.jpg 0_327.jpg 0_713.jpg 1_109.jpg 1_1486.jpg 1_521.jpg 1_908.jpg 0_1292.jpg 0_328.jpg 0_714.jpg 1_10.jpg 1_1487.jpg 1_522.jpg 1_909.jpg 0_1293.jpg 0_329.jpg 0_715.jpg 1_1100.jpg 1_1488.jpg 1_523.jpg 1_90.jpg 0_1294.jpg 0_32.jpg 0_716.jpg 1_1101.jpg 1_1489.jpg 1_524.jpg 1_910.jpg 0_1295.jpg 0_330.jpg 0_717.jpg 1_1102.jpg 1_148.jpg 1_525.jpg 1_911.jpg 0_1296.jpg 0_331.jpg 0_718.jpg 1_1103.jpg 1_1490.jpg 1_526.jpg 1_912.jpg 0_1297.jpg 0_332.jpg 0_719.jpg 1_1104.jpg 1_1491.jpg 1_527.jpg 1_913.jpg 0_1298.jpg 0_333.jpg 0_71.jpg 1_1105.jpg 1_1492.jpg 1_528.jpg 1_914.jpg 0_1299.jpg 0_334.jpg 0_720.jpg 1_1106.jpg 1_1493.jpg 1_529.jpg 1_915.jpg 0_129.jpg 0_335.jpg 0_721.jpg 1_1107.jpg 1_1494.jpg 1_52.jpg 1_916.jpg 0_12.jpg 0_336.jpg 0_722.jpg 1_1108.jpg 1_1495.jpg 1_530.jpg 1_917.jpg 0_1300.jpg 0_337.jpg 0_723.jpg 1_1109.jpg 1_1496.jpg 1_531.jpg 1_918.jpg 0_1301.jpg 0_338.jpg 0_724.jpg 1_110.jpg 1_1497.jpg 1_532.jpg 1_919.jpg 0_1302.jpg 0_339.jpg 0_725.jpg 1_1110.jpg 1_1498.jpg 1_533.jpg 1_91.jpg 0_1303.jpg 0_33.jpg 0_726.jpg 1_1111.jpg 1_1499.jpg 1_534.jpg 1_920.jpg 0_1304.jpg 0_340.jpg 0_727.jpg 1_1112.jpg 1_149.jpg 1_535.jpg 1_921.jpg 0_1305.jpg 0_341.jpg 0_728.jpg 1_1113.jpg 1_14.jpg 1_536.jpg 1_922.jpg 0_1306.jpg 0_342.jpg 0_729.jpg 1_1114.jpg 1_150.jpg 1_537.jpg 1_923.jpg 0_1307.jpg 0_343.jpg 0_72.jpg 1_1115.jpg 1_151.jpg 1_538.jpg 1_924.jpg 0_1308.jpg 0_344.jpg 0_730.jpg 1_1116.jpg 1_152.jpg 1_539.jpg 1_925.jpg 0_1309.jpg 0_345.jpg 0_731.jpg 1_1117.jpg 1_153.jpg 1_53.jpg 1_926.jpg 0_130.jpg 0_346.jpg 0_732.jpg 1_1118.jpg 1_154.jpg 1_540.jpg 1_927.jpg 0_1310.jpg 0_347.jpg 0_733.jpg 1_1119.jpg 1_155.jpg 1_541.jpg 1_928.jpg 0_1311.jpg 0_348.jpg 0_734.jpg 1_111.jpg 1_156.jpg 1_542.jpg 1_929.jpg 0_1312.jpg 0_349.jpg 0_735.jpg 1_1120.jpg 1_157.jpg 1_543.jpg 1_92.jpg 0_1313.jpg 0_34.jpg 0_736.jpg 1_1121.jpg 1_158.jpg 1_544.jpg 1_930.jpg 0_1314.jpg 0_350.jpg 0_737.jpg 1_1122.jpg 1_159.jpg 1_545.jpg 1_931.jpg 0_1315.jpg 0_351.jpg 0_738.jpg 1_1123.jpg 1_15.jpg 1_546.jpg 1_932.jpg 0_1316.jpg 0_352.jpg 0_739.jpg 1_1124.jpg 1_160.jpg 1_547.jpg 1_933.jpg 0_1317.jpg 0_353.jpg 0_73.jpg 1_1125.jpg 1_161.jpg 1_548.jpg 1_934.jpg 0_1318.jpg 0_354.jpg 0_740.jpg 1_1126.jpg 1_162.jpg 1_549.jpg 1_935.jpg 0_1319.jpg 0_355.jpg 0_741.jpg 1_1127.jpg 1_163.jpg 1_54.jpg 1_936.jpg 0_131.jpg 0_356.jpg 0_742.jpg 1_1128.jpg 1_164.jpg 1_550.jpg 1_937.jpg 0_1320.jpg 0_357.jpg 0_743.jpg 1_1129.jpg 1_165.jpg 1_551.jpg 1_938.jpg 0_1321.jpg 0_358.jpg 0_744.jpg 1_112.jpg 1_166.jpg 1_552.jpg 1_939.jpg 0_1322.jpg 0_359.jpg 0_745.jpg 1_1130.jpg 1_167.jpg 1_553.jpg 1_93.jpg 0_1323.jpg 0_35.jpg 0_746.jpg 1_1131.jpg 1_168.jpg 1_554.jpg 1_940.jpg 0_1324.jpg 0_360.jpg 0_747.jpg 1_1132.jpg 1_169.jpg 1_555.jpg 1_941.jpg 0_1325.jpg 0_361.jpg 0_748.jpg 1_1133.jpg 1_16.jpg 1_556.jpg 1_942.jpg 0_1326.jpg 0_362.jpg 0_749.jpg 1_1134.jpg 1_170.jpg 1_557.jpg 1_943.jpg 0_1327.jpg 0_363.jpg 0_74.jpg 1_1135.jpg 1_171.jpg 1_558.jpg 1_944.jpg 0_1328.jpg 0_364.jpg 0_750.jpg 1_1136.jpg 1_172.jpg 1_559.jpg 1_945.jpg 0_1329.jpg 0_365.jpg 0_751.jpg 1_1137.jpg 1_173.jpg 1_55.jpg 1_946.jpg 0_132.jpg 0_366.jpg 0_752.jpg 1_1138.jpg 1_174.jpg 1_560.jpg 1_947.jpg 0_1330.jpg 0_367.jpg 0_753.jpg 1_1139.jpg 1_175.jpg 1_561.jpg 1_948.jpg 0_1331.jpg 0_368.jpg 0_754.jpg 1_113.jpg 1_176.jpg 1_562.jpg 1_949.jpg 0_1332.jpg 0_369.jpg 0_755.jpg 1_1140.jpg 1_177.jpg 1_563.jpg 1_94.jpg 0_1333.jpg 0_36.jpg 0_756.jpg 1_1141.jpg 1_178.jpg 1_564.jpg 1_950.jpg 0_1334.jpg 0_370.jpg 0_757.jpg 1_1142.jpg 1_179.jpg 1_565.jpg 1_951.jpg 0_1335.jpg 0_371.jpg 0_758.jpg 1_1143.jpg 1_17.jpg 1_566.jpg 1_952.jpg 0_1336.jpg 0_372.jpg 0_759.jpg 1_1144.jpg 1_180.jpg 1_567.jpg 1_953.jpg 0_1337.jpg 0_373.jpg 0_75.jpg 1_1145.jpg 1_181.jpg 1_568.jpg 1_954.jpg 0_1338.jpg 0_374.jpg 0_760.jpg 1_1146.jpg 1_182.jpg 1_569.jpg 1_955.jpg 0_1339.jpg 0_375.jpg 0_761.jpg 1_1147.jpg 1_183.jpg 1_56.jpg 1_956.jpg 0_133.jpg 0_376.jpg 0_762.jpg 1_1148.jpg 1_184.jpg 1_570.jpg 1_957.jpg 0_1340.jpg 0_377.jpg 0_763.jpg 1_1149.jpg 1_185.jpg 1_571.jpg 1_958.jpg 0_1341.jpg 0_378.jpg 0_764.jpg 1_114.jpg 1_186.jpg 1_572.jpg 1_959.jpg 0_1342.jpg 0_379.jpg 0_765.jpg 1_1150.jpg 1_187.jpg 1_573.jpg 1_95.jpg 0_1343.jpg 0_37.jpg 0_766.jpg 1_1151.jpg 1_188.jpg 1_574.jpg 1_960.jpg 0_1344.jpg 0_380.jpg 0_767.jpg 1_1152.jpg 1_189.jpg 1_575.jpg 1_961.jpg 0_1345.jpg 0_381.jpg 0_768.jpg 1_1153.jpg 1_18.jpg 1_576.jpg 1_962.jpg 0_1346.jpg 0_382.jpg 0_769.jpg 1_1154.jpg 1_190.jpg 1_577.jpg 1_963.jpg 0_1347.jpg 0_383.jpg 0_76.jpg 1_1155.jpg 1_191.jpg 1_578.jpg 1_964.jpg 0_1348.jpg 0_384.jpg 0_770.jpg 1_1156.jpg 1_192.jpg 1_579.jpg 1_965.jpg 0_1349.jpg 0_385.jpg 0_771.jpg 1_1157.jpg 1_193.jpg 1_57.jpg 1_966.jpg 0_134.jpg 0_386.jpg 0_772.jpg 1_1158.jpg 1_194.jpg 1_580.jpg 1_967.jpg 0_1350.jpg 0_387.jpg 0_773.jpg 1_1159.jpg 1_195.jpg 1_581.jpg 1_968.jpg 0_1351.jpg 0_388.jpg 0_774.jpg 1_115.jpg 1_196.jpg 1_582.jpg 1_969.jpg 0_1352.jpg 0_389.jpg 0_775.jpg 1_1160.jpg 1_197.jpg 1_583.jpg 1_96.jpg 0_1353.jpg 0_38.jpg 0_776.jpg 1_1161.jpg 1_198.jpg 1_584.jpg 1_970.jpg 0_1354.jpg 0_390.jpg 0_777.jpg 1_1162.jpg 1_199.jpg 1_585.jpg 1_971.jpg 0_1355.jpg 0_391.jpg 0_778.jpg 1_1163.jpg 1_19.jpg 1_586.jpg 1_972.jpg 0_1356.jpg 0_392.jpg 0_779.jpg 1_1164.jpg 1_1.jpg 1_587.jpg 1_973.jpg 0_1357.jpg 0_393.jpg 0_77.jpg 1_1165.jpg 1_200.jpg 1_588.jpg 1_974.jpg 0_1358.jpg 0_394.jpg 0_780.jpg 1_1166.jpg 1_201.jpg 1_589.jpg 1_975.jpg 0_1359.jpg 0_395.jpg 0_781.jpg 1_1167.jpg 1_202.jpg 1_58.jpg 1_976.jpg 0_135.jpg 0_396.jpg 0_782.jpg 1_1168.jpg 1_203.jpg 1_590.jpg 1_977.jpg 0_1360.jpg 0_397.jpg 0_783.jpg 1_1169.jpg 1_204.jpg 1_591.jpg 1_978.jpg 0_1361.jpg 0_398.jpg 0_784.jpg 1_116.jpg 1_205.jpg 1_592.jpg 1_979.jpg 0_1362.jpg 0_399.jpg 0_785.jpg 1_1170.jpg 1_206.jpg 1_593.jpg 1_97.jpg 0_1363.jpg 0_39.jpg 0_786.jpg 1_1171.jpg 1_207.jpg 1_594.jpg 1_980.jpg 0_1364.jpg 0_3.jpg 0_787.jpg 1_1172.jpg 1_208.jpg 1_595.jpg 1_981.jpg 0_1365.jpg 0_400.jpg 0_788.jpg 1_1173.jpg 1_209.jpg 1_596.jpg 1_982.jpg 0_1366.jpg 0_401.jpg 0_789.jpg 1_1174.jpg 1_20.jpg 1_597.jpg 1_983.jpg 0_1367.jpg 0_402.jpg 0_78.jpg 1_1175.jpg 1_210.jpg 1_598.jpg 1_984.jpg 0_1368.jpg 0_403.jpg 0_790.jpg 1_1176.jpg 1_211.jpg 1_599.jpg 1_985.jpg 0_1369.jpg 0_404.jpg 0_791.jpg 1_1177.jpg 1_212.jpg 1_59.jpg 1_986.jpg 0_136.jpg 0_405.jpg 0_792.jpg 1_1178.jpg 1_213.jpg 1_5.jpg 1_987.jpg 0_1370.jpg 0_406.jpg 0_793.jpg 1_1179.jpg 1_214.jpg 1_600.jpg 1_988.jpg 0_1371.jpg 0_407.jpg 0_794.jpg 1_117.jpg 1_215.jpg 1_601.jpg 1_989.jpg 0_1372.jpg 0_408.jpg 0_795.jpg 1_1180.jpg 1_216.jpg 1_602.jpg 1_98.jpg 0_1373.jpg 0_409.jpg 0_796.jpg 1_1181.jpg 1_217.jpg 1_603.jpg 1_990.jpg 0_1374.jpg 0_40.jpg 0_797.jpg 1_1182.jpg 1_218.jpg 1_604.jpg 1_991.jpg 0_1375.jpg 0_410.jpg 0_798.jpg 1_1183.jpg 1_219.jpg 1_605.jpg 1_992.jpg 0_1376.jpg 0_411.jpg 0_799.jpg 1_1184.jpg 1_21.jpg 1_606.jpg 1_993.jpg 0_1377.jpg 0_412.jpg 0_79.jpg 1_1185.jpg 1_220.jpg 1_607.jpg 1_994.jpg 0_1378.jpg 0_413.jpg 0_7.jpg 1_1186.jpg 1_221.jpg 1_608.jpg 1_995.jpg 0_1379.jpg 0_414.jpg 0_800.jpg 1_1187.jpg 1_222.jpg 1_609.jpg 1_996.jpg 0_137.jpg 0_415.jpg 0_801.jpg 1_1188.jpg 1_223.jpg 1_60.jpg 1_997.jpg 0_1380.jpg 0_416.jpg 0_802.jpg 1_1189.jpg 1_224.jpg 1_610.jpg 1_998.jpg 0_1381.jpg 0_417.jpg 0_803.jpg 1_118.jpg 1_225.jpg 1_611.jpg 1_999.jpg 0_1382.jpg 0_418.jpg 0_804.jpg 1_1190.jpg 1_226.jpg 1_612.jpg 1_99.jpg 0_1383.jpg 0_419.jpg 0_805.jpg 1_1191.jpg 1_227.jpg 1_613.jpg 1_9.jpg 0_1384.jpg 0_41.jpg 0_806.jpg 1_1192.jpg 1_228.jpg 1_614.jpg 0_1385.jpg 0_420.jpg 0_807.jpg 1_1193.jpg 1_229.jpg 1_615.jpg 0_1386.jpg 0_421.jpg 0_808.jpg 1_1194.jpg 1_22.jpg 1_616.jpg ! mv Food - 5 K /* . # look at an image for fun plt . imshow ( image . load_img ( 'training/0_808.jpg' )) plt . show () # Food images start with 1, non-food images start with 0 plt . imshow ( image . load_img ( 'training/1_616.jpg' )) plt . show () ! mkdir data # Make directories to store the data Keras-style ! mkdir data / train ! mkdir data / test ! mkdir data / train / nonfood ! mkdir data / train / food ! mkdir data / test / nonfood ! mkdir data / test / food # Move the images # Note: we will consider 'training' to be the train set # 'validation' folder will be the test set # ignore the 'evaluation' set ! mv training / 0 *. jpg data / train / nonfood ! mv training / 1 *. jpg data / train / food ! mv validation / 0 *. jpg data / test / nonfood ! mv validation / 1 *. jpg data / test / food train_path = 'data/train' valid_path = 'data/test' # These images are pretty big and of different sizes # Let's load them all in as the same (smaller) size IMAGE_SIZE = [ 200 , 200 ] # useful for getting number of files image_files = glob ( train_path + '/*/*.jpg' ) valid_image_files = glob ( valid_path + '/*/*.jpg' ) # useful for getting number of classes folders = glob ( train_path + '/*' ) folders ['data/train/nonfood', 'data/train/food'] # look at an image for fun plt . imshow ( image . load_img ( np . random . choice ( image_files ))) plt . show () ptm = PretrainedModel ( input_shape = IMAGE_SIZE + [ 3 ], weights = 'imagenet' , include_top = False ) Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 58892288/58889256 [==============================] - 6s 0us/step # freeze pretrained model weights ptm . trainable = False # map the data into feature vectors # Keras image data generator returns classes one-hot encoded K = len ( folders ) # number of classes x = Flatten ()( ptm . output ) x = Dense ( K , activation = 'softmax' )( x ) # create a model object model = Model ( inputs = ptm . input , outputs = x ) # view the structure of the model model . summary () Model: \"model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 200, 200, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 200, 200, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 200, 200, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 100, 100, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 100, 100, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 100, 100, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 50, 50, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 50, 50, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 50, 50, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 50, 50, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 25, 25, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 25, 25, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 25, 25, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 25, 25, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 12, 12, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 6, 6, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 18432) 0 _________________________________________________________________ dense (Dense) (None, 2) 36866 ================================================================= Total params: 14,751,554 Trainable params: 36,866 Non-trainable params: 14,714,688 _________________________________________________________________ # create an instance of ImageDataGenerator gen_train = ImageDataGenerator ( rotation_range = 20 , width_shift_range = 0.1 , height_shift_range = 0.1 , shear_range = 0.1 , zoom_range = 0.2 , horizontal_flip = True , preprocessing_function = preprocess_input ) gen_test = ImageDataGenerator ( preprocessing_function = preprocess_input ) batch_size = 128 # create generators train_generator = gen_train . flow_from_directory ( train_path , shuffle = True , target_size = IMAGE_SIZE , batch_size = batch_size , ) valid_generator = gen_test . flow_from_directory ( valid_path , target_size = IMAGE_SIZE , batch_size = batch_size , ) Found 3000 images belonging to 2 classes. Found 1000 images belonging to 2 classes. model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit the model r = model . fit_generator ( train_generator , validation_data = valid_generator , epochs = 10 , steps_per_epoch = int ( np . ceil ( len ( image_files ) / batch_size )), validation_steps = int ( np . ceil ( len ( valid_image_files ) / batch_size )), ) Epoch 1/10 WARNING:tensorflow:From /tensorflow-2.0.0-rc0/python3.6/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where 24/24 [==============================] - 145s 6s/step - loss: 0.9769 - accuracy: 0.9183 - val_loss: 0.2264 - val_accuracy: 0.9790 Epoch 2/10 24/24 [==============================] - 103s 4s/step - loss: 0.4751 - accuracy: 0.9643 - val_loss: 0.2551 - val_accuracy: 0.9780 Epoch 3/10 24/24 [==============================] - 104s 4s/step - loss: 0.4594 - accuracy: 0.9667 - val_loss: 0.3841 - val_accuracy: 0.9730 Epoch 4/10 24/24 [==============================] - 104s 4s/step - loss: 0.3127 - accuracy: 0.9773 - val_loss: 0.2724 - val_accuracy: 0.9790 Epoch 5/10 24/24 [==============================] - 104s 4s/step - loss: 0.2851 - accuracy: 0.9780 - val_loss: 0.1958 - val_accuracy: 0.9820 Epoch 6/10 24/24 [==============================] - 103s 4s/step - loss: 0.2695 - accuracy: 0.9783 - val_loss: 0.2905 - val_accuracy: 0.9790 Epoch 7/10 24/24 [==============================] - 103s 4s/step - loss: 0.3040 - accuracy: 0.9770 - val_loss: 0.3226 - val_accuracy: 0.9760 Epoch 8/10 24/24 [==============================] - 103s 4s/step - loss: 0.2572 - accuracy: 0.9810 - val_loss: 0.2654 - val_accuracy: 0.9820 Epoch 9/10 24/24 [==============================] - 104s 4s/step - loss: 0.1766 - accuracy: 0.9867 - val_loss: 0.2159 - val_accuracy: 0.9860 Epoch 10/10 24/24 [==============================] - 104s 4s/step - loss: 0.1946 - accuracy: 0.9840 - val_loss: 0.3092 - val_accuracy: 0.9740 # create a 2nd train generator which does not use data augmentation # to get the true train accuracy train_generator2 = gen_test . flow_from_directory ( train_path , target_size = IMAGE_SIZE , batch_size = batch_size , ) model . evaluate_generator ( train_generator2 , steps = int ( np . ceil ( len ( image_files ) / batch_size ))) Found 3000 images belonging to 2 classes. [0.09657006577390703, 0.993] # loss plt . plot ( r . history [ 'loss' ], label = 'train loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val loss' ) plt . legend () plt . show () # accuracies plt . plot ( r . history [ 'accuracy' ], label = 'train acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val acc' ) plt . legend () plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Transfer Learning with Data Augmentation"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning_with_Data_Augmentation/#transfer-learning-with-data-augmentation","text":"# Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-rc0 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.0.0 # More imports from tensorflow.keras.layers import Input , Dense , Flatten from tensorflow.keras.applications.vgg16 import VGG16 as PretrainedModel , \\ preprocess_input from tensorflow.keras.models import Model from tensorflow.keras.optimizers import SGD , Adam from tensorflow.keras.preprocessing import image from tensorflow.keras.preprocessing.image import ImageDataGenerator from glob import glob import numpy as np import pandas as pd import matplotlib.pyplot as plt import sys , os # Data from: https://mmspg.epfl.ch/downloads/food-image-datasets/ # !wget --passive-ftp --prefer-family=ipv4 --ftp-user FoodImage@grebvm2.epfl.ch \\ # --ftp-password Cahc1moo -nc ftp://tremplin.epfl.ch/Food-5K.zip ! wget - nc https : // lazyprogrammer . me / course_files / Food - 5 K . zip --2019-12-04 20:50:28-- https://lazyprogrammer.me/course_files/Food-5K.zip Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.31.81.48, 104.31.80.48, 2606:4700:30::681f:5030, ... Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.31.81.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 447001986 (426M) [application/zip] Saving to: \u2018Food-5K.zip\u2019 Food-5K.zip 100%[===================>] 426.29M 3.94MB/s in 48s 2019-12-04 20:51:17 (8.87 MB/s) - \u2018Food-5K.zip\u2019 saved [447001986/447001986] ! unzip - qq - o Food - 5 K . zip ! ls Food-5K Food-5K.zip __MACOSX sample_data ! ls Food - 5 K / training 0_0.jpg 0_1387.jpg 0_422.jpg 0_809.jpg 1_1195.jpg 1_230.jpg 1_617.jpg 0_1000.jpg 0_1388.jpg 0_423.jpg 0_80.jpg 1_1196.jpg 1_231.jpg 1_618.jpg 0_1001.jpg 0_1389.jpg 0_424.jpg 0_810.jpg 1_1197.jpg 1_232.jpg 1_619.jpg 0_1002.jpg 0_138.jpg 0_425.jpg 0_811.jpg 1_1198.jpg 1_233.jpg 1_61.jpg 0_1003.jpg 0_1390.jpg 0_426.jpg 0_812.jpg 1_1199.jpg 1_234.jpg 1_620.jpg 0_1004.jpg 0_1391.jpg 0_427.jpg 0_813.jpg 1_119.jpg 1_235.jpg 1_621.jpg 0_1005.jpg 0_1392.jpg 0_428.jpg 0_814.jpg 1_11.jpg 1_236.jpg 1_622.jpg 0_1006.jpg 0_1393.jpg 0_429.jpg 0_815.jpg 1_1200.jpg 1_237.jpg 1_623.jpg 0_1007.jpg 0_1394.jpg 0_42.jpg 0_816.jpg 1_1201.jpg 1_238.jpg 1_624.jpg 0_1008.jpg 0_1395.jpg 0_430.jpg 0_817.jpg 1_1202.jpg 1_239.jpg 1_625.jpg 0_1009.jpg 0_1396.jpg 0_431.jpg 0_818.jpg 1_1203.jpg 1_23.jpg 1_626.jpg 0_100.jpg 0_1397.jpg 0_432.jpg 0_819.jpg 1_1204.jpg 1_240.jpg 1_627.jpg 0_1010.jpg 0_1398.jpg 0_433.jpg 0_81.jpg 1_1205.jpg 1_241.jpg 1_628.jpg 0_1011.jpg 0_1399.jpg 0_434.jpg 0_820.jpg 1_1206.jpg 1_242.jpg 1_629.jpg 0_1012.jpg 0_139.jpg 0_435.jpg 0_821.jpg 1_1207.jpg 1_243.jpg 1_62.jpg 0_1013.jpg 0_13.jpg 0_436.jpg 0_822.jpg 1_1208.jpg 1_244.jpg 1_630.jpg 0_1014.jpg 0_1400.jpg 0_437.jpg 0_823.jpg 1_1209.jpg 1_245.jpg 1_631.jpg 0_1015.jpg 0_1401.jpg 0_438.jpg 0_824.jpg 1_120.jpg 1_246.jpg 1_632.jpg 0_1016.jpg 0_1402.jpg 0_439.jpg 0_825.jpg 1_1210.jpg 1_247.jpg 1_633.jpg 0_1017.jpg 0_1403.jpg 0_43.jpg 0_826.jpg 1_1211.jpg 1_248.jpg 1_634.jpg 0_1018.jpg 0_1404.jpg 0_440.jpg 0_827.jpg 1_1212.jpg 1_249.jpg 1_635.jpg 0_1019.jpg 0_1405.jpg 0_441.jpg 0_828.jpg 1_1213.jpg 1_24.jpg 1_636.jpg 0_101.jpg 0_1406.jpg 0_442.jpg 0_829.jpg 1_1214.jpg 1_250.jpg 1_637.jpg 0_1020.jpg 0_1407.jpg 0_443.jpg 0_82.jpg 1_1215.jpg 1_251.jpg 1_638.jpg 0_1021.jpg 0_1408.jpg 0_444.jpg 0_830.jpg 1_1216.jpg 1_252.jpg 1_639.jpg 0_1022.jpg 0_1409.jpg 0_445.jpg 0_831.jpg 1_1217.jpg 1_253.jpg 1_63.jpg 0_1023.jpg 0_140.jpg 0_446.jpg 0_832.jpg 1_1218.jpg 1_254.jpg 1_640.jpg 0_1024.jpg 0_1410.jpg 0_447.jpg 0_833.jpg 1_1219.jpg 1_255.jpg 1_641.jpg 0_1025.jpg 0_1411.jpg 0_448.jpg 0_834.jpg 1_121.jpg 1_256.jpg 1_642.jpg 0_1026.jpg 0_1412.jpg 0_449.jpg 0_835.jpg 1_1220.jpg 1_257.jpg 1_643.jpg 0_1027.jpg 0_1413.jpg 0_44.jpg 0_836.jpg 1_1221.jpg 1_258.jpg 1_644.jpg 0_1028.jpg 0_1414.jpg 0_450.jpg 0_837.jpg 1_1222.jpg 1_259.jpg 1_645.jpg 0_1029.jpg 0_1415.jpg 0_451.jpg 0_838.jpg 1_1223.jpg 1_25.jpg 1_646.jpg 0_102.jpg 0_1416.jpg 0_452.jpg 0_839.jpg 1_1224.jpg 1_260.jpg 1_647.jpg 0_1030.jpg 0_1417.jpg 0_453.jpg 0_83.jpg 1_1225.jpg 1_261.jpg 1_648.jpg 0_1031.jpg 0_1418.jpg 0_454.jpg 0_840.jpg 1_1226.jpg 1_262.jpg 1_649.jpg 0_1032.jpg 0_1419.jpg 0_455.jpg 0_841.jpg 1_1227.jpg 1_263.jpg 1_64.jpg 0_1033.jpg 0_141.jpg 0_456.jpg 0_842.jpg 1_1228.jpg 1_264.jpg 1_650.jpg 0_1034.jpg 0_1420.jpg 0_457.jpg 0_843.jpg 1_1229.jpg 1_265.jpg 1_651.jpg 0_1035.jpg 0_1421.jpg 0_458.jpg 0_844.jpg 1_122.jpg 1_266.jpg 1_652.jpg 0_1036.jpg 0_1422.jpg 0_459.jpg 0_845.jpg 1_1230.jpg 1_267.jpg 1_653.jpg 0_1037.jpg 0_1423.jpg 0_45.jpg 0_846.jpg 1_1231.jpg 1_268.jpg 1_654.jpg 0_1038.jpg 0_1424.jpg 0_460.jpg 0_847.jpg 1_1232.jpg 1_269.jpg 1_655.jpg 0_1039.jpg 0_1425.jpg 0_461.jpg 0_848.jpg 1_1233.jpg 1_26.jpg 1_656.jpg 0_103.jpg 0_1426.jpg 0_462.jpg 0_849.jpg 1_1234.jpg 1_270.jpg 1_657.jpg 0_1040.jpg 0_1427.jpg 0_463.jpg 0_84.jpg 1_1235.jpg 1_271.jpg 1_658.jpg 0_1041.jpg 0_1428.jpg 0_464.jpg 0_850.jpg 1_1236.jpg 1_272.jpg 1_659.jpg 0_1042.jpg 0_1429.jpg 0_465.jpg 0_851.jpg 1_1237.jpg 1_273.jpg 1_65.jpg 0_1043.jpg 0_142.jpg 0_466.jpg 0_852.jpg 1_1238.jpg 1_274.jpg 1_660.jpg 0_1044.jpg 0_1430.jpg 0_467.jpg 0_853.jpg 1_1239.jpg 1_275.jpg 1_661.jpg 0_1045.jpg 0_1431.jpg 0_468.jpg 0_854.jpg 1_123.jpg 1_276.jpg 1_662.jpg 0_1046.jpg 0_1432.jpg 0_469.jpg 0_855.jpg 1_1240.jpg 1_277.jpg 1_663.jpg 0_1047.jpg 0_1433.jpg 0_46.jpg 0_856.jpg 1_1241.jpg 1_278.jpg 1_664.jpg 0_1048.jpg 0_1434.jpg 0_470.jpg 0_857.jpg 1_1242.jpg 1_279.jpg 1_665.jpg 0_1049.jpg 0_1435.jpg 0_471.jpg 0_858.jpg 1_1243.jpg 1_27.jpg 1_666.jpg 0_104.jpg 0_1436.jpg 0_472.jpg 0_859.jpg 1_1244.jpg 1_280.jpg 1_667.jpg 0_1050.jpg 0_1437.jpg 0_473.jpg 0_85.jpg 1_1245.jpg 1_281.jpg 1_668.jpg 0_1051.jpg 0_1438.jpg 0_474.jpg 0_860.jpg 1_1246.jpg 1_282.jpg 1_669.jpg 0_1052.jpg 0_1439.jpg 0_475.jpg 0_861.jpg 1_1247.jpg 1_283.jpg 1_66.jpg 0_1053.jpg 0_143.jpg 0_476.jpg 0_862.jpg 1_1248.jpg 1_284.jpg 1_670.jpg 0_1054.jpg 0_1440.jpg 0_477.jpg 0_863.jpg 1_1249.jpg 1_285.jpg 1_671.jpg 0_1055.jpg 0_1441.jpg 0_478.jpg 0_864.jpg 1_124.jpg 1_286.jpg 1_672.jpg 0_1056.jpg 0_1442.jpg 0_479.jpg 0_865.jpg 1_1250.jpg 1_287.jpg 1_673.jpg 0_1057.jpg 0_1443.jpg 0_47.jpg 0_866.jpg 1_1251.jpg 1_288.jpg 1_674.jpg 0_1058.jpg 0_1444.jpg 0_480.jpg 0_867.jpg 1_1252.jpg 1_289.jpg 1_675.jpg 0_1059.jpg 0_1445.jpg 0_481.jpg 0_868.jpg 1_1253.jpg 1_28.jpg 1_676.jpg 0_105.jpg 0_1446.jpg 0_482.jpg 0_869.jpg 1_1254.jpg 1_290.jpg 1_677.jpg 0_1060.jpg 0_1447.jpg 0_483.jpg 0_86.jpg 1_1255.jpg 1_291.jpg 1_678.jpg 0_1061.jpg 0_1448.jpg 0_484.jpg 0_870.jpg 1_1256.jpg 1_292.jpg 1_679.jpg 0_1062.jpg 0_1449.jpg 0_485.jpg 0_871.jpg 1_1257.jpg 1_293.jpg 1_67.jpg 0_1063.jpg 0_144.jpg 0_486.jpg 0_872.jpg 1_1258.jpg 1_294.jpg 1_680.jpg 0_1064.jpg 0_1450.jpg 0_487.jpg 0_873.jpg 1_1259.jpg 1_295.jpg 1_681.jpg 0_1065.jpg 0_1451.jpg 0_488.jpg 0_874.jpg 1_125.jpg 1_296.jpg 1_682.jpg 0_1066.jpg 0_1452.jpg 0_489.jpg 0_875.jpg 1_1260.jpg 1_297.jpg 1_683.jpg 0_1067.jpg 0_1453.jpg 0_48.jpg 0_876.jpg 1_1261.jpg 1_298.jpg 1_684.jpg 0_1068.jpg 0_1454.jpg 0_490.jpg 0_877.jpg 1_1262.jpg 1_299.jpg 1_685.jpg 0_1069.jpg 0_1455.jpg 0_491.jpg 0_878.jpg 1_1263.jpg 1_29.jpg 1_686.jpg 0_106.jpg 0_1456.jpg 0_492.jpg 0_879.jpg 1_1264.jpg 1_2.jpg 1_687.jpg 0_1070.jpg 0_1457.jpg 0_493.jpg 0_87.jpg 1_1265.jpg 1_300.jpg 1_688.jpg 0_1071.jpg 0_1458.jpg 0_494.jpg 0_880.jpg 1_1266.jpg 1_301.jpg 1_689.jpg 0_1072.jpg 0_1459.jpg 0_495.jpg 0_881.jpg 1_1267.jpg 1_302.jpg 1_68.jpg 0_1073.jpg 0_145.jpg 0_496.jpg 0_882.jpg 1_1268.jpg 1_303.jpg 1_690.jpg 0_1074.jpg 0_1460.jpg 0_497.jpg 0_883.jpg 1_1269.jpg 1_304.jpg 1_691.jpg 0_1075.jpg 0_1461.jpg 0_498.jpg 0_884.jpg 1_126.jpg 1_305.jpg 1_692.jpg 0_1076.jpg 0_1462.jpg 0_499.jpg 0_885.jpg 1_1270.jpg 1_306.jpg 1_693.jpg 0_1077.jpg 0_1463.jpg 0_49.jpg 0_886.jpg 1_1271.jpg 1_307.jpg 1_694.jpg 0_1078.jpg 0_1464.jpg 0_4.jpg 0_887.jpg 1_1272.jpg 1_308.jpg 1_695.jpg 0_1079.jpg 0_1465.jpg 0_500.jpg 0_888.jpg 1_1273.jpg 1_309.jpg 1_696.jpg 0_107.jpg 0_1466.jpg 0_501.jpg 0_889.jpg 1_1274.jpg 1_30.jpg 1_697.jpg 0_1080.jpg 0_1467.jpg 0_502.jpg 0_88.jpg 1_1275.jpg 1_310.jpg 1_698.jpg 0_1081.jpg 0_1468.jpg 0_503.jpg 0_890.jpg 1_1276.jpg 1_311.jpg 1_699.jpg 0_1082.jpg 0_1469.jpg 0_504.jpg 0_891.jpg 1_1277.jpg 1_312.jpg 1_69.jpg 0_1083.jpg 0_146.jpg 0_505.jpg 0_892.jpg 1_1278.jpg 1_313.jpg 1_6.jpg 0_1084.jpg 0_1470.jpg 0_506.jpg 0_893.jpg 1_1279.jpg 1_314.jpg 1_700.jpg 0_1085.jpg 0_1471.jpg 0_507.jpg 0_894.jpg 1_127.jpg 1_315.jpg 1_701.jpg 0_1086.jpg 0_1472.jpg 0_508.jpg 0_895.jpg 1_1280.jpg 1_316.jpg 1_702.jpg 0_1087.jpg 0_1473.jpg 0_509.jpg 0_896.jpg 1_1281.jpg 1_317.jpg 1_703.jpg 0_1088.jpg 0_1474.jpg 0_50.jpg 0_897.jpg 1_1282.jpg 1_318.jpg 1_704.jpg 0_1089.jpg 0_1475.jpg 0_510.jpg 0_898.jpg 1_1283.jpg 1_319.jpg 1_705.jpg 0_108.jpg 0_1476.jpg 0_511.jpg 0_899.jpg 1_1284.jpg 1_31.jpg 1_706.jpg 0_1090.jpg 0_1477.jpg 0_512.jpg 0_89.jpg 1_1285.jpg 1_320.jpg 1_707.jpg 0_1091.jpg 0_1478.jpg 0_513.jpg 0_8.jpg 1_1286.jpg 1_321.jpg 1_708.jpg 0_1092.jpg 0_1479.jpg 0_514.jpg 0_900.jpg 1_1287.jpg 1_322.jpg 1_709.jpg 0_1093.jpg 0_147.jpg 0_515.jpg 0_901.jpg 1_1288.jpg 1_323.jpg 1_70.jpg 0_1094.jpg 0_1480.jpg 0_516.jpg 0_902.jpg 1_1289.jpg 1_324.jpg 1_710.jpg 0_1095.jpg 0_1481.jpg 0_517.jpg 0_903.jpg 1_128.jpg 1_325.jpg 1_711.jpg 0_1096.jpg 0_1482.jpg 0_518.jpg 0_904.jpg 1_1290.jpg 1_326.jpg 1_712.jpg 0_1097.jpg 0_1483.jpg 0_519.jpg 0_905.jpg 1_1291.jpg 1_327.jpg 1_713.jpg 0_1098.jpg 0_1484.jpg 0_51.jpg 0_906.jpg 1_1292.jpg 1_328.jpg 1_714.jpg 0_1099.jpg 0_1485.jpg 0_520.jpg 0_907.jpg 1_1293.jpg 1_329.jpg 1_715.jpg 0_109.jpg 0_1486.jpg 0_521.jpg 0_908.jpg 1_1294.jpg 1_32.jpg 1_716.jpg 0_10.jpg 0_1487.jpg 0_522.jpg 0_909.jpg 1_1295.jpg 1_330.jpg 1_717.jpg 0_1100.jpg 0_1488.jpg 0_523.jpg 0_90.jpg 1_1296.jpg 1_331.jpg 1_718.jpg 0_1101.jpg 0_1489.jpg 0_524.jpg 0_910.jpg 1_1297.jpg 1_332.jpg 1_719.jpg 0_1102.jpg 0_148.jpg 0_525.jpg 0_911.jpg 1_1298.jpg 1_333.jpg 1_71.jpg 0_1103.jpg 0_1490.jpg 0_526.jpg 0_912.jpg 1_1299.jpg 1_334.jpg 1_720.jpg 0_1104.jpg 0_1491.jpg 0_527.jpg 0_913.jpg 1_129.jpg 1_335.jpg 1_721.jpg 0_1105.jpg 0_1492.jpg 0_528.jpg 0_914.jpg 1_12.jpg 1_336.jpg 1_722.jpg 0_1106.jpg 0_1493.jpg 0_529.jpg 0_915.jpg 1_1300.jpg 1_337.jpg 1_723.jpg 0_1107.jpg 0_1494.jpg 0_52.jpg 0_916.jpg 1_1301.jpg 1_338.jpg 1_724.jpg 0_1108.jpg 0_1495.jpg 0_530.jpg 0_917.jpg 1_1302.jpg 1_339.jpg 1_725.jpg 0_1109.jpg 0_1496.jpg 0_531.jpg 0_918.jpg 1_1303.jpg 1_33.jpg 1_726.jpg 0_110.jpg 0_1497.jpg 0_532.jpg 0_919.jpg 1_1304.jpg 1_340.jpg 1_727.jpg 0_1110.jpg 0_1498.jpg 0_533.jpg 0_91.jpg 1_1305.jpg 1_341.jpg 1_728.jpg 0_1111.jpg 0_1499.jpg 0_534.jpg 0_920.jpg 1_1306.jpg 1_342.jpg 1_729.jpg 0_1112.jpg 0_149.jpg 0_535.jpg 0_921.jpg 1_1307.jpg 1_343.jpg 1_72.jpg 0_1113.jpg 0_14.jpg 0_536.jpg 0_922.jpg 1_1308.jpg 1_344.jpg 1_730.jpg 0_1114.jpg 0_150.jpg 0_537.jpg 0_923.jpg 1_1309.jpg 1_345.jpg 1_731.jpg 0_1115.jpg 0_151.jpg 0_538.jpg 0_924.jpg 1_130.jpg 1_346.jpg 1_732.jpg 0_1116.jpg 0_152.jpg 0_539.jpg 0_925.jpg 1_1310.jpg 1_347.jpg 1_733.jpg 0_1117.jpg 0_153.jpg 0_53.jpg 0_926.jpg 1_1311.jpg 1_348.jpg 1_734.jpg 0_1118.jpg 0_154.jpg 0_540.jpg 0_927.jpg 1_1312.jpg 1_349.jpg 1_735.jpg 0_1119.jpg 0_155.jpg 0_541.jpg 0_928.jpg 1_1313.jpg 1_34.jpg 1_736.jpg 0_111.jpg 0_156.jpg 0_542.jpg 0_929.jpg 1_1314.jpg 1_350.jpg 1_737.jpg 0_1120.jpg 0_157.jpg 0_543.jpg 0_92.jpg 1_1315.jpg 1_351.jpg 1_738.jpg 0_1121.jpg 0_158.jpg 0_544.jpg 0_930.jpg 1_1316.jpg 1_352.jpg 1_739.jpg 0_1122.jpg 0_159.jpg 0_545.jpg 0_931.jpg 1_1317.jpg 1_353.jpg 1_73.jpg 0_1123.jpg 0_15.jpg 0_546.jpg 0_932.jpg 1_1318.jpg 1_354.jpg 1_740.jpg 0_1124.jpg 0_160.jpg 0_547.jpg 0_933.jpg 1_1319.jpg 1_355.jpg 1_741.jpg 0_1125.jpg 0_161.jpg 0_548.jpg 0_934.jpg 1_131.jpg 1_356.jpg 1_742.jpg 0_1126.jpg 0_162.jpg 0_549.jpg 0_935.jpg 1_1320.jpg 1_357.jpg 1_743.jpg 0_1127.jpg 0_163.jpg 0_54.jpg 0_936.jpg 1_1321.jpg 1_358.jpg 1_744.jpg 0_1128.jpg 0_164.jpg 0_550.jpg 0_937.jpg 1_1322.jpg 1_359.jpg 1_745.jpg 0_1129.jpg 0_165.jpg 0_551.jpg 0_938.jpg 1_1323.jpg 1_35.jpg 1_746.jpg 0_112.jpg 0_166.jpg 0_552.jpg 0_939.jpg 1_1324.jpg 1_360.jpg 1_747.jpg 0_1130.jpg 0_167.jpg 0_553.jpg 0_93.jpg 1_1325.jpg 1_361.jpg 1_748.jpg 0_1131.jpg 0_168.jpg 0_554.jpg 0_940.jpg 1_1326.jpg 1_362.jpg 1_749.jpg 0_1132.jpg 0_169.jpg 0_555.jpg 0_941.jpg 1_1327.jpg 1_363.jpg 1_74.jpg 0_1133.jpg 0_16.jpg 0_556.jpg 0_942.jpg 1_1328.jpg 1_364.jpg 1_750.jpg 0_1134.jpg 0_170.jpg 0_557.jpg 0_943.jpg 1_1329.jpg 1_365.jpg 1_751.jpg 0_1135.jpg 0_171.jpg 0_558.jpg 0_944.jpg 1_132.jpg 1_366.jpg 1_752.jpg 0_1136.jpg 0_172.jpg 0_559.jpg 0_945.jpg 1_1330.jpg 1_367.jpg 1_753.jpg 0_1137.jpg 0_173.jpg 0_55.jpg 0_946.jpg 1_1331.jpg 1_368.jpg 1_754.jpg 0_1138.jpg 0_174.jpg 0_560.jpg 0_947.jpg 1_1332.jpg 1_369.jpg 1_755.jpg 0_1139.jpg 0_175.jpg 0_561.jpg 0_948.jpg 1_1333.jpg 1_36.jpg 1_756.jpg 0_113.jpg 0_176.jpg 0_562.jpg 0_949.jpg 1_1334.jpg 1_370.jpg 1_757.jpg 0_1140.jpg 0_177.jpg 0_563.jpg 0_94.jpg 1_1335.jpg 1_371.jpg 1_758.jpg 0_1141.jpg 0_178.jpg 0_564.jpg 0_950.jpg 1_1336.jpg 1_372.jpg 1_759.jpg 0_1142.jpg 0_179.jpg 0_565.jpg 0_951.jpg 1_1337.jpg 1_373.jpg 1_75.jpg 0_1143.jpg 0_17.jpg 0_566.jpg 0_952.jpg 1_1338.jpg 1_374.jpg 1_760.jpg 0_1144.jpg 0_180.jpg 0_567.jpg 0_953.jpg 1_1339.jpg 1_375.jpg 1_761.jpg 0_1145.jpg 0_181.jpg 0_568.jpg 0_954.jpg 1_133.jpg 1_376.jpg 1_762.jpg 0_1146.jpg 0_182.jpg 0_569.jpg 0_955.jpg 1_1340.jpg 1_377.jpg 1_763.jpg 0_1147.jpg 0_183.jpg 0_56.jpg 0_956.jpg 1_1341.jpg 1_378.jpg 1_764.jpg 0_1148.jpg 0_184.jpg 0_570.jpg 0_957.jpg 1_1342.jpg 1_379.jpg 1_765.jpg 0_1149.jpg 0_185.jpg 0_571.jpg 0_958.jpg 1_1343.jpg 1_37.jpg 1_766.jpg 0_114.jpg 0_186.jpg 0_572.jpg 0_959.jpg 1_1344.jpg 1_380.jpg 1_767.jpg 0_1150.jpg 0_187.jpg 0_573.jpg 0_95.jpg 1_1345.jpg 1_381.jpg 1_768.jpg 0_1151.jpg 0_188.jpg 0_574.jpg 0_960.jpg 1_1346.jpg 1_382.jpg 1_769.jpg 0_1152.jpg 0_189.jpg 0_575.jpg 0_961.jpg 1_1347.jpg 1_383.jpg 1_76.jpg 0_1153.jpg 0_18.jpg 0_576.jpg 0_962.jpg 1_1348.jpg 1_384.jpg 1_770.jpg 0_1154.jpg 0_190.jpg 0_577.jpg 0_963.jpg 1_1349.jpg 1_385.jpg 1_771.jpg 0_1155.jpg 0_191.jpg 0_578.jpg 0_964.jpg 1_134.jpg 1_386.jpg 1_772.jpg 0_1156.jpg 0_192.jpg 0_579.jpg 0_965.jpg 1_1350.jpg 1_387.jpg 1_773.jpg 0_1157.jpg 0_193.jpg 0_57.jpg 0_966.jpg 1_1351.jpg 1_388.jpg 1_774.jpg 0_1158.jpg 0_194.jpg 0_580.jpg 0_967.jpg 1_1352.jpg 1_389.jpg 1_775.jpg 0_1159.jpg 0_195.jpg 0_581.jpg 0_968.jpg 1_1353.jpg 1_38.jpg 1_776.jpg 0_115.jpg 0_196.jpg 0_582.jpg 0_969.jpg 1_1354.jpg 1_390.jpg 1_777.jpg 0_1160.jpg 0_197.jpg 0_583.jpg 0_96.jpg 1_1355.jpg 1_391.jpg 1_778.jpg 0_1161.jpg 0_198.jpg 0_584.jpg 0_970.jpg 1_1356.jpg 1_392.jpg 1_779.jpg 0_1162.jpg 0_199.jpg 0_585.jpg 0_971.jpg 1_1357.jpg 1_393.jpg 1_77.jpg 0_1163.jpg 0_19.jpg 0_586.jpg 0_972.jpg 1_1358.jpg 1_394.jpg 1_780.jpg 0_1164.jpg 0_1.jpg 0_587.jpg 0_973.jpg 1_1359.jpg 1_395.jpg 1_781.jpg 0_1165.jpg 0_200.jpg 0_588.jpg 0_974.jpg 1_135.jpg 1_396.jpg 1_782.jpg 0_1166.jpg 0_201.jpg 0_589.jpg 0_975.jpg 1_1360.jpg 1_397.jpg 1_783.jpg 0_1167.jpg 0_202.jpg 0_58.jpg 0_976.jpg 1_1361.jpg 1_398.jpg 1_784.jpg 0_1168.jpg 0_203.jpg 0_590.jpg 0_977.jpg 1_1362.jpg 1_399.jpg 1_785.jpg 0_1169.jpg 0_204.jpg 0_591.jpg 0_978.jpg 1_1363.jpg 1_39.jpg 1_786.jpg 0_116.jpg 0_205.jpg 0_592.jpg 0_979.jpg 1_1364.jpg 1_3.jpg 1_787.jpg 0_1170.jpg 0_206.jpg 0_593.jpg 0_97.jpg 1_1365.jpg 1_400.jpg 1_788.jpg 0_1171.jpg 0_207.jpg 0_594.jpg 0_980.jpg 1_1366.jpg 1_401.jpg 1_789.jpg 0_1172.jpg 0_208.jpg 0_595.jpg 0_981.jpg 1_1367.jpg 1_402.jpg 1_78.jpg 0_1173.jpg 0_209.jpg 0_596.jpg 0_982.jpg 1_1368.jpg 1_403.jpg 1_790.jpg 0_1174.jpg 0_20.jpg 0_597.jpg 0_983.jpg 1_1369.jpg 1_404.jpg 1_791.jpg 0_1175.jpg 0_210.jpg 0_598.jpg 0_984.jpg 1_136.jpg 1_405.jpg 1_792.jpg 0_1176.jpg 0_211.jpg 0_599.jpg 0_985.jpg 1_1370.jpg 1_406.jpg 1_793.jpg 0_1177.jpg 0_212.jpg 0_59.jpg 0_986.jpg 1_1371.jpg 1_407.jpg 1_794.jpg 0_1178.jpg 0_213.jpg 0_5.jpg 0_987.jpg 1_1372.jpg 1_408.jpg 1_795.jpg 0_1179.jpg 0_214.jpg 0_600.jpg 0_988.jpg 1_1373.jpg 1_409.jpg 1_796.jpg 0_117.jpg 0_215.jpg 0_601.jpg 0_989.jpg 1_1374.jpg 1_40.jpg 1_797.jpg 0_1180.jpg 0_216.jpg 0_602.jpg 0_98.jpg 1_1375.jpg 1_410.jpg 1_798.jpg 0_1181.jpg 0_217.jpg 0_603.jpg 0_990.jpg 1_1376.jpg 1_411.jpg 1_799.jpg 0_1182.jpg 0_218.jpg 0_604.jpg 0_991.jpg 1_1377.jpg 1_412.jpg 1_79.jpg 0_1183.jpg 0_219.jpg 0_605.jpg 0_992.jpg 1_1378.jpg 1_413.jpg 1_7.jpg 0_1184.jpg 0_21.jpg 0_606.jpg 0_993.jpg 1_1379.jpg 1_414.jpg 1_800.jpg 0_1185.jpg 0_220.jpg 0_607.jpg 0_994.jpg 1_137.jpg 1_415.jpg 1_801.jpg 0_1186.jpg 0_221.jpg 0_608.jpg 0_995.jpg 1_1380.jpg 1_416.jpg 1_802.jpg 0_1187.jpg 0_222.jpg 0_609.jpg 0_996.jpg 1_1381.jpg 1_417.jpg 1_803.jpg 0_1188.jpg 0_223.jpg 0_60.jpg 0_997.jpg 1_1382.jpg 1_418.jpg 1_804.jpg 0_1189.jpg 0_224.jpg 0_610.jpg 0_998.jpg 1_1383.jpg 1_419.jpg 1_805.jpg 0_118.jpg 0_225.jpg 0_611.jpg 0_999.jpg 1_1384.jpg 1_41.jpg 1_806.jpg 0_1190.jpg 0_226.jpg 0_612.jpg 0_99.jpg 1_1385.jpg 1_420.jpg 1_807.jpg 0_1191.jpg 0_227.jpg 0_613.jpg 0_9.jpg 1_1386.jpg 1_421.jpg 1_808.jpg 0_1192.jpg 0_228.jpg 0_614.jpg 1_0.jpg 1_1387.jpg 1_422.jpg 1_809.jpg 0_1193.jpg 0_229.jpg 0_615.jpg 1_1000.jpg 1_1388.jpg 1_423.jpg 1_80.jpg 0_1194.jpg 0_22.jpg 0_616.jpg 1_1001.jpg 1_1389.jpg 1_424.jpg 1_810.jpg 0_1195.jpg 0_230.jpg 0_617.jpg 1_1002.jpg 1_138.jpg 1_425.jpg 1_811.jpg 0_1196.jpg 0_231.jpg 0_618.jpg 1_1003.jpg 1_1390.jpg 1_426.jpg 1_812.jpg 0_1197.jpg 0_232.jpg 0_619.jpg 1_1004.jpg 1_1391.jpg 1_427.jpg 1_813.jpg 0_1198.jpg 0_233.jpg 0_61.jpg 1_1005.jpg 1_1392.jpg 1_428.jpg 1_814.jpg 0_1199.jpg 0_234.jpg 0_620.jpg 1_1006.jpg 1_1393.jpg 1_429.jpg 1_815.jpg 0_119.jpg 0_235.jpg 0_621.jpg 1_1007.jpg 1_1394.jpg 1_42.jpg 1_816.jpg 0_11.jpg 0_236.jpg 0_622.jpg 1_1008.jpg 1_1395.jpg 1_430.jpg 1_817.jpg 0_1200.jpg 0_237.jpg 0_623.jpg 1_1009.jpg 1_1396.jpg 1_431.jpg 1_818.jpg 0_1201.jpg 0_238.jpg 0_624.jpg 1_100.jpg 1_1397.jpg 1_432.jpg 1_819.jpg 0_1202.jpg 0_239.jpg 0_625.jpg 1_1010.jpg 1_1398.jpg 1_433.jpg 1_81.jpg 0_1203.jpg 0_23.jpg 0_626.jpg 1_1011.jpg 1_1399.jpg 1_434.jpg 1_820.jpg 0_1204.jpg 0_240.jpg 0_627.jpg 1_1012.jpg 1_139.jpg 1_435.jpg 1_821.jpg 0_1205.jpg 0_241.jpg 0_628.jpg 1_1013.jpg 1_13.jpg 1_436.jpg 1_822.jpg 0_1206.jpg 0_242.jpg 0_629.jpg 1_1014.jpg 1_1400.jpg 1_437.jpg 1_823.jpg 0_1207.jpg 0_243.jpg 0_62.jpg 1_1015.jpg 1_1401.jpg 1_438.jpg 1_824.jpg 0_1208.jpg 0_244.jpg 0_630.jpg 1_1016.jpg 1_1402.jpg 1_439.jpg 1_825.jpg 0_1209.jpg 0_245.jpg 0_631.jpg 1_1017.jpg 1_1403.jpg 1_43.jpg 1_826.jpg 0_120.jpg 0_246.jpg 0_632.jpg 1_1018.jpg 1_1404.jpg 1_440.jpg 1_827.jpg 0_1210.jpg 0_247.jpg 0_633.jpg 1_1019.jpg 1_1405.jpg 1_441.jpg 1_828.jpg 0_1211.jpg 0_248.jpg 0_634.jpg 1_101.jpg 1_1406.jpg 1_442.jpg 1_829.jpg 0_1212.jpg 0_249.jpg 0_635.jpg 1_1020.jpg 1_1407.jpg 1_443.jpg 1_82.jpg 0_1213.jpg 0_24.jpg 0_636.jpg 1_1021.jpg 1_1408.jpg 1_444.jpg 1_830.jpg 0_1214.jpg 0_250.jpg 0_637.jpg 1_1022.jpg 1_1409.jpg 1_445.jpg 1_831.jpg 0_1215.jpg 0_251.jpg 0_638.jpg 1_1023.jpg 1_140.jpg 1_446.jpg 1_832.jpg 0_1216.jpg 0_252.jpg 0_639.jpg 1_1024.jpg 1_1410.jpg 1_447.jpg 1_833.jpg 0_1217.jpg 0_253.jpg 0_63.jpg 1_1025.jpg 1_1411.jpg 1_448.jpg 1_834.jpg 0_1218.jpg 0_254.jpg 0_640.jpg 1_1026.jpg 1_1412.jpg 1_449.jpg 1_835.jpg 0_1219.jpg 0_255.jpg 0_641.jpg 1_1027.jpg 1_1413.jpg 1_44.jpg 1_836.jpg 0_121.jpg 0_256.jpg 0_642.jpg 1_1028.jpg 1_1414.jpg 1_450.jpg 1_837.jpg 0_1220.jpg 0_257.jpg 0_643.jpg 1_1029.jpg 1_1415.jpg 1_451.jpg 1_838.jpg 0_1221.jpg 0_258.jpg 0_644.jpg 1_102.jpg 1_1416.jpg 1_452.jpg 1_839.jpg 0_1222.jpg 0_259.jpg 0_645.jpg 1_1030.jpg 1_1417.jpg 1_453.jpg 1_83.jpg 0_1223.jpg 0_25.jpg 0_646.jpg 1_1031.jpg 1_1418.jpg 1_454.jpg 1_840.jpg 0_1224.jpg 0_260.jpg 0_647.jpg 1_1032.jpg 1_1419.jpg 1_455.jpg 1_841.jpg 0_1225.jpg 0_261.jpg 0_648.jpg 1_1033.jpg 1_141.jpg 1_456.jpg 1_842.jpg 0_1226.jpg 0_262.jpg 0_649.jpg 1_1034.jpg 1_1420.jpg 1_457.jpg 1_843.jpg 0_1227.jpg 0_263.jpg 0_64.jpg 1_1035.jpg 1_1421.jpg 1_458.jpg 1_844.jpg 0_1228.jpg 0_264.jpg 0_650.jpg 1_1036.jpg 1_1422.jpg 1_459.jpg 1_845.jpg 0_1229.jpg 0_265.jpg 0_651.jpg 1_1037.jpg 1_1423.jpg 1_45.jpg 1_846.jpg 0_122.jpg 0_266.jpg 0_652.jpg 1_1038.jpg 1_1424.jpg 1_460.jpg 1_847.jpg 0_1230.jpg 0_267.jpg 0_653.jpg 1_1039.jpg 1_1425.jpg 1_461.jpg 1_848.jpg 0_1231.jpg 0_268.jpg 0_654.jpg 1_103.jpg 1_1426.jpg 1_462.jpg 1_849.jpg 0_1232.jpg 0_269.jpg 0_655.jpg 1_1040.jpg 1_1427.jpg 1_463.jpg 1_84.jpg 0_1233.jpg 0_26.jpg 0_656.jpg 1_1041.jpg 1_1428.jpg 1_464.jpg 1_850.jpg 0_1234.jpg 0_270.jpg 0_657.jpg 1_1042.jpg 1_1429.jpg 1_465.jpg 1_851.jpg 0_1235.jpg 0_271.jpg 0_658.jpg 1_1043.jpg 1_142.jpg 1_466.jpg 1_852.jpg 0_1236.jpg 0_272.jpg 0_659.jpg 1_1044.jpg 1_1430.jpg 1_467.jpg 1_853.jpg 0_1237.jpg 0_273.jpg 0_65.jpg 1_1045.jpg 1_1431.jpg 1_468.jpg 1_854.jpg 0_1238.jpg 0_274.jpg 0_660.jpg 1_1046.jpg 1_1432.jpg 1_469.jpg 1_855.jpg 0_1239.jpg 0_275.jpg 0_661.jpg 1_1047.jpg 1_1433.jpg 1_46.jpg 1_856.jpg 0_123.jpg 0_276.jpg 0_662.jpg 1_1048.jpg 1_1434.jpg 1_470.jpg 1_857.jpg 0_1240.jpg 0_277.jpg 0_663.jpg 1_1049.jpg 1_1435.jpg 1_471.jpg 1_858.jpg 0_1241.jpg 0_278.jpg 0_664.jpg 1_104.jpg 1_1436.jpg 1_472.jpg 1_859.jpg 0_1242.jpg 0_279.jpg 0_665.jpg 1_1050.jpg 1_1437.jpg 1_473.jpg 1_85.jpg 0_1243.jpg 0_27.jpg 0_666.jpg 1_1051.jpg 1_1438.jpg 1_474.jpg 1_860.jpg 0_1244.jpg 0_280.jpg 0_667.jpg 1_1052.jpg 1_1439.jpg 1_475.jpg 1_861.jpg 0_1245.jpg 0_281.jpg 0_668.jpg 1_1053.jpg 1_143.jpg 1_476.jpg 1_862.jpg 0_1246.jpg 0_282.jpg 0_669.jpg 1_1054.jpg 1_1440.jpg 1_477.jpg 1_863.jpg 0_1247.jpg 0_283.jpg 0_66.jpg 1_1055.jpg 1_1441.jpg 1_478.jpg 1_864.jpg 0_1248.jpg 0_284.jpg 0_670.jpg 1_1056.jpg 1_1442.jpg 1_479.jpg 1_865.jpg 0_1249.jpg 0_285.jpg 0_671.jpg 1_1057.jpg 1_1443.jpg 1_47.jpg 1_866.jpg 0_124.jpg 0_286.jpg 0_672.jpg 1_1058.jpg 1_1444.jpg 1_480.jpg 1_867.jpg 0_1250.jpg 0_287.jpg 0_673.jpg 1_1059.jpg 1_1445.jpg 1_481.jpg 1_868.jpg 0_1251.jpg 0_288.jpg 0_674.jpg 1_105.jpg 1_1446.jpg 1_482.jpg 1_869.jpg 0_1252.jpg 0_289.jpg 0_675.jpg 1_1060.jpg 1_1447.jpg 1_483.jpg 1_86.jpg 0_1253.jpg 0_28.jpg 0_676.jpg 1_1061.jpg 1_1448.jpg 1_484.jpg 1_870.jpg 0_1254.jpg 0_290.jpg 0_677.jpg 1_1062.jpg 1_1449.jpg 1_485.jpg 1_871.jpg 0_1255.jpg 0_291.jpg 0_678.jpg 1_1063.jpg 1_144.jpg 1_486.jpg 1_872.jpg 0_1256.jpg 0_292.jpg 0_679.jpg 1_1064.jpg 1_1450.jpg 1_487.jpg 1_873.jpg 0_1257.jpg 0_293.jpg 0_67.jpg 1_1065.jpg 1_1451.jpg 1_488.jpg 1_874.jpg 0_1258.jpg 0_294.jpg 0_680.jpg 1_1066.jpg 1_1452.jpg 1_489.jpg 1_875.jpg 0_1259.jpg 0_295.jpg 0_681.jpg 1_1067.jpg 1_1453.jpg 1_48.jpg 1_876.jpg 0_125.jpg 0_296.jpg 0_682.jpg 1_1068.jpg 1_1454.jpg 1_490.jpg 1_877.jpg 0_1260.jpg 0_297.jpg 0_683.jpg 1_1069.jpg 1_1455.jpg 1_491.jpg 1_878.jpg 0_1261.jpg 0_298.jpg 0_684.jpg 1_106.jpg 1_1456.jpg 1_492.jpg 1_879.jpg 0_1262.jpg 0_299.jpg 0_685.jpg 1_1070.jpg 1_1457.jpg 1_493.jpg 1_87.jpg 0_1263.jpg 0_29.jpg 0_686.jpg 1_1071.jpg 1_1458.jpg 1_494.jpg 1_880.jpg 0_1264.jpg 0_2.jpg 0_687.jpg 1_1072.jpg 1_1459.jpg 1_495.jpg 1_881.jpg 0_1265.jpg 0_300.jpg 0_688.jpg 1_1073.jpg 1_145.jpg 1_496.jpg 1_882.jpg 0_1266.jpg 0_301.jpg 0_689.jpg 1_1074.jpg 1_1460.jpg 1_497.jpg 1_883.jpg 0_1267.jpg 0_302.jpg 0_68.jpg 1_1075.jpg 1_1461.jpg 1_498.jpg 1_884.jpg 0_1268.jpg 0_303.jpg 0_690.jpg 1_1076.jpg 1_1462.jpg 1_499.jpg 1_885.jpg 0_1269.jpg 0_304.jpg 0_691.jpg 1_1077.jpg 1_1463.jpg 1_49.jpg 1_886.jpg 0_126.jpg 0_305.jpg 0_692.jpg 1_1078.jpg 1_1464.jpg 1_4.jpg 1_887.jpg 0_1270.jpg 0_306.jpg 0_693.jpg 1_1079.jpg 1_1465.jpg 1_500.jpg 1_888.jpg 0_1271.jpg 0_307.jpg 0_694.jpg 1_107.jpg 1_1466.jpg 1_501.jpg 1_889.jpg 0_1272.jpg 0_308.jpg 0_695.jpg 1_1080.jpg 1_1467.jpg 1_502.jpg 1_88.jpg 0_1273.jpg 0_309.jpg 0_696.jpg 1_1081.jpg 1_1468.jpg 1_503.jpg 1_890.jpg 0_1274.jpg 0_30.jpg 0_697.jpg 1_1082.jpg 1_1469.jpg 1_504.jpg 1_891.jpg 0_1275.jpg 0_310.jpg 0_698.jpg 1_1083.jpg 1_146.jpg 1_505.jpg 1_892.jpg 0_1276.jpg 0_311.jpg 0_699.jpg 1_1084.jpg 1_1470.jpg 1_506.jpg 1_893.jpg 0_1277.jpg 0_312.jpg 0_69.jpg 1_1085.jpg 1_1471.jpg 1_507.jpg 1_894.jpg 0_1278.jpg 0_313.jpg 0_6.jpg 1_1086.jpg 1_1472.jpg 1_508.jpg 1_895.jpg 0_1279.jpg 0_314.jpg 0_700.jpg 1_1087.jpg 1_1473.jpg 1_509.jpg 1_896.jpg 0_127.jpg 0_315.jpg 0_701.jpg 1_1088.jpg 1_1474.jpg 1_50.jpg 1_897.jpg 0_1280.jpg 0_316.jpg 0_702.jpg 1_1089.jpg 1_1475.jpg 1_510.jpg 1_898.jpg 0_1281.jpg 0_317.jpg 0_703.jpg 1_108.jpg 1_1476.jpg 1_511.jpg 1_899.jpg 0_1282.jpg 0_318.jpg 0_704.jpg 1_1090.jpg 1_1477.jpg 1_512.jpg 1_89.jpg 0_1283.jpg 0_319.jpg 0_705.jpg 1_1091.jpg 1_1478.jpg 1_513.jpg 1_8.jpg 0_1284.jpg 0_31.jpg 0_706.jpg 1_1092.jpg 1_1479.jpg 1_514.jpg 1_900.jpg 0_1285.jpg 0_320.jpg 0_707.jpg 1_1093.jpg 1_147.jpg 1_515.jpg 1_901.jpg 0_1286.jpg 0_321.jpg 0_708.jpg 1_1094.jpg 1_1480.jpg 1_516.jpg 1_902.jpg 0_1287.jpg 0_322.jpg 0_709.jpg 1_1095.jpg 1_1481.jpg 1_517.jpg 1_903.jpg 0_1288.jpg 0_323.jpg 0_70.jpg 1_1096.jpg 1_1482.jpg 1_518.jpg 1_904.jpg 0_1289.jpg 0_324.jpg 0_710.jpg 1_1097.jpg 1_1483.jpg 1_519.jpg 1_905.jpg 0_128.jpg 0_325.jpg 0_711.jpg 1_1098.jpg 1_1484.jpg 1_51.jpg 1_906.jpg 0_1290.jpg 0_326.jpg 0_712.jpg 1_1099.jpg 1_1485.jpg 1_520.jpg 1_907.jpg 0_1291.jpg 0_327.jpg 0_713.jpg 1_109.jpg 1_1486.jpg 1_521.jpg 1_908.jpg 0_1292.jpg 0_328.jpg 0_714.jpg 1_10.jpg 1_1487.jpg 1_522.jpg 1_909.jpg 0_1293.jpg 0_329.jpg 0_715.jpg 1_1100.jpg 1_1488.jpg 1_523.jpg 1_90.jpg 0_1294.jpg 0_32.jpg 0_716.jpg 1_1101.jpg 1_1489.jpg 1_524.jpg 1_910.jpg 0_1295.jpg 0_330.jpg 0_717.jpg 1_1102.jpg 1_148.jpg 1_525.jpg 1_911.jpg 0_1296.jpg 0_331.jpg 0_718.jpg 1_1103.jpg 1_1490.jpg 1_526.jpg 1_912.jpg 0_1297.jpg 0_332.jpg 0_719.jpg 1_1104.jpg 1_1491.jpg 1_527.jpg 1_913.jpg 0_1298.jpg 0_333.jpg 0_71.jpg 1_1105.jpg 1_1492.jpg 1_528.jpg 1_914.jpg 0_1299.jpg 0_334.jpg 0_720.jpg 1_1106.jpg 1_1493.jpg 1_529.jpg 1_915.jpg 0_129.jpg 0_335.jpg 0_721.jpg 1_1107.jpg 1_1494.jpg 1_52.jpg 1_916.jpg 0_12.jpg 0_336.jpg 0_722.jpg 1_1108.jpg 1_1495.jpg 1_530.jpg 1_917.jpg 0_1300.jpg 0_337.jpg 0_723.jpg 1_1109.jpg 1_1496.jpg 1_531.jpg 1_918.jpg 0_1301.jpg 0_338.jpg 0_724.jpg 1_110.jpg 1_1497.jpg 1_532.jpg 1_919.jpg 0_1302.jpg 0_339.jpg 0_725.jpg 1_1110.jpg 1_1498.jpg 1_533.jpg 1_91.jpg 0_1303.jpg 0_33.jpg 0_726.jpg 1_1111.jpg 1_1499.jpg 1_534.jpg 1_920.jpg 0_1304.jpg 0_340.jpg 0_727.jpg 1_1112.jpg 1_149.jpg 1_535.jpg 1_921.jpg 0_1305.jpg 0_341.jpg 0_728.jpg 1_1113.jpg 1_14.jpg 1_536.jpg 1_922.jpg 0_1306.jpg 0_342.jpg 0_729.jpg 1_1114.jpg 1_150.jpg 1_537.jpg 1_923.jpg 0_1307.jpg 0_343.jpg 0_72.jpg 1_1115.jpg 1_151.jpg 1_538.jpg 1_924.jpg 0_1308.jpg 0_344.jpg 0_730.jpg 1_1116.jpg 1_152.jpg 1_539.jpg 1_925.jpg 0_1309.jpg 0_345.jpg 0_731.jpg 1_1117.jpg 1_153.jpg 1_53.jpg 1_926.jpg 0_130.jpg 0_346.jpg 0_732.jpg 1_1118.jpg 1_154.jpg 1_540.jpg 1_927.jpg 0_1310.jpg 0_347.jpg 0_733.jpg 1_1119.jpg 1_155.jpg 1_541.jpg 1_928.jpg 0_1311.jpg 0_348.jpg 0_734.jpg 1_111.jpg 1_156.jpg 1_542.jpg 1_929.jpg 0_1312.jpg 0_349.jpg 0_735.jpg 1_1120.jpg 1_157.jpg 1_543.jpg 1_92.jpg 0_1313.jpg 0_34.jpg 0_736.jpg 1_1121.jpg 1_158.jpg 1_544.jpg 1_930.jpg 0_1314.jpg 0_350.jpg 0_737.jpg 1_1122.jpg 1_159.jpg 1_545.jpg 1_931.jpg 0_1315.jpg 0_351.jpg 0_738.jpg 1_1123.jpg 1_15.jpg 1_546.jpg 1_932.jpg 0_1316.jpg 0_352.jpg 0_739.jpg 1_1124.jpg 1_160.jpg 1_547.jpg 1_933.jpg 0_1317.jpg 0_353.jpg 0_73.jpg 1_1125.jpg 1_161.jpg 1_548.jpg 1_934.jpg 0_1318.jpg 0_354.jpg 0_740.jpg 1_1126.jpg 1_162.jpg 1_549.jpg 1_935.jpg 0_1319.jpg 0_355.jpg 0_741.jpg 1_1127.jpg 1_163.jpg 1_54.jpg 1_936.jpg 0_131.jpg 0_356.jpg 0_742.jpg 1_1128.jpg 1_164.jpg 1_550.jpg 1_937.jpg 0_1320.jpg 0_357.jpg 0_743.jpg 1_1129.jpg 1_165.jpg 1_551.jpg 1_938.jpg 0_1321.jpg 0_358.jpg 0_744.jpg 1_112.jpg 1_166.jpg 1_552.jpg 1_939.jpg 0_1322.jpg 0_359.jpg 0_745.jpg 1_1130.jpg 1_167.jpg 1_553.jpg 1_93.jpg 0_1323.jpg 0_35.jpg 0_746.jpg 1_1131.jpg 1_168.jpg 1_554.jpg 1_940.jpg 0_1324.jpg 0_360.jpg 0_747.jpg 1_1132.jpg 1_169.jpg 1_555.jpg 1_941.jpg 0_1325.jpg 0_361.jpg 0_748.jpg 1_1133.jpg 1_16.jpg 1_556.jpg 1_942.jpg 0_1326.jpg 0_362.jpg 0_749.jpg 1_1134.jpg 1_170.jpg 1_557.jpg 1_943.jpg 0_1327.jpg 0_363.jpg 0_74.jpg 1_1135.jpg 1_171.jpg 1_558.jpg 1_944.jpg 0_1328.jpg 0_364.jpg 0_750.jpg 1_1136.jpg 1_172.jpg 1_559.jpg 1_945.jpg 0_1329.jpg 0_365.jpg 0_751.jpg 1_1137.jpg 1_173.jpg 1_55.jpg 1_946.jpg 0_132.jpg 0_366.jpg 0_752.jpg 1_1138.jpg 1_174.jpg 1_560.jpg 1_947.jpg 0_1330.jpg 0_367.jpg 0_753.jpg 1_1139.jpg 1_175.jpg 1_561.jpg 1_948.jpg 0_1331.jpg 0_368.jpg 0_754.jpg 1_113.jpg 1_176.jpg 1_562.jpg 1_949.jpg 0_1332.jpg 0_369.jpg 0_755.jpg 1_1140.jpg 1_177.jpg 1_563.jpg 1_94.jpg 0_1333.jpg 0_36.jpg 0_756.jpg 1_1141.jpg 1_178.jpg 1_564.jpg 1_950.jpg 0_1334.jpg 0_370.jpg 0_757.jpg 1_1142.jpg 1_179.jpg 1_565.jpg 1_951.jpg 0_1335.jpg 0_371.jpg 0_758.jpg 1_1143.jpg 1_17.jpg 1_566.jpg 1_952.jpg 0_1336.jpg 0_372.jpg 0_759.jpg 1_1144.jpg 1_180.jpg 1_567.jpg 1_953.jpg 0_1337.jpg 0_373.jpg 0_75.jpg 1_1145.jpg 1_181.jpg 1_568.jpg 1_954.jpg 0_1338.jpg 0_374.jpg 0_760.jpg 1_1146.jpg 1_182.jpg 1_569.jpg 1_955.jpg 0_1339.jpg 0_375.jpg 0_761.jpg 1_1147.jpg 1_183.jpg 1_56.jpg 1_956.jpg 0_133.jpg 0_376.jpg 0_762.jpg 1_1148.jpg 1_184.jpg 1_570.jpg 1_957.jpg 0_1340.jpg 0_377.jpg 0_763.jpg 1_1149.jpg 1_185.jpg 1_571.jpg 1_958.jpg 0_1341.jpg 0_378.jpg 0_764.jpg 1_114.jpg 1_186.jpg 1_572.jpg 1_959.jpg 0_1342.jpg 0_379.jpg 0_765.jpg 1_1150.jpg 1_187.jpg 1_573.jpg 1_95.jpg 0_1343.jpg 0_37.jpg 0_766.jpg 1_1151.jpg 1_188.jpg 1_574.jpg 1_960.jpg 0_1344.jpg 0_380.jpg 0_767.jpg 1_1152.jpg 1_189.jpg 1_575.jpg 1_961.jpg 0_1345.jpg 0_381.jpg 0_768.jpg 1_1153.jpg 1_18.jpg 1_576.jpg 1_962.jpg 0_1346.jpg 0_382.jpg 0_769.jpg 1_1154.jpg 1_190.jpg 1_577.jpg 1_963.jpg 0_1347.jpg 0_383.jpg 0_76.jpg 1_1155.jpg 1_191.jpg 1_578.jpg 1_964.jpg 0_1348.jpg 0_384.jpg 0_770.jpg 1_1156.jpg 1_192.jpg 1_579.jpg 1_965.jpg 0_1349.jpg 0_385.jpg 0_771.jpg 1_1157.jpg 1_193.jpg 1_57.jpg 1_966.jpg 0_134.jpg 0_386.jpg 0_772.jpg 1_1158.jpg 1_194.jpg 1_580.jpg 1_967.jpg 0_1350.jpg 0_387.jpg 0_773.jpg 1_1159.jpg 1_195.jpg 1_581.jpg 1_968.jpg 0_1351.jpg 0_388.jpg 0_774.jpg 1_115.jpg 1_196.jpg 1_582.jpg 1_969.jpg 0_1352.jpg 0_389.jpg 0_775.jpg 1_1160.jpg 1_197.jpg 1_583.jpg 1_96.jpg 0_1353.jpg 0_38.jpg 0_776.jpg 1_1161.jpg 1_198.jpg 1_584.jpg 1_970.jpg 0_1354.jpg 0_390.jpg 0_777.jpg 1_1162.jpg 1_199.jpg 1_585.jpg 1_971.jpg 0_1355.jpg 0_391.jpg 0_778.jpg 1_1163.jpg 1_19.jpg 1_586.jpg 1_972.jpg 0_1356.jpg 0_392.jpg 0_779.jpg 1_1164.jpg 1_1.jpg 1_587.jpg 1_973.jpg 0_1357.jpg 0_393.jpg 0_77.jpg 1_1165.jpg 1_200.jpg 1_588.jpg 1_974.jpg 0_1358.jpg 0_394.jpg 0_780.jpg 1_1166.jpg 1_201.jpg 1_589.jpg 1_975.jpg 0_1359.jpg 0_395.jpg 0_781.jpg 1_1167.jpg 1_202.jpg 1_58.jpg 1_976.jpg 0_135.jpg 0_396.jpg 0_782.jpg 1_1168.jpg 1_203.jpg 1_590.jpg 1_977.jpg 0_1360.jpg 0_397.jpg 0_783.jpg 1_1169.jpg 1_204.jpg 1_591.jpg 1_978.jpg 0_1361.jpg 0_398.jpg 0_784.jpg 1_116.jpg 1_205.jpg 1_592.jpg 1_979.jpg 0_1362.jpg 0_399.jpg 0_785.jpg 1_1170.jpg 1_206.jpg 1_593.jpg 1_97.jpg 0_1363.jpg 0_39.jpg 0_786.jpg 1_1171.jpg 1_207.jpg 1_594.jpg 1_980.jpg 0_1364.jpg 0_3.jpg 0_787.jpg 1_1172.jpg 1_208.jpg 1_595.jpg 1_981.jpg 0_1365.jpg 0_400.jpg 0_788.jpg 1_1173.jpg 1_209.jpg 1_596.jpg 1_982.jpg 0_1366.jpg 0_401.jpg 0_789.jpg 1_1174.jpg 1_20.jpg 1_597.jpg 1_983.jpg 0_1367.jpg 0_402.jpg 0_78.jpg 1_1175.jpg 1_210.jpg 1_598.jpg 1_984.jpg 0_1368.jpg 0_403.jpg 0_790.jpg 1_1176.jpg 1_211.jpg 1_599.jpg 1_985.jpg 0_1369.jpg 0_404.jpg 0_791.jpg 1_1177.jpg 1_212.jpg 1_59.jpg 1_986.jpg 0_136.jpg 0_405.jpg 0_792.jpg 1_1178.jpg 1_213.jpg 1_5.jpg 1_987.jpg 0_1370.jpg 0_406.jpg 0_793.jpg 1_1179.jpg 1_214.jpg 1_600.jpg 1_988.jpg 0_1371.jpg 0_407.jpg 0_794.jpg 1_117.jpg 1_215.jpg 1_601.jpg 1_989.jpg 0_1372.jpg 0_408.jpg 0_795.jpg 1_1180.jpg 1_216.jpg 1_602.jpg 1_98.jpg 0_1373.jpg 0_409.jpg 0_796.jpg 1_1181.jpg 1_217.jpg 1_603.jpg 1_990.jpg 0_1374.jpg 0_40.jpg 0_797.jpg 1_1182.jpg 1_218.jpg 1_604.jpg 1_991.jpg 0_1375.jpg 0_410.jpg 0_798.jpg 1_1183.jpg 1_219.jpg 1_605.jpg 1_992.jpg 0_1376.jpg 0_411.jpg 0_799.jpg 1_1184.jpg 1_21.jpg 1_606.jpg 1_993.jpg 0_1377.jpg 0_412.jpg 0_79.jpg 1_1185.jpg 1_220.jpg 1_607.jpg 1_994.jpg 0_1378.jpg 0_413.jpg 0_7.jpg 1_1186.jpg 1_221.jpg 1_608.jpg 1_995.jpg 0_1379.jpg 0_414.jpg 0_800.jpg 1_1187.jpg 1_222.jpg 1_609.jpg 1_996.jpg 0_137.jpg 0_415.jpg 0_801.jpg 1_1188.jpg 1_223.jpg 1_60.jpg 1_997.jpg 0_1380.jpg 0_416.jpg 0_802.jpg 1_1189.jpg 1_224.jpg 1_610.jpg 1_998.jpg 0_1381.jpg 0_417.jpg 0_803.jpg 1_118.jpg 1_225.jpg 1_611.jpg 1_999.jpg 0_1382.jpg 0_418.jpg 0_804.jpg 1_1190.jpg 1_226.jpg 1_612.jpg 1_99.jpg 0_1383.jpg 0_419.jpg 0_805.jpg 1_1191.jpg 1_227.jpg 1_613.jpg 1_9.jpg 0_1384.jpg 0_41.jpg 0_806.jpg 1_1192.jpg 1_228.jpg 1_614.jpg 0_1385.jpg 0_420.jpg 0_807.jpg 1_1193.jpg 1_229.jpg 1_615.jpg 0_1386.jpg 0_421.jpg 0_808.jpg 1_1194.jpg 1_22.jpg 1_616.jpg ! mv Food - 5 K /* . # look at an image for fun plt . imshow ( image . load_img ( 'training/0_808.jpg' )) plt . show () # Food images start with 1, non-food images start with 0 plt . imshow ( image . load_img ( 'training/1_616.jpg' )) plt . show () ! mkdir data # Make directories to store the data Keras-style ! mkdir data / train ! mkdir data / test ! mkdir data / train / nonfood ! mkdir data / train / food ! mkdir data / test / nonfood ! mkdir data / test / food # Move the images # Note: we will consider 'training' to be the train set # 'validation' folder will be the test set # ignore the 'evaluation' set ! mv training / 0 *. jpg data / train / nonfood ! mv training / 1 *. jpg data / train / food ! mv validation / 0 *. jpg data / test / nonfood ! mv validation / 1 *. jpg data / test / food train_path = 'data/train' valid_path = 'data/test' # These images are pretty big and of different sizes # Let's load them all in as the same (smaller) size IMAGE_SIZE = [ 200 , 200 ] # useful for getting number of files image_files = glob ( train_path + '/*/*.jpg' ) valid_image_files = glob ( valid_path + '/*/*.jpg' ) # useful for getting number of classes folders = glob ( train_path + '/*' ) folders ['data/train/nonfood', 'data/train/food'] # look at an image for fun plt . imshow ( image . load_img ( np . random . choice ( image_files ))) plt . show () ptm = PretrainedModel ( input_shape = IMAGE_SIZE + [ 3 ], weights = 'imagenet' , include_top = False ) Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 58892288/58889256 [==============================] - 6s 0us/step # freeze pretrained model weights ptm . trainable = False # map the data into feature vectors # Keras image data generator returns classes one-hot encoded K = len ( folders ) # number of classes x = Flatten ()( ptm . output ) x = Dense ( K , activation = 'softmax' )( x ) # create a model object model = Model ( inputs = ptm . input , outputs = x ) # view the structure of the model model . summary () Model: \"model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 200, 200, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 200, 200, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 200, 200, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 100, 100, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 100, 100, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 100, 100, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 50, 50, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 50, 50, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 50, 50, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 50, 50, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 25, 25, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 25, 25, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 25, 25, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 25, 25, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 12, 12, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 12, 12, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 6, 6, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 18432) 0 _________________________________________________________________ dense (Dense) (None, 2) 36866 ================================================================= Total params: 14,751,554 Trainable params: 36,866 Non-trainable params: 14,714,688 _________________________________________________________________ # create an instance of ImageDataGenerator gen_train = ImageDataGenerator ( rotation_range = 20 , width_shift_range = 0.1 , height_shift_range = 0.1 , shear_range = 0.1 , zoom_range = 0.2 , horizontal_flip = True , preprocessing_function = preprocess_input ) gen_test = ImageDataGenerator ( preprocessing_function = preprocess_input ) batch_size = 128 # create generators train_generator = gen_train . flow_from_directory ( train_path , shuffle = True , target_size = IMAGE_SIZE , batch_size = batch_size , ) valid_generator = gen_test . flow_from_directory ( valid_path , target_size = IMAGE_SIZE , batch_size = batch_size , ) Found 3000 images belonging to 2 classes. Found 1000 images belonging to 2 classes. model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit the model r = model . fit_generator ( train_generator , validation_data = valid_generator , epochs = 10 , steps_per_epoch = int ( np . ceil ( len ( image_files ) / batch_size )), validation_steps = int ( np . ceil ( len ( valid_image_files ) / batch_size )), ) Epoch 1/10 WARNING:tensorflow:From /tensorflow-2.0.0-rc0/python3.6/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where 24/24 [==============================] - 145s 6s/step - loss: 0.9769 - accuracy: 0.9183 - val_loss: 0.2264 - val_accuracy: 0.9790 Epoch 2/10 24/24 [==============================] - 103s 4s/step - loss: 0.4751 - accuracy: 0.9643 - val_loss: 0.2551 - val_accuracy: 0.9780 Epoch 3/10 24/24 [==============================] - 104s 4s/step - loss: 0.4594 - accuracy: 0.9667 - val_loss: 0.3841 - val_accuracy: 0.9730 Epoch 4/10 24/24 [==============================] - 104s 4s/step - loss: 0.3127 - accuracy: 0.9773 - val_loss: 0.2724 - val_accuracy: 0.9790 Epoch 5/10 24/24 [==============================] - 104s 4s/step - loss: 0.2851 - accuracy: 0.9780 - val_loss: 0.1958 - val_accuracy: 0.9820 Epoch 6/10 24/24 [==============================] - 103s 4s/step - loss: 0.2695 - accuracy: 0.9783 - val_loss: 0.2905 - val_accuracy: 0.9790 Epoch 7/10 24/24 [==============================] - 103s 4s/step - loss: 0.3040 - accuracy: 0.9770 - val_loss: 0.3226 - val_accuracy: 0.9760 Epoch 8/10 24/24 [==============================] - 103s 4s/step - loss: 0.2572 - accuracy: 0.9810 - val_loss: 0.2654 - val_accuracy: 0.9820 Epoch 9/10 24/24 [==============================] - 104s 4s/step - loss: 0.1766 - accuracy: 0.9867 - val_loss: 0.2159 - val_accuracy: 0.9860 Epoch 10/10 24/24 [==============================] - 104s 4s/step - loss: 0.1946 - accuracy: 0.9840 - val_loss: 0.3092 - val_accuracy: 0.9740 # create a 2nd train generator which does not use data augmentation # to get the true train accuracy train_generator2 = gen_test . flow_from_directory ( train_path , target_size = IMAGE_SIZE , batch_size = batch_size , ) model . evaluate_generator ( train_generator2 , steps = int ( np . ceil ( len ( image_files ) / batch_size ))) Found 3000 images belonging to 2 classes. [0.09657006577390703, 0.993] # loss plt . plot ( r . history [ 'loss' ], label = 'train loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val loss' ) plt . legend () plt . show () # accuracies plt . plot ( r . history [ 'accuracy' ], label = 'train acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val acc' ) plt . legend () plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Transfer Learning with Data Augmentation"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/","text":"================ by Jawad Haider 00 - Numpy Arrays \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy 1.1 Installation Instructions 1.2 Using NumPy 2 NumPy Arrays 2.1 Creating NumPy Arrays 2.1.1 1. From a Python List 2.1.2 2. Built-in Methods 2.1.3 arange 2.1.4 zeros and ones 2.1.5 linspace 2.1.6 eye 2.2 Random 2.2.1 1. rand 2.2.2 2. randn 2.2.3 3. randint 2.2.4 4. seed 2.3 Array Attributes and Methods 2.3.1 1. Reshape 2.3.2 2. max, min, argmax, argmin 2.3.3 3. Shape 2.3.4 4. dtype 3 Great Job! Thats the end of this part. NumPy \u00b6 NumPy is a powerful linear algebra library for Python. What makes it so important is that almost all of the libraries in the PyData ecosystem (pandas, scipy, scikit-learn, etc.) rely on NumPy as one of their main building blocks. Plus we will use it to generate data for our analysis examples later on! NumPy is also incredibly fast, as it has bindings to C libraries. For more info on why you would want to use arrays instead of lists, check out this great StackOverflow post . We will only learn the basics of NumPy. To get started we need to install it! Installation Instructions \u00b6 NumPy is already included in your environment! You are good to go if you are using the course environment! For those not using the provided environment: It is highly recommended you install Python using the Anaconda distribution to make sure all underlying dependencies (such as Linear Algebra libraries) all sync up with the use of a conda install. If you have Anaconda, install NumPy by going to your terminal or command prompt and typing: conda install numpy If you do not have Anaconda and can not install it, please refer to Numpy\u2019s official documentation on various installation instructions. Using NumPy \u00b6 Once you\u2019ve installed NumPy you can import it as a library: import numpy as np NumPy has many built-in functions and capabilities. We won\u2019t cover them all but instead we will focus on some of the most important aspects of NumPy: vectors, arrays, matrices and number generation. Let\u2019s start by discussing arrays. NumPy Arrays \u00b6 NumPy arrays are the main way we will use NumPy throughout the course. NumPy arrays essentially come in two flavors: vectors and matrices. Vectors are strictly 1-dimensional (1D) arrays and matrices are 2D (but you should note a matrix can still have only one row or one column). Let\u2019s begin our introduction by exploring how to create NumPy arrays. Creating NumPy Arrays \u00b6 1. From a Python List \u00b6 We can create an array by directly converting a list or list of lists: my_list = [ 1 , 2 , 3 ] my_list [1, 2, 3] np . array ( my_list ) array([1, 2, 3]) my_matrix = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] my_matrix [[1, 2, 3], [4, 5, 6], [7, 8, 9]] np . array ( my_matrix ) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 2. Built-in Methods \u00b6 There are lots of built-in ways to generate arrays. arange \u00b6 Return evenly spaced values within a given interval.[ Numpy ndarray arange ] np . arange ( 0 , 10 ) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) np . arange ( 0 , 11 , 2 ) array([ 0, 2, 4, 6, 8, 10]) zeros and ones \u00b6 Generate arrays of zeros or ones.[ Numpy ndarray zeros ] np.zeros(3) np . zeros (( 5 , 5 )) array([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) np . ones ( 3 ) array([1., 1., 1.]) np . ones (( 3 , 3 )) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) linspace \u00b6 Return evenly spaced numbers over a specified interval.[ Numpy ndarray linspace ] np . linspace ( 0 , 10 , 3 ) array([ 0., 5., 10.]) np . linspace ( 0 , 5 , 20 ) array([0. , 0.26315789, 0.52631579, 0.78947368, 1.05263158, 1.31578947, 1.57894737, 1.84210526, 2.10526316, 2.36842105, 2.63157895, 2.89473684, 3.15789474, 3.42105263, 3.68421053, 3.94736842, 4.21052632, 4.47368421, 4.73684211, 5. ]) Note that .linspace() includes the stop value. To obtain an array of common fractions, increase the number of items: np . linspace ( 0 , 5 , 21 ) array([0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. , 2.25, 2.5 , 2.75, 3. , 3.25, 3.5 , 3.75, 4. , 4.25, 4.5 , 4.75, 5. ]) eye \u00b6 Creates an identity matrix [ Numpy eye ] np . eye ( 4 ) array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) Random \u00b6 Numpy also has lots of ways to create random number arrays. Here we will go through some of the most used methods from random module 1. rand \u00b6 Creates an array of the given shape and populates it with random samples from a uniform distribution over [0, 1) . [ Random rand ] np . random . rand ( 2 ) array([0.69647666, 0.14395438]) np . random . rand ( 5 , 5 ) array([[0.9112553 , 0.75849901, 0.43392287, 0.4134459 , 0.10902179], [0.66881652, 0.21265267, 0.21783956, 0.08716564, 0.46147918], [0.16064897, 0.38241433, 0.50076915, 0.58926492, 0.69837196], [0.88502465, 0.2996012 , 0.49291933, 0.75316852, 0.29998398], [0.42345042, 0.57034504, 0.94797283, 0.70571464, 0.35788149]]) 2. randn \u00b6 Returns a sample (or samples) from the \u201cstandard normal\u201d distribution [\u03c3 = 1]. Unlike rand which is uniform, values closer to zero are more likely to appear. [ Radnom randn ] np . random . randn ( 2 ) array([-0.55673554, -0.08515858]) np . random . randn ( 5 , 5 ) array([[ 0.83041645, -1.22369138, 0.258011 , 0.90984287, -0.48702078], [-0.88539528, -0.54034218, -0.39928196, 0.85910869, -0.36305332], [ 0.132046 , -1.28709664, 0.49352402, 0.80293611, 0.2601146 ], [ 0.74912365, 0.16013944, 0.39345536, -0.52355146, 1.0536796 ], [ 0.00293273, -0.14715505, -1.22460234, -0.65347358, -0.31514422]]) 3. randint \u00b6 Returns random integers from low (inclusive) to high (exclusive). [ Random randint ] np . random . randint ( 1 , 100 ) 42 np . random . randint ( 1 , 100 , 10 ) array([33, 26, 51, 78, 89, 15, 42, 68, 14, 62]) 4. seed \u00b6 Can be used to set the random state, so that the same \u201crandom\u201d results can be reproduced. [ Numpy ndarray random ] np . random . seed ( 42 ) np . random . rand ( 4 ) array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) np . random . seed ( 42 ) np . random . rand ( 4 ) array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) Array Attributes and Methods \u00b6 Let\u2019s discuss some useful attributes and methods for an array: In particular, the reshape attribute and max,min,argmax, argmin, shape & the dytpe methods Let\u2019s first create two numpy arrays to experiment with :) arr = np . arange ( 25 ) ranarr = np . random . randint ( 0 , 50 , 10 ) arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]) ranarr array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) 1. Reshape \u00b6 Returns an array containing the same data with a new shape.[ Numpy ndarray reshape ] arr . reshape ( 5 , 5 ) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]) 2. max, min, argmax, argmin \u00b6 These are useful methods for finding max or min values. Or to find their index locations using argmin or argmax ranarr array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) ranarr . max () 39 ranarr . argmax () 7 ranarr . min () 2 ranarr . argmin () 9 3. Shape \u00b6 Shape is an attribute that arrays have (not a method):[ Numpy ndarray Shape ] # Vector arr . shape (25,) # Notice the two sets of brackets arr . reshape ( 1 , 25 ) array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) arr . reshape ( 1 , 25 ) . shape (1, 25) arr . reshape ( 25 , 1 ) array([[ 0], [ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]]) arr . reshape ( 25 , 1 ) . shape (25, 1) 4. dtype \u00b6 You can also grab the data type of the object in the array:[ Numpy ndarray dtype ] arr . dtype dtype('int64') arr2 = np . array ([ 1.2 , 3.4 , 5.6 ]) arr2 . dtype dtype('float64') Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"00 NumPy Arrays"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#00-numpy-arrays","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy 1.1 Installation Instructions 1.2 Using NumPy 2 NumPy Arrays 2.1 Creating NumPy Arrays 2.1.1 1. From a Python List 2.1.2 2. Built-in Methods 2.1.3 arange 2.1.4 zeros and ones 2.1.5 linspace 2.1.6 eye 2.2 Random 2.2.1 1. rand 2.2.2 2. randn 2.2.3 3. randint 2.2.4 4. seed 2.3 Array Attributes and Methods 2.3.1 1. Reshape 2.3.2 2. max, min, argmax, argmin 2.3.3 3. Shape 2.3.4 4. dtype 3 Great Job! Thats the end of this part.","title":"00 - Numpy Arrays"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#numpy","text":"NumPy is a powerful linear algebra library for Python. What makes it so important is that almost all of the libraries in the PyData ecosystem (pandas, scipy, scikit-learn, etc.) rely on NumPy as one of their main building blocks. Plus we will use it to generate data for our analysis examples later on! NumPy is also incredibly fast, as it has bindings to C libraries. For more info on why you would want to use arrays instead of lists, check out this great StackOverflow post . We will only learn the basics of NumPy. To get started we need to install it!","title":"NumPy"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#installation-instructions","text":"NumPy is already included in your environment! You are good to go if you are using the course environment! For those not using the provided environment: It is highly recommended you install Python using the Anaconda distribution to make sure all underlying dependencies (such as Linear Algebra libraries) all sync up with the use of a conda install. If you have Anaconda, install NumPy by going to your terminal or command prompt and typing: conda install numpy If you do not have Anaconda and can not install it, please refer to Numpy\u2019s official documentation on various installation instructions.","title":"Installation Instructions"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#using-numpy","text":"Once you\u2019ve installed NumPy you can import it as a library: import numpy as np NumPy has many built-in functions and capabilities. We won\u2019t cover them all but instead we will focus on some of the most important aspects of NumPy: vectors, arrays, matrices and number generation. Let\u2019s start by discussing arrays.","title":"Using NumPy"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#numpy-arrays","text":"NumPy arrays are the main way we will use NumPy throughout the course. NumPy arrays essentially come in two flavors: vectors and matrices. Vectors are strictly 1-dimensional (1D) arrays and matrices are 2D (but you should note a matrix can still have only one row or one column). Let\u2019s begin our introduction by exploring how to create NumPy arrays.","title":"NumPy Arrays"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#creating-numpy-arrays","text":"","title":"Creating NumPy Arrays"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#1-from-a-python-list","text":"We can create an array by directly converting a list or list of lists: my_list = [ 1 , 2 , 3 ] my_list [1, 2, 3] np . array ( my_list ) array([1, 2, 3]) my_matrix = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] my_matrix [[1, 2, 3], [4, 5, 6], [7, 8, 9]] np . array ( my_matrix ) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])","title":"1. From a Python List"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#2-built-in-methods","text":"There are lots of built-in ways to generate arrays.","title":"2. Built-in Methods"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#arange","text":"Return evenly spaced values within a given interval.[ Numpy ndarray arange ] np . arange ( 0 , 10 ) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) np . arange ( 0 , 11 , 2 ) array([ 0, 2, 4, 6, 8, 10])","title":"arange"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#zeros-and-ones","text":"Generate arrays of zeros or ones.[ Numpy ndarray zeros ] np.zeros(3) np . zeros (( 5 , 5 )) array([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) np . ones ( 3 ) array([1., 1., 1.]) np . ones (( 3 , 3 )) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])","title":"zeros and ones"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#linspace","text":"Return evenly spaced numbers over a specified interval.[ Numpy ndarray linspace ] np . linspace ( 0 , 10 , 3 ) array([ 0., 5., 10.]) np . linspace ( 0 , 5 , 20 ) array([0. , 0.26315789, 0.52631579, 0.78947368, 1.05263158, 1.31578947, 1.57894737, 1.84210526, 2.10526316, 2.36842105, 2.63157895, 2.89473684, 3.15789474, 3.42105263, 3.68421053, 3.94736842, 4.21052632, 4.47368421, 4.73684211, 5. ]) Note that .linspace() includes the stop value. To obtain an array of common fractions, increase the number of items: np . linspace ( 0 , 5 , 21 ) array([0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. , 2.25, 2.5 , 2.75, 3. , 3.25, 3.5 , 3.75, 4. , 4.25, 4.5 , 4.75, 5. ])","title":"linspace"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#eye","text":"Creates an identity matrix [ Numpy eye ] np . eye ( 4 ) array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]])","title":"eye"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#random","text":"Numpy also has lots of ways to create random number arrays. Here we will go through some of the most used methods from random module","title":"Random"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#1-rand","text":"Creates an array of the given shape and populates it with random samples from a uniform distribution over [0, 1) . [ Random rand ] np . random . rand ( 2 ) array([0.69647666, 0.14395438]) np . random . rand ( 5 , 5 ) array([[0.9112553 , 0.75849901, 0.43392287, 0.4134459 , 0.10902179], [0.66881652, 0.21265267, 0.21783956, 0.08716564, 0.46147918], [0.16064897, 0.38241433, 0.50076915, 0.58926492, 0.69837196], [0.88502465, 0.2996012 , 0.49291933, 0.75316852, 0.29998398], [0.42345042, 0.57034504, 0.94797283, 0.70571464, 0.35788149]])","title":"1. rand"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#2-randn","text":"Returns a sample (or samples) from the \u201cstandard normal\u201d distribution [\u03c3 = 1]. Unlike rand which is uniform, values closer to zero are more likely to appear. [ Radnom randn ] np . random . randn ( 2 ) array([-0.55673554, -0.08515858]) np . random . randn ( 5 , 5 ) array([[ 0.83041645, -1.22369138, 0.258011 , 0.90984287, -0.48702078], [-0.88539528, -0.54034218, -0.39928196, 0.85910869, -0.36305332], [ 0.132046 , -1.28709664, 0.49352402, 0.80293611, 0.2601146 ], [ 0.74912365, 0.16013944, 0.39345536, -0.52355146, 1.0536796 ], [ 0.00293273, -0.14715505, -1.22460234, -0.65347358, -0.31514422]])","title":"2. randn"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#3-randint","text":"Returns random integers from low (inclusive) to high (exclusive). [ Random randint ] np . random . randint ( 1 , 100 ) 42 np . random . randint ( 1 , 100 , 10 ) array([33, 26, 51, 78, 89, 15, 42, 68, 14, 62])","title":"3. randint"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#4-seed","text":"Can be used to set the random state, so that the same \u201crandom\u201d results can be reproduced. [ Numpy ndarray random ] np . random . seed ( 42 ) np . random . rand ( 4 ) array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) np . random . seed ( 42 ) np . random . rand ( 4 ) array([0.37454012, 0.95071431, 0.73199394, 0.59865848])","title":"4. seed"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#array-attributes-and-methods","text":"Let\u2019s discuss some useful attributes and methods for an array: In particular, the reshape attribute and max,min,argmax, argmin, shape & the dytpe methods Let\u2019s first create two numpy arrays to experiment with :) arr = np . arange ( 25 ) ranarr = np . random . randint ( 0 , 50 , 10 ) arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]) ranarr array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2])","title":"Array Attributes and Methods"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#1-reshape","text":"Returns an array containing the same data with a new shape.[ Numpy ndarray reshape ] arr . reshape ( 5 , 5 ) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]])","title":"1. Reshape"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#2-max-min-argmax-argmin","text":"These are useful methods for finding max or min values. Or to find their index locations using argmin or argmax ranarr array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) ranarr . max () 39 ranarr . argmax () 7 ranarr . min () 2 ranarr . argmin () 9","title":"2. max, min, argmax, argmin"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#3-shape","text":"Shape is an attribute that arrays have (not a method):[ Numpy ndarray Shape ] # Vector arr . shape (25,) # Notice the two sets of brackets arr . reshape ( 1 , 25 ) array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) arr . reshape ( 1 , 25 ) . shape (1, 25) arr . reshape ( 25 , 1 ) array([[ 0], [ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]]) arr . reshape ( 25 , 1 ) . shape (25, 1)","title":"3. Shape"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#4-dtype","text":"You can also grab the data type of the object in the array:[ Numpy ndarray dtype ] arr . dtype dtype('int64') arr2 = np . array ([ 1.2 , 3.4 , 5.6 ]) arr2 . dtype dtype('float64')","title":"4. dtype"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/","text":"================ by Jawad Haider 01 - Numpy Indexing and Selection \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Indexing and Selection 1.1 Bracket Indexing and Selection 1.2 Broadcasting 1.3 Indexing a 2D array (matrices) 1.4 More Indexing Help 1.5 Conditional Selection 2 Great Job! Thats the end of this part. NumPy Indexing and Selection \u00b6 In this lecture we will discuss how to select elements or groups of elements from an array. import numpy as np #Creating sample array arr = np . arange ( 0 , 11 ) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) Bracket Indexing and Selection \u00b6 The simplest way to pick one or some elements of an array looks very similar to python lists: #Get a value at an index arr [ 8 ] 8 #Get values in a range arr [ 1 : 5 ] array([1, 2, 3, 4]) #Get values in a range arr [ 0 : 5 ] array([0, 1, 2, 3, 4]) Broadcasting \u00b6 NumPy arrays differ from normal Python lists because of their ability to broadcast. With lists, you can only reassign parts of a list with new parts of the same size and shape. That is, if you wanted to replace the first 5 elements in a list with a new value, you would have to pass in a new 5 element list. With NumPy arrays, you can broadcast a single value across a larger set of values: #Setting a value with index range (Broadcasting) arr [ 0 : 5 ] = 100 #Show arr array([100, 100, 100, 100, 100, 5, 6, 7, 8, 9, 10]) # Reset array, we'll see why I had to reset in a moment arr = np . arange ( 0 , 11 ) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) #Important notes on Slices slice_of_arr = arr [ 0 : 6 ] #Show slice slice_of_arr array([0, 1, 2, 3, 4, 5]) #Change Slice slice_of_arr [:] = 99 #Show Slice again slice_of_arr array([99, 99, 99, 99, 99, 99]) Now note the changes also occur in our original array! arr array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) Data is not copied, it\u2019s a view of the original array! This avoids memory problems! #To get a copy, need to be explicit arr_copy = arr . copy () arr_copy array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) Indexing a 2D array (matrices) \u00b6 The general format is arr_2d[row][col] or arr_2d[row,col] . I recommend using the comma notation for clarity. arr_2d = np . array (([ 5 , 10 , 15 ],[ 20 , 25 , 30 ],[ 35 , 40 , 45 ])) #Show arr_2d array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]]) #Indexing row arr_2d [ 1 ] array([20, 25, 30]) # Format is arr_2d[row][col] or arr_2d[row,col] # Getting individual element value arr_2d [ 1 ][ 0 ] 20 # Getting individual element value arr_2d [ 1 , 0 ] 20 # 2D array slicing #Shape (2,2) from top right corner arr_2d [: 2 , 1 :] array([[10, 15], [25, 30]]) #Shape bottom row arr_2d [ 2 ] array([35, 40, 45]) #Shape bottom row arr_2d [ 2 ,:] array([35, 40, 45]) More Indexing Help \u00b6 Indexing a 2D matrix can be a bit confusing at first, especially when you start to add in step size. Try google image searching NumPy indexing to find useful images, like this one: Image source Conditional Selection \u00b6 This is a very fundamental concept that will directly translate to pandas later on, make sure you understand this part! Let\u2019s briefly go over how to use brackets for selection based off of comparison operators. arr = np . arange ( 1 , 11 ) arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) arr > 4 array([False, False, False, False, True, True, True, True, True, True]) bool_arr = arr > 4 bool_arr array([False, False, False, False, True, True, True, True, True, True]) arr [ bool_arr ] array([ 5, 6, 7, 8, 9, 10]) arr [ arr > 2 ] array([ 3, 4, 5, 6, 7, 8, 9, 10]) x = 2 arr [ arr > x ] array([ 3, 4, 5, 6, 7, 8, 9, 10]) Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"01 NumPy Indexing and Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#01-numpy-indexing-and-selection","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Indexing and Selection 1.1 Bracket Indexing and Selection 1.2 Broadcasting 1.3 Indexing a 2D array (matrices) 1.4 More Indexing Help 1.5 Conditional Selection 2 Great Job! Thats the end of this part.","title":"01 - Numpy Indexing and Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#numpy-indexing-and-selection","text":"In this lecture we will discuss how to select elements or groups of elements from an array. import numpy as np #Creating sample array arr = np . arange ( 0 , 11 ) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])","title":"NumPy Indexing and Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#bracket-indexing-and-selection","text":"The simplest way to pick one or some elements of an array looks very similar to python lists: #Get a value at an index arr [ 8 ] 8 #Get values in a range arr [ 1 : 5 ] array([1, 2, 3, 4]) #Get values in a range arr [ 0 : 5 ] array([0, 1, 2, 3, 4])","title":"Bracket Indexing and Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#broadcasting","text":"NumPy arrays differ from normal Python lists because of their ability to broadcast. With lists, you can only reassign parts of a list with new parts of the same size and shape. That is, if you wanted to replace the first 5 elements in a list with a new value, you would have to pass in a new 5 element list. With NumPy arrays, you can broadcast a single value across a larger set of values: #Setting a value with index range (Broadcasting) arr [ 0 : 5 ] = 100 #Show arr array([100, 100, 100, 100, 100, 5, 6, 7, 8, 9, 10]) # Reset array, we'll see why I had to reset in a moment arr = np . arange ( 0 , 11 ) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) #Important notes on Slices slice_of_arr = arr [ 0 : 6 ] #Show slice slice_of_arr array([0, 1, 2, 3, 4, 5]) #Change Slice slice_of_arr [:] = 99 #Show Slice again slice_of_arr array([99, 99, 99, 99, 99, 99]) Now note the changes also occur in our original array! arr array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) Data is not copied, it\u2019s a view of the original array! This avoids memory problems! #To get a copy, need to be explicit arr_copy = arr . copy () arr_copy array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10])","title":"Broadcasting"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#indexing-a-2d-array-matrices","text":"The general format is arr_2d[row][col] or arr_2d[row,col] . I recommend using the comma notation for clarity. arr_2d = np . array (([ 5 , 10 , 15 ],[ 20 , 25 , 30 ],[ 35 , 40 , 45 ])) #Show arr_2d array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]]) #Indexing row arr_2d [ 1 ] array([20, 25, 30]) # Format is arr_2d[row][col] or arr_2d[row,col] # Getting individual element value arr_2d [ 1 ][ 0 ] 20 # Getting individual element value arr_2d [ 1 , 0 ] 20 # 2D array slicing #Shape (2,2) from top right corner arr_2d [: 2 , 1 :] array([[10, 15], [25, 30]]) #Shape bottom row arr_2d [ 2 ] array([35, 40, 45]) #Shape bottom row arr_2d [ 2 ,:] array([35, 40, 45])","title":"Indexing a 2D array (matrices)"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#more-indexing-help","text":"Indexing a 2D matrix can be a bit confusing at first, especially when you start to add in step size. Try google image searching NumPy indexing to find useful images, like this one: Image source","title":"More Indexing Help"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#conditional-selection","text":"This is a very fundamental concept that will directly translate to pandas later on, make sure you understand this part! Let\u2019s briefly go over how to use brackets for selection based off of comparison operators. arr = np . arange ( 1 , 11 ) arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) arr > 4 array([False, False, False, False, True, True, True, True, True, True]) bool_arr = arr > 4 bool_arr array([False, False, False, False, True, True, True, True, True, True]) arr [ bool_arr ] array([ 5, 6, 7, 8, 9, 10]) arr [ arr > 2 ] array([ 3, 4, 5, 6, 7, 8, 9, 10]) x = 2 arr [ arr > x ] array([ 3, 4, 5, 6, 7, 8, 9, 10])","title":"Conditional Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/","text":"================ by Jawad Haider 02 - Numpy Operations \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Operations 1.1 Arithmetic 1.2 Universal Array Functions 1.3 Summary Statistics on Arrays 1.4 Axis Logic 2 Great Job! Thats the end of this part. NumPy Operations \u00b6 Arithmetic \u00b6 You can easily perform array with array arithmetic, or scalar with array arithmetic. Let\u2019s see some examples: import numpy as np arr = np . arange ( 0 , 10 ) arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) arr + arr array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) arr * arr array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) arr - arr array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # This will raise a Warning on division by zero, but not an error! # It just fills the spot with nan arr / arr C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide This is separate from the ipykernel package so we can avoid doing imports until array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.]) # Also a warning (but not an error) relating to infinity 1 / arr C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide array([ inf, 1. , 0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]) arr ** 3 array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) Universal Array Functions \u00b6 NumPy comes with many universal array functions , or ufuncs , which are essentially just mathematical operations that can be applied across the array. Let\u2019s show some common ones: # Taking Square Roots np . sqrt ( arr ) array([0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) # Calculating exponential (e^) np . exp ( arr ) array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03]) # Trigonometric Functions like sine np . sin ( arr ) array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849]) # Taking the Natural Logarithm np . log ( arr ) C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458]) Summary Statistics on Arrays \u00b6 NumPy also offers common summary statistics like sum , mean and max . You would call these as methods on an array. arr = np . arange ( 0 , 10 ) arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) arr . sum () 45 arr . mean () 4.5 arr . max () 9 Other summary statistics include: arr.min() returns 0 minimum arr.var() returns 8.25 variance arr.std() returns 2.8722813232690143 standard deviation Axis Logic \u00b6 When working with 2-dimensional arrays (matrices) we have to consider rows and columns. This becomes very important when we get to the section on pandas. In array terms, axis 0 (zero) is the vertical axis (rows), and axis 1 is the horizonal axis (columns). These values (0,1) correspond to the order in which arr.shape values are returned. Let\u2019s see how this affects our summary statistic calculations from above. arr_2d = np . array ([[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ],[ 9 , 10 , 11 , 12 ]]) arr_2d array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) arr_2d . sum ( axis = 0 ) array([15, 18, 21, 24]) By passing in axis=0 , we\u2019re returning an array of sums along the vertical axis, essentially [(1+5+9), (2+6+10), (3+7+11), (4+8+12)] arr_2d . shape (3, 4) This tells us that arr_2d has 3 rows and 4 columns. In arr_2d.sum(axis=0) above, the first element in each row was summed, then the second element, and so forth. So what should arr_2d.sum(axis=1) return? # THINK ABOUT WHAT THIS WILL RETURN BEFORE RUNNING THE CELL! arr_2d . sum ( axis = 1 ) Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"02 NumPy Operations"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#02-numpy-operations","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Operations 1.1 Arithmetic 1.2 Universal Array Functions 1.3 Summary Statistics on Arrays 1.4 Axis Logic 2 Great Job! Thats the end of this part.","title":"02 - Numpy Operations"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#numpy-operations","text":"","title":"NumPy Operations"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#arithmetic","text":"You can easily perform array with array arithmetic, or scalar with array arithmetic. Let\u2019s see some examples: import numpy as np arr = np . arange ( 0 , 10 ) arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) arr + arr array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) arr * arr array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) arr - arr array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # This will raise a Warning on division by zero, but not an error! # It just fills the spot with nan arr / arr C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide This is separate from the ipykernel package so we can avoid doing imports until array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.]) # Also a warning (but not an error) relating to infinity 1 / arr C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide array([ inf, 1. , 0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]) arr ** 3 array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32)","title":"Arithmetic"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#universal-array-functions","text":"NumPy comes with many universal array functions , or ufuncs , which are essentially just mathematical operations that can be applied across the array. Let\u2019s show some common ones: # Taking Square Roots np . sqrt ( arr ) array([0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) # Calculating exponential (e^) np . exp ( arr ) array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03]) # Trigonometric Functions like sine np . sin ( arr ) array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849]) # Taking the Natural Logarithm np . log ( arr ) C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458])","title":"Universal Array Functions"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#summary-statistics-on-arrays","text":"NumPy also offers common summary statistics like sum , mean and max . You would call these as methods on an array. arr = np . arange ( 0 , 10 ) arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) arr . sum () 45 arr . mean () 4.5 arr . max () 9 Other summary statistics include: arr.min() returns 0 minimum arr.var() returns 8.25 variance arr.std() returns 2.8722813232690143 standard deviation","title":"Summary Statistics on Arrays"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#axis-logic","text":"When working with 2-dimensional arrays (matrices) we have to consider rows and columns. This becomes very important when we get to the section on pandas. In array terms, axis 0 (zero) is the vertical axis (rows), and axis 1 is the horizonal axis (columns). These values (0,1) correspond to the order in which arr.shape values are returned. Let\u2019s see how this affects our summary statistic calculations from above. arr_2d = np . array ([[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ],[ 9 , 10 , 11 , 12 ]]) arr_2d array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) arr_2d . sum ( axis = 0 ) array([15, 18, 21, 24]) By passing in axis=0 , we\u2019re returning an array of sums along the vertical axis, essentially [(1+5+9), (2+6+10), (3+7+11), (4+8+12)] arr_2d . shape (3, 4) This tells us that arr_2d has 3 rows and 4 columns. In arr_2d.sum(axis=0) above, the first element in each row was summed, then the second element, and so forth. So what should arr_2d.sum(axis=1) return? # THINK ABOUT WHAT THIS WILL RETURN BEFORE RUNNING THE CELL! arr_2d . sum ( axis = 1 )","title":"Axis Logic"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/","text":"================ by Jawad Haider 03 - NumPy Exercises \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Exercises 1.1 Numpy Indexing and Selection 1.2 NumPy Operations 1.3 Bonus Question 2 Great Job! Thats the end of this part. NumPy Exercises \u00b6 Now that we\u2019ve learned about NumPy let\u2019s test your knowledge. We\u2019ll start off with a few simple tasks and then you\u2019ll be asked some more complicated questions. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! 1. Import NumPy as np \u00b6 2. Create an array of 10 zeros \u00b6 # CODE HERE # DON'T WRITE HERE array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 3. Create an array of 10 ones \u00b6 # DON'T WRITE HERE array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) 4. Create an array of 10 fives \u00b6 # DON'T WRITE HERE array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]) 5. Create an array of the integers from 10 to 50 \u00b6 # DON'T WRITE HERE array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]) 6. Create an array of all the even integers from 10 to 50 \u00b6 # DON'T WRITE HERE array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]) 7. Create a 3x3 matrix with values ranging from 0 to 8 \u00b6 # DON'T WRITE HERE array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) 8. Create a 3x3 identity matrix \u00b6 # DON'T WRITE HERE array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) 9. Use NumPy to generate a random number between 0 and 1 \u2003NOTE: Your result\u2019s value should be different from the one shown below. \u00b6 # DON'T WRITE HERE array([0.65248055]) 10. Use NumPy to generate an array of 25 random numbers sampled from a standard normal distribution \u2003\u2002NOTE: Your result\u2019s values should be different from the ones shown below. \u00b6 # DON'T WRITE HERE array([ 1.80076712, -1.12375847, -0.98524305, 0.11673573, 1.96346762, 1.81378592, -0.33790771, 0.85012656, 0.0100703 , -0.91005957, 0.29064366, 0.69906357, 0.1774377 , -0.61958694, -0.45498611, -2.0804685 , -0.06778549, 1.06403819, 0.4311884 , -1.09853837, 1.11980469, -0.48751963, 1.32517611, -0.61775122, -0.00622865]) 11. Create the following matrix: \u00b6 # DON'T WRITE HERE array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ], [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ], [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ], [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ], [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ], [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ], [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ], [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ], [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ], [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]]) 12. Create an array of 20 linearly spaced points between 0 and 1: \u00b6 # DON'T WRITE HERE array([0. , 0.05263158, 0.10526316, 0.15789474, 0.21052632, 0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421, 0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211, 0.78947368, 0.84210526, 0.89473684, 0.94736842, 1. ]) Numpy Indexing and Selection \u00b6 Now you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs: # RUN THIS CELL - THIS IS OUR STARTING MATRIX mat = np . arange ( 1 , 26 ) . reshape ( 5 , 5 ) mat array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) 13. Write code that reproduces the output shown below. \u2003\u2002Be careful not to run the cell immediately above the output, otherwise you won\u2019t be able to see the output any more. \u00b6 # CODE HERE # DON'T WRITE HERE array([[12, 13, 14, 15], [17, 18, 19, 20], [22, 23, 24, 25]]) 14. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE 20 15. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE array([[ 2], [ 7], [12]]) 16. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE array([21, 22, 23, 24, 25]) 17. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE array([[16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) NumPy Operations \u00b6 18. Get the sum of all the values in mat \u00b6 # DON'T WRITE HERE 325 19. Get the standard deviation of the values in mat \u00b6 # DON'T WRITE HERE 7.211102550927978 20. Get the sum of all the columns in mat \u00b6 # DON'T WRITE HERE array([55, 60, 65, 70, 75]) Bonus Question \u00b6 We worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"03 NumPy Exercises"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#03-numpy-exercises","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Exercises 1.1 Numpy Indexing and Selection 1.2 NumPy Operations 1.3 Bonus Question 2 Great Job! Thats the end of this part.","title":"03 - NumPy Exercises"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#numpy-exercises","text":"Now that we\u2019ve learned about NumPy let\u2019s test your knowledge. We\u2019ll start off with a few simple tasks and then you\u2019ll be asked some more complicated questions. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"NumPy Exercises"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#1-import-numpy-as-np","text":"","title":"1. Import NumPy as np"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#2-create-an-array-of-10-zeros","text":"# CODE HERE # DON'T WRITE HERE array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","title":"2. Create an array of 10 zeros"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#3-create-an-array-of-10-ones","text":"# DON'T WRITE HERE array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])","title":"3. Create an array of 10 ones"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#4-create-an-array-of-10-fives","text":"# DON'T WRITE HERE array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])","title":"4. Create an array of 10 fives"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#5-create-an-array-of-the-integers-from-10-to-50","text":"# DON'T WRITE HERE array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])","title":"5. Create an array of the integers from 10 to 50"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#6-create-an-array-of-all-the-even-integers-from-10-to-50","text":"# DON'T WRITE HERE array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50])","title":"6. Create an array of all the even integers from 10 to 50"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#7-create-a-3x3-matrix-with-values-ranging-from-0-to-8","text":"# DON'T WRITE HERE array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])","title":"7. Create a 3x3 matrix with values ranging from 0 to 8"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#8-create-a-3x3-identity-matrix","text":"# DON'T WRITE HERE array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])","title":"8. Create a 3x3 identity matrix"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#9-use-numpy-to-generate-a-random-number-between-0-and-1-note-your-results-value-should-be-different-from-the-one-shown-below","text":"# DON'T WRITE HERE array([0.65248055])","title":"9. Use NumPy to generate a random number between 0 and 1 NOTE: Your result\u2019s value should be different from the one shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#10-use-numpy-to-generate-an-array-of-25-random-numbers-sampled-from-a-standard-normal-distribution-note-your-results-values-should-be-different-from-the-ones-shown-below","text":"# DON'T WRITE HERE array([ 1.80076712, -1.12375847, -0.98524305, 0.11673573, 1.96346762, 1.81378592, -0.33790771, 0.85012656, 0.0100703 , -0.91005957, 0.29064366, 0.69906357, 0.1774377 , -0.61958694, -0.45498611, -2.0804685 , -0.06778549, 1.06403819, 0.4311884 , -1.09853837, 1.11980469, -0.48751963, 1.32517611, -0.61775122, -0.00622865])","title":"10. Use NumPy to generate an array of 25 random numbers sampled from a standard normal distribution NOTE: Your result\u2019s values should be different from the ones shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#11-create-the-following-matrix","text":"# DON'T WRITE HERE array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ], [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ], [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ], [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ], [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ], [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ], [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ], [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ], [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ], [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]])","title":"11. Create the following matrix:"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#12-create-an-array-of-20-linearly-spaced-points-between-0-and-1","text":"# DON'T WRITE HERE array([0. , 0.05263158, 0.10526316, 0.15789474, 0.21052632, 0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421, 0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211, 0.78947368, 0.84210526, 0.89473684, 0.94736842, 1. ])","title":"12. Create an array of 20 linearly spaced points between 0 and 1:"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#numpy-indexing-and-selection","text":"Now you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs: # RUN THIS CELL - THIS IS OUR STARTING MATRIX mat = np . arange ( 1 , 26 ) . reshape ( 5 , 5 ) mat array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]])","title":"Numpy Indexing and Selection"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#13-write-code-that-reproduces-the-output-shown-below-be-careful-not-to-run-the-cell-immediately-above-the-output-otherwise-you-wont-be-able-to-see-the-output-any-more","text":"# CODE HERE # DON'T WRITE HERE array([[12, 13, 14, 15], [17, 18, 19, 20], [22, 23, 24, 25]])","title":"13. Write code that reproduces the output shown below. Be careful not to run the cell immediately above the output, otherwise you won\u2019t be able to see the output any more."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#14-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE 20","title":"14. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#15-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE array([[ 2], [ 7], [12]])","title":"15. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#16-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE array([21, 22, 23, 24, 25])","title":"16. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#17-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE array([[16, 17, 18, 19, 20], [21, 22, 23, 24, 25]])","title":"17. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#numpy-operations","text":"","title":"NumPy Operations"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#18-get-the-sum-of-all-the-values-in-mat","text":"# DON'T WRITE HERE 325","title":"18. Get the sum of all the values in mat"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#19-get-the-standard-deviation-of-the-values-in-mat","text":"# DON'T WRITE HERE 7.211102550927978","title":"19. Get the standard deviation of the values in mat"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#20-get-the-sum-of-all-the-columns-in-mat","text":"# DON'T WRITE HERE array([55, 60, 65, 70, 75])","title":"20. Get the sum of all the columns in mat"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#bonus-question","text":"We worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint","title":"Bonus Question"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/","text":"================ by Jawad Haider 04 - NumPy Exercises - Solutions \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Exercises - Solutions 1.1 Numpy Indexing and Selection 1.2 NumPy Operations 1.3 Bonus Question 2 Great Job! Thats the end of this part. NumPy Exercises - Solutions \u00b6 Now that we\u2019ve learned about NumPy let\u2019s test your knowledge. We\u2019ll start off with a few simple tasks and then you\u2019ll be asked some more complicated questions. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! 1. Import NumPy as np \u00b6 import numpy as np 2. Create an array of 10 zeros \u00b6 # CODE HERE # DON'T WRITE HERE np . zeros ( 10 ) array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 3. Create an array of 10 ones \u00b6 # DON'T WRITE HERE np . ones ( 10 ) array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) 4. Create an array of 10 fives \u00b6 # DON'T WRITE HERE np . ones ( 10 ) * 5 array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]) 5. Create an array of the integers from 10 to 50 \u00b6 # DON'T WRITE HERE np . arange ( 10 , 51 ) array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]) 6. Create an array of all the even integers from 10 to 50 \u00b6 # DON'T WRITE HERE np . arange ( 10 , 51 , 2 ) array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]) 7. Create a 3x3 matrix with values ranging from 0 to 8 \u00b6 # DON'T WRITE HERE np . arange ( 9 ) . reshape ( 3 , 3 ) array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) 8. Create a 3x3 identity matrix \u00b6 # DON'T WRITE HERE np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) 9. Use NumPy to generate a random number between 0 and 1 \u2003NOTE: Your result\u2019s value should be different from the one shown below. \u00b6 # DON'T WRITE HERE np . random . rand ( 1 ) array([0.65248055]) 10. Use NumPy to generate an array of 25 random numbers sampled from a standard normal distribution \u2003\u2002NOTE: Your result\u2019s values should be different from the ones shown below. \u00b6 # DON'T WRITE HERE np . random . randn ( 25 ) array([ 1.80076712, -1.12375847, -0.98524305, 0.11673573, 1.96346762, 1.81378592, -0.33790771, 0.85012656, 0.0100703 , -0.91005957, 0.29064366, 0.69906357, 0.1774377 , -0.61958694, -0.45498611, -2.0804685 , -0.06778549, 1.06403819, 0.4311884 , -1.09853837, 1.11980469, -0.48751963, 1.32517611, -0.61775122, -0.00622865]) 11. Create the following matrix: \u00b6 # DON'T WRITE HERE np . arange ( 1 , 101 ) . reshape ( 10 , 10 ) / 100 array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ], [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ], [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ], [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ], [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ], [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ], [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ], [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ], [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ], [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]]) 12. Create an array of 20 linearly spaced points between 0 and 1: \u00b6 # DON'T WRITE HERE np . linspace ( 0 , 1 , 20 ) array([0. , 0.05263158, 0.10526316, 0.15789474, 0.21052632, 0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421, 0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211, 0.78947368, 0.84210526, 0.89473684, 0.94736842, 1. ]) Numpy Indexing and Selection \u00b6 Now you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs: # RUN THIS CELL - THIS IS OUR STARTING MATRIX mat = np . arange ( 1 , 26 ) . reshape ( 5 , 5 ) mat array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) 13. Write code that reproduces the output shown below. \u2003\u2002Be careful not to run the cell immediately above the output, otherwise you won\u2019t be able to see the output any more. \u00b6 # CODE HERE # DON'T WRITE HERE mat [ 2 :, 1 :] array([[12, 13, 14, 15], [17, 18, 19, 20], [22, 23, 24, 25]]) 14. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE mat [ 3 , 4 ] 20 15. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE mat [: 3 , 1 : 2 ] array([[ 2], [ 7], [12]]) 16. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE mat [ 4 ,:] array([21, 22, 23, 24, 25]) 17. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE mat [ 3 : 5 ,:] array([[16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) NumPy Operations \u00b6 18. Get the sum of all the values in mat \u00b6 # DON'T WRITE HERE mat . sum () 325 19. Get the standard deviation of the values in mat \u00b6 # DON'T WRITE HERE mat . std () 7.211102550927978 20. Get the sum of all the columns in mat \u00b6 # DON'T WRITE HERE mat . sum ( axis = 0 ) array([55, 60, 65, 70, 75]) Bonus Question \u00b6 We worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint np . random . seed ( 101 ) Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"04 NumPy Exercises Solutions"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#04-numpy-exercises-solutions","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Exercises - Solutions 1.1 Numpy Indexing and Selection 1.2 NumPy Operations 1.3 Bonus Question 2 Great Job! Thats the end of this part.","title":"04 - NumPy Exercises - Solutions"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#numpy-exercises-solutions","text":"Now that we\u2019ve learned about NumPy let\u2019s test your knowledge. We\u2019ll start off with a few simple tasks and then you\u2019ll be asked some more complicated questions. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"NumPy Exercises - Solutions"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#1-import-numpy-as-np","text":"import numpy as np","title":"1. Import NumPy as np"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#2-create-an-array-of-10-zeros","text":"# CODE HERE # DON'T WRITE HERE np . zeros ( 10 ) array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","title":"2. Create an array of 10 zeros"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#3-create-an-array-of-10-ones","text":"# DON'T WRITE HERE np . ones ( 10 ) array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])","title":"3. Create an array of 10 ones"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#4-create-an-array-of-10-fives","text":"# DON'T WRITE HERE np . ones ( 10 ) * 5 array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])","title":"4. Create an array of 10 fives"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#5-create-an-array-of-the-integers-from-10-to-50","text":"# DON'T WRITE HERE np . arange ( 10 , 51 ) array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])","title":"5. Create an array of the integers from 10 to 50"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#6-create-an-array-of-all-the-even-integers-from-10-to-50","text":"# DON'T WRITE HERE np . arange ( 10 , 51 , 2 ) array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50])","title":"6. Create an array of all the even integers from 10 to 50"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#7-create-a-3x3-matrix-with-values-ranging-from-0-to-8","text":"# DON'T WRITE HERE np . arange ( 9 ) . reshape ( 3 , 3 ) array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])","title":"7. Create a 3x3 matrix with values ranging from 0 to 8"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#8-create-a-3x3-identity-matrix","text":"# DON'T WRITE HERE np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])","title":"8. Create a 3x3 identity matrix"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#9-use-numpy-to-generate-a-random-number-between-0-and-1-note-your-results-value-should-be-different-from-the-one-shown-below","text":"# DON'T WRITE HERE np . random . rand ( 1 ) array([0.65248055])","title":"9. Use NumPy to generate a random number between 0 and 1 NOTE: Your result\u2019s value should be different from the one shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#10-use-numpy-to-generate-an-array-of-25-random-numbers-sampled-from-a-standard-normal-distribution-note-your-results-values-should-be-different-from-the-ones-shown-below","text":"# DON'T WRITE HERE np . random . randn ( 25 ) array([ 1.80076712, -1.12375847, -0.98524305, 0.11673573, 1.96346762, 1.81378592, -0.33790771, 0.85012656, 0.0100703 , -0.91005957, 0.29064366, 0.69906357, 0.1774377 , -0.61958694, -0.45498611, -2.0804685 , -0.06778549, 1.06403819, 0.4311884 , -1.09853837, 1.11980469, -0.48751963, 1.32517611, -0.61775122, -0.00622865])","title":"10. Use NumPy to generate an array of 25 random numbers sampled from a standard normal distribution NOTE: Your result\u2019s values should be different from the ones shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#11-create-the-following-matrix","text":"# DON'T WRITE HERE np . arange ( 1 , 101 ) . reshape ( 10 , 10 ) / 100 array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ], [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ], [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ], [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ], [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ], [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ], [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ], [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ], [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ], [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]])","title":"11. Create the following matrix:"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#12-create-an-array-of-20-linearly-spaced-points-between-0-and-1","text":"# DON'T WRITE HERE np . linspace ( 0 , 1 , 20 ) array([0. , 0.05263158, 0.10526316, 0.15789474, 0.21052632, 0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421, 0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211, 0.78947368, 0.84210526, 0.89473684, 0.94736842, 1. ])","title":"12. Create an array of 20 linearly spaced points between 0 and 1:"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#numpy-indexing-and-selection","text":"Now you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs: # RUN THIS CELL - THIS IS OUR STARTING MATRIX mat = np . arange ( 1 , 26 ) . reshape ( 5 , 5 ) mat array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]])","title":"Numpy Indexing and Selection"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#13-write-code-that-reproduces-the-output-shown-below-be-careful-not-to-run-the-cell-immediately-above-the-output-otherwise-you-wont-be-able-to-see-the-output-any-more","text":"# CODE HERE # DON'T WRITE HERE mat [ 2 :, 1 :] array([[12, 13, 14, 15], [17, 18, 19, 20], [22, 23, 24, 25]])","title":"13. Write code that reproduces the output shown below. Be careful not to run the cell immediately above the output, otherwise you won\u2019t be able to see the output any more."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#14-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE mat [ 3 , 4 ] 20","title":"14. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#15-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE mat [: 3 , 1 : 2 ] array([[ 2], [ 7], [12]])","title":"15. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#16-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE mat [ 4 ,:] array([21, 22, 23, 24, 25])","title":"16. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#17-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE mat [ 3 : 5 ,:] array([[16, 17, 18, 19, 20], [21, 22, 23, 24, 25]])","title":"17. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#numpy-operations","text":"","title":"NumPy Operations"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#18-get-the-sum-of-all-the-values-in-mat","text":"# DON'T WRITE HERE mat . sum () 325","title":"18. Get the sum of all the values in mat"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#19-get-the-standard-deviation-of-the-values-in-mat","text":"# DON'T WRITE HERE mat . std () 7.211102550927978","title":"19. Get the standard deviation of the values in mat"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#20-get-the-sum-of-all-the-columns-in-mat","text":"# DON'T WRITE HERE mat . sum ( axis = 0 ) array([55, 60, 65, 70, 75])","title":"20. Get the sum of all the columns in mat"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#bonus-question","text":"We worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint np . random . seed ( 101 )","title":"Bonus Question"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/00-Intro-to-Pandas/","text":"================ by Jawad Haider 00 - Introduction to Pandas \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Introduction to Pandas \u00b6 In this section of the course we will learn how to use pandas for data analysis. You can think of pandas as an extremely powerful version of Excel, with a lot more features. In this section of the course, you should go through the notebooks in this order: Introduction to Pandas Series DataFrames Missing Data GroupBy Merging, Joining and Concatenating Operations Data Input and Output Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"00 Intro to Pandas"},{"location":"bootcampsnotes/pandas/00-Intro-to-Pandas/#00-introduction-to-pandas","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"00 - Introduction to Pandas"},{"location":"bootcampsnotes/pandas/00-Intro-to-Pandas/#introduction-to-pandas","text":"In this section of the course we will learn how to use pandas for data analysis. You can think of pandas as an extremely powerful version of Excel, with a lot more features. In this section of the course, you should go through the notebooks in this order: Introduction to Pandas Series DataFrames Missing Data GroupBy Merging, Joining and Concatenating Operations Data Input and Output","title":"Introduction to Pandas"},{"location":"bootcampsnotes/pandas/00-Intro-to-Pandas/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/01-Series/","text":"================ by Jawad Haider 01 - Series \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Series 1.1 Creating a Series 1.1.1 Using Lists 1.1.2 Using NumPy Arrays 1.1.3 Using Dictionaries 1.1.4 Data in a Series 1.2 Using an Index 2 Great Job! Thats the end of this part. Series \u00b6 The first main data type we will learn about for pandas is the Series data type. Let\u2019s import Pandas and explore the Series object. A Series is very similar to a NumPy array (in fact it is built on top of the NumPy array object). What differentiates the NumPy array from a Series, is that a Series can have axis labels, meaning it can be indexed by a label, instead of just a number location. It also doesn\u2019t need to hold numeric data, it can hold any arbitrary Python Object. Let\u2019s explore this concept through some examples: import numpy as np import pandas as pd Creating a Series \u00b6 You can convert a list,numpy array, or dictionary to a Series: labels = [ 'a' , 'b' , 'c' ] my_list = [ 10 , 20 , 30 ] arr = np . array ([ 10 , 20 , 30 ]) d = { 'a' : 10 , 'b' : 20 , 'c' : 30 } Using Lists \u00b6 pd . Series ( data = my_list ) 0 10 1 20 2 30 dtype: int64 pd . Series ( data = my_list , index = labels ) a 10 b 20 c 30 dtype: int64 pd . Series ( my_list , labels ) a 10 b 20 c 30 dtype: int64 Using NumPy Arrays \u00b6 pd . Series ( arr ) 0 10 1 20 2 30 dtype: int64 pd . Series ( arr , labels ) a 10 b 20 c 30 dtype: int64 Using Dictionaries \u00b6 pd . Series ( d ) a 10 b 20 c 30 dtype: int64 Data in a Series \u00b6 A pandas Series can hold a variety of object types: pd . Series ( data = labels ) 0 a 1 b 2 c dtype: object # Even functions (although unlikely that you will use this) pd . Series ([ sum , print , len ]) 0 <built-in function sum> 1 <built-in function print> 2 <built-in function len> dtype: object Using an Index \u00b6 The key to using a Series is understanding its index. Pandas makes use of these index names or numbers by allowing for fast look ups of information (works like a hash table or dictionary). Let\u2019s see some examples of how to grab information from a Series. Let us create two sereis, ser1 and ser2: ser1 = pd . Series ([ 1 , 2 , 3 , 4 ], index = [ 'USA' , 'Germany' , 'USSR' , 'Japan' ]) ser1 USA 1 Germany 2 USSR 3 Japan 4 dtype: int64 ser2 = pd . Series ([ 1 , 2 , 5 , 4 ], index = [ 'USA' , 'Germany' , 'Italy' , 'Japan' ]) ser2 USA 1 Germany 2 Italy 5 Japan 4 dtype: int64 ser1 [ 'USA' ] 1 Operations are then also done based off of index: ser1 + ser2 Germany 4.0 Italy NaN Japan 8.0 USA 2.0 USSR NaN dtype: float64 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"01 Series"},{"location":"bootcampsnotes/pandas/01-Series/#01-series","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Series 1.1 Creating a Series 1.1.1 Using Lists 1.1.2 Using NumPy Arrays 1.1.3 Using Dictionaries 1.1.4 Data in a Series 1.2 Using an Index 2 Great Job! Thats the end of this part.","title":"01 - Series"},{"location":"bootcampsnotes/pandas/01-Series/#series","text":"The first main data type we will learn about for pandas is the Series data type. Let\u2019s import Pandas and explore the Series object. A Series is very similar to a NumPy array (in fact it is built on top of the NumPy array object). What differentiates the NumPy array from a Series, is that a Series can have axis labels, meaning it can be indexed by a label, instead of just a number location. It also doesn\u2019t need to hold numeric data, it can hold any arbitrary Python Object. Let\u2019s explore this concept through some examples: import numpy as np import pandas as pd","title":"Series"},{"location":"bootcampsnotes/pandas/01-Series/#creating-a-series","text":"You can convert a list,numpy array, or dictionary to a Series: labels = [ 'a' , 'b' , 'c' ] my_list = [ 10 , 20 , 30 ] arr = np . array ([ 10 , 20 , 30 ]) d = { 'a' : 10 , 'b' : 20 , 'c' : 30 }","title":"Creating a Series"},{"location":"bootcampsnotes/pandas/01-Series/#using-lists","text":"pd . Series ( data = my_list ) 0 10 1 20 2 30 dtype: int64 pd . Series ( data = my_list , index = labels ) a 10 b 20 c 30 dtype: int64 pd . Series ( my_list , labels ) a 10 b 20 c 30 dtype: int64","title":"Using Lists"},{"location":"bootcampsnotes/pandas/01-Series/#using-numpy-arrays","text":"pd . Series ( arr ) 0 10 1 20 2 30 dtype: int64 pd . Series ( arr , labels ) a 10 b 20 c 30 dtype: int64","title":"Using NumPy Arrays"},{"location":"bootcampsnotes/pandas/01-Series/#using-dictionaries","text":"pd . Series ( d ) a 10 b 20 c 30 dtype: int64","title":"Using Dictionaries"},{"location":"bootcampsnotes/pandas/01-Series/#data-in-a-series","text":"A pandas Series can hold a variety of object types: pd . Series ( data = labels ) 0 a 1 b 2 c dtype: object # Even functions (although unlikely that you will use this) pd . Series ([ sum , print , len ]) 0 <built-in function sum> 1 <built-in function print> 2 <built-in function len> dtype: object","title":"Data in a Series"},{"location":"bootcampsnotes/pandas/01-Series/#using-an-index","text":"The key to using a Series is understanding its index. Pandas makes use of these index names or numbers by allowing for fast look ups of information (works like a hash table or dictionary). Let\u2019s see some examples of how to grab information from a Series. Let us create two sereis, ser1 and ser2: ser1 = pd . Series ([ 1 , 2 , 3 , 4 ], index = [ 'USA' , 'Germany' , 'USSR' , 'Japan' ]) ser1 USA 1 Germany 2 USSR 3 Japan 4 dtype: int64 ser2 = pd . Series ([ 1 , 2 , 5 , 4 ], index = [ 'USA' , 'Germany' , 'Italy' , 'Japan' ]) ser2 USA 1 Germany 2 Italy 5 Japan 4 dtype: int64 ser1 [ 'USA' ] 1 Operations are then also done based off of index: ser1 + ser2 Germany 4.0 Italy NaN Japan 8.0 USA 2.0 USSR NaN dtype: float64","title":"Using an Index"},{"location":"bootcampsnotes/pandas/01-Series/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/02-DataFrames/","text":"================ by Jawad Haider 02 - DataFrames \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 DataFrames 1.1 Selection and Indexing 1.1.1 Creating a new column: 1.1.2 Removing Columns 1.1.3 Selecting Rows 1.1.4 Selecting subset of rows and columns 1.1.5 Conditional Selection 1.2 More Index Details 1.3 DataFrame Summaries 2 Great Job! Thats the end of this part. DataFrames \u00b6 DataFrames are the workhorse of pandas and are directly inspired by the R programming language. We can think of a DataFrame as a bunch of Series objects put together to share the same index. Let\u2019s use pandas to explore this topic! import pandas as pd import numpy as np from numpy.random import randn np . random . seed ( 101 ) df = pd . DataFrame ( randn ( 5 , 4 ), index = 'A B C D E' . split (), columns = 'W X Y Z' . split ()) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 Selection and Indexing \u00b6 Let\u2019s learn the various methods to grab data from a DataFrame df [ 'W' ] A 2.706850 B 0.651118 C -2.018168 D 0.188695 E 0.190794 Name: W, dtype: float64 # Pass a list of column names df [[ 'W' , 'Z' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W Z A 2.706850 0.503826 B 0.651118 0.605965 C -2.018168 -0.589001 D 0.188695 0.955057 E 0.190794 0.683509 # SQL Syntax (NOT RECOMMENDED!) df . W A 2.706850 B 0.651118 C -2.018168 D 0.188695 E 0.190794 Name: W, dtype: float64 DataFrame Columns are just Series type ( df [ 'W' ]) pandas.core.series.Series Creating a new column: \u00b6 df [ 'new' ] = df [ 'W' ] + df [ 'Y' ] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 2.706850 0.628133 0.907969 0.503826 3.614819 B 0.651118 -0.319318 -0.848077 0.605965 -0.196959 C -2.018168 0.740122 0.528813 -0.589001 -1.489355 D 0.188695 -0.758872 -0.933237 0.955057 -0.744542 E 0.190794 1.978757 2.605967 0.683509 2.796762 Removing Columns \u00b6 df . drop ( 'new' , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 # Not inplace unless specified! df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 2.706850 0.628133 0.907969 0.503826 3.614819 B 0.651118 -0.319318 -0.848077 0.605965 -0.196959 C -2.018168 0.740122 0.528813 -0.589001 -1.489355 D 0.188695 -0.758872 -0.933237 0.955057 -0.744542 E 0.190794 1.978757 2.605967 0.683509 2.796762 df . drop ( 'new' , axis = 1 , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 Can also drop rows this way: df . drop ( 'E' , axis = 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 Selecting Rows \u00b6 df . loc [ 'A' ] W 2.706850 X 0.628133 Y 0.907969 Z 0.503826 Name: A, dtype: float64 Or select based off of position instead of label df . iloc [ 2 ] W -2.018168 X 0.740122 Y 0.528813 Z -0.589001 Name: C, dtype: float64 Selecting subset of rows and columns \u00b6 df . loc [ 'B' , 'Y' ] -0.8480769834036315 df . loc [[ 'A' , 'B' ],[ 'W' , 'Y' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W Y A 2.706850 0.907969 B 0.651118 -0.848077 Conditional Selection \u00b6 An important feature of pandas is conditional selection using bracket notation, very similar to numpy: df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 df > 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A True True True True B True False False True C False True True False D True False False True E True True True True df [ df > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 NaN NaN 0.605965 C NaN 0.740122 0.528813 NaN D 0.188695 NaN NaN 0.955057 E 0.190794 1.978757 2.605967 0.683509 df [ df [ 'W' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 df [ df [ 'W' ] > 0 ][ 'Y' ] A 0.907969 B -0.848077 D -0.933237 E 2.605967 Name: Y, dtype: float64 df [ df [ 'W' ] > 0 ][[ 'Y' , 'X' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Y X A 0.907969 0.628133 B -0.848077 -0.319318 D -0.933237 -0.758872 E 2.605967 1.978757 For two conditions you can use | and & with parenthesis: df [( df [ 'W' ] > 0 ) & ( df [ 'Y' ] > 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 0.190794 1.978757 2.605967 0.683509 More Index Details \u00b6 Let\u2019s discuss some more features of indexing, including resetting the index or setting it something else. We\u2019ll also talk about index hierarchy! df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 # Reset to default 0,1...n index df . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index W X Y Z 0 A 2.706850 0.628133 0.907969 0.503826 1 B 0.651118 -0.319318 -0.848077 0.605965 2 C -2.018168 0.740122 0.528813 -0.589001 3 D 0.188695 -0.758872 -0.933237 0.955057 4 E 0.190794 1.978757 2.605967 0.683509 newind = 'CA NY WY OR CO' . split () df [ 'States' ] = newind df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States A 2.706850 0.628133 0.907969 0.503826 CA B 0.651118 -0.319318 -0.848077 0.605965 NY C -2.018168 0.740122 0.528813 -0.589001 WY D 0.188695 -0.758872 -0.933237 0.955057 OR E 0.190794 1.978757 2.605967 0.683509 CO df . set_index ( 'States' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States CA 2.706850 0.628133 0.907969 0.503826 NY 0.651118 -0.319318 -0.848077 0.605965 WY -2.018168 0.740122 0.528813 -0.589001 OR 0.188695 -0.758872 -0.933237 0.955057 CO 0.190794 1.978757 2.605967 0.683509 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States A 2.706850 0.628133 0.907969 0.503826 CA B 0.651118 -0.319318 -0.848077 0.605965 NY C -2.018168 0.740122 0.528813 -0.589001 WY D 0.188695 -0.758872 -0.933237 0.955057 OR E 0.190794 1.978757 2.605967 0.683509 CO df . set_index ( 'States' , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States CA 2.706850 0.628133 0.907969 0.503826 NY 0.651118 -0.319318 -0.848077 0.605965 WY -2.018168 0.740122 0.528813 -0.589001 OR 0.188695 -0.758872 -0.933237 0.955057 CO 0.190794 1.978757 2.605967 0.683509 DataFrame Summaries \u00b6 There are a couple of ways to obtain summary data on DataFrames. df.describe() provides summary statistics on all numerical columns. df.info and df.dtypes displays the data type of all columns. df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z count 5.000000 5.000000 5.000000 5.000000 mean 0.343858 0.453764 0.452287 0.431871 std 1.681131 1.061385 1.454516 0.594708 min -2.018168 -0.758872 -0.933237 -0.589001 25% 0.188695 -0.319318 -0.848077 0.503826 50% 0.190794 0.628133 0.528813 0.605965 75% 0.651118 0.740122 0.907969 0.683509 max 2.706850 1.978757 2.605967 0.955057 df . dtypes W float64 X float64 Y float64 Z float64 dtype: object df . info () <class 'pandas.core.frame.DataFrame'> Index: 5 entries, CA to CO Data columns (total 4 columns): W 5 non-null float64 X 5 non-null float64 Y 5 non-null float64 Z 5 non-null float64 dtypes: float64(4) memory usage: 200.0+ bytes Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"02 DataFrames"},{"location":"bootcampsnotes/pandas/02-DataFrames/#02-dataframes","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 DataFrames 1.1 Selection and Indexing 1.1.1 Creating a new column: 1.1.2 Removing Columns 1.1.3 Selecting Rows 1.1.4 Selecting subset of rows and columns 1.1.5 Conditional Selection 1.2 More Index Details 1.3 DataFrame Summaries 2 Great Job! Thats the end of this part.","title":"02 - DataFrames"},{"location":"bootcampsnotes/pandas/02-DataFrames/#dataframes","text":"DataFrames are the workhorse of pandas and are directly inspired by the R programming language. We can think of a DataFrame as a bunch of Series objects put together to share the same index. Let\u2019s use pandas to explore this topic! import pandas as pd import numpy as np from numpy.random import randn np . random . seed ( 101 ) df = pd . DataFrame ( randn ( 5 , 4 ), index = 'A B C D E' . split (), columns = 'W X Y Z' . split ()) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509","title":"DataFrames"},{"location":"bootcampsnotes/pandas/02-DataFrames/#selection-and-indexing","text":"Let\u2019s learn the various methods to grab data from a DataFrame df [ 'W' ] A 2.706850 B 0.651118 C -2.018168 D 0.188695 E 0.190794 Name: W, dtype: float64 # Pass a list of column names df [[ 'W' , 'Z' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W Z A 2.706850 0.503826 B 0.651118 0.605965 C -2.018168 -0.589001 D 0.188695 0.955057 E 0.190794 0.683509 # SQL Syntax (NOT RECOMMENDED!) df . W A 2.706850 B 0.651118 C -2.018168 D 0.188695 E 0.190794 Name: W, dtype: float64 DataFrame Columns are just Series type ( df [ 'W' ]) pandas.core.series.Series","title":"Selection and Indexing"},{"location":"bootcampsnotes/pandas/02-DataFrames/#creating-a-new-column","text":"df [ 'new' ] = df [ 'W' ] + df [ 'Y' ] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 2.706850 0.628133 0.907969 0.503826 3.614819 B 0.651118 -0.319318 -0.848077 0.605965 -0.196959 C -2.018168 0.740122 0.528813 -0.589001 -1.489355 D 0.188695 -0.758872 -0.933237 0.955057 -0.744542 E 0.190794 1.978757 2.605967 0.683509 2.796762","title":"Creating a new column:"},{"location":"bootcampsnotes/pandas/02-DataFrames/#removing-columns","text":"df . drop ( 'new' , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 # Not inplace unless specified! df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 2.706850 0.628133 0.907969 0.503826 3.614819 B 0.651118 -0.319318 -0.848077 0.605965 -0.196959 C -2.018168 0.740122 0.528813 -0.589001 -1.489355 D 0.188695 -0.758872 -0.933237 0.955057 -0.744542 E 0.190794 1.978757 2.605967 0.683509 2.796762 df . drop ( 'new' , axis = 1 , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 Can also drop rows this way: df . drop ( 'E' , axis = 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057","title":"Removing Columns"},{"location":"bootcampsnotes/pandas/02-DataFrames/#selecting-rows","text":"df . loc [ 'A' ] W 2.706850 X 0.628133 Y 0.907969 Z 0.503826 Name: A, dtype: float64 Or select based off of position instead of label df . iloc [ 2 ] W -2.018168 X 0.740122 Y 0.528813 Z -0.589001 Name: C, dtype: float64","title":"Selecting Rows"},{"location":"bootcampsnotes/pandas/02-DataFrames/#selecting-subset-of-rows-and-columns","text":"df . loc [ 'B' , 'Y' ] -0.8480769834036315 df . loc [[ 'A' , 'B' ],[ 'W' , 'Y' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W Y A 2.706850 0.907969 B 0.651118 -0.848077","title":"Selecting subset of rows and columns"},{"location":"bootcampsnotes/pandas/02-DataFrames/#conditional-selection","text":"An important feature of pandas is conditional selection using bracket notation, very similar to numpy: df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 df > 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A True True True True B True False False True C False True True False D True False False True E True True True True df [ df > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 NaN NaN 0.605965 C NaN 0.740122 0.528813 NaN D 0.188695 NaN NaN 0.955057 E 0.190794 1.978757 2.605967 0.683509 df [ df [ 'W' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 df [ df [ 'W' ] > 0 ][ 'Y' ] A 0.907969 B -0.848077 D -0.933237 E 2.605967 Name: Y, dtype: float64 df [ df [ 'W' ] > 0 ][[ 'Y' , 'X' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Y X A 0.907969 0.628133 B -0.848077 -0.319318 D -0.933237 -0.758872 E 2.605967 1.978757 For two conditions you can use | and & with parenthesis: df [( df [ 'W' ] > 0 ) & ( df [ 'Y' ] > 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 0.190794 1.978757 2.605967 0.683509","title":"Conditional Selection"},{"location":"bootcampsnotes/pandas/02-DataFrames/#more-index-details","text":"Let\u2019s discuss some more features of indexing, including resetting the index or setting it something else. We\u2019ll also talk about index hierarchy! df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 # Reset to default 0,1...n index df . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index W X Y Z 0 A 2.706850 0.628133 0.907969 0.503826 1 B 0.651118 -0.319318 -0.848077 0.605965 2 C -2.018168 0.740122 0.528813 -0.589001 3 D 0.188695 -0.758872 -0.933237 0.955057 4 E 0.190794 1.978757 2.605967 0.683509 newind = 'CA NY WY OR CO' . split () df [ 'States' ] = newind df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States A 2.706850 0.628133 0.907969 0.503826 CA B 0.651118 -0.319318 -0.848077 0.605965 NY C -2.018168 0.740122 0.528813 -0.589001 WY D 0.188695 -0.758872 -0.933237 0.955057 OR E 0.190794 1.978757 2.605967 0.683509 CO df . set_index ( 'States' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States CA 2.706850 0.628133 0.907969 0.503826 NY 0.651118 -0.319318 -0.848077 0.605965 WY -2.018168 0.740122 0.528813 -0.589001 OR 0.188695 -0.758872 -0.933237 0.955057 CO 0.190794 1.978757 2.605967 0.683509 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States A 2.706850 0.628133 0.907969 0.503826 CA B 0.651118 -0.319318 -0.848077 0.605965 NY C -2.018168 0.740122 0.528813 -0.589001 WY D 0.188695 -0.758872 -0.933237 0.955057 OR E 0.190794 1.978757 2.605967 0.683509 CO df . set_index ( 'States' , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States CA 2.706850 0.628133 0.907969 0.503826 NY 0.651118 -0.319318 -0.848077 0.605965 WY -2.018168 0.740122 0.528813 -0.589001 OR 0.188695 -0.758872 -0.933237 0.955057 CO 0.190794 1.978757 2.605967 0.683509","title":"More Index Details"},{"location":"bootcampsnotes/pandas/02-DataFrames/#dataframe-summaries","text":"There are a couple of ways to obtain summary data on DataFrames. df.describe() provides summary statistics on all numerical columns. df.info and df.dtypes displays the data type of all columns. df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z count 5.000000 5.000000 5.000000 5.000000 mean 0.343858 0.453764 0.452287 0.431871 std 1.681131 1.061385 1.454516 0.594708 min -2.018168 -0.758872 -0.933237 -0.589001 25% 0.188695 -0.319318 -0.848077 0.503826 50% 0.190794 0.628133 0.528813 0.605965 75% 0.651118 0.740122 0.907969 0.683509 max 2.706850 1.978757 2.605967 0.955057 df . dtypes W float64 X float64 Y float64 Z float64 dtype: object df . info () <class 'pandas.core.frame.DataFrame'> Index: 5 entries, CA to CO Data columns (total 4 columns): W 5 non-null float64 X 5 non-null float64 Y 5 non-null float64 Z 5 non-null float64 dtypes: float64(4) memory usage: 200.0+ bytes","title":"DataFrame Summaries"},{"location":"bootcampsnotes/pandas/02-DataFrames/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/03-Missing-Data/","text":"================ by Jawad Haider 03 - Missing Data \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Missing Data 2 Great Job! Thats the end of this part. Missing Data \u00b6 Let\u2019s show a few convenient methods to deal with Missing Data in pandas: import numpy as np import pandas as pd df = pd . DataFrame ({ 'A' :[ 1 , 2 , np . nan ], 'B' :[ 5 , np . nan , np . nan ], 'C' :[ 1 , 2 , 3 ]}) df A B C 0 1.0 5.0 1 1 2.0 NaN 2 2 NaN NaN 3 df . dropna () A B C 0 1.0 5.0 1 df . dropna ( axis = 1 ) C 0 1 1 2 2 3 df . dropna ( thresh = 2 ) A B C 0 1.0 5.0 1 1 2.0 NaN 2 df . fillna ( value = 'FILL VALUE' ) A B C 0 1 5 1 1 2 FILL VALUE 2 2 FILL VALUE FILL VALUE 3 df [ 'A' ] . fillna ( value = df [ 'A' ] . mean ()) 0 1.0 1 2.0 2 1.5 Name: A, dtype: float64 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"03 Missing Data"},{"location":"bootcampsnotes/pandas/03-Missing-Data/#03-missing-data","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Missing Data 2 Great Job! Thats the end of this part.","title":"03 - Missing Data"},{"location":"bootcampsnotes/pandas/03-Missing-Data/#missing-data","text":"Let\u2019s show a few convenient methods to deal with Missing Data in pandas: import numpy as np import pandas as pd df = pd . DataFrame ({ 'A' :[ 1 , 2 , np . nan ], 'B' :[ 5 , np . nan , np . nan ], 'C' :[ 1 , 2 , 3 ]}) df A B C 0 1.0 5.0 1 1 2.0 NaN 2 2 NaN NaN 3 df . dropna () A B C 0 1.0 5.0 1 df . dropna ( axis = 1 ) C 0 1 1 2 2 3 df . dropna ( thresh = 2 ) A B C 0 1.0 5.0 1 1 2.0 NaN 2 df . fillna ( value = 'FILL VALUE' ) A B C 0 1 5 1 1 2 FILL VALUE 2 2 FILL VALUE FILL VALUE 3 df [ 'A' ] . fillna ( value = df [ 'A' ] . mean ()) 0 1.0 1 2.0 2 1.5 Name: A, dtype: float64","title":"Missing Data"},{"location":"bootcampsnotes/pandas/03-Missing-Data/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/04-Groupby/","text":"================ by Jawad Haider 04 - Groupby \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Groupby 2 Great Job! Thats the end of this part. Groupby \u00b6 The groupby method allows you to group rows of data together and call aggregate functions import pandas as pd # Create dataframe data = { 'Company' :[ 'GOOG' , 'GOOG' , 'MSFT' , 'MSFT' , 'FB' , 'FB' ], 'Person' :[ 'Sam' , 'Charlie' , 'Amy' , 'Vanessa' , 'Carl' , 'Sarah' ], 'Sales' :[ 200 , 120 , 340 , 124 , 243 , 350 ]} df = pd . DataFrame ( data ) df Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 Now you can use the .groupby() method to group rows together based off of a column name. For instance let\u2019s group based off of Company. This will create a DataFrameGroupBy object: df . groupby ( 'Company' ) <pandas.core.groupby.DataFrameGroupBy object at 0x113014128> You can save this object as a new variable: by_comp = df . groupby ( \"Company\" ) And then call aggregate methods off the object: by_comp . mean () Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 df . groupby ( 'Company' ) . mean () Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 More examples of aggregate methods: by_comp . std () Sales Company FB 75.660426 GOOG 56.568542 MSFT 152.735065 by_comp . min () Person Sales Company FB Carl 243 GOOG Charlie 120 MSFT Amy 124 by_comp . max () Person Sales Company FB Sarah 350 GOOG Sam 200 MSFT Vanessa 340 by_comp . count () Person Sales Company FB 2 2 GOOG 2 2 MSFT 2 2 by_comp . describe () Sales Company FB count 2.000000 mean 296.500000 std 75.660426 min 243.000000 25% 269.750000 50% 296.500000 75% 323.250000 max 350.000000 GOOG count 2.000000 mean 160.000000 std 56.568542 min 120.000000 25% 140.000000 50% 160.000000 75% 180.000000 max 200.000000 MSFT count 2.000000 mean 232.000000 std 152.735065 min 124.000000 25% 178.000000 50% 232.000000 75% 286.000000 max 340.000000 by_comp . describe () . transpose () Company FB GOOG MSFT count mean std min 25% 50% 75% max count mean ... 75% max count mean std min 25% 50% 75% max Sales 2.0 296.5 75.660426 243.0 269.75 296.5 323.25 350.0 2.0 160.0 ... 180.0 200.0 2.0 232.0 152.735065 124.0 178.0 232.0 286.0 340.0 1 rows \u00d7 24 columns by_comp . describe () . transpose ()[ 'GOOG' ] count mean std min 25% 50% 75% max Sales 2.0 160.0 56.568542 120.0 140.0 160.0 180.0 200.0 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"04 Groupby"},{"location":"bootcampsnotes/pandas/04-Groupby/#04-groupby","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Groupby 2 Great Job! Thats the end of this part.","title":"04 - Groupby"},{"location":"bootcampsnotes/pandas/04-Groupby/#groupby","text":"The groupby method allows you to group rows of data together and call aggregate functions import pandas as pd # Create dataframe data = { 'Company' :[ 'GOOG' , 'GOOG' , 'MSFT' , 'MSFT' , 'FB' , 'FB' ], 'Person' :[ 'Sam' , 'Charlie' , 'Amy' , 'Vanessa' , 'Carl' , 'Sarah' ], 'Sales' :[ 200 , 120 , 340 , 124 , 243 , 350 ]} df = pd . DataFrame ( data ) df Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 Now you can use the .groupby() method to group rows together based off of a column name. For instance let\u2019s group based off of Company. This will create a DataFrameGroupBy object: df . groupby ( 'Company' ) <pandas.core.groupby.DataFrameGroupBy object at 0x113014128> You can save this object as a new variable: by_comp = df . groupby ( \"Company\" ) And then call aggregate methods off the object: by_comp . mean () Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 df . groupby ( 'Company' ) . mean () Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 More examples of aggregate methods: by_comp . std () Sales Company FB 75.660426 GOOG 56.568542 MSFT 152.735065 by_comp . min () Person Sales Company FB Carl 243 GOOG Charlie 120 MSFT Amy 124 by_comp . max () Person Sales Company FB Sarah 350 GOOG Sam 200 MSFT Vanessa 340 by_comp . count () Person Sales Company FB 2 2 GOOG 2 2 MSFT 2 2 by_comp . describe () Sales Company FB count 2.000000 mean 296.500000 std 75.660426 min 243.000000 25% 269.750000 50% 296.500000 75% 323.250000 max 350.000000 GOOG count 2.000000 mean 160.000000 std 56.568542 min 120.000000 25% 140.000000 50% 160.000000 75% 180.000000 max 200.000000 MSFT count 2.000000 mean 232.000000 std 152.735065 min 124.000000 25% 178.000000 50% 232.000000 75% 286.000000 max 340.000000 by_comp . describe () . transpose () Company FB GOOG MSFT count mean std min 25% 50% 75% max count mean ... 75% max count mean std min 25% 50% 75% max Sales 2.0 296.5 75.660426 243.0 269.75 296.5 323.25 350.0 2.0 160.0 ... 180.0 200.0 2.0 232.0 152.735065 124.0 178.0 232.0 286.0 340.0 1 rows \u00d7 24 columns by_comp . describe () . transpose ()[ 'GOOG' ] count mean std min 25% 50% 75% max Sales 2.0 160.0 56.568542 120.0 140.0 160.0 180.0 200.0","title":"Groupby"},{"location":"bootcampsnotes/pandas/04-Groupby/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/05-Operations/","text":"================ by Jawad Haider 05 - Operations \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Operations 1.0.1 Info on Unique Values 1.0.2 Selecting Data 1.0.3 Applying Functions 1.0.4 Permanently Removing a Column 1.0.5 Get column and index names: 1.0.6 Sorting and Ordering a DataFrame: 2 Great Job! Thats the end of this part. Operations \u00b6 There are lots of operations with pandas that will be really useful to you, but don\u2019t fall into any distinct category. Let\u2019s show them here in this lecture: import pandas as pd df = pd . DataFrame ({ 'col1' :[ 1 , 2 , 3 , 4 ], 'col2' :[ 444 , 555 , 666 , 444 ], 'col3' :[ 'abc' , 'def' , 'ghi' , 'xyz' ]}) df . head () col1 col2 col3 0 1 444 abc 1 2 555 def 2 3 666 ghi 3 4 444 xyz Info on Unique Values \u00b6 df [ 'col2' ] . unique () array([444, 555, 666]) df [ 'col2' ] . nunique () 3 df [ 'col2' ] . value_counts () 444 2 555 1 666 1 Name: col2, dtype: int64 Selecting Data \u00b6 #Select from DataFrame using criteria from multiple columns newdf = df [( df [ 'col1' ] > 2 ) & ( df [ 'col2' ] == 444 )] newdf col1 col2 col3 3 4 444 xyz Applying Functions \u00b6 def times2 ( x ): return x * 2 df [ 'col1' ] . apply ( times2 ) 0 2 1 4 2 6 3 8 Name: col1, dtype: int64 df [ 'col3' ] . apply ( len ) 0 3 1 3 2 3 3 3 Name: col3, dtype: int64 df [ 'col1' ] . sum () 10 Permanently Removing a Column \u00b6 del df [ 'col1' ] df col2 col3 0 444 abc 1 555 def 2 666 ghi 3 444 xyz Get column and index names: \u00b6 df . columns Index(['col2', 'col3'], dtype='object') df . index RangeIndex(start=0, stop=4, step=1) Sorting and Ordering a DataFrame: \u00b6 df col2 col3 0 444 abc 1 555 def 2 666 ghi 3 444 xyz df . sort_values ( by = 'col2' ) #inplace=False by default col2 col3 0 444 abc 3 444 xyz 1 555 def 2 666 ghi Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"05 Operations"},{"location":"bootcampsnotes/pandas/05-Operations/#05-operations","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Operations 1.0.1 Info on Unique Values 1.0.2 Selecting Data 1.0.3 Applying Functions 1.0.4 Permanently Removing a Column 1.0.5 Get column and index names: 1.0.6 Sorting and Ordering a DataFrame: 2 Great Job! Thats the end of this part.","title":"05 - Operations"},{"location":"bootcampsnotes/pandas/05-Operations/#operations","text":"There are lots of operations with pandas that will be really useful to you, but don\u2019t fall into any distinct category. Let\u2019s show them here in this lecture: import pandas as pd df = pd . DataFrame ({ 'col1' :[ 1 , 2 , 3 , 4 ], 'col2' :[ 444 , 555 , 666 , 444 ], 'col3' :[ 'abc' , 'def' , 'ghi' , 'xyz' ]}) df . head () col1 col2 col3 0 1 444 abc 1 2 555 def 2 3 666 ghi 3 4 444 xyz","title":"Operations"},{"location":"bootcampsnotes/pandas/05-Operations/#info-on-unique-values","text":"df [ 'col2' ] . unique () array([444, 555, 666]) df [ 'col2' ] . nunique () 3 df [ 'col2' ] . value_counts () 444 2 555 1 666 1 Name: col2, dtype: int64","title":"Info on Unique Values"},{"location":"bootcampsnotes/pandas/05-Operations/#selecting-data","text":"#Select from DataFrame using criteria from multiple columns newdf = df [( df [ 'col1' ] > 2 ) & ( df [ 'col2' ] == 444 )] newdf col1 col2 col3 3 4 444 xyz","title":"Selecting Data"},{"location":"bootcampsnotes/pandas/05-Operations/#applying-functions","text":"def times2 ( x ): return x * 2 df [ 'col1' ] . apply ( times2 ) 0 2 1 4 2 6 3 8 Name: col1, dtype: int64 df [ 'col3' ] . apply ( len ) 0 3 1 3 2 3 3 3 Name: col3, dtype: int64 df [ 'col1' ] . sum () 10","title":"Applying Functions"},{"location":"bootcampsnotes/pandas/05-Operations/#permanently-removing-a-column","text":"del df [ 'col1' ] df col2 col3 0 444 abc 1 555 def 2 666 ghi 3 444 xyz","title":"Permanently Removing a Column"},{"location":"bootcampsnotes/pandas/05-Operations/#get-column-and-index-names","text":"df . columns Index(['col2', 'col3'], dtype='object') df . index RangeIndex(start=0, stop=4, step=1)","title":"Get column and index names:"},{"location":"bootcampsnotes/pandas/05-Operations/#sorting-and-ordering-a-dataframe","text":"df col2 col3 0 444 abc 1 555 def 2 666 ghi 3 444 xyz df . sort_values ( by = 'col2' ) #inplace=False by default col2 col3 0 444 abc 3 444 xyz 1 555 def 2 666 ghi","title":"Sorting and Ordering a DataFrame:"},{"location":"bootcampsnotes/pandas/05-Operations/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/","text":"================ by Jawad Haider 06 - Input Output \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Data Input and Output 1.1 CSV 1.1.1 CSV Input 1.1.2 CSV Output 1.2 Excel 1.2.1 Excel Input 1.2.2 Excel Output 1.3 HTML 1.3.1 HTML Input 2 Great Job! Thats the end of this part. NOTE: Typically we will just be either reading csv files directly or using pandas-datareader to pull data from the web. Consider this lecture just a quick overview of what is possible with pandas (we won\u2019t be working with SQL or Excel files in this course) Data Input and Output \u00b6 This notebook is the reference code for getting input and output, pandas can read a variety of file types using its pd.read_ methods. Let\u2019s take a look at the most common data types: import numpy as np import pandas as pd CSV \u00b6 Comma Separated Values files are text files that use commas as field delimeters. Unless you\u2019re running the virtual environment included with the course, you may need to install xlrd and openpyxl . In your terminal/command prompt run: conda install xlrd conda install openpyxl Then restart Jupyter Notebook. (or use pip install if you aren\u2019t using the Anaconda Distribution) CSV Input \u00b6 df = pd . read_csv ( 'example.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 CSV Output \u00b6 df . to_csv ( 'example.csv' , index = False ) Excel \u00b6 Pandas can read and write MS Excel files. However, this only imports data, not formulas or images. A file that contains images or macros may cause the .read_excel() method to crash. Excel Input \u00b6 pd . read_excel ( 'Excel_Sample.xlsx' , sheet_name = 'Sheet1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Excel Output \u00b6 df . to_excel ( 'Excel_Sample.xlsx' , sheet_name = 'Sheet1' ) HTML \u00b6 Pandas can read table tabs off of HTML. Unless you\u2019re running the virtual environment included with the course, you may need to install lxml , htmllib5 , and BeautifulSoup4 . In your terminal/command prompt run: conda install lxml conda install html5lib conda install beautifulsoup4 Then restart Jupyter Notebook. (or use pip install if you aren\u2019t using the Anaconda Distribution) HTML Input \u00b6 Pandas read_html function will read tables off of a webpage and return a list of DataFrame objects: df = pd . read_html ( 'http://www.fdic.gov/bank/individual/failed/banklist.html' ) df [ 0 ] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Bank Name City ST CERT Acquiring Institution Closing Date Updated Date 0 Washington Federal Bank for Savings Chicago IL 30570 Royal Savings Bank December 15, 2017 February 21, 2018 1 The Farmers and Merchants State Bank of Argonia Argonia KS 17719 Conway Bank October 13, 2017 February 21, 2018 2 Fayette County Bank Saint Elmo IL 1802 United Fidelity Bank, fsb May 26, 2017 July 26, 2017 3 Guaranty Bank, (d/b/a BestBank in Georgia & Mi... Milwaukee WI 30003 First-Citizens Bank & Trust Company May 5, 2017 March 22, 2018 4 First NBC Bank New Orleans LA 58302 Whitney Bank April 28, 2017 December 5, 2017 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"06 Data Input and Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#06-input-output","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Data Input and Output 1.1 CSV 1.1.1 CSV Input 1.1.2 CSV Output 1.2 Excel 1.2.1 Excel Input 1.2.2 Excel Output 1.3 HTML 1.3.1 HTML Input 2 Great Job! Thats the end of this part. NOTE: Typically we will just be either reading csv files directly or using pandas-datareader to pull data from the web. Consider this lecture just a quick overview of what is possible with pandas (we won\u2019t be working with SQL or Excel files in this course)","title":"06 - Input Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#data-input-and-output","text":"This notebook is the reference code for getting input and output, pandas can read a variety of file types using its pd.read_ methods. Let\u2019s take a look at the most common data types: import numpy as np import pandas as pd","title":"Data Input and Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#csv","text":"Comma Separated Values files are text files that use commas as field delimeters. Unless you\u2019re running the virtual environment included with the course, you may need to install xlrd and openpyxl . In your terminal/command prompt run: conda install xlrd conda install openpyxl Then restart Jupyter Notebook. (or use pip install if you aren\u2019t using the Anaconda Distribution)","title":"CSV"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#csv-input","text":"df = pd . read_csv ( 'example.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15","title":"CSV Input"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#csv-output","text":"df . to_csv ( 'example.csv' , index = False )","title":"CSV Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#excel","text":"Pandas can read and write MS Excel files. However, this only imports data, not formulas or images. A file that contains images or macros may cause the .read_excel() method to crash.","title":"Excel"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#excel-input","text":"pd . read_excel ( 'Excel_Sample.xlsx' , sheet_name = 'Sheet1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15","title":"Excel Input"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#excel-output","text":"df . to_excel ( 'Excel_Sample.xlsx' , sheet_name = 'Sheet1' )","title":"Excel Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#html","text":"Pandas can read table tabs off of HTML. Unless you\u2019re running the virtual environment included with the course, you may need to install lxml , htmllib5 , and BeautifulSoup4 . In your terminal/command prompt run: conda install lxml conda install html5lib conda install beautifulsoup4 Then restart Jupyter Notebook. (or use pip install if you aren\u2019t using the Anaconda Distribution)","title":"HTML"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#html-input","text":"Pandas read_html function will read tables off of a webpage and return a list of DataFrame objects: df = pd . read_html ( 'http://www.fdic.gov/bank/individual/failed/banklist.html' ) df [ 0 ] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Bank Name City ST CERT Acquiring Institution Closing Date Updated Date 0 Washington Federal Bank for Savings Chicago IL 30570 Royal Savings Bank December 15, 2017 February 21, 2018 1 The Farmers and Merchants State Bank of Argonia Argonia KS 17719 Conway Bank October 13, 2017 February 21, 2018 2 Fayette County Bank Saint Elmo IL 1802 United Fidelity Bank, fsb May 26, 2017 July 26, 2017 3 Guaranty Bank, (d/b/a BestBank in Georgia & Mi... Milwaukee WI 30003 First-Citizens Bank & Trust Company May 5, 2017 March 22, 2018 4 First NBC Bank New Orleans LA 58302 Whitney Bank April 28, 2017 December 5, 2017","title":"HTML Input"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/07-Pandas-Exercises/","text":"================ by Jawad Haider 07 - Pandas Excercise \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Pandas Exercises 2 Great Job! Thats the end of this part. Pandas Exercises \u00b6 TASK: Import pandas # CODE HERE TASK: Read in the bank.csv file that is located under the 01-Crash-Course-Pandas folder. Pay close attention to where the .csv file is located! Please don\u2019t post to the QA forums if you can\u2019t figure this one out, instead, run our solutions notebook directly to see how its done. # CODE HERE TASK: Display the first 5 rows of the data set # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no TASK: What is the average (mean) age of the people in the dataset? # CODE HERE 41.17009511170095 TASK: What is the marital status of the youngest person in the dataset? HINT # CODE HERE 'single' TASK: How many unique job categories are there? # CODE HERE 12 TASK: How many people are there per job category? (Take a peek at the expected output) # CODE HERE management 969 blue-collar 946 technician 768 admin. 478 services 417 retired 230 self-employed 183 entrepreneur 168 unemployed 128 housemaid 112 student 84 unknown 38 Name: job, dtype: int64 **TASK: What percent of people in the dataset were married? ** #CODE HERE 61.86684361866843 TASK: There is a column labeled \u201cdefault\u201d. Use pandas\u2019 .map() method to create a new column called \u201cdefault code\u201d which contains a 0 if there was no default, or a 1 if there was a default. Then show the head of the dataframe with this new column. Helpful Hint Link One Helpful Hint Link Two # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 TASK: Using pandas .apply() method, create a new column called \u201cmarital code\u201d. This column will only contained a shortened code of the possible marital status first letter. (For example \u201cm\u201d for \u201cmarried\u201d , \u201cs\u201d for \u201csingle\u201d etc\u2026 See if you can do this with a lambda expression. Lots of ways to do this one! Hint Link # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code marital code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 m 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 m 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 s 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 m 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 m TASK: What was the longest lasting duration? # CODE HERE 3025 TASK: What is the most common education level for people who are unemployed? # CODE HERE secondary 68 tertiary 32 primary 26 unknown 2 Name: education, dtype: int64 TASK: What is the average (mean) age for being unemployed? # CODE HERE 40.90625 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"07 Pandas Exercises"},{"location":"bootcampsnotes/pandas/07-Pandas-Exercises/#07-pandas-excercise","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Pandas Exercises 2 Great Job! Thats the end of this part.","title":"07 - Pandas Excercise"},{"location":"bootcampsnotes/pandas/07-Pandas-Exercises/#pandas-exercises","text":"TASK: Import pandas # CODE HERE TASK: Read in the bank.csv file that is located under the 01-Crash-Course-Pandas folder. Pay close attention to where the .csv file is located! Please don\u2019t post to the QA forums if you can\u2019t figure this one out, instead, run our solutions notebook directly to see how its done. # CODE HERE TASK: Display the first 5 rows of the data set # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no TASK: What is the average (mean) age of the people in the dataset? # CODE HERE 41.17009511170095 TASK: What is the marital status of the youngest person in the dataset? HINT # CODE HERE 'single' TASK: How many unique job categories are there? # CODE HERE 12 TASK: How many people are there per job category? (Take a peek at the expected output) # CODE HERE management 969 blue-collar 946 technician 768 admin. 478 services 417 retired 230 self-employed 183 entrepreneur 168 unemployed 128 housemaid 112 student 84 unknown 38 Name: job, dtype: int64 **TASK: What percent of people in the dataset were married? ** #CODE HERE 61.86684361866843 TASK: There is a column labeled \u201cdefault\u201d. Use pandas\u2019 .map() method to create a new column called \u201cdefault code\u201d which contains a 0 if there was no default, or a 1 if there was a default. Then show the head of the dataframe with this new column. Helpful Hint Link One Helpful Hint Link Two # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 TASK: Using pandas .apply() method, create a new column called \u201cmarital code\u201d. This column will only contained a shortened code of the possible marital status first letter. (For example \u201cm\u201d for \u201cmarried\u201d , \u201cs\u201d for \u201csingle\u201d etc\u2026 See if you can do this with a lambda expression. Lots of ways to do this one! Hint Link # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code marital code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 m 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 m 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 s 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 m 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 m TASK: What was the longest lasting duration? # CODE HERE 3025 TASK: What is the most common education level for people who are unemployed? # CODE HERE secondary 68 tertiary 32 primary 26 unknown 2 Name: education, dtype: int64 TASK: What is the average (mean) age for being unemployed? # CODE HERE 40.90625","title":"Pandas Exercises"},{"location":"bootcampsnotes/pandas/07-Pandas-Exercises/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/","text":"================ by Jawad Haider 07 - Pandas Excercise Solutions \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Pandas Exercises - Solutions 2 Great Job! Thats the end of this part. Pandas Exercises - Solutions \u00b6 TASK: Import pandas # CODE HERE import pandas as pd TASK: Read in the bank.csv file that is located under the 01-Crash-Course-Pandas folder. Pay close attention to where the .csv file is located! Please don\u2019t post to the QA forums if you can\u2019t figure this one out, instead, run our solutions notebook directly to see how its done. df = pd . read_csv ( 'bank.csv' ) TASK: Display the first 5 rows of the data set # CODE HERE df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no TASK: What is the average (mean) age of the people in the dataset? # CODE HERE df [ 'age' ] . mean () 41.17009511170095 TASK: What is the marital status of the youngest person in the dataset? HINT # CODE HERE df [ 'age' ] . idxmin () 503 df . iloc [ 503 ][ 'marital' ] 'single' TASK: How many unique job categories are there? # CODE HERE df [ 'job' ] . nunique () 12 TASK: How many people are there per job category? (Take a peek at the expected output) # CODE HERE df [ 'job' ] . value_counts () management 969 blue-collar 946 technician 768 admin. 478 services 417 retired 230 self-employed 183 entrepreneur 168 unemployed 128 housemaid 112 student 84 unknown 38 Name: job, dtype: int64 **TASK: What percent of people in the dataset were married? ** #CODE HERE # Many, many ways to do this one! Here is just one way: 100 * df [ 'marital' ] . value_counts ()[ 'married' ] / len ( df ) # df['marital].value_counts() 61.86684361866843 TASK: There is a column labeled \u201cdefault\u201d. Use pandas\u2019 .map() method to create a new column called \u201cdefault code\u201d which contains a 0 if there was no default, or a 1 if there was a default. Then show the head of the dataframe with this new column. Helpful Hint Link One Helpful Hint Link Two df [ 'default code' ] = df [ 'default' ] . map ({ 'no' : 0 , 'yes' : 1 }) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 TASK: Using pandas .apply() method, create a new column called \u201cmarital code\u201d. This column will only contained a shortened code of the possible marital status first letter. (For example \u201cm\u201d for \u201cmarried\u201d , \u201cs\u201d for \u201csingle\u201d etc\u2026 See if you can do this with a lambda expression. Lots of ways to do this one! Hint Link # CODE HERE df [ 'marital code' ] = df [ 'marital' ] . apply ( lambda status : status [ 0 ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code marital code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 m 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 m 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 s 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 m 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 m TASK: What was the longest lasting duration? # CODE HERE df [ 'duration' ] . max () 3025 TASK: What is the most common education level for people who are unemployed? # CODE HERE df [ df [ 'job' ] == 'unemployed' ][ 'education' ] . value_counts () secondary 68 tertiary 32 primary 26 unknown 2 Name: education, dtype: int64 TASK: What is the average (mean) age for being unemployed? # CODE HERE df [ df [ 'job' ] == 'unemployed' ][ 'age' ] . mean () 40.90625 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"08 Pandas Exercises Solutions"},{"location":"bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/#07-pandas-excercise-solutions","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Pandas Exercises - Solutions 2 Great Job! Thats the end of this part.","title":"07 - Pandas Excercise Solutions"},{"location":"bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/#pandas-exercises-solutions","text":"TASK: Import pandas # CODE HERE import pandas as pd TASK: Read in the bank.csv file that is located under the 01-Crash-Course-Pandas folder. Pay close attention to where the .csv file is located! Please don\u2019t post to the QA forums if you can\u2019t figure this one out, instead, run our solutions notebook directly to see how its done. df = pd . read_csv ( 'bank.csv' ) TASK: Display the first 5 rows of the data set # CODE HERE df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no TASK: What is the average (mean) age of the people in the dataset? # CODE HERE df [ 'age' ] . mean () 41.17009511170095 TASK: What is the marital status of the youngest person in the dataset? HINT # CODE HERE df [ 'age' ] . idxmin () 503 df . iloc [ 503 ][ 'marital' ] 'single' TASK: How many unique job categories are there? # CODE HERE df [ 'job' ] . nunique () 12 TASK: How many people are there per job category? (Take a peek at the expected output) # CODE HERE df [ 'job' ] . value_counts () management 969 blue-collar 946 technician 768 admin. 478 services 417 retired 230 self-employed 183 entrepreneur 168 unemployed 128 housemaid 112 student 84 unknown 38 Name: job, dtype: int64 **TASK: What percent of people in the dataset were married? ** #CODE HERE # Many, many ways to do this one! Here is just one way: 100 * df [ 'marital' ] . value_counts ()[ 'married' ] / len ( df ) # df['marital].value_counts() 61.86684361866843 TASK: There is a column labeled \u201cdefault\u201d. Use pandas\u2019 .map() method to create a new column called \u201cdefault code\u201d which contains a 0 if there was no default, or a 1 if there was a default. Then show the head of the dataframe with this new column. Helpful Hint Link One Helpful Hint Link Two df [ 'default code' ] = df [ 'default' ] . map ({ 'no' : 0 , 'yes' : 1 }) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 TASK: Using pandas .apply() method, create a new column called \u201cmarital code\u201d. This column will only contained a shortened code of the possible marital status first letter. (For example \u201cm\u201d for \u201cmarried\u201d , \u201cs\u201d for \u201csingle\u201d etc\u2026 See if you can do this with a lambda expression. Lots of ways to do this one! Hint Link # CODE HERE df [ 'marital code' ] = df [ 'marital' ] . apply ( lambda status : status [ 0 ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code marital code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 m 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 m 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 s 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 m 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 m TASK: What was the longest lasting duration? # CODE HERE df [ 'duration' ] . max () 3025 TASK: What is the most common education level for people who are unemployed? # CODE HERE df [ df [ 'job' ] == 'unemployed' ][ 'education' ] . value_counts () secondary 68 tertiary 32 primary 26 unknown 2 Name: education, dtype: int64 TASK: What is the average (mean) age for being unemployed? # CODE HERE df [ df [ 'job' ] == 'unemployed' ][ 'age' ] . mean () 40.90625","title":"Pandas Exercises - Solutions"},{"location":"bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/","text":"================ by Jawad Haider 00 - Tensor Basics \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Tensor Basics 1.1 Perform standard imports 1.2 Converting NumPy arrays to PyTorch tensors 1.3 Copying vs. sharing 1.4 Class constructors 1.5 Creating tensors from scratch 1.5.1 Uninitialized tensors with .empty() 1.5.2 Initialized tensors with .zeros() and .ones() 1.5.3 Tensors from ranges 1.5.4 Tensors from data 1.5.5 Changing the dtype of existing tensors 1.5.6 Random number tensors 1.5.7 Random number tensors that follow the input size 1.5.8 Setting the random seed 1.6 Tensor attributes 1.6.1 Great job! Tensor Basics \u00b6 This section covers: * Converting NumPy arrays to PyTorch tensors * Creating tensors from scratch Perform standard imports \u00b6 import torch import numpy as np Confirm you\u2019re using PyTorch version 1.1.0 torch . __version__ '1.1.0' Converting NumPy arrays to PyTorch tensors \u00b6 A torch.Tensor is a multi-dimensional matrix containing elements of a single data type. Calculations between tensors can only happen if the tensors share the same dtype. In some cases tensors are used as a replacement for NumPy to use the power of GPUs (more on this later). arr = np . array ([ 1 , 2 , 3 , 4 , 5 ]) print ( arr ) print ( arr . dtype ) print ( type ( arr )) [1 2 3 4 5] int32 <class 'numpy.ndarray'> x = torch . from_numpy ( arr ) # Equivalent to x = torch.as_tensor(arr) print ( x ) tensor([1, 2, 3, 4, 5], dtype=torch.int32) # Print the type of data held by the tensor print ( x . dtype ) torch.int32 # Print the tensor object type print ( type ( x )) print ( x . type ()) # this is more specific! <class 'torch.Tensor'> torch.IntTensor arr2 = np . arange ( 0. , 12. ) . reshape ( 4 , 3 ) print ( arr2 ) [[ 0. 1. 2.] [ 3. 4. 5.] [ 6. 7. 8.] [ 9. 10. 11.]] x2 = torch . from_numpy ( arr2 ) print ( x2 ) print ( x2 . type ()) tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]], dtype=torch.float64) torch.DoubleTensor Here torch.DoubleTensor refers to 64-bit floating point data. Tensor Datatypes TYPE NAME EQUIVALENT TENSOR TYPE 32-bit integer (signed) torch.int32 torch.int IntTensor 64-bit integer (signed) torch.int64 torch.long LongTensor 16-bit integer (signed) torch.int16 torch.short ShortTensor 32-bit floating point torch.float32 torch.float FloatTensor 64-bit floating point torch.float64 torch.double DoubleTensor 16-bit floating point torch.float16 torch.half HalfTensor 8-bit integer (signed) torch.int8 CharTensor 8-bit integer (unsigned) torch.uint8 ByteTensor Copying vs. sharing \u00b6 torch.from_numpy() torch.as_tensor() torch.tensor() There are a number of different functions available for creating tensors . When using torch.from_numpy() and torch.as_tensor() , the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy. # Using torch.from_numpy() arr = np . arange ( 0 , 5 ) t = torch . from_numpy ( arr ) print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) arr [ 2 ] = 77 print ( t ) tensor([ 0, 1, 77, 3, 4], dtype=torch.int32) # Using torch.tensor() arr = np . arange ( 0 , 5 ) t = torch . tensor ( arr ) print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) arr [ 2 ] = 77 print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) Class constructors \u00b6 torch.Tensor() torch.FloatTensor() torch.LongTensor() , etc. There\u2019s a subtle difference between using the factory function torch.tensor(data) and the class constructor torch.Tensor(data) . The factory function determines the dtype from the incoming data, or from a passed-in dtype argument. The class constructor torch.Tensor() is simply an alias for torch.FloatTensor(data) . Consider the following: data = np . array ([ 1 , 2 , 3 ]) a = torch . Tensor ( data ) # Equivalent to cc = torch.FloatTensor(data) print ( a , a . type ()) tensor([1., 2., 3.]) torch.FloatTensor b = torch . tensor ( data ) print ( b , b . type ()) tensor([1, 2, 3], dtype=torch.int32) torch.IntTensor c = torch . tensor ( data , dtype = torch . long ) print ( c , c . type ()) tensor([1, 2, 3]) torch.LongTensor Creating tensors from scratch \u00b6 Uninitialized tensors with .empty() \u00b6 torch.empty() returns an uninitialized tensor. Essentially a block of memory is allocated according to the size of the tensor, and any values already sitting in the block are returned. This is similar to the behavior of numpy.empty() . x = torch . empty ( 4 , 3 ) print ( x ) tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) Initialized tensors with .zeros() and .ones() \u00b6 torch.zeros(size) torch.ones(size) It\u2019s a good idea to pass in the intended dtype. x = torch . zeros ( 4 , 3 , dtype = torch . int64 ) print ( x ) tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) Tensors from ranges \u00b6 torch.arange(start,end,step) torch.linspace(start,end,steps) Note that with .arange() , end is exclusive, while with linspace() , end is inclusive. x = torch . arange ( 0 , 18 , 2 ) . reshape ( 3 , 3 ) print ( x ) tensor([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]]) x = torch . linspace ( 0 , 18 , 12 ) . reshape ( 3 , 4 ) print ( x ) tensor([[ 0.0000, 1.6364, 3.2727, 4.9091], [ 6.5455, 8.1818, 9.8182, 11.4545], [13.0909, 14.7273, 16.3636, 18.0000]]) Tensors from data \u00b6 torch.tensor() will choose the dtype based on incoming data: x = torch . tensor ([ 1 , 2 , 3 , 4 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([1, 2, 3, 4]) torch.int64 torch.LongTensor Alternatively you can set the type by the tensor method used. For a list of tensor types visit https://pytorch.org/docs/stable/tensors.html x = torch . FloatTensor ([ 5 , 6 , 7 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([5., 6., 7.]) torch.float32 torch.FloatTensor You can also pass the dtype in as an argument. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype x = torch . tensor ([ 8 , 9 , - 3 ], dtype = torch . int ) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([ 8, 9, -3], dtype=torch.int32) torch.int32 torch.IntTensor Changing the dtype of existing tensors \u00b6 Don\u2019t be tempted to use x = torch.tensor(x, dtype=torch.type) as it will raise an error about improper use of tensor cloning. Instead, use the tensor .type() method. print ( 'Old:' , x . type ()) x = x . type ( torch . int64 ) print ( 'New:' , x . type ()) Old: torch.IntTensor New: torch.LongTensor Random number tensors \u00b6 torch.rand(size) returns random samples from a uniform distribution over [0, 1) torch.randn(size) returns samples from the \u201cstandard normal\u201d distribution [\u03c3 = 1] Unlike rand which is uniform, values closer to zero are more likely to appear. torch.randint(low,high,size) returns random integers from low (inclusive) to high (exclusive) x = torch . rand ( 4 , 3 ) print ( x ) tensor([[0.0211, 0.2336, 0.6775], [0.4790, 0.5132, 0.9878], [0.7552, 0.0789, 0.1860], [0.6712, 0.1564, 0.3753]]) x = torch . randn ( 4 , 3 ) print ( x ) tensor([[ 0.7164, -0.1538, -0.9980], [-1.8252, 1.1863, -0.1523], [ 1.4093, -0.0212, -1.5598], [ 0.1831, -0.6961, 1.3497]]) x = torch . randint ( 0 , 5 , ( 4 , 3 )) print ( x ) tensor([[0, 3, 0], [1, 3, 4], [1, 2, 3], [4, 4, 3]]) Random number tensors that follow the input size \u00b6 torch.rand_like(input) torch.randn_like(input) torch.randint_like(input,low,high) these return random number tensors with the same size as input x = torch . zeros ( 2 , 5 ) print ( x ) tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) x2 = torch . randn_like ( x ) print ( x2 ) tensor([[-0.5442, -0.3149, 0.0922, 1.1829, -0.7873], [ 0.3143, 0.9465, 0.4534, 0.4623, 2.2044]]) The same syntax can be used with torch.zeros_like(input) torch.ones_like(input) x3 = torch . ones_like ( x2 ) print ( x3 ) tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) Setting the random seed \u00b6 torch.manual_seed(int) is used to obtain reproducible results torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]]) torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]]) Tensor attributes \u00b6 Besides dtype , we can look at other tensor attributes like shape , device and layout x . shape torch.Size([2, 3]) x . size () # equivalent to x.shape torch.Size([2, 3]) x . device device(type='cpu') PyTorch supports use of multiple devices , harnessing the power of one or more GPUs in addition to the CPU. We won\u2019t explore that here, but you should know that operations between tensors can only happen for tensors installed on the same device. x . layout torch.strided PyTorch has a class to hold the memory layout option. The default setting of strided will suit our purposes throughout the course. Great job! \u00b6","title":"00 Tensor Basics"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#00-tensor-basics","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Tensor Basics 1.1 Perform standard imports 1.2 Converting NumPy arrays to PyTorch tensors 1.3 Copying vs. sharing 1.4 Class constructors 1.5 Creating tensors from scratch 1.5.1 Uninitialized tensors with .empty() 1.5.2 Initialized tensors with .zeros() and .ones() 1.5.3 Tensors from ranges 1.5.4 Tensors from data 1.5.5 Changing the dtype of existing tensors 1.5.6 Random number tensors 1.5.7 Random number tensors that follow the input size 1.5.8 Setting the random seed 1.6 Tensor attributes 1.6.1 Great job!","title":"00 - Tensor Basics"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#tensor-basics","text":"This section covers: * Converting NumPy arrays to PyTorch tensors * Creating tensors from scratch","title":"Tensor Basics"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#perform-standard-imports","text":"import torch import numpy as np Confirm you\u2019re using PyTorch version 1.1.0 torch . __version__ '1.1.0'","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#converting-numpy-arrays-to-pytorch-tensors","text":"A torch.Tensor is a multi-dimensional matrix containing elements of a single data type. Calculations between tensors can only happen if the tensors share the same dtype. In some cases tensors are used as a replacement for NumPy to use the power of GPUs (more on this later). arr = np . array ([ 1 , 2 , 3 , 4 , 5 ]) print ( arr ) print ( arr . dtype ) print ( type ( arr )) [1 2 3 4 5] int32 <class 'numpy.ndarray'> x = torch . from_numpy ( arr ) # Equivalent to x = torch.as_tensor(arr) print ( x ) tensor([1, 2, 3, 4, 5], dtype=torch.int32) # Print the type of data held by the tensor print ( x . dtype ) torch.int32 # Print the tensor object type print ( type ( x )) print ( x . type ()) # this is more specific! <class 'torch.Tensor'> torch.IntTensor arr2 = np . arange ( 0. , 12. ) . reshape ( 4 , 3 ) print ( arr2 ) [[ 0. 1. 2.] [ 3. 4. 5.] [ 6. 7. 8.] [ 9. 10. 11.]] x2 = torch . from_numpy ( arr2 ) print ( x2 ) print ( x2 . type ()) tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]], dtype=torch.float64) torch.DoubleTensor Here torch.DoubleTensor refers to 64-bit floating point data.","title":"Converting NumPy arrays to PyTorch tensors"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#copying-vs-sharing","text":"torch.from_numpy() torch.as_tensor() torch.tensor() There are a number of different functions available for creating tensors . When using torch.from_numpy() and torch.as_tensor() , the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy. # Using torch.from_numpy() arr = np . arange ( 0 , 5 ) t = torch . from_numpy ( arr ) print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) arr [ 2 ] = 77 print ( t ) tensor([ 0, 1, 77, 3, 4], dtype=torch.int32) # Using torch.tensor() arr = np . arange ( 0 , 5 ) t = torch . tensor ( arr ) print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) arr [ 2 ] = 77 print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32)","title":"Copying vs. sharing"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#class-constructors","text":"torch.Tensor() torch.FloatTensor() torch.LongTensor() , etc. There\u2019s a subtle difference between using the factory function torch.tensor(data) and the class constructor torch.Tensor(data) . The factory function determines the dtype from the incoming data, or from a passed-in dtype argument. The class constructor torch.Tensor() is simply an alias for torch.FloatTensor(data) . Consider the following: data = np . array ([ 1 , 2 , 3 ]) a = torch . Tensor ( data ) # Equivalent to cc = torch.FloatTensor(data) print ( a , a . type ()) tensor([1., 2., 3.]) torch.FloatTensor b = torch . tensor ( data ) print ( b , b . type ()) tensor([1, 2, 3], dtype=torch.int32) torch.IntTensor c = torch . tensor ( data , dtype = torch . long ) print ( c , c . type ()) tensor([1, 2, 3]) torch.LongTensor","title":"Class constructors"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#creating-tensors-from-scratch","text":"","title":"Creating tensors from scratch"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#uninitialized-tensors-with-empty","text":"torch.empty() returns an uninitialized tensor. Essentially a block of memory is allocated according to the size of the tensor, and any values already sitting in the block are returned. This is similar to the behavior of numpy.empty() . x = torch . empty ( 4 , 3 ) print ( x ) tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])","title":"Uninitialized tensors with .empty()"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#initialized-tensors-with-zeros-and-ones","text":"torch.zeros(size) torch.ones(size) It\u2019s a good idea to pass in the intended dtype. x = torch . zeros ( 4 , 3 , dtype = torch . int64 ) print ( x ) tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]])","title":"Initialized tensors with .zeros() and .ones()"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#tensors-from-ranges","text":"torch.arange(start,end,step) torch.linspace(start,end,steps) Note that with .arange() , end is exclusive, while with linspace() , end is inclusive. x = torch . arange ( 0 , 18 , 2 ) . reshape ( 3 , 3 ) print ( x ) tensor([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]]) x = torch . linspace ( 0 , 18 , 12 ) . reshape ( 3 , 4 ) print ( x ) tensor([[ 0.0000, 1.6364, 3.2727, 4.9091], [ 6.5455, 8.1818, 9.8182, 11.4545], [13.0909, 14.7273, 16.3636, 18.0000]])","title":"Tensors from ranges"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#tensors-from-data","text":"torch.tensor() will choose the dtype based on incoming data: x = torch . tensor ([ 1 , 2 , 3 , 4 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([1, 2, 3, 4]) torch.int64 torch.LongTensor Alternatively you can set the type by the tensor method used. For a list of tensor types visit https://pytorch.org/docs/stable/tensors.html x = torch . FloatTensor ([ 5 , 6 , 7 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([5., 6., 7.]) torch.float32 torch.FloatTensor You can also pass the dtype in as an argument. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype x = torch . tensor ([ 8 , 9 , - 3 ], dtype = torch . int ) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([ 8, 9, -3], dtype=torch.int32) torch.int32 torch.IntTensor","title":"Tensors from data"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#changing-the-dtype-of-existing-tensors","text":"Don\u2019t be tempted to use x = torch.tensor(x, dtype=torch.type) as it will raise an error about improper use of tensor cloning. Instead, use the tensor .type() method. print ( 'Old:' , x . type ()) x = x . type ( torch . int64 ) print ( 'New:' , x . type ()) Old: torch.IntTensor New: torch.LongTensor","title":"Changing the dtype of existing tensors"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#random-number-tensors","text":"torch.rand(size) returns random samples from a uniform distribution over [0, 1) torch.randn(size) returns samples from the \u201cstandard normal\u201d distribution [\u03c3 = 1] Unlike rand which is uniform, values closer to zero are more likely to appear. torch.randint(low,high,size) returns random integers from low (inclusive) to high (exclusive) x = torch . rand ( 4 , 3 ) print ( x ) tensor([[0.0211, 0.2336, 0.6775], [0.4790, 0.5132, 0.9878], [0.7552, 0.0789, 0.1860], [0.6712, 0.1564, 0.3753]]) x = torch . randn ( 4 , 3 ) print ( x ) tensor([[ 0.7164, -0.1538, -0.9980], [-1.8252, 1.1863, -0.1523], [ 1.4093, -0.0212, -1.5598], [ 0.1831, -0.6961, 1.3497]]) x = torch . randint ( 0 , 5 , ( 4 , 3 )) print ( x ) tensor([[0, 3, 0], [1, 3, 4], [1, 2, 3], [4, 4, 3]])","title":"Random number tensors"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#random-number-tensors-that-follow-the-input-size","text":"torch.rand_like(input) torch.randn_like(input) torch.randint_like(input,low,high) these return random number tensors with the same size as input x = torch . zeros ( 2 , 5 ) print ( x ) tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) x2 = torch . randn_like ( x ) print ( x2 ) tensor([[-0.5442, -0.3149, 0.0922, 1.1829, -0.7873], [ 0.3143, 0.9465, 0.4534, 0.4623, 2.2044]]) The same syntax can be used with torch.zeros_like(input) torch.ones_like(input) x3 = torch . ones_like ( x2 ) print ( x3 ) tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]])","title":"Random number tensors that follow the input size"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#setting-the-random-seed","text":"torch.manual_seed(int) is used to obtain reproducible results torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]]) torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]])","title":"Setting the random seed"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#tensor-attributes","text":"Besides dtype , we can look at other tensor attributes like shape , device and layout x . shape torch.Size([2, 3]) x . size () # equivalent to x.shape torch.Size([2, 3]) x . device device(type='cpu') PyTorch supports use of multiple devices , harnessing the power of one or more GPUs in addition to the CPU. We won\u2019t explore that here, but you should know that operations between tensors can only happen for tensors installed on the same device. x . layout torch.strided PyTorch has a class to hold the memory layout option. The default setting of strided will suit our purposes throughout the course.","title":"Tensor attributes"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/","text":"================ by Jawad Haider 01 - Tensor Operations \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Tensor Operations 1.1 Perform standard imports 1.2 Indexing and slicing 1.3 Reshape tensors with .view() 1.3.1 Views reflect the most current data 1.3.2 Views can infer the correct size 1.3.3 Adopt another tensor\u2019s shape with .view_as() 1.4 Tensor Arithmetic 1.4.1 Basic Tensor Operations 1.5 Dot products 1.6 Matrix multiplication 1.6.1 Matrix multiplication with broadcasting 2 Advanced operations 2.1 L2 or Euclidian Norm 2.2 Number of elements 2.3 Great work! Tensor Operations \u00b6 This section covers: * Indexing and slicing * Reshaping tensors (tensor views) * Tensor arithmetic and basic operations * Dot products * Matrix multiplication * Additional, more advanced operations Perform standard imports \u00b6 import torch import numpy as np Indexing and slicing \u00b6 Extracting specific values from a tensor works just the same as with NumPy arrays Image source: http://www.scipy-lectures.org/ _images/numpy_indexing.png x = torch . arange ( 6 ) . reshape ( 3 , 2 ) print ( x ) tensor([[0, 1], [2, 3], [4, 5]]) # Grabbing the right hand column values x [:, 1 ] tensor([1, 3, 5]) # Grabbing the right hand column as a (3,1) slice x [:, 1 :] tensor([[1], [3], [5]]) Reshape tensors with .view() \u00b6 view() and reshape() do essentially the same thing by returning a reshaped tensor without changing the original tensor in place. There\u2019s a good discussion of the differences here . x = torch . arange ( 10 ) print ( x ) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) x . view ( 2 , 5 ) tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) x . view ( 5 , 2 ) tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) # x is unchanged x tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Views reflect the most current data \u00b6 z = x . view ( 2 , 5 ) x [ 0 ] = 234 print ( z ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) Views can infer the correct size \u00b6 By passing in -1 PyTorch will infer the correct value from the given tensor x . view ( 2 , - 1 ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) x . view ( - 1 , 5 ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) Adopt another tensor\u2019s shape with .view_as() \u00b6 view_as(input) only works with tensors that have the same number of elements. x . view_as ( z ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) Tensor Arithmetic \u00b6 Adding tensors can be performed a few different ways depending on the desired result. As a simple expression: a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a + b ) tensor([5., 7., 9.]) As arguments passed into a torch operation: print ( torch . add ( a , b )) tensor([5., 7., 9.]) With an output tensor passed in as an argument: result = torch . empty ( 3 ) torch . add ( a , b , out = result ) # equivalent to result=torch.add(a,b) print ( result ) tensor([5., 7., 9.]) Changing a tensor in-place a . add_ ( b ) # equivalent to a=torch.add(a,b) print ( a ) tensor([5., 7., 9.]) NOTE: Any operation that changes a tensor in-place is post-fixed with an underscore *. In the above example: a.add*(b) changed a . Basic Tensor Operations \u00b6 Arithmetic OPERATION FUNCTION DESCRIPTION a + b a.add(b) element wise addition a - b a.sub(b) subtraction a \\* b a.mul(b) multiplication a / b a.div(b) division a % b a.fmod(b) modulo (remainder after division) a b a.pow(b) power Monomial Operations OPERATION FUNCTION DESCRIPTION \\|a\\| torch.abs(a) absolute value 1/a torch.reciprocal(a) reciprocal $\\sqrt{a}$ torch.sqrt(a) square root log(a) torch.log(a) natural log e a torch.exp(a) exponential 12.34 ==\\> 12. torch.trunc(a) truncated integer 12.34 ==\\> 0.34 torch.frac(a) fractional component Trigonometry OPERATION FUNCTION DESCRIPTION sin(a) torch.sin(a) sine cos(a) torch.sin(a) cosine tan(a) torch.sin(a) tangent arcsin(a) torch.asin(a) arc sine arccos(a) torch.acos(a) arc cosine arctan(a) torch.atan(a) arc tangent sinh(a) torch.sinh(a) hyperbolic sine cosh(a) torch.cosh(a) hyperbolic cosine tanh(a) torch.tanh(a) hyperbolic tangent Summary Statistics OPERATION FUNCTION DESCRIPTION $\\sum a$ torch.sum(a) sum $\\bar a$ torch.mean(a) mean a max torch.max(a) maximum a min torch.min(a) minimum torch.max(a,b) returns a tensor of size a containing the element wise max between a and b NOTE: Most arithmetic operations require float values. Those that do work with integers return integer tensors. For example, torch.div(a,b) performs floor division (truncates the decimal) for integer types, and classic division for floats. Use the space below to experiment with different operations \u00b6 a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( torch . add ( a , b ) . sum ()) tensor(21.) Dot products \u00b6 A dot product is the sum of the products of the corresponding entries of two 1D tensors. If the tensors are both vectors, the dot product is given as: \\(\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d & e & f \\end{bmatrix} = ad + be + cf\\) If the tensors include a column vector, then the dot product is the sum of the result of the multiplied matrices. For example: \\(\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\\\ e \\\\ f \\end{bmatrix} = ad + be + cf\\) Dot products can be expressed as torch.dot(a,b) or a.dot(b) or b.dot(a) a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a . mul ( b )) # for reference print () print ( a . dot ( b )) tensor([ 4., 10., 18.]) tensor(32.) NOTE: There\u2019s a slight difference between torch.dot() and numpy.dot() . While torch.dot() only accepts 1D arguments and returns a dot product, numpy.dot() also accepts 2D arguments and performs matrix multiplication. We show matrix multiplication below. Matrix multiplication \u00b6 2D Matrix multiplication is possible when the number of columns in tensor A matches the number of rows in tensor B . In this case, the product of tensor A with size \\((x,y)\\) and tensor B with size \\((y,z)\\) results in a tensor of size \\((x,z)\\) $\\begin{bmatrix} a & b & c \\\\ d & e & f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m & n \\\\ p & q \\\\ r & s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) & (an+bq+cs) \\\\ (dm+ep+fr) & (dn+eq+fs) \\end{bmatrix}$ Image source: https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg Matrix multiplication can be computed using torch.mm(a,b) or a.mm(b) or a @ b a = torch . tensor ([[ 0 , 2 , 4 ],[ 1 , 3 , 5 ]], dtype = torch . float ) b = torch . tensor ([[ 6 , 7 ],[ 8 , 9 ],[ 10 , 11 ]], dtype = torch . float ) print ( 'a: ' , a . size ()) print ( 'b: ' , b . size ()) print ( 'a x b: ' , torch . mm ( a , b ) . size ()) a: torch.Size([2, 3]) b: torch.Size([3, 2]) a x b: torch.Size([2, 2]) print ( torch . mm ( a , b )) tensor([[56., 62.], [80., 89.]]) print ( a . mm ( b )) tensor([[56., 62.], [80., 89.]]) print ( a @ b ) tensor([[56., 62.], [80., 89.]]) Matrix multiplication with broadcasting \u00b6 Matrix multiplication that involves broadcasting can be computed using torch.matmul(a,b) or a.matmul(b) or a @ b t1 = torch . randn ( 2 , 3 , 4 ) t2 = torch . randn ( 4 , 5 ) print ( torch . matmul ( t1 , t2 ) . size ()) torch.Size([2, 3, 5]) However, the same operation raises a RuntimeError with torch.mm() : print ( torch . mm ( t1 , t2 ) . size ()) RuntimeError: matrices expected, got 3D, 2D tensors at ..\\aten\\src\\TH/generic/THTensorMath.cpp:956 Advanced operations \u00b6 L2 or Euclidian Norm \u00b6 See torch.norm() The Euclidian Norm gives the vector norm of \\(x\\) where \\(x=(x_1,x_2,...,x_n)\\) . It is calculated as \\({\\displaystyle \\left\\|{\\boldsymbol {x}}\\right\\|_{2}:={\\sqrt {x_{1}^{2}+\\cdots +x_{n}^{2}}}}\\) When applied to a matrix, torch.norm() returns the Frobenius norm by default. x = torch . tensor ([ 2. , 5. , 8. , 14. ]) x . norm () tensor(17.) Number of elements \u00b6 See torch.numel() Returns the number of elements in a tensor. x = torch . ones ( 3 , 7 ) x . numel () 21 This can be useful in certain calculations like Mean Squared Error: def mse(t1, t2): diff = t1 - t2 return torch.sum(diff * diff) / diff .numel() Great work! \u00b6","title":"01 Tensor Operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#01-tensor-operations","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Tensor Operations 1.1 Perform standard imports 1.2 Indexing and slicing 1.3 Reshape tensors with .view() 1.3.1 Views reflect the most current data 1.3.2 Views can infer the correct size 1.3.3 Adopt another tensor\u2019s shape with .view_as() 1.4 Tensor Arithmetic 1.4.1 Basic Tensor Operations 1.5 Dot products 1.6 Matrix multiplication 1.6.1 Matrix multiplication with broadcasting 2 Advanced operations 2.1 L2 or Euclidian Norm 2.2 Number of elements 2.3 Great work!","title":"01 - Tensor Operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#tensor-operations","text":"This section covers: * Indexing and slicing * Reshaping tensors (tensor views) * Tensor arithmetic and basic operations * Dot products * Matrix multiplication * Additional, more advanced operations","title":"Tensor Operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#perform-standard-imports","text":"import torch import numpy as np","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#indexing-and-slicing","text":"Extracting specific values from a tensor works just the same as with NumPy arrays Image source: http://www.scipy-lectures.org/ _images/numpy_indexing.png x = torch . arange ( 6 ) . reshape ( 3 , 2 ) print ( x ) tensor([[0, 1], [2, 3], [4, 5]]) # Grabbing the right hand column values x [:, 1 ] tensor([1, 3, 5]) # Grabbing the right hand column as a (3,1) slice x [:, 1 :] tensor([[1], [3], [5]])","title":"Indexing and slicing"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#reshape-tensors-with-view","text":"view() and reshape() do essentially the same thing by returning a reshaped tensor without changing the original tensor in place. There\u2019s a good discussion of the differences here . x = torch . arange ( 10 ) print ( x ) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) x . view ( 2 , 5 ) tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) x . view ( 5 , 2 ) tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) # x is unchanged x tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])","title":"Reshape tensors with .view()"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#views-reflect-the-most-current-data","text":"z = x . view ( 2 , 5 ) x [ 0 ] = 234 print ( z ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]])","title":"Views reflect the most current data"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#views-can-infer-the-correct-size","text":"By passing in -1 PyTorch will infer the correct value from the given tensor x . view ( 2 , - 1 ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) x . view ( - 1 , 5 ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]])","title":"Views can infer the correct size"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#adopt-another-tensors-shape-with-view_as","text":"view_as(input) only works with tensors that have the same number of elements. x . view_as ( z ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]])","title":"Adopt another tensor\u2019s shape with .view_as()"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#tensor-arithmetic","text":"Adding tensors can be performed a few different ways depending on the desired result. As a simple expression: a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a + b ) tensor([5., 7., 9.]) As arguments passed into a torch operation: print ( torch . add ( a , b )) tensor([5., 7., 9.]) With an output tensor passed in as an argument: result = torch . empty ( 3 ) torch . add ( a , b , out = result ) # equivalent to result=torch.add(a,b) print ( result ) tensor([5., 7., 9.]) Changing a tensor in-place a . add_ ( b ) # equivalent to a=torch.add(a,b) print ( a ) tensor([5., 7., 9.]) NOTE: Any operation that changes a tensor in-place is post-fixed with an underscore *. In the above example: a.add*(b) changed a .","title":"Tensor Arithmetic"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#basic-tensor-operations","text":"Arithmetic OPERATION FUNCTION DESCRIPTION a + b a.add(b) element wise addition a - b a.sub(b) subtraction a \\* b a.mul(b) multiplication a / b a.div(b) division a % b a.fmod(b) modulo (remainder after division) a b a.pow(b) power Monomial Operations OPERATION FUNCTION DESCRIPTION \\|a\\| torch.abs(a) absolute value 1/a torch.reciprocal(a) reciprocal $\\sqrt{a}$ torch.sqrt(a) square root log(a) torch.log(a) natural log e a torch.exp(a) exponential 12.34 ==\\> 12. torch.trunc(a) truncated integer 12.34 ==\\> 0.34 torch.frac(a) fractional component Trigonometry OPERATION FUNCTION DESCRIPTION sin(a) torch.sin(a) sine cos(a) torch.sin(a) cosine tan(a) torch.sin(a) tangent arcsin(a) torch.asin(a) arc sine arccos(a) torch.acos(a) arc cosine arctan(a) torch.atan(a) arc tangent sinh(a) torch.sinh(a) hyperbolic sine cosh(a) torch.cosh(a) hyperbolic cosine tanh(a) torch.tanh(a) hyperbolic tangent Summary Statistics OPERATION FUNCTION DESCRIPTION $\\sum a$ torch.sum(a) sum $\\bar a$ torch.mean(a) mean a max torch.max(a) maximum a min torch.min(a) minimum torch.max(a,b) returns a tensor of size a containing the element wise max between a and b NOTE: Most arithmetic operations require float values. Those that do work with integers return integer tensors. For example, torch.div(a,b) performs floor division (truncates the decimal) for integer types, and classic division for floats.","title":"Basic Tensor Operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#use-the-space-below-to-experiment-with-different-operations","text":"a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( torch . add ( a , b ) . sum ()) tensor(21.)","title":"Use the space below to experiment with different operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#dot-products","text":"A dot product is the sum of the products of the corresponding entries of two 1D tensors. If the tensors are both vectors, the dot product is given as: \\(\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d & e & f \\end{bmatrix} = ad + be + cf\\) If the tensors include a column vector, then the dot product is the sum of the result of the multiplied matrices. For example: \\(\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\\\ e \\\\ f \\end{bmatrix} = ad + be + cf\\) Dot products can be expressed as torch.dot(a,b) or a.dot(b) or b.dot(a) a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a . mul ( b )) # for reference print () print ( a . dot ( b )) tensor([ 4., 10., 18.]) tensor(32.) NOTE: There\u2019s a slight difference between torch.dot() and numpy.dot() . While torch.dot() only accepts 1D arguments and returns a dot product, numpy.dot() also accepts 2D arguments and performs matrix multiplication. We show matrix multiplication below.","title":"Dot products"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#matrix-multiplication","text":"2D Matrix multiplication is possible when the number of columns in tensor A matches the number of rows in tensor B . In this case, the product of tensor A with size \\((x,y)\\) and tensor B with size \\((y,z)\\) results in a tensor of size \\((x,z)\\) $\\begin{bmatrix} a & b & c \\\\ d & e & f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m & n \\\\ p & q \\\\ r & s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) & (an+bq+cs) \\\\ (dm+ep+fr) & (dn+eq+fs) \\end{bmatrix}$ Image source: https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg Matrix multiplication can be computed using torch.mm(a,b) or a.mm(b) or a @ b a = torch . tensor ([[ 0 , 2 , 4 ],[ 1 , 3 , 5 ]], dtype = torch . float ) b = torch . tensor ([[ 6 , 7 ],[ 8 , 9 ],[ 10 , 11 ]], dtype = torch . float ) print ( 'a: ' , a . size ()) print ( 'b: ' , b . size ()) print ( 'a x b: ' , torch . mm ( a , b ) . size ()) a: torch.Size([2, 3]) b: torch.Size([3, 2]) a x b: torch.Size([2, 2]) print ( torch . mm ( a , b )) tensor([[56., 62.], [80., 89.]]) print ( a . mm ( b )) tensor([[56., 62.], [80., 89.]]) print ( a @ b ) tensor([[56., 62.], [80., 89.]])","title":"Matrix multiplication"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#matrix-multiplication-with-broadcasting","text":"Matrix multiplication that involves broadcasting can be computed using torch.matmul(a,b) or a.matmul(b) or a @ b t1 = torch . randn ( 2 , 3 , 4 ) t2 = torch . randn ( 4 , 5 ) print ( torch . matmul ( t1 , t2 ) . size ()) torch.Size([2, 3, 5]) However, the same operation raises a RuntimeError with torch.mm() : print ( torch . mm ( t1 , t2 ) . size ()) RuntimeError: matrices expected, got 3D, 2D tensors at ..\\aten\\src\\TH/generic/THTensorMath.cpp:956","title":"Matrix multiplication with broadcasting"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#advanced-operations","text":"","title":"Advanced operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#l2-or-euclidian-norm","text":"See torch.norm() The Euclidian Norm gives the vector norm of \\(x\\) where \\(x=(x_1,x_2,...,x_n)\\) . It is calculated as \\({\\displaystyle \\left\\|{\\boldsymbol {x}}\\right\\|_{2}:={\\sqrt {x_{1}^{2}+\\cdots +x_{n}^{2}}}}\\) When applied to a matrix, torch.norm() returns the Frobenius norm by default. x = torch . tensor ([ 2. , 5. , 8. , 14. ]) x . norm () tensor(17.)","title":"L2 or Euclidian Norm"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#number-of-elements","text":"See torch.numel() Returns the number of elements in a tensor. x = torch . ones ( 3 , 7 ) x . numel () 21 This can be useful in certain calculations like Mean Squared Error: def mse(t1, t2): diff = t1 - t2 return torch.sum(diff * diff) / diff .numel()","title":"Number of elements"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#great-work","text":"","title":"Great work!"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/","text":"================ by Jawad Haider 02 - PyTorch Basics Exercises \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 PyTorch Basics Exercises 1.0.1 1. Perform standard imports 1.0.2 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d 1.0.3 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) 1.0.4 4. Create a tensor \u201cx\u201d from the array above 1.0.5 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 1.0.6 6. Reshape x into a 3x2 tensor 1.0.7 7. Return the right-hand column of tensor x 1.0.8 8. Without changing x, return a tensor of square values of x 1.0.9 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x 1.0.10 10. Find the matrix product of x and y 1.1 Great job! PyTorch Basics Exercises \u00b6 For these exercises we\u2019ll create a tensor and perform several operations on it. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! 1. Perform standard imports \u00b6 Import torch and NumPy # CODE HERE 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d \u00b6 This allows us to share the same \u201crandom\u201d results. # CODE HERE 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) \u00b6 # CODE HERE # DON'T WRITE HERE [3 4 2 4 4 1] 4. Create a tensor \u201cx\u201d from the array above \u00b6 # CODE HERE # DON'T WRITE HERE tensor([3, 4, 2, 4, 4, 1], dtype=torch.int32) 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 \u00b6 Note: \u2018int64\u2019 is also called \u2018LongTensor\u2019 # CODE HERE # DON'T WRITE HERE torch.LongTensor 6. Reshape x into a 3x2 tensor \u00b6 There are several ways to do this. # CODE HERE # DON'T WRITE HERE tensor([[3, 4], [2, 4], [4, 1]]) 7. Return the right-hand column of tensor x \u00b6 # CODE HERE # DON'T WRITE HERE tensor([[4], [4], [1]]) 8. Without changing x, return a tensor of square values of x \u00b6 There are several ways to do this. # CODE HERE # DON'T WRITE HERE tensor([[ 9, 16], [ 4, 16], [16, 1]]) 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x \u00b6 Use PyTorch directly (not NumPy) to create a tensor of random integers between 0 (inclusive) and 5 (exclusive). Think about what shape it should have to permit matrix multiplication. # CODE HERE # DON'T WRITE HERE tensor([[2, 2, 1], [4, 1, 0]]) 10. Find the matrix product of x and y \u00b6 # CODE HERE # DON'T WRITE HERE tensor([[22, 10, 3], [20, 8, 2], [12, 9, 4]]) Great job! \u00b6","title":"02 PyTorch Basics Exercises"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#02-pytorch-basics-exercises","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 PyTorch Basics Exercises 1.0.1 1. Perform standard imports 1.0.2 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d 1.0.3 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) 1.0.4 4. Create a tensor \u201cx\u201d from the array above 1.0.5 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 1.0.6 6. Reshape x into a 3x2 tensor 1.0.7 7. Return the right-hand column of tensor x 1.0.8 8. Without changing x, return a tensor of square values of x 1.0.9 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x 1.0.10 10. Find the matrix product of x and y 1.1 Great job!","title":"02 - PyTorch Basics Exercises"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#pytorch-basics-exercises","text":"For these exercises we\u2019ll create a tensor and perform several operations on it. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"PyTorch Basics Exercises"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#1-perform-standard-imports","text":"Import torch and NumPy # CODE HERE","title":"1. Perform standard imports"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#2-set-the-random-seed-for-numpy-and-pytorch-both-to-42","text":"This allows us to share the same \u201crandom\u201d results. # CODE HERE","title":"2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#3-create-a-numpy-array-called-arr-that-contains-6-random-integers-between-0-inclusive-and-5-exclusive","text":"# CODE HERE # DON'T WRITE HERE [3 4 2 4 4 1]","title":"3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive)"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#4-create-a-tensor-x-from-the-array-above","text":"# CODE HERE # DON'T WRITE HERE tensor([3, 4, 2, 4, 4, 1], dtype=torch.int32)","title":"4. Create a tensor \u201cx\u201d from the array above"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#5-change-the-dtype-of-x-from-int32-to-int64","text":"Note: \u2018int64\u2019 is also called \u2018LongTensor\u2019 # CODE HERE # DON'T WRITE HERE torch.LongTensor","title":"5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#6-reshape-x-into-a-3x2-tensor","text":"There are several ways to do this. # CODE HERE # DON'T WRITE HERE tensor([[3, 4], [2, 4], [4, 1]])","title":"6. Reshape x into a 3x2 tensor"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#7-return-the-right-hand-column-of-tensor-x","text":"# CODE HERE # DON'T WRITE HERE tensor([[4], [4], [1]])","title":"7. Return the right-hand column of tensor x"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#8-without-changing-x-return-a-tensor-of-square-values-of-x","text":"There are several ways to do this. # CODE HERE # DON'T WRITE HERE tensor([[ 9, 16], [ 4, 16], [16, 1]])","title":"8. Without changing x, return a tensor of square values of x"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#9-create-a-tensor-y-with-the-same-number-of-elements-as-x-that-can-be-matrix-multiplied-with-x","text":"Use PyTorch directly (not NumPy) to create a tensor of random integers between 0 (inclusive) and 5 (exclusive). Think about what shape it should have to permit matrix multiplication. # CODE HERE # DON'T WRITE HERE tensor([[2, 2, 1], [4, 1, 0]])","title":"9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#10-find-the-matrix-product-of-x-and-y","text":"# CODE HERE # DON'T WRITE HERE tensor([[22, 10, 3], [20, 8, 2], [12, 9, 4]])","title":"10. Find the matrix product of x and y"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/","text":"================ by Jawad Haider 02 - PyTorch Basics Exercises SOLUTION \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 PyTorch Basics Exercises - SOLUTIONS 1.0.1 1. Perform standard imports 1.0.2 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d 1.0.3 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) 1.0.4 4. Create a tensor \u201cx\u201d from the array above 1.0.5 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 1.0.6 6. Reshape x into a 3x2 tensor 1.0.7 7. Return the right-hand column of tensor x 1.0.8 8. Without changing x, return a tensor of square values of x 1.0.9 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x 1.0.10 10. Find the matrix product of x and y 1.1 Great job! PyTorch Basics Exercises - SOLUTIONS \u00b6 For these exercises we\u2019ll create a tensor and perform several operations on it. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! 1. Perform standard imports \u00b6 Import torch and NumPy # CODE HERE import torch import numpy as np 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d \u00b6 This allows us to share the same \u201crandom\u201d results. # CODE HERE np . random . seed ( 42 ) torch . manual_seed ( 42 ); # the semicolon suppresses the jupyter output line 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) \u00b6 # CODE HERE # DON'T WRITE HERE arr = np . random . randint ( 0 , 5 , 6 ) print ( arr ) [3 4 2 4 4 1] 4. Create a tensor \u201cx\u201d from the array above \u00b6 # CODE HERE # DON'T WRITE HERE x = torch . from_numpy ( arr ) print ( x ) tensor([3, 4, 2, 4, 4, 1], dtype=torch.int32) 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 \u00b6 Note: \u2018int64\u2019 is also called \u2018LongTensor\u2019 # CODE HERE # DON'T WRITE HERE x = x . type ( torch . int64 ) # x = x.type(torch.LongTensor) print ( x . type ()) torch.LongTensor 6. Reshape x into a 3x2 tensor \u00b6 There are several ways to do this. # CODE HERE # DON'T WRITE HERE x = x . view ( 3 , 2 ) # x = x.reshape(3,2) # x.resize_(3,2) print ( x ) tensor([[3, 4], [2, 4], [4, 1]]) 7. Return the right-hand column of tensor x \u00b6 # CODE HERE # DON'T WRITE HERE print ( x [:, 1 :]) # print(x[:,1]) tensor([[4], [4], [1]]) 8. Without changing x, return a tensor of square values of x \u00b6 There are several ways to do this. # CODE HERE # DON'T WRITE HERE print ( x * x ) # print(x**2) # print(x.mul(x)) # print(x.pow(2)) # print(torch.mul(x,x)) tensor([[ 9, 16], [ 4, 16], [16, 1]]) 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x \u00b6 Use PyTorch directly (not NumPy) to create a tensor of random integers between 0 (inclusive) and 5 (exclusive). Think about what shape it should have to permit matrix multiplication. # CODE HERE # DON'T WRITE HERE y = torch . randint ( 0 , 5 ,( 2 , 3 )) print ( y ) tensor([[2, 2, 1], [4, 1, 0]]) 10. Find the matrix product of x and y \u00b6 # CODE HERE # DON'T WRITE HERE print ( x . mm ( y )) tensor([[22, 10, 3], [20, 8, 2], [12, 9, 4]]) Great job! \u00b6","title":"03 PyTorch Basics Exercises Solutions"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#02-pytorch-basics-exercises-solution","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 PyTorch Basics Exercises - SOLUTIONS 1.0.1 1. Perform standard imports 1.0.2 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d 1.0.3 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) 1.0.4 4. Create a tensor \u201cx\u201d from the array above 1.0.5 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 1.0.6 6. Reshape x into a 3x2 tensor 1.0.7 7. Return the right-hand column of tensor x 1.0.8 8. Without changing x, return a tensor of square values of x 1.0.9 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x 1.0.10 10. Find the matrix product of x and y 1.1 Great job!","title":"02 - PyTorch Basics Exercises SOLUTION"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#pytorch-basics-exercises-solutions","text":"For these exercises we\u2019ll create a tensor and perform several operations on it. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"PyTorch Basics Exercises - SOLUTIONS"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#1-perform-standard-imports","text":"Import torch and NumPy # CODE HERE import torch import numpy as np","title":"1. Perform standard imports"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#2-set-the-random-seed-for-numpy-and-pytorch-both-to-42","text":"This allows us to share the same \u201crandom\u201d results. # CODE HERE np . random . seed ( 42 ) torch . manual_seed ( 42 ); # the semicolon suppresses the jupyter output line","title":"2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#3-create-a-numpy-array-called-arr-that-contains-6-random-integers-between-0-inclusive-and-5-exclusive","text":"# CODE HERE # DON'T WRITE HERE arr = np . random . randint ( 0 , 5 , 6 ) print ( arr ) [3 4 2 4 4 1]","title":"3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive)"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#4-create-a-tensor-x-from-the-array-above","text":"# CODE HERE # DON'T WRITE HERE x = torch . from_numpy ( arr ) print ( x ) tensor([3, 4, 2, 4, 4, 1], dtype=torch.int32)","title":"4. Create a tensor \u201cx\u201d from the array above"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#5-change-the-dtype-of-x-from-int32-to-int64","text":"Note: \u2018int64\u2019 is also called \u2018LongTensor\u2019 # CODE HERE # DON'T WRITE HERE x = x . type ( torch . int64 ) # x = x.type(torch.LongTensor) print ( x . type ()) torch.LongTensor","title":"5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#6-reshape-x-into-a-3x2-tensor","text":"There are several ways to do this. # CODE HERE # DON'T WRITE HERE x = x . view ( 3 , 2 ) # x = x.reshape(3,2) # x.resize_(3,2) print ( x ) tensor([[3, 4], [2, 4], [4, 1]])","title":"6. Reshape x into a 3x2 tensor"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#7-return-the-right-hand-column-of-tensor-x","text":"# CODE HERE # DON'T WRITE HERE print ( x [:, 1 :]) # print(x[:,1]) tensor([[4], [4], [1]])","title":"7. Return the right-hand column of tensor x"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#8-without-changing-x-return-a-tensor-of-square-values-of-x","text":"There are several ways to do this. # CODE HERE # DON'T WRITE HERE print ( x * x ) # print(x**2) # print(x.mul(x)) # print(x.pow(2)) # print(torch.mul(x,x)) tensor([[ 9, 16], [ 4, 16], [16, 1]])","title":"8. Without changing x, return a tensor of square values of x"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#9-create-a-tensor-y-with-the-same-number-of-elements-as-x-that-can-be-matrix-multiplied-with-x","text":"Use PyTorch directly (not NumPy) to create a tensor of random integers between 0 (inclusive) and 5 (exclusive). Think about what shape it should have to permit matrix multiplication. # CODE HERE # DON'T WRITE HERE y = torch . randint ( 0 , 5 ,( 2 , 3 )) print ( y ) tensor([[2, 2, 1], [4, 1, 0]])","title":"9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#10-find-the-matrix-product-of-x-and-y","text":"# CODE HERE # DON'T WRITE HERE print ( x . mm ( y )) tensor([[22, 10, 3], [20, 8, 2], [12, 9, 4]])","title":"10. Find the matrix product of x and y"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/","text":"================ by Jawad Haider 00 - PyTorch Gradients \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ PyTorch Gradients Autograd - Automatic Differentiation Back-propagation on one step Back-propagation on multiple steps Turn off tracking PyTorch Gradients \u00b6 This section covers the PyTorch autograd implementation of gradient descent. Tools include: * torch.autograd.backward() * torch.autograd.grad() Before continuing in this section, be sure to watch the theory lectures to understand the following concepts: * Error functions (step andsigmoid) * One-hot encoding * Maximum likelihood * Cross entropy(including multi-class cross entropy) * Back propagation (backprop) Additional Resources: PyTorch Notes: Autograd mechanics Autograd - Automatic Differentiation \u00b6 In previous sections we created tensors and performed a variety of operations on them, but we did nothing to store the sequence of operations, or to apply the derivative of a completed function. In this section we\u2019ll introduce the concept of the dynamic computational graph which is comprised of all the Tensor objects in the network, as well as the Functions used to create them. Note that only the input Tensors we create ourselves will not have associated Function objects. The PyTorch autograd package provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensors themselves. When a Tensor\u2019s .requires_grad attribute is set to True, it starts to track all operations on it. When an operation finishes you can call .backward() and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its .grad attribute. Let\u2019s see this in practice. Back-propagation on one step \u00b6 We\u2019ll start by applying a single polynomial function to tensor . Then we\u2019ll backprop and print the gradient . Step 1. Perform standard imports \u00b6 import torch Step 2. Create a tensor with requires_grad set to True \u00b6 This sets up computational tracking on the tensor. x = torch . tensor ( 2.0 , requires_grad = True ) Step 3. Define a function \u00b6 y = 2 * x ** 4 + x ** 3 + 3 * x ** 2 + 5 * x + 1 print ( y ) tensor(63., grad_fn=<AddBackward0>) Since was created as a result of an operation, it has an associated gradient function accessible as y.grad_fn The calculation of is done as: This is the value of when . Step 4. Backprop \u00b6 y . backward () Step 5. Display the resulting gradient \u00b6 print ( x . grad ) tensor(93.) Note that x.grad is an attribute of tensor , so we don\u2019t use parentheses. The computation is the result of This is the slope of the polynomial at the point . Back-propagation on multiple steps \u00b6 Now let\u2019s do something more complex, involving layers and between and our output layer . 1. Create \u00b6 a tensor x = torch . tensor ([[ 1. , 2 , 3 ],[ 3 , 2 , 1 ]], requires_grad = True ) print ( x ) tensor([[1., 2., 3.], [3., 2., 1.]], requires_grad=True) 2. Create the first layer with \u00b6 y = 3 * x + 2 print ( y ) tensor([[ 5., 8., 11.], [11., 8., 5.]], grad_fn=<AddBackward0>) 3. Create the second layer with \u00b6 z = 2 * y ** 2 print ( z ) tensor([[ 50., 128., 242.], [242., 128., 50.]], grad_fn=<MulBackward0>) 4. Set the output to be the matrix mean \u00b6 out = z . mean () print ( out ) tensor(140., grad_fn=<MeanBackward1>) 5. Now perform back-propagation to find the gradient of x w.r.t out \u00b6 (If you haven\u2019t seen it before, w.r.t. is an abbreviation of with respect to ) out . backward () print ( x . grad ) tensor([[10., 16., 22.], [22., 16., 10.]]) You should see a 2x3 matrix. If we call the final out tensor \u201c \u201d, we can calculate the partial derivative of with respect to as follows: To solve the derivative of we use the chain rule , where the derivative of In this case Therefore, Turn off tracking \u00b6 There may be times when we don\u2019t want or need to track the computational history. You can reset a tensor\u2019s requires_grad attribute in-place using .requires_grad_(True) (or False) as needed. When performing evaluations, it\u2019s often helpful to wrap a set of operations in with torch.no_grad(): A less-used method is to run .detach() on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor. A NOTE ABOUT TENSORS AND VARIABLES: Prior to PyTorch v0.4.0 (April 2018) Tensors ( torch.Tensor ) only held data, and tracking history was reserved for the Variable wrapper ( torch.autograd.Variable ). Since v0.4.0 tensors and variables have merged, and tracking functionality is now available through the requires_grad=True flag.","title":"00 PyTorch Gradients"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#00-pytorch-gradients","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ PyTorch Gradients Autograd - Automatic Differentiation Back-propagation on one step Back-propagation on multiple steps Turn off tracking","title":"00 - PyTorch Gradients"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#pytorch-gradients","text":"This section covers the PyTorch autograd implementation of gradient descent. Tools include: * torch.autograd.backward() * torch.autograd.grad() Before continuing in this section, be sure to watch the theory lectures to understand the following concepts: * Error functions (step andsigmoid) * One-hot encoding * Maximum likelihood * Cross entropy(including multi-class cross entropy) * Back propagation (backprop)","title":"PyTorch Gradients"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#autograd-automatic-differentiation","text":"In previous sections we created tensors and performed a variety of operations on them, but we did nothing to store the sequence of operations, or to apply the derivative of a completed function. In this section we\u2019ll introduce the concept of the dynamic computational graph which is comprised of all the Tensor objects in the network, as well as the Functions used to create them. Note that only the input Tensors we create ourselves will not have associated Function objects. The PyTorch autograd package provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensors themselves. When a Tensor\u2019s .requires_grad attribute is set to True, it starts to track all operations on it. When an operation finishes you can call .backward() and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its .grad attribute. Let\u2019s see this in practice.","title":"Autograd - Automatic Differentiation"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#back-propagation-on-one-step","text":"We\u2019ll start by applying a single polynomial function to tensor . Then we\u2019ll backprop and print the gradient .","title":"Back-propagation on one step"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#step-1-perform-standard-imports","text":"import torch","title":"Step 1. Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#step-2-create-a-tensor-with-requires_grad-set-to-true","text":"This sets up computational tracking on the tensor. x = torch . tensor ( 2.0 , requires_grad = True )","title":"Step 2. Create a tensor with requires_grad set to True"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#step-3-define-a-function","text":"y = 2 * x ** 4 + x ** 3 + 3 * x ** 2 + 5 * x + 1 print ( y ) tensor(63., grad_fn=<AddBackward0>) Since was created as a result of an operation, it has an associated gradient function accessible as y.grad_fn The calculation of is done as: This is the value of when .","title":"Step 3. Define a function"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#step-4-backprop","text":"y . backward ()","title":"Step 4. Backprop"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#step-5-display-the-resulting-gradient","text":"print ( x . grad ) tensor(93.) Note that x.grad is an attribute of tensor , so we don\u2019t use parentheses. The computation is the result of This is the slope of the polynomial at the point .","title":"Step 5. Display the resulting gradient"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#back-propagation-on-multiple-steps","text":"Now let\u2019s do something more complex, involving layers and between and our output layer .","title":"Back-propagation on multiple steps"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#1-create","text":"a tensor x = torch . tensor ([[ 1. , 2 , 3 ],[ 3 , 2 , 1 ]], requires_grad = True ) print ( x ) tensor([[1., 2., 3.], [3., 2., 1.]], requires_grad=True)","title":"1. Create"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#2-create-the-first-layer-with","text":"y = 3 * x + 2 print ( y ) tensor([[ 5., 8., 11.], [11., 8., 5.]], grad_fn=<AddBackward0>)","title":"2. Create the first layer with"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#3-create-the-second-layer-with","text":"z = 2 * y ** 2 print ( z ) tensor([[ 50., 128., 242.], [242., 128., 50.]], grad_fn=<MulBackward0>)","title":"3. Create the second layer with"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#4-set-the-output-to-be-the-matrix-mean","text":"out = z . mean () print ( out ) tensor(140., grad_fn=<MeanBackward1>)","title":"4. Set the output to be the matrix mean"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#5-now-perform-back-propagation-to-find-the-gradient-of-x-wrt-out","text":"(If you haven\u2019t seen it before, w.r.t. is an abbreviation of with respect to ) out . backward () print ( x . grad ) tensor([[10., 16., 22.], [22., 16., 10.]]) You should see a 2x3 matrix. If we call the final out tensor \u201c \u201d, we can calculate the partial derivative of with respect to as follows: To solve the derivative of we use the chain rule , where the derivative of In this case Therefore,","title":"5. Now perform back-propagation to find the gradient of x w.r.t out"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/#turn-off-tracking","text":"There may be times when we don\u2019t want or need to track the computational history. You can reset a tensor\u2019s requires_grad attribute in-place using .requires_grad_(True) (or False) as needed. When performing evaluations, it\u2019s often helpful to wrap a set of operations in with torch.no_grad(): A less-used method is to run .detach() on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor. A NOTE ABOUT TENSORS AND VARIABLES: Prior to PyTorch v0.4.0 (April 2018) Tensors ( torch.Tensor ) only held data, and tracking history was reserved for the Variable wrapper ( torch.autograd.Variable ). Since v0.4.0 tensors and variables have merged, and tracking functionality is now available through the requires_grad=True flag.","title":"Turn off tracking"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/","text":"================ by Jawad Haider 01 - Linear Regression with PyTorch \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Linear Regression with PyTorch Perform standard imports Create a column matrix of X values Create a \u201crandom\u201d array of error values Create a column matrix of y values Plot the results Simple linear model Model classes Plot the initial model Set the loss function Set the optimization Train the model Plot the loss values Plot the result Great job! Linear Regression with PyTorch \u00b6 In this section we\u2019ll use PyTorch\u2019s machine learning model to progressively develop a best-fit line for a given set of data points. Like most linear regression algorithms, we\u2019re seeking to minimize the error between our model and the actual data, using a loss function like mean-squared-error. Image source: https://commons.wikimedia.org/wiki/File:Residuals_for_Linear_Regression_Fit.png To start, we\u2019ll develop a collection of data points that appear random, but that fit a known linear equation Perform standard imports \u00b6 import torch import torch.nn as nn # we'll use this a lot going forward! import numpy as np import matplotlib.pyplot as plt % matplotlib inline Create a column matrix of X values \u00b6 We can create tensors right away rather than convert from NumPy arrays. X = torch . linspace ( 1 , 50 , 50 ) . reshape ( - 1 , 1 ) # Equivalent to # X = torch.unsqueeze(torch.linspace(1,50,50), dim=1) Create a \u201crandom\u201d array of error values \u00b6 We want 50 random integer values that collectively cancel each other out. torch . manual_seed ( 71 ) # to obtain reproducible results e = torch . randint ( - 8 , 9 ,( 50 , 1 ), dtype = torch . float ) print ( e . sum ()) tensor(0.) Create a column matrix of y values \u00b6 Here we\u2019ll set our own parameters of , plus the error amount. y will have the same shape as X and e y = 2 * X + 1 + e print ( y . shape ) torch.Size([50, 1]) Plot the results \u00b6 We have to convert tensors to NumPy arrays just for plotting. plt . scatter ( X . numpy (), y . numpy ()) plt . ylabel ( 'y' ) plt . xlabel ( 'x' ); Note that when we created tensor , we did not pass requires_grad=True . This means that doesn\u2019t have a gradient function, and y.backward() won\u2019t work. Since PyTorch is not tracking operations, it doesn\u2019t know the relationship between and . Simple linear model \u00b6 As a quick demonstration we\u2019ll show how the built-in nn.Linear() model preselects weight and bias values at random. torch . manual_seed ( 59 ) model = nn . Linear ( in_features = 1 , out_features = 1 ) print ( model . weight ) print ( model . bias ) Parameter containing: tensor([[0.1060]], requires_grad=True) Parameter containing: tensor([0.9638], requires_grad=True) Without seeing any data, the model sets a random weight of 0.1060 and a bias of 0.9638. Model classes \u00b6 PyTorch lets us define models as object classes that can store multiple model layers. In upcoming sections we\u2019ll set up several neural network layers, and determine how each layer should perform its forward pass to the next layer. For now, though, we only need a single linear layer. class Model ( nn . Module ): def __init__ ( self , in_features , out_features ): super () . __init__ () self . linear = nn . Linear ( in_features , out_features ) def forward ( self , x ): y_pred = self . linear ( x ) return y_pred NOTE: The \u201cLinear\u201d model layer used here doesn\u2019t really refer to linear regression. Instead, it describes the type of neural network layer employed. Linear layers are also called \u201cfully connected\u201d or \u201cdense\u201d layers. Going forward our models may contain linear layers, convolutional layers, and more. When Model is instantiated, we need to pass in the size (dimensions) of the incoming and outgoing features. For our purposes we\u2019ll use (1,1). As above, we can see the initial hyperparameters. torch . manual_seed ( 59 ) model = Model ( 1 , 1 ) print ( model ) print ( 'Weight:' , model . linear . weight . item ()) print ( 'Bias: ' , model . linear . bias . item ()) Model( (linear): Linear(in_features=1, out_features=1, bias=True) ) Weight: 0.10597813129425049 Bias: 0.9637961387634277 As models become more complex, it may be better to iterate over all the model parameters: for name , param in model . named_parameters (): print ( name , ' \\t ' , param . item ()) linear.weight 0.10597813129425049 linear.bias 0.9637961387634277 NOTE: In the above example we had our Model class accept arguments for the number of input and output features. For simplicity we can hardcode them into the Model: class Model(torch.nn.Module): def \\_\\_init\\_\\_(self): super().\\_\\_init\\_\\_() self.linear = Linear(1,1) model = Model() Alternatively we can use default arguments: class Model(torch.nn.Module): def \\_\\_init\\_\\_(self, in_dim=1, out_dim=1): super().\\_\\_init\\_\\_() self.linear = Linear(in_dim,out_dim) model = Model() \\# or model = Model(i,o) Now let\u2019s see the result when we pass a tensor into the model. x = torch . tensor ([ 2.0 ]) print ( model . forward ( x )) # equivalent to print(model(x)) tensor([1.1758], grad_fn=<AddBackward0>) which is confirmed with Plot the initial model \u00b6 We can plot the untrained model against our dataset to get an idea of our starting point. x1 = np . array ([ X . min (), X . max ()]) print ( x1 ) [ 1. 50.] w1 , b1 = model . linear . weight . item (), model . linear . bias . item () print ( f 'Initial weight: { w1 : .8f } , Initial bias: { b1 : .8f } ' ) print () y1 = x1 * w1 + b1 print ( y1 ) Initial weight: 0.10597813, Initial bias: 0.96379614 [1.0697743 6.2627025] plt . scatter ( X . numpy (), y . numpy ()) plt . plot ( x1 , y1 , 'r' ) plt . title ( 'Initial Model' ) plt . ylabel ( 'y' ) plt . xlabel ( 'x' ); Set the loss function \u00b6 We could write our own function to apply a Mean Squared Error (MSE) that follows Fortunately PyTorch has it built in. By convention, you\u2019ll see the variable name \u201ccriterion\u201d used, but feel free to use something like \u201clinear_loss_func\u201d if that\u2019s clearer. criterion = nn . MSELoss () Set the optimization \u00b6 Here we\u2019ll use Stochastic Gradient Descent (SGD) with an applied learning rate (lr) of 0.001. Recall that the learning rate tells the optimizer how much to adjust each parameter on the next round of calculations. Too large a step and we run the risk of overshooting the minimum, causing the algorithm to diverge. Too small and it will take a long time to converge. For more complicated (multivariate) data, you might also consider passing optional momentum and weight_decay arguments. Momentum allows the algorithm to \u201croll over\u201d small bumps to avoid local minima that can cause convergence too soon. Weight decay (also called an L2 penalty) applies to biases. For more information, see torch.optim optimizer = torch . optim . SGD ( model . parameters (), lr = 0.001 ) # You'll sometimes see this as # optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3) Train the model \u00b6 An epoch is a single pass through the entire dataset. We want to pick a sufficiently large number of epochs to reach a plateau close to our known parameters of Let\u2019s walk through the steps we\u2019re about to take: 1. Set a reasonably large number of passes epochs = 50 2. Create a list to store loss values. This will let us view our progress afterward. losses = \\[\\] for i in range(epochs): 3. Bump \u201ci\u201d so that the printed report starts at 1 i+=1 4. Create a prediction set by running \u201cX\u201d through the current model parameters y_pred = model.forward(X) 5. Calculate the loss loss = criterion(y_pred, y) 6. Add the loss value to our tracking list losses.append(loss) 7. Print the current line of results print(f\u2019epoch: {i:2} loss: {loss.item():10.8f}\u2019) 8. Gradients accumulate with every backprop. To prevent compounding we need to reset the stored gradient for each new epoch. optimizer.zero_grad() 9. Now we can backprop loss.backward() 10. Finally, we can update the hyperparameters of our model optimizer.step() epochs = 50 losses = [] for i in range ( epochs ): i += 1 y_pred = model . forward ( X ) loss = criterion ( y_pred , y ) losses . append ( loss ) print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } weight: { model . linear . weight . item () : 10.8f } \\ bias: { model . linear . bias . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () epoch: 1 loss: 3057.21679688 weight: 0.10597813 bias: 0.96379614 epoch: 2 loss: 1588.53100586 weight: 3.33490038 bias: 1.06046367 epoch: 3 loss: 830.30010986 weight: 1.01483274 bias: 0.99226278 epoch: 4 loss: 438.85241699 weight: 2.68179965 bias: 1.04252183 epoch: 5 loss: 236.76152039 weight: 1.48402119 bias: 1.00766504 epoch: 6 loss: 132.42912292 weight: 2.34460592 bias: 1.03396463 epoch: 7 loss: 78.56572723 weight: 1.72622538 bias: 1.01632178 epoch: 8 loss: 50.75775909 weight: 2.17050409 bias: 1.03025162 epoch: 9 loss: 36.40123367 weight: 1.85124576 bias: 1.02149546 epoch: 10 loss: 28.98922729 weight: 2.08060074 bias: 1.02903891 epoch: 11 loss: 25.16238213 weight: 1.91576838 bias: 1.02487016 epoch: 12 loss: 23.18647385 weight: 2.03416562 bias: 1.02911627 epoch: 13 loss: 22.16612816 weight: 1.94905841 bias: 1.02731562 epoch: 14 loss: 21.63911057 weight: 2.01017213 bias: 1.02985907 epoch: 15 loss: 21.36677170 weight: 1.96622372 bias: 1.02928054 epoch: 16 loss: 21.22591782 weight: 1.99776423 bias: 1.03094459 epoch: 17 loss: 21.15294647 weight: 1.97506487 bias: 1.03099668 epoch: 18 loss: 21.11501122 weight: 1.99133754 bias: 1.03220642 epoch: 19 loss: 21.09517670 weight: 1.97960854 bias: 1.03258383 epoch: 20 loss: 21.08468437 weight: 1.98799884 bias: 1.03355861 epoch: 21 loss: 21.07901382 weight: 1.98193336 bias: 1.03410351 epoch: 22 loss: 21.07583046 weight: 1.98625445 bias: 1.03495669 epoch: 23 loss: 21.07393837 weight: 1.98311269 bias: 1.03558779 epoch: 24 loss: 21.07269859 weight: 1.98533309 bias: 1.03637791 epoch: 25 loss: 21.07181931 weight: 1.98370099 bias: 1.03705311 epoch: 26 loss: 21.07110596 weight: 1.98483658 bias: 1.03781021 epoch: 27 loss: 21.07048416 weight: 1.98398376 bias: 1.03850794 epoch: 28 loss: 21.06991386 weight: 1.98455977 bias: 1.03924775 epoch: 29 loss: 21.06936646 weight: 1.98410904 bias: 1.03995669 epoch: 30 loss: 21.06883621 weight: 1.98439610 bias: 1.04068720 epoch: 31 loss: 21.06830788 weight: 1.98415291 bias: 1.04140162 epoch: 32 loss: 21.06778145 weight: 1.98429084 bias: 1.04212701 epoch: 33 loss: 21.06726265 weight: 1.98415494 bias: 1.04284394 epoch: 34 loss: 21.06674004 weight: 1.98421574 bias: 1.04356635 epoch: 35 loss: 21.06622314 weight: 1.98413551 bias: 1.04428422 epoch: 36 loss: 21.06570625 weight: 1.98415649 bias: 1.04500473 epoch: 37 loss: 21.06518936 weight: 1.98410451 bias: 1.04572272 epoch: 38 loss: 21.06466866 weight: 1.98410523 bias: 1.04644191 epoch: 39 loss: 21.06415749 weight: 1.98406804 bias: 1.04715967 epoch: 40 loss: 21.06363869 weight: 1.98405814 bias: 1.04787791 epoch: 41 loss: 21.06312370 weight: 1.98402870 bias: 1.04859519 epoch: 42 loss: 21.06260681 weight: 1.98401320 bias: 1.04931259 epoch: 43 loss: 21.06209564 weight: 1.98398757 bias: 1.05002928 epoch: 44 loss: 21.06157875 weight: 1.98396957 bias: 1.05074584 epoch: 45 loss: 21.06106949 weight: 1.98394585 bias: 1.05146194 epoch: 46 loss: 21.06055450 weight: 1.98392630 bias: 1.05217779 epoch: 47 loss: 21.06004143 weight: 1.98390377 bias: 1.05289316 epoch: 48 loss: 21.05953217 weight: 1.98388338 bias: 1.05360830 epoch: 49 loss: 21.05901527 weight: 1.98386145 bias: 1.05432308 epoch: 50 loss: 21.05850983 weight: 1.98384094 bias: 1.05503750 Plot the loss values \u00b6 Let\u2019s see how loss changed over time plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'epoch' ); Plot the result \u00b6 Now we\u2019ll derive y1 from the new model to plot the most recent best-fit line. w1 , b1 = model . linear . weight . item (), model . linear . bias . item () print ( f 'Current weight: { w1 : .8f } , Current bias: { b1 : .8f } ' ) print () y1 = x1 * w1 + b1 print ( x1 ) print ( y1 ) Current weight: 1.98381913, Current bias: 1.05575156 [ 1. 50.] [ 3.0395708 100.246704 ] plt . scatter ( X . numpy (), y . numpy ()) plt . plot ( x1 , y1 , 'r' ) plt . title ( 'Current Model' ) plt . ylabel ( 'y' ) plt . xlabel ( 'x' ); Great job! \u00b6","title":"01 Linear Regression with PyTorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#01-linear-regression-with-pytorch","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Linear Regression with PyTorch Perform standard imports Create a column matrix of X values Create a \u201crandom\u201d array of error values Create a column matrix of y values Plot the results Simple linear model Model classes Plot the initial model Set the loss function Set the optimization Train the model Plot the loss values Plot the result Great job!","title":"01 - Linear Regression with PyTorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#linear-regression-with-pytorch","text":"In this section we\u2019ll use PyTorch\u2019s machine learning model to progressively develop a best-fit line for a given set of data points. Like most linear regression algorithms, we\u2019re seeking to minimize the error between our model and the actual data, using a loss function like mean-squared-error. Image source: https://commons.wikimedia.org/wiki/File:Residuals_for_Linear_Regression_Fit.png To start, we\u2019ll develop a collection of data points that appear random, but that fit a known linear equation","title":"Linear Regression with PyTorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#perform-standard-imports","text":"import torch import torch.nn as nn # we'll use this a lot going forward! import numpy as np import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#create-a-column-matrix-of-x-values","text":"We can create tensors right away rather than convert from NumPy arrays. X = torch . linspace ( 1 , 50 , 50 ) . reshape ( - 1 , 1 ) # Equivalent to # X = torch.unsqueeze(torch.linspace(1,50,50), dim=1)","title":"Create a column matrix of X values"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#create-a-random-array-of-error-values","text":"We want 50 random integer values that collectively cancel each other out. torch . manual_seed ( 71 ) # to obtain reproducible results e = torch . randint ( - 8 , 9 ,( 50 , 1 ), dtype = torch . float ) print ( e . sum ()) tensor(0.)","title":"Create a \u201crandom\u201d array of error values"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#create-a-column-matrix-of-y-values","text":"Here we\u2019ll set our own parameters of , plus the error amount. y will have the same shape as X and e y = 2 * X + 1 + e print ( y . shape ) torch.Size([50, 1])","title":"Create a column matrix of y values"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#plot-the-results","text":"We have to convert tensors to NumPy arrays just for plotting. plt . scatter ( X . numpy (), y . numpy ()) plt . ylabel ( 'y' ) plt . xlabel ( 'x' ); Note that when we created tensor , we did not pass requires_grad=True . This means that doesn\u2019t have a gradient function, and y.backward() won\u2019t work. Since PyTorch is not tracking operations, it doesn\u2019t know the relationship between and .","title":"Plot the results"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#simple-linear-model","text":"As a quick demonstration we\u2019ll show how the built-in nn.Linear() model preselects weight and bias values at random. torch . manual_seed ( 59 ) model = nn . Linear ( in_features = 1 , out_features = 1 ) print ( model . weight ) print ( model . bias ) Parameter containing: tensor([[0.1060]], requires_grad=True) Parameter containing: tensor([0.9638], requires_grad=True) Without seeing any data, the model sets a random weight of 0.1060 and a bias of 0.9638.","title":"Simple linear model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#model-classes","text":"PyTorch lets us define models as object classes that can store multiple model layers. In upcoming sections we\u2019ll set up several neural network layers, and determine how each layer should perform its forward pass to the next layer. For now, though, we only need a single linear layer. class Model ( nn . Module ): def __init__ ( self , in_features , out_features ): super () . __init__ () self . linear = nn . Linear ( in_features , out_features ) def forward ( self , x ): y_pred = self . linear ( x ) return y_pred NOTE: The \u201cLinear\u201d model layer used here doesn\u2019t really refer to linear regression. Instead, it describes the type of neural network layer employed. Linear layers are also called \u201cfully connected\u201d or \u201cdense\u201d layers. Going forward our models may contain linear layers, convolutional layers, and more. When Model is instantiated, we need to pass in the size (dimensions) of the incoming and outgoing features. For our purposes we\u2019ll use (1,1). As above, we can see the initial hyperparameters. torch . manual_seed ( 59 ) model = Model ( 1 , 1 ) print ( model ) print ( 'Weight:' , model . linear . weight . item ()) print ( 'Bias: ' , model . linear . bias . item ()) Model( (linear): Linear(in_features=1, out_features=1, bias=True) ) Weight: 0.10597813129425049 Bias: 0.9637961387634277 As models become more complex, it may be better to iterate over all the model parameters: for name , param in model . named_parameters (): print ( name , ' \\t ' , param . item ()) linear.weight 0.10597813129425049 linear.bias 0.9637961387634277 NOTE: In the above example we had our Model class accept arguments for the number of input and output features. For simplicity we can hardcode them into the Model: class Model(torch.nn.Module): def \\_\\_init\\_\\_(self): super().\\_\\_init\\_\\_() self.linear = Linear(1,1) model = Model() Alternatively we can use default arguments: class Model(torch.nn.Module): def \\_\\_init\\_\\_(self, in_dim=1, out_dim=1): super().\\_\\_init\\_\\_() self.linear = Linear(in_dim,out_dim) model = Model() \\# or model = Model(i,o) Now let\u2019s see the result when we pass a tensor into the model. x = torch . tensor ([ 2.0 ]) print ( model . forward ( x )) # equivalent to print(model(x)) tensor([1.1758], grad_fn=<AddBackward0>) which is confirmed with","title":"Model classes"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#plot-the-initial-model","text":"We can plot the untrained model against our dataset to get an idea of our starting point. x1 = np . array ([ X . min (), X . max ()]) print ( x1 ) [ 1. 50.] w1 , b1 = model . linear . weight . item (), model . linear . bias . item () print ( f 'Initial weight: { w1 : .8f } , Initial bias: { b1 : .8f } ' ) print () y1 = x1 * w1 + b1 print ( y1 ) Initial weight: 0.10597813, Initial bias: 0.96379614 [1.0697743 6.2627025] plt . scatter ( X . numpy (), y . numpy ()) plt . plot ( x1 , y1 , 'r' ) plt . title ( 'Initial Model' ) plt . ylabel ( 'y' ) plt . xlabel ( 'x' );","title":"Plot the initial model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#set-the-loss-function","text":"We could write our own function to apply a Mean Squared Error (MSE) that follows Fortunately PyTorch has it built in. By convention, you\u2019ll see the variable name \u201ccriterion\u201d used, but feel free to use something like \u201clinear_loss_func\u201d if that\u2019s clearer. criterion = nn . MSELoss ()","title":"Set the loss function"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#set-the-optimization","text":"Here we\u2019ll use Stochastic Gradient Descent (SGD) with an applied learning rate (lr) of 0.001. Recall that the learning rate tells the optimizer how much to adjust each parameter on the next round of calculations. Too large a step and we run the risk of overshooting the minimum, causing the algorithm to diverge. Too small and it will take a long time to converge. For more complicated (multivariate) data, you might also consider passing optional momentum and weight_decay arguments. Momentum allows the algorithm to \u201croll over\u201d small bumps to avoid local minima that can cause convergence too soon. Weight decay (also called an L2 penalty) applies to biases. For more information, see torch.optim optimizer = torch . optim . SGD ( model . parameters (), lr = 0.001 ) # You'll sometimes see this as # optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)","title":"Set the optimization"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#train-the-model","text":"An epoch is a single pass through the entire dataset. We want to pick a sufficiently large number of epochs to reach a plateau close to our known parameters of Let\u2019s walk through the steps we\u2019re about to take: 1. Set a reasonably large number of passes epochs = 50 2. Create a list to store loss values. This will let us view our progress afterward. losses = \\[\\] for i in range(epochs): 3. Bump \u201ci\u201d so that the printed report starts at 1 i+=1 4. Create a prediction set by running \u201cX\u201d through the current model parameters y_pred = model.forward(X) 5. Calculate the loss loss = criterion(y_pred, y) 6. Add the loss value to our tracking list losses.append(loss) 7. Print the current line of results print(f\u2019epoch: {i:2} loss: {loss.item():10.8f}\u2019) 8. Gradients accumulate with every backprop. To prevent compounding we need to reset the stored gradient for each new epoch. optimizer.zero_grad() 9. Now we can backprop loss.backward() 10. Finally, we can update the hyperparameters of our model optimizer.step() epochs = 50 losses = [] for i in range ( epochs ): i += 1 y_pred = model . forward ( X ) loss = criterion ( y_pred , y ) losses . append ( loss ) print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } weight: { model . linear . weight . item () : 10.8f } \\ bias: { model . linear . bias . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () epoch: 1 loss: 3057.21679688 weight: 0.10597813 bias: 0.96379614 epoch: 2 loss: 1588.53100586 weight: 3.33490038 bias: 1.06046367 epoch: 3 loss: 830.30010986 weight: 1.01483274 bias: 0.99226278 epoch: 4 loss: 438.85241699 weight: 2.68179965 bias: 1.04252183 epoch: 5 loss: 236.76152039 weight: 1.48402119 bias: 1.00766504 epoch: 6 loss: 132.42912292 weight: 2.34460592 bias: 1.03396463 epoch: 7 loss: 78.56572723 weight: 1.72622538 bias: 1.01632178 epoch: 8 loss: 50.75775909 weight: 2.17050409 bias: 1.03025162 epoch: 9 loss: 36.40123367 weight: 1.85124576 bias: 1.02149546 epoch: 10 loss: 28.98922729 weight: 2.08060074 bias: 1.02903891 epoch: 11 loss: 25.16238213 weight: 1.91576838 bias: 1.02487016 epoch: 12 loss: 23.18647385 weight: 2.03416562 bias: 1.02911627 epoch: 13 loss: 22.16612816 weight: 1.94905841 bias: 1.02731562 epoch: 14 loss: 21.63911057 weight: 2.01017213 bias: 1.02985907 epoch: 15 loss: 21.36677170 weight: 1.96622372 bias: 1.02928054 epoch: 16 loss: 21.22591782 weight: 1.99776423 bias: 1.03094459 epoch: 17 loss: 21.15294647 weight: 1.97506487 bias: 1.03099668 epoch: 18 loss: 21.11501122 weight: 1.99133754 bias: 1.03220642 epoch: 19 loss: 21.09517670 weight: 1.97960854 bias: 1.03258383 epoch: 20 loss: 21.08468437 weight: 1.98799884 bias: 1.03355861 epoch: 21 loss: 21.07901382 weight: 1.98193336 bias: 1.03410351 epoch: 22 loss: 21.07583046 weight: 1.98625445 bias: 1.03495669 epoch: 23 loss: 21.07393837 weight: 1.98311269 bias: 1.03558779 epoch: 24 loss: 21.07269859 weight: 1.98533309 bias: 1.03637791 epoch: 25 loss: 21.07181931 weight: 1.98370099 bias: 1.03705311 epoch: 26 loss: 21.07110596 weight: 1.98483658 bias: 1.03781021 epoch: 27 loss: 21.07048416 weight: 1.98398376 bias: 1.03850794 epoch: 28 loss: 21.06991386 weight: 1.98455977 bias: 1.03924775 epoch: 29 loss: 21.06936646 weight: 1.98410904 bias: 1.03995669 epoch: 30 loss: 21.06883621 weight: 1.98439610 bias: 1.04068720 epoch: 31 loss: 21.06830788 weight: 1.98415291 bias: 1.04140162 epoch: 32 loss: 21.06778145 weight: 1.98429084 bias: 1.04212701 epoch: 33 loss: 21.06726265 weight: 1.98415494 bias: 1.04284394 epoch: 34 loss: 21.06674004 weight: 1.98421574 bias: 1.04356635 epoch: 35 loss: 21.06622314 weight: 1.98413551 bias: 1.04428422 epoch: 36 loss: 21.06570625 weight: 1.98415649 bias: 1.04500473 epoch: 37 loss: 21.06518936 weight: 1.98410451 bias: 1.04572272 epoch: 38 loss: 21.06466866 weight: 1.98410523 bias: 1.04644191 epoch: 39 loss: 21.06415749 weight: 1.98406804 bias: 1.04715967 epoch: 40 loss: 21.06363869 weight: 1.98405814 bias: 1.04787791 epoch: 41 loss: 21.06312370 weight: 1.98402870 bias: 1.04859519 epoch: 42 loss: 21.06260681 weight: 1.98401320 bias: 1.04931259 epoch: 43 loss: 21.06209564 weight: 1.98398757 bias: 1.05002928 epoch: 44 loss: 21.06157875 weight: 1.98396957 bias: 1.05074584 epoch: 45 loss: 21.06106949 weight: 1.98394585 bias: 1.05146194 epoch: 46 loss: 21.06055450 weight: 1.98392630 bias: 1.05217779 epoch: 47 loss: 21.06004143 weight: 1.98390377 bias: 1.05289316 epoch: 48 loss: 21.05953217 weight: 1.98388338 bias: 1.05360830 epoch: 49 loss: 21.05901527 weight: 1.98386145 bias: 1.05432308 epoch: 50 loss: 21.05850983 weight: 1.98384094 bias: 1.05503750","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#plot-the-loss-values","text":"Let\u2019s see how loss changed over time plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'epoch' );","title":"Plot the loss values"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#plot-the-result","text":"Now we\u2019ll derive y1 from the new model to plot the most recent best-fit line. w1 , b1 = model . linear . weight . item (), model . linear . bias . item () print ( f 'Current weight: { w1 : .8f } , Current bias: { b1 : .8f } ' ) print () y1 = x1 * w1 + b1 print ( x1 ) print ( y1 ) Current weight: 1.98381913, Current bias: 1.05575156 [ 1. 50.] [ 3.0395708 100.246704 ] plt . scatter ( X . numpy (), y . numpy ()) plt . plot ( x1 , y1 , 'r' ) plt . title ( 'Current Model' ) plt . ylabel ( 'y' ) plt . xlabel ( 'x' );","title":"Plot the result"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/","text":"================ by Jawad Haider Datasets with PyTorch Perform standard imports Loading data from files Plot the data The classic method for building train/test split tensors Using PyTorch\u2019s Dataset and DataLoader classes A Quick Note on Torchvision Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Datasets with PyTorch \u00b6 In this section we\u2019ll show how to: * load data from outside files * build random batches using PyTorch\u2019s data utilities At the end we\u2019ll briefly mention torchvision . Perform standard imports \u00b6 import torch import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Loading data from files \u00b6 We\u2019ve seen how to load NumPy arrays into PyTorch, and anyone familiar with pandas.read_csv() can use it to prepare data before forming tensors. Here we\u2019ll load the iris flower dataset saved as a .csv file. df = pd . read_csv ( '../Data/iris.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 df . shape (150, 5) Plot the data \u00b6 fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () The iris dataset consists of 50 samples each from three species of Iris ( Iris setosa , Iris virginica and Iris versicolor ), for 150 total samples. We have four features (sepal length & width, petal length & width) and three unique labels: 0. Iris setosa 1. Iris virginica 2. Iris versicolor The classic method for building train/test split tensors \u00b6 Before introducing PyTorch\u2019s Dataset and DataLoader classes, we\u2019ll take a quick look at the alternative. from sklearn.model_selection import train_test_split train_X , test_X , train_y , test_y = train_test_split ( df . drop ( 'target' , axis = 1 ) . values , df [ 'target' ] . values , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( train_X ) X_test = torch . FloatTensor ( test_X ) y_train = torch . LongTensor ( train_y ) . reshape ( - 1 , 1 ) y_test = torch . LongTensor ( test_y ) . reshape ( - 1 , 1 ) print ( f 'Training size: { len ( y_train ) } ' ) labels , counts = y_train . unique ( return_counts = True ) print ( f 'Labels: { labels } \\n Counts: { counts } ' ) Training size: 120 Labels: tensor([0, 1, 2]) Counts: tensor([42, 42, 36]) NOTE: The importance of a balanced training set is discussed in A systematic study of the class imbalance problem in convolutional neural networks by Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski (10/15/17, latest rev 10/13/18) https://arxiv.org/abs/1710.05381 For example, the authors show that oversampling a less common class so that it matches the more common classes is always the preferred choice. X_train . size () torch.Size([120, 4]) y_train . size () torch.Size([120, 1]) NOTE: It\u2019s up to us to remember which columns correspond to which features. Using PyTorch\u2019s Dataset and DataLoader classes \u00b6 A far better alternative is to leverage PyTorch\u2019s Dataset and DataLoader classes. Usually, to set up a Dataset specific to our investigation we would define our own custom class that inherits from torch.utils.data.Dataset (we\u2019ll do this in the CNN section). For now, we can use the built-in TensorDataset class. from torch.utils.data import TensorDataset , DataLoader data = df . drop ( 'target' , axis = 1 ) . values labels = df [ 'target' ] . values iris = TensorDataset ( torch . FloatTensor ( data ), torch . LongTensor ( labels )) len ( iris ) 150 type ( iris ) torch.utils.data.dataset.TensorDataset for i in iris : print ( i ) (tensor([5.1000, 3.5000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.0000, 1.4000, 0.2000]), tensor(0)) (tensor([4.7000, 3.2000, 1.3000, 0.2000]), tensor(0)) (tensor([4.6000, 3.1000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.6000, 1.4000, 0.2000]), tensor(0)) (tensor([5.4000, 3.9000, 1.7000, 0.4000]), tensor(0)) (tensor([4.6000, 3.4000, 1.4000, 0.3000]), tensor(0)) (tensor([5.0000, 3.4000, 1.5000, 0.2000]), tensor(0)) (tensor([4.4000, 2.9000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.4000, 3.7000, 1.5000, 0.2000]), tensor(0)) (tensor([4.8000, 3.4000, 1.6000, 0.2000]), tensor(0)) (tensor([4.8000, 3.0000, 1.4000, 0.1000]), tensor(0)) (tensor([4.3000, 3.0000, 1.1000, 0.1000]), tensor(0)) (tensor([5.8000, 4.0000, 1.2000, 0.2000]), tensor(0)) (tensor([5.7000, 4.4000, 1.5000, 0.4000]), tensor(0)) (tensor([5.4000, 3.9000, 1.3000, 0.4000]), tensor(0)) (tensor([5.1000, 3.5000, 1.4000, 0.3000]), tensor(0)) (tensor([5.7000, 3.8000, 1.7000, 0.3000]), tensor(0)) (tensor([5.1000, 3.8000, 1.5000, 0.3000]), tensor(0)) (tensor([5.4000, 3.4000, 1.7000, 0.2000]), tensor(0)) (tensor([5.1000, 3.7000, 1.5000, 0.4000]), tensor(0)) (tensor([4.6000, 3.6000, 1.0000, 0.2000]), tensor(0)) (tensor([5.1000, 3.3000, 1.7000, 0.5000]), tensor(0)) (tensor([4.8000, 3.4000, 1.9000, 0.2000]), tensor(0)) (tensor([5.0000, 3.0000, 1.6000, 0.2000]), tensor(0)) (tensor([5.0000, 3.4000, 1.6000, 0.4000]), tensor(0)) (tensor([5.2000, 3.5000, 1.5000, 0.2000]), tensor(0)) (tensor([5.2000, 3.4000, 1.4000, 0.2000]), tensor(0)) (tensor([4.7000, 3.2000, 1.6000, 0.2000]), tensor(0)) (tensor([4.8000, 3.1000, 1.6000, 0.2000]), tensor(0)) (tensor([5.4000, 3.4000, 1.5000, 0.4000]), tensor(0)) (tensor([5.2000, 4.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.5000, 4.2000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.0000, 3.2000, 1.2000, 0.2000]), tensor(0)) (tensor([5.5000, 3.5000, 1.3000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([4.4000, 3.0000, 1.3000, 0.2000]), tensor(0)) (tensor([5.1000, 3.4000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.5000, 1.3000, 0.3000]), tensor(0)) (tensor([4.5000, 2.3000, 1.3000, 0.3000]), tensor(0)) (tensor([4.4000, 3.2000, 1.3000, 0.2000]), tensor(0)) (tensor([5.0000, 3.5000, 1.6000, 0.6000]), tensor(0)) (tensor([5.1000, 3.8000, 1.9000, 0.4000]), tensor(0)) (tensor([4.8000, 3.0000, 1.4000, 0.3000]), tensor(0)) (tensor([5.1000, 3.8000, 1.6000, 0.2000]), tensor(0)) (tensor([4.6000, 3.2000, 1.4000, 0.2000]), tensor(0)) (tensor([5.3000, 3.7000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.3000, 1.4000, 0.2000]), tensor(0)) (tensor([7.0000, 3.2000, 4.7000, 1.4000]), tensor(1)) (tensor([6.4000, 3.2000, 4.5000, 1.5000]), tensor(1)) (tensor([6.9000, 3.1000, 4.9000, 1.5000]), tensor(1)) (tensor([5.5000, 2.3000, 4.0000, 1.3000]), tensor(1)) (tensor([6.5000, 2.8000, 4.6000, 1.5000]), tensor(1)) (tensor([5.7000, 2.8000, 4.5000, 1.3000]), tensor(1)) (tensor([6.3000, 3.3000, 4.7000, 1.6000]), tensor(1)) (tensor([4.9000, 2.4000, 3.3000, 1.0000]), tensor(1)) (tensor([6.6000, 2.9000, 4.6000, 1.3000]), tensor(1)) (tensor([5.2000, 2.7000, 3.9000, 1.4000]), tensor(1)) (tensor([5.0000, 2.0000, 3.5000, 1.0000]), tensor(1)) (tensor([5.9000, 3.0000, 4.2000, 1.5000]), tensor(1)) (tensor([6.0000, 2.2000, 4.0000, 1.0000]), tensor(1)) (tensor([6.1000, 2.9000, 4.7000, 1.4000]), tensor(1)) (tensor([5.6000, 2.9000, 3.6000, 1.3000]), tensor(1)) (tensor([6.7000, 3.1000, 4.4000, 1.4000]), tensor(1)) (tensor([5.6000, 3.0000, 4.5000, 1.5000]), tensor(1)) (tensor([5.8000, 2.7000, 4.1000, 1.0000]), tensor(1)) (tensor([6.2000, 2.2000, 4.5000, 1.5000]), tensor(1)) (tensor([5.6000, 2.5000, 3.9000, 1.1000]), tensor(1)) (tensor([5.9000, 3.2000, 4.8000, 1.8000]), tensor(1)) (tensor([6.1000, 2.8000, 4.0000, 1.3000]), tensor(1)) (tensor([6.3000, 2.5000, 4.9000, 1.5000]), tensor(1)) (tensor([6.1000, 2.8000, 4.7000, 1.2000]), tensor(1)) (tensor([6.4000, 2.9000, 4.3000, 1.3000]), tensor(1)) (tensor([6.6000, 3.0000, 4.4000, 1.4000]), tensor(1)) (tensor([6.8000, 2.8000, 4.8000, 1.4000]), tensor(1)) (tensor([6.7000, 3.0000, 5.0000, 1.7000]), tensor(1)) (tensor([6.0000, 2.9000, 4.5000, 1.5000]), tensor(1)) (tensor([5.7000, 2.6000, 3.5000, 1.0000]), tensor(1)) (tensor([5.5000, 2.4000, 3.8000, 1.1000]), tensor(1)) (tensor([5.5000, 2.4000, 3.7000, 1.0000]), tensor(1)) (tensor([5.8000, 2.7000, 3.9000, 1.2000]), tensor(1)) (tensor([6.0000, 2.7000, 5.1000, 1.6000]), tensor(1)) (tensor([5.4000, 3.0000, 4.5000, 1.5000]), tensor(1)) (tensor([6.0000, 3.4000, 4.5000, 1.6000]), tensor(1)) (tensor([6.7000, 3.1000, 4.7000, 1.5000]), tensor(1)) (tensor([6.3000, 2.3000, 4.4000, 1.3000]), tensor(1)) (tensor([5.6000, 3.0000, 4.1000, 1.3000]), tensor(1)) (tensor([5.5000, 2.5000, 4.0000, 1.3000]), tensor(1)) (tensor([5.5000, 2.6000, 4.4000, 1.2000]), tensor(1)) (tensor([6.1000, 3.0000, 4.6000, 1.4000]), tensor(1)) (tensor([5.8000, 2.6000, 4.0000, 1.2000]), tensor(1)) (tensor([5.0000, 2.3000, 3.3000, 1.0000]), tensor(1)) (tensor([5.6000, 2.7000, 4.2000, 1.3000]), tensor(1)) (tensor([5.7000, 3.0000, 4.2000, 1.2000]), tensor(1)) (tensor([5.7000, 2.9000, 4.2000, 1.3000]), tensor(1)) (tensor([6.2000, 2.9000, 4.3000, 1.3000]), tensor(1)) (tensor([5.1000, 2.5000, 3.0000, 1.1000]), tensor(1)) (tensor([5.7000, 2.8000, 4.1000, 1.3000]), tensor(1)) (tensor([6.3000, 3.3000, 6.0000, 2.5000]), tensor(2)) (tensor([5.8000, 2.7000, 5.1000, 1.9000]), tensor(2)) (tensor([7.1000, 3.0000, 5.9000, 2.1000]), tensor(2)) (tensor([6.3000, 2.9000, 5.6000, 1.8000]), tensor(2)) (tensor([6.5000, 3.0000, 5.8000, 2.2000]), tensor(2)) (tensor([7.6000, 3.0000, 6.6000, 2.1000]), tensor(2)) (tensor([4.9000, 2.5000, 4.5000, 1.7000]), tensor(2)) (tensor([7.3000, 2.9000, 6.3000, 1.8000]), tensor(2)) (tensor([6.7000, 2.5000, 5.8000, 1.8000]), tensor(2)) (tensor([7.2000, 3.6000, 6.1000, 2.5000]), tensor(2)) (tensor([6.5000, 3.2000, 5.1000, 2.0000]), tensor(2)) (tensor([6.4000, 2.7000, 5.3000, 1.9000]), tensor(2)) (tensor([6.8000, 3.0000, 5.5000, 2.1000]), tensor(2)) (tensor([5.7000, 2.5000, 5.0000, 2.0000]), tensor(2)) (tensor([5.8000, 2.8000, 5.1000, 2.4000]), tensor(2)) (tensor([6.4000, 3.2000, 5.3000, 2.3000]), tensor(2)) (tensor([6.5000, 3.0000, 5.5000, 1.8000]), tensor(2)) (tensor([7.7000, 3.8000, 6.7000, 2.2000]), tensor(2)) (tensor([7.7000, 2.6000, 6.9000, 2.3000]), tensor(2)) (tensor([6.0000, 2.2000, 5.0000, 1.5000]), tensor(2)) (tensor([6.9000, 3.2000, 5.7000, 2.3000]), tensor(2)) (tensor([5.6000, 2.8000, 4.9000, 2.0000]), tensor(2)) (tensor([7.7000, 2.8000, 6.7000, 2.0000]), tensor(2)) (tensor([6.3000, 2.7000, 4.9000, 1.8000]), tensor(2)) (tensor([6.7000, 3.3000, 5.7000, 2.1000]), tensor(2)) (tensor([7.2000, 3.2000, 6.0000, 1.8000]), tensor(2)) (tensor([6.2000, 2.8000, 4.8000, 1.8000]), tensor(2)) (tensor([6.1000, 3.0000, 4.9000, 1.8000]), tensor(2)) (tensor([6.4000, 2.8000, 5.6000, 2.1000]), tensor(2)) (tensor([7.2000, 3.0000, 5.8000, 1.6000]), tensor(2)) (tensor([7.4000, 2.8000, 6.1000, 1.9000]), tensor(2)) (tensor([7.9000, 3.8000, 6.4000, 2.0000]), tensor(2)) (tensor([6.4000, 2.8000, 5.6000, 2.2000]), tensor(2)) (tensor([6.3000, 2.8000, 5.1000, 1.5000]), tensor(2)) (tensor([6.1000, 2.6000, 5.6000, 1.4000]), tensor(2)) (tensor([7.7000, 3.0000, 6.1000, 2.3000]), tensor(2)) (tensor([6.3000, 3.4000, 5.6000, 2.4000]), tensor(2)) (tensor([6.4000, 3.1000, 5.5000, 1.8000]), tensor(2)) (tensor([6.0000, 3.0000, 4.8000, 1.8000]), tensor(2)) (tensor([6.9000, 3.1000, 5.4000, 2.1000]), tensor(2)) (tensor([6.7000, 3.1000, 5.6000, 2.4000]), tensor(2)) (tensor([6.9000, 3.1000, 5.1000, 2.3000]), tensor(2)) (tensor([5.8000, 2.7000, 5.1000, 1.9000]), tensor(2)) (tensor([6.8000, 3.2000, 5.9000, 2.3000]), tensor(2)) (tensor([6.7000, 3.3000, 5.7000, 2.5000]), tensor(2)) (tensor([6.7000, 3.0000, 5.2000, 2.3000]), tensor(2)) (tensor([6.3000, 2.5000, 5.0000, 1.9000]), tensor(2)) (tensor([6.5000, 3.0000, 5.2000, 2.0000]), tensor(2)) (tensor([6.2000, 3.4000, 5.4000, 2.3000]), tensor(2)) (tensor([5.9000, 3.0000, 5.1000, 1.8000]), tensor(2)) Once we have a dataset we can wrap it with a DataLoader. This gives us a powerful sampler that provides single- or multi-process iterators over the dataset. iris_loader = DataLoader ( iris , batch_size = 105 , shuffle = True ) for i_batch , sample_batched in enumerate ( iris_loader ): print ( i_batch , sample_batched ) 0 [tensor([[6.7000, 3.1000, 4.4000, 1.4000], [4.8000, 3.4000, 1.6000, 0.2000], [4.8000, 3.4000, 1.9000, 0.2000], [5.6000, 2.9000, 3.6000, 1.3000], [6.7000, 3.3000, 5.7000, 2.1000], [6.9000, 3.1000, 4.9000, 1.5000], [6.7000, 2.5000, 5.8000, 1.8000], [6.4000, 3.1000, 5.5000, 1.8000], [6.7000, 3.1000, 5.6000, 2.4000], [5.5000, 2.5000, 4.0000, 1.3000], [7.0000, 3.2000, 4.7000, 1.4000], [5.5000, 4.2000, 1.4000, 0.2000], [7.3000, 2.9000, 6.3000, 1.8000], [7.7000, 2.8000, 6.7000, 2.0000], [4.9000, 3.1000, 1.5000, 0.1000], [7.2000, 3.0000, 5.8000, 1.6000], [6.7000, 3.1000, 4.7000, 1.5000], [5.8000, 2.7000, 3.9000, 1.2000], [5.5000, 2.4000, 3.8000, 1.1000], [5.0000, 3.5000, 1.6000, 0.6000], [5.1000, 3.8000, 1.6000, 0.2000], [4.8000, 3.0000, 1.4000, 0.1000], [6.5000, 3.0000, 5.5000, 1.8000], [6.7000, 3.0000, 5.2000, 2.3000], [6.8000, 2.8000, 4.8000, 1.4000], [7.4000, 2.8000, 6.1000, 1.9000], [5.0000, 3.4000, 1.6000, 0.4000], [6.3000, 3.3000, 6.0000, 2.5000], [5.7000, 2.8000, 4.1000, 1.3000], [5.1000, 3.8000, 1.9000, 0.4000], [6.6000, 2.9000, 4.6000, 1.3000], [6.3000, 3.4000, 5.6000, 2.4000], [5.0000, 3.2000, 1.2000, 0.2000], [5.9000, 3.2000, 4.8000, 1.8000], [4.7000, 3.2000, 1.6000, 0.2000], [5.1000, 3.8000, 1.5000, 0.3000], [5.7000, 2.6000, 3.5000, 1.0000], [5.7000, 4.4000, 1.5000, 0.4000], [5.0000, 2.0000, 3.5000, 1.0000], [4.4000, 3.2000, 1.3000, 0.2000], [5.2000, 3.4000, 1.4000, 0.2000], [5.5000, 2.3000, 4.0000, 1.3000], [7.6000, 3.0000, 6.6000, 2.1000], [4.4000, 2.9000, 1.4000, 0.2000], [5.7000, 3.8000, 1.7000, 0.3000], [7.7000, 3.0000, 6.1000, 2.3000], [4.9000, 2.5000, 4.5000, 1.7000], [5.9000, 3.0000, 5.1000, 1.8000], [7.2000, 3.6000, 6.1000, 2.5000], [5.8000, 2.8000, 5.1000, 2.4000], [4.7000, 3.2000, 1.3000, 0.2000], [6.2000, 3.4000, 5.4000, 2.3000], [5.7000, 3.0000, 4.2000, 1.2000], [5.6000, 2.7000, 4.2000, 1.3000], [5.2000, 4.1000, 1.5000, 0.1000], [5.1000, 3.5000, 1.4000, 0.3000], [5.0000, 3.0000, 1.6000, 0.2000], [6.3000, 2.3000, 4.4000, 1.3000], [6.5000, 3.2000, 5.1000, 2.0000], [5.6000, 2.8000, 4.9000, 2.0000], [5.4000, 3.4000, 1.7000, 0.2000], [5.9000, 3.0000, 4.2000, 1.5000], [6.2000, 2.2000, 4.5000, 1.5000], [5.1000, 3.4000, 1.5000, 0.2000], [6.9000, 3.1000, 5.4000, 2.1000], [4.6000, 3.2000, 1.4000, 0.2000], [5.8000, 2.7000, 4.1000, 1.0000], [5.8000, 2.7000, 5.1000, 1.9000], [6.0000, 2.2000, 4.0000, 1.0000], [6.3000, 2.7000, 4.9000, 1.8000], [7.1000, 3.0000, 5.9000, 2.1000], [6.3000, 2.9000, 5.6000, 1.8000], [4.6000, 3.1000, 1.5000, 0.2000], [4.4000, 3.0000, 1.3000, 0.2000], [5.5000, 2.6000, 4.4000, 1.2000], [5.4000, 3.4000, 1.5000, 0.4000], [4.9000, 2.4000, 3.3000, 1.0000], [6.2000, 2.8000, 4.8000, 1.8000], [7.2000, 3.2000, 6.0000, 1.8000], [6.3000, 3.3000, 4.7000, 1.6000], [5.6000, 3.0000, 4.5000, 1.5000], [6.0000, 2.7000, 5.1000, 1.6000], [6.0000, 2.2000, 5.0000, 1.5000], [6.4000, 2.9000, 4.3000, 1.3000], [5.8000, 2.6000, 4.0000, 1.2000], [6.9000, 3.1000, 5.1000, 2.3000], [5.6000, 3.0000, 4.1000, 1.3000], [5.4000, 3.9000, 1.3000, 0.4000], [5.3000, 3.7000, 1.5000, 0.2000], [6.3000, 2.5000, 4.9000, 1.5000], [5.0000, 3.6000, 1.4000, 0.2000], [5.1000, 3.3000, 1.7000, 0.5000], [6.1000, 2.8000, 4.7000, 1.2000], [6.2000, 2.9000, 4.3000, 1.3000], [6.7000, 3.0000, 5.0000, 1.7000], [6.1000, 2.6000, 5.6000, 1.4000], [6.4000, 2.7000, 5.3000, 1.9000], [4.5000, 2.3000, 1.3000, 0.3000], [6.1000, 2.8000, 4.0000, 1.3000], [5.4000, 3.0000, 4.5000, 1.5000], [6.5000, 3.0000, 5.2000, 2.0000], [6.0000, 3.0000, 4.8000, 1.8000], [5.0000, 3.5000, 1.3000, 0.3000], [6.5000, 3.0000, 5.8000, 2.2000], [5.0000, 3.3000, 1.4000, 0.2000]]), tensor([1, 0, 0, 1, 2, 1, 2, 2, 2, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 0, 0, 0, 2, 2, 1, 2, 0, 2, 1, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 1, 1, 0, 0, 0, 1, 2, 2, 0, 1, 1, 0, 2, 0, 1, 2, 1, 2, 2, 2, 0, 0, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 1, 2, 2, 0, 1, 1, 2, 2, 0, 2, 0])] 1 [tensor([[5.5000, 2.4000, 3.7000, 1.0000], [6.4000, 3.2000, 4.5000, 1.5000], [5.6000, 2.5000, 3.9000, 1.1000], [5.1000, 3.7000, 1.5000, 0.4000], [5.0000, 2.3000, 3.3000, 1.0000], [5.0000, 3.4000, 1.5000, 0.2000], [5.7000, 2.9000, 4.2000, 1.3000], [6.1000, 3.0000, 4.9000, 1.8000], [5.1000, 2.5000, 3.0000, 1.1000], [6.0000, 2.9000, 4.5000, 1.5000], [4.3000, 3.0000, 1.1000, 0.1000], [6.4000, 2.8000, 5.6000, 2.1000], [5.1000, 3.5000, 1.4000, 0.2000], [5.7000, 2.8000, 4.5000, 1.3000], [4.9000, 3.1000, 1.5000, 0.1000], [7.7000, 2.6000, 6.9000, 2.3000], [6.6000, 3.0000, 4.4000, 1.4000], [4.8000, 3.0000, 1.4000, 0.3000], [6.4000, 2.8000, 5.6000, 2.2000], [6.8000, 3.2000, 5.9000, 2.3000], [5.4000, 3.7000, 1.5000, 0.2000], [4.9000, 3.1000, 1.5000, 0.1000], [6.9000, 3.2000, 5.7000, 2.3000], [5.8000, 2.7000, 5.1000, 1.9000], [6.1000, 2.9000, 4.7000, 1.4000], [4.6000, 3.6000, 1.0000, 0.2000], [5.2000, 3.5000, 1.5000, 0.2000], [6.8000, 3.0000, 5.5000, 2.1000], [7.7000, 3.8000, 6.7000, 2.2000], [6.0000, 3.4000, 4.5000, 1.6000], [5.7000, 2.5000, 5.0000, 2.0000], [6.5000, 2.8000, 4.6000, 1.5000], [4.6000, 3.4000, 1.4000, 0.3000], [5.2000, 2.7000, 3.9000, 1.4000], [5.5000, 3.5000, 1.3000, 0.2000], [4.9000, 3.0000, 1.4000, 0.2000], [6.3000, 2.5000, 5.0000, 1.9000], [6.1000, 3.0000, 4.6000, 1.4000], [6.4000, 3.2000, 5.3000, 2.3000], [5.8000, 4.0000, 1.2000, 0.2000], [6.3000, 2.8000, 5.1000, 1.5000], [4.8000, 3.1000, 1.6000, 0.2000], [6.7000, 3.3000, 5.7000, 2.5000], [5.4000, 3.9000, 1.7000, 0.4000], [7.9000, 3.8000, 6.4000, 2.0000]]), tensor([1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 2, 1, 0, 2, 2, 0, 0, 2, 2, 1, 0, 0, 2, 2, 1, 2, 1, 0, 1, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2])] list ( iris_loader )[ 0 ][ 1 ] . bincount () tensor([30, 36, 39]) next ( iter ( iris_loader )) [tensor([[5.4000, 3.7000, 1.5000, 0.2000], [4.7000, 3.2000, 1.3000, 0.2000], [6.1000, 3.0000, 4.6000, 1.4000], [4.3000, 3.0000, 1.1000, 0.1000], [5.0000, 3.5000, 1.3000, 0.3000], [7.2000, 3.2000, 6.0000, 1.8000], [4.8000, 3.4000, 1.9000, 0.2000], [6.4000, 3.1000, 5.5000, 1.8000], [6.6000, 3.0000, 4.4000, 1.4000], [6.8000, 3.2000, 5.9000, 2.3000], [6.4000, 3.2000, 4.5000, 1.5000], [5.0000, 2.3000, 3.3000, 1.0000], [6.0000, 2.2000, 4.0000, 1.0000], [6.7000, 3.1000, 5.6000, 2.4000], [6.0000, 2.7000, 5.1000, 1.6000], [6.2000, 2.8000, 4.8000, 1.8000], [5.4000, 3.4000, 1.7000, 0.2000], [5.4000, 3.9000, 1.7000, 0.4000], [4.6000, 3.2000, 1.4000, 0.2000], [5.2000, 2.7000, 3.9000, 1.4000], [6.0000, 3.0000, 4.8000, 1.8000], [5.7000, 2.8000, 4.5000, 1.3000], [7.7000, 2.6000, 6.9000, 2.3000], [5.2000, 3.5000, 1.5000, 0.2000], [6.4000, 2.8000, 5.6000, 2.1000], [5.7000, 3.8000, 1.7000, 0.3000], [6.3000, 2.7000, 4.9000, 1.8000], [6.8000, 2.8000, 4.8000, 1.4000], [6.5000, 3.2000, 5.1000, 2.0000], [6.9000, 3.2000, 5.7000, 2.3000], [7.6000, 3.0000, 6.6000, 2.1000], [6.5000, 2.8000, 4.6000, 1.5000], [5.4000, 3.9000, 1.3000, 0.4000], [5.6000, 3.0000, 4.5000, 1.5000], [6.3000, 2.5000, 4.9000, 1.5000], [5.2000, 4.1000, 1.5000, 0.1000], [5.6000, 3.0000, 4.1000, 1.3000], [4.9000, 3.1000, 1.5000, 0.1000], [6.2000, 2.2000, 4.5000, 1.5000], [5.9000, 3.0000, 4.2000, 1.5000], [6.7000, 2.5000, 5.8000, 1.8000], [5.1000, 3.5000, 1.4000, 0.2000], [6.5000, 3.0000, 5.2000, 2.0000], [7.2000, 3.0000, 5.8000, 1.6000], [5.9000, 3.2000, 4.8000, 1.8000], [6.7000, 3.3000, 5.7000, 2.1000], [5.5000, 2.5000, 4.0000, 1.3000], [6.6000, 2.9000, 4.6000, 1.3000], [5.0000, 3.2000, 1.2000, 0.2000], [5.3000, 3.7000, 1.5000, 0.2000], [6.2000, 3.4000, 5.4000, 2.3000], [6.3000, 3.4000, 5.6000, 2.4000], [5.8000, 2.7000, 5.1000, 1.9000], [7.0000, 3.2000, 4.7000, 1.4000], [4.6000, 3.1000, 1.5000, 0.2000], [7.3000, 2.9000, 6.3000, 1.8000], [5.5000, 2.4000, 3.8000, 1.1000], [5.6000, 2.8000, 4.9000, 2.0000], [6.2000, 2.9000, 4.3000, 1.3000], [5.0000, 3.0000, 1.6000, 0.2000], [6.9000, 3.1000, 5.1000, 2.3000], [6.4000, 2.9000, 4.3000, 1.3000], [5.6000, 2.5000, 3.9000, 1.1000], [6.7000, 3.1000, 4.4000, 1.4000], [7.7000, 3.8000, 6.7000, 2.2000], [5.8000, 4.0000, 1.2000, 0.2000], [6.7000, 3.3000, 5.7000, 2.5000], [5.5000, 2.6000, 4.4000, 1.2000], [4.4000, 3.0000, 1.3000, 0.2000], [6.5000, 3.0000, 5.8000, 2.2000], [7.7000, 2.8000, 6.7000, 2.0000], [6.1000, 2.6000, 5.6000, 1.4000], [4.9000, 2.4000, 3.3000, 1.0000], [6.7000, 3.1000, 4.7000, 1.5000], [6.0000, 3.4000, 4.5000, 1.6000], [4.5000, 2.3000, 1.3000, 0.3000], [5.1000, 3.7000, 1.5000, 0.4000], [7.2000, 3.6000, 6.1000, 2.5000], [5.6000, 2.9000, 3.6000, 1.3000], [5.8000, 2.7000, 3.9000, 1.2000], [7.1000, 3.0000, 5.9000, 2.1000], [6.0000, 2.2000, 5.0000, 1.5000], [5.0000, 3.4000, 1.5000, 0.2000], [6.9000, 3.1000, 4.9000, 1.5000], [5.7000, 2.8000, 4.1000, 1.3000], [6.3000, 2.5000, 5.0000, 1.9000], [6.4000, 2.7000, 5.3000, 1.9000], [5.0000, 2.0000, 3.5000, 1.0000], [6.3000, 3.3000, 6.0000, 2.5000], [6.3000, 2.8000, 5.1000, 1.5000], [5.5000, 2.3000, 4.0000, 1.3000], [5.8000, 2.6000, 4.0000, 1.2000], [5.7000, 2.9000, 4.2000, 1.3000], [6.7000, 3.0000, 5.2000, 2.3000], [7.4000, 2.8000, 6.1000, 1.9000], [5.7000, 2.6000, 3.5000, 1.0000], [4.9000, 2.5000, 4.5000, 1.7000], [5.8000, 2.7000, 5.1000, 1.9000], [5.7000, 3.0000, 4.2000, 1.2000], [4.9000, 3.1000, 1.5000, 0.1000], [6.1000, 2.9000, 4.7000, 1.4000], [4.8000, 3.0000, 1.4000, 0.3000], [4.4000, 3.2000, 1.3000, 0.2000], [5.1000, 3.8000, 1.9000, 0.4000], [5.7000, 4.4000, 1.5000, 0.4000]]), tensor([0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 2, 0, 0, 0, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 0, 1, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 1, 2, 1, 1, 0, 0, 2, 2, 2, 1, 0, 2, 1, 2, 1, 0, 2, 1, 1, 1, 2, 0, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 0, 2, 1, 1, 2, 2, 0, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 0, 1, 0, 0, 0, 0])] A Quick Note on Torchvision \u00b6 PyTorch offers another powerful dataset tool called torchvision , which is useful when working with image data. We\u2019ll go into a lot more detail in the Convolutional Neural Network (CNN) section. For now, just know that torchvision offers built-in image datasets like MNIST and CIFAR-10 , as well as tools for transforming images into tensors.","title":"02 DataSets with Pytorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/#datasets-with-pytorch","text":"In this section we\u2019ll show how to: * load data from outside files * build random batches using PyTorch\u2019s data utilities At the end we\u2019ll briefly mention torchvision .","title":"Datasets with PyTorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/#perform-standard-imports","text":"import torch import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/#loading-data-from-files","text":"We\u2019ve seen how to load NumPy arrays into PyTorch, and anyone familiar with pandas.read_csv() can use it to prepare data before forming tensors. Here we\u2019ll load the iris flower dataset saved as a .csv file. df = pd . read_csv ( '../Data/iris.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 df . shape (150, 5)","title":"Loading data from files"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/#plot-the-data","text":"fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () The iris dataset consists of 50 samples each from three species of Iris ( Iris setosa , Iris virginica and Iris versicolor ), for 150 total samples. We have four features (sepal length & width, petal length & width) and three unique labels: 0. Iris setosa 1. Iris virginica 2. Iris versicolor","title":"Plot the data"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/#the-classic-method-for-building-traintest-split-tensors","text":"Before introducing PyTorch\u2019s Dataset and DataLoader classes, we\u2019ll take a quick look at the alternative. from sklearn.model_selection import train_test_split train_X , test_X , train_y , test_y = train_test_split ( df . drop ( 'target' , axis = 1 ) . values , df [ 'target' ] . values , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( train_X ) X_test = torch . FloatTensor ( test_X ) y_train = torch . LongTensor ( train_y ) . reshape ( - 1 , 1 ) y_test = torch . LongTensor ( test_y ) . reshape ( - 1 , 1 ) print ( f 'Training size: { len ( y_train ) } ' ) labels , counts = y_train . unique ( return_counts = True ) print ( f 'Labels: { labels } \\n Counts: { counts } ' ) Training size: 120 Labels: tensor([0, 1, 2]) Counts: tensor([42, 42, 36]) NOTE: The importance of a balanced training set is discussed in A systematic study of the class imbalance problem in convolutional neural networks by Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski (10/15/17, latest rev 10/13/18) https://arxiv.org/abs/1710.05381 For example, the authors show that oversampling a less common class so that it matches the more common classes is always the preferred choice. X_train . size () torch.Size([120, 4]) y_train . size () torch.Size([120, 1]) NOTE: It\u2019s up to us to remember which columns correspond to which features.","title":"The classic method for building train/test split tensors"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/#using-pytorchs-dataset-and-dataloader-classes","text":"A far better alternative is to leverage PyTorch\u2019s Dataset and DataLoader classes. Usually, to set up a Dataset specific to our investigation we would define our own custom class that inherits from torch.utils.data.Dataset (we\u2019ll do this in the CNN section). For now, we can use the built-in TensorDataset class. from torch.utils.data import TensorDataset , DataLoader data = df . drop ( 'target' , axis = 1 ) . values labels = df [ 'target' ] . values iris = TensorDataset ( torch . FloatTensor ( data ), torch . LongTensor ( labels )) len ( iris ) 150 type ( iris ) torch.utils.data.dataset.TensorDataset for i in iris : print ( i ) (tensor([5.1000, 3.5000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.0000, 1.4000, 0.2000]), tensor(0)) (tensor([4.7000, 3.2000, 1.3000, 0.2000]), tensor(0)) (tensor([4.6000, 3.1000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.6000, 1.4000, 0.2000]), tensor(0)) (tensor([5.4000, 3.9000, 1.7000, 0.4000]), tensor(0)) (tensor([4.6000, 3.4000, 1.4000, 0.3000]), tensor(0)) (tensor([5.0000, 3.4000, 1.5000, 0.2000]), tensor(0)) (tensor([4.4000, 2.9000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.4000, 3.7000, 1.5000, 0.2000]), tensor(0)) (tensor([4.8000, 3.4000, 1.6000, 0.2000]), tensor(0)) (tensor([4.8000, 3.0000, 1.4000, 0.1000]), tensor(0)) (tensor([4.3000, 3.0000, 1.1000, 0.1000]), tensor(0)) (tensor([5.8000, 4.0000, 1.2000, 0.2000]), tensor(0)) (tensor([5.7000, 4.4000, 1.5000, 0.4000]), tensor(0)) (tensor([5.4000, 3.9000, 1.3000, 0.4000]), tensor(0)) (tensor([5.1000, 3.5000, 1.4000, 0.3000]), tensor(0)) (tensor([5.7000, 3.8000, 1.7000, 0.3000]), tensor(0)) (tensor([5.1000, 3.8000, 1.5000, 0.3000]), tensor(0)) (tensor([5.4000, 3.4000, 1.7000, 0.2000]), tensor(0)) (tensor([5.1000, 3.7000, 1.5000, 0.4000]), tensor(0)) (tensor([4.6000, 3.6000, 1.0000, 0.2000]), tensor(0)) (tensor([5.1000, 3.3000, 1.7000, 0.5000]), tensor(0)) (tensor([4.8000, 3.4000, 1.9000, 0.2000]), tensor(0)) (tensor([5.0000, 3.0000, 1.6000, 0.2000]), tensor(0)) (tensor([5.0000, 3.4000, 1.6000, 0.4000]), tensor(0)) (tensor([5.2000, 3.5000, 1.5000, 0.2000]), tensor(0)) (tensor([5.2000, 3.4000, 1.4000, 0.2000]), tensor(0)) (tensor([4.7000, 3.2000, 1.6000, 0.2000]), tensor(0)) (tensor([4.8000, 3.1000, 1.6000, 0.2000]), tensor(0)) (tensor([5.4000, 3.4000, 1.5000, 0.4000]), tensor(0)) (tensor([5.2000, 4.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.5000, 4.2000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.0000, 3.2000, 1.2000, 0.2000]), tensor(0)) (tensor([5.5000, 3.5000, 1.3000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([4.4000, 3.0000, 1.3000, 0.2000]), tensor(0)) (tensor([5.1000, 3.4000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.5000, 1.3000, 0.3000]), tensor(0)) (tensor([4.5000, 2.3000, 1.3000, 0.3000]), tensor(0)) (tensor([4.4000, 3.2000, 1.3000, 0.2000]), tensor(0)) (tensor([5.0000, 3.5000, 1.6000, 0.6000]), tensor(0)) (tensor([5.1000, 3.8000, 1.9000, 0.4000]), tensor(0)) (tensor([4.8000, 3.0000, 1.4000, 0.3000]), tensor(0)) (tensor([5.1000, 3.8000, 1.6000, 0.2000]), tensor(0)) (tensor([4.6000, 3.2000, 1.4000, 0.2000]), tensor(0)) (tensor([5.3000, 3.7000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.3000, 1.4000, 0.2000]), tensor(0)) (tensor([7.0000, 3.2000, 4.7000, 1.4000]), tensor(1)) (tensor([6.4000, 3.2000, 4.5000, 1.5000]), tensor(1)) (tensor([6.9000, 3.1000, 4.9000, 1.5000]), tensor(1)) (tensor([5.5000, 2.3000, 4.0000, 1.3000]), tensor(1)) (tensor([6.5000, 2.8000, 4.6000, 1.5000]), tensor(1)) (tensor([5.7000, 2.8000, 4.5000, 1.3000]), tensor(1)) (tensor([6.3000, 3.3000, 4.7000, 1.6000]), tensor(1)) (tensor([4.9000, 2.4000, 3.3000, 1.0000]), tensor(1)) (tensor([6.6000, 2.9000, 4.6000, 1.3000]), tensor(1)) (tensor([5.2000, 2.7000, 3.9000, 1.4000]), tensor(1)) (tensor([5.0000, 2.0000, 3.5000, 1.0000]), tensor(1)) (tensor([5.9000, 3.0000, 4.2000, 1.5000]), tensor(1)) (tensor([6.0000, 2.2000, 4.0000, 1.0000]), tensor(1)) (tensor([6.1000, 2.9000, 4.7000, 1.4000]), tensor(1)) (tensor([5.6000, 2.9000, 3.6000, 1.3000]), tensor(1)) (tensor([6.7000, 3.1000, 4.4000, 1.4000]), tensor(1)) (tensor([5.6000, 3.0000, 4.5000, 1.5000]), tensor(1)) (tensor([5.8000, 2.7000, 4.1000, 1.0000]), tensor(1)) (tensor([6.2000, 2.2000, 4.5000, 1.5000]), tensor(1)) (tensor([5.6000, 2.5000, 3.9000, 1.1000]), tensor(1)) (tensor([5.9000, 3.2000, 4.8000, 1.8000]), tensor(1)) (tensor([6.1000, 2.8000, 4.0000, 1.3000]), tensor(1)) (tensor([6.3000, 2.5000, 4.9000, 1.5000]), tensor(1)) (tensor([6.1000, 2.8000, 4.7000, 1.2000]), tensor(1)) (tensor([6.4000, 2.9000, 4.3000, 1.3000]), tensor(1)) (tensor([6.6000, 3.0000, 4.4000, 1.4000]), tensor(1)) (tensor([6.8000, 2.8000, 4.8000, 1.4000]), tensor(1)) (tensor([6.7000, 3.0000, 5.0000, 1.7000]), tensor(1)) (tensor([6.0000, 2.9000, 4.5000, 1.5000]), tensor(1)) (tensor([5.7000, 2.6000, 3.5000, 1.0000]), tensor(1)) (tensor([5.5000, 2.4000, 3.8000, 1.1000]), tensor(1)) (tensor([5.5000, 2.4000, 3.7000, 1.0000]), tensor(1)) (tensor([5.8000, 2.7000, 3.9000, 1.2000]), tensor(1)) (tensor([6.0000, 2.7000, 5.1000, 1.6000]), tensor(1)) (tensor([5.4000, 3.0000, 4.5000, 1.5000]), tensor(1)) (tensor([6.0000, 3.4000, 4.5000, 1.6000]), tensor(1)) (tensor([6.7000, 3.1000, 4.7000, 1.5000]), tensor(1)) (tensor([6.3000, 2.3000, 4.4000, 1.3000]), tensor(1)) (tensor([5.6000, 3.0000, 4.1000, 1.3000]), tensor(1)) (tensor([5.5000, 2.5000, 4.0000, 1.3000]), tensor(1)) (tensor([5.5000, 2.6000, 4.4000, 1.2000]), tensor(1)) (tensor([6.1000, 3.0000, 4.6000, 1.4000]), tensor(1)) (tensor([5.8000, 2.6000, 4.0000, 1.2000]), tensor(1)) (tensor([5.0000, 2.3000, 3.3000, 1.0000]), tensor(1)) (tensor([5.6000, 2.7000, 4.2000, 1.3000]), tensor(1)) (tensor([5.7000, 3.0000, 4.2000, 1.2000]), tensor(1)) (tensor([5.7000, 2.9000, 4.2000, 1.3000]), tensor(1)) (tensor([6.2000, 2.9000, 4.3000, 1.3000]), tensor(1)) (tensor([5.1000, 2.5000, 3.0000, 1.1000]), tensor(1)) (tensor([5.7000, 2.8000, 4.1000, 1.3000]), tensor(1)) (tensor([6.3000, 3.3000, 6.0000, 2.5000]), tensor(2)) (tensor([5.8000, 2.7000, 5.1000, 1.9000]), tensor(2)) (tensor([7.1000, 3.0000, 5.9000, 2.1000]), tensor(2)) (tensor([6.3000, 2.9000, 5.6000, 1.8000]), tensor(2)) (tensor([6.5000, 3.0000, 5.8000, 2.2000]), tensor(2)) (tensor([7.6000, 3.0000, 6.6000, 2.1000]), tensor(2)) (tensor([4.9000, 2.5000, 4.5000, 1.7000]), tensor(2)) (tensor([7.3000, 2.9000, 6.3000, 1.8000]), tensor(2)) (tensor([6.7000, 2.5000, 5.8000, 1.8000]), tensor(2)) (tensor([7.2000, 3.6000, 6.1000, 2.5000]), tensor(2)) (tensor([6.5000, 3.2000, 5.1000, 2.0000]), tensor(2)) (tensor([6.4000, 2.7000, 5.3000, 1.9000]), tensor(2)) (tensor([6.8000, 3.0000, 5.5000, 2.1000]), tensor(2)) (tensor([5.7000, 2.5000, 5.0000, 2.0000]), tensor(2)) (tensor([5.8000, 2.8000, 5.1000, 2.4000]), tensor(2)) (tensor([6.4000, 3.2000, 5.3000, 2.3000]), tensor(2)) (tensor([6.5000, 3.0000, 5.5000, 1.8000]), tensor(2)) (tensor([7.7000, 3.8000, 6.7000, 2.2000]), tensor(2)) (tensor([7.7000, 2.6000, 6.9000, 2.3000]), tensor(2)) (tensor([6.0000, 2.2000, 5.0000, 1.5000]), tensor(2)) (tensor([6.9000, 3.2000, 5.7000, 2.3000]), tensor(2)) (tensor([5.6000, 2.8000, 4.9000, 2.0000]), tensor(2)) (tensor([7.7000, 2.8000, 6.7000, 2.0000]), tensor(2)) (tensor([6.3000, 2.7000, 4.9000, 1.8000]), tensor(2)) (tensor([6.7000, 3.3000, 5.7000, 2.1000]), tensor(2)) (tensor([7.2000, 3.2000, 6.0000, 1.8000]), tensor(2)) (tensor([6.2000, 2.8000, 4.8000, 1.8000]), tensor(2)) (tensor([6.1000, 3.0000, 4.9000, 1.8000]), tensor(2)) (tensor([6.4000, 2.8000, 5.6000, 2.1000]), tensor(2)) (tensor([7.2000, 3.0000, 5.8000, 1.6000]), tensor(2)) (tensor([7.4000, 2.8000, 6.1000, 1.9000]), tensor(2)) (tensor([7.9000, 3.8000, 6.4000, 2.0000]), tensor(2)) (tensor([6.4000, 2.8000, 5.6000, 2.2000]), tensor(2)) (tensor([6.3000, 2.8000, 5.1000, 1.5000]), tensor(2)) (tensor([6.1000, 2.6000, 5.6000, 1.4000]), tensor(2)) (tensor([7.7000, 3.0000, 6.1000, 2.3000]), tensor(2)) (tensor([6.3000, 3.4000, 5.6000, 2.4000]), tensor(2)) (tensor([6.4000, 3.1000, 5.5000, 1.8000]), tensor(2)) (tensor([6.0000, 3.0000, 4.8000, 1.8000]), tensor(2)) (tensor([6.9000, 3.1000, 5.4000, 2.1000]), tensor(2)) (tensor([6.7000, 3.1000, 5.6000, 2.4000]), tensor(2)) (tensor([6.9000, 3.1000, 5.1000, 2.3000]), tensor(2)) (tensor([5.8000, 2.7000, 5.1000, 1.9000]), tensor(2)) (tensor([6.8000, 3.2000, 5.9000, 2.3000]), tensor(2)) (tensor([6.7000, 3.3000, 5.7000, 2.5000]), tensor(2)) (tensor([6.7000, 3.0000, 5.2000, 2.3000]), tensor(2)) (tensor([6.3000, 2.5000, 5.0000, 1.9000]), tensor(2)) (tensor([6.5000, 3.0000, 5.2000, 2.0000]), tensor(2)) (tensor([6.2000, 3.4000, 5.4000, 2.3000]), tensor(2)) (tensor([5.9000, 3.0000, 5.1000, 1.8000]), tensor(2)) Once we have a dataset we can wrap it with a DataLoader. This gives us a powerful sampler that provides single- or multi-process iterators over the dataset. iris_loader = DataLoader ( iris , batch_size = 105 , shuffle = True ) for i_batch , sample_batched in enumerate ( iris_loader ): print ( i_batch , sample_batched ) 0 [tensor([[6.7000, 3.1000, 4.4000, 1.4000], [4.8000, 3.4000, 1.6000, 0.2000], [4.8000, 3.4000, 1.9000, 0.2000], [5.6000, 2.9000, 3.6000, 1.3000], [6.7000, 3.3000, 5.7000, 2.1000], [6.9000, 3.1000, 4.9000, 1.5000], [6.7000, 2.5000, 5.8000, 1.8000], [6.4000, 3.1000, 5.5000, 1.8000], [6.7000, 3.1000, 5.6000, 2.4000], [5.5000, 2.5000, 4.0000, 1.3000], [7.0000, 3.2000, 4.7000, 1.4000], [5.5000, 4.2000, 1.4000, 0.2000], [7.3000, 2.9000, 6.3000, 1.8000], [7.7000, 2.8000, 6.7000, 2.0000], [4.9000, 3.1000, 1.5000, 0.1000], [7.2000, 3.0000, 5.8000, 1.6000], [6.7000, 3.1000, 4.7000, 1.5000], [5.8000, 2.7000, 3.9000, 1.2000], [5.5000, 2.4000, 3.8000, 1.1000], [5.0000, 3.5000, 1.6000, 0.6000], [5.1000, 3.8000, 1.6000, 0.2000], [4.8000, 3.0000, 1.4000, 0.1000], [6.5000, 3.0000, 5.5000, 1.8000], [6.7000, 3.0000, 5.2000, 2.3000], [6.8000, 2.8000, 4.8000, 1.4000], [7.4000, 2.8000, 6.1000, 1.9000], [5.0000, 3.4000, 1.6000, 0.4000], [6.3000, 3.3000, 6.0000, 2.5000], [5.7000, 2.8000, 4.1000, 1.3000], [5.1000, 3.8000, 1.9000, 0.4000], [6.6000, 2.9000, 4.6000, 1.3000], [6.3000, 3.4000, 5.6000, 2.4000], [5.0000, 3.2000, 1.2000, 0.2000], [5.9000, 3.2000, 4.8000, 1.8000], [4.7000, 3.2000, 1.6000, 0.2000], [5.1000, 3.8000, 1.5000, 0.3000], [5.7000, 2.6000, 3.5000, 1.0000], [5.7000, 4.4000, 1.5000, 0.4000], [5.0000, 2.0000, 3.5000, 1.0000], [4.4000, 3.2000, 1.3000, 0.2000], [5.2000, 3.4000, 1.4000, 0.2000], [5.5000, 2.3000, 4.0000, 1.3000], [7.6000, 3.0000, 6.6000, 2.1000], [4.4000, 2.9000, 1.4000, 0.2000], [5.7000, 3.8000, 1.7000, 0.3000], [7.7000, 3.0000, 6.1000, 2.3000], [4.9000, 2.5000, 4.5000, 1.7000], [5.9000, 3.0000, 5.1000, 1.8000], [7.2000, 3.6000, 6.1000, 2.5000], [5.8000, 2.8000, 5.1000, 2.4000], [4.7000, 3.2000, 1.3000, 0.2000], [6.2000, 3.4000, 5.4000, 2.3000], [5.7000, 3.0000, 4.2000, 1.2000], [5.6000, 2.7000, 4.2000, 1.3000], [5.2000, 4.1000, 1.5000, 0.1000], [5.1000, 3.5000, 1.4000, 0.3000], [5.0000, 3.0000, 1.6000, 0.2000], [6.3000, 2.3000, 4.4000, 1.3000], [6.5000, 3.2000, 5.1000, 2.0000], [5.6000, 2.8000, 4.9000, 2.0000], [5.4000, 3.4000, 1.7000, 0.2000], [5.9000, 3.0000, 4.2000, 1.5000], [6.2000, 2.2000, 4.5000, 1.5000], [5.1000, 3.4000, 1.5000, 0.2000], [6.9000, 3.1000, 5.4000, 2.1000], [4.6000, 3.2000, 1.4000, 0.2000], [5.8000, 2.7000, 4.1000, 1.0000], [5.8000, 2.7000, 5.1000, 1.9000], [6.0000, 2.2000, 4.0000, 1.0000], [6.3000, 2.7000, 4.9000, 1.8000], [7.1000, 3.0000, 5.9000, 2.1000], [6.3000, 2.9000, 5.6000, 1.8000], [4.6000, 3.1000, 1.5000, 0.2000], [4.4000, 3.0000, 1.3000, 0.2000], [5.5000, 2.6000, 4.4000, 1.2000], [5.4000, 3.4000, 1.5000, 0.4000], [4.9000, 2.4000, 3.3000, 1.0000], [6.2000, 2.8000, 4.8000, 1.8000], [7.2000, 3.2000, 6.0000, 1.8000], [6.3000, 3.3000, 4.7000, 1.6000], [5.6000, 3.0000, 4.5000, 1.5000], [6.0000, 2.7000, 5.1000, 1.6000], [6.0000, 2.2000, 5.0000, 1.5000], [6.4000, 2.9000, 4.3000, 1.3000], [5.8000, 2.6000, 4.0000, 1.2000], [6.9000, 3.1000, 5.1000, 2.3000], [5.6000, 3.0000, 4.1000, 1.3000], [5.4000, 3.9000, 1.3000, 0.4000], [5.3000, 3.7000, 1.5000, 0.2000], [6.3000, 2.5000, 4.9000, 1.5000], [5.0000, 3.6000, 1.4000, 0.2000], [5.1000, 3.3000, 1.7000, 0.5000], [6.1000, 2.8000, 4.7000, 1.2000], [6.2000, 2.9000, 4.3000, 1.3000], [6.7000, 3.0000, 5.0000, 1.7000], [6.1000, 2.6000, 5.6000, 1.4000], [6.4000, 2.7000, 5.3000, 1.9000], [4.5000, 2.3000, 1.3000, 0.3000], [6.1000, 2.8000, 4.0000, 1.3000], [5.4000, 3.0000, 4.5000, 1.5000], [6.5000, 3.0000, 5.2000, 2.0000], [6.0000, 3.0000, 4.8000, 1.8000], [5.0000, 3.5000, 1.3000, 0.3000], [6.5000, 3.0000, 5.8000, 2.2000], [5.0000, 3.3000, 1.4000, 0.2000]]), tensor([1, 0, 0, 1, 2, 1, 2, 2, 2, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 0, 0, 0, 2, 2, 1, 2, 0, 2, 1, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 1, 1, 0, 0, 0, 1, 2, 2, 0, 1, 1, 0, 2, 0, 1, 2, 1, 2, 2, 2, 0, 0, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 1, 2, 2, 0, 1, 1, 2, 2, 0, 2, 0])] 1 [tensor([[5.5000, 2.4000, 3.7000, 1.0000], [6.4000, 3.2000, 4.5000, 1.5000], [5.6000, 2.5000, 3.9000, 1.1000], [5.1000, 3.7000, 1.5000, 0.4000], [5.0000, 2.3000, 3.3000, 1.0000], [5.0000, 3.4000, 1.5000, 0.2000], [5.7000, 2.9000, 4.2000, 1.3000], [6.1000, 3.0000, 4.9000, 1.8000], [5.1000, 2.5000, 3.0000, 1.1000], [6.0000, 2.9000, 4.5000, 1.5000], [4.3000, 3.0000, 1.1000, 0.1000], [6.4000, 2.8000, 5.6000, 2.1000], [5.1000, 3.5000, 1.4000, 0.2000], [5.7000, 2.8000, 4.5000, 1.3000], [4.9000, 3.1000, 1.5000, 0.1000], [7.7000, 2.6000, 6.9000, 2.3000], [6.6000, 3.0000, 4.4000, 1.4000], [4.8000, 3.0000, 1.4000, 0.3000], [6.4000, 2.8000, 5.6000, 2.2000], [6.8000, 3.2000, 5.9000, 2.3000], [5.4000, 3.7000, 1.5000, 0.2000], [4.9000, 3.1000, 1.5000, 0.1000], [6.9000, 3.2000, 5.7000, 2.3000], [5.8000, 2.7000, 5.1000, 1.9000], [6.1000, 2.9000, 4.7000, 1.4000], [4.6000, 3.6000, 1.0000, 0.2000], [5.2000, 3.5000, 1.5000, 0.2000], [6.8000, 3.0000, 5.5000, 2.1000], [7.7000, 3.8000, 6.7000, 2.2000], [6.0000, 3.4000, 4.5000, 1.6000], [5.7000, 2.5000, 5.0000, 2.0000], [6.5000, 2.8000, 4.6000, 1.5000], [4.6000, 3.4000, 1.4000, 0.3000], [5.2000, 2.7000, 3.9000, 1.4000], [5.5000, 3.5000, 1.3000, 0.2000], [4.9000, 3.0000, 1.4000, 0.2000], [6.3000, 2.5000, 5.0000, 1.9000], [6.1000, 3.0000, 4.6000, 1.4000], [6.4000, 3.2000, 5.3000, 2.3000], [5.8000, 4.0000, 1.2000, 0.2000], [6.3000, 2.8000, 5.1000, 1.5000], [4.8000, 3.1000, 1.6000, 0.2000], [6.7000, 3.3000, 5.7000, 2.5000], [5.4000, 3.9000, 1.7000, 0.4000], [7.9000, 3.8000, 6.4000, 2.0000]]), tensor([1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 2, 1, 0, 2, 2, 0, 0, 2, 2, 1, 0, 0, 2, 2, 1, 2, 1, 0, 1, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2])] list ( iris_loader )[ 0 ][ 1 ] . bincount () tensor([30, 36, 39]) next ( iter ( iris_loader )) [tensor([[5.4000, 3.7000, 1.5000, 0.2000], [4.7000, 3.2000, 1.3000, 0.2000], [6.1000, 3.0000, 4.6000, 1.4000], [4.3000, 3.0000, 1.1000, 0.1000], [5.0000, 3.5000, 1.3000, 0.3000], [7.2000, 3.2000, 6.0000, 1.8000], [4.8000, 3.4000, 1.9000, 0.2000], [6.4000, 3.1000, 5.5000, 1.8000], [6.6000, 3.0000, 4.4000, 1.4000], [6.8000, 3.2000, 5.9000, 2.3000], [6.4000, 3.2000, 4.5000, 1.5000], [5.0000, 2.3000, 3.3000, 1.0000], [6.0000, 2.2000, 4.0000, 1.0000], [6.7000, 3.1000, 5.6000, 2.4000], [6.0000, 2.7000, 5.1000, 1.6000], [6.2000, 2.8000, 4.8000, 1.8000], [5.4000, 3.4000, 1.7000, 0.2000], [5.4000, 3.9000, 1.7000, 0.4000], [4.6000, 3.2000, 1.4000, 0.2000], [5.2000, 2.7000, 3.9000, 1.4000], [6.0000, 3.0000, 4.8000, 1.8000], [5.7000, 2.8000, 4.5000, 1.3000], [7.7000, 2.6000, 6.9000, 2.3000], [5.2000, 3.5000, 1.5000, 0.2000], [6.4000, 2.8000, 5.6000, 2.1000], [5.7000, 3.8000, 1.7000, 0.3000], [6.3000, 2.7000, 4.9000, 1.8000], [6.8000, 2.8000, 4.8000, 1.4000], [6.5000, 3.2000, 5.1000, 2.0000], [6.9000, 3.2000, 5.7000, 2.3000], [7.6000, 3.0000, 6.6000, 2.1000], [6.5000, 2.8000, 4.6000, 1.5000], [5.4000, 3.9000, 1.3000, 0.4000], [5.6000, 3.0000, 4.5000, 1.5000], [6.3000, 2.5000, 4.9000, 1.5000], [5.2000, 4.1000, 1.5000, 0.1000], [5.6000, 3.0000, 4.1000, 1.3000], [4.9000, 3.1000, 1.5000, 0.1000], [6.2000, 2.2000, 4.5000, 1.5000], [5.9000, 3.0000, 4.2000, 1.5000], [6.7000, 2.5000, 5.8000, 1.8000], [5.1000, 3.5000, 1.4000, 0.2000], [6.5000, 3.0000, 5.2000, 2.0000], [7.2000, 3.0000, 5.8000, 1.6000], [5.9000, 3.2000, 4.8000, 1.8000], [6.7000, 3.3000, 5.7000, 2.1000], [5.5000, 2.5000, 4.0000, 1.3000], [6.6000, 2.9000, 4.6000, 1.3000], [5.0000, 3.2000, 1.2000, 0.2000], [5.3000, 3.7000, 1.5000, 0.2000], [6.2000, 3.4000, 5.4000, 2.3000], [6.3000, 3.4000, 5.6000, 2.4000], [5.8000, 2.7000, 5.1000, 1.9000], [7.0000, 3.2000, 4.7000, 1.4000], [4.6000, 3.1000, 1.5000, 0.2000], [7.3000, 2.9000, 6.3000, 1.8000], [5.5000, 2.4000, 3.8000, 1.1000], [5.6000, 2.8000, 4.9000, 2.0000], [6.2000, 2.9000, 4.3000, 1.3000], [5.0000, 3.0000, 1.6000, 0.2000], [6.9000, 3.1000, 5.1000, 2.3000], [6.4000, 2.9000, 4.3000, 1.3000], [5.6000, 2.5000, 3.9000, 1.1000], [6.7000, 3.1000, 4.4000, 1.4000], [7.7000, 3.8000, 6.7000, 2.2000], [5.8000, 4.0000, 1.2000, 0.2000], [6.7000, 3.3000, 5.7000, 2.5000], [5.5000, 2.6000, 4.4000, 1.2000], [4.4000, 3.0000, 1.3000, 0.2000], [6.5000, 3.0000, 5.8000, 2.2000], [7.7000, 2.8000, 6.7000, 2.0000], [6.1000, 2.6000, 5.6000, 1.4000], [4.9000, 2.4000, 3.3000, 1.0000], [6.7000, 3.1000, 4.7000, 1.5000], [6.0000, 3.4000, 4.5000, 1.6000], [4.5000, 2.3000, 1.3000, 0.3000], [5.1000, 3.7000, 1.5000, 0.4000], [7.2000, 3.6000, 6.1000, 2.5000], [5.6000, 2.9000, 3.6000, 1.3000], [5.8000, 2.7000, 3.9000, 1.2000], [7.1000, 3.0000, 5.9000, 2.1000], [6.0000, 2.2000, 5.0000, 1.5000], [5.0000, 3.4000, 1.5000, 0.2000], [6.9000, 3.1000, 4.9000, 1.5000], [5.7000, 2.8000, 4.1000, 1.3000], [6.3000, 2.5000, 5.0000, 1.9000], [6.4000, 2.7000, 5.3000, 1.9000], [5.0000, 2.0000, 3.5000, 1.0000], [6.3000, 3.3000, 6.0000, 2.5000], [6.3000, 2.8000, 5.1000, 1.5000], [5.5000, 2.3000, 4.0000, 1.3000], [5.8000, 2.6000, 4.0000, 1.2000], [5.7000, 2.9000, 4.2000, 1.3000], [6.7000, 3.0000, 5.2000, 2.3000], [7.4000, 2.8000, 6.1000, 1.9000], [5.7000, 2.6000, 3.5000, 1.0000], [4.9000, 2.5000, 4.5000, 1.7000], [5.8000, 2.7000, 5.1000, 1.9000], [5.7000, 3.0000, 4.2000, 1.2000], [4.9000, 3.1000, 1.5000, 0.1000], [6.1000, 2.9000, 4.7000, 1.4000], [4.8000, 3.0000, 1.4000, 0.3000], [4.4000, 3.2000, 1.3000, 0.2000], [5.1000, 3.8000, 1.9000, 0.4000], [5.7000, 4.4000, 1.5000, 0.4000]]), tensor([0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 2, 0, 0, 0, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 0, 1, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 1, 2, 1, 1, 0, 0, 2, 2, 2, 1, 0, 2, 1, 2, 1, 0, 2, 1, 1, 1, 2, 0, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 0, 2, 1, 1, 2, 2, 0, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 0, 1, 0, 0, 0, 0])]","title":"Using PyTorch\u2019s Dataset and DataLoader classes"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/#a-quick-note-on-torchvision","text":"PyTorch offers another powerful dataset tool called torchvision , which is useful when working with image data. We\u2019ll go into a lot more detail in the Convolutional Neural Network (CNN) section. For now, just know that torchvision offers built-in image datasets like MNIST and CIFAR-10 , as well as tools for transforming images into tensors.","title":"A Quick Note on Torchvision"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/","text":"================ by Jawad Haider 03 - Basic PyTorch Neural Network \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Basic PyTorch Neural Network Perform standard imports Create a model class Load the iris dataset Plot the dataset Perform Train/Test/Split Prepare DataLoader Define loss equations and optimizations Train the model Plot the loss function Validate the model Save the trained model to a file Save the model Load a new model Apply the model to classify new, unseen data Great job! Basic PyTorch Neural Network \u00b6 Now it\u2019s time to put the pieces together. In this section we\u2019ll: * create a multi-layer deep learning model * load data * train and validate the model We\u2019ll also introduce a new step: * save and load a trained model Our goal is to develop a model capable of classifying an iris plant based on four features. This is a multi-class classification where each sample can belong to ONE of 3 classes ( Iris setosa , Iris virginica or Iris versicolor ). The network will have 4 input neurons (flower dimensions) and 3 output neurons (scores). Our loss function will compare the target label (ground truth) to the corresponding output score. NOTE: Multi-class classifications usually involve converting the target vector to a one_hot encoded matrix. That is, if 5 labels show up as tensor([0,2,1,0,1]) then we would encode them as: tensor([[1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0]]) This is easily accomplished with torch.nn.functional.one_hot() . However, our loss function torch.nn.CrossEntropyLoss() takes care of this for us. Perform standard imports \u00b6 import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader from sklearn.model_selection import train_test_split import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Create a model class \u00b6 For this exercise we\u2019re using the Iris dataset. Since a single straight line can\u2019t classify three flowers we should include at least one hidden layer in our model. In the forward section we\u2019ll use the rectified linear unit (ReLU) function as our activation function. This is available as a full module torch.nn.ReLU or as just a functional call torch.nn.functional.relu class Model ( nn . Module ): def __init__ ( self , in_features = 4 , h1 = 8 , h2 = 9 , out_features = 3 ): super () . __init__ () self . fc1 = nn . Linear ( in_features , h1 ) # input layer self . fc2 = nn . Linear ( h1 , h2 ) # hidden layer self . out = nn . Linear ( h2 , out_features ) # output layer def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . out ( x ) return x # Instantiate the Model class using parameter defaults: torch . manual_seed ( 32 ) model = Model () Load the iris dataset \u00b6 df = pd . read_csv ( '../Data/iris.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 Plot the dataset \u00b6 The iris dataset has 4 features. To get an idea how they correlate we can plot four different relationships among them. We\u2019ll use the index positions of the columns to grab their names in pairs with plots = [(0,1),(2,3),(0,2),(1,3)] . Here (0,1) sets \u201csepal length (cm)\u201d as x and \u201csepal width (cm)\u201d as y fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () Perform Train/Test/Split \u00b6 X = df . drop ( 'target' , axis = 1 ) . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( X_train ) X_test = torch . FloatTensor ( X_test ) # y_train = F.one_hot(torch.LongTensor(y_train)) # not needed with Cross Entropy Loss # y_test = F.one_hot(torch.LongTensor(y_test)) y_train = torch . LongTensor ( y_train ) y_test = torch . LongTensor ( y_test ) Prepare DataLoader \u00b6 For this analysis we don\u2019t need to create a Dataset object, but we should take advantage of PyTorch\u2019s DataLoader tool. Even though our dataset is small (120 training samples), we\u2019ll load it into our model in two batches. This technique becomes very helpful with large datasets. Note that scikit-learn already shuffled the source dataset before preparing train and test sets. We\u2019ll still benefit from the DataLoader shuffle utility for model training if we make multiple passes throught the dataset. trainloader = DataLoader ( X_train , batch_size = 60 , shuffle = True ) testloader = DataLoader ( X_test , batch_size = 60 , shuffle = False ) Define loss equations and optimizations \u00b6 As before, we\u2019ll utilize Cross Entropy with torch.nn.CrossEntropyLoss() For the optimizer, we\u2019ll use a variation of Stochastic Gradient Descent called Adam (short for Adaptive Moment Estimation), with torch.optim.Adam() # FOR REDO torch . manual_seed ( 4 ) model = Model () criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 ) Train the model \u00b6 epochs = 100 losses = [] for i in range ( epochs ): i += 1 y_pred = model . forward ( X_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 10 == 1 : print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () epoch: 1 loss: 1.09568226 epoch: 11 loss: 0.98190653 epoch: 21 loss: 0.75652307 epoch: 31 loss: 0.49447522 epoch: 41 loss: 0.34981874 epoch: 51 loss: 0.22807853 epoch: 61 loss: 0.13547322 epoch: 71 loss: 0.09162075 epoch: 81 loss: 0.07378192 epoch: 91 loss: 0.06546164 Plot the loss function \u00b6 plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'epoch' ); Validate the model \u00b6 Now we run the test set through the model to see if the loss calculation resembles the training data. # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( f ' { loss : .8f } ' ) 0.06246195 correct = 0 with torch . no_grad (): for i , data in enumerate ( X_test ): y_val = model . forward ( data ) print ( f ' { i + 1 : 2 } . { str ( y_val ) : 38 } { y_test [ i ] } ' ) if y_val . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { len ( y_test ) } = { 100 * correct / len ( y_test ) : .2f } % correct' ) 1. tensor([-0.3355, 7.3630, 1.3783]) 1 2. tensor([0.2775, 8.1554, 0.4269]) 1 3. tensor([ 11.9969, 6.1847, -19.1976]) 0 4. tensor([-2.0187, 7.9664, 4.2447]) 1 5. tensor([-6.1348, 7.9516, 11.0913]) 2 6. tensor([-10.2635, 8.3101, 17.9998]) 2 7. tensor([ 12.0542, 6.4321, -19.2909]) 0 8. tensor([ 12.9507, 6.4819, -20.7540]) 0 9. tensor([-5.7723, 8.2435, 10.5083]) 2 10. tensor([-7.8867, 8.6126, 14.0731]) 2 11. tensor([-8.7055, 8.6074, 15.4337]) 2 12. tensor([ 11.6358, 5.8167, -18.6220]) 0 13. tensor([-8.1009, 8.2331, 14.3888]) 2 14. tensor([-2.0791, 7.7752, 4.3188]) 1 15. tensor([-6.0828, 8.3916, 11.0586]) 2 16. tensor([0.1360, 7.8660, 0.6409]) 1 17. tensor([-4.0875, 7.7217, 7.6642]) 2 18. tensor([ 13.1522, 6.5911, -21.0798]) 0 19. tensor([-1.5644, 8.0222, 3.4754]) 1 20. tensor([-6.2859, 8.9728, 11.4248]) 2 21. tensor([ 12.3859, 6.2571, -19.8275]) 0 22. tensor([ 13.8200, 7.0859, -22.1528]) 0 23. tensor([-8.8470, 8.3180, 15.6476]) 2 24. tensor([ 12.1979, 6.1264, -19.5260]) 0 25. tensor([-5.8084, 7.5468, 10.5340]) 2 26. tensor([-4.4526, 7.7876, 8.2865]) 2 27. tensor([-1.4284, 7.7786, 3.2328]) 1 28. tensor([ 0.5356, 7.5360, -0.0492]) 1 29. tensor([-5.8230, 8.1573, 10.5975]) 2 30. tensor([-5.2569, 7.7476, 9.6105]) 2 29 out of 30 = 96.67% correct Here we can see that #17 was misclassified. Save the trained model to a file \u00b6 Right now model has been trained and validated, and seems to correctly classify an iris 97% of the time. Let\u2019s save this to disk. The tools we\u2019ll use are torch.save() and torch.load() There are two basic ways to save a model. The first saves/loads the state_dict (learned parameters) of the model, but not the model class. The syntax follows: Save: torch.save(model.state_dict(), PATH) Load: model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.eval() The second saves the entire model including its class and parameters as a pickle file. Care must be taken if you want to load this into another notebook to make sure all the target data is brought in properly. Save: torch.save(model, PATH) Load: model = torch.load(PATH)) model.eval() In either method, you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results. For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html Save the model \u00b6 torch . save ( model . state_dict (), 'IrisDatasetModel.pt' ) Load a new model \u00b6 We\u2019ll load a new model object and test it as we had before to make sure it worked. new_model = Model () new_model . load_state_dict ( torch . load ( 'IrisDatasetModel.pt' )) new_model . eval () Model( (fc1): Linear(in_features=4, out_features=8, bias=True) (fc2): Linear(in_features=8, out_features=9, bias=True) (out): Linear(in_features=9, out_features=3, bias=True) ) with torch . no_grad (): y_val = new_model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( f ' { loss : .8f } ' ) 0.06246195 Apply the model to classify new, unseen data \u00b6 mystery_iris = torch . tensor ([ 5.6 , 3.7 , 2.2 , 0.5 ]) Let\u2019s plot this new iris in yellow to see where it falls in relation to the others: fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' , 'Mystery iris' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) # Add a plot for our mystery iris: ax . scatter ( mystery_iris [ plots [ i ][ 0 ]], mystery_iris [ plots [ i ][ 1 ]], color = 'y' ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () Now run it through the model: with torch . no_grad (): print ( new_model ( mystery_iris )) print () print ( labels [ new_model ( mystery_iris ) . argmax ()]) tensor([ 12.2116, 7.1285, -19.5247]) Iris setosa Great job! \u00b6","title":"03 Basic PyTorch NN"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#03-basic-pytorch-neural-network","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Basic PyTorch Neural Network Perform standard imports Create a model class Load the iris dataset Plot the dataset Perform Train/Test/Split Prepare DataLoader Define loss equations and optimizations Train the model Plot the loss function Validate the model Save the trained model to a file Save the model Load a new model Apply the model to classify new, unseen data Great job!","title":"03 - Basic PyTorch Neural Network"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#basic-pytorch-neural-network","text":"Now it\u2019s time to put the pieces together. In this section we\u2019ll: * create a multi-layer deep learning model * load data * train and validate the model We\u2019ll also introduce a new step: * save and load a trained model Our goal is to develop a model capable of classifying an iris plant based on four features. This is a multi-class classification where each sample can belong to ONE of 3 classes ( Iris setosa , Iris virginica or Iris versicolor ). The network will have 4 input neurons (flower dimensions) and 3 output neurons (scores). Our loss function will compare the target label (ground truth) to the corresponding output score. NOTE: Multi-class classifications usually involve converting the target vector to a one_hot encoded matrix. That is, if 5 labels show up as tensor([0,2,1,0,1]) then we would encode them as: tensor([[1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0]]) This is easily accomplished with torch.nn.functional.one_hot() . However, our loss function torch.nn.CrossEntropyLoss() takes care of this for us.","title":"Basic PyTorch Neural Network"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#perform-standard-imports","text":"import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader from sklearn.model_selection import train_test_split import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#create-a-model-class","text":"For this exercise we\u2019re using the Iris dataset. Since a single straight line can\u2019t classify three flowers we should include at least one hidden layer in our model. In the forward section we\u2019ll use the rectified linear unit (ReLU) function as our activation function. This is available as a full module torch.nn.ReLU or as just a functional call torch.nn.functional.relu class Model ( nn . Module ): def __init__ ( self , in_features = 4 , h1 = 8 , h2 = 9 , out_features = 3 ): super () . __init__ () self . fc1 = nn . Linear ( in_features , h1 ) # input layer self . fc2 = nn . Linear ( h1 , h2 ) # hidden layer self . out = nn . Linear ( h2 , out_features ) # output layer def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . out ( x ) return x # Instantiate the Model class using parameter defaults: torch . manual_seed ( 32 ) model = Model ()","title":"Create a model class"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#load-the-iris-dataset","text":"df = pd . read_csv ( '../Data/iris.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0","title":"Load the iris dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#plot-the-dataset","text":"The iris dataset has 4 features. To get an idea how they correlate we can plot four different relationships among them. We\u2019ll use the index positions of the columns to grab their names in pairs with plots = [(0,1),(2,3),(0,2),(1,3)] . Here (0,1) sets \u201csepal length (cm)\u201d as x and \u201csepal width (cm)\u201d as y fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show ()","title":"Plot the dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#perform-traintestsplit","text":"X = df . drop ( 'target' , axis = 1 ) . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( X_train ) X_test = torch . FloatTensor ( X_test ) # y_train = F.one_hot(torch.LongTensor(y_train)) # not needed with Cross Entropy Loss # y_test = F.one_hot(torch.LongTensor(y_test)) y_train = torch . LongTensor ( y_train ) y_test = torch . LongTensor ( y_test )","title":"Perform Train/Test/Split"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#prepare-dataloader","text":"For this analysis we don\u2019t need to create a Dataset object, but we should take advantage of PyTorch\u2019s DataLoader tool. Even though our dataset is small (120 training samples), we\u2019ll load it into our model in two batches. This technique becomes very helpful with large datasets. Note that scikit-learn already shuffled the source dataset before preparing train and test sets. We\u2019ll still benefit from the DataLoader shuffle utility for model training if we make multiple passes throught the dataset. trainloader = DataLoader ( X_train , batch_size = 60 , shuffle = True ) testloader = DataLoader ( X_test , batch_size = 60 , shuffle = False )","title":"Prepare DataLoader"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#define-loss-equations-and-optimizations","text":"As before, we\u2019ll utilize Cross Entropy with torch.nn.CrossEntropyLoss() For the optimizer, we\u2019ll use a variation of Stochastic Gradient Descent called Adam (short for Adaptive Moment Estimation), with torch.optim.Adam() # FOR REDO torch . manual_seed ( 4 ) model = Model () criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 )","title":"Define loss equations and optimizations"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#train-the-model","text":"epochs = 100 losses = [] for i in range ( epochs ): i += 1 y_pred = model . forward ( X_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 10 == 1 : print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () epoch: 1 loss: 1.09568226 epoch: 11 loss: 0.98190653 epoch: 21 loss: 0.75652307 epoch: 31 loss: 0.49447522 epoch: 41 loss: 0.34981874 epoch: 51 loss: 0.22807853 epoch: 61 loss: 0.13547322 epoch: 71 loss: 0.09162075 epoch: 81 loss: 0.07378192 epoch: 91 loss: 0.06546164","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#plot-the-loss-function","text":"plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'epoch' );","title":"Plot the loss function"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#validate-the-model","text":"Now we run the test set through the model to see if the loss calculation resembles the training data. # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( f ' { loss : .8f } ' ) 0.06246195 correct = 0 with torch . no_grad (): for i , data in enumerate ( X_test ): y_val = model . forward ( data ) print ( f ' { i + 1 : 2 } . { str ( y_val ) : 38 } { y_test [ i ] } ' ) if y_val . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { len ( y_test ) } = { 100 * correct / len ( y_test ) : .2f } % correct' ) 1. tensor([-0.3355, 7.3630, 1.3783]) 1 2. tensor([0.2775, 8.1554, 0.4269]) 1 3. tensor([ 11.9969, 6.1847, -19.1976]) 0 4. tensor([-2.0187, 7.9664, 4.2447]) 1 5. tensor([-6.1348, 7.9516, 11.0913]) 2 6. tensor([-10.2635, 8.3101, 17.9998]) 2 7. tensor([ 12.0542, 6.4321, -19.2909]) 0 8. tensor([ 12.9507, 6.4819, -20.7540]) 0 9. tensor([-5.7723, 8.2435, 10.5083]) 2 10. tensor([-7.8867, 8.6126, 14.0731]) 2 11. tensor([-8.7055, 8.6074, 15.4337]) 2 12. tensor([ 11.6358, 5.8167, -18.6220]) 0 13. tensor([-8.1009, 8.2331, 14.3888]) 2 14. tensor([-2.0791, 7.7752, 4.3188]) 1 15. tensor([-6.0828, 8.3916, 11.0586]) 2 16. tensor([0.1360, 7.8660, 0.6409]) 1 17. tensor([-4.0875, 7.7217, 7.6642]) 2 18. tensor([ 13.1522, 6.5911, -21.0798]) 0 19. tensor([-1.5644, 8.0222, 3.4754]) 1 20. tensor([-6.2859, 8.9728, 11.4248]) 2 21. tensor([ 12.3859, 6.2571, -19.8275]) 0 22. tensor([ 13.8200, 7.0859, -22.1528]) 0 23. tensor([-8.8470, 8.3180, 15.6476]) 2 24. tensor([ 12.1979, 6.1264, -19.5260]) 0 25. tensor([-5.8084, 7.5468, 10.5340]) 2 26. tensor([-4.4526, 7.7876, 8.2865]) 2 27. tensor([-1.4284, 7.7786, 3.2328]) 1 28. tensor([ 0.5356, 7.5360, -0.0492]) 1 29. tensor([-5.8230, 8.1573, 10.5975]) 2 30. tensor([-5.2569, 7.7476, 9.6105]) 2 29 out of 30 = 96.67% correct Here we can see that #17 was misclassified.","title":"Validate the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#save-the-trained-model-to-a-file","text":"Right now model has been trained and validated, and seems to correctly classify an iris 97% of the time. Let\u2019s save this to disk. The tools we\u2019ll use are torch.save() and torch.load() There are two basic ways to save a model. The first saves/loads the state_dict (learned parameters) of the model, but not the model class. The syntax follows: Save: torch.save(model.state_dict(), PATH) Load: model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.eval() The second saves the entire model including its class and parameters as a pickle file. Care must be taken if you want to load this into another notebook to make sure all the target data is brought in properly. Save: torch.save(model, PATH) Load: model = torch.load(PATH)) model.eval() In either method, you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results. For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html","title":"Save the trained model to a file"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#save-the-model","text":"torch . save ( model . state_dict (), 'IrisDatasetModel.pt' )","title":"Save the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#load-a-new-model","text":"We\u2019ll load a new model object and test it as we had before to make sure it worked. new_model = Model () new_model . load_state_dict ( torch . load ( 'IrisDatasetModel.pt' )) new_model . eval () Model( (fc1): Linear(in_features=4, out_features=8, bias=True) (fc2): Linear(in_features=8, out_features=9, bias=True) (out): Linear(in_features=9, out_features=3, bias=True) ) with torch . no_grad (): y_val = new_model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( f ' { loss : .8f } ' ) 0.06246195","title":"Load a new model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#apply-the-model-to-classify-new-unseen-data","text":"mystery_iris = torch . tensor ([ 5.6 , 3.7 , 2.2 , 0.5 ]) Let\u2019s plot this new iris in yellow to see where it falls in relation to the others: fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' , 'Mystery iris' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) # Add a plot for our mystery iris: ax . scatter ( mystery_iris [ plots [ i ][ 0 ]], mystery_iris [ plots [ i ][ 1 ]], color = 'y' ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () Now run it through the model: with torch . no_grad (): print ( new_model ( mystery_iris )) print () print ( labels [ new_model ( mystery_iris ) . argmax ()]) tensor([ 12.2116, 7.1285, -19.5247]) Iris setosa","title":"Apply the model to classify new, unseen data"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/","text":"================ by Jawad Haider 04 - Full Artificial Neural Network Code Along \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Full Artificial Neural Network Code Along Working with tabular data Perform standard imports Load the NYC Taxi Fares dataset Calculate the distance traveled Add a datetime column and derive useful statistics Separate categorical from continuous columns Categorify Convert numpy arrays to tensors Set an embedding size Define a TabularModel Define loss function & optimizer Perform train/test splits Train the model Plot the loss function Validate the model Save the model Loading a saved model (starting from scratch) Feed new data through the trained model Great job! Full Artificial Neural Network Code Along \u00b6 In the last section we took in four continuous variables (lengths) to perform a classification. In this section we\u2019ll combine continuous and categorical data to perform a regression. The goal is to estimate the cost of a New York City cab ride from several inputs. The inspiration behind this code along is a recent Kaggle competition . NOTE: In this notebook we\u2019ll perform a regression with one output value. In the next one we\u2019ll perform a binary classification with two output values. Working with tabular data \u00b6 Deep learning with neural networks is often associated with sophisticated image recognition, and in upcoming sections we\u2019ll train models based on properties like pixels patterns and colors. Here we\u2019re working with tabular data (spreadsheets, SQL tables, etc.) with columns of values that may or may not be relevant. As it happens, neural networks can learn to make connections we probably wouldn\u2019t have developed on our own. However, to do this we have to handle categorical values separately from continuous ones. Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * continuous vs. categorical values * embeddings * batch normalization * dropout layers Perform standard imports \u00b6 import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Load the NYC Taxi Fares dataset \u00b6 The Kaggle competition provides a dataset with about 55 million records. The data contains only the pickup date & time, the latitude & longitude (GPS coordinates) of the pickup and dropoff locations, and the number of passengers. It is up to the contest participant to extract any further information. For instance, does the time of day matter? The day of the week? How do we determine the distance traveled from pairs of GPS coordinates? For this exercise we\u2019ve whittled the dataset down to just 120,000 records from April 11 to April 24, 2010. The records are randomly sorted. We\u2019ll show how to calculate distance from GPS coordinates, and how to create a pandas datatime object from a text column. This will let us quickly get information like day of the week, am vs. pm, etc. Let\u2019s get started! df = pd . read_csv ( '../Data/NYCTaxiFares.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 df [ 'fare_amount' ] . describe () count 120000.000000 mean 10.040326 std 7.500134 min 2.500000 25% 5.700000 50% 7.700000 75% 11.300000 max 49.900000 Name: fare_amount, dtype: float64 From this we see that fares range from \\$2.50 to \\$49.90, with a mean of \\$10.04 and a median of \\$7.70 Calculate the distance traveled \u00b6 The haversine formula calculates the distance on a sphere between two sets of GPS coordinates. Here we assign latitude values with (phi) and longitude with (lambda). The distance formula works out to where def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): \"\"\" Calculates the haversine distance between 2 sets of GPS coordinates in df \"\"\" r = 6371 # average radius of Earth in kilometers phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) d = ( r * c ) # in kilometers return d df [ 'dist_km' ] = haversine_distance ( df , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 Add a datetime column and derive useful statistics \u00b6 By creating a datetime object, we can extract information like \u201cday of the week\u201d, \u201cam vs. pm\u201d etc. Note that the data was saved in UTC time. Our data falls in April of 2010 which occurred during Daylight Savings Time in New York. For that reason, we\u2019ll make an adjustment to EDT using UTC-4 (subtracting four hours). df [ 'EDTdate' ] = pd . to_datetime ( df [ 'pickup_datetime' ] . str [: 19 ]) - pd . Timedelta ( hours = 4 ) df [ 'Hour' ] = df [ 'EDTdate' ] . dt . hour df [ 'AMorPM' ] = np . where ( df [ 'Hour' ] < 12 , 'am' , 'pm' ) df [ 'Weekday' ] = df [ 'EDTdate' ] . dt . strftime ( \" %a \" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km EDTdate Hour AMorPM Weekday 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 2010-04-19 04:17:56 4 am Mon 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2010-04-17 11:43:53 11 am Sat 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 2010-04-17 07:23:26 7 am Sat 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 2010-04-11 17:25:03 17 pm Sun 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 2010-04-16 22:19:01 22 pm Fri df [ 'EDTdate' ] . min () Timestamp('2010-04-11 00:00:10') df [ 'EDTdate' ] . max () Timestamp('2010-04-24 23:59:42') Separate categorical from continuous columns \u00b6 df . columns Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km', 'EDTdate', 'Hour', 'AMorPM', 'Weekday'], dtype='object') cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] y_col = [ 'fare_amount' ] # this column contains the labels NOTE: If you plan to use all of the columns in the data table, there\u2019s a shortcut to grab the remaining continuous columns: cont_cols = [col for col in df.columns if col not in cat_cols + y_col] Here we entered the continuous columns explicitly because there are columns we\u2019re not running through the model (pickup_datetime and EDTdate) Categorify \u00b6 Pandas offers a category dtype for converting categorical values to numerical codes. A dataset containing months of the year will be assigned 12 codes, one for each month. These will usually be the integers 0 to 11. Pandas replaces the column values with codes, and retains an index list of category values. In the steps ahead we\u2019ll call the categorical values \u201cnames\u201d and the encodings \u201ccodes\u201d. # Convert our three categorical columns to category dtypes. for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . dtypes pickup_datetime object fare_amount float64 fare_class int64 pickup_longitude float64 pickup_latitude float64 dropoff_longitude float64 dropoff_latitude float64 passenger_count int64 dist_km float64 EDTdate datetime64[ns] Hour category AMorPM category Weekday category dtype: object We can see that df[\u2018Hour\u2019] is a categorical feature by displaying some of the rows: df [ 'Hour' ] . head () 0 4 1 11 2 7 3 17 4 22 Name: Hour, dtype: category Categories (24, int64): [0, 1, 2, 3, ..., 20, 21, 22, 23] Here our categorical names are the integers 0 through 23, for a total of 24 unique categories. These values also correspond to the codes assigned to each name. We can access the category names with Series.cat.categories or just the codes with Series.cat.codes . This will make more sense if we look at df[\u2018AMorPM\u2019] : df [ 'AMorPM' ] . head () 0 am 1 am 2 am 3 pm 4 pm Name: AMorPM, dtype: category Categories (2, object): [am, pm] df [ 'AMorPM' ] . cat . categories Index(['am', 'pm'], dtype='object') df [ 'AMorPM' ] . head () . cat . codes 0 0 1 0 2 0 3 1 4 1 dtype: int8 df [ 'Weekday' ] . cat . categories Index(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype='object') df [ 'Weekday' ] . head () . cat . codes 0 1 1 2 2 2 3 3 4 0 dtype: int8 NOTE: NaN values in categorical data are assigned a code of -1. We don\u2019t have any in this particular dataset. Now we want to combine the three categorical columns into one input array using numpy.stack We don\u2019t want the Series index, just the values. hr = df [ 'Hour' ] . cat . codes . values ampm = df [ 'AMorPM' ] . cat . codes . values wkdy = df [ 'Weekday' ] . cat . codes . values cats = np . stack ([ hr , ampm , wkdy ], 1 ) cats [: 5 ] array([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]], dtype=int8) NOTE: This can be done in one line of code using a list comprehension: cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1) Don\u2019t worry about the dtype for now, we can make it int64 when we convert it to a tensor. Convert numpy arrays to tensors \u00b6 # Convert categorical variables to a tensor cats = torch . tensor ( cats , dtype = torch . int64 ) # this syntax is ok, since the source data is an array, not an existing tensor cats [: 5 ] tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]]) We can feed all of our continuous variables into the model as a tensor. Note that we\u2019re not normalizing the values here; we\u2019ll let the model perform this step. NOTE: We have to store conts and y as Float (float32) tensors, not Double (float64) in order for batch normalization to work properly. # Convert continuous variables to a tensor conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts = torch . tensor ( conts , dtype = torch . float ) conts [: 5 ] tensor([[ 40.7305, -73.9924, 40.7447, -73.9755, 1.0000, 2.1263], [ 40.7406, -73.9901, 40.7441, -73.9742, 1.0000, 1.3923], [ 40.7511, -73.9941, 40.7662, -73.9601, 2.0000, 3.3268], [ 40.7564, -73.9905, 40.7482, -73.9712, 1.0000, 1.8641], [ 40.7342, -73.9910, 40.7431, -73.9060, 1.0000, 7.2313]]) conts . type () 'torch.FloatTensor' # Convert labels to a tensor y = torch . tensor ( df [ y_col ] . values , dtype = torch . float ) . reshape ( - 1 , 1 ) y [: 5 ] tensor([[ 6.5000], [ 6.9000], [10.1000], [ 8.9000], [19.7000]]) cats . shape torch.Size([120000, 3]) conts . shape torch.Size([120000, 6]) y . shape torch.Size([120000, 1]) Set an embedding size \u00b6 The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50. # This will set embedding sizes for Hours, AMvsPM and Weekdays cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(24, 12), (2, 1), (7, 4)] Define a TabularModel \u00b6 This somewhat follows the fast.ai library The goal is to define a model based on the number of continuous columns (given by conts.shape[1] ) plus the number of categorical columns and their embeddings (given by len(emb_szs) and emb_szs respectively). The output would either be a regression (a single float value), or a classification (a group of bins and their softmax values). For this exercise our output will be a single regression value. Note that we\u2019ll assume our data contains both categorical and continuous data. You can add boolean parameters to your own model class to handle a variety of datasets. Let\u2019s walk through the steps we\u2019re about to take. See below for more detailed illustrations of the steps. 1. Extend the base Module class, set up the following parameters: - emb_szs: list of tuples: each categorical variable size is paired with an embedding size - n_cont: int: number of continuous variables - out_sz: int: output size - layers: list of ints: layer sizes - p: float: dropout probability for each layer (for simplicity we\u2019ll use the same value throughout) class TabularModel(nn.Module): def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5): super().\\_\\_init\\_\\_() 2. Set up the embedded layers with torch.nn.ModuleList() and torch.nn.Embedding() Categorical data will be filtered through these Embeddings in the forward section. self.embeds = nn.ModuleList(\\[nn.Embedding(ni, nf) for ni,nf in emb_szs\\]) 3. Set up a dropout function for the embeddings with torch.nn.Dropout() The default p-value=0.5 self.emb_drop = nn.Dropout(emb_drop) 4. Set up a normalization function for the continuous variables with torch.nn.BatchNorm1d() self.bn_cont = nn.BatchNorm1d(n_cont) 5. Set up a sequence of neural network layers where each level includes a Linear function, an activation function (we\u2019ll use ReLU ), a normalization step, and a dropout layer. We\u2019ll combine the list of layers with torch.nn.Sequential() self.bn_cont = nn.BatchNorm1d(n_cont) layerlist = \\[\\] n_emb = sum((nf for ni,nf in emb_szs)) n_in = n_emb + n_cont for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) layerlist.append(nn.BatchNorm1d(i)) layerlist.append(nn.Dropout(p)) n_in = i layerlist.append(nn.Linear(layers\\[-1\\],out_sz)) self.layers = nn.Sequential(\\*layerlist) 6. Define the forward method. Preprocess the embeddings and normalize the continuous variables before passing them through the layers. Use torch.cat() to combine multiple tensors into one. def forward(self, x_cat, x_cont): embeddings = \\[\\] for i,e in enumerate(self.embeds): embeddings.append(e(x_cat\\[:,i\\])) x = torch.cat(embeddings, 1) x = self.emb_drop(x) x_cont = self.bn_cont(x_cont) x = torch.cat(\\[x, x_cont\\], 1) x = self.layers(x) return x Breaking down the embeddings steps (this code is for illustration purposes only.) # This is our source data catz = cats [: 4 ] catz tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3]]) # This is passed in when the model is instantiated emb_szs [(24, 12), (2, 1), (7, 4)] # This is assigned inside the __init__() method selfembeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) selfembeds ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) list ( enumerate ( selfembeds )) [(0, Embedding(24, 12)), (1, Embedding(2, 1)), (2, Embedding(7, 4))] # This happens inside the forward() method embeddingz = [] for i , e in enumerate ( selfembeds ): embeddingz . append ( e ( catz [:, i ])) embeddingz [tensor([[ 0.7031, 0.5132, -1.0168, 1.3594, 0.7909, -1.2648, 0.9565, 0.6827, 0.4922, 0.1282, -0.9863, 0.2622], [ 0.0402, -0.8209, 2.5186, 0.5037, 0.0925, 0.0058, 1.0398, 1.7655, 0.0216, -0.1349, -1.0968, 1.4534], [ 0.5162, -0.6815, 1.4828, 0.1271, 1.1672, 0.8569, 1.1472, 0.7672, 0.4720, 1.3629, 1.2446, 0.3470], [ 0.1047, 0.2153, -1.4652, -1.6907, -0.0670, 0.9863, 0.7836, -1.3762, 1.6670, 0.1276, 1.2241, 0.1415]], grad_fn=<EmbeddingBackward>), tensor([[-1.1156], [-1.1156], [-1.1156], [-0.7577]], grad_fn=<EmbeddingBackward>), tensor([[-0.8579, -0.4516, 0.3814, 0.9935], [ 0.6264, -1.1347, 1.9039, 1.1867], [ 0.6264, -1.1347, 1.9039, 1.1867], [ 0.1609, -0.1231, 0.5787, -0.3180]], grad_fn=<EmbeddingBackward>)] # We concatenate the embedding sections (12,1,4) into one (17) z = torch . cat ( embeddingz , 1 ) z tensor([[ 0.7031, 0.5132, -1.0168, 1.3594, 0.7909, -1.2648, 0.9565, 0.6827, 0.4922, 0.1282, -0.9863, 0.2622, -1.1156, -0.8579, -0.4516, 0.3814, 0.9935], [ 0.0402, -0.8209, 2.5186, 0.5037, 0.0925, 0.0058, 1.0398, 1.7655, 0.0216, -0.1349, -1.0968, 1.4534, -1.1156, 0.6264, -1.1347, 1.9039, 1.1867], [ 0.5162, -0.6815, 1.4828, 0.1271, 1.1672, 0.8569, 1.1472, 0.7672, 0.4720, 1.3629, 1.2446, 0.3470, -1.1156, 0.6264, -1.1347, 1.9039, 1.1867], [ 0.1047, 0.2153, -1.4652, -1.6907, -0.0670, 0.9863, 0.7836, -1.3762, 1.6670, 0.1276, 1.2241, 0.1415, -0.7577, 0.1609, -0.1231, 0.5787, -0.3180]], grad_fn=<CatBackward>) # This was assigned under the __init__() method selfembdrop = nn . Dropout ( .4 ) z = selfembdrop ( z ) z tensor([[ 0.0000, 0.8553, -1.6946, 0.0000, 1.3182, -2.1080, 1.5942, 1.1378, 0.8204, 0.2137, -1.6438, 0.0000, -0.0000, -1.4298, -0.0000, 0.6357, 0.0000], [ 0.0000, -1.3682, 0.0000, 0.8395, 0.0000, 0.0000, 0.0000, 2.9425, 0.0359, -0.0000, -1.8280, 2.4223, -0.0000, 0.0000, -1.8912, 3.1731, 0.0000], [ 0.0000, -1.1359, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2786, 0.7867, 2.2715, 0.0000, 0.5783, -0.0000, 1.0440, -1.8912, 3.1731, 0.0000], [ 0.0000, 0.3589, -2.4420, -0.0000, -0.1117, 1.6438, 1.3059, -0.0000, 2.7783, 0.2127, 2.0402, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, -0.5300]], grad_fn=<MulBackward0>) This is how the categorical embeddings are passed into the layers. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) x = self . layers ( x ) return x torch . manual_seed ( 33 ) model = TabularModel ( emb_szs , conts . shape [ 1 ], 1 , [ 200 , 100 ], p = 0.4 ) model TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=1, bias=True) ) ) Define loss function & optimizer \u00b6 PyTorch does not offer a built-in RMSE Loss function, and it would be nice to see this in place of MSE. For this reason, we\u2019ll simply apply the torch.sqrt() function to the output of MSELoss during training. criterion = nn . MSELoss () # we'll convert this to RMSE later optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) Perform train/test splits \u00b6 At this point our batch size is the entire dataset of 120,000 records. This will take a long time to train, so you might consider reducing this. We\u2019ll use 60,000. Recall that our tensors are already randomly shuffled. batch_size = 60000 test_size = int ( batch_size * .2 ) cat_train = cats [: batch_size - test_size ] cat_test = cats [ batch_size - test_size : batch_size ] con_train = conts [: batch_size - test_size ] con_test = conts [ batch_size - test_size : batch_size ] y_train = y [: batch_size - test_size ] y_test = y [ batch_size - test_size : batch_size ] len ( cat_train ) 48000 len ( cat_test ) 12000 Train the model \u00b6 Expect this to take 30 minutes or more! We\u2019ve added code to tell us the duration at the end. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = torch . sqrt ( criterion ( y_pred , y_train )) # RMSE losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 12.49953079 epoch: 26 loss: 11.52666759 epoch: 51 loss: 10.47162533 epoch: 76 loss: 9.53090382 epoch: 101 loss: 8.72838020 epoch: 126 loss: 7.81538534 epoch: 151 loss: 6.70782852 epoch: 176 loss: 5.48520994 epoch: 201 loss: 4.37029028 epoch: 226 loss: 3.70538783 epoch: 251 loss: 3.51656818 epoch: 276 loss: 3.44707990 epoch: 300 loss: 3.42467499 Duration: 730 seconds Plot the loss function \u00b6 plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'RMSE Loss' ) plt . xlabel ( 'epoch' ); Validate the model \u00b6 Here we want to run the entire test set through the model, and compare it to the known labels. For this step we don\u2019t want to update weights and biases, so we set torch.no_grad() # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = torch . sqrt ( criterion ( y_val , y_test )) print ( f 'RMSE: { loss : .8f } ' ) RMSE: 3.34590030 This means that on average, predicted values are within \u00b1$3.31 of the actual value. Now let\u2019s look at the first 50 predicted values: print ( f ' { \"PREDICTED\" : >12 } { \"ACTUAL\" : >8 } { \"DIFF\" : >8 } ' ) for i in range ( 50 ): diff = np . abs ( y_val [ i ] . item () - y_test [ i ] . item ()) print ( f ' { i + 1 : 2 } . { y_val [ i ] . item () : 8.4f } { y_test [ i ] . item () : 8.4f } { diff : 8.4f } ' ) PREDICTED ACTUAL DIFF 1. 2.5379 2.9000 0.3621 2. 25.1634 5.7000 19.4634 3. 6.3749 7.7000 1.3251 4. 13.4677 12.5000 0.9677 5. 4.4992 4.1000 0.3992 6. 4.8968 5.3000 0.4032 7. 3.1796 3.7000 0.5204 8. 17.7814 14.5000 3.2814 9. 6.1348 5.7000 0.4348 10. 12.0325 10.1000 1.9325 11. 6.1323 4.5000 1.6323 12. 6.9208 6.1000 0.8208 13. 5.9448 6.9000 0.9552 14. 13.4625 14.1000 0.6375 15. 5.9277 4.5000 1.4277 16. 27.5778 34.1000 6.5222 17. 3.2774 12.5000 9.2226 18. 5.7506 4.1000 1.6506 19. 8.1940 8.5000 0.3060 20. 6.2858 5.3000 0.9858 21. 13.6693 11.3000 2.3693 22. 9.6759 10.5000 0.8241 23. 16.0568 15.3000 0.7568 24. 19.3310 14.9000 4.4310 25. 48.6873 49.5700 0.8827 26. 6.3257 5.3000 1.0257 27. 6.0392 3.7000 2.3392 28. 7.1921 6.5000 0.6921 29. 14.9567 14.1000 0.8567 30. 6.7476 4.9000 1.8476 31. 4.3447 3.7000 0.6447 32. 35.6969 38.6700 2.9731 33. 13.9892 12.5000 1.4892 34. 12.8934 16.5000 3.6066 35. 6.3164 5.7000 0.6164 36. 5.9684 8.9000 2.9316 37. 16.1289 22.1000 5.9711 38. 7.6541 12.1000 4.4459 39. 8.6153 10.1000 1.4847 40. 4.0447 3.3000 0.7447 41. 10.2168 8.5000 1.7168 42. 8.8325 8.1000 0.7325 43. 15.2500 14.5000 0.7500 44. 6.3571 4.9000 1.4571 45. 9.7002 8.5000 1.2002 46. 12.1134 12.1000 0.0134 47. 24.3001 23.7000 0.6001 48. 2.8357 3.7000 0.8643 49. 6.8266 9.3000 2.4734 50. 8.2082 8.1000 0.1082 So while many predictions were off by a few cents, some were off by \\$19.00. Feel free to change the batch size, test size, and number of epochs to obtain a better model. Save the model \u00b6 We can save a trained model to a file in case we want to come back later and feed new data through it. The best practice is to save the state of the model (weights & biases) and not the full definition. Also, we want to ensure that only a trained model is saved, to prevent overwriting a previously saved model with an untrained one. For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html # Make sure to save the model only after the training has happened! if len ( losses ) == epochs : torch . save ( model . state_dict (), 'TaxiFareRegrModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' ) Loading a saved model (starting from scratch) \u00b6 We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. To demonstrate, restart the kernel before proceeding. import torch import torch.nn as nn import numpy as np import pandas as pd def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): r = 6371 phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) return r * c class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) return self . layers ( x ) Now define the model. Before we can load the saved settings, we need to instantiate our TabularModel with the parameters we used before (embedding sizes, number of continuous columns, output size, layer sizes, and dropout layer p-value). emb_szs = [( 24 , 12 ), ( 2 , 1 ), ( 7 , 4 )] model2 = TabularModel ( emb_szs , 6 , 1 , [ 200 , 100 ], p = 0.4 ) Once the model is set up, loading the saved settings is a snap. model2 . load_state_dict ( torch . load ( 'TaxiFareRegrModel.pt' )); model2 . eval () # be sure to run this step! TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=1, bias=True) ) ) Next we\u2019ll define a function that takes in new parameters from the user, performs all of the preprocessing steps above, and passes the new data through our trained model. def test_data ( mdl ): # pass in the name of the new model # INPUT NEW DATA plat = float ( input ( 'What is the pickup latitude? ' )) plong = float ( input ( 'What is the pickup longitude? ' )) dlat = float ( input ( 'What is the dropoff latitude? ' )) dlong = float ( input ( 'What is the dropoff longitude? ' )) psngr = int ( input ( 'How many passengers? ' )) dt = input ( 'What is the pickup date and time? \\n Format as YYYY-MM-DD HH:MM:SS ' ) # PREPROCESS THE DATA dfx_dict = { 'pickup_latitude' : plat , 'pickup_longitude' : plong , 'dropoff_latitude' : dlat , 'dropoff_longitude' : dlong , 'passenger_count' : psngr , 'EDTdate' : dt } dfx = pd . DataFrame ( dfx_dict , index = [ 0 ]) dfx [ 'dist_km' ] = haversine_distance ( dfx , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) dfx [ 'EDTdate' ] = pd . to_datetime ( dfx [ 'EDTdate' ]) # We can skip the .astype(category) step since our fields are small, # and encode them right away dfx [ 'Hour' ] = dfx [ 'EDTdate' ] . dt . hour dfx [ 'AMorPM' ] = np . where ( dfx [ 'Hour' ] < 12 , 0 , 1 ) dfx [ 'Weekday' ] = dfx [ 'EDTdate' ] . dt . strftime ( \" %a \" ) dfx [ 'Weekday' ] = dfx [ 'Weekday' ] . replace ([ 'Fri' , 'Mon' , 'Sat' , 'Sun' , 'Thu' , 'Tue' , 'Wed' ], [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ]) . astype ( 'int64' ) # CREATE CAT AND CONT TENSORS cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] xcats = np . stack ([ dfx [ col ] . values for col in cat_cols ], 1 ) xcats = torch . tensor ( xcats , dtype = torch . int64 ) xconts = np . stack ([ dfx [ col ] . values for col in cont_cols ], 1 ) xconts = torch . tensor ( xconts , dtype = torch . float ) # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( xcats , xconts ) print ( f ' \\n The predicted fare amount is $ { z . item () : .2f } ' ) Feed new data through the trained model \u00b6 For convenience, here are the max and min values for each of the variables: Column Minimum Maximum pickup_latitude 40 41 pickup_longitude -74.5 -73.3 dropoff_latitude 40 41 dropoff_longitude -74.5 -73.3 passenger_count 1 5 EDTdate 2010-04-11 00:00:00 2010-04-24 23:59:42 Use caution! The distance between 1 degree of latitude (from 40 to 41) is 111km (69mi) and between 1 degree of longitude (from -73 to -74) is 85km (53mi). The longest cab ride in the dataset spanned a difference of only 0.243 degrees latitude and 0.284 degrees longitude. The mean difference for both latitude and longitude was about 0.02. To get a fair prediction, use values that fall close to one another. z = test_data ( model2 ) What is the pickup latitude? 40.5 What is the pickup longitude? -73.9 What is the dropoff latitude? 40.52 What is the dropoff longitude? -73.92 How many passengers? 2 What is the pickup date and time? Format as YYYY-MM-DD HH:MM:SS 2010-04-15 16:00:00 The predicted fare amount is $13.86 ## Great job!","title":"04a Full ANN Code Along Regression"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#04-full-artificial-neural-network-code-along","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Full Artificial Neural Network Code Along Working with tabular data Perform standard imports Load the NYC Taxi Fares dataset Calculate the distance traveled Add a datetime column and derive useful statistics Separate categorical from continuous columns Categorify Convert numpy arrays to tensors Set an embedding size Define a TabularModel Define loss function & optimizer Perform train/test splits Train the model Plot the loss function Validate the model Save the model Loading a saved model (starting from scratch) Feed new data through the trained model Great job!","title":"04 - Full Artificial Neural Network Code Along"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#full-artificial-neural-network-code-along","text":"In the last section we took in four continuous variables (lengths) to perform a classification. In this section we\u2019ll combine continuous and categorical data to perform a regression. The goal is to estimate the cost of a New York City cab ride from several inputs. The inspiration behind this code along is a recent Kaggle competition . NOTE: In this notebook we\u2019ll perform a regression with one output value. In the next one we\u2019ll perform a binary classification with two output values.","title":"Full Artificial Neural Network Code Along"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#working-with-tabular-data","text":"Deep learning with neural networks is often associated with sophisticated image recognition, and in upcoming sections we\u2019ll train models based on properties like pixels patterns and colors. Here we\u2019re working with tabular data (spreadsheets, SQL tables, etc.) with columns of values that may or may not be relevant. As it happens, neural networks can learn to make connections we probably wouldn\u2019t have developed on our own. However, to do this we have to handle categorical values separately from continuous ones. Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * continuous vs. categorical values * embeddings * batch normalization * dropout layers","title":"Working with tabular data"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#perform-standard-imports","text":"import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#load-the-nyc-taxi-fares-dataset","text":"The Kaggle competition provides a dataset with about 55 million records. The data contains only the pickup date & time, the latitude & longitude (GPS coordinates) of the pickup and dropoff locations, and the number of passengers. It is up to the contest participant to extract any further information. For instance, does the time of day matter? The day of the week? How do we determine the distance traveled from pairs of GPS coordinates? For this exercise we\u2019ve whittled the dataset down to just 120,000 records from April 11 to April 24, 2010. The records are randomly sorted. We\u2019ll show how to calculate distance from GPS coordinates, and how to create a pandas datatime object from a text column. This will let us quickly get information like day of the week, am vs. pm, etc. Let\u2019s get started! df = pd . read_csv ( '../Data/NYCTaxiFares.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 df [ 'fare_amount' ] . describe () count 120000.000000 mean 10.040326 std 7.500134 min 2.500000 25% 5.700000 50% 7.700000 75% 11.300000 max 49.900000 Name: fare_amount, dtype: float64 From this we see that fares range from \\$2.50 to \\$49.90, with a mean of \\$10.04 and a median of \\$7.70","title":"Load the NYC Taxi Fares dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#calculate-the-distance-traveled","text":"The haversine formula calculates the distance on a sphere between two sets of GPS coordinates. Here we assign latitude values with (phi) and longitude with (lambda). The distance formula works out to where def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): \"\"\" Calculates the haversine distance between 2 sets of GPS coordinates in df \"\"\" r = 6371 # average radius of Earth in kilometers phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) d = ( r * c ) # in kilometers return d df [ 'dist_km' ] = haversine_distance ( df , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321","title":"Calculate the distance traveled"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#add-a-datetime-column-and-derive-useful-statistics","text":"By creating a datetime object, we can extract information like \u201cday of the week\u201d, \u201cam vs. pm\u201d etc. Note that the data was saved in UTC time. Our data falls in April of 2010 which occurred during Daylight Savings Time in New York. For that reason, we\u2019ll make an adjustment to EDT using UTC-4 (subtracting four hours). df [ 'EDTdate' ] = pd . to_datetime ( df [ 'pickup_datetime' ] . str [: 19 ]) - pd . Timedelta ( hours = 4 ) df [ 'Hour' ] = df [ 'EDTdate' ] . dt . hour df [ 'AMorPM' ] = np . where ( df [ 'Hour' ] < 12 , 'am' , 'pm' ) df [ 'Weekday' ] = df [ 'EDTdate' ] . dt . strftime ( \" %a \" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km EDTdate Hour AMorPM Weekday 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 2010-04-19 04:17:56 4 am Mon 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2010-04-17 11:43:53 11 am Sat 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 2010-04-17 07:23:26 7 am Sat 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 2010-04-11 17:25:03 17 pm Sun 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 2010-04-16 22:19:01 22 pm Fri df [ 'EDTdate' ] . min () Timestamp('2010-04-11 00:00:10') df [ 'EDTdate' ] . max () Timestamp('2010-04-24 23:59:42')","title":"Add a datetime column and derive useful statistics"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#separate-categorical-from-continuous-columns","text":"df . columns Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km', 'EDTdate', 'Hour', 'AMorPM', 'Weekday'], dtype='object') cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] y_col = [ 'fare_amount' ] # this column contains the labels NOTE: If you plan to use all of the columns in the data table, there\u2019s a shortcut to grab the remaining continuous columns: cont_cols = [col for col in df.columns if col not in cat_cols + y_col] Here we entered the continuous columns explicitly because there are columns we\u2019re not running through the model (pickup_datetime and EDTdate)","title":"Separate categorical from continuous columns"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#categorify","text":"Pandas offers a category dtype for converting categorical values to numerical codes. A dataset containing months of the year will be assigned 12 codes, one for each month. These will usually be the integers 0 to 11. Pandas replaces the column values with codes, and retains an index list of category values. In the steps ahead we\u2019ll call the categorical values \u201cnames\u201d and the encodings \u201ccodes\u201d. # Convert our three categorical columns to category dtypes. for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . dtypes pickup_datetime object fare_amount float64 fare_class int64 pickup_longitude float64 pickup_latitude float64 dropoff_longitude float64 dropoff_latitude float64 passenger_count int64 dist_km float64 EDTdate datetime64[ns] Hour category AMorPM category Weekday category dtype: object We can see that df[\u2018Hour\u2019] is a categorical feature by displaying some of the rows: df [ 'Hour' ] . head () 0 4 1 11 2 7 3 17 4 22 Name: Hour, dtype: category Categories (24, int64): [0, 1, 2, 3, ..., 20, 21, 22, 23] Here our categorical names are the integers 0 through 23, for a total of 24 unique categories. These values also correspond to the codes assigned to each name. We can access the category names with Series.cat.categories or just the codes with Series.cat.codes . This will make more sense if we look at df[\u2018AMorPM\u2019] : df [ 'AMorPM' ] . head () 0 am 1 am 2 am 3 pm 4 pm Name: AMorPM, dtype: category Categories (2, object): [am, pm] df [ 'AMorPM' ] . cat . categories Index(['am', 'pm'], dtype='object') df [ 'AMorPM' ] . head () . cat . codes 0 0 1 0 2 0 3 1 4 1 dtype: int8 df [ 'Weekday' ] . cat . categories Index(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype='object') df [ 'Weekday' ] . head () . cat . codes 0 1 1 2 2 2 3 3 4 0 dtype: int8 NOTE: NaN values in categorical data are assigned a code of -1. We don\u2019t have any in this particular dataset. Now we want to combine the three categorical columns into one input array using numpy.stack We don\u2019t want the Series index, just the values. hr = df [ 'Hour' ] . cat . codes . values ampm = df [ 'AMorPM' ] . cat . codes . values wkdy = df [ 'Weekday' ] . cat . codes . values cats = np . stack ([ hr , ampm , wkdy ], 1 ) cats [: 5 ] array([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]], dtype=int8) NOTE: This can be done in one line of code using a list comprehension: cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1) Don\u2019t worry about the dtype for now, we can make it int64 when we convert it to a tensor.","title":"Categorify"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#convert-numpy-arrays-to-tensors","text":"# Convert categorical variables to a tensor cats = torch . tensor ( cats , dtype = torch . int64 ) # this syntax is ok, since the source data is an array, not an existing tensor cats [: 5 ] tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]]) We can feed all of our continuous variables into the model as a tensor. Note that we\u2019re not normalizing the values here; we\u2019ll let the model perform this step. NOTE: We have to store conts and y as Float (float32) tensors, not Double (float64) in order for batch normalization to work properly. # Convert continuous variables to a tensor conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts = torch . tensor ( conts , dtype = torch . float ) conts [: 5 ] tensor([[ 40.7305, -73.9924, 40.7447, -73.9755, 1.0000, 2.1263], [ 40.7406, -73.9901, 40.7441, -73.9742, 1.0000, 1.3923], [ 40.7511, -73.9941, 40.7662, -73.9601, 2.0000, 3.3268], [ 40.7564, -73.9905, 40.7482, -73.9712, 1.0000, 1.8641], [ 40.7342, -73.9910, 40.7431, -73.9060, 1.0000, 7.2313]]) conts . type () 'torch.FloatTensor' # Convert labels to a tensor y = torch . tensor ( df [ y_col ] . values , dtype = torch . float ) . reshape ( - 1 , 1 ) y [: 5 ] tensor([[ 6.5000], [ 6.9000], [10.1000], [ 8.9000], [19.7000]]) cats . shape torch.Size([120000, 3]) conts . shape torch.Size([120000, 6]) y . shape torch.Size([120000, 1])","title":"Convert numpy arrays to tensors"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#set-an-embedding-size","text":"The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50. # This will set embedding sizes for Hours, AMvsPM and Weekdays cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(24, 12), (2, 1), (7, 4)]","title":"Set an embedding size"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#define-a-tabularmodel","text":"This somewhat follows the fast.ai library The goal is to define a model based on the number of continuous columns (given by conts.shape[1] ) plus the number of categorical columns and their embeddings (given by len(emb_szs) and emb_szs respectively). The output would either be a regression (a single float value), or a classification (a group of bins and their softmax values). For this exercise our output will be a single regression value. Note that we\u2019ll assume our data contains both categorical and continuous data. You can add boolean parameters to your own model class to handle a variety of datasets. Let\u2019s walk through the steps we\u2019re about to take. See below for more detailed illustrations of the steps. 1. Extend the base Module class, set up the following parameters: - emb_szs: list of tuples: each categorical variable size is paired with an embedding size - n_cont: int: number of continuous variables - out_sz: int: output size - layers: list of ints: layer sizes - p: float: dropout probability for each layer (for simplicity we\u2019ll use the same value throughout) class TabularModel(nn.Module): def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5): super().\\_\\_init\\_\\_() 2. Set up the embedded layers with torch.nn.ModuleList() and torch.nn.Embedding() Categorical data will be filtered through these Embeddings in the forward section. self.embeds = nn.ModuleList(\\[nn.Embedding(ni, nf) for ni,nf in emb_szs\\]) 3. Set up a dropout function for the embeddings with torch.nn.Dropout() The default p-value=0.5 self.emb_drop = nn.Dropout(emb_drop) 4. Set up a normalization function for the continuous variables with torch.nn.BatchNorm1d() self.bn_cont = nn.BatchNorm1d(n_cont) 5. Set up a sequence of neural network layers where each level includes a Linear function, an activation function (we\u2019ll use ReLU ), a normalization step, and a dropout layer. We\u2019ll combine the list of layers with torch.nn.Sequential() self.bn_cont = nn.BatchNorm1d(n_cont) layerlist = \\[\\] n_emb = sum((nf for ni,nf in emb_szs)) n_in = n_emb + n_cont for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) layerlist.append(nn.BatchNorm1d(i)) layerlist.append(nn.Dropout(p)) n_in = i layerlist.append(nn.Linear(layers\\[-1\\],out_sz)) self.layers = nn.Sequential(\\*layerlist) 6. Define the forward method. Preprocess the embeddings and normalize the continuous variables before passing them through the layers. Use torch.cat() to combine multiple tensors into one. def forward(self, x_cat, x_cont): embeddings = \\[\\] for i,e in enumerate(self.embeds): embeddings.append(e(x_cat\\[:,i\\])) x = torch.cat(embeddings, 1) x = self.emb_drop(x) x_cont = self.bn_cont(x_cont) x = torch.cat(\\[x, x_cont\\], 1) x = self.layers(x) return x Breaking down the embeddings steps (this code is for illustration purposes only.) # This is our source data catz = cats [: 4 ] catz tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3]]) # This is passed in when the model is instantiated emb_szs [(24, 12), (2, 1), (7, 4)] # This is assigned inside the __init__() method selfembeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) selfembeds ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) list ( enumerate ( selfembeds )) [(0, Embedding(24, 12)), (1, Embedding(2, 1)), (2, Embedding(7, 4))] # This happens inside the forward() method embeddingz = [] for i , e in enumerate ( selfembeds ): embeddingz . append ( e ( catz [:, i ])) embeddingz [tensor([[ 0.7031, 0.5132, -1.0168, 1.3594, 0.7909, -1.2648, 0.9565, 0.6827, 0.4922, 0.1282, -0.9863, 0.2622], [ 0.0402, -0.8209, 2.5186, 0.5037, 0.0925, 0.0058, 1.0398, 1.7655, 0.0216, -0.1349, -1.0968, 1.4534], [ 0.5162, -0.6815, 1.4828, 0.1271, 1.1672, 0.8569, 1.1472, 0.7672, 0.4720, 1.3629, 1.2446, 0.3470], [ 0.1047, 0.2153, -1.4652, -1.6907, -0.0670, 0.9863, 0.7836, -1.3762, 1.6670, 0.1276, 1.2241, 0.1415]], grad_fn=<EmbeddingBackward>), tensor([[-1.1156], [-1.1156], [-1.1156], [-0.7577]], grad_fn=<EmbeddingBackward>), tensor([[-0.8579, -0.4516, 0.3814, 0.9935], [ 0.6264, -1.1347, 1.9039, 1.1867], [ 0.6264, -1.1347, 1.9039, 1.1867], [ 0.1609, -0.1231, 0.5787, -0.3180]], grad_fn=<EmbeddingBackward>)] # We concatenate the embedding sections (12,1,4) into one (17) z = torch . cat ( embeddingz , 1 ) z tensor([[ 0.7031, 0.5132, -1.0168, 1.3594, 0.7909, -1.2648, 0.9565, 0.6827, 0.4922, 0.1282, -0.9863, 0.2622, -1.1156, -0.8579, -0.4516, 0.3814, 0.9935], [ 0.0402, -0.8209, 2.5186, 0.5037, 0.0925, 0.0058, 1.0398, 1.7655, 0.0216, -0.1349, -1.0968, 1.4534, -1.1156, 0.6264, -1.1347, 1.9039, 1.1867], [ 0.5162, -0.6815, 1.4828, 0.1271, 1.1672, 0.8569, 1.1472, 0.7672, 0.4720, 1.3629, 1.2446, 0.3470, -1.1156, 0.6264, -1.1347, 1.9039, 1.1867], [ 0.1047, 0.2153, -1.4652, -1.6907, -0.0670, 0.9863, 0.7836, -1.3762, 1.6670, 0.1276, 1.2241, 0.1415, -0.7577, 0.1609, -0.1231, 0.5787, -0.3180]], grad_fn=<CatBackward>) # This was assigned under the __init__() method selfembdrop = nn . Dropout ( .4 ) z = selfembdrop ( z ) z tensor([[ 0.0000, 0.8553, -1.6946, 0.0000, 1.3182, -2.1080, 1.5942, 1.1378, 0.8204, 0.2137, -1.6438, 0.0000, -0.0000, -1.4298, -0.0000, 0.6357, 0.0000], [ 0.0000, -1.3682, 0.0000, 0.8395, 0.0000, 0.0000, 0.0000, 2.9425, 0.0359, -0.0000, -1.8280, 2.4223, -0.0000, 0.0000, -1.8912, 3.1731, 0.0000], [ 0.0000, -1.1359, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2786, 0.7867, 2.2715, 0.0000, 0.5783, -0.0000, 1.0440, -1.8912, 3.1731, 0.0000], [ 0.0000, 0.3589, -2.4420, -0.0000, -0.1117, 1.6438, 1.3059, -0.0000, 2.7783, 0.2127, 2.0402, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, -0.5300]], grad_fn=<MulBackward0>) This is how the categorical embeddings are passed into the layers. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) x = self . layers ( x ) return x torch . manual_seed ( 33 ) model = TabularModel ( emb_szs , conts . shape [ 1 ], 1 , [ 200 , 100 ], p = 0.4 ) model TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=1, bias=True) ) )","title":"Define a TabularModel"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#define-loss-function-optimizer","text":"PyTorch does not offer a built-in RMSE Loss function, and it would be nice to see this in place of MSE. For this reason, we\u2019ll simply apply the torch.sqrt() function to the output of MSELoss during training. criterion = nn . MSELoss () # we'll convert this to RMSE later optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#perform-traintest-splits","text":"At this point our batch size is the entire dataset of 120,000 records. This will take a long time to train, so you might consider reducing this. We\u2019ll use 60,000. Recall that our tensors are already randomly shuffled. batch_size = 60000 test_size = int ( batch_size * .2 ) cat_train = cats [: batch_size - test_size ] cat_test = cats [ batch_size - test_size : batch_size ] con_train = conts [: batch_size - test_size ] con_test = conts [ batch_size - test_size : batch_size ] y_train = y [: batch_size - test_size ] y_test = y [ batch_size - test_size : batch_size ] len ( cat_train ) 48000 len ( cat_test ) 12000","title":"Perform train/test splits"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#train-the-model","text":"Expect this to take 30 minutes or more! We\u2019ve added code to tell us the duration at the end. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = torch . sqrt ( criterion ( y_pred , y_train )) # RMSE losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 12.49953079 epoch: 26 loss: 11.52666759 epoch: 51 loss: 10.47162533 epoch: 76 loss: 9.53090382 epoch: 101 loss: 8.72838020 epoch: 126 loss: 7.81538534 epoch: 151 loss: 6.70782852 epoch: 176 loss: 5.48520994 epoch: 201 loss: 4.37029028 epoch: 226 loss: 3.70538783 epoch: 251 loss: 3.51656818 epoch: 276 loss: 3.44707990 epoch: 300 loss: 3.42467499 Duration: 730 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#plot-the-loss-function","text":"plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'RMSE Loss' ) plt . xlabel ( 'epoch' );","title":"Plot the loss function"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#validate-the-model","text":"Here we want to run the entire test set through the model, and compare it to the known labels. For this step we don\u2019t want to update weights and biases, so we set torch.no_grad() # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = torch . sqrt ( criterion ( y_val , y_test )) print ( f 'RMSE: { loss : .8f } ' ) RMSE: 3.34590030 This means that on average, predicted values are within \u00b1$3.31 of the actual value. Now let\u2019s look at the first 50 predicted values: print ( f ' { \"PREDICTED\" : >12 } { \"ACTUAL\" : >8 } { \"DIFF\" : >8 } ' ) for i in range ( 50 ): diff = np . abs ( y_val [ i ] . item () - y_test [ i ] . item ()) print ( f ' { i + 1 : 2 } . { y_val [ i ] . item () : 8.4f } { y_test [ i ] . item () : 8.4f } { diff : 8.4f } ' ) PREDICTED ACTUAL DIFF 1. 2.5379 2.9000 0.3621 2. 25.1634 5.7000 19.4634 3. 6.3749 7.7000 1.3251 4. 13.4677 12.5000 0.9677 5. 4.4992 4.1000 0.3992 6. 4.8968 5.3000 0.4032 7. 3.1796 3.7000 0.5204 8. 17.7814 14.5000 3.2814 9. 6.1348 5.7000 0.4348 10. 12.0325 10.1000 1.9325 11. 6.1323 4.5000 1.6323 12. 6.9208 6.1000 0.8208 13. 5.9448 6.9000 0.9552 14. 13.4625 14.1000 0.6375 15. 5.9277 4.5000 1.4277 16. 27.5778 34.1000 6.5222 17. 3.2774 12.5000 9.2226 18. 5.7506 4.1000 1.6506 19. 8.1940 8.5000 0.3060 20. 6.2858 5.3000 0.9858 21. 13.6693 11.3000 2.3693 22. 9.6759 10.5000 0.8241 23. 16.0568 15.3000 0.7568 24. 19.3310 14.9000 4.4310 25. 48.6873 49.5700 0.8827 26. 6.3257 5.3000 1.0257 27. 6.0392 3.7000 2.3392 28. 7.1921 6.5000 0.6921 29. 14.9567 14.1000 0.8567 30. 6.7476 4.9000 1.8476 31. 4.3447 3.7000 0.6447 32. 35.6969 38.6700 2.9731 33. 13.9892 12.5000 1.4892 34. 12.8934 16.5000 3.6066 35. 6.3164 5.7000 0.6164 36. 5.9684 8.9000 2.9316 37. 16.1289 22.1000 5.9711 38. 7.6541 12.1000 4.4459 39. 8.6153 10.1000 1.4847 40. 4.0447 3.3000 0.7447 41. 10.2168 8.5000 1.7168 42. 8.8325 8.1000 0.7325 43. 15.2500 14.5000 0.7500 44. 6.3571 4.9000 1.4571 45. 9.7002 8.5000 1.2002 46. 12.1134 12.1000 0.0134 47. 24.3001 23.7000 0.6001 48. 2.8357 3.7000 0.8643 49. 6.8266 9.3000 2.4734 50. 8.2082 8.1000 0.1082 So while many predictions were off by a few cents, some were off by \\$19.00. Feel free to change the batch size, test size, and number of epochs to obtain a better model.","title":"Validate the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#save-the-model","text":"We can save a trained model to a file in case we want to come back later and feed new data through it. The best practice is to save the state of the model (weights & biases) and not the full definition. Also, we want to ensure that only a trained model is saved, to prevent overwriting a previously saved model with an untrained one. For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html # Make sure to save the model only after the training has happened! if len ( losses ) == epochs : torch . save ( model . state_dict (), 'TaxiFareRegrModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' )","title":"Save the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#loading-a-saved-model-starting-from-scratch","text":"We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. To demonstrate, restart the kernel before proceeding. import torch import torch.nn as nn import numpy as np import pandas as pd def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): r = 6371 phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) return r * c class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) return self . layers ( x ) Now define the model. Before we can load the saved settings, we need to instantiate our TabularModel with the parameters we used before (embedding sizes, number of continuous columns, output size, layer sizes, and dropout layer p-value). emb_szs = [( 24 , 12 ), ( 2 , 1 ), ( 7 , 4 )] model2 = TabularModel ( emb_szs , 6 , 1 , [ 200 , 100 ], p = 0.4 ) Once the model is set up, loading the saved settings is a snap. model2 . load_state_dict ( torch . load ( 'TaxiFareRegrModel.pt' )); model2 . eval () # be sure to run this step! TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=1, bias=True) ) ) Next we\u2019ll define a function that takes in new parameters from the user, performs all of the preprocessing steps above, and passes the new data through our trained model. def test_data ( mdl ): # pass in the name of the new model # INPUT NEW DATA plat = float ( input ( 'What is the pickup latitude? ' )) plong = float ( input ( 'What is the pickup longitude? ' )) dlat = float ( input ( 'What is the dropoff latitude? ' )) dlong = float ( input ( 'What is the dropoff longitude? ' )) psngr = int ( input ( 'How many passengers? ' )) dt = input ( 'What is the pickup date and time? \\n Format as YYYY-MM-DD HH:MM:SS ' ) # PREPROCESS THE DATA dfx_dict = { 'pickup_latitude' : plat , 'pickup_longitude' : plong , 'dropoff_latitude' : dlat , 'dropoff_longitude' : dlong , 'passenger_count' : psngr , 'EDTdate' : dt } dfx = pd . DataFrame ( dfx_dict , index = [ 0 ]) dfx [ 'dist_km' ] = haversine_distance ( dfx , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) dfx [ 'EDTdate' ] = pd . to_datetime ( dfx [ 'EDTdate' ]) # We can skip the .astype(category) step since our fields are small, # and encode them right away dfx [ 'Hour' ] = dfx [ 'EDTdate' ] . dt . hour dfx [ 'AMorPM' ] = np . where ( dfx [ 'Hour' ] < 12 , 0 , 1 ) dfx [ 'Weekday' ] = dfx [ 'EDTdate' ] . dt . strftime ( \" %a \" ) dfx [ 'Weekday' ] = dfx [ 'Weekday' ] . replace ([ 'Fri' , 'Mon' , 'Sat' , 'Sun' , 'Thu' , 'Tue' , 'Wed' ], [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ]) . astype ( 'int64' ) # CREATE CAT AND CONT TENSORS cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] xcats = np . stack ([ dfx [ col ] . values for col in cat_cols ], 1 ) xcats = torch . tensor ( xcats , dtype = torch . int64 ) xconts = np . stack ([ dfx [ col ] . values for col in cont_cols ], 1 ) xconts = torch . tensor ( xconts , dtype = torch . float ) # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( xcats , xconts ) print ( f ' \\n The predicted fare amount is $ { z . item () : .2f } ' )","title":"Loading a saved model (starting from scratch)"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/#feed-new-data-through-the-trained-model","text":"For convenience, here are the max and min values for each of the variables: Column Minimum Maximum pickup_latitude 40 41 pickup_longitude -74.5 -73.3 dropoff_latitude 40 41 dropoff_longitude -74.5 -73.3 passenger_count 1 5 EDTdate 2010-04-11 00:00:00 2010-04-24 23:59:42 Use caution! The distance between 1 degree of latitude (from 40 to 41) is 111km (69mi) and between 1 degree of longitude (from -73 to -74) is 85km (53mi). The longest cab ride in the dataset spanned a difference of only 0.243 degrees latitude and 0.284 degrees longitude. The mean difference for both latitude and longitude was about 0.02. To get a fair prediction, use values that fall close to one another. z = test_data ( model2 ) What is the pickup latitude? 40.5 What is the pickup longitude? -73.9 What is the dropoff latitude? 40.52 What is the dropoff longitude? -73.92 How many passengers? 2 What is the pickup date and time? Format as YYYY-MM-DD HH:MM:SS 2010-04-15 16:00:00 The predicted fare amount is $13.86 ## Great job!","title":"Feed new data through the trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/","text":"================ by Jawad Haider 04b - Full Artificial Neural Network Code Along - CLASSIFICATION \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Full Artificial Neural Network Code Along - CLASSIFICATION Working with tabular data Perform standard imports Load the NYC Taxi Fares dataset Calculate the distance traveled Add a datetime column and derive useful statistics Separate categorical from continuous columns Categorify Convert numpy arrays to tensors Set an embedding size Define a TabularModel Define loss function & optimizer Perform train/test splits Train the model Plot the loss function Validate the model Save the model Loading a saved model (starting from scratch) Feed new data through the trained model Full Artificial Neural Network Code Along - CLASSIFICATION \u00b6 In the last section we took in four continuous variables (lengths) to perform a classification. In this section we\u2019ll combine continuous and categorical data to perform a similar classification. The goal is to estimate the relative cost of a New York City cab ride from several inputs. The inspiration behind this code along is a recent Kaggle competition . NOTE: This notebook differs from the previous regression notebook in that it uses \u2018fare_class\u2019 for the y set, and the output contains two values instead of one. In this exercise we\u2019re training our model to perform a binary classification, and predict whether a fare is greater or less than \\$10.00. Working with tabular data \u00b6 Deep learning with neural networks is often associated with sophisticated image recognition, and in upcoming sections we\u2019ll train models based on properties like pixels patterns and colors. Here we\u2019re working with tabular data (spreadsheets, SQL tables, etc.) with columns of values that may or may not be relevant. As it happens, neural networks can learn to make connections we probably wouldn\u2019t have developed on our own. However, to do this we have to handle categorical values separately from continuous ones. Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * continuous vs. categorical values * embeddings * batch normalization * dropout layers Perform standard imports \u00b6 import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Load the NYC Taxi Fares dataset \u00b6 The Kaggle competition provides a dataset with about 55 million records. The data contains only the pickup date & time, the latitude & longitude (GPS coordinates) of the pickup and dropoff locations, and the number of passengers. It is up to the contest participant to extract any further information. For instance, does the time of day matter? The day of the week? How do we determine the distance traveled from pairs of GPS coordinates? For this exercise we\u2019ve whittled the dataset down to just 120,000 records from April 11 to April 24, 2010. The records are randomly sorted. We\u2019ll show how to calculate distance from GPS coordinates, and how to create a pandas datatime object from a text column. This will let us quickly get information like day of the week, am vs. pm, etc. Let\u2019s get started! df = pd . read_csv ( '../Data/NYCTaxiFares.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 df [ 'fare_class' ] . value_counts () 0 80000 1 40000 Name: fare_class, dtype: int64 Conveniently, \u2154 of the data have fares under \\$10, and \u2153 have fares \\$10 and above. Fare classes correspond to fare amounts as follows: Class Values 0 \\ < \\$10.00 1 > = \\$10.00 > > > Calculate the distance traveled \u00b6 The haversine formula calculates the distance on a sphere between two sets of GPS coordinates. Here we assign latitude values with (phi) and longitude with (lambda). The distance formula works out to where def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): \"\"\" Calculates the haversine distance between 2 sets of GPS coordinates in df \"\"\" r = 6371 # average radius of Earth in kilometers phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) d = ( r * c ) # in kilometers return d df [ 'dist_km' ] = haversine_distance ( df , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 Add a datetime column and derive useful statistics \u00b6 By creating a datetime object, we can extract information like \u201cday of the week\u201d, \u201cam vs. pm\u201d etc. Note that the data was saved in UTC time. Our data falls in April of 2010 which occurred during Daylight Savings Time in New York. For that reason, we\u2019ll make an adjustment to EDT using UTC-4 (subtracting four hours). df [ 'EDTdate' ] = pd . to_datetime ( df [ 'pickup_datetime' ] . str [: 19 ]) - pd . Timedelta ( hours = 4 ) df [ 'Hour' ] = df [ 'EDTdate' ] . dt . hour df [ 'AMorPM' ] = np . where ( df [ 'Hour' ] < 12 , 'am' , 'pm' ) df [ 'Weekday' ] = df [ 'EDTdate' ] . dt . strftime ( \" %a \" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km EDTdate Hour AMorPM Weekday 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 2010-04-19 04:17:56 4 am Mon 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2010-04-17 11:43:53 11 am Sat 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 2010-04-17 07:23:26 7 am Sat 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 2010-04-11 17:25:03 17 pm Sun 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 2010-04-16 22:19:01 22 pm Fri df [ 'EDTdate' ] . min () Timestamp('2010-04-11 00:00:10') df [ 'EDTdate' ] . max () Timestamp('2010-04-24 23:59:42') Separate categorical from continuous columns \u00b6 df . columns Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km', 'EDTdate', 'Hour', 'AMorPM', 'Weekday'], dtype='object') cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] y_col = [ 'fare_class' ] # this column contains the labels NOTE: If you plan to use all of the columns in the data table, there\u2019s a shortcut to grab the remaining continuous columns: cont_cols = [col for col in df.columns if col not in cat_cols + y_col] Here we entered the continuous columns explicitly because there are columns we\u2019re not running through the model (fare_amount and EDTdate) Categorify \u00b6 Pandas offers a category dtype for converting categorical values to numerical codes. A dataset containing months of the year will be assigned 12 codes, one for each month. These will usually be the integers 0 to 11. Pandas replaces the column values with codes, and retains an index list of category values. In the steps ahead we\u2019ll call the categorical values \u201cnames\u201d and the encodings \u201ccodes\u201d. # Convert our three categorical columns to category dtypes. for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . dtypes pickup_datetime object fare_amount float64 fare_class int64 pickup_longitude float64 pickup_latitude float64 dropoff_longitude float64 dropoff_latitude float64 passenger_count int64 dist_km float64 EDTdate datetime64[ns] Hour category AMorPM category Weekday category dtype: object We can see that df[\u2018Hour\u2019] is a categorical feature by displaying some of the rows: df [ 'Hour' ] . head () 0 4 1 11 2 7 3 17 4 22 Name: Hour, dtype: category Categories (24, int64): [0, 1, 2, 3, ..., 20, 21, 22, 23] Here our categorical names are the integers 0 through 23, for a total of 24 unique categories. These values also correspond to the codes assigned to each name. We can access the category names with Series.cat.categories or just the codes with Series.cat.codes . This will make more sense if we look at df[\u2018AMorPM\u2019] : df [ 'AMorPM' ] . head () 0 am 1 am 2 am 3 pm 4 pm Name: AMorPM, dtype: category Categories (2, object): [am, pm] df [ 'AMorPM' ] . cat . categories Index(['am', 'pm'], dtype='object') df [ 'AMorPM' ] . head () . cat . codes 0 0 1 0 2 0 3 1 4 1 dtype: int8 df [ 'Weekday' ] . cat . categories Index(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype='object') df [ 'Weekday' ] . head () . cat . codes 0 1 1 2 2 2 3 3 4 0 dtype: int8 NOTE: NaN values in categorical data are assigned a code of -1. We don\u2019t have any in this particular dataset. Now we want to combine the three categorical columns into one input array using numpy.stack We don\u2019t want the Series index, just the values. hr = df [ 'Hour' ] . cat . codes . values ampm = df [ 'AMorPM' ] . cat . codes . values wkdy = df [ 'Weekday' ] . cat . codes . values cats = np . stack ([ hr , ampm , wkdy ], 1 ) cats [: 5 ] array([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]], dtype=int8) NOTE: This can be done in one line of code using a list comprehension: cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1) Don\u2019t worry about the dtype for now, we can make it int64 when we convert it to a tensor. Convert numpy arrays to tensors \u00b6 # Convert categorical variables to a tensor cats = torch . tensor ( cats , dtype = torch . int64 ) # this syntax is ok, since the source data is an array, not an existing tensor cats [: 5 ] tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]]) We can feed all of our continuous variables into the model as a tensor. We\u2019re not normalizing the values here; we\u2019ll let the model perform this step. NOTE: We have to store conts and y as Float (float32) tensors, not Double (float64) in order for batch normalization to work properly. # Convert continuous variables to a tensor conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts = torch . tensor ( conts , dtype = torch . float ) conts [: 5 ] tensor([[ 40.7305, -73.9924, 40.7447, -73.9755, 1.0000, 2.1263], [ 40.7406, -73.9901, 40.7441, -73.9742, 1.0000, 1.3923], [ 40.7511, -73.9941, 40.7662, -73.9601, 2.0000, 3.3268], [ 40.7564, -73.9905, 40.7482, -73.9712, 1.0000, 1.8641], [ 40.7342, -73.9910, 40.7431, -73.9060, 1.0000, 7.2313]]) conts . type () 'torch.FloatTensor' Note: the CrossEntropyLoss function we\u2019ll use below expects a 1d y-tensor, so we\u2019ll replace .reshape(-1,1) with .flatten() this time. # Convert labels to a tensor y = torch . tensor ( df [ y_col ] . values ) . flatten () y [: 5 ] tensor([0, 0, 1, 0, 1]) cats . shape torch.Size([120000, 3]) conts . shape torch.Size([120000, 6]) y . shape torch.Size([120000]) Set an embedding size \u00b6 The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50. # This will set embedding sizes for Hours, AMvsPM and Weekdays cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(24, 12), (2, 1), (7, 4)] Define a TabularModel \u00b6 This somewhat follows the fast.ai library The goal is to define a model based on the number of continuous columns (given by conts.shape[1] ) plus the number of categorical columns and their embeddings (given by len(emb_szs) and emb_szs respectively). The output would either be a regression (a single float value), or a classification (a group of bins and their softmax values). For this exercise our output will be a single regression value. Note that we\u2019ll assume our data contains both categorical and continuous data. You can add boolean parameters to your own model class to handle a variety of datasets. Let\u2019s walk through the steps we\u2019re about to take. See below for more detailed illustrations of the steps. 1. Extend the base Module class, set up the following parameters: - emb_szs: list of tuples: each categorical variable size is paired with an embedding size - n_cont: int: number of continuous variables - out_sz: int: output size - layers: list of ints: layer sizes - p: float: dropout probability for each layer (for simplicity we\u2019ll use the same value throughout) class TabularModel(nn.Module): def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5): super().\\_\\_init\\_\\_() 2. Set up the embedded layers with torch.nn.ModuleList() and torch.nn.Embedding() Categorical data will be filtered through these Embeddings in the forward section. self.embeds = nn.ModuleList(\\[nn.Embedding(ni, nf) for ni,nf in emb_szs\\]) 3. Set up a dropout function for the embeddings with torch.nn.Dropout() The default p-value=0.5 self.emb_drop = nn.Dropout(emb_drop) 4. Set up a normalization function for the continuous variables with torch.nn.BatchNorm1d() self.bn_cont = nn.BatchNorm1d(n_cont) 5. Set up a sequence of neural network layers where each level includes a Linear function, an activation function (we\u2019ll use ReLU ), a normalization step, and a dropout layer. We\u2019ll combine the list of layers with torch.nn.Sequential() self.bn_cont = nn.BatchNorm1d(n_cont) layerlist = \\[\\] n_emb = sum((nf for ni,nf in emb_szs)) n_in = n_emb + n_cont for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) layerlist.append(nn.BatchNorm1d(i)) layerlist.append(nn.Dropout(p)) n_in = i layerlist.append(nn.Linear(layers\\[-1\\],out_sz)) self.layers = nn.Sequential(\\*layerlist) 6. Define the forward method. Preprocess the embeddings and normalize the continuous variables before passing them through the layers. Use torch.cat() to combine multiple tensors into one. def forward(self, x_cat, x_cont): embeddings = \\[\\] for i,e in enumerate(self.embeds): embeddings.append(e(x_cat\\[:,i\\])) x = torch.cat(embeddings, 1) x = self.emb_drop(x) x_cont = self.bn_cont(x_cont) x = torch.cat(\\[x, x_cont\\], 1) x = self.layers(x) return x Breaking down the embeddings steps (this code is for illustration purposes only.) # This is our source data catz = cats [: 4 ] catz tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3]]) # This is passed in when the model is instantiated emb_szs [(24, 12), (2, 1), (7, 4)] # This is assigned inside the __init__() method selfembeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) selfembeds ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) list ( enumerate ( selfembeds )) [(0, Embedding(24, 12)), (1, Embedding(2, 1)), (2, Embedding(7, 4))] # This happens inside the forward() method embeddingz = [] for i , e in enumerate ( selfembeds ): embeddingz . append ( e ( catz [:, i ])) embeddingz [tensor([[ 0.0347, 0.3536, -1.2988, 1.6375, -0.0542, -0.2099, 0.3044, -1.2855, 0.8831, -0.7109, -0.9646, -0.1356], [-0.5039, -0.9924, 1.2296, -0.6908, 0.4641, -1.0487, 0.5577, -1.1560, 0.8318, -0.0834, 1.2123, -0.6210], [ 0.3509, 0.2216, 0.3432, 1.4547, -0.8747, 1.6727, -0.6417, -1.0160, 0.8217, -1.0531, 0.8357, -0.0637], [ 0.7978, 0.4566, 1.0926, -0.4095, -0.3366, 1.0216, 0.3601, -0.2927, 0.3536, 0.2170, -1.4778, -1.1965]], grad_fn=<EmbeddingBackward>), tensor([[-0.9676], [-0.9676], [-0.9676], [-1.0656]], grad_fn=<EmbeddingBackward>), tensor([[-2.1762, 1.0210, 1.3557, -0.1804], [-1.0131, 0.9989, -0.4746, -0.1461], [-1.0131, 0.9989, -0.4746, -0.1461], [-0.3646, -3.2237, -0.9956, 0.2598]], grad_fn=<EmbeddingBackward>)] # We concatenate the embedding sections (12,1,4) into one (17) z = torch . cat ( embeddingz , 1 ) z tensor([[ 0.0347, 0.3536, -1.2988, 1.6375, -0.0542, -0.2099, 0.3044, -1.2855, 0.8831, -0.7109, -0.9646, -0.1356, -0.9676, -2.1762, 1.0210, 1.3557, -0.1804], [-0.5039, -0.9924, 1.2296, -0.6908, 0.4641, -1.0487, 0.5577, -1.1560, 0.8318, -0.0834, 1.2123, -0.6210, -0.9676, -1.0131, 0.9989, -0.4746, -0.1461], [ 0.3509, 0.2216, 0.3432, 1.4547, -0.8747, 1.6727, -0.6417, -1.0160, 0.8217, -1.0531, 0.8357, -0.0637, -0.9676, -1.0131, 0.9989, -0.4746, -0.1461], [ 0.7978, 0.4566, 1.0926, -0.4095, -0.3366, 1.0216, 0.3601, -0.2927, 0.3536, 0.2170, -1.4778, -1.1965, -1.0656, -0.3646, -3.2237, -0.9956, 0.2598]], grad_fn=<CatBackward>) # This was assigned under the __init__() method selfembdrop = nn . Dropout ( .4 ) z = selfembdrop ( z ) z tensor([[ 0.0000, 0.0000, -2.1647, 0.0000, -0.0000, -0.3498, 0.5073, -2.1424, 0.0000, -1.1848, -1.6076, -0.2259, -1.6127, -3.6271, 0.0000, 2.2594, -0.3007], [-0.8398, -0.0000, 0.0000, -0.0000, 0.7734, -1.7478, 0.0000, -1.9267, 0.0000, -0.1390, 0.0000, -1.0350, -0.0000, -0.0000, 1.6648, -0.0000, -0.2435], [ 0.0000, 0.3693, 0.5719, 0.0000, -1.4578, 0.0000, -1.0694, -1.6933, 0.0000, -1.7552, 1.3929, -0.1062, -1.6127, -1.6886, 1.6648, -0.0000, -0.0000], [ 1.3297, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000, -0.4879, 0.0000, 0.0000, -2.4631, -1.9941, -1.7760, -0.6077, -5.3728, -1.6593, 0.4330]], grad_fn=<MulBackward0>) This is how the categorical embeddings are passed into the layers. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) x = self . layers ( x ) return x torch . manual_seed ( 33 ) model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 200 , 100 ], p = 0.4 ) # out_sz = 2 model TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=2, bias=True) ) ) Define loss function & optimizer \u00b6 For our classification we\u2019ll replace the MSE loss function with torch.nn.CrossEntropyLoss() For the optimizer, we\u2019ll continue to use torch.optim.Adam() criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) Perform train/test splits \u00b6 At this point our batch size is the entire dataset of 120,000 records. To save time we\u2019ll use the first 60,000. Recall that our tensors are already randomly shuffled. batch_size = 60000 test_size = 12000 cat_train = cats [: batch_size - test_size ] cat_test = cats [ batch_size - test_size : batch_size ] con_train = conts [: batch_size - test_size ] con_test = conts [ batch_size - test_size : batch_size ] y_train = y [: batch_size - test_size ] y_test = y [ batch_size - test_size : batch_size ] len ( cat_train ) 48000 len ( cat_test ) 12000 Train the model \u00b6 Expect this to take 30 minutes or more! We\u2019ve added code to tell us the duration at the end. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.73441482 epoch: 26 loss: 0.45090991 epoch: 51 loss: 0.35915938 epoch: 76 loss: 0.31940848 epoch: 101 loss: 0.29913244 epoch: 126 loss: 0.28824982 epoch: 151 loss: 0.28091952 epoch: 176 loss: 0.27713534 epoch: 201 loss: 0.27236161 epoch: 226 loss: 0.27171907 epoch: 251 loss: 0.26830241 epoch: 276 loss: 0.26365638 epoch: 300 loss: 0.25949642 Duration: 709 seconds Plot the loss function \u00b6 plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Cross Entropy Loss' ) plt . xlabel ( 'epoch' ); Validate the model \u00b6 # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = criterion ( y_val , y_test ) print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.25455481 Now let\u2019s look at the first 50 predicted values rows = 50 correct = 0 print ( f ' { \"MODEL OUTPUT\" : 26 } ARGMAX Y_TEST' ) for i in range ( rows ): print ( f ' { str ( y_val [ i ]) : 26 } { y_val [ i ] . argmax () : ^7 }{ y_test [ i ] : ^7 } ' ) if y_val [ i ] . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { rows } = { 100 * correct / rows : .2f } % correct' ) MODEL OUTPUT ARGMAX Y_TEST tensor([ 1.8140, -1.6443]) 0 0 tensor([-1.8268, 2.6373]) 1 0 tensor([ 1.4028, -1.9248]) 0 0 tensor([-1.9130, 1.4853]) 1 1 tensor([ 1.1757, -2.4964]) 0 0 tensor([ 2.0996, -2.2990]) 0 0 tensor([ 1.3226, -1.8349]) 0 0 tensor([-1.6211, 2.3889]) 1 1 tensor([ 2.2489, -2.4253]) 0 0 tensor([-0.4459, 1.1358]) 1 1 tensor([ 1.5145, -2.1619]) 0 0 tensor([ 0.7704, -1.9443]) 0 0 tensor([ 0.9637, -1.3796]) 0 0 tensor([-1.3527, 1.7322]) 1 1 tensor([ 1.4110, -2.4595]) 0 0 tensor([-1.4455, 2.6081]) 1 1 tensor([ 2.2798, -2.5864]) 0 1 tensor([ 1.4585, -2.7982]) 0 0 tensor([ 0.3342, -0.8995]) 0 0 tensor([ 2.0525, -1.9737]) 0 0 tensor([-1.3571, 2.1911]) 1 1 tensor([-0.4669, 0.2872]) 1 1 tensor([-2.0624, 2.2875]) 1 1 tensor([-2.1334, 2.6416]) 1 1 tensor([-3.1325, 5.1561]) 1 1 tensor([ 2.2128, -2.5172]) 0 0 tensor([ 1.0346, -1.7764]) 0 0 tensor([ 1.1221, -1.6717]) 0 0 tensor([-2.1322, 1.6714]) 1 1 tensor([ 1.5009, -1.6338]) 0 0 tensor([ 2.0387, -1.8475]) 0 0 tensor([-1.6346, 2.8899]) 1 1 tensor([-3.0129, 2.3519]) 1 1 tensor([-1.5746, 2.0000]) 1 1 tensor([ 1.3056, -2.2630]) 0 0 tensor([ 0.6631, -1.4797]) 0 0 tensor([-1.4585, 2.1836]) 1 1 tensor([ 1.0574, -1.5848]) 0 1 tensor([ 0.3376, -0.8050]) 0 1 tensor([ 1.9217, -1.9764]) 0 0 tensor([ 0.1011, -0.5529]) 0 0 tensor([ 0.6703, -0.5540]) 0 0 tensor([-0.6733, 0.8777]) 1 1 tensor([ 2.2017, -2.0445]) 0 0 tensor([-0.0442, -0.4276]) 0 0 tensor([-1.1204, 1.2558]) 1 1 tensor([-1.8170, 2.7124]) 1 1 tensor([ 1.7404, -2.0341]) 0 0 tensor([ 1.3266, -2.3039]) 0 0 tensor([-0.0671, 0.3291]) 1 0 45 out of 50 = 90.00% correct Save the model \u00b6 Save the trained model to a file in case you want to come back later and feed new data through it. # Make sure to save the model only after the training has happened! if len ( losses ) == epochs : torch . save ( model . state_dict (), 'TaxiFareClssModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' ) Loading a saved model (starting from scratch) \u00b6 We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. To demonstrate, restart the kernel before proceeding. import torch import torch.nn as nn import numpy as np import pandas as pd def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): r = 6371 phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) return r * c class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) return self . layers ( x ) Now define the model. Before we can load the saved settings, we need to instantiate our TabularModel with the parameters we used before (embedding sizes, number of continuous columns, output size, layer sizes, and dropout layer p-value). emb_szs = [( 24 , 12 ), ( 2 , 1 ), ( 7 , 4 )] model2 = TabularModel ( emb_szs , 6 , 2 , [ 200 , 100 ], p = 0.4 ) Once the model is set up, loading the saved settings is a snap. model2 . load_state_dict ( torch . load ( 'TaxiFareClssModel.pt' )); model2 . eval () # be sure to run this step! TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=2, bias=True) ) ) Next we\u2019ll define a function that takes in new parameters from the user, performs all of the preprocessing steps above, and passes the new data through our trained model. def test_data ( mdl ): # pass in the name of the new model # INPUT NEW DATA plat = float ( input ( 'What is the pickup latitude? ' )) plong = float ( input ( 'What is the pickup longitude? ' )) dlat = float ( input ( 'What is the dropoff latitude? ' )) dlong = float ( input ( 'What is the dropoff longitude? ' )) psngr = int ( input ( 'How many passengers? ' )) dt = input ( 'What is the pickup date and time? \\n Format as YYYY-MM-DD HH:MM:SS ' ) # PREPROCESS THE DATA dfx_dict = { 'pickup_latitude' : plat , 'pickup_longitude' : plong , 'dropoff_latitude' : dlat , 'dropoff_longitude' : dlong , 'passenger_count' : psngr , 'EDTdate' : dt } dfx = pd . DataFrame ( dfx_dict , index = [ 0 ]) dfx [ 'dist_km' ] = haversine_distance ( dfx , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) dfx [ 'EDTdate' ] = pd . to_datetime ( dfx [ 'EDTdate' ]) # We can skip the .astype(category) step since our fields are small, # and encode them right away dfx [ 'Hour' ] = dfx [ 'EDTdate' ] . dt . hour dfx [ 'AMorPM' ] = np . where ( dfx [ 'Hour' ] < 12 , 0 , 1 ) dfx [ 'Weekday' ] = dfx [ 'EDTdate' ] . dt . strftime ( \" %a \" ) dfx [ 'Weekday' ] = dfx [ 'Weekday' ] . replace ([ 'Fri' , 'Mon' , 'Sat' , 'Sun' , 'Thu' , 'Tue' , 'Wed' ], [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ]) . astype ( 'int64' ) # CREATE CAT AND CONT TENSORS cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] xcats = np . stack ([ dfx [ col ] . values for col in cat_cols ], 1 ) xcats = torch . tensor ( xcats , dtype = torch . int64 ) xconts = np . stack ([ dfx [ col ] . values for col in cont_cols ], 1 ) xconts = torch . tensor ( xconts , dtype = torch . float ) # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( xcats , xconts ) . argmax () . item () print ( f ' \\n The predicted fare class is { z } ' ) Feed new data through the trained model \u00b6 For convenience, here are the max and min values for each of the variables: Column Minimum Maximum pickup_latitude 40 41 pickup_longitude -74.5 -73.3 dropoff_latitude 40 41 dropoff_longitude -74.5 -73.3 passenger_count 1 5 EDTdate 2010-04-11 00:00:00 2010-04-24 23:59:42 Use caution! The distance between 1 degree of latitude (from 40 to 41) is 111km (69mi) and between 1 degree of longitude (from -73 to -74) is 85km (53mi). The longest cab ride in the dataset spanned a difference of only 0.243 degrees latitude and 0.284 degrees longitude. The mean difference for both latitude and longitude was about 0.02. To get a fair prediction, use values that fall close to one another. test_data ( model2 ) What is the pickup latitude? 40.5 What is the pickup longitude? -73.9 What is the dropoff latitude? 40.52 What is the dropoff longitude? -73.92 How many passengers? 2 What is the pickup date and time? Format as YYYY-MM-DD HH:MM:SS 2010-04-15 16:00:00 The predicted fare class is 1 Perfect! Where our regression predicted a fare value of \\~\\\\\\$14, our binary classification predicts a fare greater than \\$10. \\## Great job!","title":"04b Full ANN Code Along Classification"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#04b-full-artificial-neural-network-code-along-classification","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Full Artificial Neural Network Code Along - CLASSIFICATION Working with tabular data Perform standard imports Load the NYC Taxi Fares dataset Calculate the distance traveled Add a datetime column and derive useful statistics Separate categorical from continuous columns Categorify Convert numpy arrays to tensors Set an embedding size Define a TabularModel Define loss function & optimizer Perform train/test splits Train the model Plot the loss function Validate the model Save the model Loading a saved model (starting from scratch) Feed new data through the trained model","title":"04b - Full Artificial Neural Network Code Along - CLASSIFICATION"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#full-artificial-neural-network-code-along-classification","text":"In the last section we took in four continuous variables (lengths) to perform a classification. In this section we\u2019ll combine continuous and categorical data to perform a similar classification. The goal is to estimate the relative cost of a New York City cab ride from several inputs. The inspiration behind this code along is a recent Kaggle competition . NOTE: This notebook differs from the previous regression notebook in that it uses \u2018fare_class\u2019 for the y set, and the output contains two values instead of one. In this exercise we\u2019re training our model to perform a binary classification, and predict whether a fare is greater or less than \\$10.00.","title":"Full Artificial Neural Network Code Along - CLASSIFICATION"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#working-with-tabular-data","text":"Deep learning with neural networks is often associated with sophisticated image recognition, and in upcoming sections we\u2019ll train models based on properties like pixels patterns and colors. Here we\u2019re working with tabular data (spreadsheets, SQL tables, etc.) with columns of values that may or may not be relevant. As it happens, neural networks can learn to make connections we probably wouldn\u2019t have developed on our own. However, to do this we have to handle categorical values separately from continuous ones. Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * continuous vs. categorical values * embeddings * batch normalization * dropout layers","title":"Working with tabular data"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#perform-standard-imports","text":"import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#load-the-nyc-taxi-fares-dataset","text":"The Kaggle competition provides a dataset with about 55 million records. The data contains only the pickup date & time, the latitude & longitude (GPS coordinates) of the pickup and dropoff locations, and the number of passengers. It is up to the contest participant to extract any further information. For instance, does the time of day matter? The day of the week? How do we determine the distance traveled from pairs of GPS coordinates? For this exercise we\u2019ve whittled the dataset down to just 120,000 records from April 11 to April 24, 2010. The records are randomly sorted. We\u2019ll show how to calculate distance from GPS coordinates, and how to create a pandas datatime object from a text column. This will let us quickly get information like day of the week, am vs. pm, etc. Let\u2019s get started! df = pd . read_csv ( '../Data/NYCTaxiFares.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 df [ 'fare_class' ] . value_counts () 0 80000 1 40000 Name: fare_class, dtype: int64 Conveniently, \u2154 of the data have fares under \\$10, and \u2153 have fares \\$10 and above. Fare classes correspond to fare amounts as follows: Class Values 0 \\ < \\$10.00 1 > = \\$10.00 > > >","title":"Load the NYC Taxi Fares dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#calculate-the-distance-traveled","text":"The haversine formula calculates the distance on a sphere between two sets of GPS coordinates. Here we assign latitude values with (phi) and longitude with (lambda). The distance formula works out to where def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): \"\"\" Calculates the haversine distance between 2 sets of GPS coordinates in df \"\"\" r = 6371 # average radius of Earth in kilometers phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) d = ( r * c ) # in kilometers return d df [ 'dist_km' ] = haversine_distance ( df , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321","title":"Calculate the distance traveled"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#add-a-datetime-column-and-derive-useful-statistics","text":"By creating a datetime object, we can extract information like \u201cday of the week\u201d, \u201cam vs. pm\u201d etc. Note that the data was saved in UTC time. Our data falls in April of 2010 which occurred during Daylight Savings Time in New York. For that reason, we\u2019ll make an adjustment to EDT using UTC-4 (subtracting four hours). df [ 'EDTdate' ] = pd . to_datetime ( df [ 'pickup_datetime' ] . str [: 19 ]) - pd . Timedelta ( hours = 4 ) df [ 'Hour' ] = df [ 'EDTdate' ] . dt . hour df [ 'AMorPM' ] = np . where ( df [ 'Hour' ] < 12 , 'am' , 'pm' ) df [ 'Weekday' ] = df [ 'EDTdate' ] . dt . strftime ( \" %a \" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km EDTdate Hour AMorPM Weekday 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 2010-04-19 04:17:56 4 am Mon 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2010-04-17 11:43:53 11 am Sat 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 2010-04-17 07:23:26 7 am Sat 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 2010-04-11 17:25:03 17 pm Sun 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 2010-04-16 22:19:01 22 pm Fri df [ 'EDTdate' ] . min () Timestamp('2010-04-11 00:00:10') df [ 'EDTdate' ] . max () Timestamp('2010-04-24 23:59:42')","title":"Add a datetime column and derive useful statistics"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#separate-categorical-from-continuous-columns","text":"df . columns Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km', 'EDTdate', 'Hour', 'AMorPM', 'Weekday'], dtype='object') cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] y_col = [ 'fare_class' ] # this column contains the labels NOTE: If you plan to use all of the columns in the data table, there\u2019s a shortcut to grab the remaining continuous columns: cont_cols = [col for col in df.columns if col not in cat_cols + y_col] Here we entered the continuous columns explicitly because there are columns we\u2019re not running through the model (fare_amount and EDTdate)","title":"Separate categorical from continuous columns"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#categorify","text":"Pandas offers a category dtype for converting categorical values to numerical codes. A dataset containing months of the year will be assigned 12 codes, one for each month. These will usually be the integers 0 to 11. Pandas replaces the column values with codes, and retains an index list of category values. In the steps ahead we\u2019ll call the categorical values \u201cnames\u201d and the encodings \u201ccodes\u201d. # Convert our three categorical columns to category dtypes. for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . dtypes pickup_datetime object fare_amount float64 fare_class int64 pickup_longitude float64 pickup_latitude float64 dropoff_longitude float64 dropoff_latitude float64 passenger_count int64 dist_km float64 EDTdate datetime64[ns] Hour category AMorPM category Weekday category dtype: object We can see that df[\u2018Hour\u2019] is a categorical feature by displaying some of the rows: df [ 'Hour' ] . head () 0 4 1 11 2 7 3 17 4 22 Name: Hour, dtype: category Categories (24, int64): [0, 1, 2, 3, ..., 20, 21, 22, 23] Here our categorical names are the integers 0 through 23, for a total of 24 unique categories. These values also correspond to the codes assigned to each name. We can access the category names with Series.cat.categories or just the codes with Series.cat.codes . This will make more sense if we look at df[\u2018AMorPM\u2019] : df [ 'AMorPM' ] . head () 0 am 1 am 2 am 3 pm 4 pm Name: AMorPM, dtype: category Categories (2, object): [am, pm] df [ 'AMorPM' ] . cat . categories Index(['am', 'pm'], dtype='object') df [ 'AMorPM' ] . head () . cat . codes 0 0 1 0 2 0 3 1 4 1 dtype: int8 df [ 'Weekday' ] . cat . categories Index(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype='object') df [ 'Weekday' ] . head () . cat . codes 0 1 1 2 2 2 3 3 4 0 dtype: int8 NOTE: NaN values in categorical data are assigned a code of -1. We don\u2019t have any in this particular dataset. Now we want to combine the three categorical columns into one input array using numpy.stack We don\u2019t want the Series index, just the values. hr = df [ 'Hour' ] . cat . codes . values ampm = df [ 'AMorPM' ] . cat . codes . values wkdy = df [ 'Weekday' ] . cat . codes . values cats = np . stack ([ hr , ampm , wkdy ], 1 ) cats [: 5 ] array([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]], dtype=int8) NOTE: This can be done in one line of code using a list comprehension: cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1) Don\u2019t worry about the dtype for now, we can make it int64 when we convert it to a tensor.","title":"Categorify"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#convert-numpy-arrays-to-tensors","text":"# Convert categorical variables to a tensor cats = torch . tensor ( cats , dtype = torch . int64 ) # this syntax is ok, since the source data is an array, not an existing tensor cats [: 5 ] tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]]) We can feed all of our continuous variables into the model as a tensor. We\u2019re not normalizing the values here; we\u2019ll let the model perform this step. NOTE: We have to store conts and y as Float (float32) tensors, not Double (float64) in order for batch normalization to work properly. # Convert continuous variables to a tensor conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts = torch . tensor ( conts , dtype = torch . float ) conts [: 5 ] tensor([[ 40.7305, -73.9924, 40.7447, -73.9755, 1.0000, 2.1263], [ 40.7406, -73.9901, 40.7441, -73.9742, 1.0000, 1.3923], [ 40.7511, -73.9941, 40.7662, -73.9601, 2.0000, 3.3268], [ 40.7564, -73.9905, 40.7482, -73.9712, 1.0000, 1.8641], [ 40.7342, -73.9910, 40.7431, -73.9060, 1.0000, 7.2313]]) conts . type () 'torch.FloatTensor' Note: the CrossEntropyLoss function we\u2019ll use below expects a 1d y-tensor, so we\u2019ll replace .reshape(-1,1) with .flatten() this time. # Convert labels to a tensor y = torch . tensor ( df [ y_col ] . values ) . flatten () y [: 5 ] tensor([0, 0, 1, 0, 1]) cats . shape torch.Size([120000, 3]) conts . shape torch.Size([120000, 6]) y . shape torch.Size([120000])","title":"Convert numpy arrays to tensors"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#set-an-embedding-size","text":"The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50. # This will set embedding sizes for Hours, AMvsPM and Weekdays cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(24, 12), (2, 1), (7, 4)]","title":"Set an embedding size"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#define-a-tabularmodel","text":"This somewhat follows the fast.ai library The goal is to define a model based on the number of continuous columns (given by conts.shape[1] ) plus the number of categorical columns and their embeddings (given by len(emb_szs) and emb_szs respectively). The output would either be a regression (a single float value), or a classification (a group of bins and their softmax values). For this exercise our output will be a single regression value. Note that we\u2019ll assume our data contains both categorical and continuous data. You can add boolean parameters to your own model class to handle a variety of datasets. Let\u2019s walk through the steps we\u2019re about to take. See below for more detailed illustrations of the steps. 1. Extend the base Module class, set up the following parameters: - emb_szs: list of tuples: each categorical variable size is paired with an embedding size - n_cont: int: number of continuous variables - out_sz: int: output size - layers: list of ints: layer sizes - p: float: dropout probability for each layer (for simplicity we\u2019ll use the same value throughout) class TabularModel(nn.Module): def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5): super().\\_\\_init\\_\\_() 2. Set up the embedded layers with torch.nn.ModuleList() and torch.nn.Embedding() Categorical data will be filtered through these Embeddings in the forward section. self.embeds = nn.ModuleList(\\[nn.Embedding(ni, nf) for ni,nf in emb_szs\\]) 3. Set up a dropout function for the embeddings with torch.nn.Dropout() The default p-value=0.5 self.emb_drop = nn.Dropout(emb_drop) 4. Set up a normalization function for the continuous variables with torch.nn.BatchNorm1d() self.bn_cont = nn.BatchNorm1d(n_cont) 5. Set up a sequence of neural network layers where each level includes a Linear function, an activation function (we\u2019ll use ReLU ), a normalization step, and a dropout layer. We\u2019ll combine the list of layers with torch.nn.Sequential() self.bn_cont = nn.BatchNorm1d(n_cont) layerlist = \\[\\] n_emb = sum((nf for ni,nf in emb_szs)) n_in = n_emb + n_cont for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) layerlist.append(nn.BatchNorm1d(i)) layerlist.append(nn.Dropout(p)) n_in = i layerlist.append(nn.Linear(layers\\[-1\\],out_sz)) self.layers = nn.Sequential(\\*layerlist) 6. Define the forward method. Preprocess the embeddings and normalize the continuous variables before passing them through the layers. Use torch.cat() to combine multiple tensors into one. def forward(self, x_cat, x_cont): embeddings = \\[\\] for i,e in enumerate(self.embeds): embeddings.append(e(x_cat\\[:,i\\])) x = torch.cat(embeddings, 1) x = self.emb_drop(x) x_cont = self.bn_cont(x_cont) x = torch.cat(\\[x, x_cont\\], 1) x = self.layers(x) return x Breaking down the embeddings steps (this code is for illustration purposes only.) # This is our source data catz = cats [: 4 ] catz tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3]]) # This is passed in when the model is instantiated emb_szs [(24, 12), (2, 1), (7, 4)] # This is assigned inside the __init__() method selfembeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) selfembeds ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) list ( enumerate ( selfembeds )) [(0, Embedding(24, 12)), (1, Embedding(2, 1)), (2, Embedding(7, 4))] # This happens inside the forward() method embeddingz = [] for i , e in enumerate ( selfembeds ): embeddingz . append ( e ( catz [:, i ])) embeddingz [tensor([[ 0.0347, 0.3536, -1.2988, 1.6375, -0.0542, -0.2099, 0.3044, -1.2855, 0.8831, -0.7109, -0.9646, -0.1356], [-0.5039, -0.9924, 1.2296, -0.6908, 0.4641, -1.0487, 0.5577, -1.1560, 0.8318, -0.0834, 1.2123, -0.6210], [ 0.3509, 0.2216, 0.3432, 1.4547, -0.8747, 1.6727, -0.6417, -1.0160, 0.8217, -1.0531, 0.8357, -0.0637], [ 0.7978, 0.4566, 1.0926, -0.4095, -0.3366, 1.0216, 0.3601, -0.2927, 0.3536, 0.2170, -1.4778, -1.1965]], grad_fn=<EmbeddingBackward>), tensor([[-0.9676], [-0.9676], [-0.9676], [-1.0656]], grad_fn=<EmbeddingBackward>), tensor([[-2.1762, 1.0210, 1.3557, -0.1804], [-1.0131, 0.9989, -0.4746, -0.1461], [-1.0131, 0.9989, -0.4746, -0.1461], [-0.3646, -3.2237, -0.9956, 0.2598]], grad_fn=<EmbeddingBackward>)] # We concatenate the embedding sections (12,1,4) into one (17) z = torch . cat ( embeddingz , 1 ) z tensor([[ 0.0347, 0.3536, -1.2988, 1.6375, -0.0542, -0.2099, 0.3044, -1.2855, 0.8831, -0.7109, -0.9646, -0.1356, -0.9676, -2.1762, 1.0210, 1.3557, -0.1804], [-0.5039, -0.9924, 1.2296, -0.6908, 0.4641, -1.0487, 0.5577, -1.1560, 0.8318, -0.0834, 1.2123, -0.6210, -0.9676, -1.0131, 0.9989, -0.4746, -0.1461], [ 0.3509, 0.2216, 0.3432, 1.4547, -0.8747, 1.6727, -0.6417, -1.0160, 0.8217, -1.0531, 0.8357, -0.0637, -0.9676, -1.0131, 0.9989, -0.4746, -0.1461], [ 0.7978, 0.4566, 1.0926, -0.4095, -0.3366, 1.0216, 0.3601, -0.2927, 0.3536, 0.2170, -1.4778, -1.1965, -1.0656, -0.3646, -3.2237, -0.9956, 0.2598]], grad_fn=<CatBackward>) # This was assigned under the __init__() method selfembdrop = nn . Dropout ( .4 ) z = selfembdrop ( z ) z tensor([[ 0.0000, 0.0000, -2.1647, 0.0000, -0.0000, -0.3498, 0.5073, -2.1424, 0.0000, -1.1848, -1.6076, -0.2259, -1.6127, -3.6271, 0.0000, 2.2594, -0.3007], [-0.8398, -0.0000, 0.0000, -0.0000, 0.7734, -1.7478, 0.0000, -1.9267, 0.0000, -0.1390, 0.0000, -1.0350, -0.0000, -0.0000, 1.6648, -0.0000, -0.2435], [ 0.0000, 0.3693, 0.5719, 0.0000, -1.4578, 0.0000, -1.0694, -1.6933, 0.0000, -1.7552, 1.3929, -0.1062, -1.6127, -1.6886, 1.6648, -0.0000, -0.0000], [ 1.3297, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000, -0.4879, 0.0000, 0.0000, -2.4631, -1.9941, -1.7760, -0.6077, -5.3728, -1.6593, 0.4330]], grad_fn=<MulBackward0>) This is how the categorical embeddings are passed into the layers. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) x = self . layers ( x ) return x torch . manual_seed ( 33 ) model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 200 , 100 ], p = 0.4 ) # out_sz = 2 model TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=2, bias=True) ) )","title":"Define a TabularModel"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#define-loss-function-optimizer","text":"For our classification we\u2019ll replace the MSE loss function with torch.nn.CrossEntropyLoss() For the optimizer, we\u2019ll continue to use torch.optim.Adam() criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#perform-traintest-splits","text":"At this point our batch size is the entire dataset of 120,000 records. To save time we\u2019ll use the first 60,000. Recall that our tensors are already randomly shuffled. batch_size = 60000 test_size = 12000 cat_train = cats [: batch_size - test_size ] cat_test = cats [ batch_size - test_size : batch_size ] con_train = conts [: batch_size - test_size ] con_test = conts [ batch_size - test_size : batch_size ] y_train = y [: batch_size - test_size ] y_test = y [ batch_size - test_size : batch_size ] len ( cat_train ) 48000 len ( cat_test ) 12000","title":"Perform train/test splits"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#train-the-model","text":"Expect this to take 30 minutes or more! We\u2019ve added code to tell us the duration at the end. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.73441482 epoch: 26 loss: 0.45090991 epoch: 51 loss: 0.35915938 epoch: 76 loss: 0.31940848 epoch: 101 loss: 0.29913244 epoch: 126 loss: 0.28824982 epoch: 151 loss: 0.28091952 epoch: 176 loss: 0.27713534 epoch: 201 loss: 0.27236161 epoch: 226 loss: 0.27171907 epoch: 251 loss: 0.26830241 epoch: 276 loss: 0.26365638 epoch: 300 loss: 0.25949642 Duration: 709 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#plot-the-loss-function","text":"plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Cross Entropy Loss' ) plt . xlabel ( 'epoch' );","title":"Plot the loss function"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#validate-the-model","text":"# TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = criterion ( y_val , y_test ) print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.25455481 Now let\u2019s look at the first 50 predicted values rows = 50 correct = 0 print ( f ' { \"MODEL OUTPUT\" : 26 } ARGMAX Y_TEST' ) for i in range ( rows ): print ( f ' { str ( y_val [ i ]) : 26 } { y_val [ i ] . argmax () : ^7 }{ y_test [ i ] : ^7 } ' ) if y_val [ i ] . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { rows } = { 100 * correct / rows : .2f } % correct' ) MODEL OUTPUT ARGMAX Y_TEST tensor([ 1.8140, -1.6443]) 0 0 tensor([-1.8268, 2.6373]) 1 0 tensor([ 1.4028, -1.9248]) 0 0 tensor([-1.9130, 1.4853]) 1 1 tensor([ 1.1757, -2.4964]) 0 0 tensor([ 2.0996, -2.2990]) 0 0 tensor([ 1.3226, -1.8349]) 0 0 tensor([-1.6211, 2.3889]) 1 1 tensor([ 2.2489, -2.4253]) 0 0 tensor([-0.4459, 1.1358]) 1 1 tensor([ 1.5145, -2.1619]) 0 0 tensor([ 0.7704, -1.9443]) 0 0 tensor([ 0.9637, -1.3796]) 0 0 tensor([-1.3527, 1.7322]) 1 1 tensor([ 1.4110, -2.4595]) 0 0 tensor([-1.4455, 2.6081]) 1 1 tensor([ 2.2798, -2.5864]) 0 1 tensor([ 1.4585, -2.7982]) 0 0 tensor([ 0.3342, -0.8995]) 0 0 tensor([ 2.0525, -1.9737]) 0 0 tensor([-1.3571, 2.1911]) 1 1 tensor([-0.4669, 0.2872]) 1 1 tensor([-2.0624, 2.2875]) 1 1 tensor([-2.1334, 2.6416]) 1 1 tensor([-3.1325, 5.1561]) 1 1 tensor([ 2.2128, -2.5172]) 0 0 tensor([ 1.0346, -1.7764]) 0 0 tensor([ 1.1221, -1.6717]) 0 0 tensor([-2.1322, 1.6714]) 1 1 tensor([ 1.5009, -1.6338]) 0 0 tensor([ 2.0387, -1.8475]) 0 0 tensor([-1.6346, 2.8899]) 1 1 tensor([-3.0129, 2.3519]) 1 1 tensor([-1.5746, 2.0000]) 1 1 tensor([ 1.3056, -2.2630]) 0 0 tensor([ 0.6631, -1.4797]) 0 0 tensor([-1.4585, 2.1836]) 1 1 tensor([ 1.0574, -1.5848]) 0 1 tensor([ 0.3376, -0.8050]) 0 1 tensor([ 1.9217, -1.9764]) 0 0 tensor([ 0.1011, -0.5529]) 0 0 tensor([ 0.6703, -0.5540]) 0 0 tensor([-0.6733, 0.8777]) 1 1 tensor([ 2.2017, -2.0445]) 0 0 tensor([-0.0442, -0.4276]) 0 0 tensor([-1.1204, 1.2558]) 1 1 tensor([-1.8170, 2.7124]) 1 1 tensor([ 1.7404, -2.0341]) 0 0 tensor([ 1.3266, -2.3039]) 0 0 tensor([-0.0671, 0.3291]) 1 0 45 out of 50 = 90.00% correct","title":"Validate the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#save-the-model","text":"Save the trained model to a file in case you want to come back later and feed new data through it. # Make sure to save the model only after the training has happened! if len ( losses ) == epochs : torch . save ( model . state_dict (), 'TaxiFareClssModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' )","title":"Save the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#loading-a-saved-model-starting-from-scratch","text":"We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. To demonstrate, restart the kernel before proceeding. import torch import torch.nn as nn import numpy as np import pandas as pd def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): r = 6371 phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) return r * c class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) return self . layers ( x ) Now define the model. Before we can load the saved settings, we need to instantiate our TabularModel with the parameters we used before (embedding sizes, number of continuous columns, output size, layer sizes, and dropout layer p-value). emb_szs = [( 24 , 12 ), ( 2 , 1 ), ( 7 , 4 )] model2 = TabularModel ( emb_szs , 6 , 2 , [ 200 , 100 ], p = 0.4 ) Once the model is set up, loading the saved settings is a snap. model2 . load_state_dict ( torch . load ( 'TaxiFareClssModel.pt' )); model2 . eval () # be sure to run this step! TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=2, bias=True) ) ) Next we\u2019ll define a function that takes in new parameters from the user, performs all of the preprocessing steps above, and passes the new data through our trained model. def test_data ( mdl ): # pass in the name of the new model # INPUT NEW DATA plat = float ( input ( 'What is the pickup latitude? ' )) plong = float ( input ( 'What is the pickup longitude? ' )) dlat = float ( input ( 'What is the dropoff latitude? ' )) dlong = float ( input ( 'What is the dropoff longitude? ' )) psngr = int ( input ( 'How many passengers? ' )) dt = input ( 'What is the pickup date and time? \\n Format as YYYY-MM-DD HH:MM:SS ' ) # PREPROCESS THE DATA dfx_dict = { 'pickup_latitude' : plat , 'pickup_longitude' : plong , 'dropoff_latitude' : dlat , 'dropoff_longitude' : dlong , 'passenger_count' : psngr , 'EDTdate' : dt } dfx = pd . DataFrame ( dfx_dict , index = [ 0 ]) dfx [ 'dist_km' ] = haversine_distance ( dfx , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) dfx [ 'EDTdate' ] = pd . to_datetime ( dfx [ 'EDTdate' ]) # We can skip the .astype(category) step since our fields are small, # and encode them right away dfx [ 'Hour' ] = dfx [ 'EDTdate' ] . dt . hour dfx [ 'AMorPM' ] = np . where ( dfx [ 'Hour' ] < 12 , 0 , 1 ) dfx [ 'Weekday' ] = dfx [ 'EDTdate' ] . dt . strftime ( \" %a \" ) dfx [ 'Weekday' ] = dfx [ 'Weekday' ] . replace ([ 'Fri' , 'Mon' , 'Sat' , 'Sun' , 'Thu' , 'Tue' , 'Wed' ], [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ]) . astype ( 'int64' ) # CREATE CAT AND CONT TENSORS cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] xcats = np . stack ([ dfx [ col ] . values for col in cat_cols ], 1 ) xcats = torch . tensor ( xcats , dtype = torch . int64 ) xconts = np . stack ([ dfx [ col ] . values for col in cont_cols ], 1 ) xconts = torch . tensor ( xconts , dtype = torch . float ) # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( xcats , xconts ) . argmax () . item () print ( f ' \\n The predicted fare class is { z } ' )","title":"Loading a saved model (starting from scratch)"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/#feed-new-data-through-the-trained-model","text":"For convenience, here are the max and min values for each of the variables: Column Minimum Maximum pickup_latitude 40 41 pickup_longitude -74.5 -73.3 dropoff_latitude 40 41 dropoff_longitude -74.5 -73.3 passenger_count 1 5 EDTdate 2010-04-11 00:00:00 2010-04-24 23:59:42 Use caution! The distance between 1 degree of latitude (from 40 to 41) is 111km (69mi) and between 1 degree of longitude (from -73 to -74) is 85km (53mi). The longest cab ride in the dataset spanned a difference of only 0.243 degrees latitude and 0.284 degrees longitude. The mean difference for both latitude and longitude was about 0.02. To get a fair prediction, use values that fall close to one another. test_data ( model2 ) What is the pickup latitude? 40.5 What is the pickup longitude? -73.9 What is the dropoff latitude? 40.52 What is the dropoff longitude? -73.92 How many passengers? 2 What is the pickup date and time? Format as YYYY-MM-DD HH:MM:SS 2010-04-15 16:00:00 The predicted fare class is 1 Perfect! Where our regression predicted a fare value of \\~\\\\\\$14, our binary classification predicts a fare greater than \\$10. \\## Great job!","title":"Feed new data through the trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/","text":"================ by Jawad Haider 05 - Neural Network Exercises \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Neural Network Exercises Census Income Dataset Perform standard imports 1. Separate continuous, categorical and label column names 2. Convert categorical columns to category dtypes Optional: Shuffle the dataset 3. Set the embedding sizes 4. Create an array of categorical values 5. Convert \u201ccats\u201d to a tensor 6. Create an array of continuous values 7. Convert \u201cconts\u201d to a tensor 8. Create a label tensor 9. Create train and test sets from cats , conts , and y Define the model class 10. Set the random seed 11. Create a TabularModel instance 12. Define the loss and optimization functions Train the model 13. Plot the Cross Entropy Loss against epochs 14. Evaluate the test set 15. Calculate the overall percent accuracy BONUS: Feed new data through the trained model Great job! Neural Network Exercises \u00b6 For these exercises we\u2019ll perform a binary classification on the Census Income dataset available from the UC Irvine Machine Learning Repository The goal is to determine if an individual earns more than $50K based on a set of continuous and categorical variables. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! Census Income Dataset \u00b6 For this exercises we\u2019re using the Census Income dataset available from the UC Irvine Machine Learning Repository . The full dataset has 48,842 entries. For this exercise we have reduced the number of records, fields and field entries, and have removed entries with null values. The file income.csv has 30,000 entries Each entry contains the following information about an individual: * age : the age of an individual as an integer from 18 to 90 (continuous) * sex : Male or Female (categorical) * education : represents the highest level of education achieved by an individual (categorical) * education_num : represents education as an integer from 3 to 16 (categorical) 3 5th-6th 8 12th 13 Bachelors 4 7th-8th 9 HS-grad 14 Masters 5 9th 10 Some-college 15 Prof-school 6 10th 11 Assoc-voc 16 Doctorate 7 11th 12 Assoc-acdm marital-status : marital status of an individual (categorical) Married Divorced Married-spouse-absent Separated Widowed Never-married workclass : a general term to represent the employment status of an individual (categorical) Local-gov Private State-gov Self-emp Federal-gov occupation : the general type of occupation of an individual (categorical) Adm-clerical Handlers-cleaners Protective-serv Craft-repair Machine-op-inspct Sales Exec-managerial Other-service Tech-support Farming-fishing Prof-specialty Transport-moving hours-per-week : the hours an individual has reported to work per week as an integer from 20 to 90 (continuous) income : whether or not an individual makes more than \\$50,000 annually (label) label : income represented as an integer (0: \\<=\\$50K, 1: >\\$50K) (optional label) Perform standard imports \u00b6 Run the cell below to load the libraries needed for this exercise and the Census Income dataset. import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle % matplotlib inline df = pd . read_csv ( '../Data/income.csv' ) print ( len ( df )) df . head () 30000 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 27 Male HS-grad 9 Never-married Private Craft-repair 40 <=50K 0 1 47 Male Masters 14 Married Local-gov Exec-managerial 50 >50K 1 2 59 Male HS-grad 9 Divorced Self-emp Prof-specialty 20 <=50K 0 3 38 Female Prof-school 15 Never-married Federal-gov Prof-specialty 57 >50K 1 4 64 Female 11th 7 Widowed Private Farming-fishing 40 <=50K 0 df [ 'label' ] . value_counts () 0 21700 1 8300 Name: label, dtype: int64 df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 30000 entries, 0 to 29999 Data columns (total 10 columns): age 30000 non-null int64 sex 30000 non-null object education 30000 non-null object education-num 30000 non-null int64 marital-status 30000 non-null object workclass 30000 non-null object occupation 30000 non-null object hours-per-week 30000 non-null int64 income 30000 non-null object label 30000 non-null int64 dtypes: int64(4), object(6) memory usage: 2.3+ MB 1. Separate continuous, categorical and label column names \u00b6 You should find that there are 5 categorical columns, 2 continuous columns and 1 label. In the case of education and education-num it doesn\u2019t matter which column you use. For the label column, be sure to use label and not income . Assign the variable names \u201ccat_cols\u201d, \u201ccont_cols\u201d and \u201cy_col\u201d to the lists of names. df . columns Index(['age', 'sex', 'education', 'education-num', 'marital-status', 'workclass', 'occupation', 'hours-per-week', 'income', 'label'], dtype='object') # CODE HERE cat_cols = [ 'sex' , 'education' , 'marital-status' , 'workclass' , 'occupation' ] cont_cols = [ 'hours-per-week' , 'education-num' ] y_col = [ 'label' ] # RUN THIS CODE TO COMPARE RESULTS: print ( f 'cat_cols has { len ( cat_cols ) } columns' ) print ( f 'cont_cols has { len ( cont_cols ) } columns' ) print ( f 'y_col has { len ( y_col ) } column' ) cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column # DON'T WRITE HERE cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column 2. Convert categorical columns to category dtypes \u00b6 # CODE HERE for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 30000 entries, 0 to 29999 Data columns (total 10 columns): age 30000 non-null int64 sex 30000 non-null category education 30000 non-null category education-num 30000 non-null int64 marital-status 30000 non-null category workclass 30000 non-null category occupation 30000 non-null category hours-per-week 30000 non-null int64 income 30000 non-null object label 30000 non-null int64 dtypes: category(5), int64(4), object(1) memory usage: 1.3+ MB # DON'T WRITE HERE Optional: Shuffle the dataset \u00b6 The income.csv dataset is already shuffled. However, if you would like to try different configurations after completing the exercises, this is where you would want to shuffle the entire set. # THIS CELL IS OPTIONAL df = shuffle ( df , random_state = 101 ) df . reset_index ( drop = True , inplace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 23 Female HS-grad 9 Never-married Private Other-service 50 <=50K 0 1 37 Female Prof-school 15 Married State-gov Prof-specialty 39 >50K 1 2 34 Male Some-college 10 Divorced Private Adm-clerical 40 <=50K 0 3 31 Male HS-grad 9 Married Private Craft-repair 40 >50K 1 4 20 Female Some-college 10 Never-married Private Sales 25 <=50K 0 3. Set the embedding sizes \u00b6 Create a variable \u201ccat_szs\u201d to hold the number of categories in each variable. Then create a variable \u201cemb_szs\u201d to hold the list of (category size, embedding size) tuples. # CODE HERE cat_szs = [ len ( df [ cat ] . cat . categories ) for cat in cat_cols ] emb_szs = [( size , min ( 50 ,( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)] # DON'T WRITE HERE [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)] 4. Create an array of categorical values \u00b6 Create a NumPy array called \u201ccats\u201d that contains a stack of each categorical column .cat.codes.values Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE cats = np . stack ([ df [ cat ] . cat . codes . values for cat in cat_cols ], axis = 1 ) # RUN THIS CODE TO COMPARE RESULTS cats [: 5 ] array([[ 0, 10, 3, 2, 6], [ 0, 12, 1, 4, 7], [ 1, 13, 0, 2, 0], [ 1, 10, 1, 2, 1], [ 0, 13, 3, 2, 9]], dtype=int8) # DON'T WRITE HERE array([[ 1, 10, 3, 2, 1], [ 1, 11, 1, 1, 2], [ 1, 10, 0, 3, 7], [ 0, 12, 3, 0, 7], [ 0, 1, 5, 2, 3]], dtype=int8) 5. Convert \u201ccats\u201d to a tensor \u00b6 Convert the \u201ccats\u201d NumPy array to a tensor of dtype int64 # CODE HERE cats = torch . tensor ( cats , dtype = torch . int64 ) cats . dtype /home/jawad/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). torch.int64 # DON'T WRITE HERE 6. Create an array of continuous values \u00b6 Create a NumPy array called \u201cconts\u201d that contains a stack of each continuous column. Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE conts = np . stack ([ df [ cont ] . values for cont in cont_cols ], axis = 1 ) # RUN THIS CODE TO COMPARE RESULTS conts [: 5 ] array([[50, 9], [39, 15], [40, 10], [40, 9], [25, 10]]) # DON'T WRITE HERE array([[27, 40], [47, 50], [59, 20], [38, 57], [64, 40]], dtype=int64) 7. Convert \u201cconts\u201d to a tensor \u00b6 Convert the \u201cconts\u201d NumPy array to a tensor of dtype float32 # CODE HERE conts = torch . tensor ( conts , dtype = torch . float ) # RUN THIS CODE TO COMPARE RESULTS conts . dtype torch.float32 # DON'T WRITE HERE torch.float32 8. Create a label tensor \u00b6 Create a tensor called \u201cy\u201d from the values in the label column. Be sure to flatten the tensor so that it can be passed into the CE Loss function. # CODE HERE y = torch . tensor ( df [ y_col ] . values ) . flatten () # DON'T WRITE HERE 9. Create train and test sets from cats , conts , and y \u00b6 We use the entire batch of 30,000 records, but a smaller batch size will save time during training. We used a test size of 5,000 records, but you can choose another fixed value or a percentage of the batch size. Make sure that your test records remain separate from your training records, without overlap. To make coding slices easier, we recommend assigning batch and test sizes to simple variables like \u201cb\u201d and \u201ct\u201d. # CODE HERE b = 10000 # suggested batch size t = 2000 # suggested test size cat_train = cats [: b - t ] cat_test = cats [ b - t : b ] cont_train = conts [: b - t ] cont_test = conts [ b - t : b ] y_train = y [: b - t ] y_test = y [ b - t : b ] # DON'T WRITE HERE Define the model class \u00b6 Run the cell below to define the TabularModel model class we\u2019ve used before. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): # Call the parent __init__ super () . __init__ () # Set up the embedding, dropout, and batch normalization layer attributes self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) # Assign a variable to hold a list of layers layerlist = [] # Assign a variable to store the number of embedding and continuous layers n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont # Iterate through the passed-in \"layers\" parameter (ie, [200,100]) to build a list of layers for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) # Convert the list of layers into an attribute self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): # Extract embedding values from the incoming categorical data embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) # Perform an initial dropout on the embeddings x = self . emb_drop ( x ) # Normalize the incoming continuous data x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) # Set up model layers x = self . layers ( x ) return x 10. Set the random seed \u00b6 To obtain results that can be recreated, set a torch manual_seed (we used 33). # CODE HERE torch . manual_seed ( 33 ) <torch._C.Generator at 0x7f2830066510> # DON'T WRITE HERE <torch._C.Generator at 0x1e5e64e5e30> 11. Create a TabularModel instance \u00b6 Create an instance called \u201cmodel\u201d with one hidden layer containing 50 neurons and a dropout layer p-value of 0.4 # CODE HERE model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 50 ], 0.4 ) # RUN THIS CODE TO COMPARE RESULTS model TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) ) # DON'T WRITE HERE TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) ) 12. Define the loss and optimization functions \u00b6 Create a loss function called \u201ccriterion\u201d using CrossEntropyLoss Create an optimization function called \u201coptimizer\u201d using Adam, with a learning rate of 0.001 # CODE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) # DON'T WRITE HERE Train the model \u00b6 Run the cell below to train the model through 300 epochs. Remember, results may vary! After completing the exercises, feel free to come back to this section and experiment with different parameters. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , cont_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.33478802 epoch: 26 loss: 0.34096697 epoch: 51 loss: 0.33127704 epoch: 76 loss: 0.33429772 epoch: 101 loss: 0.32238889 epoch: 126 loss: 0.31584346 epoch: 151 loss: 0.31843153 epoch: 176 loss: 0.31027427 epoch: 201 loss: 0.31265819 epoch: 226 loss: 0.31138414 epoch: 251 loss: 0.31025240 epoch: 276 loss: 0.30936244 epoch: 300 loss: 0.30457661 Duration: 26 seconds 13. Plot the Cross Entropy Loss against epochs \u00b6 Results may vary. The shape of the plot is what matters. # CODE HERE plt . plot ( range ( epochs ), losses ) # DON'T WRITE HERE 14. Evaluate the test set \u00b6 With torch set to no_grad , pass cat_test and con_test through the trained model. Create a validation set called \u201cy_val\u201d. Compare the output to y_test using the loss function defined above. Results may vary. # CODE HERE with torch . no_grad (): y_val = model ( cat_test , cont_test ) loss = criterion ( y_val , y_test ) # RUN THIS CODE TO COMPARE RESULTS print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.31630206 # TO EVALUATE THE TEST SET CE Loss: 0.30774996 15. Calculate the overall percent accuracy \u00b6 Using a for loop, compare the argmax values of the y_val validation set to the y_test set. # CODE HERE correct = 0 for i in range ( 2000 ): if ( y_val [ i ] . argmax () == y_test [ i ] . argmax ()): correct += 1 print ( f \" { correct } out of { 2000 } = { ( correct / 2000 ) * 100 : .2f } % correct\" ) 1394 out of 2000 = 69.70 % correct # DON'T WRITE HERE 4255 out of 5000 = 85.10% correct BONUS: Feed new data through the trained model \u00b6 See if you can write a function that allows a user to input their own values, and generates a prediction. HINT : There\u2019s no need to build a DataFrame. You can use inputs to populate column variables, convert them to embeddings with a context dictionary, and pass the embedded values directly into the tensor constructors: mar = input(\"What is the person's marital status? \") mar_d = dict(Divorced=0, Married=1, Married-spouse-absent=2, Never-married=3, Separated=4, Widowed=5) mar = mar_d[mar] cats = torch.tensor([..., ..., mar, ..., ...], dtype=torch.int64).reshape(1,-1) Make sure that names are put in alphabetical order before assigning numbers. Also, be sure to run model.eval() before passing new date through. Good luck! # WRITE YOUR CODE HERE: # RUN YOUR CODE HERE: # DON'T WRITE HERE What is the person's age? (18-90) 22 What is the person's sex? (Male/Female) male What is the person's education level? (3-16) 12 What is the person's marital status? married What is the person's workclass? private What is the person's occupation? sales How many hours/week are worked? (20-90) 40 The predicted label is 0 Great job! \u00b6","title":"05 Neural Network Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#05-neural-network-exercises","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Neural Network Exercises Census Income Dataset Perform standard imports 1. Separate continuous, categorical and label column names 2. Convert categorical columns to category dtypes Optional: Shuffle the dataset 3. Set the embedding sizes 4. Create an array of categorical values 5. Convert \u201ccats\u201d to a tensor 6. Create an array of continuous values 7. Convert \u201cconts\u201d to a tensor 8. Create a label tensor 9. Create train and test sets from cats , conts , and y Define the model class 10. Set the random seed 11. Create a TabularModel instance 12. Define the loss and optimization functions Train the model 13. Plot the Cross Entropy Loss against epochs 14. Evaluate the test set 15. Calculate the overall percent accuracy BONUS: Feed new data through the trained model Great job!","title":"05 - Neural Network Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#neural-network-exercises","text":"For these exercises we\u2019ll perform a binary classification on the Census Income dataset available from the UC Irvine Machine Learning Repository The goal is to determine if an individual earns more than $50K based on a set of continuous and categorical variables. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"Neural Network Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#census-income-dataset","text":"For this exercises we\u2019re using the Census Income dataset available from the UC Irvine Machine Learning Repository . The full dataset has 48,842 entries. For this exercise we have reduced the number of records, fields and field entries, and have removed entries with null values. The file income.csv has 30,000 entries Each entry contains the following information about an individual: * age : the age of an individual as an integer from 18 to 90 (continuous) * sex : Male or Female (categorical) * education : represents the highest level of education achieved by an individual (categorical) * education_num : represents education as an integer from 3 to 16 (categorical) 3 5th-6th 8 12th 13 Bachelors 4 7th-8th 9 HS-grad 14 Masters 5 9th 10 Some-college 15 Prof-school 6 10th 11 Assoc-voc 16 Doctorate 7 11th 12 Assoc-acdm marital-status : marital status of an individual (categorical) Married Divorced Married-spouse-absent Separated Widowed Never-married workclass : a general term to represent the employment status of an individual (categorical) Local-gov Private State-gov Self-emp Federal-gov occupation : the general type of occupation of an individual (categorical) Adm-clerical Handlers-cleaners Protective-serv Craft-repair Machine-op-inspct Sales Exec-managerial Other-service Tech-support Farming-fishing Prof-specialty Transport-moving hours-per-week : the hours an individual has reported to work per week as an integer from 20 to 90 (continuous) income : whether or not an individual makes more than \\$50,000 annually (label) label : income represented as an integer (0: \\<=\\$50K, 1: >\\$50K) (optional label)","title":"Census Income Dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#perform-standard-imports","text":"Run the cell below to load the libraries needed for this exercise and the Census Income dataset. import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle % matplotlib inline df = pd . read_csv ( '../Data/income.csv' ) print ( len ( df )) df . head () 30000 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 27 Male HS-grad 9 Never-married Private Craft-repair 40 <=50K 0 1 47 Male Masters 14 Married Local-gov Exec-managerial 50 >50K 1 2 59 Male HS-grad 9 Divorced Self-emp Prof-specialty 20 <=50K 0 3 38 Female Prof-school 15 Never-married Federal-gov Prof-specialty 57 >50K 1 4 64 Female 11th 7 Widowed Private Farming-fishing 40 <=50K 0 df [ 'label' ] . value_counts () 0 21700 1 8300 Name: label, dtype: int64 df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 30000 entries, 0 to 29999 Data columns (total 10 columns): age 30000 non-null int64 sex 30000 non-null object education 30000 non-null object education-num 30000 non-null int64 marital-status 30000 non-null object workclass 30000 non-null object occupation 30000 non-null object hours-per-week 30000 non-null int64 income 30000 non-null object label 30000 non-null int64 dtypes: int64(4), object(6) memory usage: 2.3+ MB","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#1-separate-continuous-categorical-and-label-column-names","text":"You should find that there are 5 categorical columns, 2 continuous columns and 1 label. In the case of education and education-num it doesn\u2019t matter which column you use. For the label column, be sure to use label and not income . Assign the variable names \u201ccat_cols\u201d, \u201ccont_cols\u201d and \u201cy_col\u201d to the lists of names. df . columns Index(['age', 'sex', 'education', 'education-num', 'marital-status', 'workclass', 'occupation', 'hours-per-week', 'income', 'label'], dtype='object') # CODE HERE cat_cols = [ 'sex' , 'education' , 'marital-status' , 'workclass' , 'occupation' ] cont_cols = [ 'hours-per-week' , 'education-num' ] y_col = [ 'label' ] # RUN THIS CODE TO COMPARE RESULTS: print ( f 'cat_cols has { len ( cat_cols ) } columns' ) print ( f 'cont_cols has { len ( cont_cols ) } columns' ) print ( f 'y_col has { len ( y_col ) } column' ) cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column # DON'T WRITE HERE cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column","title":"1. Separate continuous, categorical and label column names"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#2-convert-categorical-columns-to-category-dtypes","text":"# CODE HERE for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 30000 entries, 0 to 29999 Data columns (total 10 columns): age 30000 non-null int64 sex 30000 non-null category education 30000 non-null category education-num 30000 non-null int64 marital-status 30000 non-null category workclass 30000 non-null category occupation 30000 non-null category hours-per-week 30000 non-null int64 income 30000 non-null object label 30000 non-null int64 dtypes: category(5), int64(4), object(1) memory usage: 1.3+ MB # DON'T WRITE HERE","title":"2. Convert categorical columns to category dtypes"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#optional-shuffle-the-dataset","text":"The income.csv dataset is already shuffled. However, if you would like to try different configurations after completing the exercises, this is where you would want to shuffle the entire set. # THIS CELL IS OPTIONAL df = shuffle ( df , random_state = 101 ) df . reset_index ( drop = True , inplace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 23 Female HS-grad 9 Never-married Private Other-service 50 <=50K 0 1 37 Female Prof-school 15 Married State-gov Prof-specialty 39 >50K 1 2 34 Male Some-college 10 Divorced Private Adm-clerical 40 <=50K 0 3 31 Male HS-grad 9 Married Private Craft-repair 40 >50K 1 4 20 Female Some-college 10 Never-married Private Sales 25 <=50K 0","title":"Optional: Shuffle the dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#3-set-the-embedding-sizes","text":"Create a variable \u201ccat_szs\u201d to hold the number of categories in each variable. Then create a variable \u201cemb_szs\u201d to hold the list of (category size, embedding size) tuples. # CODE HERE cat_szs = [ len ( df [ cat ] . cat . categories ) for cat in cat_cols ] emb_szs = [( size , min ( 50 ,( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)] # DON'T WRITE HERE [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)]","title":"3. Set the embedding sizes"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#4-create-an-array-of-categorical-values","text":"Create a NumPy array called \u201ccats\u201d that contains a stack of each categorical column .cat.codes.values Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE cats = np . stack ([ df [ cat ] . cat . codes . values for cat in cat_cols ], axis = 1 ) # RUN THIS CODE TO COMPARE RESULTS cats [: 5 ] array([[ 0, 10, 3, 2, 6], [ 0, 12, 1, 4, 7], [ 1, 13, 0, 2, 0], [ 1, 10, 1, 2, 1], [ 0, 13, 3, 2, 9]], dtype=int8) # DON'T WRITE HERE array([[ 1, 10, 3, 2, 1], [ 1, 11, 1, 1, 2], [ 1, 10, 0, 3, 7], [ 0, 12, 3, 0, 7], [ 0, 1, 5, 2, 3]], dtype=int8)","title":"4. Create an array of categorical values"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#5-convert-cats-to-a-tensor","text":"Convert the \u201ccats\u201d NumPy array to a tensor of dtype int64 # CODE HERE cats = torch . tensor ( cats , dtype = torch . int64 ) cats . dtype /home/jawad/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). torch.int64 # DON'T WRITE HERE","title":"5. Convert \u201ccats\u201d to a tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#6-create-an-array-of-continuous-values","text":"Create a NumPy array called \u201cconts\u201d that contains a stack of each continuous column. Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE conts = np . stack ([ df [ cont ] . values for cont in cont_cols ], axis = 1 ) # RUN THIS CODE TO COMPARE RESULTS conts [: 5 ] array([[50, 9], [39, 15], [40, 10], [40, 9], [25, 10]]) # DON'T WRITE HERE array([[27, 40], [47, 50], [59, 20], [38, 57], [64, 40]], dtype=int64)","title":"6. Create an array of continuous values"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#7-convert-conts-to-a-tensor","text":"Convert the \u201cconts\u201d NumPy array to a tensor of dtype float32 # CODE HERE conts = torch . tensor ( conts , dtype = torch . float ) # RUN THIS CODE TO COMPARE RESULTS conts . dtype torch.float32 # DON'T WRITE HERE torch.float32","title":"7. Convert \u201cconts\u201d to a tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#8-create-a-label-tensor","text":"Create a tensor called \u201cy\u201d from the values in the label column. Be sure to flatten the tensor so that it can be passed into the CE Loss function. # CODE HERE y = torch . tensor ( df [ y_col ] . values ) . flatten () # DON'T WRITE HERE","title":"8. Create a label tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#9-create-train-and-test-sets-from-cats-conts-and-y","text":"We use the entire batch of 30,000 records, but a smaller batch size will save time during training. We used a test size of 5,000 records, but you can choose another fixed value or a percentage of the batch size. Make sure that your test records remain separate from your training records, without overlap. To make coding slices easier, we recommend assigning batch and test sizes to simple variables like \u201cb\u201d and \u201ct\u201d. # CODE HERE b = 10000 # suggested batch size t = 2000 # suggested test size cat_train = cats [: b - t ] cat_test = cats [ b - t : b ] cont_train = conts [: b - t ] cont_test = conts [ b - t : b ] y_train = y [: b - t ] y_test = y [ b - t : b ] # DON'T WRITE HERE","title":"9. Create train and test sets from cats, conts, and y"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#define-the-model-class","text":"Run the cell below to define the TabularModel model class we\u2019ve used before. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): # Call the parent __init__ super () . __init__ () # Set up the embedding, dropout, and batch normalization layer attributes self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) # Assign a variable to hold a list of layers layerlist = [] # Assign a variable to store the number of embedding and continuous layers n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont # Iterate through the passed-in \"layers\" parameter (ie, [200,100]) to build a list of layers for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) # Convert the list of layers into an attribute self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): # Extract embedding values from the incoming categorical data embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) # Perform an initial dropout on the embeddings x = self . emb_drop ( x ) # Normalize the incoming continuous data x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) # Set up model layers x = self . layers ( x ) return x","title":"Define the model class"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#10-set-the-random-seed","text":"To obtain results that can be recreated, set a torch manual_seed (we used 33). # CODE HERE torch . manual_seed ( 33 ) <torch._C.Generator at 0x7f2830066510> # DON'T WRITE HERE <torch._C.Generator at 0x1e5e64e5e30>","title":"10. Set the random seed"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#11-create-a-tabularmodel-instance","text":"Create an instance called \u201cmodel\u201d with one hidden layer containing 50 neurons and a dropout layer p-value of 0.4 # CODE HERE model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 50 ], 0.4 ) # RUN THIS CODE TO COMPARE RESULTS model TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) ) # DON'T WRITE HERE TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) )","title":"11. Create a TabularModel instance"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#12-define-the-loss-and-optimization-functions","text":"Create a loss function called \u201ccriterion\u201d using CrossEntropyLoss Create an optimization function called \u201coptimizer\u201d using Adam, with a learning rate of 0.001 # CODE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) # DON'T WRITE HERE","title":"12. Define the loss and optimization functions"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#train-the-model","text":"Run the cell below to train the model through 300 epochs. Remember, results may vary! After completing the exercises, feel free to come back to this section and experiment with different parameters. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , cont_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.33478802 epoch: 26 loss: 0.34096697 epoch: 51 loss: 0.33127704 epoch: 76 loss: 0.33429772 epoch: 101 loss: 0.32238889 epoch: 126 loss: 0.31584346 epoch: 151 loss: 0.31843153 epoch: 176 loss: 0.31027427 epoch: 201 loss: 0.31265819 epoch: 226 loss: 0.31138414 epoch: 251 loss: 0.31025240 epoch: 276 loss: 0.30936244 epoch: 300 loss: 0.30457661 Duration: 26 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#13-plot-the-cross-entropy-loss-against-epochs","text":"Results may vary. The shape of the plot is what matters. # CODE HERE plt . plot ( range ( epochs ), losses ) # DON'T WRITE HERE","title":"13. Plot the Cross Entropy Loss against epochs"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#14-evaluate-the-test-set","text":"With torch set to no_grad , pass cat_test and con_test through the trained model. Create a validation set called \u201cy_val\u201d. Compare the output to y_test using the loss function defined above. Results may vary. # CODE HERE with torch . no_grad (): y_val = model ( cat_test , cont_test ) loss = criterion ( y_val , y_test ) # RUN THIS CODE TO COMPARE RESULTS print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.31630206 # TO EVALUATE THE TEST SET CE Loss: 0.30774996","title":"14. Evaluate the test set"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#15-calculate-the-overall-percent-accuracy","text":"Using a for loop, compare the argmax values of the y_val validation set to the y_test set. # CODE HERE correct = 0 for i in range ( 2000 ): if ( y_val [ i ] . argmax () == y_test [ i ] . argmax ()): correct += 1 print ( f \" { correct } out of { 2000 } = { ( correct / 2000 ) * 100 : .2f } % correct\" ) 1394 out of 2000 = 69.70 % correct # DON'T WRITE HERE 4255 out of 5000 = 85.10% correct","title":"15. Calculate the overall percent accuracy"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#bonus-feed-new-data-through-the-trained-model","text":"See if you can write a function that allows a user to input their own values, and generates a prediction. HINT : There\u2019s no need to build a DataFrame. You can use inputs to populate column variables, convert them to embeddings with a context dictionary, and pass the embedded values directly into the tensor constructors: mar = input(\"What is the person's marital status? \") mar_d = dict(Divorced=0, Married=1, Married-spouse-absent=2, Never-married=3, Separated=4, Widowed=5) mar = mar_d[mar] cats = torch.tensor([..., ..., mar, ..., ...], dtype=torch.int64).reshape(1,-1) Make sure that names are put in alphabetical order before assigning numbers. Also, be sure to run model.eval() before passing new date through. Good luck! # WRITE YOUR CODE HERE: # RUN YOUR CODE HERE: # DON'T WRITE HERE What is the person's age? (18-90) 22 What is the person's sex? (Male/Female) male What is the person's education level? (3-16) 12 What is the person's marital status? married What is the person's workclass? private What is the person's occupation? sales How many hours/week are worked? (20-90) 40 The predicted label is 0","title":"BONUS: Feed new data through the trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/","text":"================ by Jawad Haider 06 - Neural Network Exercises - SOLUTIONS \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Neural Network Exercises - SOLUTIONS Census Income Dataset Perform standard imports 1. Separate continuous, categorical and label column names 2. Convert categorical columns to category dtypes Optional: Shuffle the dataset 3. Set the embedding sizes 4. Create an array of categorical values 5. Convert \u201ccats\u201d to a tensor 6. Create an array of continuous values 7. Convert \u201cconts\u201d to a tensor 8. Create a label tensor 9. Create train and test sets from cats , conts , and y Define the model class 10. Set the random seed 11. Create a TabularModel instance 12. Define the loss and optimization functions Train the model 13. Plot the Cross Entropy Loss against epochs 14. Evaluate the test set 15. Calculate the overall percent accuracy BONUS: Feed new data through the trained model Great job! Neural Network Exercises - SOLUTIONS \u00b6 For these exercises we\u2019ll perform a binary classification on the Census Income dataset available from the UC Irvine Machine Learning Repository The goal is to determine if an individual earns more than $50K based on a set of continuous and categorical variables. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! Census Income Dataset \u00b6 For this exercises we\u2019re using the Census Income dataset available from the UC Irvine Machine Learning Repository . The full dataset has 48,842 entries. For this exercise we have reduced the number of records, fields and field entries, and have removed entries with null values. The file income.csv has 30,000 entries Each entry contains the following information about an individual: * age : the age of an individual as an integer from 18 to 90 (continuous) * sex : Male or Female (categorical) * education : represents the highest level of education achieved by an individual (categorical) * education_num : represents education as an integer from 3 to 16 (categorical) 3 5th-6th 8 12th 13 Bachelors 4 7th-8th 9 HS-grad 14 Masters 5 9th 10 Some-college 15 Prof-school 6 10th 11 Assoc-voc 16 Doctorate 7 11th 12 Assoc-acdm marital-status : marital status of an individual (categorical) Married Divorced Married-spouse-absent Separated Widowed Never-married workclass : a general term to represent the employment status of an individual (categorical) Local-gov Private State-gov Self-emp Federal-gov occupation : the general type of occupation of an individual (categorical) Adm-clerical Handlers-cleaners Protective-serv Craft-repair Machine-op-inspct Sales Exec-managerial Other-service Tech-support Farming-fishing Prof-specialty Transport-moving hours-per-week : the hours an individual has reported to work per week as an integer from 20 to 90 (continuous) income : whether or not an individual makes more than \\$50,000 annually (label) label : income represented as an integer (0: \\<=\\$50K, 1: >\\$50K) (optional label) Perform standard imports \u00b6 Run the cell below to load the libraries needed for this exercise and the Census Income dataset. import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle % matplotlib inline df = pd . read_csv ( '../Data/income.csv' ) print ( len ( df )) df . head () 30000 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 27 Male HS-grad 9 Never-married Private Craft-repair 40 <=50K 0 1 47 Male Masters 14 Married Local-gov Exec-managerial 50 >50K 1 2 59 Male HS-grad 9 Divorced Self-emp Prof-specialty 20 <=50K 0 3 38 Female Prof-school 15 Never-married Federal-gov Prof-specialty 57 >50K 1 4 64 Female 11th 7 Widowed Private Farming-fishing 40 <=50K 0 df [ 'label' ] . value_counts () 0 21700 1 8300 Name: label, dtype: int64 1. Separate continuous, categorical and label column names \u00b6 You should find that there are 5 categorical columns, 2 continuous columns and 1 label. In the case of education and education-num it doesn\u2019t matter which column you use. For the label column, be sure to use label and not income . Assign the variable names \u201ccat_cols\u201d, \u201ccont_cols\u201d and \u201cy_col\u201d to the lists of names. df . columns # CODE HERE # RUN THIS CODE TO COMPARE RESULTS: print ( f 'cat_cols has { len ( cat_cols ) } columns' ) print ( f 'cont_cols has { len ( cont_cols ) } columns' ) print ( f 'y_col has { len ( y_col ) } column' ) # DON'T WRITE HERE cat_cols = [ 'sex' , 'education' , 'marital-status' , 'workclass' , 'occupation' ] cont_cols = [ 'age' , 'hours-per-week' ] y_col = [ 'label' ] print ( f 'cat_cols has { len ( cat_cols ) } columns' ) # 5 print ( f 'cont_cols has { len ( cont_cols ) } columns' ) # 2 print ( f 'y_col has { len ( y_col ) } column' ) # 1 cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column 2. Convert categorical columns to category dtypes \u00b6 # CODE HERE # DON'T WRITE HERE for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) Optional: Shuffle the dataset \u00b6 The income.csv dataset is already shuffled. However, if you would like to try different configurations after completing the exercises, this is where you would want to shuffle the entire set. # THIS CELL IS OPTIONAL df = shuffle ( df , random_state = 101 ) df . reset_index ( drop = True , inplace = True ) df . head () 3. Set the embedding sizes \u00b6 Create a variable \u201ccat_szs\u201d to hold the number of categories in each variable. Then create a variable \u201cemb_szs\u201d to hold the list of (category size, embedding size) tuples. # CODE HERE # DON'T WRITE HERE cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)] 4. Create an array of categorical values \u00b6 Create a NumPy array called \u201ccats\u201d that contains a stack of each categorical column .cat.codes.values Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS cats [: 5 ] # DON'T WRITE HERE sx = df [ 'sex' ] . cat . codes . values ed = df [ 'education' ] . cat . codes . values ms = df [ 'marital-status' ] . cat . codes . values wc = df [ 'workclass' ] . cat . codes . values oc = df [ 'occupation' ] . cat . codes . values cats = np . stack ([ sx , ed , ms , wc , oc ], 1 ) cats [: 5 ] array([[ 1, 10, 3, 2, 1], [ 1, 11, 1, 1, 2], [ 1, 10, 0, 3, 7], [ 0, 12, 3, 0, 7], [ 0, 1, 5, 2, 3]], dtype=int8) 5. Convert \u201ccats\u201d to a tensor \u00b6 Convert the \u201ccats\u201d NumPy array to a tensor of dtype int64 # CODE HERE # DON'T WRITE HERE cats = torch . tensor ( cats , dtype = torch . int64 ) 6. Create an array of continuous values \u00b6 Create a NumPy array called \u201cconts\u201d that contains a stack of each continuous column. Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS conts [: 5 ] # DON'T WRITE HERE conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts [: 5 ] array([[27, 40], [47, 50], [59, 20], [38, 57], [64, 40]], dtype=int64) 7. Convert \u201cconts\u201d to a tensor \u00b6 Convert the \u201cconts\u201d NumPy array to a tensor of dtype float32 # CODE HERE # RUN THIS CODE TO COMPARE RESULTS conts . dtype # DON'T WRITE HERE conts = torch . tensor ( conts , dtype = torch . float ) conts . dtype torch.float32 8. Create a label tensor \u00b6 Create a tensor called \u201cy\u201d from the values in the label column. Be sure to flatten the tensor so that it can be passed into the CE Loss function. # CODE HERE # DON'T WRITE HERE y = torch . tensor ( df [ y_col ] . values ) . flatten () 9. Create train and test sets from cats , conts , and y \u00b6 We use the entire batch of 30,000 records, but a smaller batch size will save time during training. We used a test size of 5,000 records, but you can choose another fixed value or a percentage of the batch size. Make sure that your test records remain separate from your training records, without overlap. To make coding slices easier, we recommend assigning batch and test sizes to simple variables like \u201cb\u201d and \u201ct\u201d. # CODE HERE b = 30000 # suggested batch size t = 5000 # suggested test size # DON'T WRITE HERE b = 30000 # suggested batch size t = 5000 # suggested test size cat_train = cats [: b - t ] cat_test = cats [ b - t : b ] con_train = conts [: b - t ] con_test = conts [ b - t : b ] y_train = y [: b - t ] y_test = y [ b - t : b ] Define the model class \u00b6 Run the cell below to define the TabularModel model class we\u2019ve used before. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): # Call the parent __init__ super () . __init__ () # Set up the embedding, dropout, and batch normalization layer attributes self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) # Assign a variable to hold a list of layers layerlist = [] # Assign a variable to store the number of embedding and continuous layers n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont # Iterate through the passed-in \"layers\" parameter (ie, [200,100]) to build a list of layers for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) # Convert the list of layers into an attribute self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): # Extract embedding values from the incoming categorical data embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) # Perform an initial dropout on the embeddings x = self . emb_drop ( x ) # Normalize the incoming continuous data x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) # Set up model layers x = self . layers ( x ) return x 10. Set the random seed \u00b6 To obtain results that can be recreated, set a torch manual_seed (we used 33). # CODE HERE # DON'T WRITE HERE torch . manual_seed ( 33 ) <torch._C.Generator at 0x1e5e64e5e30> 11. Create a TabularModel instance \u00b6 Create an instance called \u201cmodel\u201d with one hidden layer containing 50 neurons and a dropout layer p-value of 0.4 # CODE HERE # RUN THIS CODE TO COMPARE RESULTS model # DON'T WRITE HERE model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 50 ], p = 0.4 ) model TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) ) 12. Define the loss and optimization functions \u00b6 Create a loss function called \u201ccriterion\u201d using CrossEntropyLoss Create an optimization function called \u201coptimizer\u201d using Adam, with a learning rate of 0.001 # CODE HERE # DON'T WRITE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) Train the model \u00b6 Run the cell below to train the model through 300 epochs. Remember, results may vary! After completing the exercises, feel free to come back to this section and experiment with different parameters. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.65308946 epoch: 26 loss: 0.54059124 epoch: 51 loss: 0.46917316 epoch: 76 loss: 0.41288978 epoch: 101 loss: 0.37744597 epoch: 126 loss: 0.35649022 epoch: 151 loss: 0.34338138 epoch: 176 loss: 0.33378774 epoch: 201 loss: 0.32601979 epoch: 226 loss: 0.32018784 epoch: 251 loss: 0.31548899 epoch: 276 loss: 0.30901730 epoch: 300 loss: 0.30690485 Duration: 170 seconds 13. Plot the Cross Entropy Loss against epochs \u00b6 Results may vary. The shape of the plot is what matters. # CODE HERE # DON'T WRITE HERE plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Cross Entropy Loss' ) plt . xlabel ( 'epoch' ); 14. Evaluate the test set \u00b6 With torch set to no_grad , pass cat_test and con_test through the trained model. Create a validation set called \u201cy_val\u201d. Compare the output to y_test using the loss function defined above. Results may vary. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS print ( f 'CE Loss: { loss : .8f } ' ) # TO EVALUATE THE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = criterion ( y_val , y_test ) print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.30774996 15. Calculate the overall percent accuracy \u00b6 Using a for loop, compare the argmax values of the y_val validation set to the y_test set. # CODE HERE # DON'T WRITE HERE rows = len ( y_test ) correct = 0 # print(f'{\"MODEL OUTPUT\":26} ARGMAX Y_TEST') for i in range ( rows ): # print(f'{str(y_val[i]):26} {y_val[i].argmax().item():^7}{y_test[i]:^7}') if y_val [ i ] . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { rows } = { 100 * correct / rows : .2f } % correct' ) 4255 out of 5000 = 85.10% correct BONUS: Feed new data through the trained model \u00b6 See if you can write a function that allows a user to input their own values, and generates a prediction. HINT : There\u2019s no need to build a DataFrame. You can use inputs to populate column variables, convert them to embeddings with a context dictionary, and pass the embedded values directly into the tensor constructors: mar = input(\"What is the person's marital status? \") mar_d = dict(Divorced=0, Married=1, Married-spouse-absent=2, Never-married=3, Separated=4, Widowed=5) mar = mar_d[mar] cats = torch.tensor([..., ..., mar, ..., ...], dtype=torch.int64).reshape(1,-1) Make sure that names are put in alphabetical order before assigning numbers. Also, be sure to run model.eval() before passing new date through. Good luck! # WRITE YOUR CODE HERE: # RUN YOUR CODE HERE: # DON'T WRITE HERE def test_data ( mdl ): # pass in the name of the model # INPUT NEW DATA age = float ( input ( \"What is the person's age? (18-90) \" )) sex = input ( \"What is the person's sex? (Male/Female) \" ) . capitalize () edn = int ( input ( \"What is the person's education level? (3-16) \" )) mar = input ( \"What is the person's marital status? \" ) . capitalize () wrk = input ( \"What is the person's workclass? \" ) . capitalize () occ = input ( \"What is the person's occupation? \" ) . capitalize () hrs = float ( input ( \"How many hours/week are worked? (20-90) \" )) # PREPROCESS THE DATA sex_d = { 'Female' : 0 , 'Male' : 1 } mar_d = { 'Divorced' : 0 , 'Married' : 1 , 'Married-spouse-absent' : 2 , 'Never-married' : 3 , 'Separated' : 4 , 'Widowed' : 5 } wrk_d = { 'Federal-gov' : 0 , 'Local-gov' : 1 , 'Private' : 2 , 'Self-emp' : 3 , 'State-gov' : 4 } occ_d = { 'Adm-clerical' : 0 , 'Craft-repair' : 1 , 'Exec-managerial' : 2 , 'Farming-fishing' : 3 , 'Handlers-cleaners' : 4 , 'Machine-op-inspct' : 5 , 'Other-service' : 6 , 'Prof-specialty' : 7 , 'Protective-serv' : 8 , 'Sales' : 9 , 'Tech-support' : 10 , 'Transport-moving' : 11 } sex = sex_d [ sex ] mar = mar_d [ mar ] wrk = wrk_d [ wrk ] occ = occ_d [ occ ] # CREATE CAT AND CONT TENSORS cats = torch . tensor ([ sex , edn , mar , wrk , occ ], dtype = torch . int64 ) . reshape ( 1 , - 1 ) conts = torch . tensor ([ age , hrs ], dtype = torch . float ) . reshape ( 1 , - 1 ) # SET MODEL TO EVAL (in case this hasn't been done) mdl . eval () # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( cats , conts ) . argmax () . item () print ( f ' \\n The predicted label is { z } ' ) test_data ( model ) What is the person's age? (18-90) 22 What is the person's sex? (Male/Female) male What is the person's education level? (3-16) 12 What is the person's marital status? married What is the person's workclass? private What is the person's occupation? sales How many hours/week are worked? (20-90) 40 The predicted label is 0 Great job! \u00b6","title":"06 Neural Network Exercises Solutions"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#06-neural-network-exercises-solutions","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Neural Network Exercises - SOLUTIONS Census Income Dataset Perform standard imports 1. Separate continuous, categorical and label column names 2. Convert categorical columns to category dtypes Optional: Shuffle the dataset 3. Set the embedding sizes 4. Create an array of categorical values 5. Convert \u201ccats\u201d to a tensor 6. Create an array of continuous values 7. Convert \u201cconts\u201d to a tensor 8. Create a label tensor 9. Create train and test sets from cats , conts , and y Define the model class 10. Set the random seed 11. Create a TabularModel instance 12. Define the loss and optimization functions Train the model 13. Plot the Cross Entropy Loss against epochs 14. Evaluate the test set 15. Calculate the overall percent accuracy BONUS: Feed new data through the trained model Great job!","title":"06 - Neural Network Exercises - SOLUTIONS"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#neural-network-exercises-solutions","text":"For these exercises we\u2019ll perform a binary classification on the Census Income dataset available from the UC Irvine Machine Learning Repository The goal is to determine if an individual earns more than $50K based on a set of continuous and categorical variables. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"Neural Network Exercises - SOLUTIONS"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#census-income-dataset","text":"For this exercises we\u2019re using the Census Income dataset available from the UC Irvine Machine Learning Repository . The full dataset has 48,842 entries. For this exercise we have reduced the number of records, fields and field entries, and have removed entries with null values. The file income.csv has 30,000 entries Each entry contains the following information about an individual: * age : the age of an individual as an integer from 18 to 90 (continuous) * sex : Male or Female (categorical) * education : represents the highest level of education achieved by an individual (categorical) * education_num : represents education as an integer from 3 to 16 (categorical) 3 5th-6th 8 12th 13 Bachelors 4 7th-8th 9 HS-grad 14 Masters 5 9th 10 Some-college 15 Prof-school 6 10th 11 Assoc-voc 16 Doctorate 7 11th 12 Assoc-acdm marital-status : marital status of an individual (categorical) Married Divorced Married-spouse-absent Separated Widowed Never-married workclass : a general term to represent the employment status of an individual (categorical) Local-gov Private State-gov Self-emp Federal-gov occupation : the general type of occupation of an individual (categorical) Adm-clerical Handlers-cleaners Protective-serv Craft-repair Machine-op-inspct Sales Exec-managerial Other-service Tech-support Farming-fishing Prof-specialty Transport-moving hours-per-week : the hours an individual has reported to work per week as an integer from 20 to 90 (continuous) income : whether or not an individual makes more than \\$50,000 annually (label) label : income represented as an integer (0: \\<=\\$50K, 1: >\\$50K) (optional label)","title":"Census Income Dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#perform-standard-imports","text":"Run the cell below to load the libraries needed for this exercise and the Census Income dataset. import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle % matplotlib inline df = pd . read_csv ( '../Data/income.csv' ) print ( len ( df )) df . head () 30000 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 27 Male HS-grad 9 Never-married Private Craft-repair 40 <=50K 0 1 47 Male Masters 14 Married Local-gov Exec-managerial 50 >50K 1 2 59 Male HS-grad 9 Divorced Self-emp Prof-specialty 20 <=50K 0 3 38 Female Prof-school 15 Never-married Federal-gov Prof-specialty 57 >50K 1 4 64 Female 11th 7 Widowed Private Farming-fishing 40 <=50K 0 df [ 'label' ] . value_counts () 0 21700 1 8300 Name: label, dtype: int64","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#1-separate-continuous-categorical-and-label-column-names","text":"You should find that there are 5 categorical columns, 2 continuous columns and 1 label. In the case of education and education-num it doesn\u2019t matter which column you use. For the label column, be sure to use label and not income . Assign the variable names \u201ccat_cols\u201d, \u201ccont_cols\u201d and \u201cy_col\u201d to the lists of names. df . columns # CODE HERE # RUN THIS CODE TO COMPARE RESULTS: print ( f 'cat_cols has { len ( cat_cols ) } columns' ) print ( f 'cont_cols has { len ( cont_cols ) } columns' ) print ( f 'y_col has { len ( y_col ) } column' ) # DON'T WRITE HERE cat_cols = [ 'sex' , 'education' , 'marital-status' , 'workclass' , 'occupation' ] cont_cols = [ 'age' , 'hours-per-week' ] y_col = [ 'label' ] print ( f 'cat_cols has { len ( cat_cols ) } columns' ) # 5 print ( f 'cont_cols has { len ( cont_cols ) } columns' ) # 2 print ( f 'y_col has { len ( y_col ) } column' ) # 1 cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column","title":"1. Separate continuous, categorical and label column names"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#2-convert-categorical-columns-to-category-dtypes","text":"# CODE HERE # DON'T WRITE HERE for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' )","title":"2. Convert categorical columns to category dtypes"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#optional-shuffle-the-dataset","text":"The income.csv dataset is already shuffled. However, if you would like to try different configurations after completing the exercises, this is where you would want to shuffle the entire set. # THIS CELL IS OPTIONAL df = shuffle ( df , random_state = 101 ) df . reset_index ( drop = True , inplace = True ) df . head ()","title":"Optional: Shuffle the dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#3-set-the-embedding-sizes","text":"Create a variable \u201ccat_szs\u201d to hold the number of categories in each variable. Then create a variable \u201cemb_szs\u201d to hold the list of (category size, embedding size) tuples. # CODE HERE # DON'T WRITE HERE cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)]","title":"3. Set the embedding sizes"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#4-create-an-array-of-categorical-values","text":"Create a NumPy array called \u201ccats\u201d that contains a stack of each categorical column .cat.codes.values Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS cats [: 5 ] # DON'T WRITE HERE sx = df [ 'sex' ] . cat . codes . values ed = df [ 'education' ] . cat . codes . values ms = df [ 'marital-status' ] . cat . codes . values wc = df [ 'workclass' ] . cat . codes . values oc = df [ 'occupation' ] . cat . codes . values cats = np . stack ([ sx , ed , ms , wc , oc ], 1 ) cats [: 5 ] array([[ 1, 10, 3, 2, 1], [ 1, 11, 1, 1, 2], [ 1, 10, 0, 3, 7], [ 0, 12, 3, 0, 7], [ 0, 1, 5, 2, 3]], dtype=int8)","title":"4. Create an array of categorical values"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#5-convert-cats-to-a-tensor","text":"Convert the \u201ccats\u201d NumPy array to a tensor of dtype int64 # CODE HERE # DON'T WRITE HERE cats = torch . tensor ( cats , dtype = torch . int64 )","title":"5. Convert \u201ccats\u201d to a tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#6-create-an-array-of-continuous-values","text":"Create a NumPy array called \u201cconts\u201d that contains a stack of each continuous column. Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS conts [: 5 ] # DON'T WRITE HERE conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts [: 5 ] array([[27, 40], [47, 50], [59, 20], [38, 57], [64, 40]], dtype=int64)","title":"6. Create an array of continuous values"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#7-convert-conts-to-a-tensor","text":"Convert the \u201cconts\u201d NumPy array to a tensor of dtype float32 # CODE HERE # RUN THIS CODE TO COMPARE RESULTS conts . dtype # DON'T WRITE HERE conts = torch . tensor ( conts , dtype = torch . float ) conts . dtype torch.float32","title":"7. Convert \u201cconts\u201d to a tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#8-create-a-label-tensor","text":"Create a tensor called \u201cy\u201d from the values in the label column. Be sure to flatten the tensor so that it can be passed into the CE Loss function. # CODE HERE # DON'T WRITE HERE y = torch . tensor ( df [ y_col ] . values ) . flatten ()","title":"8. Create a label tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#9-create-train-and-test-sets-from-cats-conts-and-y","text":"We use the entire batch of 30,000 records, but a smaller batch size will save time during training. We used a test size of 5,000 records, but you can choose another fixed value or a percentage of the batch size. Make sure that your test records remain separate from your training records, without overlap. To make coding slices easier, we recommend assigning batch and test sizes to simple variables like \u201cb\u201d and \u201ct\u201d. # CODE HERE b = 30000 # suggested batch size t = 5000 # suggested test size # DON'T WRITE HERE b = 30000 # suggested batch size t = 5000 # suggested test size cat_train = cats [: b - t ] cat_test = cats [ b - t : b ] con_train = conts [: b - t ] con_test = conts [ b - t : b ] y_train = y [: b - t ] y_test = y [ b - t : b ]","title":"9. Create train and test sets from cats, conts, and y"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#define-the-model-class","text":"Run the cell below to define the TabularModel model class we\u2019ve used before. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): # Call the parent __init__ super () . __init__ () # Set up the embedding, dropout, and batch normalization layer attributes self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) # Assign a variable to hold a list of layers layerlist = [] # Assign a variable to store the number of embedding and continuous layers n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont # Iterate through the passed-in \"layers\" parameter (ie, [200,100]) to build a list of layers for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) # Convert the list of layers into an attribute self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): # Extract embedding values from the incoming categorical data embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) # Perform an initial dropout on the embeddings x = self . emb_drop ( x ) # Normalize the incoming continuous data x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) # Set up model layers x = self . layers ( x ) return x","title":"Define the model class"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#10-set-the-random-seed","text":"To obtain results that can be recreated, set a torch manual_seed (we used 33). # CODE HERE # DON'T WRITE HERE torch . manual_seed ( 33 ) <torch._C.Generator at 0x1e5e64e5e30>","title":"10. Set the random seed"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#11-create-a-tabularmodel-instance","text":"Create an instance called \u201cmodel\u201d with one hidden layer containing 50 neurons and a dropout layer p-value of 0.4 # CODE HERE # RUN THIS CODE TO COMPARE RESULTS model # DON'T WRITE HERE model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 50 ], p = 0.4 ) model TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) )","title":"11. Create a TabularModel instance"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#12-define-the-loss-and-optimization-functions","text":"Create a loss function called \u201ccriterion\u201d using CrossEntropyLoss Create an optimization function called \u201coptimizer\u201d using Adam, with a learning rate of 0.001 # CODE HERE # DON'T WRITE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"12. Define the loss and optimization functions"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#train-the-model","text":"Run the cell below to train the model through 300 epochs. Remember, results may vary! After completing the exercises, feel free to come back to this section and experiment with different parameters. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.65308946 epoch: 26 loss: 0.54059124 epoch: 51 loss: 0.46917316 epoch: 76 loss: 0.41288978 epoch: 101 loss: 0.37744597 epoch: 126 loss: 0.35649022 epoch: 151 loss: 0.34338138 epoch: 176 loss: 0.33378774 epoch: 201 loss: 0.32601979 epoch: 226 loss: 0.32018784 epoch: 251 loss: 0.31548899 epoch: 276 loss: 0.30901730 epoch: 300 loss: 0.30690485 Duration: 170 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#13-plot-the-cross-entropy-loss-against-epochs","text":"Results may vary. The shape of the plot is what matters. # CODE HERE # DON'T WRITE HERE plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Cross Entropy Loss' ) plt . xlabel ( 'epoch' );","title":"13. Plot the Cross Entropy Loss against epochs"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#14-evaluate-the-test-set","text":"With torch set to no_grad , pass cat_test and con_test through the trained model. Create a validation set called \u201cy_val\u201d. Compare the output to y_test using the loss function defined above. Results may vary. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS print ( f 'CE Loss: { loss : .8f } ' ) # TO EVALUATE THE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = criterion ( y_val , y_test ) print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.30774996","title":"14. Evaluate the test set"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#15-calculate-the-overall-percent-accuracy","text":"Using a for loop, compare the argmax values of the y_val validation set to the y_test set. # CODE HERE # DON'T WRITE HERE rows = len ( y_test ) correct = 0 # print(f'{\"MODEL OUTPUT\":26} ARGMAX Y_TEST') for i in range ( rows ): # print(f'{str(y_val[i]):26} {y_val[i].argmax().item():^7}{y_test[i]:^7}') if y_val [ i ] . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { rows } = { 100 * correct / rows : .2f } % correct' ) 4255 out of 5000 = 85.10% correct","title":"15. Calculate the overall percent accuracy"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#bonus-feed-new-data-through-the-trained-model","text":"See if you can write a function that allows a user to input their own values, and generates a prediction. HINT : There\u2019s no need to build a DataFrame. You can use inputs to populate column variables, convert them to embeddings with a context dictionary, and pass the embedded values directly into the tensor constructors: mar = input(\"What is the person's marital status? \") mar_d = dict(Divorced=0, Married=1, Married-spouse-absent=2, Never-married=3, Separated=4, Widowed=5) mar = mar_d[mar] cats = torch.tensor([..., ..., mar, ..., ...], dtype=torch.int64).reshape(1,-1) Make sure that names are put in alphabetical order before assigning numbers. Also, be sure to run model.eval() before passing new date through. Good luck! # WRITE YOUR CODE HERE: # RUN YOUR CODE HERE: # DON'T WRITE HERE def test_data ( mdl ): # pass in the name of the model # INPUT NEW DATA age = float ( input ( \"What is the person's age? (18-90) \" )) sex = input ( \"What is the person's sex? (Male/Female) \" ) . capitalize () edn = int ( input ( \"What is the person's education level? (3-16) \" )) mar = input ( \"What is the person's marital status? \" ) . capitalize () wrk = input ( \"What is the person's workclass? \" ) . capitalize () occ = input ( \"What is the person's occupation? \" ) . capitalize () hrs = float ( input ( \"How many hours/week are worked? (20-90) \" )) # PREPROCESS THE DATA sex_d = { 'Female' : 0 , 'Male' : 1 } mar_d = { 'Divorced' : 0 , 'Married' : 1 , 'Married-spouse-absent' : 2 , 'Never-married' : 3 , 'Separated' : 4 , 'Widowed' : 5 } wrk_d = { 'Federal-gov' : 0 , 'Local-gov' : 1 , 'Private' : 2 , 'Self-emp' : 3 , 'State-gov' : 4 } occ_d = { 'Adm-clerical' : 0 , 'Craft-repair' : 1 , 'Exec-managerial' : 2 , 'Farming-fishing' : 3 , 'Handlers-cleaners' : 4 , 'Machine-op-inspct' : 5 , 'Other-service' : 6 , 'Prof-specialty' : 7 , 'Protective-serv' : 8 , 'Sales' : 9 , 'Tech-support' : 10 , 'Transport-moving' : 11 } sex = sex_d [ sex ] mar = mar_d [ mar ] wrk = wrk_d [ wrk ] occ = occ_d [ occ ] # CREATE CAT AND CONT TENSORS cats = torch . tensor ([ sex , edn , mar , wrk , occ ], dtype = torch . int64 ) . reshape ( 1 , - 1 ) conts = torch . tensor ([ age , hrs ], dtype = torch . float ) . reshape ( 1 , - 1 ) # SET MODEL TO EVAL (in case this hasn't been done) mdl . eval () # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( cats , conts ) . argmax () . item () print ( f ' \\n The predicted label is { z } ' ) test_data ( model ) What is the person's age? (18-90) 22 What is the person's sex? (Male/Female) male What is the person's education level? (3-16) 12 What is the person's marital status? married What is the person's workclass? private What is the person's occupation? sales How many hours/week are worked? (20-90) 40 The predicted label is 0","title":"BONUS: Feed new data through the trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/","text":"================ by Jawad Haider 07 - Saving and Loading Trained Models \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Saving and Loading Trained Models Saving a trained model Loading a saved model (starting from scratch) 1. Perform standard imports 2. Run the model definition 3. Instantiate the model, load parameters That\u2019s it! Saving and Loading Trained Models \u00b6 Refer back to this notebook as a refresher on saving and loading models. Saving a trained model \u00b6 Save a trained model to a file in case you want to come back later and feed new data through it. To save a trained model called \u201cmodel\u201d to a file called \u201cMyModel.pt\u201d: torch . save ( model . state_dict (), 'MyModel.pt' ) To ensure the model has been trained before saving (assumes the variables \u201closses\u201d and \u201cepochs\u201d have been defined): if len ( losses ) == epochs : torch . save ( model . state_dict (), 'MyModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' ) Loading a saved model (starting from scratch) \u00b6 We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. 1. Perform standard imports \u00b6 These will depend on the scope of the model, chosen displays, metrics, etc. # Perform standard imports import torch import torch.nn as nn import numpy as np import pandas as pd 2. Run the model definition \u00b6 We\u2019ll introduce the model shown below in the next section. class MultilayerPerceptron ( nn . Module ): def __init__ ( self , in_sz = 784 , out_sz = 10 , layers = [ 120 , 84 ]): super () . __init__ () self . fc1 = nn . Linear ( in_sz , layers [ 0 ]) self . fc2 = nn . Linear ( layers [ 0 ], layers [ 1 ]) self . fc3 = nn . Linear ( layers [ 1 ], out_sz ) def forward ( self , X ): X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) 3. Instantiate the model, load parameters \u00b6 First we instantiate the model, then we load the pre-trained weights & biases, and finally we set the model to \u201ceval\u201d mode to prevent any further backprops. model2 = MultilayerPerceptron () model2 . load_state_dict ( torch . load ( 'MyModel.pt' )); model2 . eval () # be sure to run this step! That\u2019s it! \u00b6 Toward the end of the CNN section we\u2019ll show how to import a trained model and adapt it to a new set of image data.","title":"07 Recap Saving and Loading Trained Models"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/#07-saving-and-loading-trained-models","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Saving and Loading Trained Models Saving a trained model Loading a saved model (starting from scratch) 1. Perform standard imports 2. Run the model definition 3. Instantiate the model, load parameters That\u2019s it!","title":"07 - Saving and Loading Trained Models"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/#saving-and-loading-trained-models","text":"Refer back to this notebook as a refresher on saving and loading models.","title":"Saving and Loading Trained Models"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/#saving-a-trained-model","text":"Save a trained model to a file in case you want to come back later and feed new data through it. To save a trained model called \u201cmodel\u201d to a file called \u201cMyModel.pt\u201d: torch . save ( model . state_dict (), 'MyModel.pt' ) To ensure the model has been trained before saving (assumes the variables \u201closses\u201d and \u201cepochs\u201d have been defined): if len ( losses ) == epochs : torch . save ( model . state_dict (), 'MyModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' )","title":"Saving a trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/#loading-a-saved-model-starting-from-scratch","text":"We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions.","title":"Loading a saved model (starting from scratch)"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/#1-perform-standard-imports","text":"These will depend on the scope of the model, chosen displays, metrics, etc. # Perform standard imports import torch import torch.nn as nn import numpy as np import pandas as pd","title":"1. Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/#2-run-the-model-definition","text":"We\u2019ll introduce the model shown below in the next section. class MultilayerPerceptron ( nn . Module ): def __init__ ( self , in_sz = 784 , out_sz = 10 , layers = [ 120 , 84 ]): super () . __init__ () self . fc1 = nn . Linear ( in_sz , layers [ 0 ]) self . fc2 = nn . Linear ( layers [ 0 ], layers [ 1 ]) self . fc3 = nn . Linear ( layers [ 1 ], out_sz ) def forward ( self , X ): X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 )","title":"2. Run the model definition"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/#3-instantiate-the-model-load-parameters","text":"First we instantiate the model, then we load the pre-trained weights & biases, and finally we set the model to \u201ceval\u201d mode to prevent any further backprops. model2 = MultilayerPerceptron () model2 . load_state_dict ( torch . load ( 'MyModel.pt' )); model2 . eval () # be sure to run this step!","title":"3. Instantiate the model, load parameters"},{"location":"bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/#thats-it","text":"Toward the end of the CNN section we\u2019ll show how to import a trained model and adapt it to a new set of image data.","title":"That\u2019s it!"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/","text":"================ by Jawad Haider MNIST Code Along with ANN Perform standard imports Load the MNIST dataset Load the training set Load the test set Examine a training record View the image Batch loading with DataLoader View a batch of images Define the model Count the model parameters Define loss function & optimizer Flatten the training data Train the model Plot the loss and accuracy comparisons Evaluate Test Data Display the confusion matrix Examine the misses Great job! MNIST Code Along with ANN \u00b6 Before we start working with Convolutional Neural Networks (CNN), let\u2019s model the MNIST dataset using only linear layers. In this exercise we\u2019ll use the same logic laid out in the ANN notebook. We\u2019ll reshape the MNIST data from a 28x28 image to a flattened 1x784 vector to mimic a single row of 784 features. Perform standard imports \u00b6 Torchvision should have been installed by the environment file during setup. If not, you can install it now. At the terminal with your virtual environment activated, run conda install torchvision -c pytorch or pip install torchvision import torch import torch.nn as nn import torch.nn.functional as F # adds some efficiency from torch.utils.data import DataLoader # lets us load data in batches from torchvision import datasets , transforms import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix # for evaluating results import matplotlib.pyplot as plt % matplotlib inline Load the MNIST dataset \u00b6 PyTorch makes the MNIST dataset available through torchvision . The first time it\u2019s called, the dataset will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. ### Define transform As part of the loading process, we can apply multiple transformations (reshape, convert to tensor, normalize, etc.) to the incoming data. For this exercise we only need to convert images to tensors. transform = transforms . ToTensor () Load the training set \u00b6 train_data = datasets . MNIST ( root = '../Data' , train = True , download = True , transform = transform ) train_data Dataset MNIST Number of datapoints: 60000 Split: train Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None Load the test set \u00b6 There\u2019s a companion set of MNIST data containing 10,000 records accessible by setting train=False. As before, torchvision will only download this once, and in the future will look for the local copy. test_data = datasets . MNIST ( root = '../Data' , train = False , download = True , transform = transform ) test_data Dataset MNIST Number of datapoints: 10000 Split: test Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None Examine a training record \u00b6 train_data [ 0 ] (tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706, 0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137, 0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000, 0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275, 0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922, 0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294, 0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098, 0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922, 0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922, 0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706, 0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922, 0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922, 0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]), 5) Calling the first record from train_data returns a two-item tuple. The first item is our 28x28 tensor representing the image. The second is a label, in this case the number \u201c5\u201d. image , label = train_data [ 0 ] print ( 'Shape:' , image . shape , ' \\n Label:' , label ) Shape: torch.Size([1, 28, 28]) Label: 5 View the image \u00b6 Matplotlib can interpret pixel values through a variety of colormaps . plt . imshow ( train_data [ 0 ][ 0 ] . reshape (( 28 , 28 )), cmap = \"gray\" ); plt . imshow ( train_data [ 0 ][ 0 ] . reshape (( 28 , 28 )), cmap = \"gist_yarg\" ); Batch loading with DataLoader \u00b6 Our training set contains 60,000 records. If we look ahead to our model we have 784 incoming features, hidden layers of 120 and 84 neurons, and 10 output features. Including the bias terms for each layer, the total number of parameters being trained is: \\(\\begin{split}\\quad(784\\times120)+120+(120\\times84)+84+(84\\times10)+10 &=\\\\ 94080+120+10080+84+840+10 &= 105,214\\end{split}\\) For this reason it makes sense to load training data in batches using DataLoader . torch . manual_seed ( 101 ) # for consistent results train_loader = DataLoader ( train_data , batch_size = 100 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 500 , shuffle = False ) In the cell above, train_data is a PyTorch Dataset object (an object that supports data loading and sampling). The batch_size is the number of records to be processed at a time. If it\u2019s not evenly divisible into the dataset, then the final batch contains the remainder. Setting shuffle to True means that the dataset will be shuffled after each epoch. NOTE: DataLoader takes an optional num_workers parameter that sets up how many subprocesses to use for data loading. This behaves differently with different operating systems so we\u2019ve omitted it here. See the docs for more information. View a batch of images \u00b6 Once we\u2019ve defined a DataLoader, we can create a grid of images using torchvision.utils.make_grid from torchvision.utils import make_grid np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 4 } ' )) # to widen the printed array # Grab the first batch of images for images , labels in train_loader : break # Print the first 12 labels print ( 'Labels: ' , labels [: 12 ] . numpy ()) # Print the first 12 images im = make_grid ( images [: 12 ], nrow = 12 ) # the default nrow is 8 plt . figure ( figsize = ( 10 , 4 )) # We need to transpose the images from CWH to WHC plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Labels: [ 0 5 7 8 6 7 9 7 1 3 8 4] Define the model \u00b6 For this exercise we\u2019ll use fully connected layers to develop a multilayer perceptron . Our input size is 784 once we flatten the incoming 28x28 tensors. Our output size represents the 10 possible digits. We\u2019ll set our hidden layers to [120, 84] for now. Once you\u2019ve completed the exercise feel free to come back and try different values. class MultilayerPerceptron ( nn . Module ): def __init__ ( self , in_sz = 784 , out_sz = 10 , layers = [ 120 , 84 ]): super () . __init__ () self . fc1 = nn . Linear ( in_sz , layers [ 0 ]) self . fc2 = nn . Linear ( layers [ 0 ], layers [ 1 ]) self . fc3 = nn . Linear ( layers [ 1 ], out_sz ) def forward ( self , X ): X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) torch . manual_seed ( 101 ) model = MultilayerPerceptron () model MultilayerPerceptron( (fc1): Linear(in_features=784, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) NOTE: You may have noticed our shortcut for adding ReLU to the linear layer. In the last section this was done under the **init** section as layerlist = [] for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) self.layers = nn.Sequential(*layerlist) Here we\u2019re calling F.relu() as a functional wrapper on the linear layer directly: def forward(self,X): X = F.relu(self.fc1(X)) Count the model parameters \u00b6 This optional step shows that the number of trainable parameters in our model matches the equation above. def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 94080 120 10080 84 840 10 ______ 105214 Define loss function & optimizer \u00b6 criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) Flatten the training data \u00b6 The batch tensors fed in by DataLoader have a shape of [100, 1, 28, 28]: # Load the first batch, print its shape for images , labels in train_loader : print ( 'Batch shape:' , images . size ()) break # EQUIVALENT TO: # dataiter = iter(train_loader) # images, labels = dataiter.next() # print('Batch shape:', images.size()) Batch shape: torch.Size([100, 1, 28, 28]) We can flatten them using .view() images . view ( 100 , - 1 ) . size () torch.Size([100, 784]) We\u2019ll do this just before applying the model to our data. Train the model \u00b6 This time we\u2019ll run the test data through the model during each epoch, so that we can compare loss & accuracy on the same plot. A QUICK NOTE: In the section below marked \\#Tally the number of correct predictions we include the code predicted = torch.max(y_pred.data, 1)[1] This uses the torch.max() function. torch.max() returns a tensor of maximum values, and a tensor of the indices where the max values were found. In our code we\u2019re asking for the index positions of the maximum values along dimension 1. In this way we can match predictions up to image labels. import time start_time = time . time () epochs = 10 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): b += 1 # Apply the model y_pred = model ( X_train . view ( 100 , - 1 )) # Here we flatten X_train loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 200 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 100 * b : 6 } /60000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 100 * b ) : 7.3f } %' ) # Update train loss & accuracy for the epoch train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): # Apply the model y_val = model ( X_test . view ( 500 , - 1 )) # Here we flatten X_test # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () # Update test loss & accuracy for the epoch loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 200 [ 20000/60000] loss: 0.35221729 accuracy: 82.695% epoch: 0 batch: 400 [ 40000/60000] loss: 0.32761699 accuracy: 87.340% epoch: 0 batch: 600 [ 60000/60000] loss: 0.31156573 accuracy: 89.490% epoch: 1 batch: 200 [ 20000/60000] loss: 0.20120722 accuracy: 94.800% epoch: 1 batch: 400 [ 40000/60000] loss: 0.14656080 accuracy: 95.185% epoch: 1 batch: 600 [ 60000/60000] loss: 0.12691295 accuracy: 95.478% epoch: 2 batch: 200 [ 20000/60000] loss: 0.13621402 accuracy: 96.815% epoch: 2 batch: 400 [ 40000/60000] loss: 0.07235763 accuracy: 96.790% epoch: 2 batch: 600 [ 60000/60000] loss: 0.04241359 accuracy: 96.878% epoch: 3 batch: 200 [ 20000/60000] loss: 0.09474990 accuracy: 97.635% epoch: 3 batch: 400 [ 40000/60000] loss: 0.06394162 accuracy: 97.600% epoch: 3 batch: 600 [ 60000/60000] loss: 0.07836709 accuracy: 97.562% epoch: 4 batch: 200 [ 20000/60000] loss: 0.05509195 accuracy: 98.135% epoch: 4 batch: 400 [ 40000/60000] loss: 0.06395346 accuracy: 98.125% epoch: 4 batch: 600 [ 60000/60000] loss: 0.05392118 accuracy: 98.105% epoch: 5 batch: 200 [ 20000/60000] loss: 0.03487724 accuracy: 98.515% epoch: 5 batch: 400 [ 40000/60000] loss: 0.03120600 accuracy: 98.433% epoch: 5 batch: 600 [ 60000/60000] loss: 0.03449132 accuracy: 98.402% epoch: 6 batch: 200 [ 20000/60000] loss: 0.04473587 accuracy: 98.770% epoch: 6 batch: 400 [ 40000/60000] loss: 0.05389304 accuracy: 98.770% epoch: 6 batch: 600 [ 60000/60000] loss: 0.04762774 accuracy: 98.685% epoch: 7 batch: 200 [ 20000/60000] loss: 0.01370908 accuracy: 98.885% epoch: 7 batch: 400 [ 40000/60000] loss: 0.01426961 accuracy: 98.945% epoch: 7 batch: 600 [ 60000/60000] loss: 0.04490321 accuracy: 98.902% epoch: 8 batch: 200 [ 20000/60000] loss: 0.02279496 accuracy: 99.150% epoch: 8 batch: 400 [ 40000/60000] loss: 0.03816750 accuracy: 99.060% epoch: 8 batch: 600 [ 60000/60000] loss: 0.02311455 accuracy: 99.055% epoch: 9 batch: 200 [ 20000/60000] loss: 0.01244260 accuracy: 99.330% epoch: 9 batch: 400 [ 40000/60000] loss: 0.00740430 accuracy: 99.340% epoch: 9 batch: 600 [ 60000/60000] loss: 0.01638621 accuracy: 99.280% Duration: 275 seconds Plot the loss and accuracy comparisons \u00b6 plt . plot ( train_losses , label = 'training loss' ) plt . plot ( test_losses , label = 'validation loss' ) plt . title ( 'Loss at the end of each epoch' ) plt . legend (); This shows some evidence of overfitting the training data. plt . plot ([ t / 600 for t in train_correct ], label = 'training accuracy' ) plt . plot ([ t / 100 for t in test_correct ], label = 'validation accuracy' ) plt . title ( 'Accuracy at the end of each epoch' ) plt . legend (); Evaluate Test Data \u00b6 We retained the test scores during our training session: print ( test_correct ) # contains the results of all 10 epochs print () print ( f 'Test accuracy: { test_correct [ - 1 ] . item () * 100 / 10000 : .3f } %' ) # print the most recent result as a percent [tensor(9439), tensor(9635), tensor(9666), tensor(9726), tensor(9746), tensor(9758), tensor(9737), tensor(9749), tensor(9746), tensor(9725)] Test accuracy: 97.250% However, we\u2019d like to compare the predicted values to the ground truth (the y_test labels), so we\u2019ll run the test set through the trained model all at once. # Extract the data all at once, not in batches test_load_all = DataLoader ( test_data , batch_size = 10000 , shuffle = False ) with torch . no_grad (): correct = 0 for X_test , y_test in test_load_all : y_val = model ( X_test . view ( len ( X_test ), - 1 )) # pass in a flattened view of X_test predicted = torch . max ( y_val , 1 )[ 1 ] correct += ( predicted == y_test ) . sum () print ( f 'Test accuracy: { correct . item () } / { len ( test_data ) } = { correct . item () * 100 / ( len ( test_data )) : 7.3f } %' ) Test accuracy: 9725/10000 = 97.250% Not bad considering that a random guess gives only 10% accuracy! Display the confusion matrix \u00b6 This uses scikit-learn, and the predicted values obtained above. # print a row of values for reference np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 4 } ' )) print ( np . arange ( 10 ) . reshape ( 1 , 10 )) print () # print the confusion matrix print ( confusion_matrix ( predicted . view ( - 1 ), y_test . view ( - 1 ))) [[ 0 1 2 3 4 5 6 7 8 9]] [[ 968 0 1 0 1 2 5 0 4 1] [ 0 1126 4 0 0 0 2 5 0 4] [ 2 1 1007 4 3 0 0 10 5 0] [ 1 0 5 985 0 2 1 3 4 7] [ 1 0 1 0 962 1 3 1 3 12] [ 2 1 1 13 0 882 33 1 23 16] [ 1 2 2 0 5 1 913 0 1 0] [ 1 1 5 5 3 2 0 1003 7 7] [ 2 4 6 2 1 2 1 1 925 8] [ 2 0 0 1 7 0 0 4 2 954]] This shows that the model had the greatest success with ones, twos and sevens, and the lowest with fives, sixes and eights. Examine the misses \u00b6 We can track the index positions of \u201cmissed\u201d predictions, and extract the corresponding image and label. We\u2019ll do this in batches to save screen space. misses = np . array ([]) for i in range ( len ( predicted . view ( - 1 ))): if predicted [ i ] != y_test [ i ]: misses = np . append ( misses , i ) . astype ( 'int64' ) # Display the number of misses len ( misses ) 275 # Display the first 10 index positions misses [: 10 ] array([ 61, 62, 81, 104, 115, 151, 193, 217, 247, 259], dtype=int64) # Set up an iterator to feed batched rows r = 12 # row size row = iter ( np . array_split ( misses , len ( misses ) // r + 1 )) Now that everything is set up, run and re-run the cell below to view all of the missed predictions. Use Ctrl+Enter to remain on the cell between runs. You\u2019ll see a StopIteration once all the misses have been seen. nextrow = next ( row ) print ( \"Index:\" , nextrow ) print ( \"Label:\" , y_test . index_select ( 0 , torch . tensor ( nextrow )) . numpy ()) print ( \"Guess:\" , predicted . index_select ( 0 , torch . tensor ( nextrow )) . numpy ()) images = X_test . index_select ( 0 , torch . tensor ( nextrow )) im = make_grid ( images , nrow = r ) plt . figure ( figsize = ( 10 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Index: [ 61 62 81 104 115 151 193 217 247 259 264 320] Label: [ 8 9 6 9 4 9 9 6 4 6 9 9] Guess: [ 2 5 5 5 9 8 8 5 6 0 4 8] Great job! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"00 MNIST ANN Code Along"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#mnist-code-along-with-ann","text":"Before we start working with Convolutional Neural Networks (CNN), let\u2019s model the MNIST dataset using only linear layers. In this exercise we\u2019ll use the same logic laid out in the ANN notebook. We\u2019ll reshape the MNIST data from a 28x28 image to a flattened 1x784 vector to mimic a single row of 784 features.","title":"MNIST Code Along with ANN"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#perform-standard-imports","text":"Torchvision should have been installed by the environment file during setup. If not, you can install it now. At the terminal with your virtual environment activated, run conda install torchvision -c pytorch or pip install torchvision import torch import torch.nn as nn import torch.nn.functional as F # adds some efficiency from torch.utils.data import DataLoader # lets us load data in batches from torchvision import datasets , transforms import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix # for evaluating results import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#load-the-mnist-dataset","text":"PyTorch makes the MNIST dataset available through torchvision . The first time it\u2019s called, the dataset will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. ### Define transform As part of the loading process, we can apply multiple transformations (reshape, convert to tensor, normalize, etc.) to the incoming data. For this exercise we only need to convert images to tensors. transform = transforms . ToTensor ()","title":"Load the MNIST dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#load-the-training-set","text":"train_data = datasets . MNIST ( root = '../Data' , train = True , download = True , transform = transform ) train_data Dataset MNIST Number of datapoints: 60000 Split: train Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None","title":"Load the training set"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#load-the-test-set","text":"There\u2019s a companion set of MNIST data containing 10,000 records accessible by setting train=False. As before, torchvision will only download this once, and in the future will look for the local copy. test_data = datasets . MNIST ( root = '../Data' , train = False , download = True , transform = transform ) test_data Dataset MNIST Number of datapoints: 10000 Split: test Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None","title":"Load the test set"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#examine-a-training-record","text":"train_data [ 0 ] (tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706, 0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137, 0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000, 0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275, 0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922, 0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294, 0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098, 0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922, 0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922, 0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706, 0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922, 0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922, 0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]), 5) Calling the first record from train_data returns a two-item tuple. The first item is our 28x28 tensor representing the image. The second is a label, in this case the number \u201c5\u201d. image , label = train_data [ 0 ] print ( 'Shape:' , image . shape , ' \\n Label:' , label ) Shape: torch.Size([1, 28, 28]) Label: 5","title":"Examine a training record"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#view-the-image","text":"Matplotlib can interpret pixel values through a variety of colormaps . plt . imshow ( train_data [ 0 ][ 0 ] . reshape (( 28 , 28 )), cmap = \"gray\" ); plt . imshow ( train_data [ 0 ][ 0 ] . reshape (( 28 , 28 )), cmap = \"gist_yarg\" );","title":"View the image"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#batch-loading-with-dataloader","text":"Our training set contains 60,000 records. If we look ahead to our model we have 784 incoming features, hidden layers of 120 and 84 neurons, and 10 output features. Including the bias terms for each layer, the total number of parameters being trained is: \\(\\begin{split}\\quad(784\\times120)+120+(120\\times84)+84+(84\\times10)+10 &=\\\\ 94080+120+10080+84+840+10 &= 105,214\\end{split}\\) For this reason it makes sense to load training data in batches using DataLoader . torch . manual_seed ( 101 ) # for consistent results train_loader = DataLoader ( train_data , batch_size = 100 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 500 , shuffle = False ) In the cell above, train_data is a PyTorch Dataset object (an object that supports data loading and sampling). The batch_size is the number of records to be processed at a time. If it\u2019s not evenly divisible into the dataset, then the final batch contains the remainder. Setting shuffle to True means that the dataset will be shuffled after each epoch. NOTE: DataLoader takes an optional num_workers parameter that sets up how many subprocesses to use for data loading. This behaves differently with different operating systems so we\u2019ve omitted it here. See the docs for more information.","title":"Batch loading with DataLoader"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#view-a-batch-of-images","text":"Once we\u2019ve defined a DataLoader, we can create a grid of images using torchvision.utils.make_grid from torchvision.utils import make_grid np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 4 } ' )) # to widen the printed array # Grab the first batch of images for images , labels in train_loader : break # Print the first 12 labels print ( 'Labels: ' , labels [: 12 ] . numpy ()) # Print the first 12 images im = make_grid ( images [: 12 ], nrow = 12 ) # the default nrow is 8 plt . figure ( figsize = ( 10 , 4 )) # We need to transpose the images from CWH to WHC plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Labels: [ 0 5 7 8 6 7 9 7 1 3 8 4]","title":"View a batch of images"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#define-the-model","text":"For this exercise we\u2019ll use fully connected layers to develop a multilayer perceptron . Our input size is 784 once we flatten the incoming 28x28 tensors. Our output size represents the 10 possible digits. We\u2019ll set our hidden layers to [120, 84] for now. Once you\u2019ve completed the exercise feel free to come back and try different values. class MultilayerPerceptron ( nn . Module ): def __init__ ( self , in_sz = 784 , out_sz = 10 , layers = [ 120 , 84 ]): super () . __init__ () self . fc1 = nn . Linear ( in_sz , layers [ 0 ]) self . fc2 = nn . Linear ( layers [ 0 ], layers [ 1 ]) self . fc3 = nn . Linear ( layers [ 1 ], out_sz ) def forward ( self , X ): X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) torch . manual_seed ( 101 ) model = MultilayerPerceptron () model MultilayerPerceptron( (fc1): Linear(in_features=784, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) NOTE: You may have noticed our shortcut for adding ReLU to the linear layer. In the last section this was done under the **init** section as layerlist = [] for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) self.layers = nn.Sequential(*layerlist) Here we\u2019re calling F.relu() as a functional wrapper on the linear layer directly: def forward(self,X): X = F.relu(self.fc1(X))","title":"Define the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#count-the-model-parameters","text":"This optional step shows that the number of trainable parameters in our model matches the equation above. def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 94080 120 10080 84 840 10 ______ 105214","title":"Count the model parameters"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#define-loss-function-optimizer","text":"criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#flatten-the-training-data","text":"The batch tensors fed in by DataLoader have a shape of [100, 1, 28, 28]: # Load the first batch, print its shape for images , labels in train_loader : print ( 'Batch shape:' , images . size ()) break # EQUIVALENT TO: # dataiter = iter(train_loader) # images, labels = dataiter.next() # print('Batch shape:', images.size()) Batch shape: torch.Size([100, 1, 28, 28]) We can flatten them using .view() images . view ( 100 , - 1 ) . size () torch.Size([100, 784]) We\u2019ll do this just before applying the model to our data.","title":"Flatten the training data"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#train-the-model","text":"This time we\u2019ll run the test data through the model during each epoch, so that we can compare loss & accuracy on the same plot. A QUICK NOTE: In the section below marked \\#Tally the number of correct predictions we include the code predicted = torch.max(y_pred.data, 1)[1] This uses the torch.max() function. torch.max() returns a tensor of maximum values, and a tensor of the indices where the max values were found. In our code we\u2019re asking for the index positions of the maximum values along dimension 1. In this way we can match predictions up to image labels. import time start_time = time . time () epochs = 10 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): b += 1 # Apply the model y_pred = model ( X_train . view ( 100 , - 1 )) # Here we flatten X_train loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 200 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 100 * b : 6 } /60000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 100 * b ) : 7.3f } %' ) # Update train loss & accuracy for the epoch train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): # Apply the model y_val = model ( X_test . view ( 500 , - 1 )) # Here we flatten X_test # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () # Update test loss & accuracy for the epoch loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 200 [ 20000/60000] loss: 0.35221729 accuracy: 82.695% epoch: 0 batch: 400 [ 40000/60000] loss: 0.32761699 accuracy: 87.340% epoch: 0 batch: 600 [ 60000/60000] loss: 0.31156573 accuracy: 89.490% epoch: 1 batch: 200 [ 20000/60000] loss: 0.20120722 accuracy: 94.800% epoch: 1 batch: 400 [ 40000/60000] loss: 0.14656080 accuracy: 95.185% epoch: 1 batch: 600 [ 60000/60000] loss: 0.12691295 accuracy: 95.478% epoch: 2 batch: 200 [ 20000/60000] loss: 0.13621402 accuracy: 96.815% epoch: 2 batch: 400 [ 40000/60000] loss: 0.07235763 accuracy: 96.790% epoch: 2 batch: 600 [ 60000/60000] loss: 0.04241359 accuracy: 96.878% epoch: 3 batch: 200 [ 20000/60000] loss: 0.09474990 accuracy: 97.635% epoch: 3 batch: 400 [ 40000/60000] loss: 0.06394162 accuracy: 97.600% epoch: 3 batch: 600 [ 60000/60000] loss: 0.07836709 accuracy: 97.562% epoch: 4 batch: 200 [ 20000/60000] loss: 0.05509195 accuracy: 98.135% epoch: 4 batch: 400 [ 40000/60000] loss: 0.06395346 accuracy: 98.125% epoch: 4 batch: 600 [ 60000/60000] loss: 0.05392118 accuracy: 98.105% epoch: 5 batch: 200 [ 20000/60000] loss: 0.03487724 accuracy: 98.515% epoch: 5 batch: 400 [ 40000/60000] loss: 0.03120600 accuracy: 98.433% epoch: 5 batch: 600 [ 60000/60000] loss: 0.03449132 accuracy: 98.402% epoch: 6 batch: 200 [ 20000/60000] loss: 0.04473587 accuracy: 98.770% epoch: 6 batch: 400 [ 40000/60000] loss: 0.05389304 accuracy: 98.770% epoch: 6 batch: 600 [ 60000/60000] loss: 0.04762774 accuracy: 98.685% epoch: 7 batch: 200 [ 20000/60000] loss: 0.01370908 accuracy: 98.885% epoch: 7 batch: 400 [ 40000/60000] loss: 0.01426961 accuracy: 98.945% epoch: 7 batch: 600 [ 60000/60000] loss: 0.04490321 accuracy: 98.902% epoch: 8 batch: 200 [ 20000/60000] loss: 0.02279496 accuracy: 99.150% epoch: 8 batch: 400 [ 40000/60000] loss: 0.03816750 accuracy: 99.060% epoch: 8 batch: 600 [ 60000/60000] loss: 0.02311455 accuracy: 99.055% epoch: 9 batch: 200 [ 20000/60000] loss: 0.01244260 accuracy: 99.330% epoch: 9 batch: 400 [ 40000/60000] loss: 0.00740430 accuracy: 99.340% epoch: 9 batch: 600 [ 60000/60000] loss: 0.01638621 accuracy: 99.280% Duration: 275 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#plot-the-loss-and-accuracy-comparisons","text":"plt . plot ( train_losses , label = 'training loss' ) plt . plot ( test_losses , label = 'validation loss' ) plt . title ( 'Loss at the end of each epoch' ) plt . legend (); This shows some evidence of overfitting the training data. plt . plot ([ t / 600 for t in train_correct ], label = 'training accuracy' ) plt . plot ([ t / 100 for t in test_correct ], label = 'validation accuracy' ) plt . title ( 'Accuracy at the end of each epoch' ) plt . legend ();","title":"Plot the loss and accuracy comparisons"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#evaluate-test-data","text":"We retained the test scores during our training session: print ( test_correct ) # contains the results of all 10 epochs print () print ( f 'Test accuracy: { test_correct [ - 1 ] . item () * 100 / 10000 : .3f } %' ) # print the most recent result as a percent [tensor(9439), tensor(9635), tensor(9666), tensor(9726), tensor(9746), tensor(9758), tensor(9737), tensor(9749), tensor(9746), tensor(9725)] Test accuracy: 97.250% However, we\u2019d like to compare the predicted values to the ground truth (the y_test labels), so we\u2019ll run the test set through the trained model all at once. # Extract the data all at once, not in batches test_load_all = DataLoader ( test_data , batch_size = 10000 , shuffle = False ) with torch . no_grad (): correct = 0 for X_test , y_test in test_load_all : y_val = model ( X_test . view ( len ( X_test ), - 1 )) # pass in a flattened view of X_test predicted = torch . max ( y_val , 1 )[ 1 ] correct += ( predicted == y_test ) . sum () print ( f 'Test accuracy: { correct . item () } / { len ( test_data ) } = { correct . item () * 100 / ( len ( test_data )) : 7.3f } %' ) Test accuracy: 9725/10000 = 97.250% Not bad considering that a random guess gives only 10% accuracy!","title":"Evaluate Test Data"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#display-the-confusion-matrix","text":"This uses scikit-learn, and the predicted values obtained above. # print a row of values for reference np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 4 } ' )) print ( np . arange ( 10 ) . reshape ( 1 , 10 )) print () # print the confusion matrix print ( confusion_matrix ( predicted . view ( - 1 ), y_test . view ( - 1 ))) [[ 0 1 2 3 4 5 6 7 8 9]] [[ 968 0 1 0 1 2 5 0 4 1] [ 0 1126 4 0 0 0 2 5 0 4] [ 2 1 1007 4 3 0 0 10 5 0] [ 1 0 5 985 0 2 1 3 4 7] [ 1 0 1 0 962 1 3 1 3 12] [ 2 1 1 13 0 882 33 1 23 16] [ 1 2 2 0 5 1 913 0 1 0] [ 1 1 5 5 3 2 0 1003 7 7] [ 2 4 6 2 1 2 1 1 925 8] [ 2 0 0 1 7 0 0 4 2 954]] This shows that the model had the greatest success with ones, twos and sevens, and the lowest with fives, sixes and eights.","title":"Display the confusion matrix"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#examine-the-misses","text":"We can track the index positions of \u201cmissed\u201d predictions, and extract the corresponding image and label. We\u2019ll do this in batches to save screen space. misses = np . array ([]) for i in range ( len ( predicted . view ( - 1 ))): if predicted [ i ] != y_test [ i ]: misses = np . append ( misses , i ) . astype ( 'int64' ) # Display the number of misses len ( misses ) 275 # Display the first 10 index positions misses [: 10 ] array([ 61, 62, 81, 104, 115, 151, 193, 217, 247, 259], dtype=int64) # Set up an iterator to feed batched rows r = 12 # row size row = iter ( np . array_split ( misses , len ( misses ) // r + 1 )) Now that everything is set up, run and re-run the cell below to view all of the missed predictions. Use Ctrl+Enter to remain on the cell between runs. You\u2019ll see a StopIteration once all the misses have been seen. nextrow = next ( row ) print ( \"Index:\" , nextrow ) print ( \"Label:\" , y_test . index_select ( 0 , torch . tensor ( nextrow )) . numpy ()) print ( \"Guess:\" , predicted . index_select ( 0 , torch . tensor ( nextrow )) . numpy ()) images = X_test . index_select ( 0 , torch . tensor ( nextrow )) im = make_grid ( images , nrow = r ) plt . figure ( figsize = ( 10 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Index: [ 61 62 81 104 115 151 193 217 247 259 264 320] Label: [ 8 9 6 9 4 9 9 6 4 6 9 9] Guess: [ 2 5 5 5 9 8 8 5 6 0 4 8]","title":"Examine the misses"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/#great-job","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/","text":"================ by Jawad Haider MNIST Code Along with CNN Perform standard imports Load the MNIST dataset Create loaders Define a convolutional model Define loss function & optimizer Train the model Plot the loss and accuracy comparisons Evaluate Test Data Display the confusion matrix Examine the misses Run a new image through the model Great job! MNIST Code Along with CNN \u00b6 Now that we\u2019ve seen the results of an artificial neural network model on the MNIST dataset , let\u2019s work the same data with a Convolutional Neural Network (CNN). Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * convolutional layers * filters/kernels * pooling * depth, stride and zero-padding Note that in this exercise there is no need to flatten the MNIST data, as a CNN expects 2-dimensional data. Perform standard imports \u00b6 import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms from torchvision.utils import make_grid import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt % matplotlib inline Load the MNIST dataset \u00b6 PyTorch makes the MNIST train and test datasets available through torchvision . The first time they\u2019re called, the datasets will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. Refer to the previous section for explanations of transformations, batch sizes and DataLoader . transform = transforms . ToTensor () train_data = datasets . MNIST ( root = '../Data' , train = True , download = True , transform = transform ) test_data = datasets . MNIST ( root = '../Data' , train = False , download = True , transform = transform ) train_data Dataset MNIST Number of datapoints: 60000 Split: train Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None test_data Dataset MNIST Number of datapoints: 10000 Split: test Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None Create loaders \u00b6 When working with images, we want relatively small batches; a batch size of 4 is not uncommon. train_loader = DataLoader ( train_data , batch_size = 10 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 10 , shuffle = False ) Define a convolutional model \u00b6 In the previous section we used only fully connected layers, with an input layer of 784 (our flattened 28x28 images), hidden layers of 120 and 84 neurons, and an output size representing 10 possible digits. This time we\u2019ll employ two convolutional layers and two pooling layers before feeding data through fully connected hidden layers to our output. The model follows CONV/RELU/POOL/CONV/RELU/POOL/FC/RELU/FC. Let\u2019s walk through the steps we\u2019re about to take. 1. Extend the base Module class: class ConvolutionalNetwork(nn.Module): def \\_\\_init\\_\\_(self): super().\\_\\_init\\_\\_() 2. Set up the convolutional layers with torch.nn.Conv2d() The first layer has one input channel (the grayscale color channel). We\u2019ll assign 6 output channels for feature extraction. We\u2019ll set our kernel size to 3 to make a 3x3 filter, and set the step size to 1. self.conv1 = nn.Conv2d(1, 6, 3, 1) The second layer will take our 6 input channels and deliver 16 output channels. self.conv2 = nn.Conv2d(6, 16, 3, 1) 3. Set up the fully connected layers with torch.nn.Linear() . The input size of (5x5x16) is determined by the effect of our kernels on the input image size. A 3x3 filter applied to a 28x28 image leaves a 1-pixel edge on all four sides. In one layer the size changes from 28x28 to 26x26. We could address this with zero-padding, but since an MNIST image is mostly black at the edges, we should be safe ignoring these pixels. We\u2019ll apply the kernel twice, and apply pooling layers twice, so our resulting output will be $\\;(((28-2)/2)-2)/2 = 5.5\\;$ which rounds down to 5 pixels per side. self.fc1 = nn.Linear(5\\*5\\*16, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) See below for a more detailed look at this step. 4. Define the forward method. Activations can be applied to the convolutions in one line using F.relu() and pooling is done using F.max_pool2d() def forward(self, X): X = F.relu(self.conv1(X)) X = F.max_pool2d(X, 2, 2) X = F.relu(self.conv2(X)) X = F.max_pool2d(X, 2, 2) Flatten the data for the fully connected layers: X = X.view(-1, 5\\*5\\*16) X = F.relu(self.fc1(X)) X = self.fc2(X) return F.log_softmax(X, dim=1) Breaking down the convolutional layers (this code is for illustration purposes only.) # Define layers conv1 = nn . Conv2d ( 1 , 6 , 3 , 1 ) conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) # Grab the first MNIST record for i , ( X_train , y_train ) in enumerate ( train_data ): break # Create a rank-4 tensor to be passed into the model # (train_loader will have done this already) x = X_train . view ( 1 , 1 , 28 , 28 ) print ( x . shape ) torch.Size([1, 1, 28, 28]) # Perform the first convolution/activation x = F . relu ( conv1 ( x )) print ( x . shape ) torch.Size([1, 6, 26, 26]) # Run the first pooling layer x = F . max_pool2d ( x , 2 , 2 ) print ( x . shape ) torch.Size([1, 6, 13, 13]) # Perform the second convolution/activation x = F . relu ( conv2 ( x )) print ( x . shape ) torch.Size([1, 16, 11, 11]) # Run the second pooling layer x = F . max_pool2d ( x , 2 , 2 ) print ( x . shape ) torch.Size([1, 16, 5, 5]) # Flatten the data x = x . view ( - 1 , 5 * 5 * 16 ) print ( x . shape ) torch.Size([1, 400]) This is how the convolution output is passed into the fully connected layers. Now let\u2019s run the code. class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 1 , 6 , 3 , 1 ) self . conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) self . fc1 = nn . Linear ( 5 * 5 * 16 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , X ): X = F . relu ( self . conv1 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = F . relu ( self . conv2 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = X . view ( - 1 , 5 * 5 * 16 ) X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) torch . manual_seed ( 42 ) model = ConvolutionalNetwork () model ConvolutionalNetwork( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) Including the bias terms for each layer, the total number of parameters being trained is: \\(\\quad\\begin{split}(1\\times6\\times3\\times3)+6+(6\\times16\\times3\\times3)+16+(400\\times120)+120+(120\\times84)+84+(84\\times10)+10 &=\\\\ 54+6+864+16+48000+120+10080+84+840+10 &= 60,074\\end{split}\\) def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 54 6 864 16 48000 120 10080 84 840 10 ______ 60074 Define loss function & optimizer \u00b6 criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) Train the model \u00b6 This time we\u2019ll feed the data directly into the model without flattening it first. import time start_time = time . time () epochs = 5 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): b += 1 # Apply the model y_pred = model ( X_train ) # we don't flatten X-train here loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 600 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 10 * b : 6 } /60000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 10 * b ) : 7.3f } %' ) train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): # Apply the model y_val = model ( X_test ) # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 600 [ 6000/60000] loss: 0.21188490 accuracy: 78.233% epoch: 0 batch: 1200 [ 12000/60000] loss: 0.58768761 accuracy: 85.433% epoch: 0 batch: 1800 [ 18000/60000] loss: 0.03002630 accuracy: 88.539% epoch: 0 batch: 2400 [ 24000/60000] loss: 0.02856987 accuracy: 90.396% epoch: 0 batch: 3000 [ 30000/60000] loss: 0.01619262 accuracy: 91.543% epoch: 0 batch: 3600 [ 36000/60000] loss: 0.00392615 accuracy: 92.347% epoch: 0 batch: 4200 [ 42000/60000] loss: 0.07892600 accuracy: 92.938% epoch: 0 batch: 4800 [ 48000/60000] loss: 0.00173595 accuracy: 93.458% epoch: 0 batch: 5400 [ 54000/60000] loss: 0.00021752 accuracy: 93.889% epoch: 0 batch: 6000 [ 60000/60000] loss: 0.00123056 accuracy: 94.245% epoch: 1 batch: 600 [ 6000/60000] loss: 0.03455487 accuracy: 97.967% epoch: 1 batch: 1200 [ 12000/60000] loss: 0.25245315 accuracy: 97.792% epoch: 1 batch: 1800 [ 18000/60000] loss: 0.15286988 accuracy: 97.833% epoch: 1 batch: 2400 [ 24000/60000] loss: 0.00420426 accuracy: 97.875% epoch: 1 batch: 3000 [ 30000/60000] loss: 0.00133034 accuracy: 97.897% epoch: 1 batch: 3600 [ 36000/60000] loss: 0.10122252 accuracy: 97.922% epoch: 1 batch: 4200 [ 42000/60000] loss: 0.03956039 accuracy: 97.931% epoch: 1 batch: 4800 [ 48000/60000] loss: 0.15445584 accuracy: 97.950% epoch: 1 batch: 5400 [ 54000/60000] loss: 0.02514876 accuracy: 97.998% epoch: 1 batch: 6000 [ 60000/60000] loss: 0.01102044 accuracy: 97.990% epoch: 2 batch: 600 [ 6000/60000] loss: 0.00138958 accuracy: 98.667% epoch: 2 batch: 1200 [ 12000/60000] loss: 0.00090325 accuracy: 98.533% epoch: 2 batch: 1800 [ 18000/60000] loss: 0.00036748 accuracy: 98.567% epoch: 2 batch: 2400 [ 24000/60000] loss: 0.00072705 accuracy: 98.487% epoch: 2 batch: 3000 [ 30000/60000] loss: 0.00448279 accuracy: 98.460% epoch: 2 batch: 3600 [ 36000/60000] loss: 0.00007090 accuracy: 98.508% epoch: 2 batch: 4200 [ 42000/60000] loss: 0.00142251 accuracy: 98.500% epoch: 2 batch: 4800 [ 48000/60000] loss: 0.00054714 accuracy: 98.473% epoch: 2 batch: 5400 [ 54000/60000] loss: 0.00036345 accuracy: 98.493% epoch: 2 batch: 6000 [ 60000/60000] loss: 0.00005013 accuracy: 98.515% epoch: 3 batch: 600 [ 6000/60000] loss: 0.00073732 accuracy: 99.117% epoch: 3 batch: 1200 [ 12000/60000] loss: 0.01391867 accuracy: 98.933% epoch: 3 batch: 1800 [ 18000/60000] loss: 0.00901531 accuracy: 98.806% epoch: 3 batch: 2400 [ 24000/60000] loss: 0.00081765 accuracy: 98.846% epoch: 3 batch: 3000 [ 30000/60000] loss: 0.00001284 accuracy: 98.880% epoch: 3 batch: 3600 [ 36000/60000] loss: 0.00020319 accuracy: 98.886% epoch: 3 batch: 4200 [ 42000/60000] loss: 0.00093818 accuracy: 98.879% epoch: 3 batch: 4800 [ 48000/60000] loss: 0.00241654 accuracy: 98.917% epoch: 3 batch: 5400 [ 54000/60000] loss: 0.00376742 accuracy: 98.893% epoch: 3 batch: 6000 [ 60000/60000] loss: 0.00199913 accuracy: 98.863% epoch: 4 batch: 600 [ 6000/60000] loss: 0.00138756 accuracy: 99.183% epoch: 4 batch: 1200 [ 12000/60000] loss: 0.10804896 accuracy: 99.075% epoch: 4 batch: 1800 [ 18000/60000] loss: 0.00437851 accuracy: 99.156% epoch: 4 batch: 2400 [ 24000/60000] loss: 0.00001171 accuracy: 99.104% epoch: 4 batch: 3000 [ 30000/60000] loss: 0.00447276 accuracy: 99.050% epoch: 4 batch: 3600 [ 36000/60000] loss: 0.00048421 accuracy: 99.061% epoch: 4 batch: 4200 [ 42000/60000] loss: 0.00056122 accuracy: 99.043% epoch: 4 batch: 4800 [ 48000/60000] loss: 0.00057514 accuracy: 99.033% epoch: 4 batch: 5400 [ 54000/60000] loss: 0.07913177 accuracy: 99.028% epoch: 4 batch: 6000 [ 60000/60000] loss: 0.00030920 accuracy: 99.027% Duration: 253 seconds Plot the loss and accuracy comparisons \u00b6 plt . plot ( train_losses , label = 'training loss' ) plt . plot ( test_losses , label = 'validation loss' ) plt . title ( 'Loss at the end of each epoch' ) plt . legend (); test_losses [tensor(0.0067), tensor(0.0011), tensor(2.5534e-05), tensor(9.3650e-05), tensor(0.0005)] While there may be some overfitting of the training data, there is far less than we saw with the ANN model. plt . plot ([ t / 600 for t in train_correct ], label = 'training accuracy' ) plt . plot ([ t / 100 for t in test_correct ], label = 'validation accuracy' ) plt . title ( 'Accuracy at the end of each epoch' ) plt . legend (); Evaluate Test Data \u00b6 # Extract the data all at once, not in batches test_load_all = DataLoader ( test_data , batch_size = 10000 , shuffle = False ) with torch . no_grad (): correct = 0 for X_test , y_test in test_load_all : y_val = model ( X_test ) # we don't flatten the data this time predicted = torch . max ( y_val , 1 )[ 1 ] correct += ( predicted == y_test ) . sum () print ( f 'Test accuracy: { correct . item () } / { len ( test_data ) } = { correct . item () * 100 / ( len ( test_data )) : 7.3f } %' ) Test accuracy: 9848/10000 = 98.480% Recall that our [784,120,84,10] ANN returned an accuracy of 97.25% after 10 epochs. And it used 105,214 parameters to our current 60,074. Display the confusion matrix \u00b6 # print a row of values for reference np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 4 } ' )) print ( np . arange ( 10 ) . reshape ( 1 , 10 )) print () # print the confusion matrix print ( confusion_matrix ( predicted . view ( - 1 ), y_test . view ( - 1 ))) [[ 0 1 2 3 4 5 6 7 8 9]] [[ 977 0 3 2 2 2 4 1 10 2] [ 0 1132 5 1 1 0 3 7 1 2] [ 0 0 1015 1 0 0 0 4 3 0] [ 0 2 0 1001 0 11 0 1 3 4] [ 0 0 1 0 966 0 1 0 2 2] [ 0 0 0 1 0 863 2 0 0 2] [ 1 0 0 0 3 4 948 0 0 0] [ 1 0 5 0 0 1 0 1005 1 2] [ 1 1 3 4 1 4 0 2 948 2] [ 0 0 0 0 9 7 0 8 6 993]] Examine the misses \u00b6 We can track the index positions of \u201cmissed\u201d predictions, and extract the corresponding image and label. We\u2019ll do this in batches to save screen space. misses = np . array ([]) for i in range ( len ( predicted . view ( - 1 ))): if predicted [ i ] != y_test [ i ]: misses = np . append ( misses , i ) . astype ( 'int64' ) # Display the number of misses len ( misses ) 152 # Display the first 10 index positions misses [: 10 ] array([ 18, 111, 175, 184, 247, 321, 340, 412, 445, 460], dtype=int64) # Set up an iterator to feed batched rows r = 12 # row size row = iter ( np . array_split ( misses , len ( misses ) // r + 1 )) Now that everything is set up, run and re-run the cell below to view all of the missed predictions. Use Ctrl+Enter to remain on the cell between runs. You\u2019ll see a StopIteration once all the misses have been seen. nextrow = next ( row ) print ( \"Index:\" , nextrow ) print ( \"Label:\" , y_test . index_select ( 0 , torch . tensor ( nextrow )) . numpy ()) print ( \"Guess:\" , predicted . index_select ( 0 , torch . tensor ( nextrow )) . numpy ()) images = X_test . index_select ( 0 , torch . tensor ( nextrow )) im = make_grid ( images , nrow = r ) plt . figure ( figsize = ( 10 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Index: [ 18 111 175 184 247 321 340 412 445 460 495 582] Label: [ 3 7 7 8 4 2 5 5 6 5 8 8] Guess: [ 8 1 1 3 6 7 3 3 0 9 0 2] Run a new image through the model \u00b6 We can also pass a single image through the model to obtain a prediction. Pick a number from 0 to 9999, assign it to \u201cx\u201d, and we\u2019ll use that value to select a number from the MNIST test set. x = 2019 plt . figure ( figsize = ( 1 , 1 )) plt . imshow ( test_data [ x ][ 0 ] . reshape (( 28 , 28 )), cmap = \"gist_yarg\" ); model . eval () with torch . no_grad (): new_pred = model ( test_data [ x ][ 0 ] . view ( 1 , 1 , 28 , 28 )) . argmax () print ( \"Predicted value:\" , new_pred . item ()) Predicted value: 9 Great job! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"01 MNIST with CNN"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#mnist-code-along-with-cnn","text":"Now that we\u2019ve seen the results of an artificial neural network model on the MNIST dataset , let\u2019s work the same data with a Convolutional Neural Network (CNN). Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * convolutional layers * filters/kernels * pooling * depth, stride and zero-padding Note that in this exercise there is no need to flatten the MNIST data, as a CNN expects 2-dimensional data.","title":"MNIST Code Along with CNN"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#perform-standard-imports","text":"import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms from torchvision.utils import make_grid import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#load-the-mnist-dataset","text":"PyTorch makes the MNIST train and test datasets available through torchvision . The first time they\u2019re called, the datasets will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. Refer to the previous section for explanations of transformations, batch sizes and DataLoader . transform = transforms . ToTensor () train_data = datasets . MNIST ( root = '../Data' , train = True , download = True , transform = transform ) test_data = datasets . MNIST ( root = '../Data' , train = False , download = True , transform = transform ) train_data Dataset MNIST Number of datapoints: 60000 Split: train Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None test_data Dataset MNIST Number of datapoints: 10000 Split: test Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None","title":"Load the MNIST dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#create-loaders","text":"When working with images, we want relatively small batches; a batch size of 4 is not uncommon. train_loader = DataLoader ( train_data , batch_size = 10 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 10 , shuffle = False )","title":"Create loaders"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#define-a-convolutional-model","text":"In the previous section we used only fully connected layers, with an input layer of 784 (our flattened 28x28 images), hidden layers of 120 and 84 neurons, and an output size representing 10 possible digits. This time we\u2019ll employ two convolutional layers and two pooling layers before feeding data through fully connected hidden layers to our output. The model follows CONV/RELU/POOL/CONV/RELU/POOL/FC/RELU/FC. Let\u2019s walk through the steps we\u2019re about to take. 1. Extend the base Module class: class ConvolutionalNetwork(nn.Module): def \\_\\_init\\_\\_(self): super().\\_\\_init\\_\\_() 2. Set up the convolutional layers with torch.nn.Conv2d() The first layer has one input channel (the grayscale color channel). We\u2019ll assign 6 output channels for feature extraction. We\u2019ll set our kernel size to 3 to make a 3x3 filter, and set the step size to 1. self.conv1 = nn.Conv2d(1, 6, 3, 1) The second layer will take our 6 input channels and deliver 16 output channels. self.conv2 = nn.Conv2d(6, 16, 3, 1) 3. Set up the fully connected layers with torch.nn.Linear() . The input size of (5x5x16) is determined by the effect of our kernels on the input image size. A 3x3 filter applied to a 28x28 image leaves a 1-pixel edge on all four sides. In one layer the size changes from 28x28 to 26x26. We could address this with zero-padding, but since an MNIST image is mostly black at the edges, we should be safe ignoring these pixels. We\u2019ll apply the kernel twice, and apply pooling layers twice, so our resulting output will be $\\;(((28-2)/2)-2)/2 = 5.5\\;$ which rounds down to 5 pixels per side. self.fc1 = nn.Linear(5\\*5\\*16, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) See below for a more detailed look at this step. 4. Define the forward method. Activations can be applied to the convolutions in one line using F.relu() and pooling is done using F.max_pool2d() def forward(self, X): X = F.relu(self.conv1(X)) X = F.max_pool2d(X, 2, 2) X = F.relu(self.conv2(X)) X = F.max_pool2d(X, 2, 2) Flatten the data for the fully connected layers: X = X.view(-1, 5\\*5\\*16) X = F.relu(self.fc1(X)) X = self.fc2(X) return F.log_softmax(X, dim=1) Breaking down the convolutional layers (this code is for illustration purposes only.) # Define layers conv1 = nn . Conv2d ( 1 , 6 , 3 , 1 ) conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) # Grab the first MNIST record for i , ( X_train , y_train ) in enumerate ( train_data ): break # Create a rank-4 tensor to be passed into the model # (train_loader will have done this already) x = X_train . view ( 1 , 1 , 28 , 28 ) print ( x . shape ) torch.Size([1, 1, 28, 28]) # Perform the first convolution/activation x = F . relu ( conv1 ( x )) print ( x . shape ) torch.Size([1, 6, 26, 26]) # Run the first pooling layer x = F . max_pool2d ( x , 2 , 2 ) print ( x . shape ) torch.Size([1, 6, 13, 13]) # Perform the second convolution/activation x = F . relu ( conv2 ( x )) print ( x . shape ) torch.Size([1, 16, 11, 11]) # Run the second pooling layer x = F . max_pool2d ( x , 2 , 2 ) print ( x . shape ) torch.Size([1, 16, 5, 5]) # Flatten the data x = x . view ( - 1 , 5 * 5 * 16 ) print ( x . shape ) torch.Size([1, 400]) This is how the convolution output is passed into the fully connected layers. Now let\u2019s run the code. class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 1 , 6 , 3 , 1 ) self . conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) self . fc1 = nn . Linear ( 5 * 5 * 16 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , X ): X = F . relu ( self . conv1 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = F . relu ( self . conv2 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = X . view ( - 1 , 5 * 5 * 16 ) X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) torch . manual_seed ( 42 ) model = ConvolutionalNetwork () model ConvolutionalNetwork( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) Including the bias terms for each layer, the total number of parameters being trained is: \\(\\quad\\begin{split}(1\\times6\\times3\\times3)+6+(6\\times16\\times3\\times3)+16+(400\\times120)+120+(120\\times84)+84+(84\\times10)+10 &=\\\\ 54+6+864+16+48000+120+10080+84+840+10 &= 60,074\\end{split}\\) def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 54 6 864 16 48000 120 10080 84 840 10 ______ 60074","title":"Define a convolutional model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#define-loss-function-optimizer","text":"criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#train-the-model","text":"This time we\u2019ll feed the data directly into the model without flattening it first. import time start_time = time . time () epochs = 5 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): b += 1 # Apply the model y_pred = model ( X_train ) # we don't flatten X-train here loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 600 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 10 * b : 6 } /60000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 10 * b ) : 7.3f } %' ) train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): # Apply the model y_val = model ( X_test ) # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 600 [ 6000/60000] loss: 0.21188490 accuracy: 78.233% epoch: 0 batch: 1200 [ 12000/60000] loss: 0.58768761 accuracy: 85.433% epoch: 0 batch: 1800 [ 18000/60000] loss: 0.03002630 accuracy: 88.539% epoch: 0 batch: 2400 [ 24000/60000] loss: 0.02856987 accuracy: 90.396% epoch: 0 batch: 3000 [ 30000/60000] loss: 0.01619262 accuracy: 91.543% epoch: 0 batch: 3600 [ 36000/60000] loss: 0.00392615 accuracy: 92.347% epoch: 0 batch: 4200 [ 42000/60000] loss: 0.07892600 accuracy: 92.938% epoch: 0 batch: 4800 [ 48000/60000] loss: 0.00173595 accuracy: 93.458% epoch: 0 batch: 5400 [ 54000/60000] loss: 0.00021752 accuracy: 93.889% epoch: 0 batch: 6000 [ 60000/60000] loss: 0.00123056 accuracy: 94.245% epoch: 1 batch: 600 [ 6000/60000] loss: 0.03455487 accuracy: 97.967% epoch: 1 batch: 1200 [ 12000/60000] loss: 0.25245315 accuracy: 97.792% epoch: 1 batch: 1800 [ 18000/60000] loss: 0.15286988 accuracy: 97.833% epoch: 1 batch: 2400 [ 24000/60000] loss: 0.00420426 accuracy: 97.875% epoch: 1 batch: 3000 [ 30000/60000] loss: 0.00133034 accuracy: 97.897% epoch: 1 batch: 3600 [ 36000/60000] loss: 0.10122252 accuracy: 97.922% epoch: 1 batch: 4200 [ 42000/60000] loss: 0.03956039 accuracy: 97.931% epoch: 1 batch: 4800 [ 48000/60000] loss: 0.15445584 accuracy: 97.950% epoch: 1 batch: 5400 [ 54000/60000] loss: 0.02514876 accuracy: 97.998% epoch: 1 batch: 6000 [ 60000/60000] loss: 0.01102044 accuracy: 97.990% epoch: 2 batch: 600 [ 6000/60000] loss: 0.00138958 accuracy: 98.667% epoch: 2 batch: 1200 [ 12000/60000] loss: 0.00090325 accuracy: 98.533% epoch: 2 batch: 1800 [ 18000/60000] loss: 0.00036748 accuracy: 98.567% epoch: 2 batch: 2400 [ 24000/60000] loss: 0.00072705 accuracy: 98.487% epoch: 2 batch: 3000 [ 30000/60000] loss: 0.00448279 accuracy: 98.460% epoch: 2 batch: 3600 [ 36000/60000] loss: 0.00007090 accuracy: 98.508% epoch: 2 batch: 4200 [ 42000/60000] loss: 0.00142251 accuracy: 98.500% epoch: 2 batch: 4800 [ 48000/60000] loss: 0.00054714 accuracy: 98.473% epoch: 2 batch: 5400 [ 54000/60000] loss: 0.00036345 accuracy: 98.493% epoch: 2 batch: 6000 [ 60000/60000] loss: 0.00005013 accuracy: 98.515% epoch: 3 batch: 600 [ 6000/60000] loss: 0.00073732 accuracy: 99.117% epoch: 3 batch: 1200 [ 12000/60000] loss: 0.01391867 accuracy: 98.933% epoch: 3 batch: 1800 [ 18000/60000] loss: 0.00901531 accuracy: 98.806% epoch: 3 batch: 2400 [ 24000/60000] loss: 0.00081765 accuracy: 98.846% epoch: 3 batch: 3000 [ 30000/60000] loss: 0.00001284 accuracy: 98.880% epoch: 3 batch: 3600 [ 36000/60000] loss: 0.00020319 accuracy: 98.886% epoch: 3 batch: 4200 [ 42000/60000] loss: 0.00093818 accuracy: 98.879% epoch: 3 batch: 4800 [ 48000/60000] loss: 0.00241654 accuracy: 98.917% epoch: 3 batch: 5400 [ 54000/60000] loss: 0.00376742 accuracy: 98.893% epoch: 3 batch: 6000 [ 60000/60000] loss: 0.00199913 accuracy: 98.863% epoch: 4 batch: 600 [ 6000/60000] loss: 0.00138756 accuracy: 99.183% epoch: 4 batch: 1200 [ 12000/60000] loss: 0.10804896 accuracy: 99.075% epoch: 4 batch: 1800 [ 18000/60000] loss: 0.00437851 accuracy: 99.156% epoch: 4 batch: 2400 [ 24000/60000] loss: 0.00001171 accuracy: 99.104% epoch: 4 batch: 3000 [ 30000/60000] loss: 0.00447276 accuracy: 99.050% epoch: 4 batch: 3600 [ 36000/60000] loss: 0.00048421 accuracy: 99.061% epoch: 4 batch: 4200 [ 42000/60000] loss: 0.00056122 accuracy: 99.043% epoch: 4 batch: 4800 [ 48000/60000] loss: 0.00057514 accuracy: 99.033% epoch: 4 batch: 5400 [ 54000/60000] loss: 0.07913177 accuracy: 99.028% epoch: 4 batch: 6000 [ 60000/60000] loss: 0.00030920 accuracy: 99.027% Duration: 253 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#plot-the-loss-and-accuracy-comparisons","text":"plt . plot ( train_losses , label = 'training loss' ) plt . plot ( test_losses , label = 'validation loss' ) plt . title ( 'Loss at the end of each epoch' ) plt . legend (); test_losses [tensor(0.0067), tensor(0.0011), tensor(2.5534e-05), tensor(9.3650e-05), tensor(0.0005)] While there may be some overfitting of the training data, there is far less than we saw with the ANN model. plt . plot ([ t / 600 for t in train_correct ], label = 'training accuracy' ) plt . plot ([ t / 100 for t in test_correct ], label = 'validation accuracy' ) plt . title ( 'Accuracy at the end of each epoch' ) plt . legend ();","title":"Plot the loss and accuracy comparisons"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#evaluate-test-data","text":"# Extract the data all at once, not in batches test_load_all = DataLoader ( test_data , batch_size = 10000 , shuffle = False ) with torch . no_grad (): correct = 0 for X_test , y_test in test_load_all : y_val = model ( X_test ) # we don't flatten the data this time predicted = torch . max ( y_val , 1 )[ 1 ] correct += ( predicted == y_test ) . sum () print ( f 'Test accuracy: { correct . item () } / { len ( test_data ) } = { correct . item () * 100 / ( len ( test_data )) : 7.3f } %' ) Test accuracy: 9848/10000 = 98.480% Recall that our [784,120,84,10] ANN returned an accuracy of 97.25% after 10 epochs. And it used 105,214 parameters to our current 60,074.","title":"Evaluate Test Data"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#display-the-confusion-matrix","text":"# print a row of values for reference np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 4 } ' )) print ( np . arange ( 10 ) . reshape ( 1 , 10 )) print () # print the confusion matrix print ( confusion_matrix ( predicted . view ( - 1 ), y_test . view ( - 1 ))) [[ 0 1 2 3 4 5 6 7 8 9]] [[ 977 0 3 2 2 2 4 1 10 2] [ 0 1132 5 1 1 0 3 7 1 2] [ 0 0 1015 1 0 0 0 4 3 0] [ 0 2 0 1001 0 11 0 1 3 4] [ 0 0 1 0 966 0 1 0 2 2] [ 0 0 0 1 0 863 2 0 0 2] [ 1 0 0 0 3 4 948 0 0 0] [ 1 0 5 0 0 1 0 1005 1 2] [ 1 1 3 4 1 4 0 2 948 2] [ 0 0 0 0 9 7 0 8 6 993]]","title":"Display the confusion matrix"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#examine-the-misses","text":"We can track the index positions of \u201cmissed\u201d predictions, and extract the corresponding image and label. We\u2019ll do this in batches to save screen space. misses = np . array ([]) for i in range ( len ( predicted . view ( - 1 ))): if predicted [ i ] != y_test [ i ]: misses = np . append ( misses , i ) . astype ( 'int64' ) # Display the number of misses len ( misses ) 152 # Display the first 10 index positions misses [: 10 ] array([ 18, 111, 175, 184, 247, 321, 340, 412, 445, 460], dtype=int64) # Set up an iterator to feed batched rows r = 12 # row size row = iter ( np . array_split ( misses , len ( misses ) // r + 1 )) Now that everything is set up, run and re-run the cell below to view all of the missed predictions. Use Ctrl+Enter to remain on the cell between runs. You\u2019ll see a StopIteration once all the misses have been seen. nextrow = next ( row ) print ( \"Index:\" , nextrow ) print ( \"Label:\" , y_test . index_select ( 0 , torch . tensor ( nextrow )) . numpy ()) print ( \"Guess:\" , predicted . index_select ( 0 , torch . tensor ( nextrow )) . numpy ()) images = X_test . index_select ( 0 , torch . tensor ( nextrow )) im = make_grid ( images , nrow = r ) plt . figure ( figsize = ( 10 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Index: [ 18 111 175 184 247 321 340 412 445 460 495 582] Label: [ 3 7 7 8 4 2 5 5 6 5 8 8] Guess: [ 8 1 1 3 6 7 3 3 0 9 0 2]","title":"Examine the misses"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#run-a-new-image-through-the-model","text":"We can also pass a single image through the model to obtain a prediction. Pick a number from 0 to 9999, assign it to \u201cx\u201d, and we\u2019ll use that value to select a number from the MNIST test set. x = 2019 plt . figure ( figsize = ( 1 , 1 )) plt . imshow ( test_data [ x ][ 0 ] . reshape (( 28 , 28 )), cmap = \"gist_yarg\" ); model . eval () with torch . no_grad (): new_pred = model ( test_data [ x ][ 0 ] . view ( 1 , 1 , 28 , 28 )) . argmax () print ( \"Predicted value:\" , new_pred . item ()) Predicted value: 9","title":"Run a new image through the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/#great-job","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/","text":"================ by Jawad Haider CIFAR Code Along with CNN Perform standard imports Load the CIFAR-10 dataset Create loaders Define strings for labels View a batch of images Define the model Define loss function & optimizer Train the model Optional: Save the model Plot the loss and accuracy comparisons Evaluate Test Data Display the confusion matrix Examine the misses Great job! CIFAR Code Along with CNN \u00b6 The CIFAR-10 dataset is similar to MNIST, except that instead of one color channel (grayscale) there are three channels (RGB). Where an MNIST image has a size of (1,28,28), CIFAR images are (3,32,32). There are 10 categories an image may fall under: 0. airplane 1. automobile 2. bird 3. cat 4. deer 5. dog 6. frog 7. horse 8. ship 9. truck As with the previous code along, make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * convolutional layers * filters/kernels * pooling * depth, stride and zero-padding Perform standard imports \u00b6 import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms from torchvision.utils import make_grid import numpy as np import pandas as pd import seaborn as sn # for heatmaps from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt % matplotlib inline Load the CIFAR-10 dataset \u00b6 PyTorch makes the CIFAR-10 train and test datasets available through torchvision . The first time they\u2019re called, the datasets will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. The set contains 50,000 train and 10,000 test images. Refer to the previous section for explanations of transformations, batch sizes and DataLoader . transform = transforms . ToTensor () train_data = datasets . CIFAR10 ( root = '../Data' , train = True , download = True , transform = transform ) test_data = datasets . CIFAR10 ( root = '../Data' , train = False , download = True , transform = transform ) Files already downloaded and verified Files already downloaded and verified train_data Dataset CIFAR10 Number of datapoints: 50000 Split: train Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None test_data Dataset CIFAR10 Number of datapoints: 10000 Split: test Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None Create loaders \u00b6 torch . manual_seed ( 101 ) # for reproducible results train_loader = DataLoader ( train_data , batch_size = 10 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 10 , shuffle = False ) Define strings for labels \u00b6 We can call the labels whatever we want, so long as they appear in the order of \u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019, \u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019, \u2018truck\u2019. Here we\u2019re using 5-character labels padded with spaces so that our reports line up later. class_names = [ 'plane' , ' car' , ' bird' , ' cat' , ' deer' , ' dog' , ' frog' , 'horse' , ' ship' , 'truck' ] We don\u2019t want to use the variable name \u201cclass\u201d here, as it would overwrite Python\u2019s built-in keyword. View a batch of images \u00b6 np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 5 } ' )) # to widen the printed array # Grab the first batch of 10 images for images , labels in train_loader : break # Print the labels print ( 'Label:' , labels . numpy ()) print ( 'Class: ' , * np . array ([ class_names [ i ] for i in labels ])) # Print the images im = make_grid ( images , nrow = 5 ) # the default nrow is 8 plt . figure ( figsize = ( 10 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Label: [ 3 2 0 4 9 5 1 2 4 8] Class: cat bird plane deer truck dog car bird deer ship Define the model \u00b6 In the previous section we used two convolutional layers and two pooling layers before feeding data through a fully connected hidden layer to our output. The model follows CONV/RELU/POOL/CONV/RELU/POOL/FC/RELU/FC. We\u2019ll use the same format here. The only changes are: * take in 3-channel images instead of 1-channel * adjust the size of the fully connected input Our first convolutional layer will have 3 input channels, 6 output channels, a kernel size of 3 (resulting in a 3x3 filter), and a stride length of 1 pixel. These are passed in as nn.Conv2d(3,6,3,1) class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 3 , 1 ) # changed from (1, 6, 5, 1) self . conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) self . fc1 = nn . Linear ( 6 * 6 * 16 , 120 ) # changed from (4*4*16) to fit 32x32 images with 3x3 filters self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , X ): X = F . relu ( self . conv1 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = F . relu ( self . conv2 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = X . view ( - 1 , 6 * 6 * 16 ) X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) Why (6x6x16) instead of (5x5x16) ? With MNIST the kernels and pooling layers resulted in $\\;(((28\u22122)/2)\u22122)/2=5.5 \\;$ which rounds down to 5 pixels per side. With CIFAR the result is $\\;(((32-2)/2)-2)/2 = 6.5\\;$ which rounds down to 6 pixels per side. torch . manual_seed ( 101 ) model = ConvolutionalNetwork () model ConvolutionalNetwork( (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=576, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) Including the bias terms for each layer, the total number of parameters being trained is: \\(\\quad\\begin{split}(3\\times6\\times3\\times3)+6+(6\\times16\\times3\\times3)+16+(576\\times120)+120+(120\\times84)+84+(84\\times10)+10 &=\\\\ 162+6+864+16+69120+120+10080+84+840+10 &= 81,302\\end{split}\\) def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 162 6 864 16 69120 120 10080 84 840 10 ______ 81302 Define loss function & optimizer \u00b6 criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) Train the model \u00b6 This time we\u2019ll feed the data directly into the model without flattening it first. OPTIONAL: In the event that training takes too long, you can interrupt the kernel, skip ahead to the bottom of the notebook, and load a trained version of the model that\u2019s been saved in this folder. import time start_time = time . time () epochs = 10 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): b += 1 # Apply the model y_pred = model ( X_train ) loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 1000 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 10 * b : 6 } /50000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 10 * b ) : 7.3f } %' ) train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): # Apply the model y_val = model ( X_test ) # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 1000 [ 10000/50000] loss: 1.72811735 accuracy: 26.660% epoch: 0 batch: 2000 [ 20000/50000] loss: 1.90085292 accuracy: 32.790% epoch: 0 batch: 3000 [ 30000/50000] loss: 1.77626872 accuracy: 36.507% epoch: 0 batch: 4000 [ 40000/50000] loss: 1.32156026 accuracy: 38.925% epoch: 0 batch: 5000 [ 50000/50000] loss: 1.37019920 accuracy: 40.922% epoch: 1 batch: 1000 [ 10000/50000] loss: 1.18819773 accuracy: 51.520% epoch: 1 batch: 2000 [ 20000/50000] loss: 1.21327436 accuracy: 51.725% epoch: 1 batch: 3000 [ 30000/50000] loss: 1.15835631 accuracy: 52.283% epoch: 1 batch: 4000 [ 40000/50000] loss: 1.28492486 accuracy: 52.587% epoch: 1 batch: 5000 [ 50000/50000] loss: 2.28428698 accuracy: 52.930% epoch: 2 batch: 1000 [ 10000/50000] loss: 1.22954726 accuracy: 56.750% epoch: 2 batch: 2000 [ 20000/50000] loss: 1.51806808 accuracy: 56.725% epoch: 2 batch: 3000 [ 30000/50000] loss: 0.82857972 accuracy: 56.847% epoch: 2 batch: 4000 [ 40000/50000] loss: 0.99008143 accuracy: 57.108% epoch: 2 batch: 5000 [ 50000/50000] loss: 0.74985492 accuracy: 57.280% epoch: 3 batch: 1000 [ 10000/50000] loss: 0.73941267 accuracy: 60.430% epoch: 3 batch: 2000 [ 20000/50000] loss: 0.85795957 accuracy: 60.200% epoch: 3 batch: 3000 [ 30000/50000] loss: 0.99735087 accuracy: 59.877% epoch: 3 batch: 4000 [ 40000/50000] loss: 1.01958919 accuracy: 59.965% epoch: 3 batch: 5000 [ 50000/50000] loss: 0.76560205 accuracy: 59.992% epoch: 4 batch: 1000 [ 10000/50000] loss: 1.01610267 accuracy: 62.400% epoch: 4 batch: 2000 [ 20000/50000] loss: 0.91081637 accuracy: 62.405% epoch: 4 batch: 3000 [ 30000/50000] loss: 1.82826269 accuracy: 62.287% epoch: 4 batch: 4000 [ 40000/50000] loss: 0.83664912 accuracy: 62.260% epoch: 4 batch: 5000 [ 50000/50000] loss: 1.08910477 accuracy: 62.156% epoch: 5 batch: 1000 [ 10000/50000] loss: 0.74583805 accuracy: 64.360% epoch: 5 batch: 2000 [ 20000/50000] loss: 0.55392635 accuracy: 64.360% epoch: 5 batch: 3000 [ 30000/50000] loss: 1.75524867 accuracy: 63.840% epoch: 5 batch: 4000 [ 40000/50000] loss: 1.33982396 accuracy: 63.767% epoch: 5 batch: 5000 [ 50000/50000] loss: 1.00686800 accuracy: 63.760% epoch: 6 batch: 1000 [ 10000/50000] loss: 1.14186704 accuracy: 65.820% epoch: 6 batch: 2000 [ 20000/50000] loss: 1.05023360 accuracy: 65.320% epoch: 6 batch: 3000 [ 30000/50000] loss: 1.02424061 accuracy: 65.353% epoch: 6 batch: 4000 [ 40000/50000] loss: 0.92525661 accuracy: 65.438% epoch: 6 batch: 5000 [ 50000/50000] loss: 1.16625285 accuracy: 65.342% epoch: 7 batch: 1000 [ 10000/50000] loss: 1.45434511 accuracy: 66.870% epoch: 7 batch: 2000 [ 20000/50000] loss: 1.03906512 accuracy: 66.835% epoch: 7 batch: 3000 [ 30000/50000] loss: 1.39975834 accuracy: 66.730% epoch: 7 batch: 4000 [ 40000/50000] loss: 0.61725640 accuracy: 66.528% epoch: 7 batch: 5000 [ 50000/50000] loss: 0.52788514 accuracy: 66.414% epoch: 8 batch: 1000 [ 10000/50000] loss: 1.14143419 accuracy: 69.010% epoch: 8 batch: 2000 [ 20000/50000] loss: 1.21063972 accuracy: 68.010% epoch: 8 batch: 3000 [ 30000/50000] loss: 1.08417308 accuracy: 67.817% epoch: 8 batch: 4000 [ 40000/50000] loss: 0.89879084 accuracy: 67.935% epoch: 8 batch: 5000 [ 50000/50000] loss: 1.01553142 accuracy: 67.740% epoch: 9 batch: 1000 [ 10000/50000] loss: 0.66314524 accuracy: 69.630% epoch: 9 batch: 2000 [ 20000/50000] loss: 1.15445566 accuracy: 69.450% epoch: 9 batch: 3000 [ 30000/50000] loss: 0.69587100 accuracy: 69.300% epoch: 9 batch: 4000 [ 40000/50000] loss: 0.72007024 accuracy: 68.873% epoch: 9 batch: 5000 [ 50000/50000] loss: 0.96967870 accuracy: 68.824% Duration: 1207 seconds Optional: Save the model \u00b6 This will save your trained model, without overwriting the saved model we have provided called CIFAR10-CNN-Model-master.pt torch . save ( model . state_dict (), 'CIFAR10-CNN-Model.pt' ) Plot the loss and accuracy comparisons \u00b6 plt . plot ( train_losses , label = 'training loss' ) plt . plot ( test_losses , label = 'validation loss' ) plt . title ( 'Loss at the end of each epoch' ) plt . legend (); plt . plot ([ t / 500 for t in train_correct ], label = 'training accuracy' ) plt . plot ([ t / 100 for t in test_correct ], label = 'validation accuracy' ) plt . title ( 'Accuracy at the end of each epoch' ) plt . legend (); Evaluate Test Data \u00b6 print ( test_correct ) # contains the results of all 10 epochs print () print ( f 'Test accuracy: { test_correct [ - 1 ] . item () * 100 / 10000 : .3f } %' ) # print the most recent result as a percent [tensor(4940), tensor(5519), tensor(5685), tensor(5812), tensor(5930), tensor(6048), tensor(5941), tensor(6166), tensor(6035), tensor(6105)] Test accuracy: 61.050% This is not as impressive as with MNIST, which makes sense. We would have to adjust our parameters to obtain better results. Still, it\u2019s much better than the 10% we\u2019d get with random chance! Display the confusion matrix \u00b6 In order to map predictions against ground truth, we need to run the entire test set through the model. Also, since our model was not as accurate as with MNIST, we\u2019ll use a heatmap to better display the results. # Create a loader for the entire the test set test_load_all = DataLoader ( test_data , batch_size = 10000 , shuffle = False ) with torch . no_grad (): correct = 0 for X_test , y_test in test_load_all : y_val = model ( X_test ) predicted = torch . max ( y_val , 1 )[ 1 ] correct += ( predicted == y_test ) . sum () arr = confusion_matrix ( y_test . view ( - 1 ), predicted . view ( - 1 )) df_cm = pd . DataFrame ( arr , class_names , class_names ) plt . figure ( figsize = ( 9 , 6 )) sn . heatmap ( df_cm , annot = True , fmt = \"d\" , cmap = 'BuGn' ) plt . xlabel ( \"prediction\" ) plt . ylabel ( \"label (ground truth)\" ) plt . show (); For more info on the above chart, visit the docs on scikit-learn\u2019s confusion_matrix , seaborn heatmaps , and matplotlib colormaps . Examine the misses \u00b6 We can track the index positions of \u201cmissed\u201d predictions, and extract the corresponding image and label. We\u2019ll do this in batches to save screen space. misses = np . array ([]) for i in range ( len ( predicted . view ( - 1 ))): if predicted [ i ] != y_test [ i ]: misses = np . append ( misses , i ) . astype ( 'int64' ) # Display the number of misses len ( misses ) 3895 # Display the first 8 index positions misses [: 8 ] array([ 3, 4, 6, 7, 8, 17, 20, 21], dtype=int64) # Set up an iterator to feed batched rows r = 8 # row size row = iter ( np . array_split ( misses , len ( misses ) // r + 1 )) Now that everything is set up, run and re-run the cell below to view all of the missed predictions. Use Ctrl+Enter to remain on the cell between runs. You\u2019ll see a StopIteration once all the misses have been seen. np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 5 } ' )) # to widen the printed array nextrow = next ( row ) lbls = y_test . index_select ( 0 , torch . tensor ( nextrow )) . numpy () gues = predicted . index_select ( 0 , torch . tensor ( nextrow )) . numpy () print ( \"Index:\" , nextrow ) print ( \"Label:\" , lbls ) print ( \"Class: \" , * np . array ([ class_names [ i ] for i in lbls ])) print () print ( \"Guess:\" , gues ) print ( \"Class: \" , * np . array ([ class_names [ i ] for i in gues ])) images = X_test . index_select ( 0 , torch . tensor ( nextrow )) im = make_grid ( images , nrow = r ) plt . figure ( figsize = ( 8 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Index: [ 3 4 6 7 8 17 20 21] Label: [ 0 6 1 6 3 7 7 0] Class: plane frog car frog cat horse horse plane Guess: [ 8 4 2 4 5 3 2 2] Class: ship deer bird deer dog cat bird bird Optional: Load a Saved Model In the event that training the ConvolutionalNetwork takes too long, you can load a trained version by running the following code: model2 = ConvolutionalNetwork() model2.load_state_dict(torch.load('CIFAR10-CNN-Model-master.pt')) model2.eval() # Instantiate the model and load saved parameters model2 = ConvolutionalNetwork () model2 . load_state_dict ( torch . load ( 'CIFAR10-CNN-Model-master.pt' )) model2 . eval () ConvolutionalNetwork( (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=576, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) # Evaluate the saved model against the test set test_load_all = DataLoader ( test_data , batch_size = 10000 , shuffle = False ) with torch . no_grad (): correct = 0 for X_test , y_test in test_load_all : y_val = model2 ( X_test ) predicted = torch . max ( y_val , 1 )[ 1 ] correct += ( predicted == y_test ) . sum () print ( f 'Test accuracy: { correct . item () } / { len ( test_data ) } = { correct . item () * 100 / ( len ( test_data )) : 7.3f } %' ) Test accuracy: 6105/10000 = 61.050% # Display the confusion matrix as a heatmap arr = confusion_matrix ( y_test . view ( - 1 ), predicted . view ( - 1 )) df_cm = pd . DataFrame ( arr , class_names , class_names ) plt . figure ( figsize = ( 9 , 6 )) sn . heatmap ( df_cm , annot = True , fmt = \"d\" , cmap = 'BuGn' ) plt . xlabel ( \"prediction\" ) plt . ylabel ( \"label (ground truth)\" ) plt . show (); Great job! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"02 CIFAR CNN Code Along"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#cifar-code-along-with-cnn","text":"The CIFAR-10 dataset is similar to MNIST, except that instead of one color channel (grayscale) there are three channels (RGB). Where an MNIST image has a size of (1,28,28), CIFAR images are (3,32,32). There are 10 categories an image may fall under: 0. airplane 1. automobile 2. bird 3. cat 4. deer 5. dog 6. frog 7. horse 8. ship 9. truck As with the previous code along, make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * convolutional layers * filters/kernels * pooling * depth, stride and zero-padding","title":"CIFAR Code Along with CNN"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#perform-standard-imports","text":"import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms from torchvision.utils import make_grid import numpy as np import pandas as pd import seaborn as sn # for heatmaps from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#load-the-cifar-10-dataset","text":"PyTorch makes the CIFAR-10 train and test datasets available through torchvision . The first time they\u2019re called, the datasets will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. The set contains 50,000 train and 10,000 test images. Refer to the previous section for explanations of transformations, batch sizes and DataLoader . transform = transforms . ToTensor () train_data = datasets . CIFAR10 ( root = '../Data' , train = True , download = True , transform = transform ) test_data = datasets . CIFAR10 ( root = '../Data' , train = False , download = True , transform = transform ) Files already downloaded and verified Files already downloaded and verified train_data Dataset CIFAR10 Number of datapoints: 50000 Split: train Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None test_data Dataset CIFAR10 Number of datapoints: 10000 Split: test Root Location: ../Data Transforms (if any): ToTensor() Target Transforms (if any): None","title":"Load the CIFAR-10 dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#create-loaders","text":"torch . manual_seed ( 101 ) # for reproducible results train_loader = DataLoader ( train_data , batch_size = 10 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 10 , shuffle = False )","title":"Create loaders"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#define-strings-for-labels","text":"We can call the labels whatever we want, so long as they appear in the order of \u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019, \u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019, \u2018truck\u2019. Here we\u2019re using 5-character labels padded with spaces so that our reports line up later. class_names = [ 'plane' , ' car' , ' bird' , ' cat' , ' deer' , ' dog' , ' frog' , 'horse' , ' ship' , 'truck' ] We don\u2019t want to use the variable name \u201cclass\u201d here, as it would overwrite Python\u2019s built-in keyword.","title":"Define strings for labels"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#view-a-batch-of-images","text":"np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 5 } ' )) # to widen the printed array # Grab the first batch of 10 images for images , labels in train_loader : break # Print the labels print ( 'Label:' , labels . numpy ()) print ( 'Class: ' , * np . array ([ class_names [ i ] for i in labels ])) # Print the images im = make_grid ( images , nrow = 5 ) # the default nrow is 8 plt . figure ( figsize = ( 10 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Label: [ 3 2 0 4 9 5 1 2 4 8] Class: cat bird plane deer truck dog car bird deer ship","title":"View a batch of images"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#define-the-model","text":"In the previous section we used two convolutional layers and two pooling layers before feeding data through a fully connected hidden layer to our output. The model follows CONV/RELU/POOL/CONV/RELU/POOL/FC/RELU/FC. We\u2019ll use the same format here. The only changes are: * take in 3-channel images instead of 1-channel * adjust the size of the fully connected input Our first convolutional layer will have 3 input channels, 6 output channels, a kernel size of 3 (resulting in a 3x3 filter), and a stride length of 1 pixel. These are passed in as nn.Conv2d(3,6,3,1) class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 3 , 1 ) # changed from (1, 6, 5, 1) self . conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) self . fc1 = nn . Linear ( 6 * 6 * 16 , 120 ) # changed from (4*4*16) to fit 32x32 images with 3x3 filters self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , X ): X = F . relu ( self . conv1 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = F . relu ( self . conv2 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = X . view ( - 1 , 6 * 6 * 16 ) X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) Why (6x6x16) instead of (5x5x16) ? With MNIST the kernels and pooling layers resulted in $\\;(((28\u22122)/2)\u22122)/2=5.5 \\;$ which rounds down to 5 pixels per side. With CIFAR the result is $\\;(((32-2)/2)-2)/2 = 6.5\\;$ which rounds down to 6 pixels per side. torch . manual_seed ( 101 ) model = ConvolutionalNetwork () model ConvolutionalNetwork( (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=576, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) Including the bias terms for each layer, the total number of parameters being trained is: \\(\\quad\\begin{split}(3\\times6\\times3\\times3)+6+(6\\times16\\times3\\times3)+16+(576\\times120)+120+(120\\times84)+84+(84\\times10)+10 &=\\\\ 162+6+864+16+69120+120+10080+84+840+10 &= 81,302\\end{split}\\) def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 162 6 864 16 69120 120 10080 84 840 10 ______ 81302","title":"Define the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#define-loss-function-optimizer","text":"criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#train-the-model","text":"This time we\u2019ll feed the data directly into the model without flattening it first. OPTIONAL: In the event that training takes too long, you can interrupt the kernel, skip ahead to the bottom of the notebook, and load a trained version of the model that\u2019s been saved in this folder. import time start_time = time . time () epochs = 10 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): b += 1 # Apply the model y_pred = model ( X_train ) loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 1000 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 10 * b : 6 } /50000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 10 * b ) : 7.3f } %' ) train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): # Apply the model y_val = model ( X_test ) # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 1000 [ 10000/50000] loss: 1.72811735 accuracy: 26.660% epoch: 0 batch: 2000 [ 20000/50000] loss: 1.90085292 accuracy: 32.790% epoch: 0 batch: 3000 [ 30000/50000] loss: 1.77626872 accuracy: 36.507% epoch: 0 batch: 4000 [ 40000/50000] loss: 1.32156026 accuracy: 38.925% epoch: 0 batch: 5000 [ 50000/50000] loss: 1.37019920 accuracy: 40.922% epoch: 1 batch: 1000 [ 10000/50000] loss: 1.18819773 accuracy: 51.520% epoch: 1 batch: 2000 [ 20000/50000] loss: 1.21327436 accuracy: 51.725% epoch: 1 batch: 3000 [ 30000/50000] loss: 1.15835631 accuracy: 52.283% epoch: 1 batch: 4000 [ 40000/50000] loss: 1.28492486 accuracy: 52.587% epoch: 1 batch: 5000 [ 50000/50000] loss: 2.28428698 accuracy: 52.930% epoch: 2 batch: 1000 [ 10000/50000] loss: 1.22954726 accuracy: 56.750% epoch: 2 batch: 2000 [ 20000/50000] loss: 1.51806808 accuracy: 56.725% epoch: 2 batch: 3000 [ 30000/50000] loss: 0.82857972 accuracy: 56.847% epoch: 2 batch: 4000 [ 40000/50000] loss: 0.99008143 accuracy: 57.108% epoch: 2 batch: 5000 [ 50000/50000] loss: 0.74985492 accuracy: 57.280% epoch: 3 batch: 1000 [ 10000/50000] loss: 0.73941267 accuracy: 60.430% epoch: 3 batch: 2000 [ 20000/50000] loss: 0.85795957 accuracy: 60.200% epoch: 3 batch: 3000 [ 30000/50000] loss: 0.99735087 accuracy: 59.877% epoch: 3 batch: 4000 [ 40000/50000] loss: 1.01958919 accuracy: 59.965% epoch: 3 batch: 5000 [ 50000/50000] loss: 0.76560205 accuracy: 59.992% epoch: 4 batch: 1000 [ 10000/50000] loss: 1.01610267 accuracy: 62.400% epoch: 4 batch: 2000 [ 20000/50000] loss: 0.91081637 accuracy: 62.405% epoch: 4 batch: 3000 [ 30000/50000] loss: 1.82826269 accuracy: 62.287% epoch: 4 batch: 4000 [ 40000/50000] loss: 0.83664912 accuracy: 62.260% epoch: 4 batch: 5000 [ 50000/50000] loss: 1.08910477 accuracy: 62.156% epoch: 5 batch: 1000 [ 10000/50000] loss: 0.74583805 accuracy: 64.360% epoch: 5 batch: 2000 [ 20000/50000] loss: 0.55392635 accuracy: 64.360% epoch: 5 batch: 3000 [ 30000/50000] loss: 1.75524867 accuracy: 63.840% epoch: 5 batch: 4000 [ 40000/50000] loss: 1.33982396 accuracy: 63.767% epoch: 5 batch: 5000 [ 50000/50000] loss: 1.00686800 accuracy: 63.760% epoch: 6 batch: 1000 [ 10000/50000] loss: 1.14186704 accuracy: 65.820% epoch: 6 batch: 2000 [ 20000/50000] loss: 1.05023360 accuracy: 65.320% epoch: 6 batch: 3000 [ 30000/50000] loss: 1.02424061 accuracy: 65.353% epoch: 6 batch: 4000 [ 40000/50000] loss: 0.92525661 accuracy: 65.438% epoch: 6 batch: 5000 [ 50000/50000] loss: 1.16625285 accuracy: 65.342% epoch: 7 batch: 1000 [ 10000/50000] loss: 1.45434511 accuracy: 66.870% epoch: 7 batch: 2000 [ 20000/50000] loss: 1.03906512 accuracy: 66.835% epoch: 7 batch: 3000 [ 30000/50000] loss: 1.39975834 accuracy: 66.730% epoch: 7 batch: 4000 [ 40000/50000] loss: 0.61725640 accuracy: 66.528% epoch: 7 batch: 5000 [ 50000/50000] loss: 0.52788514 accuracy: 66.414% epoch: 8 batch: 1000 [ 10000/50000] loss: 1.14143419 accuracy: 69.010% epoch: 8 batch: 2000 [ 20000/50000] loss: 1.21063972 accuracy: 68.010% epoch: 8 batch: 3000 [ 30000/50000] loss: 1.08417308 accuracy: 67.817% epoch: 8 batch: 4000 [ 40000/50000] loss: 0.89879084 accuracy: 67.935% epoch: 8 batch: 5000 [ 50000/50000] loss: 1.01553142 accuracy: 67.740% epoch: 9 batch: 1000 [ 10000/50000] loss: 0.66314524 accuracy: 69.630% epoch: 9 batch: 2000 [ 20000/50000] loss: 1.15445566 accuracy: 69.450% epoch: 9 batch: 3000 [ 30000/50000] loss: 0.69587100 accuracy: 69.300% epoch: 9 batch: 4000 [ 40000/50000] loss: 0.72007024 accuracy: 68.873% epoch: 9 batch: 5000 [ 50000/50000] loss: 0.96967870 accuracy: 68.824% Duration: 1207 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#optional-save-the-model","text":"This will save your trained model, without overwriting the saved model we have provided called CIFAR10-CNN-Model-master.pt torch . save ( model . state_dict (), 'CIFAR10-CNN-Model.pt' )","title":"Optional: Save the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#plot-the-loss-and-accuracy-comparisons","text":"plt . plot ( train_losses , label = 'training loss' ) plt . plot ( test_losses , label = 'validation loss' ) plt . title ( 'Loss at the end of each epoch' ) plt . legend (); plt . plot ([ t / 500 for t in train_correct ], label = 'training accuracy' ) plt . plot ([ t / 100 for t in test_correct ], label = 'validation accuracy' ) plt . title ( 'Accuracy at the end of each epoch' ) plt . legend ();","title":"Plot the loss and accuracy comparisons"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#evaluate-test-data","text":"print ( test_correct ) # contains the results of all 10 epochs print () print ( f 'Test accuracy: { test_correct [ - 1 ] . item () * 100 / 10000 : .3f } %' ) # print the most recent result as a percent [tensor(4940), tensor(5519), tensor(5685), tensor(5812), tensor(5930), tensor(6048), tensor(5941), tensor(6166), tensor(6035), tensor(6105)] Test accuracy: 61.050% This is not as impressive as with MNIST, which makes sense. We would have to adjust our parameters to obtain better results. Still, it\u2019s much better than the 10% we\u2019d get with random chance!","title":"Evaluate Test Data"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#display-the-confusion-matrix","text":"In order to map predictions against ground truth, we need to run the entire test set through the model. Also, since our model was not as accurate as with MNIST, we\u2019ll use a heatmap to better display the results. # Create a loader for the entire the test set test_load_all = DataLoader ( test_data , batch_size = 10000 , shuffle = False ) with torch . no_grad (): correct = 0 for X_test , y_test in test_load_all : y_val = model ( X_test ) predicted = torch . max ( y_val , 1 )[ 1 ] correct += ( predicted == y_test ) . sum () arr = confusion_matrix ( y_test . view ( - 1 ), predicted . view ( - 1 )) df_cm = pd . DataFrame ( arr , class_names , class_names ) plt . figure ( figsize = ( 9 , 6 )) sn . heatmap ( df_cm , annot = True , fmt = \"d\" , cmap = 'BuGn' ) plt . xlabel ( \"prediction\" ) plt . ylabel ( \"label (ground truth)\" ) plt . show (); For more info on the above chart, visit the docs on scikit-learn\u2019s confusion_matrix , seaborn heatmaps , and matplotlib colormaps .","title":"Display the confusion matrix"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#examine-the-misses","text":"We can track the index positions of \u201cmissed\u201d predictions, and extract the corresponding image and label. We\u2019ll do this in batches to save screen space. misses = np . array ([]) for i in range ( len ( predicted . view ( - 1 ))): if predicted [ i ] != y_test [ i ]: misses = np . append ( misses , i ) . astype ( 'int64' ) # Display the number of misses len ( misses ) 3895 # Display the first 8 index positions misses [: 8 ] array([ 3, 4, 6, 7, 8, 17, 20, 21], dtype=int64) # Set up an iterator to feed batched rows r = 8 # row size row = iter ( np . array_split ( misses , len ( misses ) // r + 1 )) Now that everything is set up, run and re-run the cell below to view all of the missed predictions. Use Ctrl+Enter to remain on the cell between runs. You\u2019ll see a StopIteration once all the misses have been seen. np . set_printoptions ( formatter = dict ( int = lambda x : f ' { x : 5 } ' )) # to widen the printed array nextrow = next ( row ) lbls = y_test . index_select ( 0 , torch . tensor ( nextrow )) . numpy () gues = predicted . index_select ( 0 , torch . tensor ( nextrow )) . numpy () print ( \"Index:\" , nextrow ) print ( \"Label:\" , lbls ) print ( \"Class: \" , * np . array ([ class_names [ i ] for i in lbls ])) print () print ( \"Guess:\" , gues ) print ( \"Class: \" , * np . array ([ class_names [ i ] for i in gues ])) images = X_test . index_select ( 0 , torch . tensor ( nextrow )) im = make_grid ( images , nrow = r ) plt . figure ( figsize = ( 8 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Index: [ 3 4 6 7 8 17 20 21] Label: [ 0 6 1 6 3 7 7 0] Class: plane frog car frog cat horse horse plane Guess: [ 8 4 2 4 5 3 2 2] Class: ship deer bird deer dog cat bird bird","title":"Examine the misses"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/#great-job","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/","text":"================ by Jawad Haider Loading Real Image Data Image files directory tree Perform standard imports Examine the data Create a list of image filenames Create a DataFrame of image sizes (width x height) Image Preprocessing Transformations transforms.Resize( size ) transforms.CenterCrop( size ) Other affine transformations transforms.RandomRotation( degrees ) Scaling is done using transforms.Resize( size ) Let\u2019s put it all together Normalization transforms.Normalize( mean, std ) Optional: De-normalize the images Up next: performing CNN on real images! Loading Real Image Data \u00b6 So far we\u2019ve only worked with toy datasets (MNIST, CIFAR-10) conveniently packaged by torchvision, where every image has the same size and shape. Now let\u2019s learn the real deal, and work from a varied collection of .jpg files. For this section we\u2019ll be working with a version of the Cats vs. Dogs dataset inspired by a classic Kaggle competition . A quick note - do not download the dataset from Kaggle! Ours is a cleaned version of the data without any 0by0 files, etc. The images are similar to ones available from the ImageNet database. We have organized the files into train and test folders, and further divided the images into CAT and DOG subfolders. In this way the file path contains the label. Image files directory tree \u00b6 . \u2514\u2500\u2500 Data \u2514\u2500\u2500 CATS_DOGS \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 CAT \u2502 \u2502 \u251c\u2500\u2500 9374.jpg \u2502 \u2502 \u251c\u2500\u2500 9375.jpg \u2502 \u2502 \u2514\u2500\u2500 ... (3,126 files) \u2502 \u2514\u2500\u2500 DOG \u2502 \u251c\u2500\u2500 9374.jpg \u2502 \u251c\u2500\u2500 9375.jpg \u2502 \u2514\u2500\u2500 ... (3,125 files) \u2502 \u2514\u2500\u2500 train \u251c\u2500\u2500 CAT \u2502 \u251c\u2500\u2500 0.jpg \u2502 \u251c\u2500\u2500 1.jpg \u2502 \u2514\u2500\u2500 ... (9,371 files) \u2514\u2500\u2500 DOG \u251c\u2500\u2500 0.jpg \u251c\u2500\u2500 1.jpg \u2514\u2500\u2500 ... (9,372 files) Perform standard imports \u00b6 import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Examine the data \u00b6 We\u2019ll us Python\u2019s built-in os module to walk the directories collecting file names. We\u2019ll then use Pillow , an extension to the Python Imaging Library (PIL), to open image files and extract image sizes. A pandas DataFrame will provide summary statistics on our data like max/min width & height from which we can choose our model parameters. import os from PIL import Image from IPython.display import display # Filter harmless warnings import warnings warnings . filterwarnings ( \"ignore\" ) NOTE: There\u2019s a known issue with Pillow v6.0.0 where it cannot open certain .jpg files, and instead raises an UnboundLocalError: local variable \u2018photoshop\u2019 referenced before assignment This is due to be fixed in an upcoming release. For example, the images affected in the CAT test set are: ['../Data/CATS_DOGS/test/CAT/10107.jpg', '../Data/CATS_DOGS/test/CAT/10404.jpg', '../Data/CATS_DOGS/test/CAT/12086.jpg', '../Data/CATS_DOGS/test/CAT/9493.jpg', '../Data/CATS_DOGS/test/CAT/9683.jpg'] If this happens, you can either install an older version of Pillow, or use our pytorch_course_env.yml file. # TEST YOUR VERSION OF PILLOW # Run this cell. If you see a picture of a cat you're all set! with Image . open ( '../Data/CATS_DOGS/test/CAT/10107.jpg' ) as im : display ( im ) Create a list of image filenames \u00b6 path = '.. \\\\ Data \\\\ CATS_DOGS \\\\ ' img_names = [] for folder , subfolders , filenames in os . walk ( path ): for img in filenames : img_names . append ( folder + ' \\\\ ' + img ) print ( 'Images: ' , len ( img_names )) Images: 24994 Create a DataFrame of image sizes (width x height) \u00b6 It\u2019s worth noting that Image.open() doesn\u2019t read the entire image into memory, so it\u2019s a reasonable way to get image sizes. Still, this can take awhile. # Start by creating a list img_sizes = [] rejected = [] for item in img_names : try : with Image . open ( item ) as img : img_sizes . append ( img . size ) except : rejected . append ( item ) print ( f 'Images: { len ( img_sizes ) } ' ) print ( f 'Rejects: { len ( rejected ) } ' ) Images: 24994 Rejects: 0 # Convert the list to a DataFrame df = pd . DataFrame ( img_sizes ) # Run summary statistics on image widths df [ 0 ] . describe () count 24994.000000 mean 404.493518 std 108.941802 min 42.000000 25% 323.000000 50% 448.000000 75% 500.000000 max 500.000000 Name: 0, dtype: float64 # Run summary statistics on image heights df [ 1 ] . describe () count 24994.000000 mean 361.037129 std 96.936811 min 33.000000 25% 302.000000 50% 375.000000 75% 421.000000 max 500.000000 Name: 1, dtype: float64 This tells us the shortest width is 42, the shortest height is 33, the largest width and height are 500, and that most images have more than 300 pixels per side. This is useful for deciding on an input size. We\u2019ll see in the next section that 224x224 will work well for our purposes (we\u2019ll take advantage of some pre-trained models that use this size!) Image Preprocessing \u00b6 Any network we define requires consistent input data. That is, the incoming image files need to have the same number of channels (3 for red/green/blue), the same depth per channel (0-255), and the same height and width. This last requirement can be tricky. How do we transform an 800x450 pixel image into one that is 224x224? In the theory lectures we covered the following: * aspect ratio : the ratio of width to height (16:9, 1:1, etc.) An 800x450 pixel image has an aspect ration of 16:9. We can change the aspect ratio of an image by cropping it, by stretching/squeezing it, or by some combination of the two. In both cases we lose some information contained in the original. Let\u2019s say we crop 175 pixels from the left and right sides of our 800x450 image, resulting in one that\u2019s 450x450. * scale : Once we\u2019ve attained the proper aspect ratio we may need to scale an image up or down to fit our input parameters. There are several libraries we can use to scale a 450x450 image down to 224x224 with minimal loss. * normalization : when images are converted to tensors, the [0,255] rgb channels are loaded into range [0,1]. We can then normalize them using the generally accepted values of mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]. For the curious, these values were obtained by the PyTorch team using a random 10,000 sample of ImageNet images. There\u2019s a good discussion of this here , and the original source code can be found here . Transformations \u00b6 Before defining our Convolutional Network, let\u2019s look at a sample image and perform various transformations on it to see their effect. dog = Image . open ( '.. \\\\ Data \\\\ CATS_DOGS \\\\ train \\\\ DOG \\\\ 14.jpg' ) print ( dog . size ) display ( dog ) (500, 387) This is how jupyter displays the original .jpg image. Note that size is given as (width, height). Let\u2019s look at a single pixel: r , g , b = dog . getpixel (( 0 , 0 )) print ( r , g , b ) 90 95 98 The pixel at position [0,0] (upper left) of the source image has an rgb value of (90,95,98). This corresponds to this color Great! Now let\u2019s look at some specific transformations. ### transforms.ToTensor() Converts a PIL Image or numpy.ndarray (HxWxC) in the range [0, 255] to a torch.FloatTensor of shape (CxHxW) in the range [0.0, 1.0] transform = transforms . Compose ([ transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 387, 500]) This is the same image converted to a tensor and displayed using matplotlib. Note that the torch dimensions follow [channel, height, width] PyTorch automatically loads the [0,255] pixel channels to [0,1]: \\(\\frac{90}{255}=0.3529\\quad\\frac{95}{255}=0.3725\\quad\\frac{98}{255}=0.3843\\) im [:, 0 , 0 ] tensor([0.3529, 0.3725, 0.3843]) transforms.Resize( size ) \u00b6 If size is a sequence like (h, w), the output size will be matched to this. If size is an integer, the smaller edge of the image will be matched to this number. i.e, if height > width, then the image will be rescaled to (size * height / width, size) transform = transforms . Compose ([ transforms . Resize ( 224 ), transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 289]) This resized the shortest side, and scaled the other. Let\u2019s try this on a small image. small_dog = Image . open ( '../Data/CATS_DOGS/train/DOG/11.jpg' ) print ( small_dog . size ) display ( small_dog ) (135, 102) im = transform ( small_dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 296]) transforms.CenterCrop( size ) \u00b6 If size is an integer instead of sequence like (h, w), a square crop of (size, size) is made. transform = transforms . Compose ([ transforms . CenterCrop ( 224 ), transforms . ToTensor () ]) im = transform ( dog ) # this crops the original image print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 224]) It may be better to resize the image first, then crop: transform = transforms . Compose ([ transforms . Resize ( 224 ), transforms . CenterCrop ( 224 ), transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 224]) Other affine transformations \u00b6 An affine transformation is one that preserves points and straight lines. Examples include rotation, reflection, and scaling. For instance, we can double the effective size of our training set simply by flipping the images. ### transforms.RandomHorizontalFlip( p=0.5 ) Horizontally flip the given PIL image randomly with a given probability. transform = transforms . Compose ([ transforms . RandomHorizontalFlip ( p = 1 ), # normally we'd set p=0.5 transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 387, 500]) transforms.RandomRotation( degrees ) \u00b6 If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). Run the cell below several times to see a sample of rotations. transform = transforms . Compose ([ transforms . RandomRotation ( 30 ), # rotate randomly between +/- 30 degrees transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 387, 500]) Scaling is done using transforms.Resize( size ) \u00b6 transform = transforms . Compose ([ transforms . Resize (( 224 , 224 )), # be sure to pass in a list or a tuple transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 224]) Let\u2019s put it all together \u00b6 transform = transforms . Compose ([ transforms . RandomHorizontalFlip ( p = 1 ), # normally we'd set p=0.5 transforms . RandomRotation ( 30 ), transforms . Resize ( 224 ), transforms . CenterCrop ( 224 ), transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 224]) Normalization \u00b6 Once the image has been loaded into a tensor, we can perform normalization on it. This serves to make convergence happen quicker during training. The values are somewhat arbitrary - you can use a mean of 0.5 and a standard deviation of 0.5 to convert a range of [0,1] to [-1,1], for example. However, research has shown that mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] work well in practice. transforms.Normalize( mean, std ) \u00b6 Given mean: (M1,\u2026,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input tensor ### \\(\\quad\\textrm {input[channel]} = \\frac{\\textrm{input[channel] - mean[channel]}}{\\textrm {std[channel]}}\\) transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). torch.Size([3, 387, 500]) Recall that before normalization, the upper-leftmost tensor had pixel values of [0.3529, 0.3725, 0.3843] . With normalization we subtract the channel mean from the input channel, then divide by the channel std. \\(\\frac{(0.3529-0.485)}{0.229}=-0.5767\\quad\\frac{(0.3725-0.456)}{0.224}=-0.3725\\quad\\frac{(0.3843-0.406)}{0.225}=-0.0964\\) # After normalization: im [:, 0 , 0 ] tensor([-0.5767, -0.3725, -0.0964]) When displayed, matplotlib clipped this particular pixel up to [0,0,0] so it appears black on the screen. However, the appearance isn\u2019t important; the goal of normalization is improved mathematical performance. Optional: De-normalize the images \u00b6 To see the image back in its true colors, we can apply an inverse-transform to the tensor being displayed. inv_normalize = transforms . Normalize ( mean = [ - 0.485 / 0.229 , - 0.456 / 0.224 , - 0.406 / 0.225 ], std = [ 1 / 0.229 , 1 / 0.224 , 1 / 0.225 ] ) im_inv = inv_normalize ( im ) plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im_inv . numpy (), ( 1 , 2 , 0 ))); Note that the original tensor was not modified: plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Up next: performing CNN on real images! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"03 Loading Real Image Data"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#loading-real-image-data","text":"So far we\u2019ve only worked with toy datasets (MNIST, CIFAR-10) conveniently packaged by torchvision, where every image has the same size and shape. Now let\u2019s learn the real deal, and work from a varied collection of .jpg files. For this section we\u2019ll be working with a version of the Cats vs. Dogs dataset inspired by a classic Kaggle competition . A quick note - do not download the dataset from Kaggle! Ours is a cleaned version of the data without any 0by0 files, etc. The images are similar to ones available from the ImageNet database. We have organized the files into train and test folders, and further divided the images into CAT and DOG subfolders. In this way the file path contains the label.","title":"Loading Real Image Data"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#image-files-directory-tree","text":". \u2514\u2500\u2500 Data \u2514\u2500\u2500 CATS_DOGS \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 CAT \u2502 \u2502 \u251c\u2500\u2500 9374.jpg \u2502 \u2502 \u251c\u2500\u2500 9375.jpg \u2502 \u2502 \u2514\u2500\u2500 ... (3,126 files) \u2502 \u2514\u2500\u2500 DOG \u2502 \u251c\u2500\u2500 9374.jpg \u2502 \u251c\u2500\u2500 9375.jpg \u2502 \u2514\u2500\u2500 ... (3,125 files) \u2502 \u2514\u2500\u2500 train \u251c\u2500\u2500 CAT \u2502 \u251c\u2500\u2500 0.jpg \u2502 \u251c\u2500\u2500 1.jpg \u2502 \u2514\u2500\u2500 ... (9,371 files) \u2514\u2500\u2500 DOG \u251c\u2500\u2500 0.jpg \u251c\u2500\u2500 1.jpg \u2514\u2500\u2500 ... (9,372 files)","title":"Image files directory tree"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#perform-standard-imports","text":"import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#examine-the-data","text":"We\u2019ll us Python\u2019s built-in os module to walk the directories collecting file names. We\u2019ll then use Pillow , an extension to the Python Imaging Library (PIL), to open image files and extract image sizes. A pandas DataFrame will provide summary statistics on our data like max/min width & height from which we can choose our model parameters. import os from PIL import Image from IPython.display import display # Filter harmless warnings import warnings warnings . filterwarnings ( \"ignore\" ) NOTE: There\u2019s a known issue with Pillow v6.0.0 where it cannot open certain .jpg files, and instead raises an UnboundLocalError: local variable \u2018photoshop\u2019 referenced before assignment This is due to be fixed in an upcoming release. For example, the images affected in the CAT test set are: ['../Data/CATS_DOGS/test/CAT/10107.jpg', '../Data/CATS_DOGS/test/CAT/10404.jpg', '../Data/CATS_DOGS/test/CAT/12086.jpg', '../Data/CATS_DOGS/test/CAT/9493.jpg', '../Data/CATS_DOGS/test/CAT/9683.jpg'] If this happens, you can either install an older version of Pillow, or use our pytorch_course_env.yml file. # TEST YOUR VERSION OF PILLOW # Run this cell. If you see a picture of a cat you're all set! with Image . open ( '../Data/CATS_DOGS/test/CAT/10107.jpg' ) as im : display ( im )","title":"Examine the data"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#create-a-list-of-image-filenames","text":"path = '.. \\\\ Data \\\\ CATS_DOGS \\\\ ' img_names = [] for folder , subfolders , filenames in os . walk ( path ): for img in filenames : img_names . append ( folder + ' \\\\ ' + img ) print ( 'Images: ' , len ( img_names )) Images: 24994","title":"Create a list of image filenames"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#create-a-dataframe-of-image-sizes-width-x-height","text":"It\u2019s worth noting that Image.open() doesn\u2019t read the entire image into memory, so it\u2019s a reasonable way to get image sizes. Still, this can take awhile. # Start by creating a list img_sizes = [] rejected = [] for item in img_names : try : with Image . open ( item ) as img : img_sizes . append ( img . size ) except : rejected . append ( item ) print ( f 'Images: { len ( img_sizes ) } ' ) print ( f 'Rejects: { len ( rejected ) } ' ) Images: 24994 Rejects: 0 # Convert the list to a DataFrame df = pd . DataFrame ( img_sizes ) # Run summary statistics on image widths df [ 0 ] . describe () count 24994.000000 mean 404.493518 std 108.941802 min 42.000000 25% 323.000000 50% 448.000000 75% 500.000000 max 500.000000 Name: 0, dtype: float64 # Run summary statistics on image heights df [ 1 ] . describe () count 24994.000000 mean 361.037129 std 96.936811 min 33.000000 25% 302.000000 50% 375.000000 75% 421.000000 max 500.000000 Name: 1, dtype: float64 This tells us the shortest width is 42, the shortest height is 33, the largest width and height are 500, and that most images have more than 300 pixels per side. This is useful for deciding on an input size. We\u2019ll see in the next section that 224x224 will work well for our purposes (we\u2019ll take advantage of some pre-trained models that use this size!)","title":"Create a DataFrame of image sizes (width x height)"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#image-preprocessing","text":"Any network we define requires consistent input data. That is, the incoming image files need to have the same number of channels (3 for red/green/blue), the same depth per channel (0-255), and the same height and width. This last requirement can be tricky. How do we transform an 800x450 pixel image into one that is 224x224? In the theory lectures we covered the following: * aspect ratio : the ratio of width to height (16:9, 1:1, etc.) An 800x450 pixel image has an aspect ration of 16:9. We can change the aspect ratio of an image by cropping it, by stretching/squeezing it, or by some combination of the two. In both cases we lose some information contained in the original. Let\u2019s say we crop 175 pixels from the left and right sides of our 800x450 image, resulting in one that\u2019s 450x450. * scale : Once we\u2019ve attained the proper aspect ratio we may need to scale an image up or down to fit our input parameters. There are several libraries we can use to scale a 450x450 image down to 224x224 with minimal loss. * normalization : when images are converted to tensors, the [0,255] rgb channels are loaded into range [0,1]. We can then normalize them using the generally accepted values of mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]. For the curious, these values were obtained by the PyTorch team using a random 10,000 sample of ImageNet images. There\u2019s a good discussion of this here , and the original source code can be found here .","title":"Image Preprocessing"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#transformations","text":"Before defining our Convolutional Network, let\u2019s look at a sample image and perform various transformations on it to see their effect. dog = Image . open ( '.. \\\\ Data \\\\ CATS_DOGS \\\\ train \\\\ DOG \\\\ 14.jpg' ) print ( dog . size ) display ( dog ) (500, 387) This is how jupyter displays the original .jpg image. Note that size is given as (width, height). Let\u2019s look at a single pixel: r , g , b = dog . getpixel (( 0 , 0 )) print ( r , g , b ) 90 95 98 The pixel at position [0,0] (upper left) of the source image has an rgb value of (90,95,98). This corresponds to this color Great! Now let\u2019s look at some specific transformations. ### transforms.ToTensor() Converts a PIL Image or numpy.ndarray (HxWxC) in the range [0, 255] to a torch.FloatTensor of shape (CxHxW) in the range [0.0, 1.0] transform = transforms . Compose ([ transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 387, 500]) This is the same image converted to a tensor and displayed using matplotlib. Note that the torch dimensions follow [channel, height, width] PyTorch automatically loads the [0,255] pixel channels to [0,1]: \\(\\frac{90}{255}=0.3529\\quad\\frac{95}{255}=0.3725\\quad\\frac{98}{255}=0.3843\\) im [:, 0 , 0 ] tensor([0.3529, 0.3725, 0.3843])","title":"Transformations"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#transformsresizesize","text":"If size is a sequence like (h, w), the output size will be matched to this. If size is an integer, the smaller edge of the image will be matched to this number. i.e, if height > width, then the image will be rescaled to (size * height / width, size) transform = transforms . Compose ([ transforms . Resize ( 224 ), transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 289]) This resized the shortest side, and scaled the other. Let\u2019s try this on a small image. small_dog = Image . open ( '../Data/CATS_DOGS/train/DOG/11.jpg' ) print ( small_dog . size ) display ( small_dog ) (135, 102) im = transform ( small_dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 296])","title":"transforms.Resize(size)"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#transformscentercropsize","text":"If size is an integer instead of sequence like (h, w), a square crop of (size, size) is made. transform = transforms . Compose ([ transforms . CenterCrop ( 224 ), transforms . ToTensor () ]) im = transform ( dog ) # this crops the original image print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 224]) It may be better to resize the image first, then crop: transform = transforms . Compose ([ transforms . Resize ( 224 ), transforms . CenterCrop ( 224 ), transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 224])","title":"transforms.CenterCrop(size)"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#other-affine-transformations","text":"An affine transformation is one that preserves points and straight lines. Examples include rotation, reflection, and scaling. For instance, we can double the effective size of our training set simply by flipping the images. ### transforms.RandomHorizontalFlip( p=0.5 ) Horizontally flip the given PIL image randomly with a given probability. transform = transforms . Compose ([ transforms . RandomHorizontalFlip ( p = 1 ), # normally we'd set p=0.5 transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 387, 500])","title":"Other affine transformations"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#transformsrandomrotationdegrees","text":"If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). Run the cell below several times to see a sample of rotations. transform = transforms . Compose ([ transforms . RandomRotation ( 30 ), # rotate randomly between +/- 30 degrees transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 387, 500])","title":"transforms.RandomRotation(degrees)"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#scaling-is-done-using-transformsresizesize","text":"transform = transforms . Compose ([ transforms . Resize (( 224 , 224 )), # be sure to pass in a list or a tuple transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 224])","title":"Scaling is done using transforms.Resize(size)"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#lets-put-it-all-together","text":"transform = transforms . Compose ([ transforms . RandomHorizontalFlip ( p = 1 ), # normally we'd set p=0.5 transforms . RandomRotation ( 30 ), transforms . Resize ( 224 ), transforms . CenterCrop ( 224 ), transforms . ToTensor () ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); torch.Size([3, 224, 224])","title":"Let\u2019s put it all together"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#normalization","text":"Once the image has been loaded into a tensor, we can perform normalization on it. This serves to make convergence happen quicker during training. The values are somewhat arbitrary - you can use a mean of 0.5 and a standard deviation of 0.5 to convert a range of [0,1] to [-1,1], for example. However, research has shown that mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] work well in practice.","title":"Normalization"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#transformsnormalizemean-std","text":"Given mean: (M1,\u2026,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input tensor ### \\(\\quad\\textrm {input[channel]} = \\frac{\\textrm{input[channel] - mean[channel]}}{\\textrm {std[channel]}}\\) transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) im = transform ( dog ) print ( im . shape ) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). torch.Size([3, 387, 500]) Recall that before normalization, the upper-leftmost tensor had pixel values of [0.3529, 0.3725, 0.3843] . With normalization we subtract the channel mean from the input channel, then divide by the channel std. \\(\\frac{(0.3529-0.485)}{0.229}=-0.5767\\quad\\frac{(0.3725-0.456)}{0.224}=-0.3725\\quad\\frac{(0.3843-0.406)}{0.225}=-0.0964\\) # After normalization: im [:, 0 , 0 ] tensor([-0.5767, -0.3725, -0.0964]) When displayed, matplotlib clipped this particular pixel up to [0,0,0] so it appears black on the screen. However, the appearance isn\u2019t important; the goal of normalization is improved mathematical performance.","title":"transforms.Normalize(mean, std)"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#optional-de-normalize-the-images","text":"To see the image back in its true colors, we can apply an inverse-transform to the tensor being displayed. inv_normalize = transforms . Normalize ( mean = [ - 0.485 / 0.229 , - 0.456 / 0.224 , - 0.406 / 0.225 ], std = [ 1 / 0.229 , 1 / 0.224 , 1 / 0.225 ] ) im_inv = inv_normalize ( im ) plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im_inv . numpy (), ( 1 , 2 , 0 ))); Note that the original tensor was not modified: plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).","title":"Optional: De-normalize the images"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/#up-next-performing-cnn-on-real-images","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Up next: performing CNN on real images!"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/","text":"================ by Jawad Haider CNN on Custom Images Image files directory tree Perform standard imports Define transforms Prepare train and test sets, loaders Display a batch of images Define the model Instantiate the model, define loss and optimization functions Looking at the trainable parameters Train the model Save the trained model Evaluate model performance Download a pretrained model Freeze feature parameters Modify the classifier Define loss function & optimizer Train the model Run a new image through the model Great job! CNN on Custom Images \u00b6 For this exercise we\u2019re using a collection of Cats and Dogs images inspired by the classic Kaggle competition . In the last section we downloaded the files, looked at the directory structure, examined the images, and performed a variety of transforms in preparation for training. In this section we\u2019ll define our model, then feed images through a training and validation sequence using DataLoader. Image files directory tree \u00b6 . \u2514\u2500\u2500 Data \u2514\u2500\u2500 CATS_DOGS \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 CAT \u2502 \u2502 \u251c\u2500\u2500 9374.jpg \u2502 \u2502 \u251c\u2500\u2500 9375.jpg \u2502 \u2502 \u2514\u2500\u2500 ... (3,126 files) \u2502 \u2514\u2500\u2500 DOG \u2502 \u251c\u2500\u2500 9374.jpg \u2502 \u251c\u2500\u2500 9375.jpg \u2502 \u2514\u2500\u2500 ... (3,125 files) \u2502 \u2514\u2500\u2500 train \u251c\u2500\u2500 CAT \u2502 \u251c\u2500\u2500 0.jpg \u2502 \u251c\u2500\u2500 1.jpg \u2502 \u2514\u2500\u2500 ... (9,371 files) \u2514\u2500\u2500 DOG \u251c\u2500\u2500 0.jpg \u251c\u2500\u2500 1.jpg \u2514\u2500\u2500 ... (9,372 files) Perform standard imports \u00b6 import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms , models # add models to the list from torchvision.utils import make_grid import os import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline # ignore harmless warnings import warnings warnings . filterwarnings ( \"ignore\" ) Define transforms \u00b6 In the previous section we looked at a variety of transforms available for data augmentation (rotate, flip, etc.) and normalization. Here we\u2019ll combine the ones we want, including the recommended normalization parameters for mean and std per channel. train_transform = transforms . Compose ([ transforms . RandomRotation ( 10 ), # rotate +/- 10 degrees transforms . RandomHorizontalFlip (), # reverse 50% of images transforms . Resize ( 224 ), # resize shortest side to 224 pixels transforms . CenterCrop ( 224 ), # crop longest side to 224 pixels at center transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) test_transform = transforms . Compose ([ transforms . Resize ( 224 ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) Prepare train and test sets, loaders \u00b6 We\u2019re going to take advantage of a built-in torchvision dataset tool called ImageFolder . root = '../Data/CATS_DOGS' train_data = datasets . ImageFolder ( os . path . join ( root , 'train' ), transform = train_transform ) test_data = datasets . ImageFolder ( os . path . join ( root , 'test' ), transform = test_transform ) torch . manual_seed ( 42 ) train_loader = DataLoader ( train_data , batch_size = 10 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 10 , shuffle = True ) class_names = train_data . classes print ( class_names ) print ( f 'Training images available: { len ( train_data ) } ' ) print ( f 'Testing images available: { len ( test_data ) } ' ) ['CAT', 'DOG'] Training images available: 18743 Testing images available: 6251 Display a batch of images \u00b6 To verify that the training loader selects cat and dog images at random, let\u2019s show a batch of loaded images. Recall that imshow clips pixel values \\<0, so the resulting display lacks contrast. We\u2019ll apply a quick inverse transform to the input tensor so that images show their \u201ctrue\u201d colors. # Grab the first batch of 10 images for images , labels in train_loader : break # Print the labels print ( 'Label:' , labels . numpy ()) print ( 'Class:' , * np . array ([ class_names [ i ] for i in labels ])) im = make_grid ( images , nrow = 5 ) # the default nrow is 8 # Inverse normalize the images inv_normalize = transforms . Normalize ( mean = [ - 0.485 / 0.229 , - 0.456 / 0.224 , - 0.406 / 0.225 ], std = [ 1 / 0.229 , 1 / 0.224 , 1 / 0.225 ] ) im_inv = inv_normalize ( im ) # Print the images plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im_inv . numpy (), ( 1 , 2 , 0 ))); Label: [1 0 1 0 0 1 0 1 0 0] Class: DOG CAT DOG CAT CAT DOG CAT DOG CAT CAT Define the model \u00b6 We\u2019ll start by using a model similar to the one we applied to the CIFAR-10 dataset, except that here we have a binary classification (2 output channels, not 10). Also, we\u2019ll add another set of convolution/pooling layers. class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 3 , 1 ) self . conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) self . fc1 = nn . Linear ( 54 * 54 * 16 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 2 ) def forward ( self , X ): X = F . relu ( self . conv1 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = F . relu ( self . conv2 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = X . view ( - 1 , 54 * 54 * 16 ) X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) Why (54x54x16) ? With 224 pixels per side, the kernels and pooling layers result in $\\;(((224-2)/2)-2)/2 = 54.5\\;$ which rounds down to 54 pixels per side. Instantiate the model, define loss and optimization functions \u00b6 We\u2019re going to call our model \u201cCNNmodel\u201d to differentiate it from an \u201cAlexNetmodel\u201d we\u2019ll use later. torch . manual_seed ( 101 ) CNNmodel = ConvolutionalNetwork () criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( CNNmodel . parameters (), lr = 0.001 ) CNNmodel ConvolutionalNetwork( (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=46656, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=2, bias=True) ) Looking at the trainable parameters \u00b6 def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >8 } ' ) print ( f '________ \\n { sum ( params ) : >8 } ' ) count_parameters ( CNNmodel ) 162 6 864 16 5598720 120 10080 84 168 2 ________ 5610222 Train the model \u00b6 In the interests of time, we\u2019ll limit the number of training batches to 800, and the number of testing batches to 300. We\u2019ll train the model on 8000 of 18743 available images, and test it on 3000 out of 6251 images. import time start_time = time . time () epochs = 3 max_trn_batch = 800 max_tst_batch = 300 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): # Limit the number of batches if b == max_trn_batch : break b += 1 # Apply the model y_pred = CNNmodel ( X_train ) loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 200 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 10 * b : 6 } /8000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 10 * b ) : 7.3f } %' ) train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): # Limit the number of batches if b == max_tst_batch : break # Apply the model y_val = CNNmodel ( X_test ) # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 200 [ 2000/8000] loss: 0.73729956 accuracy: 57.150% epoch: 0 batch: 400 [ 4000/8000] loss: 0.69267005 accuracy: 59.100% epoch: 0 batch: 600 [ 6000/8000] loss: 0.64823747 accuracy: 61.067% epoch: 0 batch: 800 [ 8000/8000] loss: 0.45315751 accuracy: 63.200% epoch: 1 batch: 200 [ 2000/8000] loss: 0.57837021 accuracy: 69.450% epoch: 1 batch: 400 [ 4000/8000] loss: 0.65500635 accuracy: 69.700% epoch: 1 batch: 600 [ 6000/8000] loss: 0.85159266 accuracy: 70.433% epoch: 1 batch: 800 [ 8000/8000] loss: 0.47595224 accuracy: 71.025% epoch: 2 batch: 200 [ 2000/8000] loss: 0.68083632 accuracy: 74.500% epoch: 2 batch: 400 [ 4000/8000] loss: 0.32654241 accuracy: 74.025% epoch: 2 batch: 600 [ 6000/8000] loss: 0.52883595 accuracy: 74.467% epoch: 2 batch: 800 [ 8000/8000] loss: 0.31269351 accuracy: 74.950% Duration: 1142 seconds Save the trained model \u00b6 torch . save ( CNNmodel . state_dict (), 'CustomImageCNNModel.pt' ) Evaluate model performance \u00b6 plt . plot ( train_losses , label = 'training loss' ) plt . plot ( test_losses , label = 'validation loss' ) plt . title ( 'Loss at the end of each epoch' ) plt . legend (); plt . plot ([ t / 80 for t in train_correct ], label = 'training accuracy' ) plt . plot ([ t / 30 for t in test_correct ], label = 'validation accuracy' ) plt . title ( 'Accuracy at the end of each epoch' ) plt . legend (); print ( test_correct ) print ( f 'Test accuracy: { test_correct [ - 1 ] . item () * 100 / 3000 : .3f } %' ) [tensor(2109), tensor(2195), tensor(2254)] Test accuracy: 75.133% Download a pretrained model \u00b6 Torchvision has a number of proven models available through torchvision.models : AlexNet VGG ResNet SqueezeNet DenseNet Inception GoogLeNet ShuffleNet MobileNet ResNeXt These have all been trained on the ImageNet database of images. Our only task is to reduce the output of the fully connected layers from (typically) 1000 categories to just 2. To access the models, you can construct a model with random weights by calling its constructor: resnet18 = models.resnet18() You can also obtain a pre-trained model by passing pretrained=True: resnet18 = models.resnet18(pretrained=True) All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. Feel free to investigate the different models available. Each one will be downloaded to a cache directory the first time they\u2019re accessed - from then on they\u2019ll be available locally. For its simplicity and effectiveness, we\u2019ll use AlexNet: AlexNetmodel = models . alexnet ( pretrained = True ) AlexNetmodel AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) This model uses torch.nn.AdaptiveAvgPool2d( output_size ) to convert the large matrix coming out of the convolutional layers to a (6x6)x256 matrix being fed into the fully connected layers. Freeze feature parameters \u00b6 We want to freeze the pre-trained weights & biases. We set .requires_grad to False so we don\u2019t backprop through them. for param in AlexNetmodel . parameters (): param . requires_grad = False Modify the classifier \u00b6 Next we need to modify the fully connected layers to produce a binary output. The section is labeled \u201cclassifier\u201d in the AlexNet model. Note that when we assign new layers, their parameters default to .requires_grad=True . torch . manual_seed ( 42 ) AlexNetmodel . classifier = nn . Sequential ( nn . Linear ( 9216 , 1024 ), nn . ReLU (), nn . Dropout ( 0.4 ), nn . Linear ( 1024 , 2 ), nn . LogSoftmax ( dim = 1 )) AlexNetmodel AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Linear(in_features=9216, out_features=1024, bias=True) (1): ReLU() (2): Dropout(p=0.4) (3): Linear(in_features=1024, out_features=2, bias=True) (4): LogSoftmax() ) ) # These are the TRAINABLE parameters: count_parameters ( AlexNetmodel ) 9437184 1024 2048 2 ________ 9440258 Define loss function & optimizer \u00b6 We only want to optimize the classifier parameters, as the feature parameters are frozen. criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( AlexNetmodel . classifier . parameters (), lr = 0.001 ) Train the model \u00b6 Remember, we\u2019re only training the fully connected layers. The convolutional layers have fixed weights and biases. For this reason, we only need to run one epoch. import time start_time = time . time () epochs = 1 max_trn_batch = 800 max_tst_batch = 300 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): if b == max_trn_batch : break b += 1 # Apply the model y_pred = AlexNetmodel ( X_train ) loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 200 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 10 * b : 6 } /8000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 10 * b ) : 7.3f } %' ) train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): if b == max_tst_batch : break # Apply the model y_val = AlexNetmodel ( X_test ) # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 200 [ 2000/8000] loss: 0.01413172 accuracy: 88.900% epoch: 0 batch: 400 [ 4000/8000] loss: 0.24275371 accuracy: 90.825% epoch: 0 batch: 600 [ 6000/8000] loss: 0.09340305 accuracy: 91.900% epoch: 0 batch: 800 [ 8000/8000] loss: 0.07545806 accuracy: 92.250% Duration: 513 seconds print ( test_correct ) print ( f 'Test accuracy: { test_correct [ - 1 ] . item () * 100 / 3000 : .3f } %' ) [tensor(2810)] Test accuracy: 93.667% Run a new image through the model \u00b6 We can also pass a single image through the model to obtain a prediction. Pick a number from 0 to 6250, assign it to \u201cx\u201d, and we\u2019ll use that value to select an image from the Cats and Dogs test set. x = 2019 im = inv_normalize ( test_data [ x ][ 0 ]) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); test_data [ x ][ 0 ] . shape torch.Size([3, 224, 224]) # CNN Model Prediction: CNNmodel . eval () with torch . no_grad (): new_pred = CNNmodel ( test_data [ x ][ 0 ] . view ( 1 , 3 , 224 , 224 )) . argmax () print ( f 'Predicted value: { new_pred . item () } { class_names [ new_pred . item ()] } ' ) Predicted value: 1 DOG # AlexNet Model Prediction: AlexNetmodel . eval () with torch . no_grad (): new_pred = AlexNetmodel ( test_data [ x ][ 0 ] . view ( 1 , 3 , 224 , 224 )) . argmax () print ( f 'Predicted value: { new_pred . item () } { class_names [ new_pred . item ()] } ' ) Predicted value: 0 CAT Great job! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"04 CNN on Custom Images"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#cnn-on-custom-images","text":"For this exercise we\u2019re using a collection of Cats and Dogs images inspired by the classic Kaggle competition . In the last section we downloaded the files, looked at the directory structure, examined the images, and performed a variety of transforms in preparation for training. In this section we\u2019ll define our model, then feed images through a training and validation sequence using DataLoader.","title":"CNN on Custom Images"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#image-files-directory-tree","text":". \u2514\u2500\u2500 Data \u2514\u2500\u2500 CATS_DOGS \u251c\u2500\u2500 test \u2502 \u251c\u2500\u2500 CAT \u2502 \u2502 \u251c\u2500\u2500 9374.jpg \u2502 \u2502 \u251c\u2500\u2500 9375.jpg \u2502 \u2502 \u2514\u2500\u2500 ... (3,126 files) \u2502 \u2514\u2500\u2500 DOG \u2502 \u251c\u2500\u2500 9374.jpg \u2502 \u251c\u2500\u2500 9375.jpg \u2502 \u2514\u2500\u2500 ... (3,125 files) \u2502 \u2514\u2500\u2500 train \u251c\u2500\u2500 CAT \u2502 \u251c\u2500\u2500 0.jpg \u2502 \u251c\u2500\u2500 1.jpg \u2502 \u2514\u2500\u2500 ... (9,371 files) \u2514\u2500\u2500 DOG \u251c\u2500\u2500 0.jpg \u251c\u2500\u2500 1.jpg \u2514\u2500\u2500 ... (9,372 files)","title":"Image files directory tree"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#perform-standard-imports","text":"import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms , models # add models to the list from torchvision.utils import make_grid import os import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline # ignore harmless warnings import warnings warnings . filterwarnings ( \"ignore\" )","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#define-transforms","text":"In the previous section we looked at a variety of transforms available for data augmentation (rotate, flip, etc.) and normalization. Here we\u2019ll combine the ones we want, including the recommended normalization parameters for mean and std per channel. train_transform = transforms . Compose ([ transforms . RandomRotation ( 10 ), # rotate +/- 10 degrees transforms . RandomHorizontalFlip (), # reverse 50% of images transforms . Resize ( 224 ), # resize shortest side to 224 pixels transforms . CenterCrop ( 224 ), # crop longest side to 224 pixels at center transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) test_transform = transforms . Compose ([ transforms . Resize ( 224 ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ])","title":"Define transforms"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#prepare-train-and-test-sets-loaders","text":"We\u2019re going to take advantage of a built-in torchvision dataset tool called ImageFolder . root = '../Data/CATS_DOGS' train_data = datasets . ImageFolder ( os . path . join ( root , 'train' ), transform = train_transform ) test_data = datasets . ImageFolder ( os . path . join ( root , 'test' ), transform = test_transform ) torch . manual_seed ( 42 ) train_loader = DataLoader ( train_data , batch_size = 10 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 10 , shuffle = True ) class_names = train_data . classes print ( class_names ) print ( f 'Training images available: { len ( train_data ) } ' ) print ( f 'Testing images available: { len ( test_data ) } ' ) ['CAT', 'DOG'] Training images available: 18743 Testing images available: 6251","title":"Prepare train and test sets, loaders"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#display-a-batch-of-images","text":"To verify that the training loader selects cat and dog images at random, let\u2019s show a batch of loaded images. Recall that imshow clips pixel values \\<0, so the resulting display lacks contrast. We\u2019ll apply a quick inverse transform to the input tensor so that images show their \u201ctrue\u201d colors. # Grab the first batch of 10 images for images , labels in train_loader : break # Print the labels print ( 'Label:' , labels . numpy ()) print ( 'Class:' , * np . array ([ class_names [ i ] for i in labels ])) im = make_grid ( images , nrow = 5 ) # the default nrow is 8 # Inverse normalize the images inv_normalize = transforms . Normalize ( mean = [ - 0.485 / 0.229 , - 0.456 / 0.224 , - 0.406 / 0.225 ], std = [ 1 / 0.229 , 1 / 0.224 , 1 / 0.225 ] ) im_inv = inv_normalize ( im ) # Print the images plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im_inv . numpy (), ( 1 , 2 , 0 ))); Label: [1 0 1 0 0 1 0 1 0 0] Class: DOG CAT DOG CAT CAT DOG CAT DOG CAT CAT","title":"Display a batch of images"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#define-the-model","text":"We\u2019ll start by using a model similar to the one we applied to the CIFAR-10 dataset, except that here we have a binary classification (2 output channels, not 10). Also, we\u2019ll add another set of convolution/pooling layers. class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 3 , 1 ) self . conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) self . fc1 = nn . Linear ( 54 * 54 * 16 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 2 ) def forward ( self , X ): X = F . relu ( self . conv1 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = F . relu ( self . conv2 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = X . view ( - 1 , 54 * 54 * 16 ) X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) Why (54x54x16) ? With 224 pixels per side, the kernels and pooling layers result in $\\;(((224-2)/2)-2)/2 = 54.5\\;$ which rounds down to 54 pixels per side.","title":"Define the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#instantiate-the-model-define-loss-and-optimization-functions","text":"We\u2019re going to call our model \u201cCNNmodel\u201d to differentiate it from an \u201cAlexNetmodel\u201d we\u2019ll use later. torch . manual_seed ( 101 ) CNNmodel = ConvolutionalNetwork () criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( CNNmodel . parameters (), lr = 0.001 ) CNNmodel ConvolutionalNetwork( (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=46656, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=2, bias=True) )","title":"Instantiate the model, define loss and optimization functions"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#looking-at-the-trainable-parameters","text":"def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >8 } ' ) print ( f '________ \\n { sum ( params ) : >8 } ' ) count_parameters ( CNNmodel ) 162 6 864 16 5598720 120 10080 84 168 2 ________ 5610222","title":"Looking at the trainable parameters"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#train-the-model","text":"In the interests of time, we\u2019ll limit the number of training batches to 800, and the number of testing batches to 300. We\u2019ll train the model on 8000 of 18743 available images, and test it on 3000 out of 6251 images. import time start_time = time . time () epochs = 3 max_trn_batch = 800 max_tst_batch = 300 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): # Limit the number of batches if b == max_trn_batch : break b += 1 # Apply the model y_pred = CNNmodel ( X_train ) loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 200 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 10 * b : 6 } /8000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 10 * b ) : 7.3f } %' ) train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): # Limit the number of batches if b == max_tst_batch : break # Apply the model y_val = CNNmodel ( X_test ) # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 200 [ 2000/8000] loss: 0.73729956 accuracy: 57.150% epoch: 0 batch: 400 [ 4000/8000] loss: 0.69267005 accuracy: 59.100% epoch: 0 batch: 600 [ 6000/8000] loss: 0.64823747 accuracy: 61.067% epoch: 0 batch: 800 [ 8000/8000] loss: 0.45315751 accuracy: 63.200% epoch: 1 batch: 200 [ 2000/8000] loss: 0.57837021 accuracy: 69.450% epoch: 1 batch: 400 [ 4000/8000] loss: 0.65500635 accuracy: 69.700% epoch: 1 batch: 600 [ 6000/8000] loss: 0.85159266 accuracy: 70.433% epoch: 1 batch: 800 [ 8000/8000] loss: 0.47595224 accuracy: 71.025% epoch: 2 batch: 200 [ 2000/8000] loss: 0.68083632 accuracy: 74.500% epoch: 2 batch: 400 [ 4000/8000] loss: 0.32654241 accuracy: 74.025% epoch: 2 batch: 600 [ 6000/8000] loss: 0.52883595 accuracy: 74.467% epoch: 2 batch: 800 [ 8000/8000] loss: 0.31269351 accuracy: 74.950% Duration: 1142 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#save-the-trained-model","text":"torch . save ( CNNmodel . state_dict (), 'CustomImageCNNModel.pt' )","title":"Save the trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#evaluate-model-performance","text":"plt . plot ( train_losses , label = 'training loss' ) plt . plot ( test_losses , label = 'validation loss' ) plt . title ( 'Loss at the end of each epoch' ) plt . legend (); plt . plot ([ t / 80 for t in train_correct ], label = 'training accuracy' ) plt . plot ([ t / 30 for t in test_correct ], label = 'validation accuracy' ) plt . title ( 'Accuracy at the end of each epoch' ) plt . legend (); print ( test_correct ) print ( f 'Test accuracy: { test_correct [ - 1 ] . item () * 100 / 3000 : .3f } %' ) [tensor(2109), tensor(2195), tensor(2254)] Test accuracy: 75.133%","title":"Evaluate model performance"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#download-a-pretrained-model","text":"Torchvision has a number of proven models available through torchvision.models : AlexNet VGG ResNet SqueezeNet DenseNet Inception GoogLeNet ShuffleNet MobileNet ResNeXt These have all been trained on the ImageNet database of images. Our only task is to reduce the output of the fully connected layers from (typically) 1000 categories to just 2. To access the models, you can construct a model with random weights by calling its constructor: resnet18 = models.resnet18() You can also obtain a pre-trained model by passing pretrained=True: resnet18 = models.resnet18(pretrained=True) All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. Feel free to investigate the different models available. Each one will be downloaded to a cache directory the first time they\u2019re accessed - from then on they\u2019ll be available locally. For its simplicity and effectiveness, we\u2019ll use AlexNet: AlexNetmodel = models . alexnet ( pretrained = True ) AlexNetmodel AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) This model uses torch.nn.AdaptiveAvgPool2d( output_size ) to convert the large matrix coming out of the convolutional layers to a (6x6)x256 matrix being fed into the fully connected layers.","title":"Download a pretrained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#freeze-feature-parameters","text":"We want to freeze the pre-trained weights & biases. We set .requires_grad to False so we don\u2019t backprop through them. for param in AlexNetmodel . parameters (): param . requires_grad = False","title":"Freeze feature parameters"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#modify-the-classifier","text":"Next we need to modify the fully connected layers to produce a binary output. The section is labeled \u201cclassifier\u201d in the AlexNet model. Note that when we assign new layers, their parameters default to .requires_grad=True . torch . manual_seed ( 42 ) AlexNetmodel . classifier = nn . Sequential ( nn . Linear ( 9216 , 1024 ), nn . ReLU (), nn . Dropout ( 0.4 ), nn . Linear ( 1024 , 2 ), nn . LogSoftmax ( dim = 1 )) AlexNetmodel AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Linear(in_features=9216, out_features=1024, bias=True) (1): ReLU() (2): Dropout(p=0.4) (3): Linear(in_features=1024, out_features=2, bias=True) (4): LogSoftmax() ) ) # These are the TRAINABLE parameters: count_parameters ( AlexNetmodel ) 9437184 1024 2048 2 ________ 9440258","title":"Modify the classifier"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#define-loss-function-optimizer","text":"We only want to optimize the classifier parameters, as the feature parameters are frozen. criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( AlexNetmodel . classifier . parameters (), lr = 0.001 )","title":"Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#train-the-model_1","text":"Remember, we\u2019re only training the fully connected layers. The convolutional layers have fixed weights and biases. For this reason, we only need to run one epoch. import time start_time = time . time () epochs = 1 max_trn_batch = 800 max_tst_batch = 300 train_losses = [] test_losses = [] train_correct = [] test_correct = [] for i in range ( epochs ): trn_corr = 0 tst_corr = 0 # Run the training batches for b , ( X_train , y_train ) in enumerate ( train_loader ): if b == max_trn_batch : break b += 1 # Apply the model y_pred = AlexNetmodel ( X_train ) loss = criterion ( y_pred , y_train ) # Tally the number of correct predictions predicted = torch . max ( y_pred . data , 1 )[ 1 ] batch_corr = ( predicted == y_train ) . sum () trn_corr += batch_corr # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # Print interim results if b % 200 == 0 : print ( f 'epoch: { i : 2 } batch: { b : 4 } [ { 10 * b : 6 } /8000] loss: { loss . item () : 10.8f } \\ accuracy: { trn_corr . item () * 100 / ( 10 * b ) : 7.3f } %' ) train_losses . append ( loss ) train_correct . append ( trn_corr ) # Run the testing batches with torch . no_grad (): for b , ( X_test , y_test ) in enumerate ( test_loader ): if b == max_tst_batch : break # Apply the model y_val = AlexNetmodel ( X_test ) # Tally the number of correct predictions predicted = torch . max ( y_val . data , 1 )[ 1 ] tst_corr += ( predicted == y_test ) . sum () loss = criterion ( y_val , y_test ) test_losses . append ( loss ) test_correct . append ( tst_corr ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 0 batch: 200 [ 2000/8000] loss: 0.01413172 accuracy: 88.900% epoch: 0 batch: 400 [ 4000/8000] loss: 0.24275371 accuracy: 90.825% epoch: 0 batch: 600 [ 6000/8000] loss: 0.09340305 accuracy: 91.900% epoch: 0 batch: 800 [ 8000/8000] loss: 0.07545806 accuracy: 92.250% Duration: 513 seconds print ( test_correct ) print ( f 'Test accuracy: { test_correct [ - 1 ] . item () * 100 / 3000 : .3f } %' ) [tensor(2810)] Test accuracy: 93.667%","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#run-a-new-image-through-the-model","text":"We can also pass a single image through the model to obtain a prediction. Pick a number from 0 to 6250, assign it to \u201cx\u201d, and we\u2019ll use that value to select an image from the Cats and Dogs test set. x = 2019 im = inv_normalize ( test_data [ x ][ 0 ]) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); test_data [ x ][ 0 ] . shape torch.Size([3, 224, 224]) # CNN Model Prediction: CNNmodel . eval () with torch . no_grad (): new_pred = CNNmodel ( test_data [ x ][ 0 ] . view ( 1 , 3 , 224 , 224 )) . argmax () print ( f 'Predicted value: { new_pred . item () } { class_names [ new_pred . item ()] } ' ) Predicted value: 1 DOG # AlexNet Model Prediction: AlexNetmodel . eval () with torch . no_grad (): new_pred = AlexNetmodel ( test_data [ x ][ 0 ] . view ( 1 , 3 , 224 , 224 )) . argmax () print ( f 'Predicted value: { new_pred . item () } { class_names [ new_pred . item ()] } ' ) Predicted value: 0 CAT","title":"Run a new image through the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/#great-job","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/","text":"================ by Jawad Haider CNN Exercises Perform standard imports, load the Fashion-MNIST dataset 1. Create data loaders 2. Examine a batch of images Downsampling 4. If the sample from question 3 is then passed through a 2x2 MaxPooling layer, what is the resulting matrix size? CNN definition 5. Define a convolutional neural network Trainable parameters 6. What is the total number of trainable parameters (weights & biases) in the model above? 7. Define loss function & optimizer 8. Train the model 9. Evaluate the model Great job! CNN Exercises \u00b6 For these exercises we\u2019ll work with the Fashion-MNIST dataset, also available through torchvision . Like MNIST, this dataset consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: 0. T-shirt/top 1. Trouser 2. Pullover 3. Dress 4. Coat 5. Sandal 6. Shirt 7. Sneaker 8. Bag 9. Ankle boot IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! Perform standard imports, load the Fashion-MNIST dataset \u00b6 Run the cell below to load the libraries needed for this exercise and the Fashion-MNIST dataset. PyTorch makes the Fashion-MNIST dataset available through torchvision . The first time it\u2019s called, the dataset will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms from torchvision.utils import make_grid import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt % matplotlib inline transform = transforms . ToTensor () train_data = datasets . FashionMNIST ( root = '../Data' , train = True , download = True , transform = transform ) test_data = datasets . FashionMNIST ( root = '../Data' , train = False , download = True , transform = transform ) class_names = [ 'T-shirt' , 'Trouser' , 'Sweater' , 'Dress' , 'Coat' , 'Sandal' , 'Shirt' , 'Sneaker' , 'Bag' , 'Boot' ] 1. Create data loaders \u00b6 Use DataLoader to create a train_loader and a test_loader . Batch sizes should be 10 for both. # CODE HERE # DON'T WRITE HERE 2. Examine a batch of images \u00b6 Use DataLoader, make_grid and matplotlib to display the first batch of 10 images. OPTIONAL: display the labels as well # CODE HERE # DON'T WRITE HERE # IMAGES ONLY # DON'T WRITE HERE # IMAGES AND LABELS Label: [9 2 5 9 4 2 1 2 7 3] Class: Boot Sweater Sandal Boot Coat Sweater Trouser Sweater Sneaker Dress Downsampling \u00b6 3. If a 28x28 image is passed through a Convolutional layer using a 5x5 filter, a step size of 1, and no padding, what is the resulting matrix size? ################################################## ###### ONLY RUN THIS TO CHECK YOUR ANSWER! ###### ################################################ # Run the code below to check your answer: conv = nn . Conv2d ( 1 , 1 , 5 , 1 ) for x , labels in train_loader : print ( 'Orig size:' , x . shape ) break x = conv ( x ) print ( 'Down size:' , x . shape ) 4. If the sample from question 3 is then passed through a 2x2 MaxPooling layer, what is the resulting matrix size? \u00b6 ################################################## ###### ONLY RUN THIS TO CHECK YOUR ANSWER! ###### ################################################ # Run the code below to check your answer: x = F . max_pool2d ( x , 2 , 2 ) print ( 'Down size:' , x . shape ) CNN definition \u00b6 5. Define a convolutional neural network \u00b6 Define a CNN model that can be trained on the Fashion-MNIST dataset. The model should contain two convolutional layers, two pooling layers, and two fully connected layers. You can use any number of neurons per layer so long as the model takes in a 28x28 image and returns an output of 10. Portions of the definition have been filled in for convenience. # CODE HERE class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () pass def forward ( self , X ): pass return torch . manual_seed ( 101 ) model = ConvolutionalNetwork () Trainable parameters \u00b6 6. What is the total number of trainable parameters (weights & biases) in the model above? \u00b6 Answers will vary depending on your model definition. # CODE HERE 7. Define loss function & optimizer \u00b6 Define a loss function called \u201ccriterion\u201d and an optimizer called \u201coptimizer\u201d. You can use any functions you want, although we used Cross Entropy Loss and Adam (learning rate of 0.001) respectively. # CODE HERE # DON'T WRITE HERE 8. Train the model \u00b6 Don\u2019t worry about tracking loss values, displaying results, or validating the test set. Just train the model through 5 epochs. We\u2019ll evaluate the trained model in the next step. OPTIONAL: print something after each epoch to indicate training progress. # CODE HERE 1 of 5 epochs completed 2 of 5 epochs completed 3 of 5 epochs completed 4 of 5 epochs completed 5 of 5 epochs completed 9. Evaluate the model \u00b6 Set model.eval() and determine the percentage correct out of 10,000 total test images. # CODE HERE Test accuracy: 8733/10000 = 87.330% Great job! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"05 CNN Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#cnn-exercises","text":"For these exercises we\u2019ll work with the Fashion-MNIST dataset, also available through torchvision . Like MNIST, this dataset consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: 0. T-shirt/top 1. Trouser 2. Pullover 3. Dress 4. Coat 5. Sandal 6. Shirt 7. Sneaker 8. Bag 9. Ankle boot IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"CNN Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#perform-standard-imports-load-the-fashion-mnist-dataset","text":"Run the cell below to load the libraries needed for this exercise and the Fashion-MNIST dataset. PyTorch makes the Fashion-MNIST dataset available through torchvision . The first time it\u2019s called, the dataset will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms from torchvision.utils import make_grid import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt % matplotlib inline transform = transforms . ToTensor () train_data = datasets . FashionMNIST ( root = '../Data' , train = True , download = True , transform = transform ) test_data = datasets . FashionMNIST ( root = '../Data' , train = False , download = True , transform = transform ) class_names = [ 'T-shirt' , 'Trouser' , 'Sweater' , 'Dress' , 'Coat' , 'Sandal' , 'Shirt' , 'Sneaker' , 'Bag' , 'Boot' ]","title":"Perform standard imports, load the Fashion-MNIST dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#1-create-data-loaders","text":"Use DataLoader to create a train_loader and a test_loader . Batch sizes should be 10 for both. # CODE HERE # DON'T WRITE HERE","title":"1. Create data loaders"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#2-examine-a-batch-of-images","text":"Use DataLoader, make_grid and matplotlib to display the first batch of 10 images. OPTIONAL: display the labels as well # CODE HERE # DON'T WRITE HERE # IMAGES ONLY # DON'T WRITE HERE # IMAGES AND LABELS Label: [9 2 5 9 4 2 1 2 7 3] Class: Boot Sweater Sandal Boot Coat Sweater Trouser Sweater Sneaker Dress","title":"2. Examine a batch of images"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#downsampling","text":"","title":"Downsampling"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#4-if-the-sample-from-question-3-is-then-passed-through-a-2x2-maxpooling-layer-what-is-the-resulting-matrix-size","text":"################################################## ###### ONLY RUN THIS TO CHECK YOUR ANSWER! ###### ################################################ # Run the code below to check your answer: x = F . max_pool2d ( x , 2 , 2 ) print ( 'Down size:' , x . shape )","title":"4. If the sample from question 3 is then passed through a 2x2 MaxPooling layer, what is the resulting matrix size?"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#cnn-definition","text":"","title":"CNN definition"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#5-define-a-convolutional-neural-network","text":"Define a CNN model that can be trained on the Fashion-MNIST dataset. The model should contain two convolutional layers, two pooling layers, and two fully connected layers. You can use any number of neurons per layer so long as the model takes in a 28x28 image and returns an output of 10. Portions of the definition have been filled in for convenience. # CODE HERE class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () pass def forward ( self , X ): pass return torch . manual_seed ( 101 ) model = ConvolutionalNetwork ()","title":"5. Define a convolutional neural network"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#trainable-parameters","text":"","title":"Trainable parameters"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#6-what-is-the-total-number-of-trainable-parameters-weights-biases-in-the-model-above","text":"Answers will vary depending on your model definition. # CODE HERE","title":"6. What is the total number of trainable parameters (weights &amp; biases) in the model above?"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#7-define-loss-function-optimizer","text":"Define a loss function called \u201ccriterion\u201d and an optimizer called \u201coptimizer\u201d. You can use any functions you want, although we used Cross Entropy Loss and Adam (learning rate of 0.001) respectively. # CODE HERE # DON'T WRITE HERE","title":"7. Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#8-train-the-model","text":"Don\u2019t worry about tracking loss values, displaying results, or validating the test set. Just train the model through 5 epochs. We\u2019ll evaluate the trained model in the next step. OPTIONAL: print something after each epoch to indicate training progress. # CODE HERE 1 of 5 epochs completed 2 of 5 epochs completed 3 of 5 epochs completed 4 of 5 epochs completed 5 of 5 epochs completed","title":"8. Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#9-evaluate-the-model","text":"Set model.eval() and determine the percentage correct out of 10,000 total test images. # CODE HERE Test accuracy: 8733/10000 = 87.330%","title":"9. Evaluate the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/#great-job","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/","text":"================ by Jawad Haider CNN Exercises - Solutions Perform standard imports, load the Fashion-MNIST dataset 1. Create data loaders 2. Examine a batch of images Downsampling 4. If the sample from question 3 is then passed through a 2x2 MaxPooling layer, what is the resulting matrix size? CNN definition 5. Define a convolutional neural network Trainable parameters 6. What is the total number of trainable parameters (weights & biases) in the model above? 7. Define loss function & optimizer 8. Train the model 9. Evaluate the model Great job! CNN Exercises - Solutions \u00b6 For these exercises we\u2019ll work with the Fashion-MNIST dataset, also available through torchvision . Like MNIST, this dataset consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: 0. T-shirt/top 1. Trouser 2. Pullover 3. Dress 4. Coat 5. Sandal 6. Shirt 7. Sneaker 8. Bag 9. Ankle boot IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! Perform standard imports, load the Fashion-MNIST dataset \u00b6 Run the cell below to load the libraries needed for this exercise and the Fashion-MNIST dataset. PyTorch makes the Fashion-MNIST dataset available through torchvision . The first time it\u2019s called, the dataset will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms from torchvision.utils import make_grid import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline transform = transforms . ToTensor () train_data = datasets . FashionMNIST ( root = '../Data' , train = True , download = True , transform = transform ) test_data = datasets . FashionMNIST ( root = '../Data' , train = False , download = True , transform = transform ) class_names = [ 'T-shirt' , 'Trouser' , 'Sweater' , 'Dress' , 'Coat' , 'Sandal' , 'Shirt' , 'Sneaker' , 'Bag' , 'Boot' ] 1. Create data loaders \u00b6 Use DataLoader to create a train_loader and a test_loader . Batch sizes should be 10 for both. # CODE HERE # DON'T WRITE HERE train_loader = DataLoader ( train_data , batch_size = 10 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 10 , shuffle = False ) 2. Examine a batch of images \u00b6 Use DataLoader, make_grid and matplotlib to display the first batch of 10 images. OPTIONAL: display the labels as well # CODE HERE # DON'T WRITE HERE # IMAGES ONLY for images , labels in train_loader : break im = make_grid ( images , nrow = 10 ) plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); # DON'T WRITE HERE # IMAGES AND LABELS for images , labels in train_loader : break print ( 'Label: ' , labels . numpy ()) print ( 'Class: ' , * np . array ([ class_names [ i ] for i in labels ])) im = make_grid ( images , nrow = 10 ) plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Label: [3 0 0 3 1 3 5 4 8 8] Class: Dress T-shirt T-shirt Dress Trouser Dress Sandal Coat Bag Bag Downsampling \u00b6 3. If a 28x28 image is passed through a Convolutional layer using a 5x5 filter, a step size of 1, and no padding, what is the resulting matrix size? A 5x5 filter leaves a two-pixel border on each side, so the overall dimension is reduced by 4. The result is a 24x24 matrix. # Run the code below to check your answer: conv = nn . Conv2d ( 1 , 1 , 5 , 1 ) for x , labels in train_loader : print ( 'Orig size:' , x . shape ) break x = conv ( x ) print ( 'Down size:' , x . shape ) Orig size: torch.Size([10, 1, 28, 28]) Down size: torch.Size([10, 1, 24, 24]) 4. If the sample from question 3 is then passed through a 2x2 MaxPooling layer, what is the resulting matrix size? \u00b6 If a 2x2 pooling layer is applied to a 24x24 matrix, each side is divided by two, and rounded down if necessary. The result is a 12x12 matrix. # Run the code below to check your answer: x = F . max_pool2d ( x , 2 , 2 ) print ( 'Down size:' , x . shape ) Down size: torch.Size([10, 1, 12, 12]) CNN definition \u00b6 5. Define a convolutional neural network \u00b6 Define a CNN model that can be trained on the Fashion-MNIST dataset. The model should contain two convolutional layers, two pooling layers, and two fully connected layers. You can use any number of neurons per layer so long as the model takes in a 28x28 image and returns an output of 10. Portions of the definition have been filled in for convenience. # DON'T WRITE HERE class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 1 , 6 , 3 , 1 ) self . conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) self . fc1 = nn . Linear ( 5 * 5 * 16 , 100 ) self . fc2 = nn . Linear ( 100 , 10 ) def forward ( self , X ): X = F . relu ( self . conv1 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = F . relu ( self . conv2 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = X . view ( - 1 , 5 * 5 * 16 ) X = F . relu ( self . fc1 ( X )) X = self . fc2 ( X ) return F . log_softmax ( X , dim = 1 ) torch . manual_seed ( 101 ) model = ConvolutionalNetwork () Trainable parameters \u00b6 6. What is the total number of trainable parameters (weights & biases) in the model above? \u00b6 Answers will vary depending on your model definition. $\\quad\\begin{split}(1\\times6\\times3\\times3)+6+(6\\times16\\times3\\times3)+16+(400\\times100)+100+(100\\times10)+10 &=\\\\ 54+6+864+16+40000+100+1000+10 &= 42,050\\end{split}$ # Run the code below to check your answer: def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 54 6 864 16 40000 100 1000 10 ______ 42050 7. Define loss function & optimizer \u00b6 Define a loss function called \u201ccriterion\u201d and an optimizer called \u201coptimizer\u201d. You can use any functions you want, although we used Cross Entropy Loss and Adam (learning rate of 0.001) respectively. # DON'T WRITE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) 8. Train the model \u00b6 Don\u2019t worry about tracking loss values, displaying results, or validating the test set. Just train the model through 5 epochs. We\u2019ll evaluate the trained model in the next step. OPTIONAL: print something after each epoch to indicate training progress. # DON'T WRITE HERE epochs = 5 for i in range ( epochs ): for X_train , y_train in train_loader : # Apply the model y_pred = model ( X_train ) loss = criterion ( y_pred , y_train ) # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # OPTIONAL print statement print ( f ' { i + 1 } of { epochs } epochs completed' ) 1 of 5 epochs completed 2 of 5 epochs completed 3 of 5 epochs completed 4 of 5 epochs completed 5 of 5 epochs completed 9. Evaluate the model \u00b6 Set model.eval() and determine the percentage correct out of 10,000 total test images. # DON'T WRITE HERE model . eval () with torch . no_grad (): correct = 0 for X_test , y_test in test_loader : y_val = model ( X_test ) predicted = torch . max ( y_val , 1 )[ 1 ] correct += ( predicted == y_test ) . sum () print ( f 'Test accuracy: { correct . item () } / { len ( test_data ) } = { correct . item () * 100 / ( len ( test_data )) : 7.3f } %' ) Test accuracy: 8733/10000 = 87.330% Great job! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"06 CNN Exercises Solutions"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#cnn-exercises-solutions","text":"For these exercises we\u2019ll work with the Fashion-MNIST dataset, also available through torchvision . Like MNIST, this dataset consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: 0. T-shirt/top 1. Trouser 2. Pullover 3. Dress 4. Coat 5. Sandal 6. Shirt 7. Sneaker 8. Bag 9. Ankle boot IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"CNN Exercises - Solutions"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#perform-standard-imports-load-the-fashion-mnist-dataset","text":"Run the cell below to load the libraries needed for this exercise and the Fashion-MNIST dataset. PyTorch makes the Fashion-MNIST dataset available through torchvision . The first time it\u2019s called, the dataset will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download. import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets , transforms from torchvision.utils import make_grid import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline transform = transforms . ToTensor () train_data = datasets . FashionMNIST ( root = '../Data' , train = True , download = True , transform = transform ) test_data = datasets . FashionMNIST ( root = '../Data' , train = False , download = True , transform = transform ) class_names = [ 'T-shirt' , 'Trouser' , 'Sweater' , 'Dress' , 'Coat' , 'Sandal' , 'Shirt' , 'Sneaker' , 'Bag' , 'Boot' ]","title":"Perform standard imports, load the Fashion-MNIST dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#1-create-data-loaders","text":"Use DataLoader to create a train_loader and a test_loader . Batch sizes should be 10 for both. # CODE HERE # DON'T WRITE HERE train_loader = DataLoader ( train_data , batch_size = 10 , shuffle = True ) test_loader = DataLoader ( test_data , batch_size = 10 , shuffle = False )","title":"1. Create data loaders"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#2-examine-a-batch-of-images","text":"Use DataLoader, make_grid and matplotlib to display the first batch of 10 images. OPTIONAL: display the labels as well # CODE HERE # DON'T WRITE HERE # IMAGES ONLY for images , labels in train_loader : break im = make_grid ( images , nrow = 10 ) plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); # DON'T WRITE HERE # IMAGES AND LABELS for images , labels in train_loader : break print ( 'Label: ' , labels . numpy ()) print ( 'Class: ' , * np . array ([ class_names [ i ] for i in labels ])) im = make_grid ( images , nrow = 10 ) plt . figure ( figsize = ( 12 , 4 )) plt . imshow ( np . transpose ( im . numpy (), ( 1 , 2 , 0 ))); Label: [3 0 0 3 1 3 5 4 8 8] Class: Dress T-shirt T-shirt Dress Trouser Dress Sandal Coat Bag Bag","title":"2. Examine a batch of images"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#downsampling","text":"","title":"Downsampling"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#4-if-the-sample-from-question-3-is-then-passed-through-a-2x2-maxpooling-layer-what-is-the-resulting-matrix-size","text":"If a 2x2 pooling layer is applied to a 24x24 matrix, each side is divided by two, and rounded down if necessary. The result is a 12x12 matrix. # Run the code below to check your answer: x = F . max_pool2d ( x , 2 , 2 ) print ( 'Down size:' , x . shape ) Down size: torch.Size([10, 1, 12, 12])","title":"4. If the sample from question 3 is then passed through a 2x2 MaxPooling layer, what is the resulting matrix size?"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#cnn-definition","text":"","title":"CNN definition"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#5-define-a-convolutional-neural-network","text":"Define a CNN model that can be trained on the Fashion-MNIST dataset. The model should contain two convolutional layers, two pooling layers, and two fully connected layers. You can use any number of neurons per layer so long as the model takes in a 28x28 image and returns an output of 10. Portions of the definition have been filled in for convenience. # DON'T WRITE HERE class ConvolutionalNetwork ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 1 , 6 , 3 , 1 ) self . conv2 = nn . Conv2d ( 6 , 16 , 3 , 1 ) self . fc1 = nn . Linear ( 5 * 5 * 16 , 100 ) self . fc2 = nn . Linear ( 100 , 10 ) def forward ( self , X ): X = F . relu ( self . conv1 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = F . relu ( self . conv2 ( X )) X = F . max_pool2d ( X , 2 , 2 ) X = X . view ( - 1 , 5 * 5 * 16 ) X = F . relu ( self . fc1 ( X )) X = self . fc2 ( X ) return F . log_softmax ( X , dim = 1 ) torch . manual_seed ( 101 ) model = ConvolutionalNetwork ()","title":"5. Define a convolutional neural network"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#trainable-parameters","text":"","title":"Trainable parameters"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#6-what-is-the-total-number-of-trainable-parameters-weights-biases-in-the-model-above","text":"Answers will vary depending on your model definition. $\\quad\\begin{split}(1\\times6\\times3\\times3)+6+(6\\times16\\times3\\times3)+16+(400\\times100)+100+(100\\times10)+10 &=\\\\ 54+6+864+16+40000+100+1000+10 &= 42,050\\end{split}$ # Run the code below to check your answer: def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 54 6 864 16 40000 100 1000 10 ______ 42050","title":"6. What is the total number of trainable parameters (weights &amp; biases) in the model above?"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#7-define-loss-function-optimizer","text":"Define a loss function called \u201ccriterion\u201d and an optimizer called \u201coptimizer\u201d. You can use any functions you want, although we used Cross Entropy Loss and Adam (learning rate of 0.001) respectively. # DON'T WRITE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"7. Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#8-train-the-model","text":"Don\u2019t worry about tracking loss values, displaying results, or validating the test set. Just train the model through 5 epochs. We\u2019ll evaluate the trained model in the next step. OPTIONAL: print something after each epoch to indicate training progress. # DON'T WRITE HERE epochs = 5 for i in range ( epochs ): for X_train , y_train in train_loader : # Apply the model y_pred = model ( X_train ) loss = criterion ( y_pred , y_train ) # Update parameters optimizer . zero_grad () loss . backward () optimizer . step () # OPTIONAL print statement print ( f ' { i + 1 } of { epochs } epochs completed' ) 1 of 5 epochs completed 2 of 5 epochs completed 3 of 5 epochs completed 4 of 5 epochs completed 5 of 5 epochs completed","title":"8. Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#9-evaluate-the-model","text":"Set model.eval() and determine the percentage correct out of 10,000 total test images. # DON'T WRITE HERE model . eval () with torch . no_grad (): correct = 0 for X_test , y_test in test_loader : y_val = model ( X_test ) predicted = torch . max ( y_val , 1 )[ 1 ] correct += ( predicted == y_test ) . sum () print ( f 'Test accuracy: { correct . item () } / { len ( test_data ) } = { correct . item () * 100 / ( len ( test_data )) : 7.3f } %' ) Test accuracy: 8733/10000 = 87.330%","title":"9. Evaluate the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/#great-job","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/","text":"================ by Jawad Haider Basic RNN - Recurrent Neural Networks Advantages of an LSTM Perform standard imports Create a sine wave dataset Create train and test sets Prepare the training data Define an LSTM model Instantiate the model, define loss & optimization functions Predicting future values Train and simultaneously evaluate the model Forecasting into an unknown future Train the model Predict future values, plot the result Great job! Basic RNN - Recurrent Neural Networks \u00b6 Up to now we\u2019ve used neural networks to classify static images. What happens when the thing we\u2019re trying to explain changes over time? What if a predicted value depends on a series of past behaviors? We can train networks to tell us that an image contains a car. How do we answer the question \u201cIs the car moving? Where will it be a minute from now?\u201d This challenge of incorporating a series of measurements over time into the model parameters is addressed by Recurrent Neural Networks (RNNs). Be sure to watch the theory lectures. You should be comfortable with: * conditional memory * deep sequence modeling * vanishing gradients * gated cells * long short-term memory (LSTM) cells PyTorch offers a number of RNN layers and options. * torch.nn.RNN() provides a basic model which applies a multilayer RNN with either tanh or ReLU non-linearity functions to an input sequence. As we learned in the theory lectures, however, this has its limits. * torch.nn.LSTM() adds a multi-layer long short-term memory (LSTM) process which greatly extends the memory of the RNN. Advantages of an LSTM \u00b6 For each element in the input sequence, an LSTM layer computes the following functions: \\(\\begin{array}{ll} \\\\ i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\ f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\ g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\ o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\ c_t = f_t * c_{(t-1)} + i_t * g_t \\\\ h_t = o_t * \\tanh(c_t) \\\\ \\end{array}\\) where \\(h_t\\) is the hidden state at time \\(t\\) , \\(c_t\\) is the cell state at time \\(t\\) , \\(x_t\\) is the input at time \\(t\\) , \\(h_{(t-1)}\\) is the hidden state of the layer at time \\(t-1\\) or the initial hidden state at time \\(0\\) , and \\(i_t, f_t, g_t, o_t\\) are the input, forget, cell, and output gates, respectively. \\(\\sigma\\) is the sigmoid function, and \\(*\\) is the Hadamard product. To demonstrate the potential of LSTMs, we\u2019ll look at a simple sine wave. Our goal is, given a value, predict the next value in the sequence. Due to the cyclical nature of sine waves, an typical neural network won\u2019t know if it should predict upward or downward, while an LSTM is capable of learning patterns of values. Perform standard imports \u00b6 import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Create a sine wave dataset \u00b6 For this exercise we\u2019ll look at a simple sine wave. We\u2019ll take 800 data points and assign 40 points per full cycle, for a total of 20 complete cycles. We\u2019ll train our model on all but the last cycle, and use that to evaluate our test predictions. # Create & plot data points x = torch . linspace ( 0 , 799 , steps = 800 ) y = torch . sin ( x * 2 * 3.1416 / 40 ) plt . figure ( figsize = ( 12 , 4 )) plt . xlim ( - 10 , 801 ) plt . grid ( True ) plt . plot ( y . numpy ()); Create train and test sets \u00b6 We want to take the first 760 samples in our series as a training sequence, and the last 40 for testing. NOTE: We tend to use the terms \u201cseries\u201d and \u201csequence\u201d interchangeably. Usually \u201cseries\u201d refers to the entire population of data, or the full time series, and \u201csequence\u201d refers to some portion of it. test_size = 40 train_set = y [: - test_size ] test_set = y [ - test_size :] Prepare the training data \u00b6 When working with LSTM models, we start by dividing the training sequence into a series of overlapping \u201cwindows\u201d. Each window consists of a connected string of samples. The label used for comparison is equal to the next value in the sequence. In this way our network learns what value should follow a given pattern of preceding values. Note: although the LSTM layer produces a prediction for each sample in the window, we only care about the last one. For example, say we have a series of 15 records, and a window size of 5. We feed \\([x_1,..,x_5]\\) into the model, and compare the prediction to \\(x_6\\) . Then we backprop, update parameters, and feed \\([x_2,..,x_6]\\) into the model. We compare the new output to \\(x_7\\) and so forth up to \\([x_{10},..,x_{14}]\\) . To simplify matters, we\u2019ll define a function called input_data that builds a list of (seq, label) tuples. Windows overlap, so the first tuple might contain \\(([x_1,..,x_5],[x_6])\\) , the second would have \\(([x_2,..,x_6],[x_7])\\) , etc. Here \\(k\\) is the width of the window. Due to the overlap, we\u2019ll have a total number of (seq, label) tuples equal to \\(\\textrm{len}(series)-k\\) def input_data ( seq , ws ): # ws is the window size out = [] L = len ( seq ) for i in range ( L - ws ): window = seq [ i : i + ws ] label = seq [ i + ws : i + ws + 1 ] out . append (( window , label )) return out NOTE: \u201cWindows\u201d are different from \u201cbatches\u201d. In our example we\u2019ll feed one window into the model at a time, so our batch size would be 1. If we passed two windows into the model before we backprop and update weights, our batch size would be 2. To train on our sine wave data we\u2019ll use a window size of 40 (one entire cycle). # From above: # test_size = 40 # train_set = y[:-test_size] # test_set = y[-test_size:] window_size = 40 # Create the training dataset of sequence/label tuples: train_data = input_data ( train_set , window_size ) len ( train_data ) # this should equal 760-40 720 # Display the first (seq/label) tuple in train_data train_data [ 0 ] (tensor([ 0.0000e+00, 1.5643e-01, 3.0902e-01, 4.5399e-01, 5.8779e-01, 7.0711e-01, 8.0902e-01, 8.9101e-01, 9.5106e-01, 9.8769e-01, 1.0000e+00, 9.8769e-01, 9.5106e-01, 8.9100e-01, 8.0901e-01, 7.0710e-01, 5.8778e-01, 4.5398e-01, 3.0901e-01, 1.5643e-01, -7.2400e-06, -1.5644e-01, -3.0902e-01, -4.5400e-01, -5.8779e-01, -7.0711e-01, -8.0902e-01, -8.9101e-01, -9.5106e-01, -9.8769e-01, -1.0000e+00, -9.8769e-01, -9.5105e-01, -8.9100e-01, -8.0901e-01, -7.0710e-01, -5.8777e-01, -4.5398e-01, -3.0900e-01, -1.5642e-01]), tensor([1.4480e-05])) torch . set_printoptions ( sci_mode = False ) # to improve the appearance of tensors train_data [ 0 ] (tensor([ 0.0000, 0.1564, 0.3090, 0.4540, 0.5878, 0.7071, 0.8090, 0.8910, 0.9511, 0.9877, 1.0000, 0.9877, 0.9511, 0.8910, 0.8090, 0.7071, 0.5878, 0.4540, 0.3090, 0.1564, -0.0000, -0.1564, -0.3090, -0.4540, -0.5878, -0.7071, -0.8090, -0.8910, -0.9511, -0.9877, -1.0000, -0.9877, -0.9511, -0.8910, -0.8090, -0.7071, -0.5878, -0.4540, -0.3090, -0.1564]), tensor([ 0.0000])) Define an LSTM model \u00b6 Our model will have one LSTM layer with an input size of 1 and a hidden size of 50, followed by a fully-connected layer to reduce the output to the prediction size of 1. NOTE: You will often see the terms input_dim and hidden_dim used in place of input_size and hidden_size . They mean the same thing. We\u2019ll stick to input_size and hidden_size to stay consistent with PyTorch\u2019s built-in keywords. During training we pass three tensors through the LSTM layer - the sequence, the hidden state \\(h_0\\) and the cell state \\(c_0\\) . This means we need to initialize \\(h_0\\) and \\(c_0\\) . This can be done with random values, but we\u2019ll use zeros instead. class LSTM ( nn . Module ): def __init__ ( self , input_size = 1 , hidden_size = 50 , out_size = 1 ): super () . __init__ () self . hidden_size = hidden_size # Add an LSTM layer: self . lstm = nn . LSTM ( input_size , hidden_size ) # Add a fully-connected layer: self . linear = nn . Linear ( hidden_size , out_size ) # Initialize h0 and c0: self . hidden = ( torch . zeros ( 1 , 1 , hidden_size ), torch . zeros ( 1 , 1 , hidden_size )) def forward ( self , seq ): lstm_out , self . hidden = self . lstm ( seq . view ( len ( seq ), 1 , - 1 ), self . hidden ) pred = self . linear ( lstm_out . view ( len ( seq ), - 1 )) return pred [ - 1 ] # we only care about the last prediction Instantiate the model, define loss & optimization functions \u00b6 Since we\u2019re comparing single values, we\u2019ll use torch.nn.MSELoss Also, we\u2019ve found that torch.optim.SGD converges faster for this application than torch.optim.Adam torch . manual_seed ( 42 ) model = LSTM () criterion = nn . MSELoss () optimizer = torch . optim . SGD ( model . parameters (), lr = 0.01 ) model LSTM( (lstm): LSTM(1, 50) (linear): Linear(in_features=50, out_features=1, bias=True) ) def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 200 10000 200 200 50 1 ______ 10651 Predicting future values \u00b6 To show how an LSTM model improves after each epoch, we\u2019ll run predictions and plot the results. Our goal is to predict the last sequence of 40 values, and compare them to the known data in our test set. However, we have to be careful not to use test data in the predictions - that is, each new prediction derives from previously predicted values. The trick is to take the last known window, predict the next value, then append the predicted value to the sequence and run a new prediction on a window that includes the value we\u2019ve just predicted. It\u2019s like adding track in front of the train as it\u2019s moving. Image source: https://giphy.com/gifs/aardman-cartoon-train-3oz8xtBx06mcZWoNJm In this way, a well-trained model should follow any regular trends/cycles in the data. Train and simultaneously evaluate the model \u00b6 We\u2019ll train 10 epochs. For clarity, we\u2019ll \u201czoom in\u201d on the test set, and only display from point 700 to the end. epochs = 10 future = 40 for i in range ( epochs ): # tuple-unpack the train_data set for seq , y_train in train_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) y_pred = model ( seq ) loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # print training result print ( f 'Epoch: { i + 1 : 2 } Loss: { loss . item () : 10.8f } ' ) # MAKE PREDICTIONS # start with a list of the last 10 training records preds = train_set [ - window_size :] . tolist () for f in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) loss = criterion ( torch . tensor ( preds [ - window_size :]), y [ 760 :]) print ( f 'Loss on test predictions: { loss } ' ) # Plot from point 700 to the end plt . figure ( figsize = ( 12 , 4 )) plt . xlim ( 700 , 801 ) plt . grid ( True ) plt . plot ( y . numpy ()) plt . plot ( range ( 760 , 800 ), preds [ window_size :]) plt . show () Epoch: 1 Loss: 0.09212875 Loss on test predictions: 0.6071590185165405 Epoch: 2 Loss: 0.06506767 Loss on test predictions: 0.565098762512207 Epoch: 3 Loss: 0.04198047 Loss on test predictions: 0.5199716687202454 Epoch: 4 Loss: 0.01784276 Loss on test predictions: 0.42209967970848083 Epoch: 5 Loss: 0.00288710 Loss on test predictions: 0.16624116897583008 Epoch: 6 Loss: 0.00032008 Loss on test predictions: 0.03055439703166485 Epoch: 7 Loss: 0.00012969 Loss on test predictions: 0.014990181662142277 Epoch: 8 Loss: 0.00012007 Loss on test predictions: 0.011856676079332829 Epoch: 9 Loss: 0.00012656 Loss on test predictions: 0.010163827799260616 Epoch: 10 Loss: 0.00013195 Loss on test predictions: 0.00889757089316845 Forecasting into an unknown future \u00b6 We\u2019ll continue to train our model, this time using the entire dataset. Then we\u2019ll predict what the next 40 points should be. Train the model \u00b6 Expect this to take a few minutes. epochs = 10 window_size = 40 future = 40 # Create the full set of sequence/label tuples: all_data = input_data ( y , window_size ) len ( all_data ) # this should equal 800-40 760 import time start_time = time . time () for i in range ( epochs ): # tuple-unpack the entire set of data for seq , y_train in all_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) y_pred = model ( seq ) loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # print training result print ( f 'Epoch: { i + 1 : 2 } Loss: { loss . item () : 10.8f } ' ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) Epoch: 1 Loss: 0.00013453 Epoch: 2 Loss: 0.00013443 Epoch: 3 Loss: 0.00013232 Epoch: 4 Loss: 0.00012879 Epoch: 5 Loss: 0.00012434 Epoch: 6 Loss: 0.00011931 Epoch: 7 Loss: 0.00011398 Epoch: 8 Loss: 0.00010854 Epoch: 9 Loss: 0.00010313 Epoch: 10 Loss: 0.00009784 Duration: 173 seconds Predict future values, plot the result \u00b6 preds = y [ - window_size :] . tolist () for i in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): # Reset the hidden parameters model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) plt . figure ( figsize = ( 12 , 4 )) plt . xlim ( - 10 , 841 ) plt . grid ( True ) plt . plot ( y . numpy ()) plt . plot ( range ( 800 , 800 + future ), preds [ window_size :]) plt . show () Great job! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"00 Basic RNN"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#basic-rnn-recurrent-neural-networks","text":"Up to now we\u2019ve used neural networks to classify static images. What happens when the thing we\u2019re trying to explain changes over time? What if a predicted value depends on a series of past behaviors? We can train networks to tell us that an image contains a car. How do we answer the question \u201cIs the car moving? Where will it be a minute from now?\u201d This challenge of incorporating a series of measurements over time into the model parameters is addressed by Recurrent Neural Networks (RNNs). Be sure to watch the theory lectures. You should be comfortable with: * conditional memory * deep sequence modeling * vanishing gradients * gated cells * long short-term memory (LSTM) cells PyTorch offers a number of RNN layers and options. * torch.nn.RNN() provides a basic model which applies a multilayer RNN with either tanh or ReLU non-linearity functions to an input sequence. As we learned in the theory lectures, however, this has its limits. * torch.nn.LSTM() adds a multi-layer long short-term memory (LSTM) process which greatly extends the memory of the RNN.","title":"Basic RNN - Recurrent Neural Networks"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#advantages-of-an-lstm","text":"For each element in the input sequence, an LSTM layer computes the following functions: \\(\\begin{array}{ll} \\\\ i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\ f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\ g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\ o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\ c_t = f_t * c_{(t-1)} + i_t * g_t \\\\ h_t = o_t * \\tanh(c_t) \\\\ \\end{array}\\) where \\(h_t\\) is the hidden state at time \\(t\\) , \\(c_t\\) is the cell state at time \\(t\\) , \\(x_t\\) is the input at time \\(t\\) , \\(h_{(t-1)}\\) is the hidden state of the layer at time \\(t-1\\) or the initial hidden state at time \\(0\\) , and \\(i_t, f_t, g_t, o_t\\) are the input, forget, cell, and output gates, respectively. \\(\\sigma\\) is the sigmoid function, and \\(*\\) is the Hadamard product. To demonstrate the potential of LSTMs, we\u2019ll look at a simple sine wave. Our goal is, given a value, predict the next value in the sequence. Due to the cyclical nature of sine waves, an typical neural network won\u2019t know if it should predict upward or downward, while an LSTM is capable of learning patterns of values.","title":"Advantages of an LSTM"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#perform-standard-imports","text":"import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#create-a-sine-wave-dataset","text":"For this exercise we\u2019ll look at a simple sine wave. We\u2019ll take 800 data points and assign 40 points per full cycle, for a total of 20 complete cycles. We\u2019ll train our model on all but the last cycle, and use that to evaluate our test predictions. # Create & plot data points x = torch . linspace ( 0 , 799 , steps = 800 ) y = torch . sin ( x * 2 * 3.1416 / 40 ) plt . figure ( figsize = ( 12 , 4 )) plt . xlim ( - 10 , 801 ) plt . grid ( True ) plt . plot ( y . numpy ());","title":"Create a sine wave dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#create-train-and-test-sets","text":"We want to take the first 760 samples in our series as a training sequence, and the last 40 for testing. NOTE: We tend to use the terms \u201cseries\u201d and \u201csequence\u201d interchangeably. Usually \u201cseries\u201d refers to the entire population of data, or the full time series, and \u201csequence\u201d refers to some portion of it. test_size = 40 train_set = y [: - test_size ] test_set = y [ - test_size :]","title":"Create train and test sets"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#prepare-the-training-data","text":"When working with LSTM models, we start by dividing the training sequence into a series of overlapping \u201cwindows\u201d. Each window consists of a connected string of samples. The label used for comparison is equal to the next value in the sequence. In this way our network learns what value should follow a given pattern of preceding values. Note: although the LSTM layer produces a prediction for each sample in the window, we only care about the last one. For example, say we have a series of 15 records, and a window size of 5. We feed \\([x_1,..,x_5]\\) into the model, and compare the prediction to \\(x_6\\) . Then we backprop, update parameters, and feed \\([x_2,..,x_6]\\) into the model. We compare the new output to \\(x_7\\) and so forth up to \\([x_{10},..,x_{14}]\\) . To simplify matters, we\u2019ll define a function called input_data that builds a list of (seq, label) tuples. Windows overlap, so the first tuple might contain \\(([x_1,..,x_5],[x_6])\\) , the second would have \\(([x_2,..,x_6],[x_7])\\) , etc. Here \\(k\\) is the width of the window. Due to the overlap, we\u2019ll have a total number of (seq, label) tuples equal to \\(\\textrm{len}(series)-k\\) def input_data ( seq , ws ): # ws is the window size out = [] L = len ( seq ) for i in range ( L - ws ): window = seq [ i : i + ws ] label = seq [ i + ws : i + ws + 1 ] out . append (( window , label )) return out NOTE: \u201cWindows\u201d are different from \u201cbatches\u201d. In our example we\u2019ll feed one window into the model at a time, so our batch size would be 1. If we passed two windows into the model before we backprop and update weights, our batch size would be 2. To train on our sine wave data we\u2019ll use a window size of 40 (one entire cycle). # From above: # test_size = 40 # train_set = y[:-test_size] # test_set = y[-test_size:] window_size = 40 # Create the training dataset of sequence/label tuples: train_data = input_data ( train_set , window_size ) len ( train_data ) # this should equal 760-40 720 # Display the first (seq/label) tuple in train_data train_data [ 0 ] (tensor([ 0.0000e+00, 1.5643e-01, 3.0902e-01, 4.5399e-01, 5.8779e-01, 7.0711e-01, 8.0902e-01, 8.9101e-01, 9.5106e-01, 9.8769e-01, 1.0000e+00, 9.8769e-01, 9.5106e-01, 8.9100e-01, 8.0901e-01, 7.0710e-01, 5.8778e-01, 4.5398e-01, 3.0901e-01, 1.5643e-01, -7.2400e-06, -1.5644e-01, -3.0902e-01, -4.5400e-01, -5.8779e-01, -7.0711e-01, -8.0902e-01, -8.9101e-01, -9.5106e-01, -9.8769e-01, -1.0000e+00, -9.8769e-01, -9.5105e-01, -8.9100e-01, -8.0901e-01, -7.0710e-01, -5.8777e-01, -4.5398e-01, -3.0900e-01, -1.5642e-01]), tensor([1.4480e-05])) torch . set_printoptions ( sci_mode = False ) # to improve the appearance of tensors train_data [ 0 ] (tensor([ 0.0000, 0.1564, 0.3090, 0.4540, 0.5878, 0.7071, 0.8090, 0.8910, 0.9511, 0.9877, 1.0000, 0.9877, 0.9511, 0.8910, 0.8090, 0.7071, 0.5878, 0.4540, 0.3090, 0.1564, -0.0000, -0.1564, -0.3090, -0.4540, -0.5878, -0.7071, -0.8090, -0.8910, -0.9511, -0.9877, -1.0000, -0.9877, -0.9511, -0.8910, -0.8090, -0.7071, -0.5878, -0.4540, -0.3090, -0.1564]), tensor([ 0.0000]))","title":"Prepare the training data"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#define-an-lstm-model","text":"Our model will have one LSTM layer with an input size of 1 and a hidden size of 50, followed by a fully-connected layer to reduce the output to the prediction size of 1. NOTE: You will often see the terms input_dim and hidden_dim used in place of input_size and hidden_size . They mean the same thing. We\u2019ll stick to input_size and hidden_size to stay consistent with PyTorch\u2019s built-in keywords. During training we pass three tensors through the LSTM layer - the sequence, the hidden state \\(h_0\\) and the cell state \\(c_0\\) . This means we need to initialize \\(h_0\\) and \\(c_0\\) . This can be done with random values, but we\u2019ll use zeros instead. class LSTM ( nn . Module ): def __init__ ( self , input_size = 1 , hidden_size = 50 , out_size = 1 ): super () . __init__ () self . hidden_size = hidden_size # Add an LSTM layer: self . lstm = nn . LSTM ( input_size , hidden_size ) # Add a fully-connected layer: self . linear = nn . Linear ( hidden_size , out_size ) # Initialize h0 and c0: self . hidden = ( torch . zeros ( 1 , 1 , hidden_size ), torch . zeros ( 1 , 1 , hidden_size )) def forward ( self , seq ): lstm_out , self . hidden = self . lstm ( seq . view ( len ( seq ), 1 , - 1 ), self . hidden ) pred = self . linear ( lstm_out . view ( len ( seq ), - 1 )) return pred [ - 1 ] # we only care about the last prediction","title":"Define an LSTM model"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#instantiate-the-model-define-loss-optimization-functions","text":"Since we\u2019re comparing single values, we\u2019ll use torch.nn.MSELoss Also, we\u2019ve found that torch.optim.SGD converges faster for this application than torch.optim.Adam torch . manual_seed ( 42 ) model = LSTM () criterion = nn . MSELoss () optimizer = torch . optim . SGD ( model . parameters (), lr = 0.01 ) model LSTM( (lstm): LSTM(1, 50) (linear): Linear(in_features=50, out_features=1, bias=True) ) def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 200 10000 200 200 50 1 ______ 10651","title":"Instantiate the model, define loss &amp; optimization functions"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#predicting-future-values","text":"To show how an LSTM model improves after each epoch, we\u2019ll run predictions and plot the results. Our goal is to predict the last sequence of 40 values, and compare them to the known data in our test set. However, we have to be careful not to use test data in the predictions - that is, each new prediction derives from previously predicted values. The trick is to take the last known window, predict the next value, then append the predicted value to the sequence and run a new prediction on a window that includes the value we\u2019ve just predicted. It\u2019s like adding track in front of the train as it\u2019s moving. Image source: https://giphy.com/gifs/aardman-cartoon-train-3oz8xtBx06mcZWoNJm In this way, a well-trained model should follow any regular trends/cycles in the data.","title":"Predicting future values"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#train-and-simultaneously-evaluate-the-model","text":"We\u2019ll train 10 epochs. For clarity, we\u2019ll \u201czoom in\u201d on the test set, and only display from point 700 to the end. epochs = 10 future = 40 for i in range ( epochs ): # tuple-unpack the train_data set for seq , y_train in train_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) y_pred = model ( seq ) loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # print training result print ( f 'Epoch: { i + 1 : 2 } Loss: { loss . item () : 10.8f } ' ) # MAKE PREDICTIONS # start with a list of the last 10 training records preds = train_set [ - window_size :] . tolist () for f in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) loss = criterion ( torch . tensor ( preds [ - window_size :]), y [ 760 :]) print ( f 'Loss on test predictions: { loss } ' ) # Plot from point 700 to the end plt . figure ( figsize = ( 12 , 4 )) plt . xlim ( 700 , 801 ) plt . grid ( True ) plt . plot ( y . numpy ()) plt . plot ( range ( 760 , 800 ), preds [ window_size :]) plt . show () Epoch: 1 Loss: 0.09212875 Loss on test predictions: 0.6071590185165405 Epoch: 2 Loss: 0.06506767 Loss on test predictions: 0.565098762512207 Epoch: 3 Loss: 0.04198047 Loss on test predictions: 0.5199716687202454 Epoch: 4 Loss: 0.01784276 Loss on test predictions: 0.42209967970848083 Epoch: 5 Loss: 0.00288710 Loss on test predictions: 0.16624116897583008 Epoch: 6 Loss: 0.00032008 Loss on test predictions: 0.03055439703166485 Epoch: 7 Loss: 0.00012969 Loss on test predictions: 0.014990181662142277 Epoch: 8 Loss: 0.00012007 Loss on test predictions: 0.011856676079332829 Epoch: 9 Loss: 0.00012656 Loss on test predictions: 0.010163827799260616 Epoch: 10 Loss: 0.00013195 Loss on test predictions: 0.00889757089316845","title":"Train and simultaneously evaluate the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#forecasting-into-an-unknown-future","text":"We\u2019ll continue to train our model, this time using the entire dataset. Then we\u2019ll predict what the next 40 points should be.","title":"Forecasting into an unknown future"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#train-the-model","text":"Expect this to take a few minutes. epochs = 10 window_size = 40 future = 40 # Create the full set of sequence/label tuples: all_data = input_data ( y , window_size ) len ( all_data ) # this should equal 800-40 760 import time start_time = time . time () for i in range ( epochs ): # tuple-unpack the entire set of data for seq , y_train in all_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) y_pred = model ( seq ) loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # print training result print ( f 'Epoch: { i + 1 : 2 } Loss: { loss . item () : 10.8f } ' ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) Epoch: 1 Loss: 0.00013453 Epoch: 2 Loss: 0.00013443 Epoch: 3 Loss: 0.00013232 Epoch: 4 Loss: 0.00012879 Epoch: 5 Loss: 0.00012434 Epoch: 6 Loss: 0.00011931 Epoch: 7 Loss: 0.00011398 Epoch: 8 Loss: 0.00010854 Epoch: 9 Loss: 0.00010313 Epoch: 10 Loss: 0.00009784 Duration: 173 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#predict-future-values-plot-the-result","text":"preds = y [ - window_size :] . tolist () for i in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): # Reset the hidden parameters model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) plt . figure ( figsize = ( 12 , 4 )) plt . xlim ( - 10 , 841 ) plt . grid ( True ) plt . plot ( y . numpy ()) plt . plot ( range ( 800 , 800 + future ), preds [ window_size :]) plt . show ()","title":"Predict future values, plot the result"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/#great-job","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/","text":"================ by Jawad Haider RNN on a Time Series Perform standard imports Load the dataset Plotting time series data Prepare the data Normalize the data Prepare data for LSTM Define the model Instantiate the model, define loss and optimization functions Train the model Run predictions and compare to known test set Invert the normalization Forecast into an unknown future Predict future values, plot the result Great job! RNN on a Time Series \u00b6 For these examples we\u2019re using data from the Federal Reserve Economic Database (FRED) concerning Sales of Beer, Wine, and Distilled Alcoholic Beverages in millions of dollars from January 1992 to January 2019 (325 records). Data source: https://fred.stlouisfed.org/series/S4248SM144NCEN Perform standard imports \u00b6 import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline # This relates to plotting datetime values with matplotlib: from pandas.plotting import register_matplotlib_converters register_matplotlib_converters () Load the dataset \u00b6 We\u2019ll take advantage of pandas\u2019 built-in DatetimeIndex by passing parse_dates=True df = pd . read_csv ( '../Data/TimeSeriesData/Alcohol_Sales.csv' , index_col = 0 , parse_dates = True ) len ( df ) 325 # Always a good idea with time series data: df . dropna ( inplace = True ) len ( df ) 325 df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } S4248SM144NCEN DATE 1992-01-01 3459 1992-02-01 3458 1992-03-01 4002 1992-04-01 4564 1992-05-01 4221 df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } S4248SM144NCEN DATE 2018-09-01 12396 2018-10-01 13914 2018-11-01 14174 2018-12-01 15504 2019-01-01 10718 Plotting time series data \u00b6 We can add titles, axis labels, and other features to the plot. We\u2019re going to tighten the x-axis to fit the width of the actual data with plt.autoscale(axis=\u2018x\u2019,tight=True) . Alternatively you could set your own limits with plt.xlim(pd.Timestamp(\u20181992-01-01\u2019), pd.Timestamp(\u20182019-01-01\u2019)) or some other values. plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'S4248SM144NCEN' ]) plt . show () Prepare the data \u00b6 In the next steps we\u2019ll divide the data into train/test sets, then normalize the training values so that they fall between -1 and 1 (to improve training). We\u2019ll train the model, then predict into a period that matches the test set. Finally, we\u2019ll forecast into an unknown future. # Extract values from the source .csv file y = df [ 'S4248SM144NCEN' ] . values . astype ( float ) # Define a test size test_size = 12 # Create train and test sets train_set = y [: - test_size ] test_set = y [ - test_size :] test_set array([10415., 12683., 11919., 14138., 14583., 12640., 14257., 12396., 13914., 14174., 15504., 10718.]) It\u2019s worth noting that in our previous exercise the train and test sets were tensors. Here they\u2019re numpy arrays. This is because one of the steps we\u2019re about to perform outputs an array, and we\u2019d have to turn it into a tensor anyway. Normalize the data \u00b6 The formula for normalizing data around zero is: ### \\(X_{norm} = \\frac{X - \\mu} {\\sigma}\\) where \\(\\mu\\) is the population mean, and \\(\\sigma\\) is the population standard deviation. Recall that back in the CNN section we transformed image files using torchvision.transforms.Normalize( mean, std ) , both because it was built into DataLoader, and because our pretrained models expected specific normalization values. Ultimately we want to perform min/max feature scaling so that our values fall between -1 and 1, as this makes hyperparameters converge faster. The formula for this would be: ### \\(X^{\\prime} = a + \\frac{(X - X_{min}) (b - a)} {X_{max} - X_{min}}\\) where \\(a={-1}\\) and \\(b=1\\) We can use scikit-learn to do this, with sklearn.preprocessing.MinMaxScaler() NOTE: We only want to normalize the training set to avoid data leakage. If we include the test set then the higher average values of the test set could become part of the signal in the training set. There\u2019s a good article on data leakage here . After using transformed data to train the model and generate predictions, we\u2019ll inverse_transform the predicted values so that we can compare them to the actual test data. from sklearn.preprocessing import MinMaxScaler # Instantiate a scaler with a feature range from -1 to 1 scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) # Normalize the training set train_norm = scaler . fit_transform ( train_set . reshape ( - 1 , 1 )) train_norm . min () -1.0 train_norm . max () 1.0 train_norm . mean () -0.2041940178388313 type ( train_norm ) numpy.ndarray Prepare data for LSTM \u00b6 Here we\u2019ll create our list of (seq/label) tuples from the training set. Recall that an LSTM consumes a window of samples toward the first prediction, so the size of our training set will become ((325 - test_size) - window_size). # Convert train_norm from an array to a tensor train_norm = torch . FloatTensor ( train_norm ) . view ( - 1 ) # Define a window size window_size = 12 # Define function to create seq/label tuples def input_data ( seq , ws ): # ws is the window size out = [] L = len ( seq ) for i in range ( L - ws ): window = seq [ i : i + ws ] label = seq [ i + ws : i + ws + 1 ] out . append (( window , label )) return out # Apply the input_data function to train_norm train_data = input_data ( train_norm , window_size ) len ( train_data ) # this should equal 325-12-12 301 # Display the first seq/label tuple in the train data train_data [ 0 ] (tensor([-0.9268, -0.9270, -0.8340, -0.7379, -0.7966, -0.7439, -0.7547, -0.8109, -0.8128, -0.7901, -0.7933, -0.6743]), tensor([-1.])) Define the model \u00b6 This time we\u2019ll use an LSTM layer of size (1,100). class LSTMnetwork ( nn . Module ): def __init__ ( self , input_size = 1 , hidden_size = 100 , output_size = 1 ): super () . __init__ () self . hidden_size = hidden_size # Add an LSTM layer: self . lstm = nn . LSTM ( input_size , hidden_size ) # Add a fully-connected layer: self . linear = nn . Linear ( hidden_size , output_size ) # Initialize h0 and c0: self . hidden = ( torch . zeros ( 1 , 1 , self . hidden_size ), torch . zeros ( 1 , 1 , self . hidden_size )) def forward ( self , seq ): lstm_out , self . hidden = self . lstm ( seq . view ( len ( seq ), 1 , - 1 ), self . hidden ) pred = self . linear ( lstm_out . view ( len ( seq ), - 1 )) return pred [ - 1 ] # we only want the last value Instantiate the model, define loss and optimization functions \u00b6 torch . manual_seed ( 101 ) model = LSTMnetwork () criterion = nn . MSELoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) model LSTMnetwork( (lstm): LSTM(1, 100) (linear): Linear(in_features=100, out_features=1, bias=True) ) def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 400 40000 400 400 100 1 ______ 41301 Train the model \u00b6 epochs = 100 import time start_time = time . time () for epoch in range ( epochs ): # extract the sequence & label from the training data for seq , y_train in train_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) y_pred = model ( seq ) loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # print training result print ( f 'Epoch: { epoch + 1 : 2 } Loss: { loss . item () : 10.8f } ' ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) Epoch: 1 Loss: 0.25314346 Epoch: 2 Loss: 0.37523878 Epoch: 3 Loss: 0.39012098 Epoch: 4 Loss: 0.34949699 Epoch: 5 Loss: 0.33077896 Epoch: 6 Loss: 0.33230731 Epoch: 7 Loss: 0.32667691 Epoch: 8 Loss: 0.32077038 Epoch: 9 Loss: 0.30651760 Epoch: 10 Loss: 0.29517019 Epoch: 11 Loss: 0.26913935 Epoch: 12 Loss: 0.25979361 Epoch: 13 Loss: 0.24600053 Epoch: 14 Loss: 0.22227803 Epoch: 15 Loss: 0.18593730 Epoch: 16 Loss: 0.18445705 Epoch: 17 Loss: 0.17432655 Epoch: 18 Loss: 0.25822312 Epoch: 19 Loss: 0.22420478 Epoch: 20 Loss: 0.23121868 Epoch: 21 Loss: 0.19758533 Epoch: 22 Loss: 0.18988022 Epoch: 23 Loss: 0.21032184 Epoch: 24 Loss: 0.16950732 Epoch: 25 Loss: 0.14586549 Epoch: 26 Loss: 0.11828811 Epoch: 27 Loss: 0.00027535 Epoch: 28 Loss: 0.00681852 Epoch: 29 Loss: 0.05630350 Epoch: 30 Loss: 0.01223376 Epoch: 31 Loss: 0.02709176 Epoch: 32 Loss: 0.03447094 Epoch: 33 Loss: 0.02249899 Epoch: 34 Loss: 0.01159327 Epoch: 35 Loss: 0.01592799 Epoch: 36 Loss: 0.00303702 Epoch: 37 Loss: 0.00015524 Epoch: 38 Loss: 0.00017455 Epoch: 39 Loss: 0.00034566 Epoch: 40 Loss: 0.00000055 Epoch: 41 Loss: 0.00002718 Epoch: 42 Loss: 0.00010465 Epoch: 43 Loss: 0.00106159 Epoch: 44 Loss: 0.00250127 Epoch: 45 Loss: 0.00288290 Epoch: 46 Loss: 0.00362020 Epoch: 47 Loss: 0.00371592 Epoch: 48 Loss: 0.00542405 Epoch: 49 Loss: 0.00589132 Epoch: 50 Loss: 0.00713205 Epoch: 51 Loss: 0.00702253 Epoch: 52 Loss: 0.00773653 Epoch: 53 Loss: 0.00765838 Epoch: 54 Loss: 0.00816493 Epoch: 55 Loss: 0.00806901 Epoch: 56 Loss: 0.00782317 Epoch: 57 Loss: 0.00831962 Epoch: 58 Loss: 0.00794266 Epoch: 59 Loss: 0.00764198 Epoch: 60 Loss: 0.00736130 Epoch: 61 Loss: 0.00857497 Epoch: 62 Loss: 0.00942086 Epoch: 63 Loss: 0.00335479 Epoch: 64 Loss: 0.00118402 Epoch: 65 Loss: 0.00320848 Epoch: 66 Loss: 0.00212591 Epoch: 67 Loss: 0.00150738 Epoch: 68 Loss: 0.00153509 Epoch: 69 Loss: 0.00182797 Epoch: 70 Loss: 0.00180291 Epoch: 71 Loss: 0.00140510 Epoch: 72 Loss: 0.00096806 Epoch: 73 Loss: 0.00047413 Epoch: 74 Loss: 0.00035952 Epoch: 75 Loss: 0.00008912 Epoch: 76 Loss: 0.00112789 Epoch: 77 Loss: 0.00002196 Epoch: 78 Loss: 0.00695060 Epoch: 79 Loss: 0.00038595 Epoch: 80 Loss: 0.00011149 Epoch: 81 Loss: 0.00043127 Epoch: 82 Loss: 0.00040868 Epoch: 83 Loss: 0.00032796 Epoch: 84 Loss: 0.00048195 Epoch: 85 Loss: 0.00049252 Epoch: 86 Loss: 0.00037195 Epoch: 87 Loss: 0.00072067 Epoch: 88 Loss: 0.00096091 Epoch: 89 Loss: 0.00063866 Epoch: 90 Loss: 0.00073793 Epoch: 91 Loss: 0.00092162 Epoch: 92 Loss: 0.00084968 Epoch: 93 Loss: 0.00073029 Epoch: 94 Loss: 0.00015117 Epoch: 95 Loss: 0.00018061 Epoch: 96 Loss: 0.00010094 Epoch: 97 Loss: 0.00006259 Epoch: 98 Loss: 0.00049196 Epoch: 99 Loss: 0.00033408 Epoch: 100 Loss: 0.00050564 Duration: 278 seconds Run predictions and compare to known test set \u00b6 future = 12 # Add the last window of training values to the list of predictions preds = train_norm [ - window_size :] . tolist () # Set the model to evaluation mode model . eval () for i in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) # Display predicted values preds [ window_size :] # equivalent to preds[-future:] [0.3245275914669037, 0.5227924585342407, 0.5454435348510742, 0.8364425301551819, 1.104264736175537, 0.30810344219207764, 0.8207511901855469, 0.5576714873313904, 0.5653725862503052, 0.8198413848876953, 0.9293676018714905, 0.12615284323692322] Invert the normalization \u00b6 We want to compare our test predictions to the original data, so we need to undo the previous normalization step. Note that inverse_transform uses the most recently applied parameters; we can rescale based on the test data, but not on the previous training data. true_predictions = scaler . inverse_transform ( np . array ( preds [ window_size :]) . reshape ( - 1 , 1 )) true_predictions array([[10778.82414629], [11938.5744862 ], [12071.07195711], [13773.27058014], [15339.89657426], [10682.7510851 ], [13681.48408699], [12142.59936514], [12187.64694327], [13676.1621809 ], [14316.83578715], [ 9618.43105651]]) df [ 'S4248SM144NCEN' ][ - 12 :] DATE 2018-02-01 10415 2018-03-01 12683 2018-04-01 11919 2018-05-01 14138 2018-06-01 14583 2018-07-01 12640 2018-08-01 14257 2018-09-01 12396 2018-10-01 13914 2018-11-01 14174 2018-12-01 15504 2019-01-01 10718 Name: S4248SM144NCEN, dtype: int64 It looks like our predictions weren\u2019t that far off! ## Plot the results Our original data contains a datetime index, but our predicted values do not. We can create a range of dates using NumPy that are spaced one month apart using dtype=\u2018datetime64[M]\u2019 , and then store them with day values to match our dataset with .astype(\u2018datetime64[D]\u2019) . # Remember that the stop date has to be later than the last predicted value. x = np . arange ( '2018-02-01' , '2019-02-01' , dtype = 'datetime64[M]' ) . astype ( 'datetime64[D]' ) x array(['2018-02-01', '2018-03-01', '2018-04-01', '2018-05-01', '2018-06-01', '2018-07-01', '2018-08-01', '2018-09-01', '2018-10-01', '2018-11-01', '2018-12-01', '2019-01-01'], dtype='datetime64[D]') plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'S4248SM144NCEN' ]) plt . plot ( x , true_predictions ) plt . show () # Plot the end of the graph fig = plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) fig . autofmt_xdate () # Select the end of the graph with slice notation: plt . plot ( df [ 'S4248SM144NCEN' ][ '2017-01-01' :]) plt . plot ( x , true_predictions ) plt . show () For more information on x-axis date formatting in matplotlib, check out matplotlib.figure.Figure.autofmt_xdate and matplotlib.dates.DateFormatter Forecast into an unknown future \u00b6 This time we\u2019ll continue training the model using the entire dataset, and predict 12 steps into the future. epochs = 100 # set model to back to training mode model . train () # feature scale the entire dataset y_norm = scaler . fit_transform ( y . reshape ( - 1 , 1 )) y_norm = torch . FloatTensor ( y_norm ) . view ( - 1 ) all_data = input_data ( y_norm , window_size ) import time start_time = time . time () for epoch in range ( epochs ): # train on the full set of sequences for seq , y_train in all_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) y_pred = model ( seq ) loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # print training result print ( f 'Epoch: { epoch + 1 : 2 } Loss: { loss . item () : 10.8f } ' ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) Epoch: 1 Loss: 0.02658496 Epoch: 2 Loss: 0.02985313 Epoch: 3 Loss: 0.01421225 Epoch: 4 Loss: 0.01419733 Epoch: 5 Loss: 0.01290122 Epoch: 6 Loss: 0.01244991 Epoch: 7 Loss: 0.01119690 Epoch: 8 Loss: 0.00854800 Epoch: 9 Loss: 0.00788417 Epoch: 10 Loss: 0.00792318 Epoch: 11 Loss: 0.00556064 Epoch: 12 Loss: 0.00598394 Epoch: 13 Loss: 0.00537636 Epoch: 14 Loss: 0.00649350 Epoch: 15 Loss: 0.00798889 Epoch: 16 Loss: 0.00699036 Epoch: 17 Loss: 0.00741554 Epoch: 18 Loss: 0.00671443 Epoch: 19 Loss: 0.00555703 Epoch: 20 Loss: 0.00517232 Epoch: 21 Loss: 0.00519288 Epoch: 22 Loss: 0.00412102 Epoch: 23 Loss: 0.00535119 Epoch: 24 Loss: 0.00888540 Epoch: 25 Loss: 0.00753472 Epoch: 26 Loss: 0.00589289 Epoch: 27 Loss: 0.00580891 Epoch: 28 Loss: 0.00776316 Epoch: 29 Loss: 0.00629479 Epoch: 30 Loss: 0.00895946 Epoch: 31 Loss: 0.00883982 Epoch: 32 Loss: 0.00968022 Epoch: 33 Loss: 0.00415197 Epoch: 34 Loss: 0.00383917 Epoch: 35 Loss: 0.00253066 Epoch: 36 Loss: 0.00149691 Epoch: 37 Loss: 0.00340568 Epoch: 38 Loss: 0.00264327 Epoch: 39 Loss: 0.00205079 Epoch: 40 Loss: 0.00436785 Epoch: 41 Loss: 0.00065083 Epoch: 42 Loss: 0.00063657 Epoch: 43 Loss: 0.00052040 Epoch: 44 Loss: 0.00060114 Epoch: 45 Loss: 0.00001630 Epoch: 46 Loss: 0.00138428 Epoch: 47 Loss: 0.00005873 Epoch: 48 Loss: 0.00092580 Epoch: 49 Loss: 0.00028546 Epoch: 50 Loss: 0.00265119 Epoch: 51 Loss: 0.00000118 Epoch: 52 Loss: 0.00076291 Epoch: 53 Loss: 0.00050618 Epoch: 54 Loss: 0.00063220 Epoch: 55 Loss: 0.00001087 Epoch: 56 Loss: 0.00001940 Epoch: 57 Loss: 0.00038629 Epoch: 58 Loss: 0.00057537 Epoch: 59 Loss: 0.00001950 Epoch: 60 Loss: 0.00133490 Epoch: 61 Loss: 0.00203123 Epoch: 62 Loss: 0.00004929 Epoch: 63 Loss: 0.00098640 Epoch: 64 Loss: 0.00086584 Epoch: 65 Loss: 0.00075280 Epoch: 66 Loss: 0.00050309 Epoch: 67 Loss: 0.00211758 Epoch: 68 Loss: 0.00260141 Epoch: 69 Loss: 0.00385657 Epoch: 70 Loss: 0.00009936 Epoch: 71 Loss: 0.00036399 Epoch: 72 Loss: 0.00154005 Epoch: 73 Loss: 0.00036845 Epoch: 74 Loss: 0.00021908 Epoch: 75 Loss: 0.01171840 Epoch: 76 Loss: 0.00335702 Epoch: 77 Loss: 0.00374896 Epoch: 78 Loss: 0.00772837 Epoch: 79 Loss: 0.00071883 Epoch: 80 Loss: 0.00118207 Epoch: 81 Loss: 0.00225649 Epoch: 82 Loss: 0.00011194 Epoch: 83 Loss: 0.00061754 Epoch: 84 Loss: 0.00208528 Epoch: 85 Loss: 0.01009495 Epoch: 86 Loss: 0.00545910 Epoch: 87 Loss: 0.00120122 Epoch: 88 Loss: 0.00036599 Epoch: 89 Loss: 0.00008251 Epoch: 90 Loss: 0.00085317 Epoch: 91 Loss: 0.00470886 Epoch: 92 Loss: 0.00336765 Epoch: 93 Loss: 0.00118269 Epoch: 94 Loss: 0.00009316 Epoch: 95 Loss: 0.00007733 Epoch: 96 Loss: 0.00416005 Epoch: 97 Loss: 0.00755412 Epoch: 98 Loss: 0.00003969 Epoch: 99 Loss: 0.00005710 Epoch: 100 Loss: 0.00009160 Duration: 297 seconds Predict future values, plot the result \u00b6 window_size = 12 future = 12 L = len ( y ) preds = y_norm [ - window_size :] . tolist () model . eval () for i in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): # Reset the hidden parameters here! model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) # Inverse-normalize the prediction set true_predictions = scaler . inverse_transform ( np . array ( preds ) . reshape ( - 1 , 1 )) # PLOT THE RESULT # Set a data range for the predicted data. # Remember that the stop date has to be later than the last predicted value. x = np . arange ( '2019-02-01' , '2020-02-01' , dtype = 'datetime64[M]' ) . astype ( 'datetime64[D]' ) plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'S4248SM144NCEN' ]) plt . plot ( x , true_predictions [ window_size :]) plt . show () fig = plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) fig . autofmt_xdate () plt . plot ( df [ 'S4248SM144NCEN' ][ '2017-01-01' :]) plt . plot ( x , true_predictions [ window_size :]) plt . show () Great job! \u00b6 BONUS: To save time in the future, we\u2019ve written a function that will take in a time series training data set, and output a tensor of (seq, label) tuples. # Load dependencies from sklearn.preprocessing import MinMaxScaler # Instantiate a scaler \"\"\" This has to be done outside the function definition so that we can inverse_transform the prediction set later on. \"\"\" scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) # Extract values from the source .csv file df = pd . read_csv ( '../Data/TimeSeriesData/Alcohol_Sales.csv' , index_col = 0 , parse_dates = True ) y = df [ 'S4248SM144NCEN' ] . values . astype ( float ) # Define a test size test_size = 12 # Create the training set of values train_set = y [: - test_size ] # DEFINE A FUNCTION: def create_train_data ( seq , ws = 12 ): \"\"\"Takes in a training sequence and window size (ws) of default size 12, returns a tensor of (seq/label) tuples\"\"\" seq_norm = scaler . fit_transform ( seq . reshape ( - 1 , 1 )) seq_norm = torch . FloatTensor ( seq_norm ) . view ( - 1 ) out = [] L = len ( seq_norm ) for i in range ( L - ws ): window = seq_norm [ i : i + ws ] label = seq_norm [ i + ws : i + ws + 1 ] out . append (( window , label )) return out # Apply the function to train_set train_data = create_train_data ( train_set , 12 ) len ( train_data ) # this should equal 313-12 301 train_data [ 0 ] (tensor([-0.9268, -0.9270, -0.8340, -0.7379, -0.7966, -0.7439, -0.7547, -0.8109, -0.8128, -0.7901, -0.7933, -0.6743]), tensor([-1.])) help ( create_train_data ) Help on function create_train_data in module __main__: create_train_data(seq, ws=12) Takes in a training sequence and window size (ws) of default size 12, returns a tensor of (seq/label) tuples Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"01 RNN on a Time Series"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#rnn-on-a-time-series","text":"For these examples we\u2019re using data from the Federal Reserve Economic Database (FRED) concerning Sales of Beer, Wine, and Distilled Alcoholic Beverages in millions of dollars from January 1992 to January 2019 (325 records). Data source: https://fred.stlouisfed.org/series/S4248SM144NCEN","title":"RNN on a Time Series"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#perform-standard-imports","text":"import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline # This relates to plotting datetime values with matplotlib: from pandas.plotting import register_matplotlib_converters register_matplotlib_converters ()","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#load-the-dataset","text":"We\u2019ll take advantage of pandas\u2019 built-in DatetimeIndex by passing parse_dates=True df = pd . read_csv ( '../Data/TimeSeriesData/Alcohol_Sales.csv' , index_col = 0 , parse_dates = True ) len ( df ) 325 # Always a good idea with time series data: df . dropna ( inplace = True ) len ( df ) 325 df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } S4248SM144NCEN DATE 1992-01-01 3459 1992-02-01 3458 1992-03-01 4002 1992-04-01 4564 1992-05-01 4221 df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } S4248SM144NCEN DATE 2018-09-01 12396 2018-10-01 13914 2018-11-01 14174 2018-12-01 15504 2019-01-01 10718","title":"Load the dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#plotting-time-series-data","text":"We can add titles, axis labels, and other features to the plot. We\u2019re going to tighten the x-axis to fit the width of the actual data with plt.autoscale(axis=\u2018x\u2019,tight=True) . Alternatively you could set your own limits with plt.xlim(pd.Timestamp(\u20181992-01-01\u2019), pd.Timestamp(\u20182019-01-01\u2019)) or some other values. plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'S4248SM144NCEN' ]) plt . show ()","title":"Plotting time series data"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#prepare-the-data","text":"In the next steps we\u2019ll divide the data into train/test sets, then normalize the training values so that they fall between -1 and 1 (to improve training). We\u2019ll train the model, then predict into a period that matches the test set. Finally, we\u2019ll forecast into an unknown future. # Extract values from the source .csv file y = df [ 'S4248SM144NCEN' ] . values . astype ( float ) # Define a test size test_size = 12 # Create train and test sets train_set = y [: - test_size ] test_set = y [ - test_size :] test_set array([10415., 12683., 11919., 14138., 14583., 12640., 14257., 12396., 13914., 14174., 15504., 10718.]) It\u2019s worth noting that in our previous exercise the train and test sets were tensors. Here they\u2019re numpy arrays. This is because one of the steps we\u2019re about to perform outputs an array, and we\u2019d have to turn it into a tensor anyway.","title":"Prepare the data"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#normalize-the-data","text":"The formula for normalizing data around zero is: ### \\(X_{norm} = \\frac{X - \\mu} {\\sigma}\\) where \\(\\mu\\) is the population mean, and \\(\\sigma\\) is the population standard deviation. Recall that back in the CNN section we transformed image files using torchvision.transforms.Normalize( mean, std ) , both because it was built into DataLoader, and because our pretrained models expected specific normalization values. Ultimately we want to perform min/max feature scaling so that our values fall between -1 and 1, as this makes hyperparameters converge faster. The formula for this would be: ### \\(X^{\\prime} = a + \\frac{(X - X_{min}) (b - a)} {X_{max} - X_{min}}\\) where \\(a={-1}\\) and \\(b=1\\) We can use scikit-learn to do this, with sklearn.preprocessing.MinMaxScaler() NOTE: We only want to normalize the training set to avoid data leakage. If we include the test set then the higher average values of the test set could become part of the signal in the training set. There\u2019s a good article on data leakage here . After using transformed data to train the model and generate predictions, we\u2019ll inverse_transform the predicted values so that we can compare them to the actual test data. from sklearn.preprocessing import MinMaxScaler # Instantiate a scaler with a feature range from -1 to 1 scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) # Normalize the training set train_norm = scaler . fit_transform ( train_set . reshape ( - 1 , 1 )) train_norm . min () -1.0 train_norm . max () 1.0 train_norm . mean () -0.2041940178388313 type ( train_norm ) numpy.ndarray","title":"Normalize the data"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#prepare-data-for-lstm","text":"Here we\u2019ll create our list of (seq/label) tuples from the training set. Recall that an LSTM consumes a window of samples toward the first prediction, so the size of our training set will become ((325 - test_size) - window_size). # Convert train_norm from an array to a tensor train_norm = torch . FloatTensor ( train_norm ) . view ( - 1 ) # Define a window size window_size = 12 # Define function to create seq/label tuples def input_data ( seq , ws ): # ws is the window size out = [] L = len ( seq ) for i in range ( L - ws ): window = seq [ i : i + ws ] label = seq [ i + ws : i + ws + 1 ] out . append (( window , label )) return out # Apply the input_data function to train_norm train_data = input_data ( train_norm , window_size ) len ( train_data ) # this should equal 325-12-12 301 # Display the first seq/label tuple in the train data train_data [ 0 ] (tensor([-0.9268, -0.9270, -0.8340, -0.7379, -0.7966, -0.7439, -0.7547, -0.8109, -0.8128, -0.7901, -0.7933, -0.6743]), tensor([-1.]))","title":"Prepare data for LSTM"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#define-the-model","text":"This time we\u2019ll use an LSTM layer of size (1,100). class LSTMnetwork ( nn . Module ): def __init__ ( self , input_size = 1 , hidden_size = 100 , output_size = 1 ): super () . __init__ () self . hidden_size = hidden_size # Add an LSTM layer: self . lstm = nn . LSTM ( input_size , hidden_size ) # Add a fully-connected layer: self . linear = nn . Linear ( hidden_size , output_size ) # Initialize h0 and c0: self . hidden = ( torch . zeros ( 1 , 1 , self . hidden_size ), torch . zeros ( 1 , 1 , self . hidden_size )) def forward ( self , seq ): lstm_out , self . hidden = self . lstm ( seq . view ( len ( seq ), 1 , - 1 ), self . hidden ) pred = self . linear ( lstm_out . view ( len ( seq ), - 1 )) return pred [ - 1 ] # we only want the last value","title":"Define the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#instantiate-the-model-define-loss-and-optimization-functions","text":"torch . manual_seed ( 101 ) model = LSTMnetwork () criterion = nn . MSELoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) model LSTMnetwork( (lstm): LSTM(1, 100) (linear): Linear(in_features=100, out_features=1, bias=True) ) def count_parameters ( model ): params = [ p . numel () for p in model . parameters () if p . requires_grad ] for item in params : print ( f ' { item : >6 } ' ) print ( f '______ \\n { sum ( params ) : >6 } ' ) count_parameters ( model ) 400 40000 400 400 100 1 ______ 41301","title":"Instantiate the model, define loss and optimization functions"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#train-the-model","text":"epochs = 100 import time start_time = time . time () for epoch in range ( epochs ): # extract the sequence & label from the training data for seq , y_train in train_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) y_pred = model ( seq ) loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # print training result print ( f 'Epoch: { epoch + 1 : 2 } Loss: { loss . item () : 10.8f } ' ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) Epoch: 1 Loss: 0.25314346 Epoch: 2 Loss: 0.37523878 Epoch: 3 Loss: 0.39012098 Epoch: 4 Loss: 0.34949699 Epoch: 5 Loss: 0.33077896 Epoch: 6 Loss: 0.33230731 Epoch: 7 Loss: 0.32667691 Epoch: 8 Loss: 0.32077038 Epoch: 9 Loss: 0.30651760 Epoch: 10 Loss: 0.29517019 Epoch: 11 Loss: 0.26913935 Epoch: 12 Loss: 0.25979361 Epoch: 13 Loss: 0.24600053 Epoch: 14 Loss: 0.22227803 Epoch: 15 Loss: 0.18593730 Epoch: 16 Loss: 0.18445705 Epoch: 17 Loss: 0.17432655 Epoch: 18 Loss: 0.25822312 Epoch: 19 Loss: 0.22420478 Epoch: 20 Loss: 0.23121868 Epoch: 21 Loss: 0.19758533 Epoch: 22 Loss: 0.18988022 Epoch: 23 Loss: 0.21032184 Epoch: 24 Loss: 0.16950732 Epoch: 25 Loss: 0.14586549 Epoch: 26 Loss: 0.11828811 Epoch: 27 Loss: 0.00027535 Epoch: 28 Loss: 0.00681852 Epoch: 29 Loss: 0.05630350 Epoch: 30 Loss: 0.01223376 Epoch: 31 Loss: 0.02709176 Epoch: 32 Loss: 0.03447094 Epoch: 33 Loss: 0.02249899 Epoch: 34 Loss: 0.01159327 Epoch: 35 Loss: 0.01592799 Epoch: 36 Loss: 0.00303702 Epoch: 37 Loss: 0.00015524 Epoch: 38 Loss: 0.00017455 Epoch: 39 Loss: 0.00034566 Epoch: 40 Loss: 0.00000055 Epoch: 41 Loss: 0.00002718 Epoch: 42 Loss: 0.00010465 Epoch: 43 Loss: 0.00106159 Epoch: 44 Loss: 0.00250127 Epoch: 45 Loss: 0.00288290 Epoch: 46 Loss: 0.00362020 Epoch: 47 Loss: 0.00371592 Epoch: 48 Loss: 0.00542405 Epoch: 49 Loss: 0.00589132 Epoch: 50 Loss: 0.00713205 Epoch: 51 Loss: 0.00702253 Epoch: 52 Loss: 0.00773653 Epoch: 53 Loss: 0.00765838 Epoch: 54 Loss: 0.00816493 Epoch: 55 Loss: 0.00806901 Epoch: 56 Loss: 0.00782317 Epoch: 57 Loss: 0.00831962 Epoch: 58 Loss: 0.00794266 Epoch: 59 Loss: 0.00764198 Epoch: 60 Loss: 0.00736130 Epoch: 61 Loss: 0.00857497 Epoch: 62 Loss: 0.00942086 Epoch: 63 Loss: 0.00335479 Epoch: 64 Loss: 0.00118402 Epoch: 65 Loss: 0.00320848 Epoch: 66 Loss: 0.00212591 Epoch: 67 Loss: 0.00150738 Epoch: 68 Loss: 0.00153509 Epoch: 69 Loss: 0.00182797 Epoch: 70 Loss: 0.00180291 Epoch: 71 Loss: 0.00140510 Epoch: 72 Loss: 0.00096806 Epoch: 73 Loss: 0.00047413 Epoch: 74 Loss: 0.00035952 Epoch: 75 Loss: 0.00008912 Epoch: 76 Loss: 0.00112789 Epoch: 77 Loss: 0.00002196 Epoch: 78 Loss: 0.00695060 Epoch: 79 Loss: 0.00038595 Epoch: 80 Loss: 0.00011149 Epoch: 81 Loss: 0.00043127 Epoch: 82 Loss: 0.00040868 Epoch: 83 Loss: 0.00032796 Epoch: 84 Loss: 0.00048195 Epoch: 85 Loss: 0.00049252 Epoch: 86 Loss: 0.00037195 Epoch: 87 Loss: 0.00072067 Epoch: 88 Loss: 0.00096091 Epoch: 89 Loss: 0.00063866 Epoch: 90 Loss: 0.00073793 Epoch: 91 Loss: 0.00092162 Epoch: 92 Loss: 0.00084968 Epoch: 93 Loss: 0.00073029 Epoch: 94 Loss: 0.00015117 Epoch: 95 Loss: 0.00018061 Epoch: 96 Loss: 0.00010094 Epoch: 97 Loss: 0.00006259 Epoch: 98 Loss: 0.00049196 Epoch: 99 Loss: 0.00033408 Epoch: 100 Loss: 0.00050564 Duration: 278 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#run-predictions-and-compare-to-known-test-set","text":"future = 12 # Add the last window of training values to the list of predictions preds = train_norm [ - window_size :] . tolist () # Set the model to evaluation mode model . eval () for i in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) # Display predicted values preds [ window_size :] # equivalent to preds[-future:] [0.3245275914669037, 0.5227924585342407, 0.5454435348510742, 0.8364425301551819, 1.104264736175537, 0.30810344219207764, 0.8207511901855469, 0.5576714873313904, 0.5653725862503052, 0.8198413848876953, 0.9293676018714905, 0.12615284323692322]","title":"Run predictions and compare to known test set"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#invert-the-normalization","text":"We want to compare our test predictions to the original data, so we need to undo the previous normalization step. Note that inverse_transform uses the most recently applied parameters; we can rescale based on the test data, but not on the previous training data. true_predictions = scaler . inverse_transform ( np . array ( preds [ window_size :]) . reshape ( - 1 , 1 )) true_predictions array([[10778.82414629], [11938.5744862 ], [12071.07195711], [13773.27058014], [15339.89657426], [10682.7510851 ], [13681.48408699], [12142.59936514], [12187.64694327], [13676.1621809 ], [14316.83578715], [ 9618.43105651]]) df [ 'S4248SM144NCEN' ][ - 12 :] DATE 2018-02-01 10415 2018-03-01 12683 2018-04-01 11919 2018-05-01 14138 2018-06-01 14583 2018-07-01 12640 2018-08-01 14257 2018-09-01 12396 2018-10-01 13914 2018-11-01 14174 2018-12-01 15504 2019-01-01 10718 Name: S4248SM144NCEN, dtype: int64 It looks like our predictions weren\u2019t that far off! ## Plot the results Our original data contains a datetime index, but our predicted values do not. We can create a range of dates using NumPy that are spaced one month apart using dtype=\u2018datetime64[M]\u2019 , and then store them with day values to match our dataset with .astype(\u2018datetime64[D]\u2019) . # Remember that the stop date has to be later than the last predicted value. x = np . arange ( '2018-02-01' , '2019-02-01' , dtype = 'datetime64[M]' ) . astype ( 'datetime64[D]' ) x array(['2018-02-01', '2018-03-01', '2018-04-01', '2018-05-01', '2018-06-01', '2018-07-01', '2018-08-01', '2018-09-01', '2018-10-01', '2018-11-01', '2018-12-01', '2019-01-01'], dtype='datetime64[D]') plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'S4248SM144NCEN' ]) plt . plot ( x , true_predictions ) plt . show () # Plot the end of the graph fig = plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) fig . autofmt_xdate () # Select the end of the graph with slice notation: plt . plot ( df [ 'S4248SM144NCEN' ][ '2017-01-01' :]) plt . plot ( x , true_predictions ) plt . show () For more information on x-axis date formatting in matplotlib, check out matplotlib.figure.Figure.autofmt_xdate and matplotlib.dates.DateFormatter","title":"Invert the normalization"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#forecast-into-an-unknown-future","text":"This time we\u2019ll continue training the model using the entire dataset, and predict 12 steps into the future. epochs = 100 # set model to back to training mode model . train () # feature scale the entire dataset y_norm = scaler . fit_transform ( y . reshape ( - 1 , 1 )) y_norm = torch . FloatTensor ( y_norm ) . view ( - 1 ) all_data = input_data ( y_norm , window_size ) import time start_time = time . time () for epoch in range ( epochs ): # train on the full set of sequences for seq , y_train in all_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) y_pred = model ( seq ) loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # print training result print ( f 'Epoch: { epoch + 1 : 2 } Loss: { loss . item () : 10.8f } ' ) print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) Epoch: 1 Loss: 0.02658496 Epoch: 2 Loss: 0.02985313 Epoch: 3 Loss: 0.01421225 Epoch: 4 Loss: 0.01419733 Epoch: 5 Loss: 0.01290122 Epoch: 6 Loss: 0.01244991 Epoch: 7 Loss: 0.01119690 Epoch: 8 Loss: 0.00854800 Epoch: 9 Loss: 0.00788417 Epoch: 10 Loss: 0.00792318 Epoch: 11 Loss: 0.00556064 Epoch: 12 Loss: 0.00598394 Epoch: 13 Loss: 0.00537636 Epoch: 14 Loss: 0.00649350 Epoch: 15 Loss: 0.00798889 Epoch: 16 Loss: 0.00699036 Epoch: 17 Loss: 0.00741554 Epoch: 18 Loss: 0.00671443 Epoch: 19 Loss: 0.00555703 Epoch: 20 Loss: 0.00517232 Epoch: 21 Loss: 0.00519288 Epoch: 22 Loss: 0.00412102 Epoch: 23 Loss: 0.00535119 Epoch: 24 Loss: 0.00888540 Epoch: 25 Loss: 0.00753472 Epoch: 26 Loss: 0.00589289 Epoch: 27 Loss: 0.00580891 Epoch: 28 Loss: 0.00776316 Epoch: 29 Loss: 0.00629479 Epoch: 30 Loss: 0.00895946 Epoch: 31 Loss: 0.00883982 Epoch: 32 Loss: 0.00968022 Epoch: 33 Loss: 0.00415197 Epoch: 34 Loss: 0.00383917 Epoch: 35 Loss: 0.00253066 Epoch: 36 Loss: 0.00149691 Epoch: 37 Loss: 0.00340568 Epoch: 38 Loss: 0.00264327 Epoch: 39 Loss: 0.00205079 Epoch: 40 Loss: 0.00436785 Epoch: 41 Loss: 0.00065083 Epoch: 42 Loss: 0.00063657 Epoch: 43 Loss: 0.00052040 Epoch: 44 Loss: 0.00060114 Epoch: 45 Loss: 0.00001630 Epoch: 46 Loss: 0.00138428 Epoch: 47 Loss: 0.00005873 Epoch: 48 Loss: 0.00092580 Epoch: 49 Loss: 0.00028546 Epoch: 50 Loss: 0.00265119 Epoch: 51 Loss: 0.00000118 Epoch: 52 Loss: 0.00076291 Epoch: 53 Loss: 0.00050618 Epoch: 54 Loss: 0.00063220 Epoch: 55 Loss: 0.00001087 Epoch: 56 Loss: 0.00001940 Epoch: 57 Loss: 0.00038629 Epoch: 58 Loss: 0.00057537 Epoch: 59 Loss: 0.00001950 Epoch: 60 Loss: 0.00133490 Epoch: 61 Loss: 0.00203123 Epoch: 62 Loss: 0.00004929 Epoch: 63 Loss: 0.00098640 Epoch: 64 Loss: 0.00086584 Epoch: 65 Loss: 0.00075280 Epoch: 66 Loss: 0.00050309 Epoch: 67 Loss: 0.00211758 Epoch: 68 Loss: 0.00260141 Epoch: 69 Loss: 0.00385657 Epoch: 70 Loss: 0.00009936 Epoch: 71 Loss: 0.00036399 Epoch: 72 Loss: 0.00154005 Epoch: 73 Loss: 0.00036845 Epoch: 74 Loss: 0.00021908 Epoch: 75 Loss: 0.01171840 Epoch: 76 Loss: 0.00335702 Epoch: 77 Loss: 0.00374896 Epoch: 78 Loss: 0.00772837 Epoch: 79 Loss: 0.00071883 Epoch: 80 Loss: 0.00118207 Epoch: 81 Loss: 0.00225649 Epoch: 82 Loss: 0.00011194 Epoch: 83 Loss: 0.00061754 Epoch: 84 Loss: 0.00208528 Epoch: 85 Loss: 0.01009495 Epoch: 86 Loss: 0.00545910 Epoch: 87 Loss: 0.00120122 Epoch: 88 Loss: 0.00036599 Epoch: 89 Loss: 0.00008251 Epoch: 90 Loss: 0.00085317 Epoch: 91 Loss: 0.00470886 Epoch: 92 Loss: 0.00336765 Epoch: 93 Loss: 0.00118269 Epoch: 94 Loss: 0.00009316 Epoch: 95 Loss: 0.00007733 Epoch: 96 Loss: 0.00416005 Epoch: 97 Loss: 0.00755412 Epoch: 98 Loss: 0.00003969 Epoch: 99 Loss: 0.00005710 Epoch: 100 Loss: 0.00009160 Duration: 297 seconds","title":"Forecast into an unknown future"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#predict-future-values-plot-the-result","text":"window_size = 12 future = 12 L = len ( y ) preds = y_norm [ - window_size :] . tolist () model . eval () for i in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): # Reset the hidden parameters here! model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) # Inverse-normalize the prediction set true_predictions = scaler . inverse_transform ( np . array ( preds ) . reshape ( - 1 , 1 )) # PLOT THE RESULT # Set a data range for the predicted data. # Remember that the stop date has to be later than the last predicted value. x = np . arange ( '2019-02-01' , '2020-02-01' , dtype = 'datetime64[M]' ) . astype ( 'datetime64[D]' ) plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'S4248SM144NCEN' ]) plt . plot ( x , true_predictions [ window_size :]) plt . show () fig = plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Beer, Wine, and Alcohol Sales' ) plt . ylabel ( 'Sales (millions of dollars)' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) fig . autofmt_xdate () plt . plot ( df [ 'S4248SM144NCEN' ][ '2017-01-01' :]) plt . plot ( x , true_predictions [ window_size :]) plt . show ()","title":"Predict future values, plot the result"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/","text":"================ by Jawad Haider RNN Exercises Perform standard imports, load and plot the dataset Prepare the data 1. Divide the data into train and test sets 2. Normalize the training set 3. Prepare data for LSTM 4. Define the model 5. Define loss and optimization functions 6. Train the model 9. Evaluate the model using the test set 10. Inverse transform the predicted values BONUS EXERCISE: Plot the result Great job! RNN Exercises \u00b6 For these exercises we\u2019re using data from the Federal Reserve Economic Database (FRED) concerning Electricity and Gas Utilities Production from January 1992 to January 2019 (325 records). Data source: https://fred.stlouisfed.org/series/IPG2211A2N In the exercises below you\u2019ll be asked to do the following: * Perform standard imports, load & plot the dataset (code provided) * Prepare data for an LSTM model * Define the LSTM model, loss and optimization functions * Train the model * Evaluate the model on test data * OPTIONAL: Plot the results IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! Perform standard imports, load and plot the dataset \u00b6 Run the cells below to load the libraries needed for this exercise and the Energy Production dataset, and to plot the data. # RUN THIS CELL import torch import torch.nn as nn from sklearn.preprocessing import MinMaxScaler import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline from pandas.plotting import register_matplotlib_converters register_matplotlib_converters () df = pd . read_csv ( '../Data/TimeSeriesData/Energy_Production.csv' , index_col = 0 , parse_dates = True ) df . dropna ( inplace = True ) print ( len ( df )) df . head () 325 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } IPG2211A2N DATE 1992-01-01 85.5560 1992-02-01 80.4178 1992-03-01 74.7390 1992-04-01 69.8367 1992-05-01 67.3781 # RUN THIS CELL plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Industrial Production Index for Electricity and Gas Utilities' ) plt . ylabel ( 'Index 2012=100, Not Seasonally Adjusted' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'IPG2211A2N' ]) plt . show () Prepare the data \u00b6 For the first set of exercises we\u2019ll * divide the data into train and test sets * normalize the training set * prepare windowed seq/label tuples for an LSTM model 1. Divide the data into train and test sets \u00b6 Working with a window_size of 12, divide the dataset into a sequence of 313 training records (including the window), and a test set of 12 records. # CODE HERE y = df [ 'IPG2211A2N' ] . values . astype ( float ) test_size = window_size = train_set = test_set = # Run the code below to check your results: print ( f 'Train: { len ( train_set ) } ' ) print ( f 'Test: { len ( test_set ) } ' ) # DON'T WRITE HERE Train: 313 Test: 12 2. Normalize the training set \u00b6 Feature scale the training set to fit within the range [-1,1]. # CODE HERE scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) train_norm = # Run the code below to check your results: print ( f 'First item, original: { train_set [ 0 ] } ' ) print ( f 'First item, scaled: { train_norm [ 0 ] } ' ) # DON'T WRITE HERE First item, original: 85.556 First item, scaled: [-0.4091274] 3. Prepare data for LSTM \u00b6 Prepare the list of windowed sequence/label tuples to be fed into an LSTM model. # RUN THIS CELL train_norm = torch . FloatTensor ( train_norm ) . view ( - 1 ) def input_data ( seq , ws ): out = [] L = len ( seq ) for i in range ( L - ws ): window = seq [ i : i + ws ] label = seq [ i + ws : i + ws + 1 ] out . append (( window , label )) return out # CODE HERE train_data = # Run the code below to check your results: print ( f 'Train_data: { len ( train_data ) } ' ) # should equal 301 # DON'T WRITE HERE Train_data: 301 4. Define the model \u00b6 Design a model that has a (1,64) LSTM layer and a (64,1) fully-connected linear layer. Be sure to initialize \\(h_0\\) and \\(c_0\\) , and return only the last predicted value. # CODE HERE class LSTMnetwork ( nn . Module ): # Run the code below to check your results: torch . manual_seed ( 101 ) model = LSTMnetwork () model # DON'T WRITE HERE LSTMnetwork( (lstm): LSTM(1, 64) (linear): Linear(in_features=64, out_features=1, bias=True) ) 5. Define loss and optimization functions \u00b6 Define a loss function called \u201ccriterion\u201d and an optimizer called \u201coptimizer\u201d. You can use any functions you want, although we used MSELoss and Adam (learning rate of 0.001) respectively. # CODE HERE # DON'T WRITE HERE 6. Train the model \u00b6 Don\u2019t worry about tracking loss values, displaying results, or validating the test set. Just train the model through 50 epochs. We\u2019ll evaluate the trained model in the next step. OPTIONAL: print something after each epoch to indicate training progress. # CODE HERE # DON'T WRITE HERE 1 of 50 epochs completed 2 of 50 epochs completed 3 of 50 epochs completed 4 of 50 epochs completed 5 of 50 epochs completed 6 of 50 epochs completed 7 of 50 epochs completed 8 of 50 epochs completed 9 of 50 epochs completed 10 of 50 epochs completed 11 of 50 epochs completed 12 of 50 epochs completed 13 of 50 epochs completed 14 of 50 epochs completed 15 of 50 epochs completed 16 of 50 epochs completed 17 of 50 epochs completed 18 of 50 epochs completed 19 of 50 epochs completed 20 of 50 epochs completed 21 of 50 epochs completed 22 of 50 epochs completed 23 of 50 epochs completed 24 of 50 epochs completed 25 of 50 epochs completed 26 of 50 epochs completed 27 of 50 epochs completed 28 of 50 epochs completed 29 of 50 epochs completed 30 of 50 epochs completed 31 of 50 epochs completed 32 of 50 epochs completed 33 of 50 epochs completed 34 of 50 epochs completed 35 of 50 epochs completed 36 of 50 epochs completed 37 of 50 epochs completed 38 of 50 epochs completed 39 of 50 epochs completed 40 of 50 epochs completed 41 of 50 epochs completed 42 of 50 epochs completed 43 of 50 epochs completed 44 of 50 epochs completed 45 of 50 epochs completed 46 of 50 epochs completed 47 of 50 epochs completed 48 of 50 epochs completed 49 of 50 epochs completed 50 of 50 epochs completed 9. Evaluate the model using the test set \u00b6 Be sure to re-initialize the hidden parameters \\(h_0\\) and \\(c_0\\) before running the model. # CODE HERE future = preds = model . eval () for i in range ( future ): # Run the code below to check your results: preds [ window_size :] # DON'T WRITE HERE [0.25382155179977417, -0.0027704648673534393, -0.343053936958313, -0.21152164041996002, 0.23945370316505432, 0.4895053505897522, 0.24688751995563507, -0.08669154345989227, -0.25793153047561646, 0.022461334243416786, 0.5438402891159058, 0.6108715534210205] 10. Inverse transform the predicted values \u00b6 Rescale the predicted values up to the original test set range. # CODE HERE true_predictions = # Run the code below to check your results: true_predictions # DON'T WRITE HERE array([[105.95129313], [ 98.05736803], [ 87.58871716], [ 91.63524249], [105.50927345], [113.20198736], [105.73797111], [ 95.47557801], [ 90.20746543], [ 98.83361172], [114.87357457], [116.93575791]]) BONUS EXERCISE: Plot the result \u00b6 Plot the true_predictions values together with the original data. Remember to create a range of datetime values for the predicted data. # CODE HERE # CODE HERE TO DISPLAY THE END OF THE GRAPH # DON'T WRITE HERE # DON'T WRITE HERE Great job! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"02 RNN Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#rnn-exercises","text":"For these exercises we\u2019re using data from the Federal Reserve Economic Database (FRED) concerning Electricity and Gas Utilities Production from January 1992 to January 2019 (325 records). Data source: https://fred.stlouisfed.org/series/IPG2211A2N In the exercises below you\u2019ll be asked to do the following: * Perform standard imports, load & plot the dataset (code provided) * Prepare data for an LSTM model * Define the LSTM model, loss and optimization functions * Train the model * Evaluate the model on test data * OPTIONAL: Plot the results IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"RNN Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#perform-standard-imports-load-and-plot-the-dataset","text":"Run the cells below to load the libraries needed for this exercise and the Energy Production dataset, and to plot the data. # RUN THIS CELL import torch import torch.nn as nn from sklearn.preprocessing import MinMaxScaler import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline from pandas.plotting import register_matplotlib_converters register_matplotlib_converters () df = pd . read_csv ( '../Data/TimeSeriesData/Energy_Production.csv' , index_col = 0 , parse_dates = True ) df . dropna ( inplace = True ) print ( len ( df )) df . head () 325 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } IPG2211A2N DATE 1992-01-01 85.5560 1992-02-01 80.4178 1992-03-01 74.7390 1992-04-01 69.8367 1992-05-01 67.3781 # RUN THIS CELL plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Industrial Production Index for Electricity and Gas Utilities' ) plt . ylabel ( 'Index 2012=100, Not Seasonally Adjusted' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'IPG2211A2N' ]) plt . show ()","title":"Perform standard imports, load and plot the dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#prepare-the-data","text":"For the first set of exercises we\u2019ll * divide the data into train and test sets * normalize the training set * prepare windowed seq/label tuples for an LSTM model","title":"Prepare the data"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#1-divide-the-data-into-train-and-test-sets","text":"Working with a window_size of 12, divide the dataset into a sequence of 313 training records (including the window), and a test set of 12 records. # CODE HERE y = df [ 'IPG2211A2N' ] . values . astype ( float ) test_size = window_size = train_set = test_set = # Run the code below to check your results: print ( f 'Train: { len ( train_set ) } ' ) print ( f 'Test: { len ( test_set ) } ' ) # DON'T WRITE HERE Train: 313 Test: 12","title":"1. Divide the data into train and test sets"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#2-normalize-the-training-set","text":"Feature scale the training set to fit within the range [-1,1]. # CODE HERE scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) train_norm = # Run the code below to check your results: print ( f 'First item, original: { train_set [ 0 ] } ' ) print ( f 'First item, scaled: { train_norm [ 0 ] } ' ) # DON'T WRITE HERE First item, original: 85.556 First item, scaled: [-0.4091274]","title":"2. Normalize the training set"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#3-prepare-data-for-lstm","text":"Prepare the list of windowed sequence/label tuples to be fed into an LSTM model. # RUN THIS CELL train_norm = torch . FloatTensor ( train_norm ) . view ( - 1 ) def input_data ( seq , ws ): out = [] L = len ( seq ) for i in range ( L - ws ): window = seq [ i : i + ws ] label = seq [ i + ws : i + ws + 1 ] out . append (( window , label )) return out # CODE HERE train_data = # Run the code below to check your results: print ( f 'Train_data: { len ( train_data ) } ' ) # should equal 301 # DON'T WRITE HERE Train_data: 301","title":"3. Prepare data for LSTM"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#4-define-the-model","text":"Design a model that has a (1,64) LSTM layer and a (64,1) fully-connected linear layer. Be sure to initialize \\(h_0\\) and \\(c_0\\) , and return only the last predicted value. # CODE HERE class LSTMnetwork ( nn . Module ): # Run the code below to check your results: torch . manual_seed ( 101 ) model = LSTMnetwork () model # DON'T WRITE HERE LSTMnetwork( (lstm): LSTM(1, 64) (linear): Linear(in_features=64, out_features=1, bias=True) )","title":"4. Define the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#5-define-loss-and-optimization-functions","text":"Define a loss function called \u201ccriterion\u201d and an optimizer called \u201coptimizer\u201d. You can use any functions you want, although we used MSELoss and Adam (learning rate of 0.001) respectively. # CODE HERE # DON'T WRITE HERE","title":"5. Define loss and optimization functions"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#6-train-the-model","text":"Don\u2019t worry about tracking loss values, displaying results, or validating the test set. Just train the model through 50 epochs. We\u2019ll evaluate the trained model in the next step. OPTIONAL: print something after each epoch to indicate training progress. # CODE HERE # DON'T WRITE HERE 1 of 50 epochs completed 2 of 50 epochs completed 3 of 50 epochs completed 4 of 50 epochs completed 5 of 50 epochs completed 6 of 50 epochs completed 7 of 50 epochs completed 8 of 50 epochs completed 9 of 50 epochs completed 10 of 50 epochs completed 11 of 50 epochs completed 12 of 50 epochs completed 13 of 50 epochs completed 14 of 50 epochs completed 15 of 50 epochs completed 16 of 50 epochs completed 17 of 50 epochs completed 18 of 50 epochs completed 19 of 50 epochs completed 20 of 50 epochs completed 21 of 50 epochs completed 22 of 50 epochs completed 23 of 50 epochs completed 24 of 50 epochs completed 25 of 50 epochs completed 26 of 50 epochs completed 27 of 50 epochs completed 28 of 50 epochs completed 29 of 50 epochs completed 30 of 50 epochs completed 31 of 50 epochs completed 32 of 50 epochs completed 33 of 50 epochs completed 34 of 50 epochs completed 35 of 50 epochs completed 36 of 50 epochs completed 37 of 50 epochs completed 38 of 50 epochs completed 39 of 50 epochs completed 40 of 50 epochs completed 41 of 50 epochs completed 42 of 50 epochs completed 43 of 50 epochs completed 44 of 50 epochs completed 45 of 50 epochs completed 46 of 50 epochs completed 47 of 50 epochs completed 48 of 50 epochs completed 49 of 50 epochs completed 50 of 50 epochs completed","title":"6. Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#9-evaluate-the-model-using-the-test-set","text":"Be sure to re-initialize the hidden parameters \\(h_0\\) and \\(c_0\\) before running the model. # CODE HERE future = preds = model . eval () for i in range ( future ): # Run the code below to check your results: preds [ window_size :] # DON'T WRITE HERE [0.25382155179977417, -0.0027704648673534393, -0.343053936958313, -0.21152164041996002, 0.23945370316505432, 0.4895053505897522, 0.24688751995563507, -0.08669154345989227, -0.25793153047561646, 0.022461334243416786, 0.5438402891159058, 0.6108715534210205]","title":"9. Evaluate the model using the test set"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#10-inverse-transform-the-predicted-values","text":"Rescale the predicted values up to the original test set range. # CODE HERE true_predictions = # Run the code below to check your results: true_predictions # DON'T WRITE HERE array([[105.95129313], [ 98.05736803], [ 87.58871716], [ 91.63524249], [105.50927345], [113.20198736], [105.73797111], [ 95.47557801], [ 90.20746543], [ 98.83361172], [114.87357457], [116.93575791]])","title":"10. Inverse transform the predicted values"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#bonus-exercise-plot-the-result","text":"Plot the true_predictions values together with the original data. Remember to create a range of datetime values for the predicted data. # CODE HERE # CODE HERE TO DISPLAY THE END OF THE GRAPH # DON'T WRITE HERE # DON'T WRITE HERE","title":"BONUS EXERCISE: Plot the result"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/#great-job","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/","text":"================ by Jawad Haider RNN Exercises - Solutions Perform standard imports, load and plot the dataset Prepare the data 1. Divide the data into train and test sets 2. Normalize the training set 3. Prepare data for LSTM 4. Define the model 5. Define loss and optimization functions 6. Train the model 9. Evaluate the model using the test set 10. Inverse transform the predicted values BONUS EXERCISE: Plot the result Great job! RNN Exercises - Solutions \u00b6 For these exercises we\u2019re using data from the Federal Reserve Economic Database (FRED) concerning Electricity and Gas Utilities Production from January 1992 to January 2019 (325 records). Data source: https://fred.stlouisfed.org/series/IPG2211A2N In the exercises below you\u2019ll be asked to do the following: * Perform standard imports, load & plot the dataset (code provided) * Prepare data for an LSTM model * Define the LSTM model, loss and optimization functions * Train the model * Evaluate the model on test data * OPTIONAL: Plot the results IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! Perform standard imports, load and plot the dataset \u00b6 Run the cells below to load the libraries needed for this exercise and the Energy Production dataset, and to plot the data. # RUN THIS CELL import torch import torch.nn as nn from sklearn.preprocessing import MinMaxScaler import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline from pandas.plotting import register_matplotlib_converters register_matplotlib_converters () df = pd . read_csv ( '../Data/TimeSeriesData/Energy_Production.csv' , index_col = 0 , parse_dates = True ) df . dropna ( inplace = True ) print ( len ( df )) df . head () 325 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } IPG2211A2N DATE 1992-01-01 85.5560 1992-02-01 80.4178 1992-03-01 74.7390 1992-04-01 69.8367 1992-05-01 67.3781 # RUN THIS CELL plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Industrial Production Index for Electricity and Gas Utilities' ) plt . ylabel ( 'Index 2012=100, Not Seasonally Adjusted' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'IPG2211A2N' ]) plt . show () Prepare the data \u00b6 For the first set of exercises we\u2019ll * divide the data into train and test sets * normalize the training set * prepare windowed seq/label tuples for an LSTM model 1. Divide the data into train and test sets \u00b6 Working with a window_size of 12, divide the dataset into a sequence of 313 training records (including the window), and a test set of 12 records. # CODE HERE y = df [ 'IPG2211A2N' ] . values . astype ( float ) test_size = window_size = train_set = test_set = # Run the code below to check your results: print ( f 'Train: { len ( train_set ) } ' ) print ( f 'Test: { len ( test_set ) } ' ) # DON'T WRITE HERE y = df [ 'IPG2211A2N' ] . values . astype ( float ) test_size = 12 window_size = 12 train_set = y [: - test_size ] test_set = y [ - test_size :] print ( f 'Train: { len ( train_set ) } ' ) print ( f 'Test: { len ( test_set ) } ' ) Train: 313 Test: 12 2. Normalize the training set \u00b6 Feature scale the training set to fit within the range [-1,1]. # CODE HERE scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) train_norm = # Run the code below to check your results: print ( f 'First item, original: { train_set [ 0 ] } ' ) print ( f 'First item, scaled: { train_norm [ 0 ] } ' ) # DON'T WRITE HERE scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) train_norm = scaler . fit_transform ( train_set . reshape ( - 1 , 1 )) print ( f 'First item, original: { train_set [ 0 ] } ' ) print ( f 'First item, scaled: { train_norm [ 0 ] } ' ) First item, original: 85.556 First item, scaled: [-0.4091274] 3. Prepare data for LSTM \u00b6 Prepare the list of windowed sequence/label tuples to be fed into an LSTM model. # RUN THIS CELL train_norm = torch . FloatTensor ( train_norm ) . view ( - 1 ) def input_data ( seq , ws ): out = [] L = len ( seq ) for i in range ( L - ws ): window = seq [ i : i + ws ] label = seq [ i + ws : i + ws + 1 ] out . append (( window , label )) return out # CODE HERE train_data = # Run the code below to check your results: print ( f 'Train_data: { len ( train_data ) } ' ) # should equal 301 # DON'T WRITE HERE train_data = input_data ( train_norm , window_size ) print ( f 'Train_data: { len ( train_data ) } ' ) Train_data: 301 4. Define the model \u00b6 Design a model that has a (1,64) LSTM layer and a (64,1) fully-connected linear layer. Be sure to initialize \\(h_0\\) and \\(c_0\\) , and return only the last predicted value. # CODE HERE class LSTMnetwork ( nn . Module ): # Run the code below to check your results: torch . manual_seed ( 101 ) model = LSTMnetwork () model # DON'T WRITE HERE class LSTMnetwork ( nn . Module ): def __init__ ( self , input_size = 1 , hidden_size = 64 , output_size = 1 ): super () . __init__ () self . hidden_size = hidden_size # Add an LSTM layer: self . lstm = nn . LSTM ( input_size , hidden_size ) # Add a fully-connected layer: self . linear = nn . Linear ( hidden_size , output_size ) # Initialize h0 and c0: self . hidden = ( torch . zeros ( 1 , 1 , self . hidden_size ), torch . zeros ( 1 , 1 , self . hidden_size )) def forward ( self , seq ): lstm_out , self . hidden = self . lstm ( seq . view ( len ( seq ), 1 , - 1 ), self . hidden ) pred = self . linear ( lstm_out . view ( len ( seq ), - 1 )) return pred [ - 1 ] torch . manual_seed ( 101 ) model = LSTMnetwork () model LSTMnetwork( (lstm): LSTM(1, 64) (linear): Linear(in_features=64, out_features=1, bias=True) ) 5. Define loss and optimization functions \u00b6 Define a loss function called \u201ccriterion\u201d and an optimizer called \u201coptimizer\u201d. You can use any functions you want, although we used MSELoss and Adam (learning rate of 0.001) respectively. # CODE HERE # DON'T WRITE HERE criterion = nn . MSELoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) 6. Train the model \u00b6 Don\u2019t worry about tracking loss values, displaying results, or validating the test set. Just train the model through 50 epochs. We\u2019ll evaluate the trained model in the next step. OPTIONAL: print something after each epoch to indicate training progress. # CODE HERE # DON'T WRITE HERE epochs = 50 for i in range ( epochs ): for seq , y_train in train_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) # apply the model y_pred = model ( seq ) # update parameters loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # OPTIONAL print statement print ( f ' { i + 1 } of { epochs } epochs completed' ) 1 of 50 epochs completed 2 of 50 epochs completed 3 of 50 epochs completed 4 of 50 epochs completed 5 of 50 epochs completed 6 of 50 epochs completed 7 of 50 epochs completed 8 of 50 epochs completed 9 of 50 epochs completed 10 of 50 epochs completed 11 of 50 epochs completed 12 of 50 epochs completed 13 of 50 epochs completed 14 of 50 epochs completed 15 of 50 epochs completed 16 of 50 epochs completed 17 of 50 epochs completed 18 of 50 epochs completed 19 of 50 epochs completed 20 of 50 epochs completed 21 of 50 epochs completed 22 of 50 epochs completed 23 of 50 epochs completed 24 of 50 epochs completed 25 of 50 epochs completed 26 of 50 epochs completed 27 of 50 epochs completed 28 of 50 epochs completed 29 of 50 epochs completed 30 of 50 epochs completed 31 of 50 epochs completed 32 of 50 epochs completed 33 of 50 epochs completed 34 of 50 epochs completed 35 of 50 epochs completed 36 of 50 epochs completed 37 of 50 epochs completed 38 of 50 epochs completed 39 of 50 epochs completed 40 of 50 epochs completed 41 of 50 epochs completed 42 of 50 epochs completed 43 of 50 epochs completed 44 of 50 epochs completed 45 of 50 epochs completed 46 of 50 epochs completed 47 of 50 epochs completed 48 of 50 epochs completed 49 of 50 epochs completed 50 of 50 epochs completed 9. Evaluate the model using the test set \u00b6 Be sure to re-initialize the hidden parameters \\(h_0\\) and \\(c_0\\) before running the model. # CODE HERE future = preds = model . eval () for i in range ( future ): # Run the code below to check your results: preds [ window_size :] # DON'T WRITE HERE future = 12 preds = train_norm [ - window_size :] . tolist () model . eval () for i in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) preds [ window_size :] [0.25382155179977417, -0.0027704648673534393, -0.343053936958313, -0.21152164041996002, 0.23945370316505432, 0.4895053505897522, 0.24688751995563507, -0.08669154345989227, -0.25793153047561646, 0.022461334243416786, 0.5438402891159058, 0.6108715534210205] 10. Inverse transform the predicted values \u00b6 Rescale the predicted values up to the original test set range. # CODE HERE true_predictions = # Run the code below to check your results: true_predictions # DON'T WRITE HERE true_predictions = scaler . inverse_transform ( np . array ( preds [ window_size :]) . reshape ( - 1 , 1 )) true_predictions array([[105.95129313], [ 98.05736803], [ 87.58871716], [ 91.63524249], [105.50927345], [113.20198736], [105.73797111], [ 95.47557801], [ 90.20746543], [ 98.83361172], [114.87357457], [116.93575791]]) BONUS EXERCISE: Plot the result \u00b6 Plot the true_predictions values together with the original data. Remember to create a range of datetime values for the predicted data. # CODE HERE # CODE HERE TO DISPLAY THE END OF THE GRAPH # DON'T WRITE HERE x = np . arange ( '2018-02-01' , '2019-02-01' , dtype = 'datetime64[M]' ) . astype ( 'datetime64[D]' ) plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Industrial Production Index for Electricity and Gas Utilities' ) plt . ylabel ( 'Index 2012=100, Not Seasonally Adjusted' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'IPG2211A2N' ]) plt . plot ( x , true_predictions ) plt . show () # DON'T WRITE HERE fig = plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Industrial Production Index for Electricity and Gas Utilities' ) plt . ylabel ( 'Index 2012=100, Not Seasonally Adjusted' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) fig . autofmt_xdate () plt . plot ( df [ 'IPG2211A2N' ][ '2017-01-01' :]) plt . plot ( x , true_predictions ) plt . show () Great job! \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"03 RNN Exercises Solutions"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#rnn-exercises-solutions","text":"For these exercises we\u2019re using data from the Federal Reserve Economic Database (FRED) concerning Electricity and Gas Utilities Production from January 1992 to January 2019 (325 records). Data source: https://fred.stlouisfed.org/series/IPG2211A2N In the exercises below you\u2019ll be asked to do the following: * Perform standard imports, load & plot the dataset (code provided) * Prepare data for an LSTM model * Define the LSTM model, loss and optimization functions * Train the model * Evaluate the model on test data * OPTIONAL: Plot the results IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"RNN Exercises - Solutions"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#perform-standard-imports-load-and-plot-the-dataset","text":"Run the cells below to load the libraries needed for this exercise and the Energy Production dataset, and to plot the data. # RUN THIS CELL import torch import torch.nn as nn from sklearn.preprocessing import MinMaxScaler import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline from pandas.plotting import register_matplotlib_converters register_matplotlib_converters () df = pd . read_csv ( '../Data/TimeSeriesData/Energy_Production.csv' , index_col = 0 , parse_dates = True ) df . dropna ( inplace = True ) print ( len ( df )) df . head () 325 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } IPG2211A2N DATE 1992-01-01 85.5560 1992-02-01 80.4178 1992-03-01 74.7390 1992-04-01 69.8367 1992-05-01 67.3781 # RUN THIS CELL plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Industrial Production Index for Electricity and Gas Utilities' ) plt . ylabel ( 'Index 2012=100, Not Seasonally Adjusted' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'IPG2211A2N' ]) plt . show ()","title":"Perform standard imports, load and plot the dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#prepare-the-data","text":"For the first set of exercises we\u2019ll * divide the data into train and test sets * normalize the training set * prepare windowed seq/label tuples for an LSTM model","title":"Prepare the data"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#1-divide-the-data-into-train-and-test-sets","text":"Working with a window_size of 12, divide the dataset into a sequence of 313 training records (including the window), and a test set of 12 records. # CODE HERE y = df [ 'IPG2211A2N' ] . values . astype ( float ) test_size = window_size = train_set = test_set = # Run the code below to check your results: print ( f 'Train: { len ( train_set ) } ' ) print ( f 'Test: { len ( test_set ) } ' ) # DON'T WRITE HERE y = df [ 'IPG2211A2N' ] . values . astype ( float ) test_size = 12 window_size = 12 train_set = y [: - test_size ] test_set = y [ - test_size :] print ( f 'Train: { len ( train_set ) } ' ) print ( f 'Test: { len ( test_set ) } ' ) Train: 313 Test: 12","title":"1. Divide the data into train and test sets"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#2-normalize-the-training-set","text":"Feature scale the training set to fit within the range [-1,1]. # CODE HERE scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) train_norm = # Run the code below to check your results: print ( f 'First item, original: { train_set [ 0 ] } ' ) print ( f 'First item, scaled: { train_norm [ 0 ] } ' ) # DON'T WRITE HERE scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) train_norm = scaler . fit_transform ( train_set . reshape ( - 1 , 1 )) print ( f 'First item, original: { train_set [ 0 ] } ' ) print ( f 'First item, scaled: { train_norm [ 0 ] } ' ) First item, original: 85.556 First item, scaled: [-0.4091274]","title":"2. Normalize the training set"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#3-prepare-data-for-lstm","text":"Prepare the list of windowed sequence/label tuples to be fed into an LSTM model. # RUN THIS CELL train_norm = torch . FloatTensor ( train_norm ) . view ( - 1 ) def input_data ( seq , ws ): out = [] L = len ( seq ) for i in range ( L - ws ): window = seq [ i : i + ws ] label = seq [ i + ws : i + ws + 1 ] out . append (( window , label )) return out # CODE HERE train_data = # Run the code below to check your results: print ( f 'Train_data: { len ( train_data ) } ' ) # should equal 301 # DON'T WRITE HERE train_data = input_data ( train_norm , window_size ) print ( f 'Train_data: { len ( train_data ) } ' ) Train_data: 301","title":"3. Prepare data for LSTM"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#4-define-the-model","text":"Design a model that has a (1,64) LSTM layer and a (64,1) fully-connected linear layer. Be sure to initialize \\(h_0\\) and \\(c_0\\) , and return only the last predicted value. # CODE HERE class LSTMnetwork ( nn . Module ): # Run the code below to check your results: torch . manual_seed ( 101 ) model = LSTMnetwork () model # DON'T WRITE HERE class LSTMnetwork ( nn . Module ): def __init__ ( self , input_size = 1 , hidden_size = 64 , output_size = 1 ): super () . __init__ () self . hidden_size = hidden_size # Add an LSTM layer: self . lstm = nn . LSTM ( input_size , hidden_size ) # Add a fully-connected layer: self . linear = nn . Linear ( hidden_size , output_size ) # Initialize h0 and c0: self . hidden = ( torch . zeros ( 1 , 1 , self . hidden_size ), torch . zeros ( 1 , 1 , self . hidden_size )) def forward ( self , seq ): lstm_out , self . hidden = self . lstm ( seq . view ( len ( seq ), 1 , - 1 ), self . hidden ) pred = self . linear ( lstm_out . view ( len ( seq ), - 1 )) return pred [ - 1 ] torch . manual_seed ( 101 ) model = LSTMnetwork () model LSTMnetwork( (lstm): LSTM(1, 64) (linear): Linear(in_features=64, out_features=1, bias=True) )","title":"4. Define the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#5-define-loss-and-optimization-functions","text":"Define a loss function called \u201ccriterion\u201d and an optimizer called \u201coptimizer\u201d. You can use any functions you want, although we used MSELoss and Adam (learning rate of 0.001) respectively. # CODE HERE # DON'T WRITE HERE criterion = nn . MSELoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"5. Define loss and optimization functions"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#6-train-the-model","text":"Don\u2019t worry about tracking loss values, displaying results, or validating the test set. Just train the model through 50 epochs. We\u2019ll evaluate the trained model in the next step. OPTIONAL: print something after each epoch to indicate training progress. # CODE HERE # DON'T WRITE HERE epochs = 50 for i in range ( epochs ): for seq , y_train in train_data : # reset the parameters and hidden states optimizer . zero_grad () model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) # apply the model y_pred = model ( seq ) # update parameters loss = criterion ( y_pred , y_train ) loss . backward () optimizer . step () # OPTIONAL print statement print ( f ' { i + 1 } of { epochs } epochs completed' ) 1 of 50 epochs completed 2 of 50 epochs completed 3 of 50 epochs completed 4 of 50 epochs completed 5 of 50 epochs completed 6 of 50 epochs completed 7 of 50 epochs completed 8 of 50 epochs completed 9 of 50 epochs completed 10 of 50 epochs completed 11 of 50 epochs completed 12 of 50 epochs completed 13 of 50 epochs completed 14 of 50 epochs completed 15 of 50 epochs completed 16 of 50 epochs completed 17 of 50 epochs completed 18 of 50 epochs completed 19 of 50 epochs completed 20 of 50 epochs completed 21 of 50 epochs completed 22 of 50 epochs completed 23 of 50 epochs completed 24 of 50 epochs completed 25 of 50 epochs completed 26 of 50 epochs completed 27 of 50 epochs completed 28 of 50 epochs completed 29 of 50 epochs completed 30 of 50 epochs completed 31 of 50 epochs completed 32 of 50 epochs completed 33 of 50 epochs completed 34 of 50 epochs completed 35 of 50 epochs completed 36 of 50 epochs completed 37 of 50 epochs completed 38 of 50 epochs completed 39 of 50 epochs completed 40 of 50 epochs completed 41 of 50 epochs completed 42 of 50 epochs completed 43 of 50 epochs completed 44 of 50 epochs completed 45 of 50 epochs completed 46 of 50 epochs completed 47 of 50 epochs completed 48 of 50 epochs completed 49 of 50 epochs completed 50 of 50 epochs completed","title":"6. Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#9-evaluate-the-model-using-the-test-set","text":"Be sure to re-initialize the hidden parameters \\(h_0\\) and \\(c_0\\) before running the model. # CODE HERE future = preds = model . eval () for i in range ( future ): # Run the code below to check your results: preds [ window_size :] # DON'T WRITE HERE future = 12 preds = train_norm [ - window_size :] . tolist () model . eval () for i in range ( future ): seq = torch . FloatTensor ( preds [ - window_size :]) with torch . no_grad (): model . hidden = ( torch . zeros ( 1 , 1 , model . hidden_size ), torch . zeros ( 1 , 1 , model . hidden_size )) preds . append ( model ( seq ) . item ()) preds [ window_size :] [0.25382155179977417, -0.0027704648673534393, -0.343053936958313, -0.21152164041996002, 0.23945370316505432, 0.4895053505897522, 0.24688751995563507, -0.08669154345989227, -0.25793153047561646, 0.022461334243416786, 0.5438402891159058, 0.6108715534210205]","title":"9. Evaluate the model using the test set"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#10-inverse-transform-the-predicted-values","text":"Rescale the predicted values up to the original test set range. # CODE HERE true_predictions = # Run the code below to check your results: true_predictions # DON'T WRITE HERE true_predictions = scaler . inverse_transform ( np . array ( preds [ window_size :]) . reshape ( - 1 , 1 )) true_predictions array([[105.95129313], [ 98.05736803], [ 87.58871716], [ 91.63524249], [105.50927345], [113.20198736], [105.73797111], [ 95.47557801], [ 90.20746543], [ 98.83361172], [114.87357457], [116.93575791]])","title":"10. Inverse transform the predicted values"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#bonus-exercise-plot-the-result","text":"Plot the true_predictions values together with the original data. Remember to create a range of datetime values for the predicted data. # CODE HERE # CODE HERE TO DISPLAY THE END OF THE GRAPH # DON'T WRITE HERE x = np . arange ( '2018-02-01' , '2019-02-01' , dtype = 'datetime64[M]' ) . astype ( 'datetime64[D]' ) plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Industrial Production Index for Electricity and Gas Utilities' ) plt . ylabel ( 'Index 2012=100, Not Seasonally Adjusted' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) plt . plot ( df [ 'IPG2211A2N' ]) plt . plot ( x , true_predictions ) plt . show () # DON'T WRITE HERE fig = plt . figure ( figsize = ( 12 , 4 )) plt . title ( 'Industrial Production Index for Electricity and Gas Utilities' ) plt . ylabel ( 'Index 2012=100, Not Seasonally Adjusted' ) plt . grid ( True ) plt . autoscale ( axis = 'x' , tight = True ) fig . autofmt_xdate () plt . plot ( df [ 'IPG2211A2N' ][ '2017-01-01' :]) plt . plot ( x , true_predictions ) plt . show ()","title":"BONUS EXERCISE: Plot the result"},{"location":"bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/#great-job","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/","text":"================ by Jawad Haider What is CUDA? How do I install PyTorch for GPU? How do I know if I have CUDA available? Using GPU and CUDA Using CUDA instead of CPU Sending Models to GPU Convert Tensors to .cuda() tensors What is CUDA? \u00b6 Most people confuse CUDA for a language or maybe an API. It is not. It\u2019s more than that. CUDA is a parallel computing platform and programming model that makes using a GPU for general purpose computing simple and elegant. The developer still programs in the familiar C, C++, Fortran, or an ever expanding list of supported languages, and incorporates extensions of these languages in the form of a few basic keywords. These keywords let the developer express massive amounts of parallelism and direct the compiler to the portion of the application that maps to the GPU. How do I install PyTorch for GPU? \u00b6 Refer to video, its dependent on whether you have an NVIDIA GPU card or not. How do I know if I have CUDA available? \u00b6 import torch torch . cuda . is_available () # True True Using GPU and CUDA \u00b6 We\u2019ve provided 2 versions of our yml file, a GPU version and a CPU version. To use GPU, you need to either manually create a virtual environment, please watch the video related to this lecture, as not every computer can run GPU, you need CUDA and an NVIDIA GPU. ## Get Id of default device torch . cuda . current_device () 0 # 0 torch . cuda . get_device_name ( 0 ) # Get name device with ID '0' 'GeForce GTX 1080 Ti' # Returns the current GPU memory usage by # tensors in bytes for a given device torch . cuda . memory_allocated () 0 # Returns the current GPU memory managed by the # caching allocator in bytes for a given device torch . cuda . memory_cached () 0 Using CUDA instead of CPU \u00b6 # CPU a = torch . FloatTensor ([ 1. , 2. ]) a tensor([1., 2.]) a . device device(type='cpu') # GPU a = torch . FloatTensor ([ 1. , 2. ]) . cuda () a . device device(type='cuda', index=0) torch . cuda . memory_allocated () 512 Sending Models to GPU \u00b6 import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader from sklearn.model_selection import train_test_split import pandas as pd import matplotlib.pyplot as plt % matplotlib inline class Model ( nn . Module ): def __init__ ( self , in_features = 4 , h1 = 8 , h2 = 9 , out_features = 3 ): super () . __init__ () self . fc1 = nn . Linear ( in_features , h1 ) # input layer self . fc2 = nn . Linear ( h1 , h2 ) # hidden layer self . out = nn . Linear ( h2 , out_features ) # output layer def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . out ( x ) return x torch . manual_seed ( 32 ) model = Model () # From the discussions here: discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda next ( model . parameters ()) . is_cuda False gpumodel = model . cuda () next ( gpumodel . parameters ()) . is_cuda True df = pd . read_csv ( '../Data/iris.csv' ) X = df . drop ( 'target' , axis = 1 ) . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 ) Convert Tensors to .cuda() tensors \u00b6 X_train = torch . FloatTensor ( X_train ) . cuda () X_test = torch . FloatTensor ( X_test ) . cuda () y_train = torch . LongTensor ( y_train ) . cuda () y_test = torch . LongTensor ( y_test ) . cuda () trainloader = DataLoader ( X_train , batch_size = 60 , shuffle = True ) testloader = DataLoader ( X_test , batch_size = 60 , shuffle = False ) criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 ) import time epochs = 100 losses = [] start = time . time () for i in range ( epochs ): i += 1 y_pred = gpumodel . forward ( X_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 10 == 1 : print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'TOTAL TRAINING TIME: { time . time () - start } ' ) epoch: 1 loss: 1.15071142 epoch: 11 loss: 0.93773186 epoch: 21 loss: 0.77982736 epoch: 31 loss: 0.60996711 epoch: 41 loss: 0.40083539 epoch: 51 loss: 0.25436994 epoch: 61 loss: 0.15052448 epoch: 71 loss: 0.10086147 epoch: 81 loss: 0.08127660 epoch: 91 loss: 0.07230931 TOTAL TRAINING TIME: 0.4668765068054199 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"00 Using GPU and CUDA"},{"location":"bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/#what-is-cuda","text":"Most people confuse CUDA for a language or maybe an API. It is not. It\u2019s more than that. CUDA is a parallel computing platform and programming model that makes using a GPU for general purpose computing simple and elegant. The developer still programs in the familiar C, C++, Fortran, or an ever expanding list of supported languages, and incorporates extensions of these languages in the form of a few basic keywords. These keywords let the developer express massive amounts of parallelism and direct the compiler to the portion of the application that maps to the GPU.","title":"What is CUDA?"},{"location":"bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/#how-do-i-install-pytorch-for-gpu","text":"Refer to video, its dependent on whether you have an NVIDIA GPU card or not.","title":"How do I install PyTorch for GPU?"},{"location":"bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/#how-do-i-know-if-i-have-cuda-available","text":"import torch torch . cuda . is_available () # True True","title":"How do I know if I have CUDA available?"},{"location":"bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/#using-gpu-and-cuda","text":"We\u2019ve provided 2 versions of our yml file, a GPU version and a CPU version. To use GPU, you need to either manually create a virtual environment, please watch the video related to this lecture, as not every computer can run GPU, you need CUDA and an NVIDIA GPU. ## Get Id of default device torch . cuda . current_device () 0 # 0 torch . cuda . get_device_name ( 0 ) # Get name device with ID '0' 'GeForce GTX 1080 Ti' # Returns the current GPU memory usage by # tensors in bytes for a given device torch . cuda . memory_allocated () 0 # Returns the current GPU memory managed by the # caching allocator in bytes for a given device torch . cuda . memory_cached () 0","title":"Using GPU and CUDA"},{"location":"bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/#using-cuda-instead-of-cpu","text":"# CPU a = torch . FloatTensor ([ 1. , 2. ]) a tensor([1., 2.]) a . device device(type='cpu') # GPU a = torch . FloatTensor ([ 1. , 2. ]) . cuda () a . device device(type='cuda', index=0) torch . cuda . memory_allocated () 512","title":"Using CUDA instead of CPU"},{"location":"bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/#sending-models-to-gpu","text":"import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader from sklearn.model_selection import train_test_split import pandas as pd import matplotlib.pyplot as plt % matplotlib inline class Model ( nn . Module ): def __init__ ( self , in_features = 4 , h1 = 8 , h2 = 9 , out_features = 3 ): super () . __init__ () self . fc1 = nn . Linear ( in_features , h1 ) # input layer self . fc2 = nn . Linear ( h1 , h2 ) # hidden layer self . out = nn . Linear ( h2 , out_features ) # output layer def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . out ( x ) return x torch . manual_seed ( 32 ) model = Model () # From the discussions here: discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda next ( model . parameters ()) . is_cuda False gpumodel = model . cuda () next ( gpumodel . parameters ()) . is_cuda True df = pd . read_csv ( '../Data/iris.csv' ) X = df . drop ( 'target' , axis = 1 ) . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 )","title":"Sending Models to GPU"},{"location":"bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/#convert-tensors-to-cuda-tensors","text":"X_train = torch . FloatTensor ( X_train ) . cuda () X_test = torch . FloatTensor ( X_test ) . cuda () y_train = torch . LongTensor ( y_train ) . cuda () y_test = torch . LongTensor ( y_test ) . cuda () trainloader = DataLoader ( X_train , batch_size = 60 , shuffle = True ) testloader = DataLoader ( X_test , batch_size = 60 , shuffle = False ) criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 ) import time epochs = 100 losses = [] start = time . time () for i in range ( epochs ): i += 1 y_pred = gpumodel . forward ( X_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 10 == 1 : print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'TOTAL TRAINING TIME: { time . time () - start } ' ) epoch: 1 loss: 1.15071142 epoch: 11 loss: 0.93773186 epoch: 21 loss: 0.77982736 epoch: 31 loss: 0.60996711 epoch: 41 loss: 0.40083539 epoch: 51 loss: 0.25436994 epoch: 61 loss: 0.15052448 epoch: 71 loss: 0.10086147 epoch: 81 loss: 0.08127660 epoch: 91 loss: 0.07230931 TOTAL TRAINING TIME: 0.4668765068054199 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Convert Tensors to .cuda() tensors"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/","text":"================ by Jawad Haider RNN for Text Generation Generating Text (encoded variables) Imports Get Text Data Encode Entire Text One Hot Encoding \u2014\u2014\u2014\u2014\u2013 Creating Training Batches \u2014\u2014\u2014\u2014\u2014\u2013 Example of generating a batch GPU Check Creating the LSTM Model Instance of the Model Optimizer and Loss Training Data and Validation Data Training the Network Variables \u2014\u2014- Saving the Model Load Model Generating Predictions RNN for Text Generation \u00b6 Generating Text (encoded variables) \u00b6 We saw how to generate continuous values, now let\u2019s see how to generalize this to generate categorical sequences (such as words or letters). Imports \u00b6 import torch from torch import nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt % matplotlib inline Get Text Data \u00b6 with open ( '../Data/shakespeare.txt' , 'r' , encoding = 'utf8' ) as f : text = f . read () text [: 1000 ] \"\\n 1\\n From fairest creatures we desire increase,\\n That thereby beauty's rose might never die,\\n But as the riper should by time decease,\\n His tender heir might bear his memory:\\n But thou contracted to thine own bright eyes,\\n Feed'st thy light's flame with self-substantial fuel,\\n Making a famine where abundance lies,\\n Thy self thy foe, to thy sweet self too cruel:\\n Thou that art now the world's fresh ornament,\\n And only herald to the gaudy spring,\\n Within thine own bud buriest thy content,\\n And tender churl mak'st waste in niggarding:\\n Pity the world, or else this glutton be,\\n To eat the world's due, by the grave and thee.\\n\\n\\n 2\\n When forty winters shall besiege thy brow,\\n And dig deep trenches in thy beauty's field,\\n Thy youth's proud livery so gazed on now,\\n Will be a tattered weed of small worth held: \\n Then being asked, where all thy beauty lies,\\n Where all the treasure of thy lusty days;\\n To say within thine own deep su\" print ( text [: 1000 ]) 1 From fairest creatures we desire increase, That thereby beauty's rose might never die, But as the riper should by time decease, His tender heir might bear his memory: But thou contracted to thine own bright eyes, Feed'st thy light's flame with self-substantial fuel, Making a famine where abundance lies, Thy self thy foe, to thy sweet self too cruel: Thou that art now the world's fresh ornament, And only herald to the gaudy spring, Within thine own bud buriest thy content, And tender churl mak'st waste in niggarding: Pity the world, or else this glutton be, To eat the world's due, by the grave and thee. 2 When forty winters shall besiege thy brow, And dig deep trenches in thy beauty's field, Thy youth's proud livery so gazed on now, Will be a tattered weed of small worth held: Then being asked, where all thy beauty lies, Where all the treasure of thy lusty days; To say within thine own deep su len ( text ) 5445609 Encode Entire Text \u00b6 all_characters = set ( text ) # all_characters decoder = dict ( enumerate ( all_characters )) # decoder # decoder.items() encoder = { char : ind for ind , char in decoder . items ()} # encoder encoded_text = np . array ([ encoder [ char ] for char in text ]) encoded_text [: 500 ] array([51, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 41, 51, 26, 26, 31, 29, 39, 17, 26, 73, 14, 60, 29, 46, 78, 21, 26, 35, 29, 46, 14, 21, 6, 29, 46, 78, 26, 2, 46, 26, 65, 46, 78, 60, 29, 46, 26, 60, 55, 35, 29, 46, 14, 78, 46, 63, 51, 26, 26, 62, 7, 14, 21, 26, 21, 7, 46, 29, 46, 74, 38, 26, 74, 46, 14, 6, 21, 38, 77, 78, 26, 29, 39, 78, 46, 26, 17, 60, 82, 7, 21, 26, 55, 46, 27, 46, 29, 26, 65, 60, 46, 63, 51, 26, 26, 67, 6, 21, 26, 14, 78, 26, 21, 7, 46, 26, 29, 60, 22, 46, 29, 26, 78, 7, 39, 6, 43, 65, 26, 74, 38, 26, 21, 60, 17, 46, 26, 65, 46, 35, 46, 14, 78, 46, 63, 51, 26, 26, 36, 60, 78, 26, 21, 46, 55, 65, 46, 29, 26, 7, 46, 60, 29, 26, 17, 60, 82, 7, 21, 26, 74, 46, 14, 29, 26, 7, 60, 78, 26, 17, 46, 17, 39, 29, 38, 10, 51, 26, 26, 67, 6, 21, 26, 21, 7, 39, 6, 26, 35, 39, 55, 21, 29, 14, 35, 21, 46, 65, 26, 21, 39, 26, 21, 7, 60, 55, 46, 26, 39, 2, 55, 26, 74, 29, 60, 82, 7, 21, 26, 46, 38, 46, 78, 63, 51, 26, 26, 31, 46, 46, 65, 77, 78, 21, 26, 21, 7, 38, 26, 43, 60, 82, 7, 21, 77, 78, 26, 73, 43, 14, 17, 46, 26, 2, 60, 21, 7, 26, 78, 46, 43, 73, 49, 78, 6, 74, 78, 21, 14, 55, 21, 60, 14, 43, 26, 73, 6, 46, 43, 63, 51, 26, 26, 3, 14, 28, 60, 55, 82, 26, 14, 26, 73, 14, 17, 60, 55, 46, 26, 2, 7, 46, 29, 46, 26, 14, 74, 6, 55, 65, 14, 55, 35, 46, 26, 43, 60, 46, 78, 63, 51, 26, 26, 62, 7, 38, 26, 78, 46, 43, 73, 26, 21, 7, 38, 26, 73, 39, 46, 63, 26, 21, 39, 26, 21, 7, 38, 26, 78, 2, 46, 46, 21, 26, 78, 46, 43, 73, 26, 21, 39, 39, 26, 35, 29, 6, 46, 43, 10, 51, 26, 26, 62, 7, 39, 6, 26, 21, 7, 14, 21, 26, 14, 29, 21, 26, 55, 39, 2, 26, 21, 7, 46, 26, 2, 39, 29, 43, 65, 77, 78, 26, 73, 29, 46, 78, 7, 26, 39, 29, 55, 14, 17, 46, 55, 21, 63, 51, 26, 26, 56, 55, 65, 26, 39, 55, 43, 38, 26, 7, 46, 29, 14, 43, 65, 26, 21, 39, 26, 21, 7, 46, 26, 82, 14, 6, 65, 38, 26, 78, 22, 29, 60, 55, 82, 63, 51, 26, 26, 40, 60, 21, 7, 60, 55, 26, 21, 7, 60, 55, 46, 26, 39, 2, 55, 26, 74, 6]) One Hot Encoding \u00b6 As previously discussed, we need to one-hot encode our data inorder for it to work with the network structure. Make sure to review numpy if any of these operations confuse you! def one_hot_encoder ( encoded_text , num_uni_chars ): ''' encoded_text : batch of encoded text num_uni_chars = number of unique characters (len(set(text))) ''' # METHOD FROM: # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay # Create a placeholder for zeros. one_hot = np . zeros (( encoded_text . size , num_uni_chars )) # Convert data type for later use with pytorch (errors if we dont!) one_hot = one_hot . astype ( np . float32 ) # Using fancy indexing fill in the 1s at the correct index locations one_hot [ np . arange ( one_hot . shape [ 0 ]), encoded_text . flatten ()] = 1.0 # Reshape it so it matches the batch sahe one_hot = one_hot . reshape (( * encoded_text . shape , num_uni_chars )) return one_hot one_hot_encoder ( np . array ([ 1 , 2 , 0 ]), 3 ) array([[0., 1., 0.], [0., 0., 1.], [1., 0., 0.]], dtype=float32) \u2014\u2014\u2014\u2014\u2013 \u00b6 Creating Training Batches \u00b6 We need to create a function that will generate batches of characters along with the next character in the sequence as a label. \u2014\u2014\u2014\u2014\u2014\u2013 \u00b6 example_text = np . arange ( 10 ) example_text array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) # If we wanted 5 batches example_text . reshape (( 5 , - 1 )) array([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) def generate_batches ( encoded_text , samp_per_batch = 10 , seq_len = 50 ): ''' Generate (using yield) batches for training. X: Encoded Text of length seq_len Y: Encoded Text shifted by one Example: X: [[1 2 3]] Y: [[ 2 3 4]] encoded_text : Complete Encoded Text to make batches from batch_size : Number of samples per batch seq_len : Length of character sequence ''' # Total number of characters per batch # Example: If samp_per_batch is 2 and seq_len is 50, then 100 # characters come out per batch. char_per_batch = samp_per_batch * seq_len # Number of batches available to make # Use int() to roun to nearest integer num_batches_avail = int ( len ( encoded_text ) / char_per_batch ) # Cut off end of encoded_text that # won't fit evenly into a batch encoded_text = encoded_text [: num_batches_avail * char_per_batch ] # Reshape text into rows the size of a batch encoded_text = encoded_text . reshape (( samp_per_batch , - 1 )) # Go through each row in array. for n in range ( 0 , encoded_text . shape [ 1 ], seq_len ): # Grab feature characters x = encoded_text [:, n : n + seq_len ] # y is the target shifted over by 1 y = np . zeros_like ( x ) # try : y [:, : - 1 ] = x [:, 1 :] y [:, - 1 ] = encoded_text [:, n + seq_len ] # FOR POTENTIAL INDEXING ERROR AT THE END except : y [:, : - 1 ] = x [:, 1 :] y [:, - 1 ] = encoded_text [:, 0 ] yield x , y Example of generating a batch \u00b6 sample_text = encoded_text [: 20 ] sample_text array([51, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]) batch_generator = generate_batches ( sample_text , samp_per_batch = 2 , seq_len = 5 ) # Grab first batch x , y = next ( batch_generator ) x array([[51, 26, 26, 26, 26], [26, 26, 26, 26, 26]]) y array([[26, 26, 26, 26, 26], [26, 26, 26, 26, 26]]) GPU Check \u00b6 Remember this will take a lot longer on CPU! torch . cuda . is_available () True Creating the LSTM Model \u00b6 Note! We will have options for GPU users and CPU users. CPU will take MUCH LONGER to train and you may encounter RAM issues depending on your hardware. If that is the case, consider using cloud services like AWS, GCP, or Azure. Note, these may cost you money to use! class CharModel ( nn . Module ): def __init__ ( self , all_chars , num_hidden = 256 , num_layers = 4 , drop_prob = 0.5 , use_gpu = False ): # SET UP ATTRIBUTES super () . __init__ () self . drop_prob = drop_prob self . num_layers = num_layers self . num_hidden = num_hidden self . use_gpu = use_gpu #CHARACTER SET, ENCODER, and DECODER self . all_chars = all_chars self . decoder = dict ( enumerate ( all_chars )) self . encoder = { char : ind for ind , char in decoder . items ()} self . lstm = nn . LSTM ( len ( self . all_chars ), num_hidden , num_layers , dropout = drop_prob , batch_first = True ) self . dropout = nn . Dropout ( drop_prob ) self . fc_linear = nn . Linear ( num_hidden , len ( self . all_chars )) def forward ( self , x , hidden ): lstm_output , hidden = self . lstm ( x , hidden ) drop_output = self . dropout ( lstm_output ) drop_output = drop_output . contiguous () . view ( - 1 , self . num_hidden ) final_out = self . fc_linear ( drop_output ) return final_out , hidden def hidden_state ( self , batch_size ): ''' Used as separate method to account for both GPU and CPU users. ''' if self . use_gpu : hidden = ( torch . zeros ( self . num_layers , batch_size , self . num_hidden ) . cuda (), torch . zeros ( self . num_layers , batch_size , self . num_hidden ) . cuda ()) else : hidden = ( torch . zeros ( self . num_layers , batch_size , self . num_hidden ), torch . zeros ( self . num_layers , batch_size , self . num_hidden )) return hidden Instance of the Model \u00b6 model = CharModel ( all_chars = all_characters , num_hidden = 512 , num_layers = 3 , drop_prob = 0.5 , use_gpu = True , ) total_param = [] for p in model . parameters (): total_param . append ( int ( p . numel ())) Try to make the total_parameters be roughly the same magnitude as the number of characters in the text. sum ( total_param ) 5470292 len ( encoded_text ) 5445609 Optimizer and Loss \u00b6 optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) criterion = nn . CrossEntropyLoss () Training Data and Validation Data \u00b6 # percentage of data to be used for training train_percent = 0.1 len ( encoded_text ) 5445609 int ( len ( encoded_text ) * ( train_percent )) 544560 train_ind = int ( len ( encoded_text ) * ( train_percent )) train_data = encoded_text [: train_ind ] val_data = encoded_text [ train_ind :] Training the Network \u00b6 Variables \u00b6 Feel free to play around with these values! ## VARIABLES # Epochs to train for epochs = 50 # batch size batch_size = 128 # Length of sequence seq_len = 100 # for printing report purposes # always start at 0 tracker = 0 # number of characters in text num_char = max ( encoded_text ) + 1 # Set model to train model . train () # Check to see if using GPU if model . use_gpu : model . cuda () for i in range ( epochs ): hidden = model . hidden_state ( batch_size ) for x , y in generate_batches ( train_data , batch_size , seq_len ): tracker += 1 # One Hot Encode incoming data x = one_hot_encoder ( x , num_char ) # Convert Numpy Arrays to Tensor inputs = torch . from_numpy ( x ) targets = torch . from_numpy ( y ) # Adjust for GPU if necessary if model . use_gpu : inputs = inputs . cuda () targets = targets . cuda () # Reset Hidden State # If we dont' reset we would backpropagate through all training history hidden = tuple ([ state . data for state in hidden ]) model . zero_grad () lstm_output , hidden = model . forward ( inputs , hidden ) loss = criterion ( lstm_output , targets . view ( batch_size * seq_len ) . long ()) loss . backward () # POSSIBLE EXPLODING GRADIENT PROBLEM! # LET\"S CLIP JUST IN CASE nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = 5 ) optimizer . step () ################################### ### CHECK ON VALIDATION SET ###### ################################# if tracker % 25 == 0 : val_hidden = model . hidden_state ( batch_size ) val_losses = [] model . eval () for x , y in generate_batches ( val_data , batch_size , seq_len ): # One Hot Encode incoming data x = one_hot_encoder ( x , num_char ) # Convert Numpy Arrays to Tensor inputs = torch . from_numpy ( x ) targets = torch . from_numpy ( y ) # Adjust for GPU if necessary if model . use_gpu : inputs = inputs . cuda () targets = targets . cuda () # Reset Hidden State # If we dont' reset we would backpropagate through # all training history val_hidden = tuple ([ state . data for state in val_hidden ]) lstm_output , val_hidden = model . forward ( inputs , val_hidden ) val_loss = criterion ( lstm_output , targets . view ( batch_size * seq_len ) . long ()) val_losses . append ( val_loss . item ()) # Reset to training model after val for loop model . train () print ( f \"Epoch: { i } Step: { tracker } Val Loss: { val_loss . item () } \" ) Epoch: 0 Step: 25 Val Loss: 3.241183280944824 Epoch: 1 Step: 50 Val Loss: 3.2209343910217285 Epoch: 1 Step: 75 Val Loss: 3.2246036529541016 Epoch: 2 Step: 100 Val Loss: 3.103549003601074 Epoch: 2 Step: 125 Val Loss: 3.0078160762786865 Epoch: 3 Step: 150 Val Loss: 2.8424694538116455 Epoch: 4 Step: 175 Val Loss: 2.7311224937438965 Epoch: 4 Step: 200 Val Loss: 2.6245357990264893 Epoch: 5 Step: 225 Val Loss: 2.530056953430176 Epoch: 5 Step: 250 Val Loss: 2.511744737625122 Epoch: 6 Step: 275 Val Loss: 2.424506187438965 Epoch: 7 Step: 300 Val Loss: 2.3757734298706055 Epoch: 7 Step: 325 Val Loss: 2.3281121253967285 Epoch: 8 Step: 350 Val Loss: 2.287860631942749 Epoch: 8 Step: 375 Val Loss: 2.258666515350342 Epoch: 9 Step: 400 Val Loss: 2.219432830810547 Epoch: 10 Step: 425 Val Loss: 2.1962826251983643 Epoch: 10 Step: 450 Val Loss: 2.1531155109405518 Epoch: 11 Step: 475 Val Loss: 2.12485408782959 Epoch: 11 Step: 500 Val Loss: 2.102055072784424 Epoch: 12 Step: 525 Val Loss: 2.0815775394439697 Epoch: 13 Step: 550 Val Loss: 2.065098524093628 Epoch: 13 Step: 575 Val Loss: 2.045565366744995 Epoch: 14 Step: 600 Val Loss: 2.024740695953369 Epoch: 14 Step: 625 Val Loss: 2.002650737762451 Epoch: 15 Step: 650 Val Loss: 1.9918841123580933 Epoch: 16 Step: 675 Val Loss: 1.973698616027832 Epoch: 16 Step: 700 Val Loss: 1.9563044309616089 Epoch: 17 Step: 725 Val Loss: 1.941154956817627 Epoch: 17 Step: 750 Val Loss: 1.9296284914016724 Epoch: 18 Step: 775 Val Loss: 1.9155606031417847 Epoch: 19 Step: 800 Val Loss: 1.9066412448883057 Epoch: 19 Step: 825 Val Loss: 1.8944147825241089 Epoch: 20 Step: 850 Val Loss: 1.8825374841690063 Epoch: 20 Step: 875 Val Loss: 1.8753138780593872 Epoch: 21 Step: 900 Val Loss: 1.8679431676864624 Epoch: 22 Step: 925 Val Loss: 1.8626611232757568 Epoch: 22 Step: 950 Val Loss: 1.8534228801727295 Epoch: 23 Step: 975 Val Loss: 1.8416558504104614 Epoch: 23 Step: 1000 Val Loss: 1.8408966064453125 Epoch: 24 Step: 1025 Val Loss: 1.832461953163147 Epoch: 24 Step: 1050 Val Loss: 1.8274987936019897 Epoch: 25 Step: 1075 Val Loss: 1.8215422630310059 Epoch: 26 Step: 1100 Val Loss: 1.8141027688980103 Epoch: 26 Step: 1125 Val Loss: 1.8090591430664062 Epoch: 27 Step: 1150 Val Loss: 1.808109998703003 Epoch: 27 Step: 1175 Val Loss: 1.798502802848816 Epoch: 28 Step: 1200 Val Loss: 1.8020660877227783 Epoch: 29 Step: 1225 Val Loss: 1.7935495376586914 Epoch: 29 Step: 1250 Val Loss: 1.7842048406600952 Epoch: 30 Step: 1275 Val Loss: 1.7775088548660278 Epoch: 30 Step: 1300 Val Loss: 1.7796084880828857 Epoch: 31 Step: 1325 Val Loss: 1.778605341911316 Epoch: 32 Step: 1350 Val Loss: 1.778555154800415 Epoch: 32 Step: 1375 Val Loss: 1.7726141214370728 Epoch: 33 Step: 1400 Val Loss: 1.7713408470153809 Epoch: 33 Step: 1425 Val Loss: 1.7647587060928345 Epoch: 34 Step: 1450 Val Loss: 1.7639307975769043 Epoch: 35 Step: 1475 Val Loss: 1.7668451070785522 Epoch: 35 Step: 1500 Val Loss: 1.7553269863128662 Epoch: 36 Step: 1525 Val Loss: 1.7537274360656738 Epoch: 36 Step: 1550 Val Loss: 1.7476931810379028 Epoch: 37 Step: 1575 Val Loss: 1.7471405267715454 Epoch: 38 Step: 1600 Val Loss: 1.748685359954834 Epoch: 38 Step: 1625 Val Loss: 1.7501276731491089 Epoch: 39 Step: 1650 Val Loss: 1.7491378784179688 Epoch: 39 Step: 1675 Val Loss: 1.73957097530365 Epoch: 40 Step: 1700 Val Loss: 1.7412303686141968 Epoch: 41 Step: 1725 Val Loss: 1.7421422004699707 Epoch: 41 Step: 1750 Val Loss: 1.7420353889465332 Epoch: 42 Step: 1775 Val Loss: 1.732686161994934 Epoch: 42 Step: 1800 Val Loss: 1.7336872816085815 Epoch: 43 Step: 1825 Val Loss: 1.7360546588897705 Epoch: 44 Step: 1850 Val Loss: 1.7357029914855957 Epoch: 44 Step: 1875 Val Loss: 1.736457109451294 Epoch: 45 Step: 1900 Val Loss: 1.7330776453018188 Epoch: 45 Step: 1925 Val Loss: 1.7337615489959717 Epoch: 46 Step: 1950 Val Loss: 1.738358736038208 Epoch: 47 Step: 1975 Val Loss: 1.7346129417419434 Epoch: 47 Step: 2000 Val Loss: 1.743545413017273 Epoch: 48 Step: 2025 Val Loss: 1.7326579093933105 Epoch: 48 Step: 2050 Val Loss: 1.7226899862289429 Epoch: 49 Step: 2075 Val Loss: 1.7329885959625244 Epoch: 49 Step: 2100 Val Loss: 1.7302632331848145 \u2014\u2014- \u00b6 Saving the Model \u00b6 https://pytorch.org/tutorials/beginner/saving_loading_models.html # Be careful to overwrite our original name file! model_name = 'example.net' torch . save ( model . state_dict (), model_name ) Load Model \u00b6 # MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING! model = CharModel ( all_chars = all_characters , num_hidden = 512 , num_layers = 3 , drop_prob = 0.5 , use_gpu = True , ) model . load_state_dict ( torch . load ( model_name )) model . eval () CharModel( (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5) (dropout): Dropout(p=0.5) (fc_linear): Linear(in_features=512, out_features=84, bias=True) ) Generating Predictions \u00b6 def predict_next_char ( model , char , hidden = None , k = 1 ): # Encode raw letters with model encoded_text = model . encoder [ char ] # set as numpy array for one hot encoding # NOTE THE [[ ]] dimensions!! encoded_text = np . array ([[ encoded_text ]]) # One hot encoding encoded_text = one_hot_encoder ( encoded_text , len ( model . all_chars )) # Convert to Tensor inputs = torch . from_numpy ( encoded_text ) # Check for CPU if ( model . use_gpu ): inputs = inputs . cuda () # Grab hidden states hidden = tuple ([ state . data for state in hidden ]) # Run model and get predicted output lstm_out , hidden = model ( inputs , hidden ) # Convert lstm_out to probabilities probs = F . softmax ( lstm_out , dim = 1 ) . data if ( model . use_gpu ): # move back to CPU to use with numpy probs = probs . cpu () # k determines how many characters to consider # for our probability choice. # https://pytorch.org/docs/stable/torch.html#torch.topk # Return k largest probabilities in tensor probs , index_positions = probs . topk ( k ) index_positions = index_positions . numpy () . squeeze () # Create array of probabilities probs = probs . numpy () . flatten () # Convert to probabilities per index probs = probs / probs . sum () # randomly choose a character based on probabilities char = np . random . choice ( index_positions , p = probs ) # return the encoded value of the predicted char and the hidden state return model . decoder [ char ], hidden def generate_text ( model , size , seed = 'The' , k = 1 ): # CHECK FOR GPU if ( model . use_gpu ): model . cuda () else : model . cpu () # Evaluation mode model . eval () # begin output from initial seed output_chars = [ c for c in seed ] # intiate hidden state hidden = model . hidden_state ( 1 ) # predict the next character for every character in seed for char in seed : char , hidden = predict_next_char ( model , char , hidden , k = k ) # add initial characters to output output_chars . append ( char ) # Now generate for size requested for i in range ( size ): # predict based off very last letter in output_chars char , hidden = predict_next_char ( model , output_chars [ - 1 ], hidden , k = k ) # add predicted character output_chars . append ( char ) # return string of predicted text return '' . join ( output_chars ) print ( generate_text ( model , 1000 , seed = 'The ' , k = 3 )) The will true and breathed to me. If thou wert better to the stare and send thee, Which hath any trives and sound and stretged, That have the better send of the constance, That then that thou shaltst but that have seem surpet And we had been the self-fight and had their strange, With his sward shall strave a servant state. Where this't she is that to the wind of held That have this serve that she he with the child Which they were beauty of their command strowes And truth and strength to the serves and song. If thou say'st he that hath seen this should still To she with his both shall see him. The world was a solder thou to heaven with me, And should this can stay that I heave make Which his charge in her shames, and to his state. That have tho stol'd of this starts to have, And we and to the cheeks that to the stol'd To serve the courtier time of that sense is. In the summer that that shall not, That he will s Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"00 RNN for Text Generation "},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#rnn-for-text-generation","text":"","title":"RNN for Text Generation"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#generating-text-encoded-variables","text":"We saw how to generate continuous values, now let\u2019s see how to generalize this to generate categorical sequences (such as words or letters).","title":"Generating Text (encoded variables)"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#imports","text":"import torch from torch import nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt % matplotlib inline","title":"Imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#get-text-data","text":"with open ( '../Data/shakespeare.txt' , 'r' , encoding = 'utf8' ) as f : text = f . read () text [: 1000 ] \"\\n 1\\n From fairest creatures we desire increase,\\n That thereby beauty's rose might never die,\\n But as the riper should by time decease,\\n His tender heir might bear his memory:\\n But thou contracted to thine own bright eyes,\\n Feed'st thy light's flame with self-substantial fuel,\\n Making a famine where abundance lies,\\n Thy self thy foe, to thy sweet self too cruel:\\n Thou that art now the world's fresh ornament,\\n And only herald to the gaudy spring,\\n Within thine own bud buriest thy content,\\n And tender churl mak'st waste in niggarding:\\n Pity the world, or else this glutton be,\\n To eat the world's due, by the grave and thee.\\n\\n\\n 2\\n When forty winters shall besiege thy brow,\\n And dig deep trenches in thy beauty's field,\\n Thy youth's proud livery so gazed on now,\\n Will be a tattered weed of small worth held: \\n Then being asked, where all thy beauty lies,\\n Where all the treasure of thy lusty days;\\n To say within thine own deep su\" print ( text [: 1000 ]) 1 From fairest creatures we desire increase, That thereby beauty's rose might never die, But as the riper should by time decease, His tender heir might bear his memory: But thou contracted to thine own bright eyes, Feed'st thy light's flame with self-substantial fuel, Making a famine where abundance lies, Thy self thy foe, to thy sweet self too cruel: Thou that art now the world's fresh ornament, And only herald to the gaudy spring, Within thine own bud buriest thy content, And tender churl mak'st waste in niggarding: Pity the world, or else this glutton be, To eat the world's due, by the grave and thee. 2 When forty winters shall besiege thy brow, And dig deep trenches in thy beauty's field, Thy youth's proud livery so gazed on now, Will be a tattered weed of small worth held: Then being asked, where all thy beauty lies, Where all the treasure of thy lusty days; To say within thine own deep su len ( text ) 5445609","title":"Get Text Data"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#encode-entire-text","text":"all_characters = set ( text ) # all_characters decoder = dict ( enumerate ( all_characters )) # decoder # decoder.items() encoder = { char : ind for ind , char in decoder . items ()} # encoder encoded_text = np . array ([ encoder [ char ] for char in text ]) encoded_text [: 500 ] array([51, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 41, 51, 26, 26, 31, 29, 39, 17, 26, 73, 14, 60, 29, 46, 78, 21, 26, 35, 29, 46, 14, 21, 6, 29, 46, 78, 26, 2, 46, 26, 65, 46, 78, 60, 29, 46, 26, 60, 55, 35, 29, 46, 14, 78, 46, 63, 51, 26, 26, 62, 7, 14, 21, 26, 21, 7, 46, 29, 46, 74, 38, 26, 74, 46, 14, 6, 21, 38, 77, 78, 26, 29, 39, 78, 46, 26, 17, 60, 82, 7, 21, 26, 55, 46, 27, 46, 29, 26, 65, 60, 46, 63, 51, 26, 26, 67, 6, 21, 26, 14, 78, 26, 21, 7, 46, 26, 29, 60, 22, 46, 29, 26, 78, 7, 39, 6, 43, 65, 26, 74, 38, 26, 21, 60, 17, 46, 26, 65, 46, 35, 46, 14, 78, 46, 63, 51, 26, 26, 36, 60, 78, 26, 21, 46, 55, 65, 46, 29, 26, 7, 46, 60, 29, 26, 17, 60, 82, 7, 21, 26, 74, 46, 14, 29, 26, 7, 60, 78, 26, 17, 46, 17, 39, 29, 38, 10, 51, 26, 26, 67, 6, 21, 26, 21, 7, 39, 6, 26, 35, 39, 55, 21, 29, 14, 35, 21, 46, 65, 26, 21, 39, 26, 21, 7, 60, 55, 46, 26, 39, 2, 55, 26, 74, 29, 60, 82, 7, 21, 26, 46, 38, 46, 78, 63, 51, 26, 26, 31, 46, 46, 65, 77, 78, 21, 26, 21, 7, 38, 26, 43, 60, 82, 7, 21, 77, 78, 26, 73, 43, 14, 17, 46, 26, 2, 60, 21, 7, 26, 78, 46, 43, 73, 49, 78, 6, 74, 78, 21, 14, 55, 21, 60, 14, 43, 26, 73, 6, 46, 43, 63, 51, 26, 26, 3, 14, 28, 60, 55, 82, 26, 14, 26, 73, 14, 17, 60, 55, 46, 26, 2, 7, 46, 29, 46, 26, 14, 74, 6, 55, 65, 14, 55, 35, 46, 26, 43, 60, 46, 78, 63, 51, 26, 26, 62, 7, 38, 26, 78, 46, 43, 73, 26, 21, 7, 38, 26, 73, 39, 46, 63, 26, 21, 39, 26, 21, 7, 38, 26, 78, 2, 46, 46, 21, 26, 78, 46, 43, 73, 26, 21, 39, 39, 26, 35, 29, 6, 46, 43, 10, 51, 26, 26, 62, 7, 39, 6, 26, 21, 7, 14, 21, 26, 14, 29, 21, 26, 55, 39, 2, 26, 21, 7, 46, 26, 2, 39, 29, 43, 65, 77, 78, 26, 73, 29, 46, 78, 7, 26, 39, 29, 55, 14, 17, 46, 55, 21, 63, 51, 26, 26, 56, 55, 65, 26, 39, 55, 43, 38, 26, 7, 46, 29, 14, 43, 65, 26, 21, 39, 26, 21, 7, 46, 26, 82, 14, 6, 65, 38, 26, 78, 22, 29, 60, 55, 82, 63, 51, 26, 26, 40, 60, 21, 7, 60, 55, 26, 21, 7, 60, 55, 46, 26, 39, 2, 55, 26, 74, 6])","title":"Encode Entire Text"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#one-hot-encoding","text":"As previously discussed, we need to one-hot encode our data inorder for it to work with the network structure. Make sure to review numpy if any of these operations confuse you! def one_hot_encoder ( encoded_text , num_uni_chars ): ''' encoded_text : batch of encoded text num_uni_chars = number of unique characters (len(set(text))) ''' # METHOD FROM: # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay # Create a placeholder for zeros. one_hot = np . zeros (( encoded_text . size , num_uni_chars )) # Convert data type for later use with pytorch (errors if we dont!) one_hot = one_hot . astype ( np . float32 ) # Using fancy indexing fill in the 1s at the correct index locations one_hot [ np . arange ( one_hot . shape [ 0 ]), encoded_text . flatten ()] = 1.0 # Reshape it so it matches the batch sahe one_hot = one_hot . reshape (( * encoded_text . shape , num_uni_chars )) return one_hot one_hot_encoder ( np . array ([ 1 , 2 , 0 ]), 3 ) array([[0., 1., 0.], [0., 0., 1.], [1., 0., 0.]], dtype=float32)","title":"One Hot Encoding"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#_1","text":"","title":"\u2014\u2014\u2014\u2014\u2013"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#creating-training-batches","text":"We need to create a function that will generate batches of characters along with the next character in the sequence as a label.","title":"Creating Training Batches"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#_2","text":"example_text = np . arange ( 10 ) example_text array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) # If we wanted 5 batches example_text . reshape (( 5 , - 1 )) array([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) def generate_batches ( encoded_text , samp_per_batch = 10 , seq_len = 50 ): ''' Generate (using yield) batches for training. X: Encoded Text of length seq_len Y: Encoded Text shifted by one Example: X: [[1 2 3]] Y: [[ 2 3 4]] encoded_text : Complete Encoded Text to make batches from batch_size : Number of samples per batch seq_len : Length of character sequence ''' # Total number of characters per batch # Example: If samp_per_batch is 2 and seq_len is 50, then 100 # characters come out per batch. char_per_batch = samp_per_batch * seq_len # Number of batches available to make # Use int() to roun to nearest integer num_batches_avail = int ( len ( encoded_text ) / char_per_batch ) # Cut off end of encoded_text that # won't fit evenly into a batch encoded_text = encoded_text [: num_batches_avail * char_per_batch ] # Reshape text into rows the size of a batch encoded_text = encoded_text . reshape (( samp_per_batch , - 1 )) # Go through each row in array. for n in range ( 0 , encoded_text . shape [ 1 ], seq_len ): # Grab feature characters x = encoded_text [:, n : n + seq_len ] # y is the target shifted over by 1 y = np . zeros_like ( x ) # try : y [:, : - 1 ] = x [:, 1 :] y [:, - 1 ] = encoded_text [:, n + seq_len ] # FOR POTENTIAL INDEXING ERROR AT THE END except : y [:, : - 1 ] = x [:, 1 :] y [:, - 1 ] = encoded_text [:, 0 ] yield x , y","title":"\u2014\u2014\u2014\u2014\u2014\u2013"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#example-of-generating-a-batch","text":"sample_text = encoded_text [: 20 ] sample_text array([51, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]) batch_generator = generate_batches ( sample_text , samp_per_batch = 2 , seq_len = 5 ) # Grab first batch x , y = next ( batch_generator ) x array([[51, 26, 26, 26, 26], [26, 26, 26, 26, 26]]) y array([[26, 26, 26, 26, 26], [26, 26, 26, 26, 26]])","title":"Example of generating a batch"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#gpu-check","text":"Remember this will take a lot longer on CPU! torch . cuda . is_available () True","title":"GPU Check"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#creating-the-lstm-model","text":"Note! We will have options for GPU users and CPU users. CPU will take MUCH LONGER to train and you may encounter RAM issues depending on your hardware. If that is the case, consider using cloud services like AWS, GCP, or Azure. Note, these may cost you money to use! class CharModel ( nn . Module ): def __init__ ( self , all_chars , num_hidden = 256 , num_layers = 4 , drop_prob = 0.5 , use_gpu = False ): # SET UP ATTRIBUTES super () . __init__ () self . drop_prob = drop_prob self . num_layers = num_layers self . num_hidden = num_hidden self . use_gpu = use_gpu #CHARACTER SET, ENCODER, and DECODER self . all_chars = all_chars self . decoder = dict ( enumerate ( all_chars )) self . encoder = { char : ind for ind , char in decoder . items ()} self . lstm = nn . LSTM ( len ( self . all_chars ), num_hidden , num_layers , dropout = drop_prob , batch_first = True ) self . dropout = nn . Dropout ( drop_prob ) self . fc_linear = nn . Linear ( num_hidden , len ( self . all_chars )) def forward ( self , x , hidden ): lstm_output , hidden = self . lstm ( x , hidden ) drop_output = self . dropout ( lstm_output ) drop_output = drop_output . contiguous () . view ( - 1 , self . num_hidden ) final_out = self . fc_linear ( drop_output ) return final_out , hidden def hidden_state ( self , batch_size ): ''' Used as separate method to account for both GPU and CPU users. ''' if self . use_gpu : hidden = ( torch . zeros ( self . num_layers , batch_size , self . num_hidden ) . cuda (), torch . zeros ( self . num_layers , batch_size , self . num_hidden ) . cuda ()) else : hidden = ( torch . zeros ( self . num_layers , batch_size , self . num_hidden ), torch . zeros ( self . num_layers , batch_size , self . num_hidden )) return hidden","title":"Creating the LSTM Model"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#instance-of-the-model","text":"model = CharModel ( all_chars = all_characters , num_hidden = 512 , num_layers = 3 , drop_prob = 0.5 , use_gpu = True , ) total_param = [] for p in model . parameters (): total_param . append ( int ( p . numel ())) Try to make the total_parameters be roughly the same magnitude as the number of characters in the text. sum ( total_param ) 5470292 len ( encoded_text ) 5445609","title":"Instance of the Model"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#optimizer-and-loss","text":"optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) criterion = nn . CrossEntropyLoss ()","title":"Optimizer and Loss"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#training-data-and-validation-data","text":"# percentage of data to be used for training train_percent = 0.1 len ( encoded_text ) 5445609 int ( len ( encoded_text ) * ( train_percent )) 544560 train_ind = int ( len ( encoded_text ) * ( train_percent )) train_data = encoded_text [: train_ind ] val_data = encoded_text [ train_ind :]","title":"Training Data and Validation Data"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#training-the-network","text":"","title":"Training the Network"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#variables","text":"Feel free to play around with these values! ## VARIABLES # Epochs to train for epochs = 50 # batch size batch_size = 128 # Length of sequence seq_len = 100 # for printing report purposes # always start at 0 tracker = 0 # number of characters in text num_char = max ( encoded_text ) + 1 # Set model to train model . train () # Check to see if using GPU if model . use_gpu : model . cuda () for i in range ( epochs ): hidden = model . hidden_state ( batch_size ) for x , y in generate_batches ( train_data , batch_size , seq_len ): tracker += 1 # One Hot Encode incoming data x = one_hot_encoder ( x , num_char ) # Convert Numpy Arrays to Tensor inputs = torch . from_numpy ( x ) targets = torch . from_numpy ( y ) # Adjust for GPU if necessary if model . use_gpu : inputs = inputs . cuda () targets = targets . cuda () # Reset Hidden State # If we dont' reset we would backpropagate through all training history hidden = tuple ([ state . data for state in hidden ]) model . zero_grad () lstm_output , hidden = model . forward ( inputs , hidden ) loss = criterion ( lstm_output , targets . view ( batch_size * seq_len ) . long ()) loss . backward () # POSSIBLE EXPLODING GRADIENT PROBLEM! # LET\"S CLIP JUST IN CASE nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = 5 ) optimizer . step () ################################### ### CHECK ON VALIDATION SET ###### ################################# if tracker % 25 == 0 : val_hidden = model . hidden_state ( batch_size ) val_losses = [] model . eval () for x , y in generate_batches ( val_data , batch_size , seq_len ): # One Hot Encode incoming data x = one_hot_encoder ( x , num_char ) # Convert Numpy Arrays to Tensor inputs = torch . from_numpy ( x ) targets = torch . from_numpy ( y ) # Adjust for GPU if necessary if model . use_gpu : inputs = inputs . cuda () targets = targets . cuda () # Reset Hidden State # If we dont' reset we would backpropagate through # all training history val_hidden = tuple ([ state . data for state in val_hidden ]) lstm_output , val_hidden = model . forward ( inputs , val_hidden ) val_loss = criterion ( lstm_output , targets . view ( batch_size * seq_len ) . long ()) val_losses . append ( val_loss . item ()) # Reset to training model after val for loop model . train () print ( f \"Epoch: { i } Step: { tracker } Val Loss: { val_loss . item () } \" ) Epoch: 0 Step: 25 Val Loss: 3.241183280944824 Epoch: 1 Step: 50 Val Loss: 3.2209343910217285 Epoch: 1 Step: 75 Val Loss: 3.2246036529541016 Epoch: 2 Step: 100 Val Loss: 3.103549003601074 Epoch: 2 Step: 125 Val Loss: 3.0078160762786865 Epoch: 3 Step: 150 Val Loss: 2.8424694538116455 Epoch: 4 Step: 175 Val Loss: 2.7311224937438965 Epoch: 4 Step: 200 Val Loss: 2.6245357990264893 Epoch: 5 Step: 225 Val Loss: 2.530056953430176 Epoch: 5 Step: 250 Val Loss: 2.511744737625122 Epoch: 6 Step: 275 Val Loss: 2.424506187438965 Epoch: 7 Step: 300 Val Loss: 2.3757734298706055 Epoch: 7 Step: 325 Val Loss: 2.3281121253967285 Epoch: 8 Step: 350 Val Loss: 2.287860631942749 Epoch: 8 Step: 375 Val Loss: 2.258666515350342 Epoch: 9 Step: 400 Val Loss: 2.219432830810547 Epoch: 10 Step: 425 Val Loss: 2.1962826251983643 Epoch: 10 Step: 450 Val Loss: 2.1531155109405518 Epoch: 11 Step: 475 Val Loss: 2.12485408782959 Epoch: 11 Step: 500 Val Loss: 2.102055072784424 Epoch: 12 Step: 525 Val Loss: 2.0815775394439697 Epoch: 13 Step: 550 Val Loss: 2.065098524093628 Epoch: 13 Step: 575 Val Loss: 2.045565366744995 Epoch: 14 Step: 600 Val Loss: 2.024740695953369 Epoch: 14 Step: 625 Val Loss: 2.002650737762451 Epoch: 15 Step: 650 Val Loss: 1.9918841123580933 Epoch: 16 Step: 675 Val Loss: 1.973698616027832 Epoch: 16 Step: 700 Val Loss: 1.9563044309616089 Epoch: 17 Step: 725 Val Loss: 1.941154956817627 Epoch: 17 Step: 750 Val Loss: 1.9296284914016724 Epoch: 18 Step: 775 Val Loss: 1.9155606031417847 Epoch: 19 Step: 800 Val Loss: 1.9066412448883057 Epoch: 19 Step: 825 Val Loss: 1.8944147825241089 Epoch: 20 Step: 850 Val Loss: 1.8825374841690063 Epoch: 20 Step: 875 Val Loss: 1.8753138780593872 Epoch: 21 Step: 900 Val Loss: 1.8679431676864624 Epoch: 22 Step: 925 Val Loss: 1.8626611232757568 Epoch: 22 Step: 950 Val Loss: 1.8534228801727295 Epoch: 23 Step: 975 Val Loss: 1.8416558504104614 Epoch: 23 Step: 1000 Val Loss: 1.8408966064453125 Epoch: 24 Step: 1025 Val Loss: 1.832461953163147 Epoch: 24 Step: 1050 Val Loss: 1.8274987936019897 Epoch: 25 Step: 1075 Val Loss: 1.8215422630310059 Epoch: 26 Step: 1100 Val Loss: 1.8141027688980103 Epoch: 26 Step: 1125 Val Loss: 1.8090591430664062 Epoch: 27 Step: 1150 Val Loss: 1.808109998703003 Epoch: 27 Step: 1175 Val Loss: 1.798502802848816 Epoch: 28 Step: 1200 Val Loss: 1.8020660877227783 Epoch: 29 Step: 1225 Val Loss: 1.7935495376586914 Epoch: 29 Step: 1250 Val Loss: 1.7842048406600952 Epoch: 30 Step: 1275 Val Loss: 1.7775088548660278 Epoch: 30 Step: 1300 Val Loss: 1.7796084880828857 Epoch: 31 Step: 1325 Val Loss: 1.778605341911316 Epoch: 32 Step: 1350 Val Loss: 1.778555154800415 Epoch: 32 Step: 1375 Val Loss: 1.7726141214370728 Epoch: 33 Step: 1400 Val Loss: 1.7713408470153809 Epoch: 33 Step: 1425 Val Loss: 1.7647587060928345 Epoch: 34 Step: 1450 Val Loss: 1.7639307975769043 Epoch: 35 Step: 1475 Val Loss: 1.7668451070785522 Epoch: 35 Step: 1500 Val Loss: 1.7553269863128662 Epoch: 36 Step: 1525 Val Loss: 1.7537274360656738 Epoch: 36 Step: 1550 Val Loss: 1.7476931810379028 Epoch: 37 Step: 1575 Val Loss: 1.7471405267715454 Epoch: 38 Step: 1600 Val Loss: 1.748685359954834 Epoch: 38 Step: 1625 Val Loss: 1.7501276731491089 Epoch: 39 Step: 1650 Val Loss: 1.7491378784179688 Epoch: 39 Step: 1675 Val Loss: 1.73957097530365 Epoch: 40 Step: 1700 Val Loss: 1.7412303686141968 Epoch: 41 Step: 1725 Val Loss: 1.7421422004699707 Epoch: 41 Step: 1750 Val Loss: 1.7420353889465332 Epoch: 42 Step: 1775 Val Loss: 1.732686161994934 Epoch: 42 Step: 1800 Val Loss: 1.7336872816085815 Epoch: 43 Step: 1825 Val Loss: 1.7360546588897705 Epoch: 44 Step: 1850 Val Loss: 1.7357029914855957 Epoch: 44 Step: 1875 Val Loss: 1.736457109451294 Epoch: 45 Step: 1900 Val Loss: 1.7330776453018188 Epoch: 45 Step: 1925 Val Loss: 1.7337615489959717 Epoch: 46 Step: 1950 Val Loss: 1.738358736038208 Epoch: 47 Step: 1975 Val Loss: 1.7346129417419434 Epoch: 47 Step: 2000 Val Loss: 1.743545413017273 Epoch: 48 Step: 2025 Val Loss: 1.7326579093933105 Epoch: 48 Step: 2050 Val Loss: 1.7226899862289429 Epoch: 49 Step: 2075 Val Loss: 1.7329885959625244 Epoch: 49 Step: 2100 Val Loss: 1.7302632331848145","title":"Variables"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#-","text":"","title":"\u2014\u2014-"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#saving-the-model","text":"https://pytorch.org/tutorials/beginner/saving_loading_models.html # Be careful to overwrite our original name file! model_name = 'example.net' torch . save ( model . state_dict (), model_name )","title":"Saving the Model"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#load-model","text":"# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING! model = CharModel ( all_chars = all_characters , num_hidden = 512 , num_layers = 3 , drop_prob = 0.5 , use_gpu = True , ) model . load_state_dict ( torch . load ( model_name )) model . eval () CharModel( (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5) (dropout): Dropout(p=0.5) (fc_linear): Linear(in_features=512, out_features=84, bias=True) )","title":"Load Model"},{"location":"bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/#generating-predictions","text":"def predict_next_char ( model , char , hidden = None , k = 1 ): # Encode raw letters with model encoded_text = model . encoder [ char ] # set as numpy array for one hot encoding # NOTE THE [[ ]] dimensions!! encoded_text = np . array ([[ encoded_text ]]) # One hot encoding encoded_text = one_hot_encoder ( encoded_text , len ( model . all_chars )) # Convert to Tensor inputs = torch . from_numpy ( encoded_text ) # Check for CPU if ( model . use_gpu ): inputs = inputs . cuda () # Grab hidden states hidden = tuple ([ state . data for state in hidden ]) # Run model and get predicted output lstm_out , hidden = model ( inputs , hidden ) # Convert lstm_out to probabilities probs = F . softmax ( lstm_out , dim = 1 ) . data if ( model . use_gpu ): # move back to CPU to use with numpy probs = probs . cpu () # k determines how many characters to consider # for our probability choice. # https://pytorch.org/docs/stable/torch.html#torch.topk # Return k largest probabilities in tensor probs , index_positions = probs . topk ( k ) index_positions = index_positions . numpy () . squeeze () # Create array of probabilities probs = probs . numpy () . flatten () # Convert to probabilities per index probs = probs / probs . sum () # randomly choose a character based on probabilities char = np . random . choice ( index_positions , p = probs ) # return the encoded value of the predicted char and the hidden state return model . decoder [ char ], hidden def generate_text ( model , size , seed = 'The' , k = 1 ): # CHECK FOR GPU if ( model . use_gpu ): model . cuda () else : model . cpu () # Evaluation mode model . eval () # begin output from initial seed output_chars = [ c for c in seed ] # intiate hidden state hidden = model . hidden_state ( 1 ) # predict the next character for every character in seed for char in seed : char , hidden = predict_next_char ( model , char , hidden , k = k ) # add initial characters to output output_chars . append ( char ) # Now generate for size requested for i in range ( size ): # predict based off very last letter in output_chars char , hidden = predict_next_char ( model , output_chars [ - 1 ], hidden , k = k ) # add predicted character output_chars . append ( char ) # return string of predicted text return '' . join ( output_chars ) print ( generate_text ( model , 1000 , seed = 'The ' , k = 3 )) The will true and breathed to me. If thou wert better to the stare and send thee, Which hath any trives and sound and stretged, That have the better send of the constance, That then that thou shaltst but that have seem surpet And we had been the self-fight and had their strange, With his sward shall strave a servant state. Where this't she is that to the wind of held That have this serve that she he with the child Which they were beauty of their command strowes And truth and strength to the serves and song. If thou say'st he that hath seen this should still To she with his both shall see him. The world was a solder thou to heaven with me, And should this can stay that I heave make Which his charge in her shames, and to his state. That have tho stol'd of this starts to have, And we and to the cheeks that to the stol'd To serve the courtier time of that sense is. In the summer that that shall not, That he will s Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Generating Predictions"},{"location":"competitiveprogramming/","text":"Competitive Programming \u00b6 Here you will find Competitive coding problems from LeetCode, CodeForces, Exercisms and many more Checkout the problems and thier solution with concise explainations: \u00b6 I will add the explaination soon in the blogs section: For now I am just mentioning my repos for the solutions and problems Solutions in Python \u00b6 50 Leetcode Problems Challenge Data Structure Algorithms Solutions in Java \u00b6 Algorithmic Problems In Java Most Common Competitive Problems Competitive Programming Java Java Practices How To Program Practice Your contribution is wellcome \u00b6","title":"Competitive Programming"},{"location":"competitiveprogramming/#competitive-programming","text":"Here you will find Competitive coding problems from LeetCode, CodeForces, Exercisms and many more","title":"Competitive Programming"},{"location":"competitiveprogramming/#checkout-the-problems-and-thier-solution-with-concise-explainations","text":"I will add the explaination soon in the blogs section: For now I am just mentioning my repos for the solutions and problems","title":"Checkout the problems and thier solution with concise explainations:"},{"location":"competitiveprogramming/#solutions-in-python","text":"50 Leetcode Problems Challenge Data Structure Algorithms","title":"Solutions in Python"},{"location":"competitiveprogramming/#solutions-in-java","text":"Algorithmic Problems In Java Most Common Competitive Problems Competitive Programming Java Java Practices How To Program Practice","title":"Solutions in Java"},{"location":"competitiveprogramming/#your-contribution-is-wellcome","text":"","title":"Your contribution is wellcome"},{"location":"corecs/","text":"Core Computer Science Courses \u00b6 Here you will find notes related to core cs courses I m Planning to add notes for the following courses: \u00b6 Foundamentals of Programming with Python Introduction to Computer Science Data Structure & Algorithms (in Python & Java) Object Oriented Programming (Using Java & Python) R Programming MATLAB I will add blogs about each course and the importance for your future career as well \u00b6 Check out the blogs frequectly !","title":"Core Computer Science"},{"location":"corecs/#core-computer-science-courses","text":"Here you will find notes related to core cs courses","title":"Core Computer Science Courses"},{"location":"corecs/#i-m-planning-to-add-notes-for-the-following-courses","text":"Foundamentals of Programming with Python Introduction to Computer Science Data Structure & Algorithms (in Python & Java) Object Oriented Programming (Using Java & Python) R Programming MATLAB","title":"I m Planning to add notes for the following courses:"},{"location":"corecs/#i-will-add-blogs-about-each-course-and-the-importance-for-your-future-career-as-well","text":"Check out the blogs frequectly !","title":"I will add blogs about each course and the importance for your future career as well"},{"location":"myresearch/","text":"Publications \u00b6 Here you will find my research publications, conference papers and much more Journal Publications \u00b6 \u201cAn Effective Classification Methodology for Brain MRI Classification Based on Statistical Features, DWT, and Blended ANN\u201d. IEEE Access Dec 2021 Doi \u201cA Smart Approximation Algorithm for Minimum Vertex Cover Problem Based on Min-to-Min Strategy\u201d. International Journal of Advance Computer Science and Applications (IJACSA) Vol. 11 No. 12, 2020 Doi Conference proceedings: \u00b6 15 th International Conference on Machine Vision A Novel and Efficient Methodology Based on Blended Machine Learning Techniques for Brain MRI Classification To be held on 18-22 Nov 2022, in Rome Italy","title":"My Research"},{"location":"myresearch/#publications","text":"Here you will find my research publications, conference papers and much more","title":"Publications"},{"location":"myresearch/#journal-publications","text":"\u201cAn Effective Classification Methodology for Brain MRI Classification Based on Statistical Features, DWT, and Blended ANN\u201d. IEEE Access Dec 2021 Doi \u201cA Smart Approximation Algorithm for Minimum Vertex Cover Problem Based on Min-to-Min Strategy\u201d. International Journal of Advance Computer Science and Applications (IJACSA) Vol. 11 No. 12, 2020 Doi","title":"Journal Publications"},{"location":"myresearch/#conference-proceedings","text":"15 th International Conference on Machine Vision A Novel and Efficient Methodology Based on Blended Machine Learning Techniques for Brain MRI Classification To be held on 18-22 Nov 2022, in Rome Italy","title":"Conference proceedings:"}]}