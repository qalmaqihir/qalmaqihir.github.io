{"config":{"indexing":"full","lang":["en","ru"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]"},"docs":[{"location":"","text":"Welcome to cs-notes \u00b6 This site holds my notes for different computer science courses. In the start I wanted to dedicate this to only Machine Learning part, but then I extended it to all Computer Science. Help us spread this content: \u00b6 share, like, comment, save & watch LinkedIn Twitter Github Instagram Telegram Youtube The courses and notes are as follow: \u00b6 Books Notes \u00b6 contains notes for some well known books Bootcamps Notes \u00b6 contains notes for some of the most popular bootcamps for machine learning, data structures and algorithms and many other topics in computer science Competitive Programming \u00b6 Here you will see problem solving and solutions with explaination to coding challenges from Leetcode, code forces, exercism and many other Core Computer Science \u00b6 Here you will find topics related to Computer Science core subjects, programming languages, and much more My Research \u00b6 My Research publication and conference papers for anyone interested in doing research in the fields of Machine Learning, Combinotorial Optimization Problem Blog \u00b6 section will have severl tutorials for machine learning, deep learning, AI, Data Science and any other topic which the mind. Your Contributions are highly valued \u00b6 Fort this repo and add your favorite notes Send me a pull request I will check the worth of your notes and hopefully Merge your pull requests to make the World a better Place :) Your suggestions are also highly needed and encouraged \u00b6 Write on any of the following socials Stay Connected LinkedIn Twitter Github Instagram Telegram Youtube","title":"Welcome to cs-notes :notebook:"},{"location":"#welcome-to-cs-notes","text":"This site holds my notes for different computer science courses. In the start I wanted to dedicate this to only Machine Learning part, but then I extended it to all Computer Science.","title":"Welcome to cs-notes"},{"location":"#help-us-spread-this-content","text":"share, like, comment, save & watch LinkedIn Twitter Github Instagram Telegram Youtube","title":"Help us spread this content:"},{"location":"#the-courses-and-notes-are-as-follow","text":"","title":"The courses and notes are as follow:"},{"location":"#books-notes","text":"contains notes for some well known books","title":"Books Notes"},{"location":"#bootcamps-notes","text":"contains notes for some of the most popular bootcamps for machine learning, data structures and algorithms and many other topics in computer science","title":"Bootcamps Notes"},{"location":"#competitive-programming","text":"Here you will see problem solving and solutions with explaination to coding challenges from Leetcode, code forces, exercism and many other","title":"Competitive Programming"},{"location":"#core-computer-science","text":"Here you will find topics related to Computer Science core subjects, programming languages, and much more","title":"Core Computer Science"},{"location":"#my-research","text":"My Research publication and conference papers for anyone interested in doing research in the fields of Machine Learning, Combinotorial Optimization Problem","title":"My Research"},{"location":"#blog","text":"section will have severl tutorials for machine learning, deep learning, AI, Data Science and any other topic which the mind.","title":"Blog"},{"location":"#your-contributions-are-highly-valued","text":"Fort this repo and add your favorite notes Send me a pull request I will check the worth of your notes and hopefully Merge your pull requests to make the World a better Place :)","title":"Your Contributions are highly valued"},{"location":"#your-suggestions-are-also-highly-needed-and-encouraged","text":"Write on any of the following socials Stay Connected LinkedIn Twitter Github Instagram Telegram Youtube","title":"Your suggestions are also highly needed and encouraged"},{"location":"about/","text":"About me: \u00b6 \"The desire to know is natural to good man\" Leonardo da Vinci My name is Jawad Haider; a computer science graduate (2022); researcher; a certified tensorflow developer; a human being :) Bio ! \u00b6 Having an ethereal self I love to inspire others. while at the same time I learn a lot of new things from them. By my iota of experience from teaching, volunteering, and leading camps I had a judicious ego which is more flexible to consider all the aspects of a team task. I have the skill to make people feel their knowledge and time are worthy. I want them to love, live and laugh and spread the jubilance where ever they go.\" Throw me to the wolves and I will return leading the pack.\" Hobbies \u00b6 I play chess , run marathons , do nature photography , listen to podcasts by Lex Fridman , Read books - poetry, novel apart from cs My aims \u00b6 Actions without aims are useless as glass without water. For work details: \u00b6 LinkedIn For research details: \u00b6 Google Scholar","title":"About me: :bear:"},{"location":"about/#about-me","text":"\"The desire to know is natural to good man\" Leonardo da Vinci My name is Jawad Haider; a computer science graduate (2022); researcher; a certified tensorflow developer; a human being :)","title":"About me:"},{"location":"about/#bio","text":"Having an ethereal self I love to inspire others. while at the same time I learn a lot of new things from them. By my iota of experience from teaching, volunteering, and leading camps I had a judicious ego which is more flexible to consider all the aspects of a team task. I have the skill to make people feel their knowledge and time are worthy. I want them to love, live and laugh and spread the jubilance where ever they go.\" Throw me to the wolves and I will return leading the pack.\"","title":"Bio !"},{"location":"about/#hobbies","text":"I play chess , run marathons , do nature photography , listen to podcasts by Lex Fridman , Read books - poetry, novel apart from cs","title":"Hobbies"},{"location":"about/#my-aims","text":"Actions without aims are useless as glass without water.","title":"My aims"},{"location":"about/#for-work-details","text":"LinkedIn","title":"For work details:"},{"location":"about/#for-research-details","text":"Google Scholar","title":"For research details:"},{"location":"contact/","text":"Contact us: \u00b6 Check the socials to reach out for more \u00b6 Your reach out is like a call from the outerworld For official queries email Your suggestions are also highly needed and encouraged \u00b6 Write on any of the following socials Stay Connected LinkedIn Twitter Github Instagram Telegram Youtube","title":"Contact us: :telephone:"},{"location":"contact/#contact-us","text":"","title":"Contact us:"},{"location":"contact/#check-the-socials-to-reach-out-for-more","text":"Your reach out is like a call from the outerworld For official queries email","title":"Check the socials to reach out for more"},{"location":"contact/#your-suggestions-are-also-highly-needed-and-encouraged","text":"Write on any of the following socials Stay Connected LinkedIn Twitter Github Instagram Telegram Youtube","title":"Your suggestions are also highly needed and encouraged"},{"location":"blogs/","text":"Blogs \u00b6 All will be listed here; you can search and find the one you need. Will add Soon ... \u00b6 Hope you are enjoyed the content in other tabs, subtabs. For now I am doing my best to manage everything by my own , and could not add the guides here I understand your eagerness. But right now I need time and your patience \u00b6 Thank you for the kindess and have a great study \u00b6 \"Life is like riding a bicycle. To keep your balance, you must keep moving.\" Albert Einstein you will soon find what you are looking for. regards, Jawad","title":"Blogs :newspaper:"},{"location":"blogs/#blogs","text":"All will be listed here; you can search and find the one you need.","title":"Blogs"},{"location":"blogs/#will-add-soon","text":"Hope you are enjoyed the content in other tabs, subtabs. For now I am doing my best to manage everything by my own , and could not add the guides here","title":"Will add Soon ..."},{"location":"blogs/#i-understand-your-eagerness-but-right-now-i-need-time-and-your-patience","text":"","title":"I understand your eagerness. But right now I need time and your patience"},{"location":"blogs/#thank-you-for-the-kindess-and-have-a-great-study","text":"\"Life is like riding a bicycle. To keep your balance, you must keep moving.\" Albert Einstein you will soon find what you are looking for. regards, Jawad","title":"Thank you for the kindess and have a great study"},{"location":"blogs/2022/How-to-contribute/","text":"Nothing to read yet. Will find something to add soon...","title":"How to contribute"},{"location":"blogs/2022/How-to-use-these-resources/","text":"How-to-use-these-resources \u00b6 Will add Soon ... \u00b6 Hope you are enjoyed the content in other tabs, subtabs. For now I am doing my best to manage everything by my own, and could not add the guides here I understand your eagerness. But right now I need time and your patience \u00b6 Thank you for the kindess and have a great study","title":"How-to-use-these-resources"},{"location":"blogs/2022/How-to-use-these-resources/#how-to-use-these-resources","text":"","title":"How-to-use-these-resources"},{"location":"blogs/2022/How-to-use-these-resources/#will-add-soon","text":"Hope you are enjoyed the content in other tabs, subtabs. For now I am doing my best to manage everything by my own, and could not add the guides here","title":"Will add Soon ..."},{"location":"blogs/2022/How-to-use-these-resources/#i-understand-your-eagerness-but-right-now-i-need-time-and-your-patience","text":"Thank you for the kindess and have a great study","title":"I understand your eagerness. But right now I need time and your patience"},{"location":"booksnotes/","text":"Books Notes \u00b6 Here you will find notes related to the most well-know books authored by the top researchers in ML The list will increase will time :: \u00b6 Feel free to send pull requests to add your favorte books :: Currently we have : \u00b6 Data Science Handbook by Jake VanderPlas The book's website a great source and the github repo for the great notebooks... go there for more fruits ! :backpack: Machine Learning Handbook","title":"Books Notes :books: :notes:"},{"location":"booksnotes/#books-notes","text":"Here you will find notes related to the most well-know books authored by the top researchers in ML","title":"Books Notes"},{"location":"booksnotes/#the-list-will-increase-will-time","text":"Feel free to send pull requests to add your favorte books ::","title":"The list will increase will time ::"},{"location":"booksnotes/#currently-we-have","text":"Data Science Handbook by Jake VanderPlas The book's website a great source and the github repo for the great notebooks... go there for more fruits ! :backpack: Machine Learning Handbook","title":"Currently we have :"},{"location":"booksnotes/pythonDataScienceHandBook/","text":"Data Science Handbook \u00b6 by Jake VanderPlas This is a great book to refer when using any of the Data Science python libraries and Traditional ML algorithm implementation. The book's website a great source and the github repo for the great notebooks... go there for more fruits ! :backpack: Table of content: \u00b6 Jump to any section you want Chapter 1: Introduction to Numpy Chapter 2: Data Manipulation with Pandas Chapter 3: Visualization with Matplotlib Chapter 4: Machine Learning","title":"chapt 1"},{"location":"booksnotes/pythonDataScienceHandBook/#data-science-handbook","text":"by Jake VanderPlas This is a great book to refer when using any of the Data Science python libraries and Traditional ML algorithm implementation. The book's website a great source and the github repo for the great notebooks... go there for more fruits ! :backpack:","title":"Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/#table-of-content","text":"Jump to any section you want Chapter 1: Introduction to Numpy Chapter 2: Data Manipulation with Pandas Chapter 3: Visualization with Matplotlib Chapter 4: Machine Learning","title":"Table of content:"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 00 - Understanding Data Types in Python \u00b6 Understanding Data Types in Python A Python Integer Is More Than Just an Integer A Python List Is More Than Just a List Fixed typed Arrays in Python Creating Arrays from Scratch NumPy Standard Data Types Next: Basics of NumPy Arrays Understanding Data Types in Python \u00b6 Users of Python are often drawn in by its ease of use, one piece of which is dynamic typing. While a statically typed language like C or Java requires each variable to be explicitly declared, a dynamically typed language like Python skips this specification. A Python Integer Is More Than Just an Integer \u00b6 struct _longobject { long ob_refcnt; PyTypeObject *ob_type; size_t ob_size; long ob_digit[1]; }; A single integer in Python 3.4 actually contains four pieces: \u2022 ob_refcnt, a reference count that helps Python silently handle memory allocation and deallocation \u2022 ob_type, which encodes the type of the variable \u2022 ob_size, which specifies the size of the following data members \u2022 ob_digit, which contains the actual integer value that we expect the Python variable to represent A Python List Is More Than Just a List \u00b6 l1 = list ( range ( 10 )) l1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] type ( l1 [ 0 ]) int l2 = list ( str ( i ) for i in range ( 10 )) l2 ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] type ( l2 [ 0 ]) str # Becaause of the dynamically typing our list can even be heterogenious l3 = [ True , 3.146 , 7 , \"Name\" ] [ type ( i ) for i in l3 ] [bool, float, int, str] But this flexibility comes at a cost: to allow these flexible types, each item in the list must contain its own type info, reference count, and other information\u2014that is, each item is a complete Python object. In the special case that all variables are of the same type, much of this information is redundant: it can be much more efficient to store data in a fixed-type array. The difference between a dynamic-type list and a fixed-type (NumPy-style) array given below At the implementation level, the array essentially contains a single pointer to one con\u2010 tiguous block of data. The Python list, on the other hand, contains a pointer to a block of pointers, each of which in turn points to a full Python object like the Python integer we saw earlier. Again, the advantage of the list is flexibility: because each list element is a full structure containing both data and type information, the list can be filled with data of any desired type. Fixed-type NumPy-style arrays lack this flexibil\u2010 ity, but are much more efficient for storing and manipulating data Fixed typed Arrays in Python \u00b6 Much more useful, however, is the ndarray object of the NumPy package. While Python\u2019s array object provides efficient storage of array-based data, NumPy adds to this efficient operations on that data. import array l = list ( range ( 10 )) a = array . array ( 'i' , l ) #Here 'i' is a type code indicating the contents are integers a array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) type ( a ) array.array We\u2019ll start with the standard NumPy import, under the alias np: ## Creating Arrays from Python Lists import numpy as np # Integer array np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) array([1, 2, 3, 4, 5, 6]) np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = 'float32' ) # Explicitly set the data type of the resulting array array([1., 2., 3., 4., 5., 6.], dtype=float32) # Nested list results in multi-dimensional arrays check = np . array ([[[ 1 , 3 , 5 ],[ 2 , 4 , 6 ]],[[ 3 , 7 , 11 ],[ 0 , 0 , 0 ]]]) check array([[[ 1, 3, 5], [ 2, 4, 6]], [[ 3, 7, 11], [ 0, 0, 0]]]) check . shape (2, 2, 3) np . array ([ range ( i , i + 3 ) for i in [ 2 , 4 , 6 ]]) array([[2, 3, 4], [4, 5, 6], [6, 7, 8]]) Creating Arrays from Scratch \u00b6 Especially for larger arrays, it is more efficient to create arrays from scratch using rou\u2010 tines built into NumPy # Create a length-10 integer array filled with zeros np . zeros ( 10 , dtype = int ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # Create a 2,3 flaot matrix filled with zeros np . zeros (( 2 , 3 ), dtype = float ) array([[0., 0., 0.], [0., 0., 0.]]) # Create a length-5 array filled with ones np . ones ( 5 ) array([1., 1., 1., 1., 1.]) # Create a 4x3 matrix filled with ones np . ones (( 4 , 3 ), dtype = int ) array([[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]) # Create a 4x3 matrix filled with the given number np . full (( 3 , 3 ), 7 ) array([[7, 7, 7], [7, 7, 7], [7, 7, 7]]) # Create an array filled with a linear sequence # Starting at 0, ending at 20, stepping by 2 # (this is similar to the built-in range() function) np . arange ( 0 , 20 , 2 ) array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) # Create an array of five values evenly spaced between 0 and 1 np . linspace ( 0 , 1 , 5 ) array([0. , 0.25, 0.5 , 0.75, 1. ]) # Create a 3x3 array of uniformly distributed # random values between 0 and 1 np . random . random (( 3 , 3 )) array([[0.32813402, 0.33063616, 0.13412605], [0.41941497, 0.97530927, 0.04965349], [0.00563929, 0.26416415, 0.84203272]]) # Create a 3x3 array of normally distributed random values # with mean 0 and standard deviation 1 np . random . normal ( 0 , 1 ,( 3 , 3 )) array([[ 1.08133877, -1.1355389 , 1.04280763], [ 0.91102377, 0.77094415, 0.14660595], [ 2.32307703, 0.69711284, 1.26467304]]) # Create a 3x3 array of random integers in the interval [0, 10) np . random . randint ( 0 , 10 ,( 3 , 3 )) array([[2, 4, 8], [5, 8, 1], [5, 8, 3]]) # Create a 3x3 identity matrix np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) # Create an uninitialized array of three integers # The values will be whatever happens to already exist at that # memory location np . empty ( 3 ) array([1., 1., 1.]) NumPy Standard Data Types \u00b6 NumPy arrays contain values of a single type, so it is important to have detailed knowledge of those types and their limitations. Because NumPy is built in C, the types will be familiar to users of C, Fortran, and other related languages. np . zeros ( 5 , dtype = 'int16' ) array([0, 0, 0, 0, 0], dtype=int16) np . zeros ( 5 , dtype = np . int16 ) array([0, 0, 0, 0, 0], dtype=int16) Next: Basics of NumPy Arrays \u00b6 page 42","title":"00 Understanding Data Types in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#00-understanding-data-types-in-python","text":"Understanding Data Types in Python A Python Integer Is More Than Just an Integer A Python List Is More Than Just a List Fixed typed Arrays in Python Creating Arrays from Scratch NumPy Standard Data Types Next: Basics of NumPy Arrays","title":"00 - Understanding Data Types in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#understanding-data-types-in-python","text":"Users of Python are often drawn in by its ease of use, one piece of which is dynamic typing. While a statically typed language like C or Java requires each variable to be explicitly declared, a dynamically typed language like Python skips this specification.","title":"Understanding Data Types in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#a-python-integer-is-more-than-just-an-integer","text":"struct _longobject { long ob_refcnt; PyTypeObject *ob_type; size_t ob_size; long ob_digit[1]; }; A single integer in Python 3.4 actually contains four pieces: \u2022 ob_refcnt, a reference count that helps Python silently handle memory allocation and deallocation \u2022 ob_type, which encodes the type of the variable \u2022 ob_size, which specifies the size of the following data members \u2022 ob_digit, which contains the actual integer value that we expect the Python variable to represent","title":"A Python Integer Is More Than Just an Integer"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#a-python-list-is-more-than-just-a-list","text":"l1 = list ( range ( 10 )) l1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] type ( l1 [ 0 ]) int l2 = list ( str ( i ) for i in range ( 10 )) l2 ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] type ( l2 [ 0 ]) str # Becaause of the dynamically typing our list can even be heterogenious l3 = [ True , 3.146 , 7 , \"Name\" ] [ type ( i ) for i in l3 ] [bool, float, int, str] But this flexibility comes at a cost: to allow these flexible types, each item in the list must contain its own type info, reference count, and other information\u2014that is, each item is a complete Python object. In the special case that all variables are of the same type, much of this information is redundant: it can be much more efficient to store data in a fixed-type array. The difference between a dynamic-type list and a fixed-type (NumPy-style) array given below At the implementation level, the array essentially contains a single pointer to one con\u2010 tiguous block of data. The Python list, on the other hand, contains a pointer to a block of pointers, each of which in turn points to a full Python object like the Python integer we saw earlier. Again, the advantage of the list is flexibility: because each list element is a full structure containing both data and type information, the list can be filled with data of any desired type. Fixed-type NumPy-style arrays lack this flexibil\u2010 ity, but are much more efficient for storing and manipulating data","title":"A Python List Is More Than Just a List"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#fixed-typed-arrays-in-python","text":"Much more useful, however, is the ndarray object of the NumPy package. While Python\u2019s array object provides efficient storage of array-based data, NumPy adds to this efficient operations on that data. import array l = list ( range ( 10 )) a = array . array ( 'i' , l ) #Here 'i' is a type code indicating the contents are integers a array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) type ( a ) array.array We\u2019ll start with the standard NumPy import, under the alias np: ## Creating Arrays from Python Lists import numpy as np # Integer array np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) array([1, 2, 3, 4, 5, 6]) np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = 'float32' ) # Explicitly set the data type of the resulting array array([1., 2., 3., 4., 5., 6.], dtype=float32) # Nested list results in multi-dimensional arrays check = np . array ([[[ 1 , 3 , 5 ],[ 2 , 4 , 6 ]],[[ 3 , 7 , 11 ],[ 0 , 0 , 0 ]]]) check array([[[ 1, 3, 5], [ 2, 4, 6]], [[ 3, 7, 11], [ 0, 0, 0]]]) check . shape (2, 2, 3) np . array ([ range ( i , i + 3 ) for i in [ 2 , 4 , 6 ]]) array([[2, 3, 4], [4, 5, 6], [6, 7, 8]])","title":"Fixed typed Arrays in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#creating-arrays-from-scratch","text":"Especially for larger arrays, it is more efficient to create arrays from scratch using rou\u2010 tines built into NumPy # Create a length-10 integer array filled with zeros np . zeros ( 10 , dtype = int ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # Create a 2,3 flaot matrix filled with zeros np . zeros (( 2 , 3 ), dtype = float ) array([[0., 0., 0.], [0., 0., 0.]]) # Create a length-5 array filled with ones np . ones ( 5 ) array([1., 1., 1., 1., 1.]) # Create a 4x3 matrix filled with ones np . ones (( 4 , 3 ), dtype = int ) array([[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]) # Create a 4x3 matrix filled with the given number np . full (( 3 , 3 ), 7 ) array([[7, 7, 7], [7, 7, 7], [7, 7, 7]]) # Create an array filled with a linear sequence # Starting at 0, ending at 20, stepping by 2 # (this is similar to the built-in range() function) np . arange ( 0 , 20 , 2 ) array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) # Create an array of five values evenly spaced between 0 and 1 np . linspace ( 0 , 1 , 5 ) array([0. , 0.25, 0.5 , 0.75, 1. ]) # Create a 3x3 array of uniformly distributed # random values between 0 and 1 np . random . random (( 3 , 3 )) array([[0.32813402, 0.33063616, 0.13412605], [0.41941497, 0.97530927, 0.04965349], [0.00563929, 0.26416415, 0.84203272]]) # Create a 3x3 array of normally distributed random values # with mean 0 and standard deviation 1 np . random . normal ( 0 , 1 ,( 3 , 3 )) array([[ 1.08133877, -1.1355389 , 1.04280763], [ 0.91102377, 0.77094415, 0.14660595], [ 2.32307703, 0.69711284, 1.26467304]]) # Create a 3x3 array of random integers in the interval [0, 10) np . random . randint ( 0 , 10 ,( 3 , 3 )) array([[2, 4, 8], [5, 8, 1], [5, 8, 3]]) # Create a 3x3 identity matrix np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) # Create an uninitialized array of three integers # The values will be whatever happens to already exist at that # memory location np . empty ( 3 ) array([1., 1., 1.])","title":"Creating Arrays from Scratch"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#numpy-standard-data-types","text":"NumPy arrays contain values of a single type, so it is important to have detailed knowledge of those types and their limitations. Because NumPy is built in C, the types will be familiar to users of C, Fortran, and other related languages. np . zeros ( 5 , dtype = 'int16' ) array([0, 0, 0, 0, 0], dtype=int16) np . zeros ( 5 , dtype = np . int16 ) array([0, 0, 0, 0, 0], dtype=int16)","title":"NumPy Standard Data Types"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/#next-basics-of-numpy-arrays","text":"page 42","title":"Next: Basics of NumPy Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 01 - The Basics of NumPy Arrays \u00b6 The Basics of NumPy Arrays Array Attributes Array Indexing: Accessing Single Elements Array Slicing: Accessing Subarrays One-dimensional subarrays Multidimensional subarrays Accessing array rows and columns. Subarrays as no-copy views Creating copies of arrays Reshaping of Arrays Array Concatenation and Splitting Concatenation of arrays Splitting of arrays The Basics of NumPy Arrays \u00b6 This section will present several examples using NumPy array manipulation to access data and subarrays, and to split, reshape, and join the arrays. We\u2019ll cover a few categories of basic array manipulations here: Attributes of arrays Determining the size, shape, memory consumption, and data types of arrays Indexing of arrays Getting and setting the value of individual array elements Slicing of arrays Getting and setting smaller subarrays within a larger array Reshaping of arrays Changing the shape of a given array Joining and splitting of arrays Combining multiple arrays into one, and splitting one array into many Array Attributes \u00b6 import numpy as np np . random . seed ( 0 ) x1 = np . random . randint ( 10 , size = 6 ) # 1-d array x2 = np . random . randint ( 10 , size = ( 3 , 4 )) # 2-d array (matrix) x3 = np . random . randint ( 10 , size = ( 3 , 4 , 5 )) # 3-d array print ( x3 ) print ( \"x3 ndim: \" , x3 . ndim ) print ( \"x3.shape: \" , x3 . shape ) print ( \"x3.size: \" , x3 . size ) [[[8 1 5 9 8] [9 4 3 0 3] [5 0 2 3 8] [1 3 3 3 7]] [[0 1 9 9 0] [4 7 3 2 7] [2 0 0 4 5] [5 6 8 4 1]] [[4 9 8 1 1] [7 9 9 3 6] [7 2 0 3 5] [9 4 4 6 4]]] x3 ndim: 3 x3.shape: (3, 4, 5) x3.size: 60 print ( x2 ) print ( \"x2 ndim: \" , x2 . ndim ) print ( \"x2.shape: \" , x2 . shape ) print ( \"x2.size: \" , x2 . size ) [[3 5 2 4] [7 6 8 8] [1 6 7 7]] x2 ndim: 2 x2.shape: (3, 4) x2.size: 12 print ( x1 ) print ( \"x1 ndim: \" , x1 . ndim ) print ( \"x1.shape: \" , x1 . shape ) print ( \"x1.size: \" , x1 . size ) [5 0 3 3 7 9] x1 ndim: 1 x1.shape: (6,) x1.size: 6 print ( \"dtype: \" , x3 . dtype ) dtype: int64 print ( \"itemsize: \" , x3 . itemsize , \"bytes\" ) # size of each array element itemsize: 8 bytes print ( \"nbytes: \" , x3 . nbytes , \" bytes\" ) # total size of the array -> sum of bytes of each array element nbytes: 480 bytes Array Indexing: Accessing Single Elements \u00b6 If you are familiar with Python\u2019s standard list indexing, indexing in NumPy will feel quite familiar. In a one-dimensional array, you can access the ith value (counting from zero) by specifying the desired index in square brackets, just as with Python lists: x1 array([5, 0, 3, 3, 7, 9]) x1 [ - 1 ] 9 x1 [ 0 ] 5 #In a multidimensional array, you access items using a comma-separated tuple of indices: x2 array([[3, 5, 2, 4], [7, 6, 8, 8], [1, 6, 7, 7]]) x2 [ 0 , 0 ] 3 # same for modifying any element of the array x2 [ 0 , 3 ] 4 x2 [ 0 , 3 ] =- 4 x2 [ 0 , 3 ] -4 x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) Keep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated. Don\u2019t be caught unaware by this behavior! x1 [ 0 ] 5 x1 [ 0 ] =- 5.78 # this will the decimal part x1 [ 0 ] -5 Array Slicing: Accessing Subarrays \u00b6 Just as we can use square brackets to access individual array elements, we can also use them to access subarrays with the slice notation, marked by the colon (:) character. The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array x, use this: x[start:stop:step] One-dimensional subarrays \u00b6 x = np . arange ( 10 ) x array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) x [: 5 ] array([0, 1, 2, 3, 4]) x [ 1 : len ( x ) - 3 ] array([1, 2, 3, 4, 5, 6]) x [ len ( x ) // 2 :] # after mid array([5, 6, 7, 8, 9]) x [:: 2 ] # Every other element array([0, 2, 4, 6, 8]) x [ 1 :: 2 ] array([1, 3, 5, 7, 9]) A potentially confusing case is when the step value is negative. In this case, the defaults for start and stop are swapped. This becomes a convenient way to reverse an array: x [:: - 1 ] # All elements reverse array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) x [ 5 :: - 2 ] array([5, 3, 1]) Multidimensional subarrays \u00b6 Multidimensional slices work in the same way, with multiple slices separated by com\u2010 mas. x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2 [: 2 ,: 3 ] #two rows and three columns array([[3, 5, 2], [7, 6, 8]]) x2 [: 3 ,:: 2 ] # three rows and every other column array([[3, 2], [7, 8], [1, 7]]) Accessing array rows and columns. \u00b6 One commonly needed routine is accessing single rows or columns of an array. You can do this by combining indexing and slicing, using an empty slice marked by a single colon (:): x2 [:, 0 ] # first column array([3, 7, 1]) x2 [ 0 ,:] # First row array([ 3, 5, 2, -4]) #In the case of row access, the empty slice can be omitted for a more compact syntax:s x2 [ 0 ] array([ 3, 5, 2, -4]) Subarrays as no-copy views \u00b6 One important\u2014and extremely useful\u2014thing to know about array slices is that they return views rather than copies of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) # Extracting a 2x2 subarray from this x2_sub = x2 [: 2 ,: 2 ] x2_sub array([[3, 5], [7, 6]]) # Modifying this subarray will also modify the original array x2_sub [ 0 , 0 ] =- 9 x2 array([[-9, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2_sub array([[-9, 5], [ 7, 6]]) This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer. Creating copies of arrays \u00b6 Despite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the copy() method: x2_sub_copy = x2 [: 2 ,: 2 ] . copy () x2_sub_copy array([[-9, 5], [ 7, 6]]) If we now modify this subarray, the original array is not touched: \u00b6 x2_sub_copy [ 0 , 0 ] =- 999 x2 array([[-9, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2_sub_copy array([[-999, 5], [ 7, 6]]) Reshaping of Arrays \u00b6 Another useful type of operation is reshaping of arrays. The most flexible way of doing this is with the reshape() method. grid = np . arange ( 1 , 10 ) . reshape (( 3 , 3 )) grid array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Note that for this to work, the size of the initial array must match the size of the reshaped array. Where possible, the reshape method will use a no-copy view of the initial array, but with noncontiguous memory buffers this is not always the case. Another common reshaping pattern is the conversion of a one-dimensional array into a two-dimensional row or column matrix. You can do this with the reshape method, or more easily by making use of the newaxis keyword within a slice opera\u2010 tion: x = np . array ([ 1 , 2 , 3 , 4 ]) x array([1, 2, 3, 4]) x . reshape (( 1 , 4 )) # row vector reshape via reshape array([[1, 2, 3, 4]]) x . reshape (( len ( x ), 1 )) #Column vector reshape via array([[1], [2], [3], [4]]) x [ np . newaxis ,:] array([[1, 2, 3, 4]]) x [:, np . newaxis ] array([[1], [2], [3], [4]]) Array Concatenation and Splitting \u00b6 Concatenation of arrays \u00b6 Concatenation, or joining of two arrays in NumPy, is primarily accomplished through the routines np.concatenate, np.vstack, and np.hstack . np.concatenate takes a tuple or list of arrays as its first argument. * You can also concatenate more than two arrays at once * np.concatenate can also be used for two-dimensional arrays x = np . array ([ 2 , 4 , 6 , 8 ]) y = np . array ([ 1 , 3 , 5 , 7 ]) np . concatenate ([ x , y ]) array([2, 4, 6, 8, 1, 3, 5, 7]) z = np . zeros ( 4 ) z array([0., 0., 0., 0.]) np . concatenate ([ x , z , y ]) # Three arrays concatenate in given order array([2., 4., 6., 8., 0., 0., 0., 0., 1., 3., 5., 7.]) grid1 = np . random . randint ( 10 , 50 , size = ( 2 , 4 )) grid2 = np . random . randint ( 0 , 1 , size = ( 2 , 3 )) grid1 array([[39, 13, 44, 23], [49, 31, 19, 10]]) grid2 array([[0, 0, 0], [0, 0, 0]]) np . concatenate ([ grid1 , grid2 ]) ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 4 and the array at index 1 has size 3 np . concatenate ([ grid1 , grid2 ], axis = 1 ) array([[39, 13, 44, 23, 0, 0, 0], [49, 31, 19, 10, 0, 0, 0]]) grid2 = np . random . randint ( 0 , 10 , size = ( 2 , 4 )) grid2 array([[3, 0, 5, 0], [1, 2, 4, 2]]) np . concatenate ([ grid1 , grid2 ], axis = 0 ) array([[13, 22, 46, 24], [25, 30, 45, 33], [ 3, 0, 5, 0], [ 1, 2, 4, 2]]) np . concatenate ([ grid1 , grid2 ], axis = 1 ) array([[13, 22, 46, 24, 3, 0, 5, 0], [25, 30, 45, 33, 1, 2, 4, 2]]) For working with arrays of mixed dimensions, it can be clearer to use the np.vstack (vertical stack) and np.hstack (horizontal stack) functions x = np . array ([ 1 , 2 , 3 ]) grid = np . random . randint ( 1 , 10 , size = ( 2 , 3 )) x array([1, 2, 3]) grid array([[5, 7, 9], [3, 4, 1]]) np . vstack ([ x , grid ]) array([[1, 2, 3], [5, 7, 9], [3, 4, 1]]) Splitting of arrays \u00b6 The opposite of concatenation is splitting, which is implemented by the functions np.split, np.hsplit, and np.vsplit. For each of these, we can pass a list of indices giving the split points: x = [ 1 , 2 , 3 , 99 , 100 , 101 , 3 , 2 , 1 ] x1 , x2 , x3 = np . split ( x ,[ 3 , 5 ]) # split will be at index 3 & 5 x1 array([1, 2, 3]) x2 array([ 99, 100]) x3 #Notice that N split points lead to N + 1 subarrays. #The related functions np.hsplit and np.vsplit are similar: array([101, 3, 2, 1]) grid = np . arange ( 16 ) . reshape (( 4 , 4 )) upper , lower = np . vsplit ( grid ,[ 2 ]) upper array([[0, 1, 2, 3], [4, 5, 6, 7]]) lower array([[ 8, 9, 10, 11], [12, 13, 14, 15]]) grid array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) left , right = np . hsplit ( grid ,[ 2 ]) left array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13]]) right array([[ 2, 3], [ 6, 7], [10, 11], [14, 15]]) Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"01 basics of numpy arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#01-the-basics-of-numpy-arrays","text":"The Basics of NumPy Arrays Array Attributes Array Indexing: Accessing Single Elements Array Slicing: Accessing Subarrays One-dimensional subarrays Multidimensional subarrays Accessing array rows and columns. Subarrays as no-copy views Creating copies of arrays Reshaping of Arrays Array Concatenation and Splitting Concatenation of arrays Splitting of arrays","title":"01 - The Basics of NumPy Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#the-basics-of-numpy-arrays","text":"This section will present several examples using NumPy array manipulation to access data and subarrays, and to split, reshape, and join the arrays. We\u2019ll cover a few categories of basic array manipulations here: Attributes of arrays Determining the size, shape, memory consumption, and data types of arrays Indexing of arrays Getting and setting the value of individual array elements Slicing of arrays Getting and setting smaller subarrays within a larger array Reshaping of arrays Changing the shape of a given array Joining and splitting of arrays Combining multiple arrays into one, and splitting one array into many","title":"The Basics of NumPy Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#array-attributes","text":"import numpy as np np . random . seed ( 0 ) x1 = np . random . randint ( 10 , size = 6 ) # 1-d array x2 = np . random . randint ( 10 , size = ( 3 , 4 )) # 2-d array (matrix) x3 = np . random . randint ( 10 , size = ( 3 , 4 , 5 )) # 3-d array print ( x3 ) print ( \"x3 ndim: \" , x3 . ndim ) print ( \"x3.shape: \" , x3 . shape ) print ( \"x3.size: \" , x3 . size ) [[[8 1 5 9 8] [9 4 3 0 3] [5 0 2 3 8] [1 3 3 3 7]] [[0 1 9 9 0] [4 7 3 2 7] [2 0 0 4 5] [5 6 8 4 1]] [[4 9 8 1 1] [7 9 9 3 6] [7 2 0 3 5] [9 4 4 6 4]]] x3 ndim: 3 x3.shape: (3, 4, 5) x3.size: 60 print ( x2 ) print ( \"x2 ndim: \" , x2 . ndim ) print ( \"x2.shape: \" , x2 . shape ) print ( \"x2.size: \" , x2 . size ) [[3 5 2 4] [7 6 8 8] [1 6 7 7]] x2 ndim: 2 x2.shape: (3, 4) x2.size: 12 print ( x1 ) print ( \"x1 ndim: \" , x1 . ndim ) print ( \"x1.shape: \" , x1 . shape ) print ( \"x1.size: \" , x1 . size ) [5 0 3 3 7 9] x1 ndim: 1 x1.shape: (6,) x1.size: 6 print ( \"dtype: \" , x3 . dtype ) dtype: int64 print ( \"itemsize: \" , x3 . itemsize , \"bytes\" ) # size of each array element itemsize: 8 bytes print ( \"nbytes: \" , x3 . nbytes , \" bytes\" ) # total size of the array -> sum of bytes of each array element nbytes: 480 bytes","title":"Array Attributes"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#array-indexing-accessing-single-elements","text":"If you are familiar with Python\u2019s standard list indexing, indexing in NumPy will feel quite familiar. In a one-dimensional array, you can access the ith value (counting from zero) by specifying the desired index in square brackets, just as with Python lists: x1 array([5, 0, 3, 3, 7, 9]) x1 [ - 1 ] 9 x1 [ 0 ] 5 #In a multidimensional array, you access items using a comma-separated tuple of indices: x2 array([[3, 5, 2, 4], [7, 6, 8, 8], [1, 6, 7, 7]]) x2 [ 0 , 0 ] 3 # same for modifying any element of the array x2 [ 0 , 3 ] 4 x2 [ 0 , 3 ] =- 4 x2 [ 0 , 3 ] -4 x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) Keep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated. Don\u2019t be caught unaware by this behavior! x1 [ 0 ] 5 x1 [ 0 ] =- 5.78 # this will the decimal part x1 [ 0 ] -5","title":"Array Indexing: Accessing Single Elements"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#array-slicing-accessing-subarrays","text":"Just as we can use square brackets to access individual array elements, we can also use them to access subarrays with the slice notation, marked by the colon (:) character. The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array x, use this: x[start:stop:step]","title":"Array Slicing: Accessing Subarrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#one-dimensional-subarrays","text":"x = np . arange ( 10 ) x array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) x [: 5 ] array([0, 1, 2, 3, 4]) x [ 1 : len ( x ) - 3 ] array([1, 2, 3, 4, 5, 6]) x [ len ( x ) // 2 :] # after mid array([5, 6, 7, 8, 9]) x [:: 2 ] # Every other element array([0, 2, 4, 6, 8]) x [ 1 :: 2 ] array([1, 3, 5, 7, 9]) A potentially confusing case is when the step value is negative. In this case, the defaults for start and stop are swapped. This becomes a convenient way to reverse an array: x [:: - 1 ] # All elements reverse array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) x [ 5 :: - 2 ] array([5, 3, 1])","title":"One-dimensional subarrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#multidimensional-subarrays","text":"Multidimensional slices work in the same way, with multiple slices separated by com\u2010 mas. x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2 [: 2 ,: 3 ] #two rows and three columns array([[3, 5, 2], [7, 6, 8]]) x2 [: 3 ,:: 2 ] # three rows and every other column array([[3, 2], [7, 8], [1, 7]])","title":"Multidimensional subarrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#accessing-array-rows-and-columns","text":"One commonly needed routine is accessing single rows or columns of an array. You can do this by combining indexing and slicing, using an empty slice marked by a single colon (:): x2 [:, 0 ] # first column array([3, 7, 1]) x2 [ 0 ,:] # First row array([ 3, 5, 2, -4]) #In the case of row access, the empty slice can be omitted for a more compact syntax:s x2 [ 0 ] array([ 3, 5, 2, -4])","title":"Accessing array rows and columns."},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#subarrays-as-no-copy-views","text":"One important\u2014and extremely useful\u2014thing to know about array slices is that they return views rather than copies of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies x2 array([[ 3, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) # Extracting a 2x2 subarray from this x2_sub = x2 [: 2 ,: 2 ] x2_sub array([[3, 5], [7, 6]]) # Modifying this subarray will also modify the original array x2_sub [ 0 , 0 ] =- 9 x2 array([[-9, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2_sub array([[-9, 5], [ 7, 6]]) This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer.","title":"Subarrays as no-copy views"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#creating-copies-of-arrays","text":"Despite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the copy() method: x2_sub_copy = x2 [: 2 ,: 2 ] . copy () x2_sub_copy array([[-9, 5], [ 7, 6]])","title":"Creating copies of arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#if-we-now-modify-this-subarray-the-original-array-is-not-touched","text":"x2_sub_copy [ 0 , 0 ] =- 999 x2 array([[-9, 5, 2, -4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) x2_sub_copy array([[-999, 5], [ 7, 6]])","title":"If we now modify this subarray, the original array is not touched:"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#reshaping-of-arrays","text":"Another useful type of operation is reshaping of arrays. The most flexible way of doing this is with the reshape() method. grid = np . arange ( 1 , 10 ) . reshape (( 3 , 3 )) grid array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Note that for this to work, the size of the initial array must match the size of the reshaped array. Where possible, the reshape method will use a no-copy view of the initial array, but with noncontiguous memory buffers this is not always the case. Another common reshaping pattern is the conversion of a one-dimensional array into a two-dimensional row or column matrix. You can do this with the reshape method, or more easily by making use of the newaxis keyword within a slice opera\u2010 tion: x = np . array ([ 1 , 2 , 3 , 4 ]) x array([1, 2, 3, 4]) x . reshape (( 1 , 4 )) # row vector reshape via reshape array([[1, 2, 3, 4]]) x . reshape (( len ( x ), 1 )) #Column vector reshape via array([[1], [2], [3], [4]]) x [ np . newaxis ,:] array([[1, 2, 3, 4]]) x [:, np . newaxis ] array([[1], [2], [3], [4]])","title":"Reshaping of Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#array-concatenation-and-splitting","text":"","title":"Array Concatenation and Splitting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#concatenation-of-arrays","text":"Concatenation, or joining of two arrays in NumPy, is primarily accomplished through the routines np.concatenate, np.vstack, and np.hstack . np.concatenate takes a tuple or list of arrays as its first argument. * You can also concatenate more than two arrays at once * np.concatenate can also be used for two-dimensional arrays x = np . array ([ 2 , 4 , 6 , 8 ]) y = np . array ([ 1 , 3 , 5 , 7 ]) np . concatenate ([ x , y ]) array([2, 4, 6, 8, 1, 3, 5, 7]) z = np . zeros ( 4 ) z array([0., 0., 0., 0.]) np . concatenate ([ x , z , y ]) # Three arrays concatenate in given order array([2., 4., 6., 8., 0., 0., 0., 0., 1., 3., 5., 7.]) grid1 = np . random . randint ( 10 , 50 , size = ( 2 , 4 )) grid2 = np . random . randint ( 0 , 1 , size = ( 2 , 3 )) grid1 array([[39, 13, 44, 23], [49, 31, 19, 10]]) grid2 array([[0, 0, 0], [0, 0, 0]]) np . concatenate ([ grid1 , grid2 ]) ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 4 and the array at index 1 has size 3 np . concatenate ([ grid1 , grid2 ], axis = 1 ) array([[39, 13, 44, 23, 0, 0, 0], [49, 31, 19, 10, 0, 0, 0]]) grid2 = np . random . randint ( 0 , 10 , size = ( 2 , 4 )) grid2 array([[3, 0, 5, 0], [1, 2, 4, 2]]) np . concatenate ([ grid1 , grid2 ], axis = 0 ) array([[13, 22, 46, 24], [25, 30, 45, 33], [ 3, 0, 5, 0], [ 1, 2, 4, 2]]) np . concatenate ([ grid1 , grid2 ], axis = 1 ) array([[13, 22, 46, 24, 3, 0, 5, 0], [25, 30, 45, 33, 1, 2, 4, 2]]) For working with arrays of mixed dimensions, it can be clearer to use the np.vstack (vertical stack) and np.hstack (horizontal stack) functions x = np . array ([ 1 , 2 , 3 ]) grid = np . random . randint ( 1 , 10 , size = ( 2 , 3 )) x array([1, 2, 3]) grid array([[5, 7, 9], [3, 4, 1]]) np . vstack ([ x , grid ]) array([[1, 2, 3], [5, 7, 9], [3, 4, 1]])","title":"Concatenation of arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#splitting-of-arrays","text":"The opposite of concatenation is splitting, which is implemented by the functions np.split, np.hsplit, and np.vsplit. For each of these, we can pass a list of indices giving the split points: x = [ 1 , 2 , 3 , 99 , 100 , 101 , 3 , 2 , 1 ] x1 , x2 , x3 = np . split ( x ,[ 3 , 5 ]) # split will be at index 3 & 5 x1 array([1, 2, 3]) x2 array([ 99, 100]) x3 #Notice that N split points lead to N + 1 subarrays. #The related functions np.hsplit and np.vsplit are similar: array([101, 3, 2, 1]) grid = np . arange ( 16 ) . reshape (( 4 , 4 )) upper , lower = np . vsplit ( grid ,[ 2 ]) upper array([[0, 1, 2, 3], [4, 5, 6, 7]]) lower array([[ 8, 9, 10, 11], [12, 13, 14, 15]]) grid array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) left , right = np . hsplit ( grid ,[ 2 ]) left array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13]]) right array([[ 2, 3], [ 6, 7], [10, 11], [14, 15]])","title":"Splitting of arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 02 - Computation on NumPy Arrays: Universal Functions \u00b6 Computation on NumPy Arrays: Universal Functions The Slowness of Loops Introducing UFuncs Exploring NumPy\u2019s UFuncs Array arithmetic Specialized ufuncs Advanced Ufunc Features Specifying output Aggregates Outer products Computation on NumPy Arrays: Universal Functions \u00b6 Computation on NumPy arrays can be very fast, or it can be very slow. The key to making it fast is to use vectorized operations, generally implemented through Num\u2010 Py\u2019s universal functions (ufuncs). The Slowness of Loops \u00b6 Python\u2019s default implementation (known as CPython) does some operations very slowly. This is in part due to the dynamic, interpreted nature of the language: the fact that types are flexible, so that sequences of operations cannot be compiled down to efficient machine code as in languages like C and Fortran. # Example of reciprocal of each item in the list import numpy as np np . random . seed ( 0 ) def compute_reciprocals ( values ): output = np . empty ( len ( values )) for i in range ( len ( values )): output [ i ] = 1.0 / values [ i ] return output values = np . random . randint ( 1 , 10 , size = 5 ) compute_reciprocals ( values ) array([0.16666667, 1. , 0.25 , 0.25 , 0.125 ]) big_array = np . random . randint ( 1 , 100 , size = 100000 ) % timeit compute_reciprocals ( big_array ) 167 ms \u00b1 5.95 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Introducing UFuncs \u00b6 For many types of operations, NumPy provides a convenient interface into just this kind of statically typed, compiled routine. This is known as a vectorized operation. You can accomplish this by simply performing an operation on the array, which will then be applied to each element. This vectorized approach is designed to push the loop into the compiled layer that underlies NumPy, leading to much faster execution. print ( compute_reciprocals ( values )) print ( 1.0 / values ) [0.16666667 1. 0.25 0.25 0.125 ] [0.16666667 1. 0.25 0.25 0.125 ] % timeit ( 1 / big_array ) 129 \u00b5s \u00b1 12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Vectorized operations in NumPy are implemented via ufuncs, whose main purpose is to quickly execute repeated operations on values in NumPy arrays. Ufuncs are extremely flexible\u2014before we saw an operation between a scalar and an array, but we can also operate between two arrays: np . arange ( 5 ) / np . arange ( 1 , 6 ) array([0. , 0.5 , 0.66666667, 0.75 , 0.8 ]) # Works for multi-d arraays x = np . arange ( 9 ) . reshape (( 3 , 3 )) 2 * x array([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]]) Exploring NumPy\u2019s UFuncs \u00b6 Array arithmetic \u00b6 print ( 'x = ' , x ) x = [[0 1 2] [3 4 5] [6 7 8]] print ( 'x-3 \\n ' , np . subtract ( x , 3 )) print ( 'x+13 \\n ' , np . add ( x , 13 )) print ( 'x*9 \\n ' , np . multiply ( x , 9 )) print ( 'x/6 \\n ' , np . subtract ( x , 6 )) print ( 'x//6 \\n ' , np . subtract ( x , 6 )) print ( 'x**4 \\n ' , np . power ( x , 4 )) print ( 'x%4 \\n ' , np . mod ( x , 4 )) x-3 [[-3 -2 -1] [ 0 1 2] [ 3 4 5]] x+13 [[13 14 15] [16 17 18] [19 20 21]] x*9 [[ 0 9 18] [27 36 45] [54 63 72]] x/6 [[-6 -5 -4] [-3 -2 -1] [ 0 1 2]] x//6 [[-6 -5 -4] [-3 -2 -1] [ 0 1 2]] x**4 [[ 0 1 16] [ 81 256 625] [1296 2401 4096]] x%4 [[0 1 2] [3 0 1] [2 3 0]] # Absolute value x = np . array ([ - 2 , 3 , - 4 , - 9 , 0 ]) abs ( x ) array([2, 3, 4, 9, 0]) np . absolute ( x ) array([2, 3, 4, 9, 0]) # Trig thet = np . linspace ( 0 , np . pi , 3 ) print ( \"Theta =\" , thet ) print ( \"Sin(theta) =\" , np . sin ( thet )) print ( \"Cos(theta) =\" , np . cos ( thet )) print ( \" \\n Inverse tri \\n \" ) print ( \"Theta =\" , thet ) print ( \"arcSin(theta) =\" , np . arcsin ( thet )) print ( \"arcCos(theta) =\" , np . arccos ( thet )) Theta = [0. 1.57079633 3.14159265] Sin(theta) = [0.0000000e+00 1.0000000e+00 1.2246468e-16] Cos(theta) = [ 1.000000e+00 6.123234e-17 -1.000000e+00] Inverse tri Theta = [0. 1.57079633 3.14159265] arcSin(theta) = [ 0. nan nan] arcCos(theta) = [1.57079633 nan nan] /tmp/ipykernel_22625/2951825817.py:9: RuntimeWarning: invalid value encountered in arcsin print(\"arcSin(theta) =\",np.arcsin(thet)) /tmp/ipykernel_22625/2951825817.py:10: RuntimeWarning: invalid value encountered in arccos print(\"arcCos(theta) =\",np.arccos(thet)) # Logarthm and Exponents x = [ 1 , 2 , 3 ] print ( \"x=\" , x ) print ( \"e^x=\" , np . exp ( x )) print ( \"2^x=\" , np . exp2 ( x )) print ( \"3^x=\" , np . power ( 3 , x )) x= [1, 2, 3] e^x= [ 2.71828183 7.3890561 20.08553692] 2^x= [2. 4. 8.] 3^x= [ 3 9 27] Specialized ufuncs \u00b6 NumPy has many more ufuncs available, including hyperbolic trig functions, bitwise arithmetic, comparison operators, conversions from radians to degrees, rounding and remainders, and much more. A look through the NumPy documentation reveals a lot of interesting functionality. Another excellent source for more specialized and obscure ufuncs is the submodule scipy.special. If you want to compute some obscure mathematical function on your data, chances are it is implemented in scipy.special. There are far too many functions to list them all, but the following snippet shows a couple that might come up in a statistics context from scipy import special #Gamma functions (generalized factorials) and related functions x = [ 1 , 5 , 10 ] print ( \"gamma(x)=\" , special . gamma ( x )) print ( \"ln|gamma(x)| =\" , special . gammaln ( x )) print ( \"beta(x, 2)=\" , special . beta ( x , 2 )) gamma(x)= [1.0000e+00 2.4000e+01 3.6288e+05] ln|gamma(x)| = [ 0. 3.17805383 12.80182748] beta(x, 2)= [0.5 0.03333333 0.00909091] # Error function (integral of Gaussian) # its complement, and its inverse x = np . array ([ 0 , 0.3 , 0.7 , 1.0 ]) print ( \"erf(x) =\" , special . erf ( x )) print ( \"erfc(x) =\" , special . erfc ( x )) print ( \"erfinv(x) =\" , special . erfinv ( x )) erf(x) = [0. 0.32862676 0.67780119 0.84270079] erfc(x) = [1. 0.67137324 0.32219881 0.15729921] erfinv(x) = [0. 0.27246271 0.73286908 inf] Advanced Ufunc Features \u00b6 Specifying output \u00b6 For large calculations, it is sometimes useful to be able to specify the array where the result of the calculation will be stored. Rather than creating a temporary array, you can use this to write computation results directly to the memory location where you\u2019d like them to be. # THis can be done using the out argument of the function x = np . arange ( 5 ) y = np . empty ( 5 ) np . multiply ( x , 10 , out = y ) print ( y ) [ 0. 10. 20. 30. 40.] # This can be done with array views y = np . zeros ( 10 ) np . power ( 2 , x , out = y [:: 2 ]) print ( y ) [ 1. 0. 2. 0. 4. 0. 8. 0. 16. 0.] np . power ( 2 , x , out = 2 ** x ) array([ 1, 2, 4, 8, 16]) Aggregates \u00b6 For binary ufuncs, there are some interesting aggregates that can be computed directly from the object. For example, if we\u2019d like to reduce an array with a particular operation, we can use the reduce method of any ufunc. A reduce repeatedly applies a given operation to the elements of an array until only a single result remains. # Calling reduce on the add ufunc x = np . arange ( 1 , 6 ) np . add . reduce ( x ) 15 x = np . arange ( 1 , 6 ) np . sum ( x ) 15 np . multiply ( x ) TypeError: multiply() takes from 2 to 3 positional arguments but 1 were given np . add ( x ) TypeError: add() takes from 2 to 3 positional arguments but 1 were given np . multiply . reduce ( x ) 120 # To store all the imtermmediate results of the computation we can use accumulate np . add . accumulate ( x ) array([ 1, 3, 6, 10, 15]) x array([1, 2, 3, 4, 5]) Note that for these particular cases, there are dedicated NumPy functions to compute the results (np.sum, np.prod, np.cumsum, np.cumprod), The ufunc.at and ufunc.reduceat methods Outer products \u00b6 Finally, any ufunc can compute the output of all pairs of two different inputs using the outer method. np . multiply . outer ( x , x ) array([[ 1, 2, 3, 4, 5], [ 2, 4, 6, 8, 10], [ 3, 6, 9, 12, 15], [ 4, 8, 12, 16, 20], [ 5, 10, 15, 20, 25]]) Another extremely useful feature of ufuncs is the ability to operate between arrays of different sizes and shapes, a set of operations known as broadcasting. Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"02 Computation on NumPy Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#02-computation-on-numpy-arrays-universal-functions","text":"Computation on NumPy Arrays: Universal Functions The Slowness of Loops Introducing UFuncs Exploring NumPy\u2019s UFuncs Array arithmetic Specialized ufuncs Advanced Ufunc Features Specifying output Aggregates Outer products","title":"02 - Computation on NumPy Arrays: Universal Functions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#computation-on-numpy-arrays-universal-functions","text":"Computation on NumPy arrays can be very fast, or it can be very slow. The key to making it fast is to use vectorized operations, generally implemented through Num\u2010 Py\u2019s universal functions (ufuncs).","title":"Computation on NumPy Arrays: Universal Functions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#the-slowness-of-loops","text":"Python\u2019s default implementation (known as CPython) does some operations very slowly. This is in part due to the dynamic, interpreted nature of the language: the fact that types are flexible, so that sequences of operations cannot be compiled down to efficient machine code as in languages like C and Fortran. # Example of reciprocal of each item in the list import numpy as np np . random . seed ( 0 ) def compute_reciprocals ( values ): output = np . empty ( len ( values )) for i in range ( len ( values )): output [ i ] = 1.0 / values [ i ] return output values = np . random . randint ( 1 , 10 , size = 5 ) compute_reciprocals ( values ) array([0.16666667, 1. , 0.25 , 0.25 , 0.125 ]) big_array = np . random . randint ( 1 , 100 , size = 100000 ) % timeit compute_reciprocals ( big_array ) 167 ms \u00b1 5.95 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)","title":"The Slowness of Loops"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#introducing-ufuncs","text":"For many types of operations, NumPy provides a convenient interface into just this kind of statically typed, compiled routine. This is known as a vectorized operation. You can accomplish this by simply performing an operation on the array, which will then be applied to each element. This vectorized approach is designed to push the loop into the compiled layer that underlies NumPy, leading to much faster execution. print ( compute_reciprocals ( values )) print ( 1.0 / values ) [0.16666667 1. 0.25 0.25 0.125 ] [0.16666667 1. 0.25 0.25 0.125 ] % timeit ( 1 / big_array ) 129 \u00b5s \u00b1 12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Vectorized operations in NumPy are implemented via ufuncs, whose main purpose is to quickly execute repeated operations on values in NumPy arrays. Ufuncs are extremely flexible\u2014before we saw an operation between a scalar and an array, but we can also operate between two arrays: np . arange ( 5 ) / np . arange ( 1 , 6 ) array([0. , 0.5 , 0.66666667, 0.75 , 0.8 ]) # Works for multi-d arraays x = np . arange ( 9 ) . reshape (( 3 , 3 )) 2 * x array([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]])","title":"Introducing UFuncs"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#exploring-numpys-ufuncs","text":"","title":"Exploring NumPy\u2019s UFuncs"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#array-arithmetic","text":"print ( 'x = ' , x ) x = [[0 1 2] [3 4 5] [6 7 8]] print ( 'x-3 \\n ' , np . subtract ( x , 3 )) print ( 'x+13 \\n ' , np . add ( x , 13 )) print ( 'x*9 \\n ' , np . multiply ( x , 9 )) print ( 'x/6 \\n ' , np . subtract ( x , 6 )) print ( 'x//6 \\n ' , np . subtract ( x , 6 )) print ( 'x**4 \\n ' , np . power ( x , 4 )) print ( 'x%4 \\n ' , np . mod ( x , 4 )) x-3 [[-3 -2 -1] [ 0 1 2] [ 3 4 5]] x+13 [[13 14 15] [16 17 18] [19 20 21]] x*9 [[ 0 9 18] [27 36 45] [54 63 72]] x/6 [[-6 -5 -4] [-3 -2 -1] [ 0 1 2]] x//6 [[-6 -5 -4] [-3 -2 -1] [ 0 1 2]] x**4 [[ 0 1 16] [ 81 256 625] [1296 2401 4096]] x%4 [[0 1 2] [3 0 1] [2 3 0]] # Absolute value x = np . array ([ - 2 , 3 , - 4 , - 9 , 0 ]) abs ( x ) array([2, 3, 4, 9, 0]) np . absolute ( x ) array([2, 3, 4, 9, 0]) # Trig thet = np . linspace ( 0 , np . pi , 3 ) print ( \"Theta =\" , thet ) print ( \"Sin(theta) =\" , np . sin ( thet )) print ( \"Cos(theta) =\" , np . cos ( thet )) print ( \" \\n Inverse tri \\n \" ) print ( \"Theta =\" , thet ) print ( \"arcSin(theta) =\" , np . arcsin ( thet )) print ( \"arcCos(theta) =\" , np . arccos ( thet )) Theta = [0. 1.57079633 3.14159265] Sin(theta) = [0.0000000e+00 1.0000000e+00 1.2246468e-16] Cos(theta) = [ 1.000000e+00 6.123234e-17 -1.000000e+00] Inverse tri Theta = [0. 1.57079633 3.14159265] arcSin(theta) = [ 0. nan nan] arcCos(theta) = [1.57079633 nan nan] /tmp/ipykernel_22625/2951825817.py:9: RuntimeWarning: invalid value encountered in arcsin print(\"arcSin(theta) =\",np.arcsin(thet)) /tmp/ipykernel_22625/2951825817.py:10: RuntimeWarning: invalid value encountered in arccos print(\"arcCos(theta) =\",np.arccos(thet)) # Logarthm and Exponents x = [ 1 , 2 , 3 ] print ( \"x=\" , x ) print ( \"e^x=\" , np . exp ( x )) print ( \"2^x=\" , np . exp2 ( x )) print ( \"3^x=\" , np . power ( 3 , x )) x= [1, 2, 3] e^x= [ 2.71828183 7.3890561 20.08553692] 2^x= [2. 4. 8.] 3^x= [ 3 9 27]","title":"Array arithmetic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#specialized-ufuncs","text":"NumPy has many more ufuncs available, including hyperbolic trig functions, bitwise arithmetic, comparison operators, conversions from radians to degrees, rounding and remainders, and much more. A look through the NumPy documentation reveals a lot of interesting functionality. Another excellent source for more specialized and obscure ufuncs is the submodule scipy.special. If you want to compute some obscure mathematical function on your data, chances are it is implemented in scipy.special. There are far too many functions to list them all, but the following snippet shows a couple that might come up in a statistics context from scipy import special #Gamma functions (generalized factorials) and related functions x = [ 1 , 5 , 10 ] print ( \"gamma(x)=\" , special . gamma ( x )) print ( \"ln|gamma(x)| =\" , special . gammaln ( x )) print ( \"beta(x, 2)=\" , special . beta ( x , 2 )) gamma(x)= [1.0000e+00 2.4000e+01 3.6288e+05] ln|gamma(x)| = [ 0. 3.17805383 12.80182748] beta(x, 2)= [0.5 0.03333333 0.00909091] # Error function (integral of Gaussian) # its complement, and its inverse x = np . array ([ 0 , 0.3 , 0.7 , 1.0 ]) print ( \"erf(x) =\" , special . erf ( x )) print ( \"erfc(x) =\" , special . erfc ( x )) print ( \"erfinv(x) =\" , special . erfinv ( x )) erf(x) = [0. 0.32862676 0.67780119 0.84270079] erfc(x) = [1. 0.67137324 0.32219881 0.15729921] erfinv(x) = [0. 0.27246271 0.73286908 inf]","title":"Specialized ufuncs"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#advanced-ufunc-features","text":"","title":"Advanced Ufunc Features"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#specifying-output","text":"For large calculations, it is sometimes useful to be able to specify the array where the result of the calculation will be stored. Rather than creating a temporary array, you can use this to write computation results directly to the memory location where you\u2019d like them to be. # THis can be done using the out argument of the function x = np . arange ( 5 ) y = np . empty ( 5 ) np . multiply ( x , 10 , out = y ) print ( y ) [ 0. 10. 20. 30. 40.] # This can be done with array views y = np . zeros ( 10 ) np . power ( 2 , x , out = y [:: 2 ]) print ( y ) [ 1. 0. 2. 0. 4. 0. 8. 0. 16. 0.] np . power ( 2 , x , out = 2 ** x ) array([ 1, 2, 4, 8, 16])","title":"Specifying output"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#aggregates","text":"For binary ufuncs, there are some interesting aggregates that can be computed directly from the object. For example, if we\u2019d like to reduce an array with a particular operation, we can use the reduce method of any ufunc. A reduce repeatedly applies a given operation to the elements of an array until only a single result remains. # Calling reduce on the add ufunc x = np . arange ( 1 , 6 ) np . add . reduce ( x ) 15 x = np . arange ( 1 , 6 ) np . sum ( x ) 15 np . multiply ( x ) TypeError: multiply() takes from 2 to 3 positional arguments but 1 were given np . add ( x ) TypeError: add() takes from 2 to 3 positional arguments but 1 were given np . multiply . reduce ( x ) 120 # To store all the imtermmediate results of the computation we can use accumulate np . add . accumulate ( x ) array([ 1, 3, 6, 10, 15]) x array([1, 2, 3, 4, 5]) Note that for these particular cases, there are dedicated NumPy functions to compute the results (np.sum, np.prod, np.cumsum, np.cumprod), The ufunc.at and ufunc.reduceat methods","title":"Aggregates"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#outer-products","text":"Finally, any ufunc can compute the output of all pairs of two different inputs using the outer method. np . multiply . outer ( x , x ) array([[ 1, 2, 3, 4, 5], [ 2, 4, 6, 8, 10], [ 3, 6, 9, 12, 15], [ 4, 8, 12, 16, 20], [ 5, 10, 15, 20, 25]]) Another extremely useful feature of ufuncs is the ability to operate between arrays of different sizes and shapes, a set of operations known as broadcasting.","title":"Outer products"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 03 - Aggregations: Min, Max, and Everything in Between \u00b6 Aggregations: Min, Max, and Everything in Between Multidimensional aggregates Some aggregation functions Example: What Is the Average Height of US Presidents? Aggregations: Min, Max, and Everything in Between \u00b6 Often when you are faced with a large amount of data, a first step is to compute sum\u2010 mary statistics for the data in question. Perhaps the most common summary statistics are the mean and standard deviation, which allow you to summarize the \u201ctypical\u201d val\u2010 ues in a dataset, but other aggregates are useful as well (the sum, product, median, minimum and maximum, quantiles, etc.). l = np . random . random ( 100 ) sum ( l ) 47.51294159911191 np . sum ( l ) 47.5129415991119 big_array = np . random . rand ( 100000 ) % timeit sum ( big_array ) % timeit np . sum ( big_array ) 6.73 ms \u00b1 88.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 31.5 \u00b5s \u00b1 326 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Be careful, though: the sum function and the np.sum function are not identical, which can sometimes lead to confusion! In particular, their optional arguments have differ\u2010 ent meanings, and np.sum is aware of multiple array dimensions min ( big_array ), max ( big_array ) (1.4444103570987465e-06, 0.9999881721555508) np . min ( big_array ), np . max ( big_array ) (1.4444103570987465e-06, 0.9999881721555508) % timeit min ( big_array ) % timeit np . min ( big_array ) 5.44 ms \u00b1 49.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 35.5 \u00b5s \u00b1 64.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Multidimensional aggregates \u00b6 m = np . random . random (( 3 , 4 )) print ( m ) [[0.15322668 0.10058762 0.85504471 0.19779527] [0.24515716 0.61526756 0.9677193 0.0045308 ] [0.57826711 0.49512073 0.68039294 0.43271134]] m . sum () 5.325821234912031 # Aggregation functions take an additional argument specifying the axis along which the aggregate is computed. m . min ( axis = 0 ) array([0.15322668, 0.10058762, 0.68039294, 0.0045308 ]) np . max ( m ) 0.9677193034993363 m . max ( axis = 0 ) array([0.57826711, 0.61526756, 0.9677193 , 0.43271134]) m . min ( axis = 1 ) array([0.10058762, 0.0045308 , 0.43271134]) The way the axis is specified here can be confusing to users coming from other lan\u2010 guages. The axis keyword specifies the dimension of the array that will be collapsed, rather than the dimension that will be returned. So specifying axis=0 means that the The way the axis is specified here can be confusing to users coming from other lan\u2010 guages. The axis keyword specifies the dimension of the array that will be collapsed, rather than the dimension that will be returned. So specifying axis=0 means that the first axis will be collapsed: for two-dimensional arrays, this means that values within each column will be aggregated. Some aggregation functions \u00b6 Example: What Is the Average Height of US Presidents? \u00b6 Aggregates available in NumPy can be extremely useful for summarizing a set of val\u2010 ues. As a simple example, let\u2019s consider the heights of all US presidents. ! head - 4 ../ data / president_heights . csv order,name,height(cm) 1,George Washington,189 2,John Adams,170 3,Thomas Jefferson,189 import pandas as pd data = pd . read_csv ( \"../data/president_heights.csv\" ) height = np . array ( data [ 'height(cm)' ]) print ( height ) [189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173 174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183 177 185 188 188 182 185] data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order name height(cm) 0 1 George Washington 189 1 2 John Adams 170 2 3 Thomas Jefferson 189 3 4 James Madison 163 4 5 James Monroe 183 print ( f \"Mean Height = \" , np . mean ( height )) print ( f \"St.dv Height = \" , np . std ( height )) Mean Height = 179.73809523809524 St.dv Height = 6.931843442745892 print ( f \"Mean Height = \" , height . mean ()) print ( f \"St.dv Height = \" , height . std ()) Mean Height = 179.73809523809524 St.dv Height = 6.931843442745892 print ( f \"Max Height = \" , np . max ( height )) print ( f \"Min Height = \" , np . min ( height )) Max Height = 193 Min Height = 163 print ( f \"25th precentile Height = \" , np . percentile ( height , 25 )) print ( f \"Median Height = \" , np . median ( height )) print ( f \"75th Percentile = \" , np . percentile ( height , 75 )) 25th precentile Height = 174.25 Median Height = 182.0 75th Percentile = 183.0 We see that the median height of US presidents is 182 cm, or just shy of six feet % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . hist ( heights ) plt . title ( 'Height Distribution of US Presidents' ) plt . xlabel ( 'height (cm)' ) plt . ylabel ( 'number' );","title":"03 Aggregation Min Max"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#03-aggregations-min-max-and-everything-in-between","text":"Aggregations: Min, Max, and Everything in Between Multidimensional aggregates Some aggregation functions Example: What Is the Average Height of US Presidents?","title":"03 - Aggregations: Min, Max, and Everything in Between"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#aggregations-min-max-and-everything-in-between","text":"Often when you are faced with a large amount of data, a first step is to compute sum\u2010 mary statistics for the data in question. Perhaps the most common summary statistics are the mean and standard deviation, which allow you to summarize the \u201ctypical\u201d val\u2010 ues in a dataset, but other aggregates are useful as well (the sum, product, median, minimum and maximum, quantiles, etc.). l = np . random . random ( 100 ) sum ( l ) 47.51294159911191 np . sum ( l ) 47.5129415991119 big_array = np . random . rand ( 100000 ) % timeit sum ( big_array ) % timeit np . sum ( big_array ) 6.73 ms \u00b1 88.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 31.5 \u00b5s \u00b1 326 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) Be careful, though: the sum function and the np.sum function are not identical, which can sometimes lead to confusion! In particular, their optional arguments have differ\u2010 ent meanings, and np.sum is aware of multiple array dimensions min ( big_array ), max ( big_array ) (1.4444103570987465e-06, 0.9999881721555508) np . min ( big_array ), np . max ( big_array ) (1.4444103570987465e-06, 0.9999881721555508) % timeit min ( big_array ) % timeit np . min ( big_array ) 5.44 ms \u00b1 49.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 35.5 \u00b5s \u00b1 64.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)","title":"Aggregations: Min, Max, and Everything in Between"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#multidimensional-aggregates","text":"m = np . random . random (( 3 , 4 )) print ( m ) [[0.15322668 0.10058762 0.85504471 0.19779527] [0.24515716 0.61526756 0.9677193 0.0045308 ] [0.57826711 0.49512073 0.68039294 0.43271134]] m . sum () 5.325821234912031 # Aggregation functions take an additional argument specifying the axis along which the aggregate is computed. m . min ( axis = 0 ) array([0.15322668, 0.10058762, 0.68039294, 0.0045308 ]) np . max ( m ) 0.9677193034993363 m . max ( axis = 0 ) array([0.57826711, 0.61526756, 0.9677193 , 0.43271134]) m . min ( axis = 1 ) array([0.10058762, 0.0045308 , 0.43271134]) The way the axis is specified here can be confusing to users coming from other lan\u2010 guages. The axis keyword specifies the dimension of the array that will be collapsed, rather than the dimension that will be returned. So specifying axis=0 means that the The way the axis is specified here can be confusing to users coming from other lan\u2010 guages. The axis keyword specifies the dimension of the array that will be collapsed, rather than the dimension that will be returned. So specifying axis=0 means that the first axis will be collapsed: for two-dimensional arrays, this means that values within each column will be aggregated.","title":"Multidimensional aggregates"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#some-aggregation-functions","text":"","title":"Some aggregation functions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/#example-what-is-the-average-height-of-us-presidents","text":"Aggregates available in NumPy can be extremely useful for summarizing a set of val\u2010 ues. As a simple example, let\u2019s consider the heights of all US presidents. ! head - 4 ../ data / president_heights . csv order,name,height(cm) 1,George Washington,189 2,John Adams,170 3,Thomas Jefferson,189 import pandas as pd data = pd . read_csv ( \"../data/president_heights.csv\" ) height = np . array ( data [ 'height(cm)' ]) print ( height ) [189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173 174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183 177 185 188 188 182 185] data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order name height(cm) 0 1 George Washington 189 1 2 John Adams 170 2 3 Thomas Jefferson 189 3 4 James Madison 163 4 5 James Monroe 183 print ( f \"Mean Height = \" , np . mean ( height )) print ( f \"St.dv Height = \" , np . std ( height )) Mean Height = 179.73809523809524 St.dv Height = 6.931843442745892 print ( f \"Mean Height = \" , height . mean ()) print ( f \"St.dv Height = \" , height . std ()) Mean Height = 179.73809523809524 St.dv Height = 6.931843442745892 print ( f \"Max Height = \" , np . max ( height )) print ( f \"Min Height = \" , np . min ( height )) Max Height = 193 Min Height = 163 print ( f \"25th precentile Height = \" , np . percentile ( height , 25 )) print ( f \"Median Height = \" , np . median ( height )) print ( f \"75th Percentile = \" , np . percentile ( height , 75 )) 25th precentile Height = 174.25 Median Height = 182.0 75th Percentile = 183.0 We see that the median height of US presidents is 182 cm, or just shy of six feet % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . hist ( heights ) plt . title ( 'Height Distribution of US Presidents' ) plt . xlabel ( 'height (cm)' ) plt . ylabel ( 'number' );","title":"Example: What Is the Average Height of US Presidents?"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 04 - Computation on Arrays: Broadcasting \u00b6 Computation on Arrays: Broadcasting Introducing Broadcasting Rules of Broadcasting Broadcasting in Practice Centering an array Plotting a two-dimensional function Computation on Arrays: Broadcasting \u00b6 We saw in the previous section how NumPy\u2019s universal functions can be used to vec\u2010 torize operations and thereby remove slow Python loops. Another means of vectoriz\u2010 ing operations is to use NumPy\u2019s broadcasting functionality. Broadcasting is simply a set of rules for applying binary ufuncs (addition, subtraction, multiplication, etc.) on arrays of different sizes. Introducing Broadcasting \u00b6 Recall that for arrays of the same size, binary operations are performed on an element-by-element basis: import numpy as np a = np . array ([ 0 , 2 , 3 , 4 , 5 ]) b = np . array ([ 7 , 8 , 9 , 9 , 6 ]) a + b array([ 7, 10, 12, 13, 11]) Broadcasting allows these types of binary operations to be performed on arrays of dif\u2010 ferent sizes\u2014for example, we can just as easily add a scalar (think of it as a zero- dimensional array) to an array a + 5 array([ 5, 7, 8, 9, 10]) We can think of this as an operation that stretches or duplicates the value 5 into the array [5, 5, 5] , and adds the results. The advantage of NumPy\u2019s broadcasting is that this duplication of values does not actually take place, but it is a useful mental model as we think about broadcasting. _ b - 9 array([-2, -1, 0, 0, -3]) b * 10 array([70, 80, 90, 90, 60]) a / 3 array([0. , 0.66666667, 1. , 1.33333333, 1.66666667]) m = np . ones (( 3 , 3 )) m array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) m + a ValueError: operands could not be broadcast together with shapes (3,3) (5,) m = np . ones (( 5 , 5 )) m array([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) m + a array([[1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.]]) Here the one-dimensional array a is stretched, or broadcast, across the second dimension in order to match the shape of M. # Broadcasting of both arrays a = np . arange ( 3 ) b = np . arange ( 3 )[:, np . newaxis ] a array([0, 1, 2]) b array([[0], [1], [2]]) a + b array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) b + a array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) Here the one-dimensional array a is stretched, or broadcast, across the second dimension in order to match the shape of M. Rules of Broadcasting \u00b6 Broadcasting in NumPy follows a strict set of rules to determine the interaction between the two arrays: * Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side. * Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape. * Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised. m = np . ones (( 2 , 3 )) a = np . arange ( 3 ) m , a (array([[1., 1., 1.], [1., 1., 1.]]), array([0, 1, 2])) m + a array([[1., 2., 3.], [1., 2., 3.]]) a = np . arange ( 3 ) . reshape (( 3 , 1 )) b = np . arange ( 3 ) a array([[0], [1], [2]]) b array([0, 1, 2]) a + b array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) Broadcasting in Practice \u00b6 Centering an array \u00b6 x = np . random . random (( 10 , 3 )) x array([[0.83823659, 0.53520822, 0.88885158], [0.89147897, 0.41948707, 0.42342134], [0.96876136, 0.46312611, 0.31711275], [0.47652192, 0.25392243, 0.25454173], [0.6239708 , 0.97644488, 0.97034953], [0.33707785, 0.24628063, 0.25686595], [0.12018414, 0.77360894, 0.19437752], [0.90201827, 0.18844719, 0.99512986], [0.84278929, 0.69803136, 0.53420956], [0.13620165, 0.90381885, 0.78541803]]) x_mean = x . mean ( 0 ) x_mean array([0.61372408, 0.54583757, 0.56202779]) # center the x arry by subtracting the mean (a boradcasting operation) x_centered = x - x_mean x_centered array([[ 0.22451251, -0.01062935, 0.3268238 ], [ 0.27775488, -0.1263505 , -0.13860644], [ 0.35503727, -0.08271145, -0.24491504], [-0.13720216, -0.29191514, -0.30748605], [ 0.01024672, 0.43060732, 0.40832174], [-0.27664623, -0.29955694, -0.30516184], [-0.49353995, 0.22777137, -0.36765026], [ 0.28829418, -0.35739038, 0.43310208], [ 0.22906521, 0.15219379, -0.02781823], [-0.47752244, 0.35798128, 0.22339024]]) Plotting a two-dimensional function \u00b6 One place that broadcasting is very useful is in displaying images based on two- dimensional functions. If we want to define a function z = f(x, y), broadcasting can be used to compute the function across the gri x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 50 )[:, np . newaxis ] x array([0. , 0.10204082, 0.20408163, 0.30612245, 0.40816327, 0.51020408, 0.6122449 , 0.71428571, 0.81632653, 0.91836735, 1.02040816, 1.12244898, 1.2244898 , 1.32653061, 1.42857143, 1.53061224, 1.63265306, 1.73469388, 1.83673469, 1.93877551, 2.04081633, 2.14285714, 2.24489796, 2.34693878, 2.44897959, 2.55102041, 2.65306122, 2.75510204, 2.85714286, 2.95918367, 3.06122449, 3.16326531, 3.26530612, 3.36734694, 3.46938776, 3.57142857, 3.67346939, 3.7755102 , 3.87755102, 3.97959184, 4.08163265, 4.18367347, 4.28571429, 4.3877551 , 4.48979592, 4.59183673, 4.69387755, 4.79591837, 4.89795918, 5. ]) y array([[0. ], [0.10204082], [0.20408163], [0.30612245], [0.40816327], [0.51020408], [0.6122449 ], [0.71428571], [0.81632653], [0.91836735], [1.02040816], [1.12244898], [1.2244898 ], [1.32653061], [1.42857143], [1.53061224], [1.63265306], [1.73469388], [1.83673469], [1.93877551], [2.04081633], [2.14285714], [2.24489796], [2.34693878], [2.44897959], [2.55102041], [2.65306122], [2.75510204], [2.85714286], [2.95918367], [3.06122449], [3.16326531], [3.26530612], [3.36734694], [3.46938776], [3.57142857], [3.67346939], [3.7755102 ], [3.87755102], [3.97959184], [4.08163265], [4.18367347], [4.28571429], [4.3877551 ], [4.48979592], [4.59183673], [4.69387755], [4.79591837], [4.89795918], [5. ]]) z = np . sin ( x ) ** 10 + np . cos ( 10 + x * y ) * np . cos ( x ) % matplotlib inline import matplotlib.pyplot as plt plt . imshow ( z , origin = 'lower' , extent = [ 0 , 5 , 0 , 5 ], cmap = 'viridis' ) plt . colorbar ();","title":"04 Computation on Arrays Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#04-computation-on-arrays-broadcasting","text":"Computation on Arrays: Broadcasting Introducing Broadcasting Rules of Broadcasting Broadcasting in Practice Centering an array Plotting a two-dimensional function","title":"04 - Computation on Arrays: Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#computation-on-arrays-broadcasting","text":"We saw in the previous section how NumPy\u2019s universal functions can be used to vec\u2010 torize operations and thereby remove slow Python loops. Another means of vectoriz\u2010 ing operations is to use NumPy\u2019s broadcasting functionality. Broadcasting is simply a set of rules for applying binary ufuncs (addition, subtraction, multiplication, etc.) on arrays of different sizes.","title":"Computation on Arrays: Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#introducing-broadcasting","text":"Recall that for arrays of the same size, binary operations are performed on an element-by-element basis: import numpy as np a = np . array ([ 0 , 2 , 3 , 4 , 5 ]) b = np . array ([ 7 , 8 , 9 , 9 , 6 ]) a + b array([ 7, 10, 12, 13, 11]) Broadcasting allows these types of binary operations to be performed on arrays of dif\u2010 ferent sizes\u2014for example, we can just as easily add a scalar (think of it as a zero- dimensional array) to an array a + 5 array([ 5, 7, 8, 9, 10]) We can think of this as an operation that stretches or duplicates the value 5 into the array [5, 5, 5] , and adds the results. The advantage of NumPy\u2019s broadcasting is that this duplication of values does not actually take place, but it is a useful mental model as we think about broadcasting. _ b - 9 array([-2, -1, 0, 0, -3]) b * 10 array([70, 80, 90, 90, 60]) a / 3 array([0. , 0.66666667, 1. , 1.33333333, 1.66666667]) m = np . ones (( 3 , 3 )) m array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) m + a ValueError: operands could not be broadcast together with shapes (3,3) (5,) m = np . ones (( 5 , 5 )) m array([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) m + a array([[1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.], [1., 3., 4., 5., 6.]]) Here the one-dimensional array a is stretched, or broadcast, across the second dimension in order to match the shape of M. # Broadcasting of both arrays a = np . arange ( 3 ) b = np . arange ( 3 )[:, np . newaxis ] a array([0, 1, 2]) b array([[0], [1], [2]]) a + b array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) b + a array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) Here the one-dimensional array a is stretched, or broadcast, across the second dimension in order to match the shape of M.","title":"Introducing Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#rules-of-broadcasting","text":"Broadcasting in NumPy follows a strict set of rules to determine the interaction between the two arrays: * Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side. * Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape. * Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised. m = np . ones (( 2 , 3 )) a = np . arange ( 3 ) m , a (array([[1., 1., 1.], [1., 1., 1.]]), array([0, 1, 2])) m + a array([[1., 2., 3.], [1., 2., 3.]]) a = np . arange ( 3 ) . reshape (( 3 , 1 )) b = np . arange ( 3 ) a array([[0], [1], [2]]) b array([0, 1, 2]) a + b array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])","title":"Rules of Broadcasting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#broadcasting-in-practice","text":"","title":"Broadcasting in Practice"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#centering-an-array","text":"x = np . random . random (( 10 , 3 )) x array([[0.83823659, 0.53520822, 0.88885158], [0.89147897, 0.41948707, 0.42342134], [0.96876136, 0.46312611, 0.31711275], [0.47652192, 0.25392243, 0.25454173], [0.6239708 , 0.97644488, 0.97034953], [0.33707785, 0.24628063, 0.25686595], [0.12018414, 0.77360894, 0.19437752], [0.90201827, 0.18844719, 0.99512986], [0.84278929, 0.69803136, 0.53420956], [0.13620165, 0.90381885, 0.78541803]]) x_mean = x . mean ( 0 ) x_mean array([0.61372408, 0.54583757, 0.56202779]) # center the x arry by subtracting the mean (a boradcasting operation) x_centered = x - x_mean x_centered array([[ 0.22451251, -0.01062935, 0.3268238 ], [ 0.27775488, -0.1263505 , -0.13860644], [ 0.35503727, -0.08271145, -0.24491504], [-0.13720216, -0.29191514, -0.30748605], [ 0.01024672, 0.43060732, 0.40832174], [-0.27664623, -0.29955694, -0.30516184], [-0.49353995, 0.22777137, -0.36765026], [ 0.28829418, -0.35739038, 0.43310208], [ 0.22906521, 0.15219379, -0.02781823], [-0.47752244, 0.35798128, 0.22339024]])","title":"Centering an array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/#plotting-a-two-dimensional-function","text":"One place that broadcasting is very useful is in displaying images based on two- dimensional functions. If we want to define a function z = f(x, y), broadcasting can be used to compute the function across the gri x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 50 )[:, np . newaxis ] x array([0. , 0.10204082, 0.20408163, 0.30612245, 0.40816327, 0.51020408, 0.6122449 , 0.71428571, 0.81632653, 0.91836735, 1.02040816, 1.12244898, 1.2244898 , 1.32653061, 1.42857143, 1.53061224, 1.63265306, 1.73469388, 1.83673469, 1.93877551, 2.04081633, 2.14285714, 2.24489796, 2.34693878, 2.44897959, 2.55102041, 2.65306122, 2.75510204, 2.85714286, 2.95918367, 3.06122449, 3.16326531, 3.26530612, 3.36734694, 3.46938776, 3.57142857, 3.67346939, 3.7755102 , 3.87755102, 3.97959184, 4.08163265, 4.18367347, 4.28571429, 4.3877551 , 4.48979592, 4.59183673, 4.69387755, 4.79591837, 4.89795918, 5. ]) y array([[0. ], [0.10204082], [0.20408163], [0.30612245], [0.40816327], [0.51020408], [0.6122449 ], [0.71428571], [0.81632653], [0.91836735], [1.02040816], [1.12244898], [1.2244898 ], [1.32653061], [1.42857143], [1.53061224], [1.63265306], [1.73469388], [1.83673469], [1.93877551], [2.04081633], [2.14285714], [2.24489796], [2.34693878], [2.44897959], [2.55102041], [2.65306122], [2.75510204], [2.85714286], [2.95918367], [3.06122449], [3.16326531], [3.26530612], [3.36734694], [3.46938776], [3.57142857], [3.67346939], [3.7755102 ], [3.87755102], [3.97959184], [4.08163265], [4.18367347], [4.28571429], [4.3877551 ], [4.48979592], [4.59183673], [4.69387755], [4.79591837], [4.89795918], [5. ]]) z = np . sin ( x ) ** 10 + np . cos ( 10 + x * y ) * np . cos ( x ) % matplotlib inline import matplotlib.pyplot as plt plt . imshow ( z , origin = 'lower' , extent = [ 0 , 5 , 0 , 5 ], cmap = 'viridis' ) plt . colorbar ();","title":"Plotting a two-dimensional function"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 05 - Comparisons, Masks, and Boolean Logic \u00b6 Comparisons, Masks, and Boolean Logic Example: Counting Rainy Days Digging into the data Comparison Operators as ufuncs Working with Boolean Arrays Counting entries Boolean Operators Boolean Arrays as Masks Comparisons, Masks, and Boolean Logic \u00b6 This section covers the use of Boolean masks to examine and manipulate values within NumPy arrays. Masking comes up when you want to extract, modify, count, or otherwise manipulate values in an array based on some criterion: for example, you might wish to count all values greater than a certain value, or perhaps remove all out\u2010 liers that are above some threshold. In NumPy, Boolean masking is often the most efficient way to accomplish these types of tasks. Example: Counting Rainy Days \u00b6 import numpy as np import pandas as pd rainfall = pd . read_csv ( \"../data/Seattle2014.csv\" )[ 'PRCP' ] . values rainfall array([ 0, 41, 15, 0, 0, 3, 122, 97, 58, 43, 213, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 89, 216, 0, 23, 20, 0, 0, 0, 0, 0, 0, 51, 5, 183, 170, 46, 18, 94, 117, 264, 145, 152, 10, 30, 28, 25, 61, 130, 3, 0, 0, 0, 5, 191, 107, 165, 467, 30, 0, 323, 43, 188, 0, 0, 5, 69, 81, 277, 3, 0, 5, 0, 0, 0, 0, 0, 41, 36, 3, 221, 140, 0, 0, 0, 0, 25, 0, 46, 0, 0, 46, 0, 0, 0, 0, 0, 0, 5, 109, 185, 0, 137, 0, 51, 142, 89, 124, 0, 33, 69, 0, 0, 0, 0, 0, 333, 160, 51, 0, 0, 137, 20, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38, 0, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 64, 0, 5, 36, 13, 0, 8, 3, 0, 0, 0, 0, 0, 0, 18, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 193, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 5, 127, 216, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 13, 0, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 3, 183, 203, 43, 89, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 74, 0, 76, 71, 86, 0, 33, 150, 0, 117, 10, 320, 94, 41, 61, 15, 8, 127, 5, 254, 170, 0, 18, 109, 41, 48, 41, 0, 0, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 152, 5, 119, 13, 183, 3, 33, 343, 36, 0, 0, 0, 0, 8, 30, 74, 0, 91, 99, 130, 69, 0, 0, 0, 0, 0, 28, 130, 30, 196, 0, 0, 206, 53, 0, 0, 33, 41, 0, 0, 0]) inches = rainfall / 254 inches . shape (365,) % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . hist ( inches , 40 ); Digging into the data \u00b6 Comparison Operators as ufuncs \u00b6 x = np . array ([ 0 , 0 , 0 , 1 , 2 , 3 , 4 , 5 ]) x < 3 array([ True, True, True, True, True, False, False, False]) x > 2 array([False, False, False, False, False, True, True, True]) x >= 2 array([False, False, False, False, True, True, True, True]) x <= 3 array([ True, True, True, True, True, True, False, False]) x != 3 array([ True, True, True, True, True, False, True, True]) x == 3 array([False, False, False, False, False, True, False, False]) ( 2 * x ) == ( x ** 2 ) array([ True, True, True, False, True, False, False, False]) rng = np . random . RandomState ( 0 ) x = rng . randint ( 10 , size = ( 3 , 4 )) x array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) x < 4 array([[False, True, True, True], [False, False, True, False], [ True, False, False, False]]) Working with Boolean Arrays \u00b6 Counting entries \u00b6 print ( x ) [[5 0 3 3] [7 9 3 5] [2 4 7 6]] np . count_nonzero ( x < 4 ) 5 np . sum ( x > 3 ) 7 np . sum ( x < 3 , axis = 1 ) #checking each row array([1, 0, 1]) np . sum ( x < 3 , axis = 0 ) #checking each col array([1, 1, 0, 0]) If we\u2019re interested in quickly checking whether any or all the values are true, we can use (you guessed it) np.any() or np.all(): np . any ( x > 8 ) True np . any ( x < 5 ) True np . all ( x == 6 ) False np . all ( x < 10 ) True np . all ( x < 8 , axis = 1 ) array([ True, False, True]) Boolean Operators \u00b6 We\u2019ve already seen how we might count, say, all days with rain less than four inches, or all days with rain greater than two inches. But what if we want to know about all days with rain less than four inches and greater than one inch? This is accomplished through Python\u2019s bitwise logic operators, &, |, ^, and ~. Like with the standard arith\u2010 metic operators, NumPy overloads these as ufuncs that work element-wise on (usu\u2010 ally Boolean) arrays. np . sum (( inches > 0.5 ) & ( inches < 1 )) 29 inches > ( 0.5 & inches ) < 1 TypeError: ufunc 'bitwise_and' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'' print ( \"Number of days without rain: \" , np . sum ( inches == 0 )) print ( \"Number of days with rain: \" , np . sum ( inches != 0 )) print ( \"Days with more than 0.5 inches: \" , np . sum ( inches > 0.5 )) print ( \"Rainy days with <0.1 inches: \" , np . sum (( inches > 0 ) & ( inches < 0.2 ))) Number of days without rain: 215 Number of days with rain: 150 Days with more than 0.5 inches: 37 Rainy days with <0.1 inches: 75 Boolean Arrays as Masks \u00b6 In the preceding section, we looked at aggregates computed directly on Boolean arrays. A more powerful pattern is to use Boolean arrays as masks, to select particular subsets of the data themselves. x array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) x < 5 array([[False, True, True, True], [False, False, True, False], [ True, True, False, False]]) x [ x < 5 ] array([0, 3, 3, 3, 2, 4]) # Construct a mask of all rainy days rainy = ( inches > 0 ) # A construct of all summer days summer = ( np . arange ( 365 ) - 172 < 90 ) & ( np . arange ( 365 ) - 172 > 0 ) rainy array([False, True, True, False, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, True, False, True, True, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, False, True, False, False, False, False, False, True, True, True, True, True, False, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, False, True, True, False, False, False, False, False, True, True, True, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, True, True, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, False, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, False, True, True, True, True, False, False, False, False, False, True, True, True, True, False, False, True, True, False, False, True, True, False, False, False]) summer array([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]) print ( \"Median precip on rainy days in 2014 (inches): \" , np . median ( inches [ rainy ])) print ( \"Median precip on summber days in 2014 (inches): \" , np . median ( inches [ summer ])) print ( \"Maximum precip on summer in 2014 (inches): \" , np . max ( inches [ summer ])) print ( \"Median precip on summer in 2014 (inches) on non-summer rainy days: \" , np . median ( inches [ rainy & ~ summer ]) ) Median precip on rainy days in 2014 (inches): 0.19488188976377951 Median precip on summber days in 2014 (inches): 0.0 Maximum precip on summer in 2014 (inches): 0.8503937007874016 Median precip on summer in 2014 (inches) on non-summer rainy days: 0.20078740157480315 By combining Boolean operations, masking operations, and aggregates, we can very quickly answer these sorts of questions for our dataset. _ When you use & and | on integers, the expression operates on the bits of the element, applying the and or the or to the individual bits making up the number: When you use and or or, it\u2019s equivalent to asking Python to treat the object as a single Boolean entity. In Python, all nonzero integers will evaluate as True. Thus","title":"05 Comparisons  Masks and Boolean Logic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#05-comparisons-masks-and-boolean-logic","text":"Comparisons, Masks, and Boolean Logic Example: Counting Rainy Days Digging into the data Comparison Operators as ufuncs Working with Boolean Arrays Counting entries Boolean Operators Boolean Arrays as Masks","title":"05 - Comparisons, Masks, and Boolean Logic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#comparisons-masks-and-boolean-logic","text":"This section covers the use of Boolean masks to examine and manipulate values within NumPy arrays. Masking comes up when you want to extract, modify, count, or otherwise manipulate values in an array based on some criterion: for example, you might wish to count all values greater than a certain value, or perhaps remove all out\u2010 liers that are above some threshold. In NumPy, Boolean masking is often the most efficient way to accomplish these types of tasks.","title":"Comparisons, Masks, and Boolean Logic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#example-counting-rainy-days","text":"import numpy as np import pandas as pd rainfall = pd . read_csv ( \"../data/Seattle2014.csv\" )[ 'PRCP' ] . values rainfall array([ 0, 41, 15, 0, 0, 3, 122, 97, 58, 43, 213, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 89, 216, 0, 23, 20, 0, 0, 0, 0, 0, 0, 51, 5, 183, 170, 46, 18, 94, 117, 264, 145, 152, 10, 30, 28, 25, 61, 130, 3, 0, 0, 0, 5, 191, 107, 165, 467, 30, 0, 323, 43, 188, 0, 0, 5, 69, 81, 277, 3, 0, 5, 0, 0, 0, 0, 0, 41, 36, 3, 221, 140, 0, 0, 0, 0, 25, 0, 46, 0, 0, 46, 0, 0, 0, 0, 0, 0, 5, 109, 185, 0, 137, 0, 51, 142, 89, 124, 0, 33, 69, 0, 0, 0, 0, 0, 333, 160, 51, 0, 0, 137, 20, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38, 0, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 64, 0, 5, 36, 13, 0, 8, 3, 0, 0, 0, 0, 0, 0, 18, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 193, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 5, 127, 216, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 13, 0, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 3, 183, 203, 43, 89, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 74, 0, 76, 71, 86, 0, 33, 150, 0, 117, 10, 320, 94, 41, 61, 15, 8, 127, 5, 254, 170, 0, 18, 109, 41, 48, 41, 0, 0, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 152, 5, 119, 13, 183, 3, 33, 343, 36, 0, 0, 0, 0, 8, 30, 74, 0, 91, 99, 130, 69, 0, 0, 0, 0, 0, 28, 130, 30, 196, 0, 0, 206, 53, 0, 0, 33, 41, 0, 0, 0]) inches = rainfall / 254 inches . shape (365,) % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . hist ( inches , 40 );","title":"Example: Counting Rainy Days"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#digging-into-the-data","text":"","title":"Digging into the data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#comparison-operators-as-ufuncs","text":"x = np . array ([ 0 , 0 , 0 , 1 , 2 , 3 , 4 , 5 ]) x < 3 array([ True, True, True, True, True, False, False, False]) x > 2 array([False, False, False, False, False, True, True, True]) x >= 2 array([False, False, False, False, True, True, True, True]) x <= 3 array([ True, True, True, True, True, True, False, False]) x != 3 array([ True, True, True, True, True, False, True, True]) x == 3 array([False, False, False, False, False, True, False, False]) ( 2 * x ) == ( x ** 2 ) array([ True, True, True, False, True, False, False, False]) rng = np . random . RandomState ( 0 ) x = rng . randint ( 10 , size = ( 3 , 4 )) x array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) x < 4 array([[False, True, True, True], [False, False, True, False], [ True, False, False, False]])","title":"Comparison Operators as ufuncs"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#working-with-boolean-arrays","text":"","title":"Working with Boolean Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#counting-entries","text":"print ( x ) [[5 0 3 3] [7 9 3 5] [2 4 7 6]] np . count_nonzero ( x < 4 ) 5 np . sum ( x > 3 ) 7 np . sum ( x < 3 , axis = 1 ) #checking each row array([1, 0, 1]) np . sum ( x < 3 , axis = 0 ) #checking each col array([1, 1, 0, 0]) If we\u2019re interested in quickly checking whether any or all the values are true, we can use (you guessed it) np.any() or np.all(): np . any ( x > 8 ) True np . any ( x < 5 ) True np . all ( x == 6 ) False np . all ( x < 10 ) True np . all ( x < 8 , axis = 1 ) array([ True, False, True])","title":"Counting entries"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#boolean-operators","text":"We\u2019ve already seen how we might count, say, all days with rain less than four inches, or all days with rain greater than two inches. But what if we want to know about all days with rain less than four inches and greater than one inch? This is accomplished through Python\u2019s bitwise logic operators, &, |, ^, and ~. Like with the standard arith\u2010 metic operators, NumPy overloads these as ufuncs that work element-wise on (usu\u2010 ally Boolean) arrays. np . sum (( inches > 0.5 ) & ( inches < 1 )) 29 inches > ( 0.5 & inches ) < 1 TypeError: ufunc 'bitwise_and' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'' print ( \"Number of days without rain: \" , np . sum ( inches == 0 )) print ( \"Number of days with rain: \" , np . sum ( inches != 0 )) print ( \"Days with more than 0.5 inches: \" , np . sum ( inches > 0.5 )) print ( \"Rainy days with <0.1 inches: \" , np . sum (( inches > 0 ) & ( inches < 0.2 ))) Number of days without rain: 215 Number of days with rain: 150 Days with more than 0.5 inches: 37 Rainy days with <0.1 inches: 75","title":"Boolean Operators"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/#boolean-arrays-as-masks","text":"In the preceding section, we looked at aggregates computed directly on Boolean arrays. A more powerful pattern is to use Boolean arrays as masks, to select particular subsets of the data themselves. x array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) x < 5 array([[False, True, True, True], [False, False, True, False], [ True, True, False, False]]) x [ x < 5 ] array([0, 3, 3, 3, 2, 4]) # Construct a mask of all rainy days rainy = ( inches > 0 ) # A construct of all summer days summer = ( np . arange ( 365 ) - 172 < 90 ) & ( np . arange ( 365 ) - 172 > 0 ) rainy array([False, True, True, False, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, True, False, True, True, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, False, True, False, False, False, False, False, True, True, True, True, True, False, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, False, True, True, False, False, False, False, False, True, True, True, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, True, True, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, False, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, False, True, True, True, True, False, False, False, False, False, True, True, True, True, False, False, True, True, False, False, True, True, False, False, False]) summer array([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]) print ( \"Median precip on rainy days in 2014 (inches): \" , np . median ( inches [ rainy ])) print ( \"Median precip on summber days in 2014 (inches): \" , np . median ( inches [ summer ])) print ( \"Maximum precip on summer in 2014 (inches): \" , np . max ( inches [ summer ])) print ( \"Median precip on summer in 2014 (inches) on non-summer rainy days: \" , np . median ( inches [ rainy & ~ summer ]) ) Median precip on rainy days in 2014 (inches): 0.19488188976377951 Median precip on summber days in 2014 (inches): 0.0 Maximum precip on summer in 2014 (inches): 0.8503937007874016 Median precip on summer in 2014 (inches) on non-summer rainy days: 0.20078740157480315 By combining Boolean operations, masking operations, and aggregates, we can very quickly answer these sorts of questions for our dataset. _ When you use & and | on integers, the expression operates on the bits of the element, applying the and or the or to the individual bits making up the number: When you use and or or, it\u2019s equivalent to asking Python to treat the object as a single Boolean entity. In Python, all nonzero integers will evaluate as True. Thus","title":"Boolean Arrays as Masks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 06 - Comparisons, Masks, and Boolean Logic \u00b6 Fancy Indexing Exploring Fancy Indexing Combined Indexing Example: Selecting Random Points Modifying Values with Fancy Indexing Exmple Binning Data Fancy Indexing \u00b6 In the previous sections, we saw how to access and modify portions of arrays using simple indices (e.g., arr[0] ), slices (e.g., arr[:5] ), and Boolean masks (e.g., arr [arr> 0] ). In this section, we\u2019ll look at another style of array indexing, known as fancy indexing. Fancy indexing is like the simple indexing we\u2019ve already seen, but we pass arrays of indices in place of single scalars. This allows us to very quickly access and modify complicated subsets of an array\u2019s values. Exploring Fancy Indexing \u00b6 Fancy indexing is conceptually simple: it means passing an array of indices to access multiple array elements at once. import numpy as np rand = np . random . RandomState ( 42 ) x = rand . randint ( 100 , size = 10 ) print ( x ) [51 92 14 71 60 20 82 86 74 74] # three different elements [ x [ 3 ], x [ 7 ], x [ 2 ]] [71, 86, 14] # alternatively, we could do like this ind = [ 3 , 7 , 4 ] x [ ind ] array([71, 86, 60]) With fancy indexing, the shape of the result reflects the shape of the index arrays rather than the shape of the array being indexed: ind = np . array ([[ 3 , 7 ],[ 4 , 5 ]]) x [ ind ] array([[71, 86], [60, 20]]) # Multi-dimensional fancy indexing x = np . arange ( 12 ) . reshape (( 3 , 4 )) x array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) row = np . array ([ 0 , 1 , 2 ]) col = np . array ([ 2 , 1 , 3 ]) x [ row , col ] array([ 2, 5, 11]) x [ row [:, np . newaxis ], col ] array([[ 2, 1, 3], [ 6, 5, 7], [10, 9, 11]]) ##Here, each row value is matched with each column vector, exactly as we saw in broad\u2010 ##casting of arithmetic operations row [:, np . newaxis ] array([[0], [1], [2]]) row [:, np . newaxis ] * col array([[0, 0, 0], [2, 1, 3], [4, 2, 6]]) ## It is always important to remember with fancy indexing that the return value reflects ##the broadcasted shape of the indices, rather than the shape of the array being indexed. Combined Indexing \u00b6 For even more powerful operations, fancy indexing can be combined with the other indexing schemes x array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) # Combing fancy and simple index x [ 2 ,[ 2 , 0 , 1 ]] array([10, 8, 9]) # Combining fancy indexing with masking mask = np . array ([ 1 , 0 , 1 , 0 ], dtype = bool ) x [ row [:, np . newaxis ], mask ] array([[ 0, 2], [ 4, 6], [ 8, 10]]) Example: Selecting Random Points \u00b6 One common use of fancy indexing is the selection of subsets of rows from a matrix. For example, we might have an N by D matrix representing N points in D dimen\u2010 sions, such as the following points drawn from a two-dimensional normal distribu\u2010 tion mean = [ 0 , 0 ] conv = [[ 1 , 2 ], [ 2 , 5 ]] x = rand . multivariate_normal ( mean , conv , 100 ) x . shape (100, 2) print ( x ) [[-0.644508 -0.46220608] [ 0.7376352 1.21236921] [ 0.88151763 1.12795177] [ 2.04998983 5.97778598] [-0.1711348 -2.06258746] [ 0.67956979 0.83705124] [ 1.46860232 1.22961093] [ 0.35282131 1.49875397] [-2.51552505 -5.64629995] [ 0.0843329 -0.3543059 ] [ 0.19199272 1.48901291] [-0.02566217 -0.74987887] [ 1.00569227 2.25287315] [ 0.49514263 1.18939673] [ 0.0629872 0.57349278] [ 0.75093031 2.99487004] [-3.0236127 -6.00766046] [-0.53943081 -0.3478899 ] [ 1.53817376 1.99973464] [-0.50886808 -1.81099656] [ 1.58115602 2.86410319] [ 0.99305043 2.54294059] [-0.87753796 -1.15767204] [-1.11518048 -1.87508012] [ 0.4299908 0.36324254] [ 0.97253528 3.53815717] [ 0.32124996 0.33137032] [-0.74618649 -2.77366681] [-0.88473953 -1.81495444] [ 0.98783862 2.30280401] [-1.2033623 -2.04402725] [-1.51101746 -3.2818741 ] [-2.76337717 -7.66760648] [ 0.39158553 0.87949228] [ 0.91181024 3.32968944] [-0.84202629 -2.01226547] [ 1.06586877 0.95500019] [ 0.44457363 1.87828298] [ 0.35936721 0.40554974] [-0.90649669 -0.93486441] [-0.35790389 -0.52363012] [-1.33461668 -3.03203218] [ 0.02815138 0.79654924] [ 0.37785618 0.51409383] [-1.06505097 -2.88726779] [ 2.32083881 5.97698647] [ 0.47605744 0.83634485] [-0.35490984 -1.03657119] [ 0.57532883 -0.79997124] [ 0.33399913 2.32597923] [ 0.6575612 -0.22389518] [ 1.3707365 2.2348831 ] [ 0.07099548 -0.29685467] [ 0.6074983 1.47089233] [-0.34226126 -1.10666237] [ 0.69226246 1.21504303] [-0.31112937 -0.75912097] [-0.26888327 -1.89366817] [ 0.42044896 1.85189522] [ 0.21115245 2.00781492] [-1.83106042 -2.91352836] [ 0.7841796 1.97640753] [ 0.10259314 1.24690575] [-1.91100558 -3.66800923] [ 0.13143756 -0.07833855] [-0.1317045 -1.64159158] [-0.14547282 -1.34125678] [-0.51172373 -1.40960773] [ 0.69758045 0.72563649] [ 0.11677083 0.88385162] [-1.16586444 -2.24482237] [-2.23176235 -2.63958101] [ 0.37857234 0.69112594] [ 0.87475323 3.400675 ] [-0.86864365 -3.03568353] [-1.03637857 -1.18469125] [-0.53334959 -0.37039911] [ 0.30414557 -0.5828419 ] [-1.47656656 -2.13046298] [-0.31332021 -1.7895623 ] [ 1.12659538 1.49627535] [-1.19675798 -1.51633442] [-0.75210154 -0.79770535] [ 0.74577693 1.95834451] [ 1.56094354 2.9330816 ] [-0.72009966 -1.99780959] [-1.32319163 -2.61218347] [-2.56215914 -6.08410838] [ 1.31256297 3.13143269] [ 0.51575983 2.30284639] [ 0.01374713 -0.11539344] [-0.16863279 0.39422355] [ 0.12065651 1.13236323] [-0.83504984 -2.38632016] [ 1.05185885 1.98418223] [-0.69144553 -1.56919875] [-1.2567603 -1.125898 ] [ 0.09619333 -0.64335574] [-0.99658689 -2.35038099] [-1.21405259 -1.77693724]] % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . scatter ( x [:, 0 ], x [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f507340ffd0> Let\u2019s use fancy indexing to select 20 random points. We\u2019ll do this by first choosing 20 random indices with no repeats, and use these indices to select a portion of the origi\u2010 nal array indices = np . random . choice ( x . shape [ 0 ], 20 , replace = False ) indices array([80, 87, 70, 77, 18, 56, 41, 42, 84, 9, 85, 61, 6, 71, 24, 69, 8, 22, 23, 54]) selection = x [ indices ] selection array([[ 1.12659538, 1.49627535], [-2.56215914, -6.08410838], [-1.16586444, -2.24482237], [ 0.30414557, -0.5828419 ], [ 1.53817376, 1.99973464], [-0.31112937, -0.75912097], [-1.33461668, -3.03203218], [ 0.02815138, 0.79654924], [ 1.56094354, 2.9330816 ], [ 0.0843329 , -0.3543059 ], [-0.72009966, -1.99780959], [ 0.7841796 , 1.97640753], [ 1.46860232, 1.22961093], [-2.23176235, -2.63958101], [ 0.4299908 , 0.36324254], [ 0.11677083, 0.88385162], [-2.51552505, -5.64629995], [-0.87753796, -1.15767204], [-1.11518048, -1.87508012], [-0.34226126, -1.10666237]]) plt . scatter ( x [:, 0 ], x [:, 1 ], alpha = 0.3 ) plt . scatter ( selection [:, 0 ], selection [:, 1 ], facecolor = 'none' , s = 200 ); This sort of strategy is often used to quickly partition datasets, as is often needed in train/test splitting for validation of statistical models Modifying Values with Fancy Indexing \u00b6 Just as fancy indexing can be used to access parts of an array, it can also be used to modify parts of an array. For example, imagine we have an array of indices and we\u2019d like to set the corresponding items in an array to some value: x = np . arange ( 10 ) i = np . array ([ 2 , 1 , 8 , 4 ]) x [ i ] =- 9 x array([ 0, -9, -9, 3, -9, 5, 6, 7, -9, 9]) We can use any assignment-type operator for this Notice, though, that repeated indices with these operations can cause some poten\u2010 tially unexpected results x [ i ] -= 10 x array([ 0, -19, -19, 3, -19, 5, 6, 7, -19, 9]) x = np . zeros ( 10 ) x [[ 0 , 0 ]] = [ 4 , 6 ] x array([6., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) Where did the 4 go? The result of this operation is to first assign x[0] = 4 , followed by x[0] = 6 . The result, of course, is that x[0] contains the value 6. i = [ 2 , 3 , 3 , 4 , 4 , 4 ] x [ i ] += 1 x array([6., 0., 1., 1., 1., 0., 0., 0., 0., 0.]) You might expect that x[3] would contain the value 2, and x[4] would contain the value 3, as this is how many times each index is repeated. Why is this not the case? Conceptually, this is because x[i] += 1 is meant as a shorthand of x[i] = x[i] + 1. x[i] + 1 is evaluated, and then the result is assigned to the indices in x. With this in mind, it is not the augmentation that happens multiple times, but the assignment, which leads to the rather nonintuitive results. So what if you want the other behavior where the operation is repeated? For this, you can use the at() method of ufuncs (available since NumPy 1.8), and do the following x = np . zeros ( 10 ) np . add . at ( x , i , 1 ) x array([0., 0., 1., 2., 3., 0., 0., 0., 0., 0.]) The at() method does an in-place application of the given operator at the specified indices (here, i) with the specified value (here, 1). Another method that is similar in spirit is the reduceat() method of ufuncs, which you can read about in the NumPy documentation. Exmple Binning Data \u00b6 np . random . seed ( 42 ) x = np . random . randn ( 100 ) # compute a histogram by hand bins = np . linspace ( - 5 , 5 , 20 ) counts = np . zeros_like ( bins ) # find the approperiate bin for each x i = np . searchsorted ( bins , x ) # add 1 to each of these bins np . add . at ( counts , i , 1 ) # plot the results plt . plot ( bins , counts ) #linestyle='steps' Of course, it would be silly to have to do this each time you want to plot a histogram. This is why Matplotlib provides the plt.hist() routine, which does the same in a single line: plt . hist ( x , bins , histtype = 'step' ); print ( \"NumPy routine\" ) % timeit counts , edges = np . histogram ( x , bins ) NumPy routine 29.1 \u00b5s \u00b1 4.23 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) print ( \"custom routine\" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) custom routine 13.2 \u00b5s \u00b1 1.27 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each) Our own one-line algorithm is several times faster than the optimized algorithm in NumPy! How can this be? If you dig into the np.histogram source code (you can do this in IPython by typing np.histogram??), you\u2019ll see that it\u2019s quite a bit more involved than the simple search-and-count that we\u2019ve done; this is because NumPy\u2019s algorithm is more flexible, and particularly is designed for better performance when the number of data points becomes large: x = np . random . randn ( 1000000 ) print ( \"NumPy routine: \" ) % timeit counts , edges = np . histogram ( x , bins ) print ( \"Custom routine: \" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) NumPy routine: 67.3 ms \u00b1 5.67 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Custom routine:","title":"06 Fany Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#06-comparisons-masks-and-boolean-logic","text":"Fancy Indexing Exploring Fancy Indexing Combined Indexing Example: Selecting Random Points Modifying Values with Fancy Indexing Exmple Binning Data","title":"06 - Comparisons, Masks, and Boolean Logic"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#fancy-indexing","text":"In the previous sections, we saw how to access and modify portions of arrays using simple indices (e.g., arr[0] ), slices (e.g., arr[:5] ), and Boolean masks (e.g., arr [arr> 0] ). In this section, we\u2019ll look at another style of array indexing, known as fancy indexing. Fancy indexing is like the simple indexing we\u2019ve already seen, but we pass arrays of indices in place of single scalars. This allows us to very quickly access and modify complicated subsets of an array\u2019s values.","title":"Fancy Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#exploring-fancy-indexing","text":"Fancy indexing is conceptually simple: it means passing an array of indices to access multiple array elements at once. import numpy as np rand = np . random . RandomState ( 42 ) x = rand . randint ( 100 , size = 10 ) print ( x ) [51 92 14 71 60 20 82 86 74 74] # three different elements [ x [ 3 ], x [ 7 ], x [ 2 ]] [71, 86, 14] # alternatively, we could do like this ind = [ 3 , 7 , 4 ] x [ ind ] array([71, 86, 60]) With fancy indexing, the shape of the result reflects the shape of the index arrays rather than the shape of the array being indexed: ind = np . array ([[ 3 , 7 ],[ 4 , 5 ]]) x [ ind ] array([[71, 86], [60, 20]]) # Multi-dimensional fancy indexing x = np . arange ( 12 ) . reshape (( 3 , 4 )) x array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) row = np . array ([ 0 , 1 , 2 ]) col = np . array ([ 2 , 1 , 3 ]) x [ row , col ] array([ 2, 5, 11]) x [ row [:, np . newaxis ], col ] array([[ 2, 1, 3], [ 6, 5, 7], [10, 9, 11]]) ##Here, each row value is matched with each column vector, exactly as we saw in broad\u2010 ##casting of arithmetic operations row [:, np . newaxis ] array([[0], [1], [2]]) row [:, np . newaxis ] * col array([[0, 0, 0], [2, 1, 3], [4, 2, 6]]) ## It is always important to remember with fancy indexing that the return value reflects ##the broadcasted shape of the indices, rather than the shape of the array being indexed.","title":"Exploring Fancy Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#combined-indexing","text":"For even more powerful operations, fancy indexing can be combined with the other indexing schemes x array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) # Combing fancy and simple index x [ 2 ,[ 2 , 0 , 1 ]] array([10, 8, 9]) # Combining fancy indexing with masking mask = np . array ([ 1 , 0 , 1 , 0 ], dtype = bool ) x [ row [:, np . newaxis ], mask ] array([[ 0, 2], [ 4, 6], [ 8, 10]])","title":"Combined Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#example-selecting-random-points","text":"One common use of fancy indexing is the selection of subsets of rows from a matrix. For example, we might have an N by D matrix representing N points in D dimen\u2010 sions, such as the following points drawn from a two-dimensional normal distribu\u2010 tion mean = [ 0 , 0 ] conv = [[ 1 , 2 ], [ 2 , 5 ]] x = rand . multivariate_normal ( mean , conv , 100 ) x . shape (100, 2) print ( x ) [[-0.644508 -0.46220608] [ 0.7376352 1.21236921] [ 0.88151763 1.12795177] [ 2.04998983 5.97778598] [-0.1711348 -2.06258746] [ 0.67956979 0.83705124] [ 1.46860232 1.22961093] [ 0.35282131 1.49875397] [-2.51552505 -5.64629995] [ 0.0843329 -0.3543059 ] [ 0.19199272 1.48901291] [-0.02566217 -0.74987887] [ 1.00569227 2.25287315] [ 0.49514263 1.18939673] [ 0.0629872 0.57349278] [ 0.75093031 2.99487004] [-3.0236127 -6.00766046] [-0.53943081 -0.3478899 ] [ 1.53817376 1.99973464] [-0.50886808 -1.81099656] [ 1.58115602 2.86410319] [ 0.99305043 2.54294059] [-0.87753796 -1.15767204] [-1.11518048 -1.87508012] [ 0.4299908 0.36324254] [ 0.97253528 3.53815717] [ 0.32124996 0.33137032] [-0.74618649 -2.77366681] [-0.88473953 -1.81495444] [ 0.98783862 2.30280401] [-1.2033623 -2.04402725] [-1.51101746 -3.2818741 ] [-2.76337717 -7.66760648] [ 0.39158553 0.87949228] [ 0.91181024 3.32968944] [-0.84202629 -2.01226547] [ 1.06586877 0.95500019] [ 0.44457363 1.87828298] [ 0.35936721 0.40554974] [-0.90649669 -0.93486441] [-0.35790389 -0.52363012] [-1.33461668 -3.03203218] [ 0.02815138 0.79654924] [ 0.37785618 0.51409383] [-1.06505097 -2.88726779] [ 2.32083881 5.97698647] [ 0.47605744 0.83634485] [-0.35490984 -1.03657119] [ 0.57532883 -0.79997124] [ 0.33399913 2.32597923] [ 0.6575612 -0.22389518] [ 1.3707365 2.2348831 ] [ 0.07099548 -0.29685467] [ 0.6074983 1.47089233] [-0.34226126 -1.10666237] [ 0.69226246 1.21504303] [-0.31112937 -0.75912097] [-0.26888327 -1.89366817] [ 0.42044896 1.85189522] [ 0.21115245 2.00781492] [-1.83106042 -2.91352836] [ 0.7841796 1.97640753] [ 0.10259314 1.24690575] [-1.91100558 -3.66800923] [ 0.13143756 -0.07833855] [-0.1317045 -1.64159158] [-0.14547282 -1.34125678] [-0.51172373 -1.40960773] [ 0.69758045 0.72563649] [ 0.11677083 0.88385162] [-1.16586444 -2.24482237] [-2.23176235 -2.63958101] [ 0.37857234 0.69112594] [ 0.87475323 3.400675 ] [-0.86864365 -3.03568353] [-1.03637857 -1.18469125] [-0.53334959 -0.37039911] [ 0.30414557 -0.5828419 ] [-1.47656656 -2.13046298] [-0.31332021 -1.7895623 ] [ 1.12659538 1.49627535] [-1.19675798 -1.51633442] [-0.75210154 -0.79770535] [ 0.74577693 1.95834451] [ 1.56094354 2.9330816 ] [-0.72009966 -1.99780959] [-1.32319163 -2.61218347] [-2.56215914 -6.08410838] [ 1.31256297 3.13143269] [ 0.51575983 2.30284639] [ 0.01374713 -0.11539344] [-0.16863279 0.39422355] [ 0.12065651 1.13236323] [-0.83504984 -2.38632016] [ 1.05185885 1.98418223] [-0.69144553 -1.56919875] [-1.2567603 -1.125898 ] [ 0.09619333 -0.64335574] [-0.99658689 -2.35038099] [-1.21405259 -1.77693724]] % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . scatter ( x [:, 0 ], x [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f507340ffd0> Let\u2019s use fancy indexing to select 20 random points. We\u2019ll do this by first choosing 20 random indices with no repeats, and use these indices to select a portion of the origi\u2010 nal array indices = np . random . choice ( x . shape [ 0 ], 20 , replace = False ) indices array([80, 87, 70, 77, 18, 56, 41, 42, 84, 9, 85, 61, 6, 71, 24, 69, 8, 22, 23, 54]) selection = x [ indices ] selection array([[ 1.12659538, 1.49627535], [-2.56215914, -6.08410838], [-1.16586444, -2.24482237], [ 0.30414557, -0.5828419 ], [ 1.53817376, 1.99973464], [-0.31112937, -0.75912097], [-1.33461668, -3.03203218], [ 0.02815138, 0.79654924], [ 1.56094354, 2.9330816 ], [ 0.0843329 , -0.3543059 ], [-0.72009966, -1.99780959], [ 0.7841796 , 1.97640753], [ 1.46860232, 1.22961093], [-2.23176235, -2.63958101], [ 0.4299908 , 0.36324254], [ 0.11677083, 0.88385162], [-2.51552505, -5.64629995], [-0.87753796, -1.15767204], [-1.11518048, -1.87508012], [-0.34226126, -1.10666237]]) plt . scatter ( x [:, 0 ], x [:, 1 ], alpha = 0.3 ) plt . scatter ( selection [:, 0 ], selection [:, 1 ], facecolor = 'none' , s = 200 ); This sort of strategy is often used to quickly partition datasets, as is often needed in train/test splitting for validation of statistical models","title":"Example: Selecting Random Points"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#modifying-values-with-fancy-indexing","text":"Just as fancy indexing can be used to access parts of an array, it can also be used to modify parts of an array. For example, imagine we have an array of indices and we\u2019d like to set the corresponding items in an array to some value: x = np . arange ( 10 ) i = np . array ([ 2 , 1 , 8 , 4 ]) x [ i ] =- 9 x array([ 0, -9, -9, 3, -9, 5, 6, 7, -9, 9]) We can use any assignment-type operator for this Notice, though, that repeated indices with these operations can cause some poten\u2010 tially unexpected results x [ i ] -= 10 x array([ 0, -19, -19, 3, -19, 5, 6, 7, -19, 9]) x = np . zeros ( 10 ) x [[ 0 , 0 ]] = [ 4 , 6 ] x array([6., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) Where did the 4 go? The result of this operation is to first assign x[0] = 4 , followed by x[0] = 6 . The result, of course, is that x[0] contains the value 6. i = [ 2 , 3 , 3 , 4 , 4 , 4 ] x [ i ] += 1 x array([6., 0., 1., 1., 1., 0., 0., 0., 0., 0.]) You might expect that x[3] would contain the value 2, and x[4] would contain the value 3, as this is how many times each index is repeated. Why is this not the case? Conceptually, this is because x[i] += 1 is meant as a shorthand of x[i] = x[i] + 1. x[i] + 1 is evaluated, and then the result is assigned to the indices in x. With this in mind, it is not the augmentation that happens multiple times, but the assignment, which leads to the rather nonintuitive results. So what if you want the other behavior where the operation is repeated? For this, you can use the at() method of ufuncs (available since NumPy 1.8), and do the following x = np . zeros ( 10 ) np . add . at ( x , i , 1 ) x array([0., 0., 1., 2., 3., 0., 0., 0., 0., 0.]) The at() method does an in-place application of the given operator at the specified indices (here, i) with the specified value (here, 1). Another method that is similar in spirit is the reduceat() method of ufuncs, which you can read about in the NumPy documentation.","title":"Modifying Values with Fancy Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/#exmple-binning-data","text":"np . random . seed ( 42 ) x = np . random . randn ( 100 ) # compute a histogram by hand bins = np . linspace ( - 5 , 5 , 20 ) counts = np . zeros_like ( bins ) # find the approperiate bin for each x i = np . searchsorted ( bins , x ) # add 1 to each of these bins np . add . at ( counts , i , 1 ) # plot the results plt . plot ( bins , counts ) #linestyle='steps' Of course, it would be silly to have to do this each time you want to plot a histogram. This is why Matplotlib provides the plt.hist() routine, which does the same in a single line: plt . hist ( x , bins , histtype = 'step' ); print ( \"NumPy routine\" ) % timeit counts , edges = np . histogram ( x , bins ) NumPy routine 29.1 \u00b5s \u00b1 4.23 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) print ( \"custom routine\" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) custom routine 13.2 \u00b5s \u00b1 1.27 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each) Our own one-line algorithm is several times faster than the optimized algorithm in NumPy! How can this be? If you dig into the np.histogram source code (you can do this in IPython by typing np.histogram??), you\u2019ll see that it\u2019s quite a bit more involved than the simple search-and-count that we\u2019ve done; this is because NumPy\u2019s algorithm is more flexible, and particularly is designed for better performance when the number of data points becomes large: x = np . random . randn ( 1000000 ) print ( \"NumPy routine: \" ) % timeit counts , edges = np . histogram ( x , bins ) print ( \"Custom routine: \" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) NumPy routine: 67.3 ms \u00b1 5.67 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Custom routine:","title":"Exmple Binning Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 07 - Sorting Arrays \u00b6 Sorting Arrays Fast Sorting in NumPy: np.sort and np.argsort Sorting along rows or columns Partial Sorts: Partitioning Example: k-Nearest Neighbors Sorting Arrays \u00b6 Up to this point we have been concerned mainly with tools to access and operate on array data with NumPy. This section covers algorithms related to sorting values in NumPy arrays. These algorithms are a favorite topic in introductory computer sci\u2010 ence courses: if you\u2019ve ever taken one, you probably have had dreams (or, depending on your temperament, nightmares) about insertion sorts, selection sorts, merge sorts, quick sorts, bubble sorts, and many, many more. All are means of accomplishing a similar task: sorting the values in a list or array. # Selection sort import numpy as np def selection_sort ( x ): for i in range ( len ( x )): swap = i + np . argmin ( x [ i :]) ( x [ i ], x [ swap ]) = ( x [ swap ], x [ i ]) return x x = np . array ([ 2 , 1 , 4 , 3 , 5 ]) selection_sort ( x ) array([1, 2, 3, 4, 5]) def bogosort ( x ): while np . any ( x [: - 1 ] > x [ 1 :]): np . random . shuffle ( x ) return x bogosort ( x ) array([1, 2, 3, 4, 5]) Fast Sorting in NumPy: np.sort and np.argsort \u00b6 Although Python has built-in sort and sorted functions to work with lists, we won\u2019t discuss them here because NumPy\u2019s np.sort function turns out to be much more efficient and useful for our purposes. By default np.sort uses an \ufffd N log N , quick\u2010 sort algorithm, though mergesort and heapsort are also available. For most applica\u2010 tions, the default quicksort is more than sufficient. x = np . array ([ 2 , 1 , 4 , 3 , 5 , 9 , 6 , 7 , 8 ]) np . sort ( x ) array([1, 2, 3, 4, 5, 6, 7, 8, 9]) #the sort method of array x . sort () x array([1, 2, 3, 4, 5, 6, 7, 8, 9]) A related function is argsort, which instead returns the indices of the sorted elements: x = np . array ([ 2 , 1 , 4 , 3 , 5 , 9 , 6 , 7 , 8 ]) i = np . argsort ( x ) i array([1, 0, 3, 2, 4, 6, 7, 8, 5]) # Now we can employ the fancy indexing to apply this index to sort the array x [ i ] array([1, 2, 3, 4, 5, 6, 7, 8, 9]) Sorting along rows or columns \u00b6 A useful feature of NumPy\u2019s sorting algorithms is the ability to sort along specific rows or columns of a multidimensional array using the axis argument. rand = np . random . RandomState ( 42 ) X = rand . randint ( 0 , 10 ,( 4 , 6 )) X array([[6, 3, 7, 4, 6, 9], [2, 6, 7, 4, 3, 7], [7, 2, 5, 4, 1, 7], [5, 1, 4, 0, 9, 5]]) # TO sort each column of X np . sort ( X , axis = 0 ) array([[2, 1, 4, 0, 1, 5], [5, 2, 5, 4, 3, 7], [6, 3, 7, 4, 6, 7], [7, 6, 7, 4, 9, 9]]) # TO sort each row of X np . sort ( X , axis = 1 ) array([[3, 4, 6, 6, 7, 9], [2, 3, 4, 6, 7, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 5, 9]]) Keep in mind that this treats each row or column as an independent array, and any relationships between the row or column values will be lost! Partial Sorts: Partitioning \u00b6 Sometimes we\u2019re not interested in sorting the entire array, but simply want to find the K smallest values in the array. NumPy provides this in the np.partition function. x = np . array ([ 7 , 2 , 3 , 1 , 6 , 5 , 4 ]) np . partition ( x , 3 ) array([2, 1, 3, 4, 6, 5, 7]) Similarly to sorting, we can partition along an arbitrary axis of a multidimensional array: np . partition ( X , 2 , axis = 1 ) array([[3, 4, 6, 7, 6, 9], [2, 3, 4, 7, 6, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 9, 5]]) X array([[6, 3, 7, 4, 6, 9], [2, 6, 7, 4, 3, 7], [7, 2, 5, 4, 1, 7], [5, 1, 4, 0, 9, 5]]) Example: k-Nearest Neighbors \u00b6 Let\u2019s quickly see how we might use this argsort function along multiple axes to find the nearest neighbors of each point in a set. X = rand . rand ( 10 , 2 ) X array([[0.00706631, 0.02306243], [0.52477466, 0.39986097], [0.04666566, 0.97375552], [0.23277134, 0.09060643], [0.61838601, 0.38246199], [0.98323089, 0.46676289], [0.85994041, 0.68030754], [0.45049925, 0.01326496], [0.94220176, 0.56328822], [0.3854165 , 0.01596625]]) % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ); X [:, np . newaxis ,:] . shape (10, 1, 2) X [ np . newaxis ,:,:] . shape (1, 10, 2) np . sum (( X [:, np . newaxis ,:] - X [ np . newaxis ,:,:]) ** 2 , axis = 1 ) array([[3.58879608, 2.10035624], [1.11234028, 0.97393943], [3.21004462, 4.71429169], [1.85007077, 1.68955454], [1.2368154 , 0.96342078], [3.39460409, 1.07078426], [2.36761815, 1.97878248], [1.13827476, 2.16752178], [3.01908822, 1.36825955], [1.25169759, 2.14881166]]) # Break downing the above code difference = X [:, np . newaxis ,:] - X [ np . newaxis ,:,:] difference . shape (10, 10, 2) square = difference ** 2 square . shape (10, 10, 2) dist_sqr = square . sum ( - 1 ) dist_sqr array([[0. , 0.40999909, 0.90538547, 0.05550496, 0.50287983, 1.14976739, 1.15936537, 0.19672877, 1.16632222, 0.14319923], [0.40999909, 0. , 0.55794316, 0.18090431, 0.00906581, 0.21465798, 0.19098635, 0.15497331, 0.20095384, 0.16679585], [0.90538547, 0.55794316, 0. , 0.81458763, 0.67649219, 1.13419594, 0.74752753, 1.08562368, 0.9704683 , 1.03211241], [0.05550496, 0.18090431, 0.81458763, 0. , 0.23387834, 0.70468321, 0.74108843, 0.05338715, 0.72671958, 0.0288717 ], [0.50287983, 0.00906581, 0.67649219, 0.23387834, 0. , 0.14021843, 0.1470605 , 0.16449241, 0.13755476, 0.18859392], [1.14976739, 0.21465798, 1.13419594, 0.70468321, 0.14021843, 0. , 0.06080186, 0.48946337, 0.01100053, 0.56059965], [1.15936537, 0.19098635, 0.74752753, 0.74108843, 0.1470605 , 0.06080186, 0. , 0.61258786, 0.02046045, 0.66652228], [0.19672877, 0.15497331, 1.08562368, 0.05338715, 0.16449241, 0.48946337, 0.61258786, 0. , 0.54429694, 0.00424306], [1.16632222, 0.20095384, 0.9704683 , 0.72671958, 0.13755476, 0.01100053, 0.02046045, 0.54429694, 0. , 0.60957115], [0.14319923, 0.16679585, 1.03211241, 0.0288717 , 0.18859392, 0.56059965, 0.66652228, 0.00424306, 0.60957115, 0. ]]) dist_sqr . diagonal () array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) nearest = np . argsort ( dist_sqr , axis = 1 ) nearest array([[0, 3, 9, 7, 1, 4, 2, 5, 6, 8], [1, 4, 7, 9, 3, 6, 8, 5, 0, 2], [2, 1, 4, 6, 3, 0, 8, 9, 7, 5], [3, 9, 7, 0, 1, 4, 5, 8, 6, 2], [4, 1, 8, 5, 6, 7, 9, 3, 0, 2], [5, 8, 6, 4, 1, 7, 9, 3, 2, 0], [6, 8, 5, 4, 1, 7, 9, 3, 2, 0], [7, 9, 3, 1, 4, 0, 5, 8, 6, 2], [8, 5, 6, 4, 1, 7, 9, 3, 2, 0], [9, 7, 3, 0, 1, 4, 5, 8, 6, 2]]) By using a full sort here, we\u2019ve actually done more work than we need to in this case. If we\u2019re simply interested in the nearest k neighbors, all we need is to partition each row so that the smallest k + 1 squared distances come first, with larger distances fill\u2010 ing the remaining positions of the array. We can do this with the np.argpartition function: k = 2 nearest_partition = np . argpartition ( dist_sqr , k + 1 , axis = 1 ) nearest_partition array([[3, 0, 9, 7, 1, 4, 2, 5, 8, 6], [1, 4, 7, 9, 3, 5, 6, 2, 8, 0], [2, 1, 4, 6, 3, 0, 5, 7, 8, 9], [3, 9, 7, 0, 1, 5, 6, 2, 8, 4], [1, 8, 4, 5, 7, 6, 9, 3, 2, 0], [5, 8, 6, 4, 1, 7, 9, 3, 2, 0], [6, 8, 5, 4, 1, 7, 9, 3, 2, 0], [7, 9, 3, 1, 4, 5, 6, 2, 8, 0], [8, 5, 6, 4, 1, 7, 9, 3, 2, 0], [3, 9, 7, 0, 1, 5, 6, 2, 8, 4]]) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ) <matplotlib.collections.PathCollection at 0x7f51448d7e80> # Draw the lines from each point to its two nearest neighbors k = 2 for i in range ( X . shape [ 0 ]): for j in nearest_partition [ i ,: k + 1 ]: plt . plot ( * zip ( X [ j ], X [ i ]), color = 'black' )","title":"07 Sorted Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#07-sorting-arrays","text":"Sorting Arrays Fast Sorting in NumPy: np.sort and np.argsort Sorting along rows or columns Partial Sorts: Partitioning Example: k-Nearest Neighbors","title":"07 -  Sorting Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#sorting-arrays","text":"Up to this point we have been concerned mainly with tools to access and operate on array data with NumPy. This section covers algorithms related to sorting values in NumPy arrays. These algorithms are a favorite topic in introductory computer sci\u2010 ence courses: if you\u2019ve ever taken one, you probably have had dreams (or, depending on your temperament, nightmares) about insertion sorts, selection sorts, merge sorts, quick sorts, bubble sorts, and many, many more. All are means of accomplishing a similar task: sorting the values in a list or array. # Selection sort import numpy as np def selection_sort ( x ): for i in range ( len ( x )): swap = i + np . argmin ( x [ i :]) ( x [ i ], x [ swap ]) = ( x [ swap ], x [ i ]) return x x = np . array ([ 2 , 1 , 4 , 3 , 5 ]) selection_sort ( x ) array([1, 2, 3, 4, 5]) def bogosort ( x ): while np . any ( x [: - 1 ] > x [ 1 :]): np . random . shuffle ( x ) return x bogosort ( x ) array([1, 2, 3, 4, 5])","title":"Sorting Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#fast-sorting-in-numpy-npsort-and-npargsort","text":"Although Python has built-in sort and sorted functions to work with lists, we won\u2019t discuss them here because NumPy\u2019s np.sort function turns out to be much more efficient and useful for our purposes. By default np.sort uses an \ufffd N log N , quick\u2010 sort algorithm, though mergesort and heapsort are also available. For most applica\u2010 tions, the default quicksort is more than sufficient. x = np . array ([ 2 , 1 , 4 , 3 , 5 , 9 , 6 , 7 , 8 ]) np . sort ( x ) array([1, 2, 3, 4, 5, 6, 7, 8, 9]) #the sort method of array x . sort () x array([1, 2, 3, 4, 5, 6, 7, 8, 9]) A related function is argsort, which instead returns the indices of the sorted elements: x = np . array ([ 2 , 1 , 4 , 3 , 5 , 9 , 6 , 7 , 8 ]) i = np . argsort ( x ) i array([1, 0, 3, 2, 4, 6, 7, 8, 5]) # Now we can employ the fancy indexing to apply this index to sort the array x [ i ] array([1, 2, 3, 4, 5, 6, 7, 8, 9])","title":"Fast Sorting in NumPy: np.sort and np.argsort"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#sorting-along-rows-or-columns","text":"A useful feature of NumPy\u2019s sorting algorithms is the ability to sort along specific rows or columns of a multidimensional array using the axis argument. rand = np . random . RandomState ( 42 ) X = rand . randint ( 0 , 10 ,( 4 , 6 )) X array([[6, 3, 7, 4, 6, 9], [2, 6, 7, 4, 3, 7], [7, 2, 5, 4, 1, 7], [5, 1, 4, 0, 9, 5]]) # TO sort each column of X np . sort ( X , axis = 0 ) array([[2, 1, 4, 0, 1, 5], [5, 2, 5, 4, 3, 7], [6, 3, 7, 4, 6, 7], [7, 6, 7, 4, 9, 9]]) # TO sort each row of X np . sort ( X , axis = 1 ) array([[3, 4, 6, 6, 7, 9], [2, 3, 4, 6, 7, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 5, 9]]) Keep in mind that this treats each row or column as an independent array, and any relationships between the row or column values will be lost!","title":"Sorting along rows or columns"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#partial-sorts-partitioning","text":"Sometimes we\u2019re not interested in sorting the entire array, but simply want to find the K smallest values in the array. NumPy provides this in the np.partition function. x = np . array ([ 7 , 2 , 3 , 1 , 6 , 5 , 4 ]) np . partition ( x , 3 ) array([2, 1, 3, 4, 6, 5, 7]) Similarly to sorting, we can partition along an arbitrary axis of a multidimensional array: np . partition ( X , 2 , axis = 1 ) array([[3, 4, 6, 7, 6, 9], [2, 3, 4, 7, 6, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 9, 5]]) X array([[6, 3, 7, 4, 6, 9], [2, 6, 7, 4, 3, 7], [7, 2, 5, 4, 1, 7], [5, 1, 4, 0, 9, 5]])","title":"Partial Sorts: Partitioning"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/#example-k-nearest-neighbors","text":"Let\u2019s quickly see how we might use this argsort function along multiple axes to find the nearest neighbors of each point in a set. X = rand . rand ( 10 , 2 ) X array([[0.00706631, 0.02306243], [0.52477466, 0.39986097], [0.04666566, 0.97375552], [0.23277134, 0.09060643], [0.61838601, 0.38246199], [0.98323089, 0.46676289], [0.85994041, 0.68030754], [0.45049925, 0.01326496], [0.94220176, 0.56328822], [0.3854165 , 0.01596625]]) % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ); X [:, np . newaxis ,:] . shape (10, 1, 2) X [ np . newaxis ,:,:] . shape (1, 10, 2) np . sum (( X [:, np . newaxis ,:] - X [ np . newaxis ,:,:]) ** 2 , axis = 1 ) array([[3.58879608, 2.10035624], [1.11234028, 0.97393943], [3.21004462, 4.71429169], [1.85007077, 1.68955454], [1.2368154 , 0.96342078], [3.39460409, 1.07078426], [2.36761815, 1.97878248], [1.13827476, 2.16752178], [3.01908822, 1.36825955], [1.25169759, 2.14881166]]) # Break downing the above code difference = X [:, np . newaxis ,:] - X [ np . newaxis ,:,:] difference . shape (10, 10, 2) square = difference ** 2 square . shape (10, 10, 2) dist_sqr = square . sum ( - 1 ) dist_sqr array([[0. , 0.40999909, 0.90538547, 0.05550496, 0.50287983, 1.14976739, 1.15936537, 0.19672877, 1.16632222, 0.14319923], [0.40999909, 0. , 0.55794316, 0.18090431, 0.00906581, 0.21465798, 0.19098635, 0.15497331, 0.20095384, 0.16679585], [0.90538547, 0.55794316, 0. , 0.81458763, 0.67649219, 1.13419594, 0.74752753, 1.08562368, 0.9704683 , 1.03211241], [0.05550496, 0.18090431, 0.81458763, 0. , 0.23387834, 0.70468321, 0.74108843, 0.05338715, 0.72671958, 0.0288717 ], [0.50287983, 0.00906581, 0.67649219, 0.23387834, 0. , 0.14021843, 0.1470605 , 0.16449241, 0.13755476, 0.18859392], [1.14976739, 0.21465798, 1.13419594, 0.70468321, 0.14021843, 0. , 0.06080186, 0.48946337, 0.01100053, 0.56059965], [1.15936537, 0.19098635, 0.74752753, 0.74108843, 0.1470605 , 0.06080186, 0. , 0.61258786, 0.02046045, 0.66652228], [0.19672877, 0.15497331, 1.08562368, 0.05338715, 0.16449241, 0.48946337, 0.61258786, 0. , 0.54429694, 0.00424306], [1.16632222, 0.20095384, 0.9704683 , 0.72671958, 0.13755476, 0.01100053, 0.02046045, 0.54429694, 0. , 0.60957115], [0.14319923, 0.16679585, 1.03211241, 0.0288717 , 0.18859392, 0.56059965, 0.66652228, 0.00424306, 0.60957115, 0. ]]) dist_sqr . diagonal () array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) nearest = np . argsort ( dist_sqr , axis = 1 ) nearest array([[0, 3, 9, 7, 1, 4, 2, 5, 6, 8], [1, 4, 7, 9, 3, 6, 8, 5, 0, 2], [2, 1, 4, 6, 3, 0, 8, 9, 7, 5], [3, 9, 7, 0, 1, 4, 5, 8, 6, 2], [4, 1, 8, 5, 6, 7, 9, 3, 0, 2], [5, 8, 6, 4, 1, 7, 9, 3, 2, 0], [6, 8, 5, 4, 1, 7, 9, 3, 2, 0], [7, 9, 3, 1, 4, 0, 5, 8, 6, 2], [8, 5, 6, 4, 1, 7, 9, 3, 2, 0], [9, 7, 3, 0, 1, 4, 5, 8, 6, 2]]) By using a full sort here, we\u2019ve actually done more work than we need to in this case. If we\u2019re simply interested in the nearest k neighbors, all we need is to partition each row so that the smallest k + 1 squared distances come first, with larger distances fill\u2010 ing the remaining positions of the array. We can do this with the np.argpartition function: k = 2 nearest_partition = np . argpartition ( dist_sqr , k + 1 , axis = 1 ) nearest_partition array([[3, 0, 9, 7, 1, 4, 2, 5, 8, 6], [1, 4, 7, 9, 3, 5, 6, 2, 8, 0], [2, 1, 4, 6, 3, 0, 5, 7, 8, 9], [3, 9, 7, 0, 1, 5, 6, 2, 8, 4], [1, 8, 4, 5, 7, 6, 9, 3, 2, 0], [5, 8, 6, 4, 1, 7, 9, 3, 2, 0], [6, 8, 5, 4, 1, 7, 9, 3, 2, 0], [7, 9, 3, 1, 4, 5, 6, 2, 8, 0], [8, 5, 6, 4, 1, 7, 9, 3, 2, 0], [3, 9, 7, 0, 1, 5, 6, 2, 8, 4]]) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ) <matplotlib.collections.PathCollection at 0x7f51448d7e80> # Draw the lines from each point to its two nearest neighbors k = 2 for i in range ( X . shape [ 0 ]): for j in nearest_partition [ i ,: k + 1 ]: plt . plot ( * zip ( X [ j ], X [ i ]), color = 'black' )","title":"Example: k-Nearest Neighbors"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 1 - Introduction to Numpy \u00b6 08 - Structured Data: NumPy\u2019s Structured Arrays \u00b6 Structured Data: NumPy\u2019s Structured Arrays Structured Data: NumPy\u2019s Structured Arrays \u00b6 While often our data can be well represented by a homogeneous array of values,sometimes this is not the case. This section demonstrates the use of NumPy\u2019s struc\u2010 tured arrays and record arrays, which provide efficient storage for compound, heterogeneous data. While the patterns shown here are useful for simple operations, scenarios like this often lend themselves to the use of Pandas DataFrames. Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials Top","title":"08  Structured Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/#chpt-1-introduction-to-numpy","text":"","title":"Chpt 1 - Introduction to Numpy"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/#08-structured-data-numpys-structured-arrays","text":"Structured Data: NumPy\u2019s Structured Arrays","title":"08 - Structured Data: NumPy\u2019s Structured Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/#structured-data-numpys-structured-arrays","text":"While often our data can be well represented by a homogeneous array of values,sometimes this is not the case. This section demonstrates the use of NumPy\u2019s struc\u2010 tured arrays and record arrays, which provide efficient storage for compound, heterogeneous data. While the patterns shown here are useful for simple operations, scenarios like this often lend themselves to the use of Pandas DataFrames.","title":"Structured Data: NumPy\u2019s Structured Arrays"},{"location":"booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials Top","title":"Great Job! Thats the end of this part."},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 01 - Introducing Pandas Objects \u00b6 Introducing Pandas Objects The Pandas Series Object Series as generalized NumPy array Series as specialized dictionary Constructing Series objects The Pandas DataFrame Object DataFrame as a generalized NumPy array DataFrame as specialized dictionary The Pandas Index Object Index as immutable array Index as ordered set Introducing Pandas Objects \u00b6 At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices. let\u2019s introduce the three fundamental Pandas data structures: the Series, DataFrame, and Index. import numpy as np import pandas as pd The Pandas Series Object \u00b6 A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array data = pd . Series ([ 0.25 , 0.5 , 3.1415 , 2.729 , 1.0 ,]) data 0 0.2500 1 0.5000 2 3.1415 3 2.7290 4 1.0000 dtype: float64 data . values array([0.25 , 0.5 , 3.1415, 2.729 , 1. ]) data . index RangeIndex(start=0, stop=5, step=1) # Like Numpy array, data can be accessed by the associated index vi the [] notation data [ 1 ] 0.5 data [ 1 : 3 ] 1 0.5000 2 3.1415 dtype: float64 Series as generalized NumPy array \u00b6 From what we\u2019ve seen so far, it may look like the Series object is basically interchangeable with a one-dimensional NumPy array. The essential difference is the presence of the index: while the NumPy array has an implicitly defined integer index usedvo access the values, the Pandas Series has an explicitly defined index associated with the values. # This explicit index definition gives the Series object additional capabilities... # Like index can be not only integer data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 data [ 'b' ] 0.5 # We can use non-contigious values for index like data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 25 , 1 , 0 , 75 ]) data 25 0.25 1 0.50 0 0.75 75 1.00 dtype: float64 data [ 1 ] 0.5 Series as specialized dictionary \u00b6 In this way, you can think of a Pandas Series a bit like a specialization of a Python dictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a Series is a structure that maps typed keys to a set of typed values. This typing is important: just as the type-specific compiled code behind a NumPy array makes it more efficient than a Python list for certain operations, the type information of a Pandas Series makes it much more efficient than Python dictionaries for certain operations. We can make the Series-as-dictionary analogy even more clear by constructing a Series object directly from a Python dictionary population_dict = { 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 } population = pd . Series ( population_dict ) population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 By default, a Series will be created where the index is drawn from the sorted keys. From here, typical dictionary-style item access can be performed: population [ 'California' ] 38332521 # Unlike a dictionary, though, the Series also supports array-style operations such as slicing population [ 'California' : 'Florida' ] California 38332521 Texas 26448193 New York 19651127 Florida 19552860 dtype: int64 Constructing Series objects \u00b6 We\u2019ve already seen a few ways of constructing a Pandas Series from scratch; all of them are some version of the following: >>> pd.Series(data, index=index) where index is an optional argument, and data can be one of many entities. data can be a list or NumPy array, in which case index defaults to an integer sequence pd . Series ([ 2 , 4 , 6 ]) 0 2 1 4 2 6 dtype: int64 data can be a scalar, which is repeated to fill the specified index: data = pd . Series ( 5 , index = [ 100 , 200 , 300 , 400 ]) data 100 5 200 5 300 5 400 5 dtype: int64 data can be a dictionary, in which index defaults to the sorted dictionary keys pd . Series ({ 2 : 'a' , 3 : 'c' , 5 : 'e' , 0 : 'i' }) 2 a 3 c 5 e 0 i dtype: object The Pandas DataFrame Object \u00b6 The next fundamental structure in Pandas is the DataFrame. Like the Series object discussed in the previous section, the DataFrame can be thought of either as a gener\u2010 alization of a NumPy array, or as a specialization of a Python dictionary. We\u2019ll now take a look at each of these perspectives. DataFrame as a generalized NumPy array \u00b6 If a Series is an analog of a one-dimensional array with flexible indices, a DataFrame is an analog of a two-dimensional array with both flexible row indices and flexible column names. Just as you might think of a two-dimensional array as an ordered sequence of aligned one-dimensional columns, you can think of a DataFrame as a sequence of aligned Series objects. Here, by \u201caligned\u201d we mean that they share the same index. area_dict = { 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 } area_dict {'California': 423967, 'Texas': 695662, 'New York': 141297, 'Florida': 170312, 'Illinois': 149995} area = pd . Series ( area_dict ) area California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 dtype: int64 population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 type ( area ) pandas.core.series.Series type ( population ) pandas.core.series.Series states = pd . DataFrame ({ 'population' : population , 'area' : area }) states .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area California 38332521 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 Like the Series object, the DataFrame has an index attribute that gives access to the index labels states . columns Index(['population', 'area'], dtype='object') states . index Index(['California', 'Texas', 'New York', 'Florida', 'Illinois'], dtype='object') DataFrame as specialized dictionary \u00b6 Similarly, we can also think of a DataFrame as a specialization of a dictionary. Where a dictionary maps a key to a value, a DataFrame maps a column name to a Series of column data. For example, asking for the \u2018area\u2019 attribute returns the Series object containing the areas we saw earlier states [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 states [ 'area' ][ 0 ] 423967 states [ 'area' ][: 3 ] California 423967 Texas 695662 New York 141297 Name: area, dtype: int64 states [ 'area' ][ 0 , 0 ] KeyError: 'key of type tuple not found and not a MultiIndex' Notice the potential point of confusion here: in a two-dimensional NumPy array, data[0] will return the first row. For a DataFrame, data['col0'] will return the first column. Because of this, it is probably better to think about DataFrames as generalized dictionaries rather than generalized arrays, though both ways of looking at the situa\u2010 tion can be useful. # Constructing dataframes from signle series object population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 pd . DataFrame ( data = population ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 pd . DataFrame ( data = population , columns = [ 'population' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 # Constructing dataframes from list of dicts data = [{ 'a' : i , 'b' : 2 * i } for i in range ( 4 )] data [{'a': 0, 'b': 0}, {'a': 1, 'b': 2}, {'a': 2, 'b': 4}, {'a': 3, 'b': 6}] pd . DataFrame ( data ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b 0 0 0 1 1 2 2 2 4 3 3 6 Even if some keys in the dictionary are missing, Pandas will fill them in with NaN (i.e., \u201cnot a number\u201d) values: pd . DataFrame ([{ 'a' : 1 , 'b' : 2 },{ 'b' : 3 , 'c' : 4 }]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c 0 1.0 2 NaN 1 NaN 3 4.0 # Constructing dataframes from a dictionary of Series Objects pd . DataFrame ({ 'population' : population , 'area' : area }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area California 38332521 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 # Constructing dataframes from two-d Numpy array pd . DataFrame ( np . random . rand ( 3 , 2 ), columns = [ 'foo' , 'bar' ], index = [ 'a' , 'b' , 'c' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } foo bar a 0.952378 0.229453 b 0.305751 0.208598 c 0.569426 0.843111 The Pandas Index Object \u00b6 We have seen here that both the Series and DataFrame objects contain an explicit index that lets you reference and modify data. This Index object is an interesting structure in itself, and it can be thought of either as an immutable array or as an ordered set (technically a multiset, as Index objects may contain repeated values). Those views have some interesting consequences in the operations available on Index objects. As a simple example, let\u2019s construct an Index from a list of integers ind = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) ind Int64Index([2, 3, 5, 7, 11], dtype='int64') type ( ind ) pandas.core.indexes.numeric.Int64Index Index as immutable array \u00b6 The Index object in many ways operates like an array. For example, we can use stan\u2010 dard Python indexing notation to retrieve values or slices ind [ 1 ] 3 ind [:: 2 ] Int64Index([2, 5, 11], dtype='int64') # Index objects also have many of the attributes familiar from NumPy arrays: print ( ind . size , ind . shape , ind . ndim , ind . dtype ) 5 (5,) 1 int64 #One difference between Index objects and NumPy arrays is that indices are immuta\u2010 #ble\u2014that is, they cannot be modified via the normal means ind [ 1 ] = 0 TypeError: Index does not support mutable operations Index as ordered set \u00b6 Pandas objects are designed to facilitate operations such as joins across datasets, which depend on many aspects of set arithmetic. the conventions used by Python\u2019s built-in set data structure, so that unions, intersec\u2010 tions, differences, and other combinations can be computed in a familiar way the conventions used by Python\u2019s built-in set data structure, so that unions, intersec\u2010 tions, differences, and other combinations can be computed in a familiar way. ind_a = pd . Index ([ 1 , 3 , 5 , 7 , 9 ]) ind_b = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) ind_a & ind_b # Intersection /tmp/ipykernel_229290/4215377278.py:1: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__. Use index.intersection(other) instead. ind_a & ind_b # Intersection Int64Index([3, 5, 7], dtype='int64') ind_a | ind_b # Union operation /tmp/ipykernel_229290/3034377863.py:1: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__. Use index.union(other) instead. ind_a | ind_b # Union operation Int64Index([1, 2, 3, 5, 7, 9, 11], dtype='int64') ind_a ^ ind_b # Symmetric difference /tmp/ipykernel_229290/3946211992.py:1: FutureWarning: Index.__xor__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__xor__. Use index.symmetric_difference(other) instead. ind_a^ind_b # Symmetric difference Int64Index([1, 2, 9, 11], dtype='int64')","title":"01 Introduction to Pandas Objects"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#01-introducing-pandas-objects","text":"Introducing Pandas Objects The Pandas Series Object Series as generalized NumPy array Series as specialized dictionary Constructing Series objects The Pandas DataFrame Object DataFrame as a generalized NumPy array DataFrame as specialized dictionary The Pandas Index Object Index as immutable array Index as ordered set","title":"01 - Introducing Pandas Objects"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#introducing-pandas-objects","text":"At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices. let\u2019s introduce the three fundamental Pandas data structures: the Series, DataFrame, and Index. import numpy as np import pandas as pd","title":"Introducing Pandas Objects"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#the-pandas-series-object","text":"A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array data = pd . Series ([ 0.25 , 0.5 , 3.1415 , 2.729 , 1.0 ,]) data 0 0.2500 1 0.5000 2 3.1415 3 2.7290 4 1.0000 dtype: float64 data . values array([0.25 , 0.5 , 3.1415, 2.729 , 1. ]) data . index RangeIndex(start=0, stop=5, step=1) # Like Numpy array, data can be accessed by the associated index vi the [] notation data [ 1 ] 0.5 data [ 1 : 3 ] 1 0.5000 2 3.1415 dtype: float64","title":"The Pandas Series Object"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#series-as-generalized-numpy-array","text":"From what we\u2019ve seen so far, it may look like the Series object is basically interchangeable with a one-dimensional NumPy array. The essential difference is the presence of the index: while the NumPy array has an implicitly defined integer index usedvo access the values, the Pandas Series has an explicitly defined index associated with the values. # This explicit index definition gives the Series object additional capabilities... # Like index can be not only integer data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 data [ 'b' ] 0.5 # We can use non-contigious values for index like data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 25 , 1 , 0 , 75 ]) data 25 0.25 1 0.50 0 0.75 75 1.00 dtype: float64 data [ 1 ] 0.5","title":"Series as generalized NumPy array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#series-as-specialized-dictionary","text":"In this way, you can think of a Pandas Series a bit like a specialization of a Python dictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a Series is a structure that maps typed keys to a set of typed values. This typing is important: just as the type-specific compiled code behind a NumPy array makes it more efficient than a Python list for certain operations, the type information of a Pandas Series makes it much more efficient than Python dictionaries for certain operations. We can make the Series-as-dictionary analogy even more clear by constructing a Series object directly from a Python dictionary population_dict = { 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 } population = pd . Series ( population_dict ) population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 By default, a Series will be created where the index is drawn from the sorted keys. From here, typical dictionary-style item access can be performed: population [ 'California' ] 38332521 # Unlike a dictionary, though, the Series also supports array-style operations such as slicing population [ 'California' : 'Florida' ] California 38332521 Texas 26448193 New York 19651127 Florida 19552860 dtype: int64","title":"Series as specialized dictionary"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#constructing-series-objects","text":"We\u2019ve already seen a few ways of constructing a Pandas Series from scratch; all of them are some version of the following: >>> pd.Series(data, index=index) where index is an optional argument, and data can be one of many entities. data can be a list or NumPy array, in which case index defaults to an integer sequence pd . Series ([ 2 , 4 , 6 ]) 0 2 1 4 2 6 dtype: int64 data can be a scalar, which is repeated to fill the specified index: data = pd . Series ( 5 , index = [ 100 , 200 , 300 , 400 ]) data 100 5 200 5 300 5 400 5 dtype: int64 data can be a dictionary, in which index defaults to the sorted dictionary keys pd . Series ({ 2 : 'a' , 3 : 'c' , 5 : 'e' , 0 : 'i' }) 2 a 3 c 5 e 0 i dtype: object","title":"Constructing Series objects"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#the-pandas-dataframe-object","text":"The next fundamental structure in Pandas is the DataFrame. Like the Series object discussed in the previous section, the DataFrame can be thought of either as a gener\u2010 alization of a NumPy array, or as a specialization of a Python dictionary. We\u2019ll now take a look at each of these perspectives.","title":"The Pandas DataFrame Object"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#dataframe-as-a-generalized-numpy-array","text":"If a Series is an analog of a one-dimensional array with flexible indices, a DataFrame is an analog of a two-dimensional array with both flexible row indices and flexible column names. Just as you might think of a two-dimensional array as an ordered sequence of aligned one-dimensional columns, you can think of a DataFrame as a sequence of aligned Series objects. Here, by \u201caligned\u201d we mean that they share the same index. area_dict = { 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 } area_dict {'California': 423967, 'Texas': 695662, 'New York': 141297, 'Florida': 170312, 'Illinois': 149995} area = pd . Series ( area_dict ) area California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 dtype: int64 population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 type ( area ) pandas.core.series.Series type ( population ) pandas.core.series.Series states = pd . DataFrame ({ 'population' : population , 'area' : area }) states .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area California 38332521 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 Like the Series object, the DataFrame has an index attribute that gives access to the index labels states . columns Index(['population', 'area'], dtype='object') states . index Index(['California', 'Texas', 'New York', 'Florida', 'Illinois'], dtype='object')","title":"DataFrame as a generalized NumPy array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#dataframe-as-specialized-dictionary","text":"Similarly, we can also think of a DataFrame as a specialization of a dictionary. Where a dictionary maps a key to a value, a DataFrame maps a column name to a Series of column data. For example, asking for the \u2018area\u2019 attribute returns the Series object containing the areas we saw earlier states [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 states [ 'area' ][ 0 ] 423967 states [ 'area' ][: 3 ] California 423967 Texas 695662 New York 141297 Name: area, dtype: int64 states [ 'area' ][ 0 , 0 ] KeyError: 'key of type tuple not found and not a MultiIndex' Notice the potential point of confusion here: in a two-dimensional NumPy array, data[0] will return the first row. For a DataFrame, data['col0'] will return the first column. Because of this, it is probably better to think about DataFrames as generalized dictionaries rather than generalized arrays, though both ways of looking at the situa\u2010 tion can be useful. # Constructing dataframes from signle series object population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 pd . DataFrame ( data = population ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 pd . DataFrame ( data = population , columns = [ 'population' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 # Constructing dataframes from list of dicts data = [{ 'a' : i , 'b' : 2 * i } for i in range ( 4 )] data [{'a': 0, 'b': 0}, {'a': 1, 'b': 2}, {'a': 2, 'b': 4}, {'a': 3, 'b': 6}] pd . DataFrame ( data ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b 0 0 0 1 1 2 2 2 4 3 3 6 Even if some keys in the dictionary are missing, Pandas will fill them in with NaN (i.e., \u201cnot a number\u201d) values: pd . DataFrame ([{ 'a' : 1 , 'b' : 2 },{ 'b' : 3 , 'c' : 4 }]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c 0 1.0 2 NaN 1 NaN 3 4.0 # Constructing dataframes from a dictionary of Series Objects pd . DataFrame ({ 'population' : population , 'area' : area }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area California 38332521 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 # Constructing dataframes from two-d Numpy array pd . DataFrame ( np . random . rand ( 3 , 2 ), columns = [ 'foo' , 'bar' ], index = [ 'a' , 'b' , 'c' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } foo bar a 0.952378 0.229453 b 0.305751 0.208598 c 0.569426 0.843111","title":"DataFrame as specialized dictionary"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#the-pandas-index-object","text":"We have seen here that both the Series and DataFrame objects contain an explicit index that lets you reference and modify data. This Index object is an interesting structure in itself, and it can be thought of either as an immutable array or as an ordered set (technically a multiset, as Index objects may contain repeated values). Those views have some interesting consequences in the operations available on Index objects. As a simple example, let\u2019s construct an Index from a list of integers ind = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) ind Int64Index([2, 3, 5, 7, 11], dtype='int64') type ( ind ) pandas.core.indexes.numeric.Int64Index","title":"The Pandas Index Object"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#index-as-immutable-array","text":"The Index object in many ways operates like an array. For example, we can use stan\u2010 dard Python indexing notation to retrieve values or slices ind [ 1 ] 3 ind [:: 2 ] Int64Index([2, 5, 11], dtype='int64') # Index objects also have many of the attributes familiar from NumPy arrays: print ( ind . size , ind . shape , ind . ndim , ind . dtype ) 5 (5,) 1 int64 #One difference between Index objects and NumPy arrays is that indices are immuta\u2010 #ble\u2014that is, they cannot be modified via the normal means ind [ 1 ] = 0 TypeError: Index does not support mutable operations","title":"Index as immutable array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/#index-as-ordered-set","text":"Pandas objects are designed to facilitate operations such as joins across datasets, which depend on many aspects of set arithmetic. the conventions used by Python\u2019s built-in set data structure, so that unions, intersec\u2010 tions, differences, and other combinations can be computed in a familiar way the conventions used by Python\u2019s built-in set data structure, so that unions, intersec\u2010 tions, differences, and other combinations can be computed in a familiar way. ind_a = pd . Index ([ 1 , 3 , 5 , 7 , 9 ]) ind_b = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) ind_a & ind_b # Intersection /tmp/ipykernel_229290/4215377278.py:1: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__. Use index.intersection(other) instead. ind_a & ind_b # Intersection Int64Index([3, 5, 7], dtype='int64') ind_a | ind_b # Union operation /tmp/ipykernel_229290/3034377863.py:1: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__. Use index.union(other) instead. ind_a | ind_b # Union operation Int64Index([1, 2, 3, 5, 7, 9, 11], dtype='int64') ind_a ^ ind_b # Symmetric difference /tmp/ipykernel_229290/3946211992.py:1: FutureWarning: Index.__xor__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__xor__. Use index.symmetric_difference(other) instead. ind_a^ind_b # Symmetric difference Int64Index([1, 2, 9, 11], dtype='int64')","title":"Index as ordered set"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 02 - Data Indexing and Selection \u00b6 Data Indexing and Selection Data Selection in Series Series as one-dimensional array Indexers: loc, iloc, and ix Data Selection in DataFrame DataFrame as a Dictionary DataFrame as 2D array Data Indexing and Selection \u00b6 In Chapter 2, we looked in detail at methods and tools to access, set, and modify values in NumPy arrays. These included indexing (e.g., arr[2, 1] ), slicing (e.g., arr[:,1:5]) , masking (e.g., arr[arr > 0] ), fancy indexing (e.g., arr[0, [1, 5]] ), and combinations thereof (e.g., arr[:, [1, 5]] ). Here we\u2019ll look at similar means of accessing and modifying values in Pandas Series and DataFrame objects. If you have used the NumPy patterns, the corresponding patterns in Pandas will feel very famil\u2010 iar, though there are a few quirks to be aware of. Data Selection in Series \u00b6 As we saw in the previous section, a Series object acts in many ways like a one- dimensional NumPy array, and in many ways like a standard Python dictionary. If we keep these two overlapping analogies in mind, it will help us to understand the patterns of data indexing and selection in these arrays. ### Series as dictionary Like a dictionary, the Series object provides a mapping from a collection of keys to a collection of values: import pandas as pd data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 data [ 'b' ] 0.5 # also use dictionary-like python experssions and methods to examine the keys/indices and values 'a' in data True data . keys () Index(['a', 'b', 'c', 'd'], dtype='object') list ( data . items ()) [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)] Series objects can even be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a Series by assigning to a new index value data [ 'e' ] = 1.25 data a 0.25 b 0.50 c 0.75 d 1.00 e 1.25 dtype: float64 Series as one-dimensional array \u00b6 A Series builds on this dictionary-like interface and provides array-style item selec\u2010 tion via the same basic mechanisms as NumPy arrays\u2014that is, slices, masking, and fancy indexing. Examples of these are as follows: # slicing by explicit index data [ 'a' : 'c' ] a 0.25 b 0.50 c 0.75 dtype: float64 #slicing by implicit integer index data [ 0 : 2 ] a 0.25 b 0.50 dtype: float64 #masking data [( data > 0.3 ) & ( data < 0.8 )] b 0.50 c 0.75 dtype: float64 #fancy indexing data [[ 'a' , 'e' ]] a 0.25 e 1.25 dtype: float64 Among these, slicing may be the source of the most confusion. Notice that when you are slicing with an explicit index (i.e., data['a':'c'] ), the final index is included in the slice, while when you\u2019re slicing with an implicit index (i.e., data[0:2] ), the final index is excluded from the slice. Indexers: loc, iloc, and ix \u00b6 These slicing and indexing conventions can be a source of confusion. For example, if your Series has an explicit integer index, an indexing operation such as data[1] will use the explicit indices, while a slicing operation like data[1:3] will use the implicit Python-style index. data = pd . Series ([ 'a' , 'b' , 'c' ], index = [ 1 , 3 , 5 ]) data 1 a 3 b 5 c dtype: object # explicit index when indexing data [ 1 ] 'a' data [ 3 ] 'b' # implicit index when slicing data [ 1 : 3 ] 3 b 5 c dtype: object Because of this potential confusion in the case of integer indexes, Pandas provides some special indexer attributes that explicitly expose certain indexing schemes. These are not functional methods, but attributes that expose a particular slicing interface to the data in the Series. # First, the loc attribute allows indexing and slicing that always references the explicit index: data . loc [ 1 ] 'a' data . loc [ 1 : 3 ] 1 a 3 b dtype: object # The iloc attribute allows indexing and slicing that always references the implicit Python-style index data . iloc [ 1 ] 'b' data . iloc [ 0 ] 'a' data . loc [ 0 ] KeyError: 0 data . iloc [ 1 : 3 ] 3 b 5 c dtype: object data . loc [ 1 : 3 ] 1 a 3 b dtype: object Data Selection in DataFrame \u00b6 DataFrame as a Dictionary \u00b6 area = pd . Series ({ 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 }) pop = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 }) data = pd . DataFrame ({ 'area' : area , 'pop' : pop }) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 Florida 170312 19552860 Illinois 149995 12882135 data [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . area California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . area is data [ 'area' ] True data . pop is data [ 'pop' ] # pop method is refered instead of pop in our datafram False data [ 'density' ] = data [ 'pop' ] / data [ 'area' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop density California 423967 38332521 90.413926 Texas 695662 26448193 38.018740 New York 141297 19651127 139.076746 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763 DataFrame as 2D array \u00b6 data . values array([[4.23967000e+05, 3.83325210e+07, 9.04139261e+01], [6.95662000e+05, 2.64481930e+07, 3.80187404e+01], [1.41297000e+05, 1.96511270e+07, 1.39076746e+02], [1.70312000e+05, 1.95528600e+07, 1.14806121e+02], [1.49995000e+05, 1.28821350e+07, 8.58837628e+01]]) # Transposing the values data . T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } California Texas New York Florida Illinois area 4.239670e+05 6.956620e+05 1.412970e+05 1.703120e+05 1.499950e+05 pop 3.833252e+07 2.644819e+07 1.965113e+07 1.955286e+07 1.288214e+07 density 9.041393e+01 3.801874e+01 1.390767e+02 1.148061e+02 8.588376e+01 data . values [ 0 ] array([4.23967000e+05, 3.83325210e+07, 9.04139261e+01]) data [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . iloc [: 3 ,: 2 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 data . loc [: 'Illinois' ,: 'pop' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 Florida 170312 19552860 Illinois 149995 12882135 data . loc [ data . density > 100 ,[ 'pop' , 'density' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop density New York 19651127 139.076746 Florida 19552860 114.806121 data . iloc [ 0 , 2 ] = 90 data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop density California 423967 38332521 90.000000 Texas 695662 26448193 38.018740 New York 141297 19651127 139.076746 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763","title":"02 Data Indexing and Selection"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#02-data-indexing-and-selection","text":"Data Indexing and Selection Data Selection in Series Series as one-dimensional array Indexers: loc, iloc, and ix Data Selection in DataFrame DataFrame as a Dictionary DataFrame as 2D array","title":"02 - Data Indexing and Selection"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#data-indexing-and-selection","text":"In Chapter 2, we looked in detail at methods and tools to access, set, and modify values in NumPy arrays. These included indexing (e.g., arr[2, 1] ), slicing (e.g., arr[:,1:5]) , masking (e.g., arr[arr > 0] ), fancy indexing (e.g., arr[0, [1, 5]] ), and combinations thereof (e.g., arr[:, [1, 5]] ). Here we\u2019ll look at similar means of accessing and modifying values in Pandas Series and DataFrame objects. If you have used the NumPy patterns, the corresponding patterns in Pandas will feel very famil\u2010 iar, though there are a few quirks to be aware of.","title":"Data Indexing and Selection"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#data-selection-in-series","text":"As we saw in the previous section, a Series object acts in many ways like a one- dimensional NumPy array, and in many ways like a standard Python dictionary. If we keep these two overlapping analogies in mind, it will help us to understand the patterns of data indexing and selection in these arrays. ### Series as dictionary Like a dictionary, the Series object provides a mapping from a collection of keys to a collection of values: import pandas as pd data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 data [ 'b' ] 0.5 # also use dictionary-like python experssions and methods to examine the keys/indices and values 'a' in data True data . keys () Index(['a', 'b', 'c', 'd'], dtype='object') list ( data . items ()) [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)] Series objects can even be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a Series by assigning to a new index value data [ 'e' ] = 1.25 data a 0.25 b 0.50 c 0.75 d 1.00 e 1.25 dtype: float64","title":"Data Selection in Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#series-as-one-dimensional-array","text":"A Series builds on this dictionary-like interface and provides array-style item selec\u2010 tion via the same basic mechanisms as NumPy arrays\u2014that is, slices, masking, and fancy indexing. Examples of these are as follows: # slicing by explicit index data [ 'a' : 'c' ] a 0.25 b 0.50 c 0.75 dtype: float64 #slicing by implicit integer index data [ 0 : 2 ] a 0.25 b 0.50 dtype: float64 #masking data [( data > 0.3 ) & ( data < 0.8 )] b 0.50 c 0.75 dtype: float64 #fancy indexing data [[ 'a' , 'e' ]] a 0.25 e 1.25 dtype: float64 Among these, slicing may be the source of the most confusion. Notice that when you are slicing with an explicit index (i.e., data['a':'c'] ), the final index is included in the slice, while when you\u2019re slicing with an implicit index (i.e., data[0:2] ), the final index is excluded from the slice.","title":"Series as one-dimensional array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#indexers-loc-iloc-and-ix","text":"These slicing and indexing conventions can be a source of confusion. For example, if your Series has an explicit integer index, an indexing operation such as data[1] will use the explicit indices, while a slicing operation like data[1:3] will use the implicit Python-style index. data = pd . Series ([ 'a' , 'b' , 'c' ], index = [ 1 , 3 , 5 ]) data 1 a 3 b 5 c dtype: object # explicit index when indexing data [ 1 ] 'a' data [ 3 ] 'b' # implicit index when slicing data [ 1 : 3 ] 3 b 5 c dtype: object Because of this potential confusion in the case of integer indexes, Pandas provides some special indexer attributes that explicitly expose certain indexing schemes. These are not functional methods, but attributes that expose a particular slicing interface to the data in the Series. # First, the loc attribute allows indexing and slicing that always references the explicit index: data . loc [ 1 ] 'a' data . loc [ 1 : 3 ] 1 a 3 b dtype: object # The iloc attribute allows indexing and slicing that always references the implicit Python-style index data . iloc [ 1 ] 'b' data . iloc [ 0 ] 'a' data . loc [ 0 ] KeyError: 0 data . iloc [ 1 : 3 ] 3 b 5 c dtype: object data . loc [ 1 : 3 ] 1 a 3 b dtype: object","title":"Indexers: loc, iloc, and ix"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#data-selection-in-dataframe","text":"","title":"Data Selection in DataFrame"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#dataframe-as-a-dictionary","text":"area = pd . Series ({ 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 }) pop = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 }) data = pd . DataFrame ({ 'area' : area , 'pop' : pop }) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 Florida 170312 19552860 Illinois 149995 12882135 data [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . area California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . area is data [ 'area' ] True data . pop is data [ 'pop' ] # pop method is refered instead of pop in our datafram False data [ 'density' ] = data [ 'pop' ] / data [ 'area' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop density California 423967 38332521 90.413926 Texas 695662 26448193 38.018740 New York 141297 19651127 139.076746 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763","title":"DataFrame as a Dictionary"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/#dataframe-as-2d-array","text":"data . values array([[4.23967000e+05, 3.83325210e+07, 9.04139261e+01], [6.95662000e+05, 2.64481930e+07, 3.80187404e+01], [1.41297000e+05, 1.96511270e+07, 1.39076746e+02], [1.70312000e+05, 1.95528600e+07, 1.14806121e+02], [1.49995000e+05, 1.28821350e+07, 8.58837628e+01]]) # Transposing the values data . T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } California Texas New York Florida Illinois area 4.239670e+05 6.956620e+05 1.412970e+05 1.703120e+05 1.499950e+05 pop 3.833252e+07 2.644819e+07 1.965113e+07 1.955286e+07 1.288214e+07 density 9.041393e+01 3.801874e+01 1.390767e+02 1.148061e+02 8.588376e+01 data . values [ 0 ] array([4.23967000e+05, 3.83325210e+07, 9.04139261e+01]) data [ 'area' ] California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 data . iloc [: 3 ,: 2 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 data . loc [: 'Illinois' ,: 'pop' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop California 423967 38332521 Texas 695662 26448193 New York 141297 19651127 Florida 170312 19552860 Illinois 149995 12882135 data . loc [ data . density > 100 ,[ 'pop' , 'density' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop density New York 19651127 139.076746 Florida 19552860 114.806121 data . iloc [ 0 , 2 ] = 90 data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area pop density California 423967 38332521 90.000000 Texas 695662 26448193 38.018740 New York 141297 19651127 139.076746 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763","title":"DataFrame as 2D array"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 03 -Operations on Data in Pandas \u00b6 import numpy as np import pandas as pd rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . randint ( 0 , 10 , 4 )) ser 0 6 1 3 2 7 3 4 dtype: int64 df = pd . DataFrame ( rng . randint ( 0 , 10 ,( 3 , 4 )), columns = [ 'A' , \"B\" , 'C' , 'D' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 np . exp ( ser ) 0 403.428793 1 20.085537 2 1096.633158 3 54.598150 dtype: float64 np . sin ( df * np . pi / 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 -1.000000 7.071068e-01 1.000000 -1.000000e+00 1 -0.707107 1.224647e-16 0.707107 -7.071068e-01 2 -0.707107 1.000000e+00 -0.707107 1.224647e-16 area = pd . Series ({ 'Alaska' : 1723337 , 'Texas' : 695662 , 'California' : 423967 }, name = 'area' ) population = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 }, name = 'population' ) area Alaska 1723337 Texas 695662 California 423967 Name: area, dtype: int64 population California 38332521 Texas 26448193 New York 19651127 Name: population, dtype: int64 population / area Alaska NaN California 90.413926 New York NaN Texas 38.018740 dtype: float64 area . index | population . index /tmp/ipykernel_15930/3572280633.py:1: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__. Use index.union(other) instead. area.index | population.index Index(['Alaska', 'California', 'New York', 'Texas'], dtype='object')","title":"03 Operating on Data in Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/#03-operations-on-data-in-pandas","text":"import numpy as np import pandas as pd rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . randint ( 0 , 10 , 4 )) ser 0 6 1 3 2 7 3 4 dtype: int64 df = pd . DataFrame ( rng . randint ( 0 , 10 ,( 3 , 4 )), columns = [ 'A' , \"B\" , 'C' , 'D' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 np . exp ( ser ) 0 403.428793 1 20.085537 2 1096.633158 3 54.598150 dtype: float64 np . sin ( df * np . pi / 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 -1.000000 7.071068e-01 1.000000 -1.000000e+00 1 -0.707107 1.224647e-16 0.707107 -7.071068e-01 2 -0.707107 1.000000e+00 -0.707107 1.224647e-16 area = pd . Series ({ 'Alaska' : 1723337 , 'Texas' : 695662 , 'California' : 423967 }, name = 'area' ) population = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 }, name = 'population' ) area Alaska 1723337 Texas 695662 California 423967 Name: area, dtype: int64 population California 38332521 Texas 26448193 New York 19651127 Name: population, dtype: int64 population / area Alaska NaN California 90.413926 New York NaN Texas 38.018740 dtype: float64 area . index | population . index /tmp/ipykernel_15930/3572280633.py:1: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__. Use index.union(other) instead. area.index | population.index Index(['Alaska', 'California', 'New York', 'Texas'], dtype='object')","title":"03 -Operations on Data in Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 04 - Handling Missing Data \u00b6 None: Pythonic missing data NaN: Missing numerical data Detecting null values Filling null values None: Pythonic missing data \u00b6 import numpy as np import pandas as pd vals1 = np . array ([ 1 , None , 3 , 4 ]) vals1 array([1, None, 3, 4], dtype=object) for dtype in [ 'object' , 'int' ]: print ( \"dtype = \" , dtype ) % timeit np . arange ( 1E6 , dtype = dtype ) . sum () print () dtype = object 57.5 ms \u00b1 707 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) dtype = int 1.09 ms \u00b1 35.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) # My get error on arregate functions of numpy vals1 . sum () TypeError: unsupported operand type(s) for +: 'int' and 'NoneType' NaN: Missing numerical data \u00b6 vals2 = np . array ([ 1 , np . nan , 3 , 4 ]) vals2 . dtype vals1 . dtype 1 + np . nan 0 * np . nan vals2 . sum (), vals2 . min (), vals2 . max () NumPy does provide some special aggregations that will ignore these missing values Detecting null values \u00b6 Pandas data structures have two useful methods for detecting null data: isnull() and notnull(). Either one will return a Boolean mask over the data. data . isnull () # Deleting all null values data = pd . Series ([ 1 , np . nan , 'hello' , None ]) data df [ 3 ] = np . nan df NameError: name 'df' is not defined rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . randint ( 0 , 10 , 4 )) df = pd . DataFrame ( rng . randint ( 0 , 10 ,( 3 , 4 )), columns = [ 'A' , \"B\" , 'C' , 'D' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 df [ 3 ] = np . nan df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 3 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . columns = [ 1 , 2 , 3 , 4 , 5 ] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . dropna ( axis = 'columns' , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 df . dropna ( axis = 'rows' , thresh = 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN Filling null values \u00b6 Sometimes rather than dropping NA values, you\u2019d rather replace them with a valid value. This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values. You could do this in-place using the isnull() method as a mask, but because it is such a common operation Pandas provides the fillna() method, which returns a copy of the array with the null values replaced. data = pd . Series ([ 1 , np . nan , 2 , None , 3 ], index = list ( 'abcde' )) data a 1.0 b NaN c 2.0 d NaN e 3.0 dtype: float64 # filling na values with a single 0 sum ( data . isnull ()) 2 data . fillna ( - 1 ) a 1.0 b -1.0 c 2.0 d -1.0 e 3.0 dtype: float64 #forward-fill --> propagates the previous value forward data . fillna ( method = 'ffill' ) a 1.0 b 1.0 c 2.0 d 2.0 e 3.0 dtype: float64 # Back fill, to propgate the next value backward data . fillna ( method = 'bfill' ) a 1.0 b 2.0 c 2.0 d 3.0 e 3.0 dtype: float64 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . fillna ( method = 'ffill' , axis = 1 ) # if the previous value is not available during a forward fill, the NA value remains .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6.0 9.0 2.0 6.0 6.0 1 7.0 4.0 3.0 7.0 7.0 2 7.0 2.0 5.0 4.0 4.0","title":"04 Handling Missing Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#04-handling-missing-data","text":"None: Pythonic missing data NaN: Missing numerical data Detecting null values Filling null values","title":"04 - Handling Missing Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#none-pythonic-missing-data","text":"import numpy as np import pandas as pd vals1 = np . array ([ 1 , None , 3 , 4 ]) vals1 array([1, None, 3, 4], dtype=object) for dtype in [ 'object' , 'int' ]: print ( \"dtype = \" , dtype ) % timeit np . arange ( 1E6 , dtype = dtype ) . sum () print () dtype = object 57.5 ms \u00b1 707 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) dtype = int 1.09 ms \u00b1 35.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) # My get error on arregate functions of numpy vals1 . sum () TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'","title":"None: Pythonic missing data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#nan-missing-numerical-data","text":"vals2 = np . array ([ 1 , np . nan , 3 , 4 ]) vals2 . dtype vals1 . dtype 1 + np . nan 0 * np . nan vals2 . sum (), vals2 . min (), vals2 . max () NumPy does provide some special aggregations that will ignore these missing values","title":"NaN: Missing numerical data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#detecting-null-values","text":"Pandas data structures have two useful methods for detecting null data: isnull() and notnull(). Either one will return a Boolean mask over the data. data . isnull () # Deleting all null values data = pd . Series ([ 1 , np . nan , 'hello' , None ]) data df [ 3 ] = np . nan df NameError: name 'df' is not defined rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . randint ( 0 , 10 , 4 )) df = pd . DataFrame ( rng . randint ( 0 , 10 ,( 3 , 4 )), columns = [ 'A' , \"B\" , 'C' , 'D' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 df [ 3 ] = np . nan df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 3 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . columns = [ 1 , 2 , 3 , 4 , 5 ] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . dropna ( axis = 'columns' , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 df . dropna ( axis = 'rows' , thresh = 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN","title":"Detecting null values"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/#filling-null-values","text":"Sometimes rather than dropping NA values, you\u2019d rather replace them with a valid value. This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values. You could do this in-place using the isnull() method as a mask, but because it is such a common operation Pandas provides the fillna() method, which returns a copy of the array with the null values replaced. data = pd . Series ([ 1 , np . nan , 2 , None , 3 ], index = list ( 'abcde' )) data a 1.0 b NaN c 2.0 d NaN e 3.0 dtype: float64 # filling na values with a single 0 sum ( data . isnull ()) 2 data . fillna ( - 1 ) a 1.0 b -1.0 c 2.0 d -1.0 e 3.0 dtype: float64 #forward-fill --> propagates the previous value forward data . fillna ( method = 'ffill' ) a 1.0 b 1.0 c 2.0 d 2.0 e 3.0 dtype: float64 # Back fill, to propgate the next value backward data . fillna ( method = 'bfill' ) a 1.0 b 2.0 c 2.0 d 3.0 e 3.0 dtype: float64 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6 9 2 6 NaN 1 7 4 3 7 NaN 2 7 2 5 4 NaN df . fillna ( method = 'ffill' , axis = 1 ) # if the previous value is not available during a forward fill, the NA value remains .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 0 6.0 9.0 2.0 6.0 6.0 1 7.0 4.0 3.0 7.0 7.0 2 7.0 2.0 5.0 4.0 4.0","title":"Filling null values"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 05 - Hierarchical Indexing \u00b6 Hierarchical Indexing (Multi-indexing) A Multiply Indexed Series Methods of MultiIndex Creation Explicit MultiIndex constructors MultiIndex level names MultiIndex for columns Indexing and Slicing a MultiIndex Rearranging Multi-Indices Sorted and unsorted indices Index setting and resetting Data Aggregations on Multi-Indices Hierarchical Indexing (Multi-indexing) \u00b6 a far more common pattern in practice is to make use of hierarchical indexing (also known as multi-indexing) to incorporate multiple index levels within a single index. In this way, higher-dimensional data can be compactly represented within the familiar one-dimensional Series and two-dimensional DataFrame objects. A Multiply Indexed Series \u00b6 Let\u2019s start by considering how we might represent two-dimensional data within a one-dimensional Series. For concreteness, we will consider a series of data where each point has a character and numerical key. import numpy as np import pandas as pd # the bad way index = [( 'California' , 2000 ), ( 'California' , 2010 ), ( 'New York' , 2000 ), ( 'New York' , 2010 ), ( 'Texas' , 2000 ), ( 'Texas' , 2010 )] populations = [ 33871648 , 37253956 , 18976457 , 19378102 , 20851820 , 25145561 ] pop = pd . Series ( populations , index = index ) pop (California, 2000) 33871648 (California, 2010) 37253956 (New York, 2000) 18976457 (New York, 2010) 19378102 (Texas, 2000) 20851820 (Texas, 2010) 25145561 dtype: int64 # Indexing pop [( 'New York' , 2010 ):( 'Texas' , 2010 )] (New York, 2010) 19378102 (Texas, 2000) 20851820 (Texas, 2010) 25145561 dtype: int64 #A better way index = pd . MultiIndex . from_tuples ( index ) index MultiIndex([('California', 2000), ('California', 2010), ( 'New York', 2000), ( 'New York', 2010), ( 'Texas', 2000), ( 'Texas', 2010)], ) pop = pop . reindex ( index ) pop California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop [:, 2010 ] California 37253956 New York 19378102 Texas 25145561 dtype: int64 # Multi-index as extr dimension pop_df = pop . unstack () pop_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2000 2010 California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561 pop_df . stack () California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop_df = pd . DataFrame ({ 'total' : pop , 'under18' : [ 9267089 , 9284094 , 4687374 , 4318033 , 5906301 , 6879014 ]}) pop_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total under18 California 2000 33871648 9267089 2010 37253956 9284094 New York 2000 18976457 4687374 2010 19378102 4318033 Texas 2000 20851820 5906301 2010 25145561 6879014 Methods of MultiIndex Creation \u00b6 The most straightforward way to construct a multiply indexed Series or DataFrame is to simply pass a list of two or more index arrays to the constructor. df = pd . DataFrame ( np . random . rand ( 4 , 2 ), index = [[ 'a' , 'a' , 'b' , 'b' ],[ 1 , 2 , 1 , 2 ]], columns = [ 'data1' , 'data2' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 a 1 0.812128 0.312338 2 0.769851 0.255045 b 1 0.904529 0.364216 2 0.139294 0.501778 data = {( 'California' , 2000 ): 33871648 , ( 'California' , 2010 ): 37253956 , ( 'Texas' , 2000 ): 20851820 , ( 'Texas' , 2010 ): 25145561 , ( 'New York' , 2000 ): 18976457 , ( 'New York' , 2010 ): 19378102 } pd . Series ( data ) California 2000 33871648 2010 37253956 Texas 2000 20851820 2010 25145561 New York 2000 18976457 2010 19378102 dtype: int64 Explicit MultiIndex constructors \u00b6 For more flexibility in how the index is constructed, you can instead use the class method constructors available in the pd.MultiIndex. For example, as we did before, you can construct the MultiIndex from a simple list of arrays, giving the index values within each level: pd . MultiIndex . from_arrays ([[ 'a' , 'a' , 'b' , 'b' ],[ 1 , 2 , 1 , 2 ]]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) pd . MultiIndex . from_tuples ([( 'a' , 1 ),( 'a' , 2 ),( 'b' , 1 ),( 'b' , 2 )]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) pd . MultiIndex . from_product ([[ 'a' , 'b' ],[ 1 , 2 ]]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) MultiIndex level names \u00b6 Sometimes it is convenient to name the levels of the MultiIndex. You can accomplish this by passing the names argument to any of the above MultiIndex constructors, or by setting the names attribute of the index after the fact: pop . index MultiIndex([('California', 2000), ('California', 2010), ( 'New York', 2000), ( 'New York', 2010), ( 'Texas', 2000), ( 'Texas', 2010)], ) pop . index . names = [ 'state' , 'year' ] pop state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year 2000 2010 state California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561 MultiIndex for columns \u00b6 In a DataFrame, the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well # hierarchical indices and columns index = pd . MultiIndex . from_product ([[ 2013 , 2014 ], [ 1 , 2 ]], names = [ 'year' , 'visit' ]) columns = pd . MultiIndex . from_product ([[ 'Bob' , 'Guido' , 'Sue' ], [ 'HR' , 'Temp' ]], names = [ 'subject' , 'type' ]) # mock some data data = np . round ( np . random . randn ( 4 , 6 ), 1 ) data [:, :: 2 ] *= 10 data += 37 # create the DataFrame health_data = pd . DataFrame ( data , index = index , columns = columns ) health_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } subject Bob Guido Sue type HR Temp HR Temp HR Temp year visit 2013 1 43.0 36.1 37.0 35.9 39.0 38.6 2 13.0 35.6 36.0 37.6 30.0 35.8 2014 1 35.0 38.2 47.0 35.6 43.0 37.2 2 43.0 37.8 25.0 36.4 34.0 36.7 health_data [ 'Guido' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type HR Temp year visit 2013 1 37.0 35.9 2 36.0 37.6 2014 1 47.0 35.6 2 25.0 36.4 Indexing and Slicing a MultiIndex \u00b6 Indexing and slicing on a MultiIndex is designed to be intuitive, and it helps if you think about the indices as added dimensions. We\u2019ll first look at indexing multiply indexed Series, and then multiply indexed DataFrames. # Mutiply indexed Series pop state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop [ 'California' ] year 2000 33871648 2010 37253956 dtype: int64 pop [ 'California' ][ '2000' ] KeyError: '2000' pop [ 'California' , '2000' ] pop . loc [ 'California' : 'New York' ] pop [:, 2000 ] pop [ pop > 2200000 ] pop [[ 'California' , 'Texas' ]] Rearranging Multi-Indices \u00b6 One of the keys to working with multiply indexed data is knowing how to effectively transform the data. There are a number of operations that will preserve all the infor\u2010 mation in the dataset, but rearrange it for the purposes of various computations. We saw a brief example of this in the stack() and unstack() methods, but there are many more ways to finely control the rearrangement of data between hierarchical indices and columns, Sorted and unsorted indices \u00b6 Earlier, we briefly mentioned a caveat, but we should emphasize it more here. Many of the MultiIndex slicing operations will fail if the index is not sorted. Let\u2019s take a look at this here. We\u2019ll start by creating some simple multiply indexed data where the indices are not lexographically sorted: index = pd . MultiIndex . from_product ([[ 'a' , 'c' , 'b' ], [ 1 , 2 ]]) data = pd . Series ( np . random . rand ( 6 ), index = index ) data . index . names = [ 'char' , 'int' ] data try : data [ 'a' : 'b' ] except KeyError as e : print ( type ( e )) print ( e ) data = data . sort_index () data try : print ( data [ 'a' : 'b' ]) except KeyError as e : print ( type ( e )) print ( e ) pop pop . unstack ( level = 0 ) pop . unstack ( level = 1 ) pop . unstack () . stack () # Get the original dataset Index setting and resetting \u00b6 Another way to rearrange hierarchical data is to turn the index labels into columns; this can be accomplished with the reset_index method. Calling this on the popula\u2010 tion dictionary will result in a DataFrame with a state and year column holding the information that was formerly in the index. pop_flat = pop . reset_index ( name = 'population' ) pop_flat pop_flat . set_index ([ 'state' , 'year' ]) Data Aggregations on Multi-Indices \u00b6 We\u2019ve previously seen that Pandas has built-in data aggregation methods, such as mean(), sum(), and max(). For hierarchically indexed data, these can be passed a level parameter that controls which subset of the data the aggregate is computed on health_data data_mean = health_data . mean ( level = 'year' ) data_mean data_mean = health_data . mean ( axis = 1 , level = 'type' ) data_mean #pip install -U jupyter notebook Requirement already satisfied: jupyter in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (1.0.0) Requirement already satisfied: notebook in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (6.4.8) Collecting notebook Downloading notebook-6.4.12-py3-none-any.whl (9.9 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.9/9.9 MB 3.2 MB/s eta 0:00:00m eta 0:00:010:01:010m Requirement already satisfied: ipywidgets in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (7.6.5) Requirement already satisfied: ipykernel in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.9.1) Requirement already satisfied: qtconsole in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (5.3.0) Requirement already satisfied: nbconvert in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.4.4) Requirement already satisfied: jupyter-console in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.4.0) Requirement already satisfied: terminado>=0.8.3 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.13.1) Requirement already satisfied: argon2-cffi in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (21.3.0) Requirement already satisfied: ipython-genutils in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.2.0) Requirement already satisfied: nest-asyncio>=1.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (1.5.5) Requirement already satisfied: prometheus-client in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.14.1) Requirement already satisfied: jupyter-core>=4.6.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (4.10.0) Requirement already satisfied: traitlets>=4.2.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (5.1.1) Requirement already satisfied: jinja2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (3.1.2) Requirement already satisfied: jupyter-client>=5.3.4 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (6.1.12) Requirement already satisfied: nbformat in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (5.3.0) Requirement already satisfied: Send2Trash>=1.8.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (1.8.0) Requirement already satisfied: pyzmq>=17 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (23.2.0) Requirement already satisfied: tornado>=6.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (6.1) Requirement already satisfied: python-dateutil>=2.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter-client>=5.3.4->notebook) (2.8.2) Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.5.13) Requirement already satisfied: bleach in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (4.1.0) Requirement already satisfied: pandocfilters>=1.4.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (1.5.0) Requirement already satisfied: jupyterlab-pygments in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.1.2) Requirement already satisfied: pygments>=2.4.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (2.11.2) Requirement already satisfied: testpath in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.6.0) Requirement already satisfied: defusedxml in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.7.1) Requirement already satisfied: beautifulsoup4 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (4.11.1) Requirement already satisfied: entrypoints>=0.2.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.4) Requirement already satisfied: mistune<2,>=0.8.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.8.4) Requirement already satisfied: MarkupSafe>=2.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jinja2->notebook) (2.1.1) Requirement already satisfied: jsonschema>=2.6 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbformat->notebook) (4.4.0) Requirement already satisfied: fastjsonschema in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbformat->notebook) (2.15.1) Requirement already satisfied: ptyprocess in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from terminado>=0.8.3->notebook) (0.7.0) Requirement already satisfied: argon2-cffi-bindings in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook) (21.2.0) Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (1.5.1) Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (0.1.2) Requirement already satisfied: ipython>=7.23.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (8.2.0) Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter) (3.5.2) Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter) (1.0.0) Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter-console->jupyter) (3.0.20) Requirement already satisfied: qtpy>=2.0.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from qtconsole->jupyter) (2.0.1) Requirement already satisfied: decorator in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1) Requirement already satisfied: jedi>=0.16 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.1) Requirement already satisfied: setuptools>=18.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (63.4.1) Requirement already satisfied: pexpect>4.3 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0) Requirement already satisfied: pickleshare in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5) Requirement already satisfied: stack-data in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0) Requirement already satisfied: backcall in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0) Requirement already satisfied: attrs>=17.4.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->notebook) (21.4.0) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->notebook) (0.18.0) Requirement already satisfied: wcwidth in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.5) Requirement already satisfied: six>=1.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.1->jupyter-client>=5.3.4->notebook) (1.16.0) Requirement already satisfied: packaging in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from qtpy>=2.0.1->qtconsole->jupyter) (21.3) Requirement already satisfied: cffi>=1.0.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook) (1.15.1) Requirement already satisfied: soupsieve>1.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.3.1) Requirement already satisfied: webencodings in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->jupyter) (0.5.1) Requirement already satisfied: pycparser in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook) (2.21) Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from packaging->qtpy>=2.0.1->qtconsole->jupyter) (3.0.4) Requirement already satisfied: executing in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.8.3) Requirement already satisfied: asttokens in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.5) Requirement already satisfied: pure-eval in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2) Installing collected packages: notebook Attempting uninstall: notebook Found existing installation: notebook 6.4.8 Uninstalling notebook-6.4.8: Successfully uninstalled notebook-6.4.8 Successfully installed notebook-6.4.12 Note: you may need to restart the kernel to use updated packages.","title":"05 Hierarchical Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#05-hierarchical-indexing","text":"Hierarchical Indexing (Multi-indexing) A Multiply Indexed Series Methods of MultiIndex Creation Explicit MultiIndex constructors MultiIndex level names MultiIndex for columns Indexing and Slicing a MultiIndex Rearranging Multi-Indices Sorted and unsorted indices Index setting and resetting Data Aggregations on Multi-Indices","title":"05 - Hierarchical Indexing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#hierarchical-indexing-multi-indexing","text":"a far more common pattern in practice is to make use of hierarchical indexing (also known as multi-indexing) to incorporate multiple index levels within a single index. In this way, higher-dimensional data can be compactly represented within the familiar one-dimensional Series and two-dimensional DataFrame objects.","title":"Hierarchical Indexing (Multi-indexing)"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#a-multiply-indexed-series","text":"Let\u2019s start by considering how we might represent two-dimensional data within a one-dimensional Series. For concreteness, we will consider a series of data where each point has a character and numerical key. import numpy as np import pandas as pd # the bad way index = [( 'California' , 2000 ), ( 'California' , 2010 ), ( 'New York' , 2000 ), ( 'New York' , 2010 ), ( 'Texas' , 2000 ), ( 'Texas' , 2010 )] populations = [ 33871648 , 37253956 , 18976457 , 19378102 , 20851820 , 25145561 ] pop = pd . Series ( populations , index = index ) pop (California, 2000) 33871648 (California, 2010) 37253956 (New York, 2000) 18976457 (New York, 2010) 19378102 (Texas, 2000) 20851820 (Texas, 2010) 25145561 dtype: int64 # Indexing pop [( 'New York' , 2010 ):( 'Texas' , 2010 )] (New York, 2010) 19378102 (Texas, 2000) 20851820 (Texas, 2010) 25145561 dtype: int64 #A better way index = pd . MultiIndex . from_tuples ( index ) index MultiIndex([('California', 2000), ('California', 2010), ( 'New York', 2000), ( 'New York', 2010), ( 'Texas', 2000), ( 'Texas', 2010)], ) pop = pop . reindex ( index ) pop California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop [:, 2010 ] California 37253956 New York 19378102 Texas 25145561 dtype: int64 # Multi-index as extr dimension pop_df = pop . unstack () pop_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2000 2010 California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561 pop_df . stack () California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop_df = pd . DataFrame ({ 'total' : pop , 'under18' : [ 9267089 , 9284094 , 4687374 , 4318033 , 5906301 , 6879014 ]}) pop_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total under18 California 2000 33871648 9267089 2010 37253956 9284094 New York 2000 18976457 4687374 2010 19378102 4318033 Texas 2000 20851820 5906301 2010 25145561 6879014","title":"A Multiply Indexed Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#methods-of-multiindex-creation","text":"The most straightforward way to construct a multiply indexed Series or DataFrame is to simply pass a list of two or more index arrays to the constructor. df = pd . DataFrame ( np . random . rand ( 4 , 2 ), index = [[ 'a' , 'a' , 'b' , 'b' ],[ 1 , 2 , 1 , 2 ]], columns = [ 'data1' , 'data2' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 a 1 0.812128 0.312338 2 0.769851 0.255045 b 1 0.904529 0.364216 2 0.139294 0.501778 data = {( 'California' , 2000 ): 33871648 , ( 'California' , 2010 ): 37253956 , ( 'Texas' , 2000 ): 20851820 , ( 'Texas' , 2010 ): 25145561 , ( 'New York' , 2000 ): 18976457 , ( 'New York' , 2010 ): 19378102 } pd . Series ( data ) California 2000 33871648 2010 37253956 Texas 2000 20851820 2010 25145561 New York 2000 18976457 2010 19378102 dtype: int64","title":"Methods of MultiIndex Creation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#explicit-multiindex-constructors","text":"For more flexibility in how the index is constructed, you can instead use the class method constructors available in the pd.MultiIndex. For example, as we did before, you can construct the MultiIndex from a simple list of arrays, giving the index values within each level: pd . MultiIndex . from_arrays ([[ 'a' , 'a' , 'b' , 'b' ],[ 1 , 2 , 1 , 2 ]]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) pd . MultiIndex . from_tuples ([( 'a' , 1 ),( 'a' , 2 ),( 'b' , 1 ),( 'b' , 2 )]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], ) pd . MultiIndex . from_product ([[ 'a' , 'b' ],[ 1 , 2 ]]) MultiIndex([('a', 1), ('a', 2), ('b', 1), ('b', 2)], )","title":"Explicit MultiIndex constructors"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#multiindex-level-names","text":"Sometimes it is convenient to name the levels of the MultiIndex. You can accomplish this by passing the names argument to any of the above MultiIndex constructors, or by setting the names attribute of the index after the fact: pop . index MultiIndex([('California', 2000), ('California', 2010), ( 'New York', 2000), ( 'New York', 2010), ( 'Texas', 2000), ( 'Texas', 2010)], ) pop . index . names = [ 'state' , 'year' ] pop state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year 2000 2010 state California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561","title":"MultiIndex level names"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#multiindex-for-columns","text":"In a DataFrame, the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well # hierarchical indices and columns index = pd . MultiIndex . from_product ([[ 2013 , 2014 ], [ 1 , 2 ]], names = [ 'year' , 'visit' ]) columns = pd . MultiIndex . from_product ([[ 'Bob' , 'Guido' , 'Sue' ], [ 'HR' , 'Temp' ]], names = [ 'subject' , 'type' ]) # mock some data data = np . round ( np . random . randn ( 4 , 6 ), 1 ) data [:, :: 2 ] *= 10 data += 37 # create the DataFrame health_data = pd . DataFrame ( data , index = index , columns = columns ) health_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } subject Bob Guido Sue type HR Temp HR Temp HR Temp year visit 2013 1 43.0 36.1 37.0 35.9 39.0 38.6 2 13.0 35.6 36.0 37.6 30.0 35.8 2014 1 35.0 38.2 47.0 35.6 43.0 37.2 2 43.0 37.8 25.0 36.4 34.0 36.7 health_data [ 'Guido' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type HR Temp year visit 2013 1 37.0 35.9 2 36.0 37.6 2014 1 47.0 35.6 2 25.0 36.4","title":"MultiIndex for columns"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#indexing-and-slicing-a-multiindex","text":"Indexing and slicing on a MultiIndex is designed to be intuitive, and it helps if you think about the indices as added dimensions. We\u2019ll first look at indexing multiply indexed Series, and then multiply indexed DataFrames. # Mutiply indexed Series pop state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 pop [ 'California' ] year 2000 33871648 2010 37253956 dtype: int64 pop [ 'California' ][ '2000' ] KeyError: '2000' pop [ 'California' , '2000' ] pop . loc [ 'California' : 'New York' ] pop [:, 2000 ] pop [ pop > 2200000 ] pop [[ 'California' , 'Texas' ]]","title":"Indexing and Slicing a MultiIndex"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#rearranging-multi-indices","text":"One of the keys to working with multiply indexed data is knowing how to effectively transform the data. There are a number of operations that will preserve all the infor\u2010 mation in the dataset, but rearrange it for the purposes of various computations. We saw a brief example of this in the stack() and unstack() methods, but there are many more ways to finely control the rearrangement of data between hierarchical indices and columns,","title":"Rearranging Multi-Indices"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#sorted-and-unsorted-indices","text":"Earlier, we briefly mentioned a caveat, but we should emphasize it more here. Many of the MultiIndex slicing operations will fail if the index is not sorted. Let\u2019s take a look at this here. We\u2019ll start by creating some simple multiply indexed data where the indices are not lexographically sorted: index = pd . MultiIndex . from_product ([[ 'a' , 'c' , 'b' ], [ 1 , 2 ]]) data = pd . Series ( np . random . rand ( 6 ), index = index ) data . index . names = [ 'char' , 'int' ] data try : data [ 'a' : 'b' ] except KeyError as e : print ( type ( e )) print ( e ) data = data . sort_index () data try : print ( data [ 'a' : 'b' ]) except KeyError as e : print ( type ( e )) print ( e ) pop pop . unstack ( level = 0 ) pop . unstack ( level = 1 ) pop . unstack () . stack () # Get the original dataset","title":"Sorted and unsorted indices"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#index-setting-and-resetting","text":"Another way to rearrange hierarchical data is to turn the index labels into columns; this can be accomplished with the reset_index method. Calling this on the popula\u2010 tion dictionary will result in a DataFrame with a state and year column holding the information that was formerly in the index. pop_flat = pop . reset_index ( name = 'population' ) pop_flat pop_flat . set_index ([ 'state' , 'year' ])","title":"Index setting and resetting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/#data-aggregations-on-multi-indices","text":"We\u2019ve previously seen that Pandas has built-in data aggregation methods, such as mean(), sum(), and max(). For hierarchically indexed data, these can be passed a level parameter that controls which subset of the data the aggregate is computed on health_data data_mean = health_data . mean ( level = 'year' ) data_mean data_mean = health_data . mean ( axis = 1 , level = 'type' ) data_mean #pip install -U jupyter notebook Requirement already satisfied: jupyter in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (1.0.0) Requirement already satisfied: notebook in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (6.4.8) Collecting notebook Downloading notebook-6.4.12-py3-none-any.whl (9.9 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.9/9.9 MB 3.2 MB/s eta 0:00:00m eta 0:00:010:01:010m Requirement already satisfied: ipywidgets in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (7.6.5) Requirement already satisfied: ipykernel in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.9.1) Requirement already satisfied: qtconsole in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (5.3.0) Requirement already satisfied: nbconvert in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.4.4) Requirement already satisfied: jupyter-console in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter) (6.4.0) Requirement already satisfied: terminado>=0.8.3 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.13.1) Requirement already satisfied: argon2-cffi in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (21.3.0) Requirement already satisfied: ipython-genutils in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.2.0) Requirement already satisfied: nest-asyncio>=1.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (1.5.5) Requirement already satisfied: prometheus-client in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (0.14.1) Requirement already satisfied: jupyter-core>=4.6.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (4.10.0) Requirement already satisfied: traitlets>=4.2.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (5.1.1) Requirement already satisfied: jinja2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (3.1.2) Requirement already satisfied: jupyter-client>=5.3.4 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (6.1.12) Requirement already satisfied: nbformat in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (5.3.0) Requirement already satisfied: Send2Trash>=1.8.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (1.8.0) Requirement already satisfied: pyzmq>=17 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (23.2.0) Requirement already satisfied: tornado>=6.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from notebook) (6.1) Requirement already satisfied: python-dateutil>=2.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter-client>=5.3.4->notebook) (2.8.2) Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.5.13) Requirement already satisfied: bleach in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (4.1.0) Requirement already satisfied: pandocfilters>=1.4.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (1.5.0) Requirement already satisfied: jupyterlab-pygments in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.1.2) Requirement already satisfied: pygments>=2.4.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (2.11.2) Requirement already satisfied: testpath in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.6.0) Requirement already satisfied: defusedxml in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.7.1) Requirement already satisfied: beautifulsoup4 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (4.11.1) Requirement already satisfied: entrypoints>=0.2.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.4) Requirement already satisfied: mistune<2,>=0.8.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter) (0.8.4) Requirement already satisfied: MarkupSafe>=2.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jinja2->notebook) (2.1.1) Requirement already satisfied: jsonschema>=2.6 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbformat->notebook) (4.4.0) Requirement already satisfied: fastjsonschema in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from nbformat->notebook) (2.15.1) Requirement already satisfied: ptyprocess in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from terminado>=0.8.3->notebook) (0.7.0) Requirement already satisfied: argon2-cffi-bindings in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook) (21.2.0) Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (1.5.1) Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (0.1.2) Requirement already satisfied: ipython>=7.23.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter) (8.2.0) Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter) (3.5.2) Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter) (1.0.0) Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jupyter-console->jupyter) (3.0.20) Requirement already satisfied: qtpy>=2.0.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from qtconsole->jupyter) (2.0.1) Requirement already satisfied: decorator in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1) Requirement already satisfied: jedi>=0.16 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.1) Requirement already satisfied: setuptools>=18.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (63.4.1) Requirement already satisfied: pexpect>4.3 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0) Requirement already satisfied: pickleshare in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5) Requirement already satisfied: stack-data in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0) Requirement already satisfied: backcall in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0) Requirement already satisfied: attrs>=17.4.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->notebook) (21.4.0) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->notebook) (0.18.0) Requirement already satisfied: wcwidth in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.5) Requirement already satisfied: six>=1.5 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.1->jupyter-client>=5.3.4->notebook) (1.16.0) Requirement already satisfied: packaging in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from qtpy>=2.0.1->qtconsole->jupyter) (21.3) Requirement already satisfied: cffi>=1.0.1 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook) (1.15.1) Requirement already satisfied: soupsieve>1.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.3.1) Requirement already satisfied: webencodings in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->jupyter) (0.5.1) Requirement already satisfied: pycparser in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook) (2.21) Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from packaging->qtpy>=2.0.1->qtconsole->jupyter) (3.0.4) Requirement already satisfied: executing in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.8.3) Requirement already satisfied: asttokens in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.5) Requirement already satisfied: pure-eval in /home/qalmaqihir/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2) Installing collected packages: notebook Attempting uninstall: notebook Found existing installation: notebook 6.4.8 Uninstalling notebook-6.4.8: Successfully uninstalled notebook-6.4.8 Successfully installed notebook-6.4.12 Note: you may need to restart the kernel to use updated packages.","title":"Data Aggregations on Multi-Indices"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 06 - Combine Dataset: Concate and Append \u00b6 Combining Datasets: Concat and Append Simple Concatenation with pd.concat Duplicate indices Catching the repeats as an error. If you\u2019d like to simply verify that the indices in the Ignoring the index. Sometimes the index itself does not matter, and you would prefer Adding MultiIndex keys. Another alternative is to use the keys option to specify a label Concatenation with joins The append() method Combining Datasets: Concat and Append \u00b6 Some of the most interesting studies of data come from combining different data sources. These operations can involve anything from very straightforward concatena\u2010 tion of two different datasets, to more complicated database-style joins and merges that correctly handle any overlaps between the datasets. Series and DataFrames are built with this type of operation in mind, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward. import numpy as np import pandas as pd def make_df ( cols , ind ): data = { c :[ str ( c ) + str ( i ) for i in ind ] for c in cols } return pd . DataFrame ( data , ind ) make_df ( 'ABC' , range ( 3 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C 0 A0 B0 C0 1 A1 B1 C1 2 A2 B2 C2 # Recall Numpy concatenation x = [ 1 , 2 , 3 ] y = [ 4 , 3 , 6 ] z = [ 9 , 8 , 0 ] np . concatenate ([ x , y , z ]) array([1, 2, 3, 4, 3, 6, 9, 8, 0]) x = [[ 2 , 4 ],[ 3 , 5 ]] x = np . concatenate ([ x , x ], axis = 1 ) x array([[2, 4, 2, 4], [3, 5, 3, 5]]) Simple Concatenation with pd.concat \u00b6 Pandas has a function, pd.concat(), which has a similar syntax to np.concatenate but contains a number of options that we\u2019ll discuss momentarily #Signature in Pandas v0.18 pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True) ser1 = pd . Series ([ 'A' , 'B' , 'C' ], index = [ 1 , 2 , 3 ]) ser2 = pd . Series ([ 'D' , 'E' , 'F' ], index = [ 4 , 5 , 6 ]) pd . concat ([ ser1 , ser2 ]) 1 A 2 B 3 C 4 D 5 E 6 F dtype: object df3 = make_df ( \"AB\" ,[ 0 , 1 ]) df4 = make_df ( \"CD\" ,[ 0 , 1 ]) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 df4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C D 0 C0 D0 1 C1 D1 pd . concat ([ df3 , df4 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 NaN NaN 1 A1 B1 NaN NaN 0 NaN NaN C0 D0 1 NaN NaN C1 D1 pd . concat ([ df3 , df4 ], axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 Duplicate indices \u00b6 One important difference between np.concatenate and pd.concat is that Pandas concatenation preserves indices, even if the result will have duplicate indices! x = make_df ( \"AB\" ,[ 0 , 1 ]) y = make_df ( \"AB\" ,[ 2 , 3 ]) x .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A2 B2 3 A3 B3 y . index Int64Index([2, 3], dtype='int64') x . index = y . index x . index Int64Index([2, 3], dtype='int64') x .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A0 B0 3 A1 B1 y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A2 B2 3 A3 B3 pd . concat ([ x , y ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A0 B0 3 A1 B1 2 A2 B2 3 A3 B3 pd . concat ([ x , y ], axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B A B 2 A0 B0 A2 B2 3 A1 B1 A3 B3 Catching the repeats as an error. If you\u2019d like to simply verify that the indices in the \u00b6 result of pd.concat() do not overlap, you can specify the verify_integrity flag. With this set to True, the concatenation will raise an exception if there are duplicate indices. Here is an example, where for clarity we\u2019ll catch and print the error message: try : pd . concat ([ x , y ], verify_integrity = True ) except ValueError as e : print ( \"ValueError:\" , e ) ValueError: Indexes have overlapping values: Int64Index([2, 3], dtype='int64') Ignoring the index. Sometimes the index itself does not matter, and you would prefer \u00b6 it to simply be ignored. You can specify this option using the ignore_index flag. With this set to True, the concatenation will create a new integer index for the resulting Series: print ( x ); print ( y ) A B 2 A0 B0 3 A1 B1 A B 2 A2 B2 3 A3 B3 pd . concat ([ x , y ], ignore_index = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 2 A2 B2 3 A3 B3 Adding MultiIndex keys. Another alternative is to use the keys option to specify a label \u00b6 for the data sources; the result will be a hierarchically indexed series containing the data: pd . concat ([ x , y ], keys = [ 'x' , 'y' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B x 2 A0 B0 3 A1 B1 y 2 A2 B2 3 A3 B3 Concatenation with joins \u00b6 In the simple examples we just looked at, we were mainly concatenating DataFrames with shared column names. In practice, data from different sources might have differ\u2010 ent sets of column names, and pd.concat offers several options in this case. Consider the concatenation of the following two DataFrames, which have some (but not all!) columns in common: df5 = make_df ( 'ABC' ,[ 1 , 2 ]) df6 = make_df ( 'BCD' ,[ 3 , 4 ]) df6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C D 3 B3 C3 D3 4 B4 C4 D4 df5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C 1 A1 B1 C1 2 A2 B2 C2 pd . concat ([ df5 , df6 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 1 A1 B1 C1 NaN 2 A2 B2 C2 NaN 3 NaN B3 C3 D3 4 NaN B4 C4 D4 By default, the entries for which no data is available are filled with NA values. To change this, we can specify one of several options for the join and join_axes param\u2010 eters of the concatenate function. pd . concat ([ df5 , df6 ], join = 'inner' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C 1 B1 C1 2 B2 C2 3 B3 C3 4 B4 C4 pd . concat ([ df5 , df6 ], join = 'outer' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 1 A1 B1 C1 NaN 2 A2 B2 C2 NaN 3 NaN B3 C3 D3 4 NaN B4 C4 D4 The append() method \u00b6 Because direct array concatenation is so common, Series and DataFrame objects have an append method that can accomplish the same thing in fewer keystrokes. For example, rather than calling pd.concat([df1, df2]), you can simply call df1.append(df2): df1 = make_df ( 'AB' ,[ 1 , 2 ]) df2 = make_df ( 'AB' ,[ 3 , 4 ]) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 1 A1 B1 2 A2 B2 df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 3 A3 B3 4 A4 B4 df1 . append ( df2 ) /tmp/ipykernel_36950/3062608662.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df1.append(df2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 1 A1 B1 2 A2 B2 3 A3 B3 4 A4 B4","title":"06 Combine dataset Concat and Append"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#06-combine-dataset-concate-and-append","text":"Combining Datasets: Concat and Append Simple Concatenation with pd.concat Duplicate indices Catching the repeats as an error. If you\u2019d like to simply verify that the indices in the Ignoring the index. Sometimes the index itself does not matter, and you would prefer Adding MultiIndex keys. Another alternative is to use the keys option to specify a label Concatenation with joins The append() method","title":"06 - Combine Dataset: Concate and Append"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#combining-datasets-concat-and-append","text":"Some of the most interesting studies of data come from combining different data sources. These operations can involve anything from very straightforward concatena\u2010 tion of two different datasets, to more complicated database-style joins and merges that correctly handle any overlaps between the datasets. Series and DataFrames are built with this type of operation in mind, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward. import numpy as np import pandas as pd def make_df ( cols , ind ): data = { c :[ str ( c ) + str ( i ) for i in ind ] for c in cols } return pd . DataFrame ( data , ind ) make_df ( 'ABC' , range ( 3 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C 0 A0 B0 C0 1 A1 B1 C1 2 A2 B2 C2 # Recall Numpy concatenation x = [ 1 , 2 , 3 ] y = [ 4 , 3 , 6 ] z = [ 9 , 8 , 0 ] np . concatenate ([ x , y , z ]) array([1, 2, 3, 4, 3, 6, 9, 8, 0]) x = [[ 2 , 4 ],[ 3 , 5 ]] x = np . concatenate ([ x , x ], axis = 1 ) x array([[2, 4, 2, 4], [3, 5, 3, 5]])","title":"Combining Datasets: Concat and Append"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#simple-concatenation-with-pdconcat","text":"Pandas has a function, pd.concat(), which has a similar syntax to np.concatenate but contains a number of options that we\u2019ll discuss momentarily #Signature in Pandas v0.18 pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True) ser1 = pd . Series ([ 'A' , 'B' , 'C' ], index = [ 1 , 2 , 3 ]) ser2 = pd . Series ([ 'D' , 'E' , 'F' ], index = [ 4 , 5 , 6 ]) pd . concat ([ ser1 , ser2 ]) 1 A 2 B 3 C 4 D 5 E 6 F dtype: object df3 = make_df ( \"AB\" ,[ 0 , 1 ]) df4 = make_df ( \"CD\" ,[ 0 , 1 ]) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 df4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C D 0 C0 D0 1 C1 D1 pd . concat ([ df3 , df4 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 NaN NaN 1 A1 B1 NaN NaN 0 NaN NaN C0 D0 1 NaN NaN C1 D1 pd . concat ([ df3 , df4 ], axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1","title":"Simple Concatenation with pd.concat"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#duplicate-indices","text":"One important difference between np.concatenate and pd.concat is that Pandas concatenation preserves indices, even if the result will have duplicate indices! x = make_df ( \"AB\" ,[ 0 , 1 ]) y = make_df ( \"AB\" ,[ 2 , 3 ]) x .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A2 B2 3 A3 B3 y . index Int64Index([2, 3], dtype='int64') x . index = y . index x . index Int64Index([2, 3], dtype='int64') x .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A0 B0 3 A1 B1 y .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A2 B2 3 A3 B3 pd . concat ([ x , y ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2 A0 B0 3 A1 B1 2 A2 B2 3 A3 B3 pd . concat ([ x , y ], axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B A B 2 A0 B0 A2 B2 3 A1 B1 A3 B3","title":"Duplicate indices"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#catching-the-repeats-as-an-error-if-youd-like-to-simply-verify-that-the-indices-in-the","text":"result of pd.concat() do not overlap, you can specify the verify_integrity flag. With this set to True, the concatenation will raise an exception if there are duplicate indices. Here is an example, where for clarity we\u2019ll catch and print the error message: try : pd . concat ([ x , y ], verify_integrity = True ) except ValueError as e : print ( \"ValueError:\" , e ) ValueError: Indexes have overlapping values: Int64Index([2, 3], dtype='int64')","title":"Catching the repeats as an error. If you\u2019d like to simply verify that the indices in the"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#ignoring-the-index-sometimes-the-index-itself-does-not-matter-and-you-would-prefer","text":"it to simply be ignored. You can specify this option using the ignore_index flag. With this set to True, the concatenation will create a new integer index for the resulting Series: print ( x ); print ( y ) A B 2 A0 B0 3 A1 B1 A B 2 A2 B2 3 A3 B3 pd . concat ([ x , y ], ignore_index = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 A0 B0 1 A1 B1 2 A2 B2 3 A3 B3","title":"Ignoring the index. Sometimes the index itself does not matter, and you would prefer"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#adding-multiindex-keys-another-alternative-is-to-use-the-keys-option-to-specify-a-label","text":"for the data sources; the result will be a hierarchically indexed series containing the data: pd . concat ([ x , y ], keys = [ 'x' , 'y' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B x 2 A0 B0 3 A1 B1 y 2 A2 B2 3 A3 B3","title":"Adding MultiIndex keys. Another alternative is to use the keys option to specify a label"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#concatenation-with-joins","text":"In the simple examples we just looked at, we were mainly concatenating DataFrames with shared column names. In practice, data from different sources might have differ\u2010 ent sets of column names, and pd.concat offers several options in this case. Consider the concatenation of the following two DataFrames, which have some (but not all!) columns in common: df5 = make_df ( 'ABC' ,[ 1 , 2 ]) df6 = make_df ( 'BCD' ,[ 3 , 4 ]) df6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C D 3 B3 C3 D3 4 B4 C4 D4 df5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C 1 A1 B1 C1 2 A2 B2 C2 pd . concat ([ df5 , df6 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 1 A1 B1 C1 NaN 2 A2 B2 C2 NaN 3 NaN B3 C3 D3 4 NaN B4 C4 D4 By default, the entries for which no data is available are filled with NA values. To change this, we can specify one of several options for the join and join_axes param\u2010 eters of the concatenate function. pd . concat ([ df5 , df6 ], join = 'inner' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C 1 B1 C1 2 B2 C2 3 B3 C3 4 B4 C4 pd . concat ([ df5 , df6 ], join = 'outer' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 1 A1 B1 C1 NaN 2 A2 B2 C2 NaN 3 NaN B3 C3 D3 4 NaN B4 C4 D4","title":"Concatenation with joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/#the-append-method","text":"Because direct array concatenation is so common, Series and DataFrame objects have an append method that can accomplish the same thing in fewer keystrokes. For example, rather than calling pd.concat([df1, df2]), you can simply call df1.append(df2): df1 = make_df ( 'AB' ,[ 1 , 2 ]) df2 = make_df ( 'AB' ,[ 3 , 4 ]) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 1 A1 B1 2 A2 B2 df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 3 A3 B3 4 A4 B4 df1 . append ( df2 ) /tmp/ipykernel_36950/3062608662.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df1.append(df2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 1 A1 B1 2 A2 B2 3 A3 B3 4 A4 B4","title":"The append() method"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 07 - Combining Datasets: Merge and Join \u00b6 Combining Datasets: Merge and Join Categories of Joins One-to-one joins Many-to-one joins Many-to-many joins Specification of the Merge Key The on keyword The left_on and right_on keywords Specifying Set Arithmetic for Joins Overlapping Column Names: The suffixes Keyword import numpy as np import pandas as pd Combining Datasets: Merge and Join \u00b6 One essential feature offered by Pandas is its high-performance, in-memory join and merge operations. If you have ever worked with databases, you should be familiar with this type of data interaction. The main interface for this is the pd.merge func\u2010 tion, and we\u2019ll see a few examples of how this can work in practice. ### Relational Algebra The behavior implemented in pd.merge() is a subset of what is known as relational algebra, which is a formal set of rules for manipulating relational data, and forms the conceptual foundation of operations available in most databases. The strength of the relational algebra approach is that it proposes several primitive operations, which become the building blocks of more complicated operations on any dataset. With this lexicon of fundamental operations implemented efficiently in a database or other pro\u2010 gram, a wide range of fairly complicated composite operations can be performed. Pandas implements several of these fundamental building blocks in the pd.merge() function and the related join() method of Series and DataFrames. As we will see, these let you efficiently link data from different sources. Categories of Joins \u00b6 The pd.merge() function implements a number of types of joins: the one-to-one, many-to-one, and many-to-many joins. All three types of joins are accessed via an identical call to the pd.merge() interface; the type of join performed depends on the form of the input data. One-to-one joins \u00b6 Perhaps the simplest type of merge expression is the one-to-one join, which is in many ways very similar to the column-wise concatenation df1 = pd . DataFrame ({ 'employee' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'group' : [ 'Accounting' , 'Engineering' , 'Engineering' , 'HR' ]}) df2 = pd . DataFrame ({ 'employee' : [ 'Lisa' , 'Bob' , 'Jake' , 'Sue' ], 'hire_date' : [ 2004 , 2008 , 2012 , 2014 ]}) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee hire_date 0 Lisa 2004 1 Bob 2008 2 Jake 2012 3 Sue 2014 df3 = pd . merge ( df1 , df2 ) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 Many-to-one joins \u00b6 Many-to-one joins are joins in which one of the two key columns contains duplicate entries. For the many-to-one case, the resulting DataFrame will preserve those dupli\u2010 cate entries as appropriate. df4 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Engineering' , 'HR' ], 'supervisor' : [ 'Carly' , 'Guido' , 'Steve' ]}) df4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } group supervisor 0 Accounting Carly 1 Engineering Guido 2 HR Steve df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 pd . merge ( df3 , df4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date supervisor 0 Bob Accounting 2008 Carly 1 Jake Engineering 2012 Guido 2 Lisa Engineering 2004 Guido 3 Sue HR 2014 Steve Many-to-many joins \u00b6 Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined. If the key column in both the left and right array contains duplicates, then the result is a many-to-many merge. This will be perhaps most clear with a concrete example df5 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Accounting' , 'Engineering' , 'Engineering' , 'HR' , 'HR' ], 'skills' : [ 'math' , 'spreadsheets' , 'coding' , 'linux' , 'spreadsheets' , 'organization' ]}) df5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } group skills 0 Accounting math 1 Accounting spreadsheets 2 Engineering coding 3 Engineering linux 4 HR spreadsheets 5 HR organization df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR pd . merge ( df1 , df5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group skills 0 Bob Accounting math 1 Bob Accounting spreadsheets 2 Jake Engineering coding 3 Jake Engineering linux 4 Lisa Engineering coding 5 Lisa Engineering linux 6 Sue HR spreadsheets 7 Sue HR organization Specification of the Merge Key \u00b6 We\u2019ve already seen the default behavior of pd.merge(): it looks for one or more matching column names between the two inputs, and uses this as the key. However, often the column names will not match so nicely, and pd.merge() provides a variety of options for handling this. The on keyword \u00b6 Most simply, you can explicitly specify the name of the key column using the on key\u2010 word, which takes a column name or a list of column names: df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee hire_date 0 Lisa 2004 1 Bob 2008 2 Jake 2012 3 Sue 2014 pd . merge ( df1 , df2 , on = 'employee' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 The left_on and right_on keywords \u00b6 At times you may wish to merge two datasets with different column names; for exam\u2010 ple, we may have a dataset in which the employee name is labeled as \u201cname\u201d rather than \u201cemployee\u201d. In this case, we can use the left_on and right_on keywords to specify the two column names: df3 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'salary' : [ 70000 , 80000 , 120000 , 90000 ]}) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name salary 0 Bob 70000 1 Jake 80000 2 Lisa 120000 3 Sue 90000 df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR pd . merge ( df1 , df3 , left_on = 'employee' , right_on = 'name' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group name salary 0 Bob Accounting Bob 70000 1 Jake Engineering Jake 80000 2 Lisa Engineering Lisa 120000 3 Sue HR Sue 90000 # to drop the redundant column pd . merge ( df1 , df3 , left_on = 'employee' , right_on = 'name' ) . drop ( 'name' , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group salary 0 Bob Accounting 70000 1 Jake Engineering 80000 2 Lisa Engineering 120000 3 Sue HR 90000 Specifying Set Arithmetic for Joins \u00b6 In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join. This comes up when a value appears in one key column but not the other. df6 = pd . DataFrame ({ 'name' : [ 'Peter' , 'Paul' , 'Mary' ], 'food' : [ 'fish' , 'beans' , 'bread' ]}, columns = [ 'name' , 'food' ]) df7 = pd . DataFrame ({ 'name' : [ 'Mary' , 'Joseph' ], 'drink' : [ 'wine' , 'beer' ]}, columns = [ 'name' , 'drink' ]) df6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food 0 Peter fish 1 Paul beans 2 Mary bread df7 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name drink 0 Mary wine 1 Joseph beer pd . merge ( df6 , df7 , how = 'inner' ) # Interection .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Mary bread wine pd . merge ( df6 , df7 , how = 'outer' ) # union .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Peter fish NaN 1 Paul beans NaN 2 Mary bread wine 3 Joseph NaN beer pd . merge ( df6 , df7 , how = 'left' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Peter fish NaN 1 Paul beans NaN 2 Mary bread wine pd . merge ( df6 , df7 , how = 'right' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Mary bread wine 1 Joseph NaN beer Overlapping Column Names: The suffixes Keyword \u00b6 Finally, you may end up in a case where your two input DataFrames have conflicting column names. df8 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 1 , 2 , 3 , 4 ]}) df8 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank 0 Bob 1 1 Jake 2 2 Lisa 3 3 Sue 4 df9 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 3 , 1 , 4 , 2 ]}) df9 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank 0 Bob 3 1 Jake 1 2 Lisa 4 3 Sue 2 pd . merge ( df8 , df9 , on = 'name' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank_x rank_y 0 Bob 1 3 1 Jake 2 1 2 Lisa 3 4 3 Sue 4 2 **Because the output would have two conflicting column names, the merge function automatically appends a suffix _x or _y to make the output columns unique. If these defaults are inappropriate, it is possible to specify a custom suffix using the suffixes keyword:** pd . merge ( df8 , df9 , on = 'name' , suffixes = [ '_L' , '_R' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank_L rank_R 0 Bob 1 3 1 Jake 2 1 2 Lisa 3 4 3 Sue 4 2","title":"07 Combining Dataset Merge and Join"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#07-combining-datasets-merge-and-join","text":"Combining Datasets: Merge and Join Categories of Joins One-to-one joins Many-to-one joins Many-to-many joins Specification of the Merge Key The on keyword The left_on and right_on keywords Specifying Set Arithmetic for Joins Overlapping Column Names: The suffixes Keyword import numpy as np import pandas as pd","title":"07 - Combining Datasets: Merge and Join"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#combining-datasets-merge-and-join","text":"One essential feature offered by Pandas is its high-performance, in-memory join and merge operations. If you have ever worked with databases, you should be familiar with this type of data interaction. The main interface for this is the pd.merge func\u2010 tion, and we\u2019ll see a few examples of how this can work in practice. ### Relational Algebra The behavior implemented in pd.merge() is a subset of what is known as relational algebra, which is a formal set of rules for manipulating relational data, and forms the conceptual foundation of operations available in most databases. The strength of the relational algebra approach is that it proposes several primitive operations, which become the building blocks of more complicated operations on any dataset. With this lexicon of fundamental operations implemented efficiently in a database or other pro\u2010 gram, a wide range of fairly complicated composite operations can be performed. Pandas implements several of these fundamental building blocks in the pd.merge() function and the related join() method of Series and DataFrames. As we will see, these let you efficiently link data from different sources.","title":"Combining Datasets: Merge and Join"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#categories-of-joins","text":"The pd.merge() function implements a number of types of joins: the one-to-one, many-to-one, and many-to-many joins. All three types of joins are accessed via an identical call to the pd.merge() interface; the type of join performed depends on the form of the input data.","title":"Categories of Joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#one-to-one-joins","text":"Perhaps the simplest type of merge expression is the one-to-one join, which is in many ways very similar to the column-wise concatenation df1 = pd . DataFrame ({ 'employee' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'group' : [ 'Accounting' , 'Engineering' , 'Engineering' , 'HR' ]}) df2 = pd . DataFrame ({ 'employee' : [ 'Lisa' , 'Bob' , 'Jake' , 'Sue' ], 'hire_date' : [ 2004 , 2008 , 2012 , 2014 ]}) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee hire_date 0 Lisa 2004 1 Bob 2008 2 Jake 2012 3 Sue 2014 df3 = pd . merge ( df1 , df2 ) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014","title":"One-to-one joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#many-to-one-joins","text":"Many-to-one joins are joins in which one of the two key columns contains duplicate entries. For the many-to-one case, the resulting DataFrame will preserve those dupli\u2010 cate entries as appropriate. df4 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Engineering' , 'HR' ], 'supervisor' : [ 'Carly' , 'Guido' , 'Steve' ]}) df4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } group supervisor 0 Accounting Carly 1 Engineering Guido 2 HR Steve df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 pd . merge ( df3 , df4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date supervisor 0 Bob Accounting 2008 Carly 1 Jake Engineering 2012 Guido 2 Lisa Engineering 2004 Guido 3 Sue HR 2014 Steve","title":"Many-to-one joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#many-to-many-joins","text":"Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined. If the key column in both the left and right array contains duplicates, then the result is a many-to-many merge. This will be perhaps most clear with a concrete example df5 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Accounting' , 'Engineering' , 'Engineering' , 'HR' , 'HR' ], 'skills' : [ 'math' , 'spreadsheets' , 'coding' , 'linux' , 'spreadsheets' , 'organization' ]}) df5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } group skills 0 Accounting math 1 Accounting spreadsheets 2 Engineering coding 3 Engineering linux 4 HR spreadsheets 5 HR organization df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR pd . merge ( df1 , df5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group skills 0 Bob Accounting math 1 Bob Accounting spreadsheets 2 Jake Engineering coding 3 Jake Engineering linux 4 Lisa Engineering coding 5 Lisa Engineering linux 6 Sue HR spreadsheets 7 Sue HR organization","title":"Many-to-many joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#specification-of-the-merge-key","text":"We\u2019ve already seen the default behavior of pd.merge(): it looks for one or more matching column names between the two inputs, and uses this as the key. However, often the column names will not match so nicely, and pd.merge() provides a variety of options for handling this.","title":"Specification of the Merge Key"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#the-on-keyword","text":"Most simply, you can explicitly specify the name of the key column using the on key\u2010 word, which takes a column name or a list of column names: df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee hire_date 0 Lisa 2004 1 Bob 2008 2 Jake 2012 3 Sue 2014 pd . merge ( df1 , df2 , on = 'employee' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014","title":"The on keyword"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#the-left_on-and-right_on-keywords","text":"At times you may wish to merge two datasets with different column names; for exam\u2010 ple, we may have a dataset in which the employee name is labeled as \u201cname\u201d rather than \u201cemployee\u201d. In this case, we can use the left_on and right_on keywords to specify the two column names: df3 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'salary' : [ 70000 , 80000 , 120000 , 90000 ]}) df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name salary 0 Bob 70000 1 Jake 80000 2 Lisa 120000 3 Sue 90000 df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group 0 Bob Accounting 1 Jake Engineering 2 Lisa Engineering 3 Sue HR pd . merge ( df1 , df3 , left_on = 'employee' , right_on = 'name' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group name salary 0 Bob Accounting Bob 70000 1 Jake Engineering Jake 80000 2 Lisa Engineering Lisa 120000 3 Sue HR Sue 90000 # to drop the redundant column pd . merge ( df1 , df3 , left_on = 'employee' , right_on = 'name' ) . drop ( 'name' , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee group salary 0 Bob Accounting 70000 1 Jake Engineering 80000 2 Lisa Engineering 120000 3 Sue HR 90000","title":"The left_on and right_on keywords"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#specifying-set-arithmetic-for-joins","text":"In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join. This comes up when a value appears in one key column but not the other. df6 = pd . DataFrame ({ 'name' : [ 'Peter' , 'Paul' , 'Mary' ], 'food' : [ 'fish' , 'beans' , 'bread' ]}, columns = [ 'name' , 'food' ]) df7 = pd . DataFrame ({ 'name' : [ 'Mary' , 'Joseph' ], 'drink' : [ 'wine' , 'beer' ]}, columns = [ 'name' , 'drink' ]) df6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food 0 Peter fish 1 Paul beans 2 Mary bread df7 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name drink 0 Mary wine 1 Joseph beer pd . merge ( df6 , df7 , how = 'inner' ) # Interection .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Mary bread wine pd . merge ( df6 , df7 , how = 'outer' ) # union .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Peter fish NaN 1 Paul beans NaN 2 Mary bread wine 3 Joseph NaN beer pd . merge ( df6 , df7 , how = 'left' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Peter fish NaN 1 Paul beans NaN 2 Mary bread wine pd . merge ( df6 , df7 , how = 'right' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name food drink 0 Mary bread wine 1 Joseph NaN beer","title":"Specifying Set Arithmetic for Joins"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/#overlapping-column-names-the-suffixes-keyword","text":"Finally, you may end up in a case where your two input DataFrames have conflicting column names. df8 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 1 , 2 , 3 , 4 ]}) df8 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank 0 Bob 1 1 Jake 2 2 Lisa 3 3 Sue 4 df9 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 3 , 1 , 4 , 2 ]}) df9 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank 0 Bob 3 1 Jake 1 2 Lisa 4 3 Sue 2 pd . merge ( df8 , df9 , on = 'name' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank_x rank_y 0 Bob 1 3 1 Jake 2 1 2 Lisa 3 4 3 Sue 4 2 **Because the output would have two conflicting column names, the merge function automatically appends a suffix _x or _y to make the output columns unique. If these defaults are inappropriate, it is possible to specify a custom suffix using the suffixes keyword:** pd . merge ( df8 , df9 , on = 'name' , suffixes = [ '_L' , '_R' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name rank_L rank_R 0 Bob 1 3 1 Jake 2 1 2 Lisa 3 4 3 Sue 4 2","title":"Overlapping Column Names: The suffixes Keyword"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 07 - Aggregation and Grouping \u00b6 Aggregation and Grouping Aggregate, filter, transform, apply Aggregation and Grouping \u00b6 An essential piece of analysis of large data is efficient summarization: computing aggregations like sum(), mean(), median(), min(), and max(), in which a single num\u2010 ber gives insight into the nature of a potentially large dataset. In this section, we\u2019ll explore aggregations in Pandas, from simple operations akin to what we\u2019ve seen on NumPy arrays, to more sophisticated operations based on the concept of a groupby. ## Planets Data import seaborn as sns import numpy as np import pandas as pd planets = sns . load_dataset ( 'planets' ) planets . shape (1035, 6) planets . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 # simple aggregation in Pandas rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . rand ( 5 )) ser 0 0.374540 1 0.950714 2 0.731994 3 0.598658 4 0.156019 dtype: float64 ser . sum () 2.811925491708157 ser . mean () 0.5623850983416314 df = pd . DataFrame ({ 'A' : rng . rand ( 5 ), 'B' : rng . rand ( 5 )}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 0.155995 0.020584 1 0.058084 0.969910 2 0.866176 0.832443 3 0.601115 0.212339 4 0.708073 0.181825 df . mean () A 0.477888 B 0.443420 dtype: float64 df . sum () A 2.389442 B 2.217101 dtype: float64 planets . dropna () . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } number orbital_period mass distance year count 498.00000 498.000000 498.000000 498.000000 498.000000 mean 1.73494 835.778671 2.509320 52.068213 2007.377510 std 1.17572 1469.128259 3.636274 46.596041 4.167284 min 1.00000 1.328300 0.003600 1.350000 1989.000000 25% 1.00000 38.272250 0.212500 24.497500 2005.000000 50% 1.00000 357.000000 1.245000 39.940000 2009.000000 75% 2.00000 999.600000 2.867500 59.332500 2011.000000 max 6.00000 17337.500000 25.000000 354.000000 2014.000000 planets . count () method 1035 number 1035 orbital_period 992 mass 513 distance 808 year 1035 dtype: int64 These are all methods of DataFrame and Series objects. To go deeper into the data, however, simple aggregates are often not enough. The next level of data summarization is the groupby operation, which allows you to quickly and efficiently compute aggregates on subsets of data. ## GroupBy: Split, Apply, Combine Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so- called groupby operation. The name \u201cgroup by\u201d comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: split, apply, combine. ### Split, apply, combine A canonical example of this split-apply-combine operation, where the \u201capply\u201d is a summation aggregation: \u2022 The split step involves breaking up and grouping a DataFrame depending on the value of the specified key. \u2022 The apply step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups. \u2022 The combine step merges the results of these operations into an output array. While we could certainly do this manually using some combination of the masking, aggregation, and merging commands covered earlier, it\u2019s important to realize that the intermediate splits do not need to be explicitly instantiated. Rather, the GroupBy can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way. The power of the GroupBy is that it abstracts away these steps: the user need not think about how the computation is done under the hood, but rather thinks about the operation as a whole. df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data' : range ( 6 )}, columns = [ 'key' , 'data' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data 0 A 0 1 B 1 2 C 2 3 A 3 4 B 4 5 C 5 df . groupby ( 'key' ) # a groupby object is created <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f9e6c0b6a30> Notice that what is returned is not a set of DataFrames, but a DataFrameGroupBy object. This object is where the magic is: you can think of it as a special view of the DataFrame, which is poised to dig into the groups but does no actual computation until the aggregation is applied. This \u201clazy evaluation\u201d approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user. df . groupby ( 'key' ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data key A 3 B 5 C 7 Let\u2019s introduce some of the other func\u2010 tionality that can be used with the basic GroupBy operation. Column indexing. The GroupBy object supports column indexing in the same way as the DataFrame, and returns a modified GroupBy object. planets . groupby ( 'method' ) <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f9e6e0c71f0> planets . groupby ( 'method' )[ 'orbital_period' ] <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f9e6c56f9a0> Here we\u2019ve selected a particular Series group from the original DataFrame group by reference to its column name. As with the GroupBy object, no computation is done until we call some aggregate on the object: planets . groupby ( 'method' )[ 'orbital_period' ] . median () method Astrometry 631.180000 Eclipse Timing Variations 4343.500000 Imaging 27500.000000 Microlensing 3300.000000 Orbital Brightness Modulation 0.342887 Pulsar Timing 66.541900 Pulsation Timing Variations 1170.000000 Radial Velocity 360.200000 Transit 5.714932 Transit Timing Variations 57.011000 Name: orbital_period, dtype: float64 Iteration over groups . The GroupBy object supports direct iteration over the groups, returning each group as a Series or DataFrame for ( method , group ) in planets . groupby ( 'method' ): print ( \" {0:30s} shape= {1} \" . format ( method , group . shape )) Astrometry shape=(2, 6) Eclipse Timing Variations shape=(9, 6) Imaging shape=(38, 6) Microlensing shape=(23, 6) Orbital Brightness Modulation shape=(3, 6) Pulsar Timing shape=(5, 6) Pulsation Timing Variations shape=(1, 6) Radial Velocity shape=(553, 6) Transit shape=(397, 6) Transit Timing Variations shape=(4, 6) Dispatch methods. Through some Python class magic, any method not explicitly implemented by the GroupBy object will be passed through and called on the groups, whether they are DataFrame or Series objects. For example, you can use the describe() method of DataFrames to perform a set of aggregations that describe each group in the data: planets . groupby ( 'method' )[ 'year' ] . describe () . unstack () method count Astrometry 2.0 Eclipse Timing Variations 9.0 Imaging 38.0 Microlensing 23.0 Orbital Brightness Modulation 3.0 ... max Pulsar Timing 2011.0 Pulsation Timing Variations 2007.0 Radial Velocity 2014.0 Transit 2014.0 Transit Timing Variations 2014.0 Length: 80, dtype: float64 Aggregate, filter, transform, apply \u00b6 The preceding discussion focused on aggregation for the combine operation, but there are more options available. In particular, GroupBy objects have aggregate(), filter(), transform(), and apply() methods that efficiently implement a variety of useful operations before combining the grouped data. rng = np . random . RandomState ( 0 ) df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data1' : range ( 6 ), 'data2' : rng . randint ( 0 , 10 , 6 )}, columns = [ 'key' , 'data1' , 'data2' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 Aggregation. We\u2019re now familiar with GroupBy aggregations with sum(), median(), and the like, but the aggregate() method allows for even more flexibility. It can take a string, a function, or a list thereof, and compute all the aggregates at once. Here is a quick example combining all these: df . groupby ( 'key' ) . aggregate ([ 'min' , np . median , max ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } data1 data2 min median max min median max key A 0 1.5 3 3 4.0 5 B 1 2.5 4 0 3.5 7 C 2 3.5 5 3 6.0 9 df . groupby ( 'key' ) . aggregate ({ 'data1' : 'min' , 'data2' : 'max' }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 0 5 B 1 7 C 2 9 Filtering. A filtering operation allows you to drop data based on the group proper\u2010 ties. For example, we might want to keep all groups in which the standard deviation is larger than some critical value: def filter_func ( x ): return x [ 'data2' ] . std () > 4 print ( df ) key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 df . groupby ( 'key' ) . std () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 2.12132 1.414214 B 2.12132 4.949747 C 2.12132 4.242641 df . groupby ( 'key' ) . filter ( filter_func ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 1 B 1 0 2 C 2 3 4 B 4 7 5 C 5 9 Transformation. While aggregation must return a reduced version of the data, trans\u2010 formation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input. A common example is to center the data by subtracting the group-wise mean: df . groupby ( 'key' ) . transform ( lambda s : s - s . mean ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 0 -1.5 1.0 1 -1.5 -3.5 2 -1.5 -3.0 3 1.5 -1.0 4 1.5 3.5 5 1.5 3.0 The apply() method. The apply() method lets you apply an arbitrary function to the group results. The function should take a DataFrame, and return either a Pandas object (e.g., DataFrame, Series) or a scalar; the combine operation will be tailored to the type of output returned. def norm_by_data2 ( x ): x [ 'data1' ] /= x [ 'data2' ] . sum () return x df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 df . groupby ( 'key' ) . apply ( norm_by_data2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0.000000 5 1 B 0.142857 0 2 C 0.166667 3 3 A 0.375000 3 4 B 0.571429 7 5 C 0.416667 9 Specifying the split key In the simple examples presented before, we split the DataFrame on a single column name. This is just one of many options by which the groups can be defined, and we\u2019ll go through some other options for group specification here. A list, array, series, or index providing the grouping keys. The key can be any series or list with a length matching that of the DataFrame. l = [ 0 , 1 , 0 , 1 , 2 , 0 ] df . groupby ( l ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 0 7 17 1 4 3 2 4 7 df . groupby ( 'key' ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 3 8 B 5 7 C 7 12","title":"08 Aggregation and Grouping"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/#07-aggregation-and-grouping","text":"Aggregation and Grouping Aggregate, filter, transform, apply","title":"07 - Aggregation and Grouping"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/#aggregation-and-grouping","text":"An essential piece of analysis of large data is efficient summarization: computing aggregations like sum(), mean(), median(), min(), and max(), in which a single num\u2010 ber gives insight into the nature of a potentially large dataset. In this section, we\u2019ll explore aggregations in Pandas, from simple operations akin to what we\u2019ve seen on NumPy arrays, to more sophisticated operations based on the concept of a groupby. ## Planets Data import seaborn as sns import numpy as np import pandas as pd planets = sns . load_dataset ( 'planets' ) planets . shape (1035, 6) planets . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 # simple aggregation in Pandas rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . rand ( 5 )) ser 0 0.374540 1 0.950714 2 0.731994 3 0.598658 4 0.156019 dtype: float64 ser . sum () 2.811925491708157 ser . mean () 0.5623850983416314 df = pd . DataFrame ({ 'A' : rng . rand ( 5 ), 'B' : rng . rand ( 5 )}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 0.155995 0.020584 1 0.058084 0.969910 2 0.866176 0.832443 3 0.601115 0.212339 4 0.708073 0.181825 df . mean () A 0.477888 B 0.443420 dtype: float64 df . sum () A 2.389442 B 2.217101 dtype: float64 planets . dropna () . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } number orbital_period mass distance year count 498.00000 498.000000 498.000000 498.000000 498.000000 mean 1.73494 835.778671 2.509320 52.068213 2007.377510 std 1.17572 1469.128259 3.636274 46.596041 4.167284 min 1.00000 1.328300 0.003600 1.350000 1989.000000 25% 1.00000 38.272250 0.212500 24.497500 2005.000000 50% 1.00000 357.000000 1.245000 39.940000 2009.000000 75% 2.00000 999.600000 2.867500 59.332500 2011.000000 max 6.00000 17337.500000 25.000000 354.000000 2014.000000 planets . count () method 1035 number 1035 orbital_period 992 mass 513 distance 808 year 1035 dtype: int64 These are all methods of DataFrame and Series objects. To go deeper into the data, however, simple aggregates are often not enough. The next level of data summarization is the groupby operation, which allows you to quickly and efficiently compute aggregates on subsets of data. ## GroupBy: Split, Apply, Combine Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so- called groupby operation. The name \u201cgroup by\u201d comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: split, apply, combine. ### Split, apply, combine A canonical example of this split-apply-combine operation, where the \u201capply\u201d is a summation aggregation: \u2022 The split step involves breaking up and grouping a DataFrame depending on the value of the specified key. \u2022 The apply step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups. \u2022 The combine step merges the results of these operations into an output array. While we could certainly do this manually using some combination of the masking, aggregation, and merging commands covered earlier, it\u2019s important to realize that the intermediate splits do not need to be explicitly instantiated. Rather, the GroupBy can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way. The power of the GroupBy is that it abstracts away these steps: the user need not think about how the computation is done under the hood, but rather thinks about the operation as a whole. df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data' : range ( 6 )}, columns = [ 'key' , 'data' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data 0 A 0 1 B 1 2 C 2 3 A 3 4 B 4 5 C 5 df . groupby ( 'key' ) # a groupby object is created <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f9e6c0b6a30> Notice that what is returned is not a set of DataFrames, but a DataFrameGroupBy object. This object is where the magic is: you can think of it as a special view of the DataFrame, which is poised to dig into the groups but does no actual computation until the aggregation is applied. This \u201clazy evaluation\u201d approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user. df . groupby ( 'key' ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data key A 3 B 5 C 7 Let\u2019s introduce some of the other func\u2010 tionality that can be used with the basic GroupBy operation. Column indexing. The GroupBy object supports column indexing in the same way as the DataFrame, and returns a modified GroupBy object. planets . groupby ( 'method' ) <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f9e6e0c71f0> planets . groupby ( 'method' )[ 'orbital_period' ] <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f9e6c56f9a0> Here we\u2019ve selected a particular Series group from the original DataFrame group by reference to its column name. As with the GroupBy object, no computation is done until we call some aggregate on the object: planets . groupby ( 'method' )[ 'orbital_period' ] . median () method Astrometry 631.180000 Eclipse Timing Variations 4343.500000 Imaging 27500.000000 Microlensing 3300.000000 Orbital Brightness Modulation 0.342887 Pulsar Timing 66.541900 Pulsation Timing Variations 1170.000000 Radial Velocity 360.200000 Transit 5.714932 Transit Timing Variations 57.011000 Name: orbital_period, dtype: float64 Iteration over groups . The GroupBy object supports direct iteration over the groups, returning each group as a Series or DataFrame for ( method , group ) in planets . groupby ( 'method' ): print ( \" {0:30s} shape= {1} \" . format ( method , group . shape )) Astrometry shape=(2, 6) Eclipse Timing Variations shape=(9, 6) Imaging shape=(38, 6) Microlensing shape=(23, 6) Orbital Brightness Modulation shape=(3, 6) Pulsar Timing shape=(5, 6) Pulsation Timing Variations shape=(1, 6) Radial Velocity shape=(553, 6) Transit shape=(397, 6) Transit Timing Variations shape=(4, 6) Dispatch methods. Through some Python class magic, any method not explicitly implemented by the GroupBy object will be passed through and called on the groups, whether they are DataFrame or Series objects. For example, you can use the describe() method of DataFrames to perform a set of aggregations that describe each group in the data: planets . groupby ( 'method' )[ 'year' ] . describe () . unstack () method count Astrometry 2.0 Eclipse Timing Variations 9.0 Imaging 38.0 Microlensing 23.0 Orbital Brightness Modulation 3.0 ... max Pulsar Timing 2011.0 Pulsation Timing Variations 2007.0 Radial Velocity 2014.0 Transit 2014.0 Transit Timing Variations 2014.0 Length: 80, dtype: float64","title":"Aggregation and Grouping"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/#aggregate-filter-transform-apply","text":"The preceding discussion focused on aggregation for the combine operation, but there are more options available. In particular, GroupBy objects have aggregate(), filter(), transform(), and apply() methods that efficiently implement a variety of useful operations before combining the grouped data. rng = np . random . RandomState ( 0 ) df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data1' : range ( 6 ), 'data2' : rng . randint ( 0 , 10 , 6 )}, columns = [ 'key' , 'data1' , 'data2' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 Aggregation. We\u2019re now familiar with GroupBy aggregations with sum(), median(), and the like, but the aggregate() method allows for even more flexibility. It can take a string, a function, or a list thereof, and compute all the aggregates at once. Here is a quick example combining all these: df . groupby ( 'key' ) . aggregate ([ 'min' , np . median , max ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } data1 data2 min median max min median max key A 0 1.5 3 3 4.0 5 B 1 2.5 4 0 3.5 7 C 2 3.5 5 3 6.0 9 df . groupby ( 'key' ) . aggregate ({ 'data1' : 'min' , 'data2' : 'max' }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 0 5 B 1 7 C 2 9 Filtering. A filtering operation allows you to drop data based on the group proper\u2010 ties. For example, we might want to keep all groups in which the standard deviation is larger than some critical value: def filter_func ( x ): return x [ 'data2' ] . std () > 4 print ( df ) key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 df . groupby ( 'key' ) . std () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 2.12132 1.414214 B 2.12132 4.949747 C 2.12132 4.242641 df . groupby ( 'key' ) . filter ( filter_func ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 1 B 1 0 2 C 2 3 4 B 4 7 5 C 5 9 Transformation. While aggregation must return a reduced version of the data, trans\u2010 formation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input. A common example is to center the data by subtracting the group-wise mean: df . groupby ( 'key' ) . transform ( lambda s : s - s . mean ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 0 -1.5 1.0 1 -1.5 -3.5 2 -1.5 -3.0 3 1.5 -1.0 4 1.5 3.5 5 1.5 3.0 The apply() method. The apply() method lets you apply an arbitrary function to the group results. The function should take a DataFrame, and return either a Pandas object (e.g., DataFrame, Series) or a scalar; the combine operation will be tailored to the type of output returned. def norm_by_data2 ( x ): x [ 'data1' ] /= x [ 'data2' ] . sum () return x df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 3 A 3 3 4 B 4 7 5 C 5 9 df . groupby ( 'key' ) . apply ( norm_by_data2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 0 A 0.000000 5 1 B 0.142857 0 2 C 0.166667 3 3 A 0.375000 3 4 B 0.571429 7 5 C 0.416667 9 Specifying the split key In the simple examples presented before, we split the DataFrame on a single column name. This is just one of many options by which the groups can be defined, and we\u2019ll go through some other options for group specification here. A list, array, series, or index providing the grouping keys. The key can be any series or list with a length matching that of the DataFrame. l = [ 0 , 1 , 0 , 1 , 2 , 0 ] df . groupby ( l ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 0 7 17 1 4 3 2 4 7 df . groupby ( 'key' ) . sum () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 3 8 B 5 7 C 7 12","title":"Aggregate, filter, transform, apply"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 08 - Pivot Table \u00b6 Pivot Tables Motivating pivot tables Pivot Tables by Hand Pivot Table Syntax Multilevel pivot tables Pivot Tables \u00b6 We have seen how the GroupBy abstraction lets us explore relationships within a data\u2010 set. A pivot table is a similar operation that is commonly seen in spreadsheets and other programs that operate on tabular data. The pivot table takes simple column- wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data. The difference between pivot tables and GroupBy can sometimes cause confusion; it helps me to think of pivot tables as essentially a multidimensional version of GroupBy aggregation. That is, you split- apply-combine, but both the split and the combine happen across not a one- dimensional index, but across a two-dimensional grid. Motivating pivot tables \u00b6 import numpy as np import pandas as pd import seaborn as sns titanic = sns . load_dataset ( 'titanic' ) titanic . head () URLError: <urlopen error [Errno -3] Temporary failure in name resolution> Pivot Tables by Hand \u00b6 To start learning more about this data, we might begin by grouping it according to gender, survival status, or some combination thereof. If you have read the previous section, you might be tempted to apply a GroupBy operation\u2014for example, let\u2019s look at survival rate by gender: titanic . groupby ( 'sex' )[ '[survived]' ] . mean () NameError: name 'titanic' is not defined This immediately gives us some insight: overall, three of every four females on board survived, while only one in five males survived! This is useful, but we might like to go one step deeper and look at survival by both sex and, say, class. Using the vocabulary of GroupBy, we might proceed using something like this: we group by class and gender, select survival, apply a mean aggregate, com\u2010 bine the resulting groups, and then unstack the hierarchical index to reveal the hidden multidimensionality. In code: titanic . groupby ([ 'sex' , 'class' ])[ 'survived' ] . aggregate ( 'mean' ) . unstack () NameError: name 'titanic' is not defined Pivot Table Syntax \u00b6 Here is the equivalent to the preceding operation using the pivot_table method of DataFrames titanic . pivot_table ( 'survived' , index = 'sex' , column = 'class' ) NameError: name 'titanic' is not defined Multilevel pivot tables \u00b6 Just as in the GroupBy, the grouping in pivot tables can be specified with multiple lev\u2010 els, and via a number of options. For example, we might be interested in looking at age as a third dimension. We\u2019ll bin the age using the pd.cut function: age = pd . cut ( titanic [ 'age' ],[ 0 , 18 , 80 ]) titanic . pivot_table ( 'survived' ,[ 'sex' , ge ], 'class' ) NameError: name 'titanic' is not defined fare = pd . cut ( titanic [ 'fare' ], 2 ) titanic . pivot_table ( 'survived' ,[ 'sex' , ge ],[ fare , 'class' ]) NameError: name 'titanic' is not defined ### There are some additional options for the pivot tables # call signature as of Pandas 0.18 DataFrame . pivot_table ( data , values = None , index = None , columns = None , aggfunc = 'mean' , fill_value = None , margins = False , dropna = True , margins_name = 'All' ) NameError: name 'DataFrame' is not defined","title":"09 Pivot Tables"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#08-pivot-table","text":"Pivot Tables Motivating pivot tables Pivot Tables by Hand Pivot Table Syntax Multilevel pivot tables","title":"08 - Pivot Table"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#pivot-tables","text":"We have seen how the GroupBy abstraction lets us explore relationships within a data\u2010 set. A pivot table is a similar operation that is commonly seen in spreadsheets and other programs that operate on tabular data. The pivot table takes simple column- wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data. The difference between pivot tables and GroupBy can sometimes cause confusion; it helps me to think of pivot tables as essentially a multidimensional version of GroupBy aggregation. That is, you split- apply-combine, but both the split and the combine happen across not a one- dimensional index, but across a two-dimensional grid.","title":"Pivot Tables"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#motivating-pivot-tables","text":"import numpy as np import pandas as pd import seaborn as sns titanic = sns . load_dataset ( 'titanic' ) titanic . head () URLError: <urlopen error [Errno -3] Temporary failure in name resolution>","title":"Motivating pivot tables"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#pivot-tables-by-hand","text":"To start learning more about this data, we might begin by grouping it according to gender, survival status, or some combination thereof. If you have read the previous section, you might be tempted to apply a GroupBy operation\u2014for example, let\u2019s look at survival rate by gender: titanic . groupby ( 'sex' )[ '[survived]' ] . mean () NameError: name 'titanic' is not defined This immediately gives us some insight: overall, three of every four females on board survived, while only one in five males survived! This is useful, but we might like to go one step deeper and look at survival by both sex and, say, class. Using the vocabulary of GroupBy, we might proceed using something like this: we group by class and gender, select survival, apply a mean aggregate, com\u2010 bine the resulting groups, and then unstack the hierarchical index to reveal the hidden multidimensionality. In code: titanic . groupby ([ 'sex' , 'class' ])[ 'survived' ] . aggregate ( 'mean' ) . unstack () NameError: name 'titanic' is not defined","title":"Pivot Tables by Hand"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#pivot-table-syntax","text":"Here is the equivalent to the preceding operation using the pivot_table method of DataFrames titanic . pivot_table ( 'survived' , index = 'sex' , column = 'class' ) NameError: name 'titanic' is not defined","title":"Pivot Table Syntax"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/#multilevel-pivot-tables","text":"Just as in the GroupBy, the grouping in pivot tables can be specified with multiple lev\u2010 els, and via a number of options. For example, we might be interested in looking at age as a third dimension. We\u2019ll bin the age using the pd.cut function: age = pd . cut ( titanic [ 'age' ],[ 0 , 18 , 80 ]) titanic . pivot_table ( 'survived' ,[ 'sex' , ge ], 'class' ) NameError: name 'titanic' is not defined fare = pd . cut ( titanic [ 'fare' ], 2 ) titanic . pivot_table ( 'survived' ,[ 'sex' , ge ],[ fare , 'class' ]) NameError: name 'titanic' is not defined ### There are some additional options for the pivot tables # call signature as of Pandas 0.18 DataFrame . pivot_table ( data , values = None , index = None , columns = None , aggfunc = 'mean' , fill_value = None , margins = False , dropna = True , margins_name = 'All' ) NameError: name 'DataFrame' is not defined","title":"Multilevel pivot tables"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 10 - Vectorized String Operations \u00b6 Vectorized String Operations Tables of Pandas String Methods Vectorized String Operations \u00b6 One strength of Python is its relative ease in handling and manipulating string data. Pandas builds on this and provides a comprehensive set of vectorized string operations that become an essential piece of the type of munging required when one is working with (read: cleaning up) real-world data. ## Introducing Pandas String Operations We saw in previous sections how tools like NumPy and Pandas generalize arithmetic operations so that we can easily and quickly perform the same operation on many array elements. import numpy as np x = np . array ([ 2 , 3 , 5 , 7 , 11 , 13 ]) x * 2 array([ 4, 6, 10, 14, 22, 26]) This vectorization of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done. For arrays of strings, NumPy does not provide such simple access, and thus you\u2019re stuck using a more verbose loop syntax: data = [ 'peteR' , 'khan' , 'Haider' , 'KILLY' , 'GuIDol' , ' ' ] [ s . capitalize () for s in data ] ['Peter', 'Khan', 'Haider', 'Killy', 'Guidol', ' '] data = [ 'peteR' , 'khan' , 'Haider' , None , 'KILLY' , 'GuIDol' ] [ s . capitalize () for s in data ] AttributeError: 'NoneType' object has no attribute 'capitalize' Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the str attribute of Pandas Series and Index objects containing strings. import pandas as pd names = pd . Series ( data ) names 0 peteR 1 khan 2 Haider 3 None 4 KILLY 5 GuIDol dtype: object # Now we can capitalize All without any error names . str . capitalize () 0 Peter 1 Khan 2 Haider 3 None 4 Killy 5 Guidol dtype: object Tables of Pandas String Methods \u00b6 If you have a good understanding of string manipulation in Python, most of Pandas\u2019 string syntax is intuitive enough that it\u2019s probably sufficient to just list a table of avail\u2010 able methods; we will start with that here, before diving deeper into a few of the sub\u2010 tleties. monte = pd . Series ([ 'Graham Chapman' , 'John Cleese' , 'Terry Gilliam' , 'Eric Idle' , 'Terry Jones' , 'Michael Palin' ]) monte 0 Graham Chapman 1 John Cleese 2 Terry Gilliam 3 Eric Idle 4 Terry Jones 5 Michael Palin dtype: object Methods similar to Python string methods Nearly all Python\u2019s built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas str methods that mirror Python string methods: len() lower() translate() ljust() upper() startswith() isupper() islower() rjust() find() endswith() isnumeric() center() rfind() isalnum() rsplit() rstrip() capitalize() isspace() partition() lstrip() swapcase() rpartition()`","title":"10 Vectoried String Operations"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/#10-vectorized-string-operations","text":"Vectorized String Operations Tables of Pandas String Methods","title":"10 - Vectorized String Operations"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/#vectorized-string-operations","text":"One strength of Python is its relative ease in handling and manipulating string data. Pandas builds on this and provides a comprehensive set of vectorized string operations that become an essential piece of the type of munging required when one is working with (read: cleaning up) real-world data. ## Introducing Pandas String Operations We saw in previous sections how tools like NumPy and Pandas generalize arithmetic operations so that we can easily and quickly perform the same operation on many array elements. import numpy as np x = np . array ([ 2 , 3 , 5 , 7 , 11 , 13 ]) x * 2 array([ 4, 6, 10, 14, 22, 26]) This vectorization of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done. For arrays of strings, NumPy does not provide such simple access, and thus you\u2019re stuck using a more verbose loop syntax: data = [ 'peteR' , 'khan' , 'Haider' , 'KILLY' , 'GuIDol' , ' ' ] [ s . capitalize () for s in data ] ['Peter', 'Khan', 'Haider', 'Killy', 'Guidol', ' '] data = [ 'peteR' , 'khan' , 'Haider' , None , 'KILLY' , 'GuIDol' ] [ s . capitalize () for s in data ] AttributeError: 'NoneType' object has no attribute 'capitalize' Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the str attribute of Pandas Series and Index objects containing strings. import pandas as pd names = pd . Series ( data ) names 0 peteR 1 khan 2 Haider 3 None 4 KILLY 5 GuIDol dtype: object # Now we can capitalize All without any error names . str . capitalize () 0 Peter 1 Khan 2 Haider 3 None 4 Killy 5 Guidol dtype: object","title":"Vectorized String Operations"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/#tables-of-pandas-string-methods","text":"If you have a good understanding of string manipulation in Python, most of Pandas\u2019 string syntax is intuitive enough that it\u2019s probably sufficient to just list a table of avail\u2010 able methods; we will start with that here, before diving deeper into a few of the sub\u2010 tleties. monte = pd . Series ([ 'Graham Chapman' , 'John Cleese' , 'Terry Gilliam' , 'Eric Idle' , 'Terry Jones' , 'Michael Palin' ]) monte 0 Graham Chapman 1 John Cleese 2 Terry Gilliam 3 Eric Idle 4 Terry Jones 5 Michael Palin dtype: object Methods similar to Python string methods Nearly all Python\u2019s built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas str methods that mirror Python string methods: len() lower() translate() ljust() upper() startswith() isupper() islower() rjust() find() endswith() isnumeric() center() rfind() isalnum() rsplit() rstrip() capitalize() isspace() partition() lstrip() swapcase() rpartition()`","title":"Tables of Pandas String Methods"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 11 - Working with Time Series \u00b6 Working with Time Series Dates and Times in Python Native Python dates and times: datetime and dateutil Dates and times in Pandas: Best of both worlds Pandas Time Series: Indexing by Time Pandas Time Series Data Structures Regular sequences: pd.date_range() Working with Time Series \u00b6 Pandas was developed in the context of financial modeling, so as you might expect, it contains a fairly extensive set of tools for working with dates, times, and time- indexed data. Date and time data comes in a few flavors, which we will discuss here: \u2022 Time stamps reference particular moments in time (e.g., July 4 th , 2015, at 7:00a.m.). \u2022 Time intervals and periods reference a length of time between a particular beginning and end point\u2014for example, the year 2015. Periods usually reference a special case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods constituting days). \u2022 Time deltas or durations reference an exact length of time (e.g., a duration of 22.56 seconds). Dates and Times in Python \u00b6 The Python world has a number of available representations of dates, times, deltas, and timespans. While the time series tools provided by Pandas tend to be the most useful for data science applications, it is helpful to see their relationship to other packages used in Python. Native Python dates and times: datetime and dateutil \u00b6 Python\u2019s basic objects for working with dates and times reside in the built-in date time module. Along with the third-party dateutil module, you can use it to quickly perform a host of useful functionalities on dates and times. For example, you can manually build a date using the datetime type: from datetime import datetime datetime ( year = 2022 , month = 7 , day = 30 ) datetime.datetime(2022, 7, 30, 0, 0) # Using the dateutil moduel, we can parse dates from different string formats from dateutil import parser date = parser . parse ( \"30th of August 2022\" ) date datetime.datetime(2022, 8, 30, 0, 0) # once we have a datetime object, we can do things like printng the day of the week date . strftime ( '%A' ) #In the final line, we\u2019ve used one of the standard string format codes for printing dates #(\"%A\"), 'Tuesday' A related package to be aware of is pytz, which contains tools for working with the most migraine-inducing piece of time series data: time zones. The power of datetime and dateutil lies in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in. Where they break down is when you wish to work with large arrays of dates and times: just as lists of Python numerical variables are subopti\u2010 mal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates. ### Typed arrays of times: NumPy\u2019s datetime64 The weaknesses of Python\u2019s datetime format inspired the NumPy team to add a set of native time series data type to NumPy. The datetime64 dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be represented very compactly. The date time64 requires a very specific input format: import numpy as np date = np . array ( '2022-07-30' , dtype = np . datetime64 ) date array('2022-07-30', dtype='datetime64[D]') # Now we can do vectorized operation on it date + np . arange ( 12 ) array(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06', '2022-08-07', '2022-08-08', '2022-08-09', '2022-08-10'], dtype='datetime64[D]') Because of the uniform type in NumPy datetime64 arrays, this type of operation can be accomplished much more quickly than if we were working directly with Python\u2019s datetime objects, especially as arrays get large (we introduced this type of vectoriza\u2010 tion in \u201cComputation on NumPy Arrays: Universal Functions\u201d on page 50). One detail of the datetime64 and timedelta64 objects is that they are built on a fun\u2010 damental time unit. Because the datetime64 object is limited to 64-bit precision, the range of encodable times is 264 times this fundamental unit. In other words, date time64 imposes a trade-off between time resolution and maximum time span. For example, if you want a time resolution of one nanosecond, you only have enough information to encode a range of 264 nanoseconds, or just under 600 years. NumPy will infer the desired unit from the input; for example, here is a day-based datetime: np . datetime64 ( '2022-07-30 12:00' ) numpy.datetime64('2022-07-30T12:00') Notice that the time zone is automatically set to the local time on the computer exe\u2010 cuting the code. You can force any desired fundamental unit using one of many for\u2010 mat codes; for example, here we\u2019ll force a nanosecond-based time: np . datetime64 ( '2022-7-30 12:59.50' , 'ns' ) ValueError: Error parsing datetime string \"2022-7-30 12:59.50\" at position 5 Dates and times in Pandas: Best of both worlds \u00b6 Pandas builds upon all the tools just discussed to provide a Timestamp object, which combines the ease of use of datetime and dateutil with the efficient storage and vectorized interface of numpy.datetime64. From a group of these Timestamp objects, Pandas can construct a DatetimeIndex that can be used to index data in a Series or DataFrame; we\u2019ll see many examples of this below. For example, we can use Pandas tools to repeat the demonstration from above. We can parse a flexibly formatted string date, and use format codes to output the day of the week: import pandas as pd date = pd . to_datetime ( '30th August, 2022' ) date Timestamp('2022-08-30 00:00:00') date . strftime ( '%A' ) 'Tuesday' # Additionaly we can do Numpy-style Vectorized operations directly on this object date + pd . to_timedelta ( np . arange ( 12 ), 'D' ) DatetimeIndex(['2022-08-30', '2022-08-31', '2022-09-01', '2022-09-02', '2022-09-03', '2022-09-04', '2022-09-05', '2022-09-06', '2022-09-07', '2022-09-08', '2022-09-09', '2022-09-10'], dtype='datetime64[ns]', freq=None) Pandas Time Series: Indexing by Time \u00b6 Where the Pandas time series tools really become useful is when you begin to index data by timestamps. For example, we can construct a Series object that has time- indexed data: index = pd . DatetimeIndex ([ '2022-07-04' , '2022-08-04' , '2023-07-04' , '2023-08-04' ]) data = pd . Series ([ 0 , 1 , 2 , 3 ], index = index ) data 2022-07-04 0 2022-08-04 1 2023-07-04 2 2023-08-04 3 dtype: int64 data [ '2022-07-04' : '2023-07-04' ] 2022-07-04 0 2022-08-04 1 2023-07-04 2 dtype: int64 # just passing year to obtain a slice data [ '2022' ] 2022-07-04 0 2022-08-04 1 dtype: int64 data [ '2023' ] 2023-07-04 2 2023-08-04 3 dtype: int64 Pandas Time Series Data Structures \u00b6 \u2022 For time stamps, Pandas provides the Timestamp type. As mentioned before, it is essentially a replacement for Python\u2019s native datetime, but is based on the more efficient numpy.datetime64 data type. The associated index structure is DatetimeIndex. \u2022 For time periods, Pandas provides the Period type. This encodes a fixed frequency interval based on numpy.datetime64. The associated index structure is PeriodIndex. \u2022 For time deltas or durations, Pandas provides the Timedelta type. Timedelta is a more efficient replacement for Python\u2019s native datetime.timedelta type, and is based on numpy.timedelta64. The associated index structure is TimedeltaIndex. The most fundamental of these date/time objects are the Timestamp and DatetimeIn dex objects. While these class objects can be invoked directly, it is more common to use the pd.to_datetime() function, which can parse a wide variety of formats. Passing a single date to pd.to_datetime() yields a Timestamp; passing a series of dates by default yields a DatetimeIndex: dates = pd . to_datetime ([ datetime ( 2022 , 7 , 30 ), '30th of July, 2022' , '2022-7-30' , '20220730' ]) dates DatetimeIndex(['2022-07-30', '2022-07-30', '2022-07-30', '2022-07-30'], dtype='datetime64[ns]', freq=None) Any DatetimeIndex can be converted to a PeriodIndex with the to_period() func\u2010 tion with the addition of a frequency code; here we\u2019ll use \u2018D\u2019 to indicate daily frequency: dates . to_period ( 'D' ) PeriodIndex(['2022-07-30', '2022-07-30', '2022-07-30', '2022-07-30'], dtype='period[D]') # The above is a TimedeltaIndex, we can subtract one date from another dates - dates [ 0 ] TimedeltaIndex(['0 days', '0 days', '0 days', '0 days'], dtype='timedelta64[ns]', freq=None) dates = pd . to_datetime ([ datetime ( 2022 , 7 , 28 ), '29th of July, 2022' , '2022-7-30' , '20220731' ]) dates DatetimeIndex(['2022-07-28', '2022-07-29', '2022-07-30', '2022-07-31'], dtype='datetime64[ns]', freq=None) dates . to_period ( 'D' ) PeriodIndex(['2022-07-28', '2022-07-29', '2022-07-30', '2022-07-31'], dtype='period[D]') dates - dates [ 0 ] TimedeltaIndex(['0 days', '1 days', '2 days', '3 days'], dtype='timedelta64[ns]', freq=None) dates - dates [ 1 ] TimedeltaIndex(['-1 days', '0 days', '1 days', '2 days'], dtype='timedelta64[ns]', freq=None) Regular sequences: pd.date_range() \u00b6 To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: pd.date_range() for timestamps, pd.period_range() for periods, and pd.timedelta_range() for time deltas. we have seen that Python range() and NumPy\u2019s np.arange() turn a startpoint, endpoint, and optional stepsize into a sequence. Similarly, pd.date_range() accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates. By default, the fre\u2010 quency is one day: pd . date_range ( '2022-07-30' , '2023-07-30' ) DatetimeIndex(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06', '2022-08-07', '2022-08-08', ... '2023-07-21', '2023-07-22', '2023-07-23', '2023-07-24', '2023-07-25', '2023-07-26', '2023-07-27', '2023-07-28', '2023-07-29', '2023-07-30'], dtype='datetime64[ns]', length=366, freq='D') Alternatively, the date range can be specified not with a start- and endpoint, but with a startpoint and a number of periods: pd . date_range ( '2022-07-30' , periods = 8 ) DatetimeIndex(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06'], dtype='datetime64[ns]', freq='D') You can modify the spacing by altering the freq argument, which defaults to D. For example, here we will construct a range of hourly timestamps: pd . date_range ( '2022-07-30' , periods = 8 , freq = 'H' ) DatetimeIndex(['2022-07-30 00:00:00', '2022-07-30 01:00:00', '2022-07-30 02:00:00', '2022-07-30 03:00:00', '2022-07-30 04:00:00', '2022-07-30 05:00:00', '2022-07-30 06:00:00', '2022-07-30 07:00:00'], dtype='datetime64[ns]', freq='H') To create regular sequences of period or time delta values, the very similar pd.period_range() and pd.timedelta_range() functions are useful. Here are some monthly periods: pd . period_range ( '2022-07' , periods = 8 , freq = 'M' ) PeriodIndex(['2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02'], dtype='period[M]') # and a sequence of durations increasing by an hour pd . timedelta_range ( 0 , periods = 10 , freq = 'H' ) TimedeltaIndex(['0 days 00:00:00', '0 days 01:00:00', '0 days 02:00:00', '0 days 03:00:00', '0 days 04:00:00', '0 days 05:00:00', '0 days 06:00:00', '0 days 07:00:00', '0 days 08:00:00', '0 days 09:00:00'], dtype='timedelta64[ns]', freq='H')","title":"11 Working with Time Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#11-working-with-time-series","text":"Working with Time Series Dates and Times in Python Native Python dates and times: datetime and dateutil Dates and times in Pandas: Best of both worlds Pandas Time Series: Indexing by Time Pandas Time Series Data Structures Regular sequences: pd.date_range()","title":"11 - Working with Time Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#working-with-time-series","text":"Pandas was developed in the context of financial modeling, so as you might expect, it contains a fairly extensive set of tools for working with dates, times, and time- indexed data. Date and time data comes in a few flavors, which we will discuss here: \u2022 Time stamps reference particular moments in time (e.g., July 4 th , 2015, at 7:00a.m.). \u2022 Time intervals and periods reference a length of time between a particular beginning and end point\u2014for example, the year 2015. Periods usually reference a special case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods constituting days). \u2022 Time deltas or durations reference an exact length of time (e.g., a duration of 22.56 seconds).","title":"Working with Time Series"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#dates-and-times-in-python","text":"The Python world has a number of available representations of dates, times, deltas, and timespans. While the time series tools provided by Pandas tend to be the most useful for data science applications, it is helpful to see their relationship to other packages used in Python.","title":"Dates and Times in Python"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#native-python-dates-and-times-datetime-and-dateutil","text":"Python\u2019s basic objects for working with dates and times reside in the built-in date time module. Along with the third-party dateutil module, you can use it to quickly perform a host of useful functionalities on dates and times. For example, you can manually build a date using the datetime type: from datetime import datetime datetime ( year = 2022 , month = 7 , day = 30 ) datetime.datetime(2022, 7, 30, 0, 0) # Using the dateutil moduel, we can parse dates from different string formats from dateutil import parser date = parser . parse ( \"30th of August 2022\" ) date datetime.datetime(2022, 8, 30, 0, 0) # once we have a datetime object, we can do things like printng the day of the week date . strftime ( '%A' ) #In the final line, we\u2019ve used one of the standard string format codes for printing dates #(\"%A\"), 'Tuesday' A related package to be aware of is pytz, which contains tools for working with the most migraine-inducing piece of time series data: time zones. The power of datetime and dateutil lies in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in. Where they break down is when you wish to work with large arrays of dates and times: just as lists of Python numerical variables are subopti\u2010 mal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates. ### Typed arrays of times: NumPy\u2019s datetime64 The weaknesses of Python\u2019s datetime format inspired the NumPy team to add a set of native time series data type to NumPy. The datetime64 dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be represented very compactly. The date time64 requires a very specific input format: import numpy as np date = np . array ( '2022-07-30' , dtype = np . datetime64 ) date array('2022-07-30', dtype='datetime64[D]') # Now we can do vectorized operation on it date + np . arange ( 12 ) array(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06', '2022-08-07', '2022-08-08', '2022-08-09', '2022-08-10'], dtype='datetime64[D]') Because of the uniform type in NumPy datetime64 arrays, this type of operation can be accomplished much more quickly than if we were working directly with Python\u2019s datetime objects, especially as arrays get large (we introduced this type of vectoriza\u2010 tion in \u201cComputation on NumPy Arrays: Universal Functions\u201d on page 50). One detail of the datetime64 and timedelta64 objects is that they are built on a fun\u2010 damental time unit. Because the datetime64 object is limited to 64-bit precision, the range of encodable times is 264 times this fundamental unit. In other words, date time64 imposes a trade-off between time resolution and maximum time span. For example, if you want a time resolution of one nanosecond, you only have enough information to encode a range of 264 nanoseconds, or just under 600 years. NumPy will infer the desired unit from the input; for example, here is a day-based datetime: np . datetime64 ( '2022-07-30 12:00' ) numpy.datetime64('2022-07-30T12:00') Notice that the time zone is automatically set to the local time on the computer exe\u2010 cuting the code. You can force any desired fundamental unit using one of many for\u2010 mat codes; for example, here we\u2019ll force a nanosecond-based time: np . datetime64 ( '2022-7-30 12:59.50' , 'ns' ) ValueError: Error parsing datetime string \"2022-7-30 12:59.50\" at position 5","title":"Native Python dates and times: datetime and dateutil"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#dates-and-times-in-pandas-best-of-both-worlds","text":"Pandas builds upon all the tools just discussed to provide a Timestamp object, which combines the ease of use of datetime and dateutil with the efficient storage and vectorized interface of numpy.datetime64. From a group of these Timestamp objects, Pandas can construct a DatetimeIndex that can be used to index data in a Series or DataFrame; we\u2019ll see many examples of this below. For example, we can use Pandas tools to repeat the demonstration from above. We can parse a flexibly formatted string date, and use format codes to output the day of the week: import pandas as pd date = pd . to_datetime ( '30th August, 2022' ) date Timestamp('2022-08-30 00:00:00') date . strftime ( '%A' ) 'Tuesday' # Additionaly we can do Numpy-style Vectorized operations directly on this object date + pd . to_timedelta ( np . arange ( 12 ), 'D' ) DatetimeIndex(['2022-08-30', '2022-08-31', '2022-09-01', '2022-09-02', '2022-09-03', '2022-09-04', '2022-09-05', '2022-09-06', '2022-09-07', '2022-09-08', '2022-09-09', '2022-09-10'], dtype='datetime64[ns]', freq=None)","title":"Dates and times in Pandas: Best of both worlds"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#pandas-time-series-indexing-by-time","text":"Where the Pandas time series tools really become useful is when you begin to index data by timestamps. For example, we can construct a Series object that has time- indexed data: index = pd . DatetimeIndex ([ '2022-07-04' , '2022-08-04' , '2023-07-04' , '2023-08-04' ]) data = pd . Series ([ 0 , 1 , 2 , 3 ], index = index ) data 2022-07-04 0 2022-08-04 1 2023-07-04 2 2023-08-04 3 dtype: int64 data [ '2022-07-04' : '2023-07-04' ] 2022-07-04 0 2022-08-04 1 2023-07-04 2 dtype: int64 # just passing year to obtain a slice data [ '2022' ] 2022-07-04 0 2022-08-04 1 dtype: int64 data [ '2023' ] 2023-07-04 2 2023-08-04 3 dtype: int64","title":"Pandas Time Series: Indexing by Time"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#pandas-time-series-data-structures","text":"\u2022 For time stamps, Pandas provides the Timestamp type. As mentioned before, it is essentially a replacement for Python\u2019s native datetime, but is based on the more efficient numpy.datetime64 data type. The associated index structure is DatetimeIndex. \u2022 For time periods, Pandas provides the Period type. This encodes a fixed frequency interval based on numpy.datetime64. The associated index structure is PeriodIndex. \u2022 For time deltas or durations, Pandas provides the Timedelta type. Timedelta is a more efficient replacement for Python\u2019s native datetime.timedelta type, and is based on numpy.timedelta64. The associated index structure is TimedeltaIndex. The most fundamental of these date/time objects are the Timestamp and DatetimeIn dex objects. While these class objects can be invoked directly, it is more common to use the pd.to_datetime() function, which can parse a wide variety of formats. Passing a single date to pd.to_datetime() yields a Timestamp; passing a series of dates by default yields a DatetimeIndex: dates = pd . to_datetime ([ datetime ( 2022 , 7 , 30 ), '30th of July, 2022' , '2022-7-30' , '20220730' ]) dates DatetimeIndex(['2022-07-30', '2022-07-30', '2022-07-30', '2022-07-30'], dtype='datetime64[ns]', freq=None) Any DatetimeIndex can be converted to a PeriodIndex with the to_period() func\u2010 tion with the addition of a frequency code; here we\u2019ll use \u2018D\u2019 to indicate daily frequency: dates . to_period ( 'D' ) PeriodIndex(['2022-07-30', '2022-07-30', '2022-07-30', '2022-07-30'], dtype='period[D]') # The above is a TimedeltaIndex, we can subtract one date from another dates - dates [ 0 ] TimedeltaIndex(['0 days', '0 days', '0 days', '0 days'], dtype='timedelta64[ns]', freq=None) dates = pd . to_datetime ([ datetime ( 2022 , 7 , 28 ), '29th of July, 2022' , '2022-7-30' , '20220731' ]) dates DatetimeIndex(['2022-07-28', '2022-07-29', '2022-07-30', '2022-07-31'], dtype='datetime64[ns]', freq=None) dates . to_period ( 'D' ) PeriodIndex(['2022-07-28', '2022-07-29', '2022-07-30', '2022-07-31'], dtype='period[D]') dates - dates [ 0 ] TimedeltaIndex(['0 days', '1 days', '2 days', '3 days'], dtype='timedelta64[ns]', freq=None) dates - dates [ 1 ] TimedeltaIndex(['-1 days', '0 days', '1 days', '2 days'], dtype='timedelta64[ns]', freq=None)","title":"Pandas Time Series Data Structures"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/#regular-sequences-pddate_range","text":"To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: pd.date_range() for timestamps, pd.period_range() for periods, and pd.timedelta_range() for time deltas. we have seen that Python range() and NumPy\u2019s np.arange() turn a startpoint, endpoint, and optional stepsize into a sequence. Similarly, pd.date_range() accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates. By default, the fre\u2010 quency is one day: pd . date_range ( '2022-07-30' , '2023-07-30' ) DatetimeIndex(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06', '2022-08-07', '2022-08-08', ... '2023-07-21', '2023-07-22', '2023-07-23', '2023-07-24', '2023-07-25', '2023-07-26', '2023-07-27', '2023-07-28', '2023-07-29', '2023-07-30'], dtype='datetime64[ns]', length=366, freq='D') Alternatively, the date range can be specified not with a start- and endpoint, but with a startpoint and a number of periods: pd . date_range ( '2022-07-30' , periods = 8 ) DatetimeIndex(['2022-07-30', '2022-07-31', '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05', '2022-08-06'], dtype='datetime64[ns]', freq='D') You can modify the spacing by altering the freq argument, which defaults to D. For example, here we will construct a range of hourly timestamps: pd . date_range ( '2022-07-30' , periods = 8 , freq = 'H' ) DatetimeIndex(['2022-07-30 00:00:00', '2022-07-30 01:00:00', '2022-07-30 02:00:00', '2022-07-30 03:00:00', '2022-07-30 04:00:00', '2022-07-30 05:00:00', '2022-07-30 06:00:00', '2022-07-30 07:00:00'], dtype='datetime64[ns]', freq='H') To create regular sequences of period or time delta values, the very similar pd.period_range() and pd.timedelta_range() functions are useful. Here are some monthly periods: pd . period_range ( '2022-07' , periods = 8 , freq = 'M' ) PeriodIndex(['2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02'], dtype='period[M]') # and a sequence of durations increasing by an hour pd . timedelta_range ( 0 , periods = 10 , freq = 'H' ) TimedeltaIndex(['0 days 00:00:00', '0 days 01:00:00', '0 days 02:00:00', '0 days 03:00:00', '0 days 04:00:00', '0 days 05:00:00', '0 days 06:00:00', '0 days 07:00:00', '0 days 08:00:00', '0 days 09:00:00'], dtype='timedelta64[ns]', freq='H')","title":"Regular sequences: pd.date_range()"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 12 - Frequency Offset \u00b6 Frequencies and Offsets Frequencies and Offsets \u00b6 Fundamental to these Pandas time series tools is the concept of a frequency or date offset. Just as we saw the D (day) and H (hour) codes previously, we can use such codes to specify any desired frequency spacing. Code Description MS Month start BMS Business month start QS Quarter start BQS Business quarter start AS Year start BAS Business year start Code Description Code Description D Calendar day B Business day W Weekly M Month end BM Business month end Q Quarter end BQ Business quarter end A Year end BA Business year end H Hours BH Business hours T Minutes S Seconds L Milliseonds U Microseconds N Nanoseconds Additionally, you can change the month used to mark any quarterly or annual code by adding a three-letter month code as a suffix: \u2022 Q-JAN, BQ-FEB, QS-MAR, BQS-APR, etc. \u2022 A-JAN, BA-FEB, AS-MAR, BAS-APR, etc. In the same way, you can modify the split-point of the weekly frequency by adding a three-letter weekday code: \u2022 W-SUN, W-MON, W-TUE, W-WED, etc. On top of this, codes can be combined with numbers to specify other frequencies. For example, for a frequency of 2 hours 30 minutes, we can combine the hour (H) and minute (T) codes as follows: import pandas as pd pd . timedelta_range ( 0 , periods = 9 , freq = '2H30T' ) TimedeltaIndex(['0 days 00:00:00', '0 days 02:30:00', '0 days 05:00:00', '0 days 07:30:00', '0 days 10:00:00', '0 days 12:30:00', '0 days 15:00:00', '0 days 17:30:00', '0 days 20:00:00'], dtype='timedelta64[ns]', freq='150T') All of these short codes refer to specific instances of Pandas time series offsets, which can be found in the pd.tseries.offsets module. For example, we can create a busi\u2010 ness day offset directly as follows: from pandas.tseries.offsets import BDay pd . date_range ( '2022-07-28' , periods = 5 , freq = BDay ()) DatetimeIndex(['2022-07-28', '2022-07-29', '2022-08-01', '2022-08-02', '2022-08-03'], dtype='datetime64[ns]', freq='B')","title":"12 Frequency Offset"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/#12-frequency-offset","text":"Frequencies and Offsets","title":"12 - Frequency Offset"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/#frequencies-and-offsets","text":"Fundamental to these Pandas time series tools is the concept of a frequency or date offset. Just as we saw the D (day) and H (hour) codes previously, we can use such codes to specify any desired frequency spacing. Code Description MS Month start BMS Business month start QS Quarter start BQS Business quarter start AS Year start BAS Business year start Code Description Code Description D Calendar day B Business day W Weekly M Month end BM Business month end Q Quarter end BQ Business quarter end A Year end BA Business year end H Hours BH Business hours T Minutes S Seconds L Milliseonds U Microseconds N Nanoseconds Additionally, you can change the month used to mark any quarterly or annual code by adding a three-letter month code as a suffix: \u2022 Q-JAN, BQ-FEB, QS-MAR, BQS-APR, etc. \u2022 A-JAN, BA-FEB, AS-MAR, BAS-APR, etc. In the same way, you can modify the split-point of the weekly frequency by adding a three-letter weekday code: \u2022 W-SUN, W-MON, W-TUE, W-WED, etc. On top of this, codes can be combined with numbers to specify other frequencies. For example, for a frequency of 2 hours 30 minutes, we can combine the hour (H) and minute (T) codes as follows: import pandas as pd pd . timedelta_range ( 0 , periods = 9 , freq = '2H30T' ) TimedeltaIndex(['0 days 00:00:00', '0 days 02:30:00', '0 days 05:00:00', '0 days 07:30:00', '0 days 10:00:00', '0 days 12:30:00', '0 days 15:00:00', '0 days 17:30:00', '0 days 20:00:00'], dtype='timedelta64[ns]', freq='150T') All of these short codes refer to specific instances of Pandas time series offsets, which can be found in the pd.tseries.offsets module. For example, we can create a busi\u2010 ness day offset directly as follows: from pandas.tseries.offsets import BDay pd . date_range ( '2022-07-28' , periods = 5 , freq = BDay ()) DatetimeIndex(['2022-07-28', '2022-07-29', '2022-08-01', '2022-08-02', '2022-08-03'], dtype='datetime64[ns]', freq='B')","title":"Frequencies and Offsets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 13 - Resampling, Shifting and Windowing \u00b6 Resampling, Shifting, and Windowing Resampling and converting frequencies Time-shifts Rolling windows Resampling, Shifting, and Windowing \u00b6 The ability to use dates and times as indices to intuitively organize and access data is an important piece of the Pandas time series tools. The benefits of indexed data in general (automatic alignment during operations, intuitive data slicing and access, etc.) still apply, and Pandas provides several additional time series\u2013specific operations. ! conda install pandas - datareader - y Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /home/qalmaqihir/anaconda3 added / updated specs: - pandas-datareader The following packages will be downloaded: package | build ---------------------------|----------------- conda-4.14.0 | py39h06a4308_0 915 KB pandas-datareader-0.10.0 | pyhd3eb1b0_0 71 KB ------------------------------------------------------------ Total: 987 KB The following NEW packages will be INSTALLED: pandas-datareader pkgs/main/noarch::pandas-datareader-0.10.0-pyhd3eb1b0_0 The following packages will be UPDATED: conda 4.13.0-py39h06a4308_0 --> 4.14.0-py39h06a4308_0 Downloading and Extracting Packages pandas-datareader-0. | 71 KB | ##################################### | 100% conda-4.14.0 | 915 KB | ##################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done import pandas as pd from pandas_datareader import data google = data . DataReader ( 'GOOG' , start = '2004' , end = '2017' , data_source = 'yahoo' ) google . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } High Low Open Close Volume Adj Close Date 2004-08-19 2.591785 2.390042 2.490664 2.499133 897427216.0 2.499133 2004-08-20 2.716817 2.503118 2.515820 2.697639 458857488.0 2.697639 2004-08-23 2.826406 2.716070 2.758411 2.724787 366857939.0 2.724787 2004-08-24 2.779581 2.579581 2.770615 2.611960 306396159.0 2.611960 2004-08-25 2.689918 2.587302 2.614201 2.640104 184645512.0 2.640104 google . Close Date 2004-08-19 2.499133 2004-08-20 2.697639 2004-08-23 2.724787 2004-08-24 2.611960 2004-08-25 2.640104 ... 2016-12-23 39.495499 2016-12-27 39.577499 2016-12-28 39.252499 2016-12-29 39.139500 2016-12-30 38.591000 Name: Close, Length: 3115, dtype: float64 #google['Close'] % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () google [ 'Close' ] . plot (); Resampling and converting frequencies \u00b6 One common need for time series data is resampling at a higher or lower frequency. You can do this using the resample() method, or the much simpler asfreq() method. The primary difference between the two is that resample() is fundamentally a data aggregation, while asfreq() is fundamentally a data selection. google [ \"Close\" ] . plot ( alpha = 0.5 , style = '-' ) google [ \"Close\" ] . resample ( 'BA' ) . mean () . plot ( style = ':' ) google [ \"Close\" ] . asfreq ( 'BA' ) . plot ( style = '--' ); plt . legend ([ 'input' , 'resample' , 'asfreq' ], loc = 'upper left' ) <matplotlib.legend.Legend at 0x7fe1d96367c0> goog = google [ 'Close' ] fig , ax = plt . subplots ( 2 , sharex = True ) data = goog . iloc [: 10 ] data . asfreq ( 'D' ) . plot ( ax = ax [ 0 ], marker = 'o' ) data . asfreq ( 'D' , method = 'bfill' ) . plot ( ax = ax [ 1 ], style = '-o' ) data . asfreq ( 'D' , method = 'ffill' ) . plot ( ax = ax [ 1 ], style = '--o' ) ax [ 1 ] . legend ([ \"back-fill\" , \"forward-fill\" ]); Time-shifts \u00b6 Another common time series\u2013specific operation is shifting of data in time. Pandas has two closely related methods for computing this: shift() and tshift(). In short, the difference between them is that shift() shifts the data, while tshift() shifts the index. In both cases, the shift is specified in multiples of the frequenc fig , ax = plt . subplots ( 3 , sharey = True ) # apply a frequency to the data goog = goog . asfreq ( 'D' , method = 'pad' ) goog . plot ( ax = ax [ 0 ]) goog . shift ( 900 ) . plot ( ax = ax [ 1 ]) goog . tshift ( 900 ) . plot ( ax = ax [ 2 ]) # legends and annotations local_max = pd . to_datetime ( '2007-11-05' ) offset = pd . Timedelta ( 900 , 'D' ) ax [ 0 ] . legend ([ 'input' ], loc = 2 ) ax [ 0 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 0 ] . axvline ( local_max , alpha = 0.3 , color = 'red' ) ax [ 1 ] . legend ([ 'shift(900)' ], loc = 2 ) ax [ 1 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 1 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ) ax [ 2 ] . legend ([ 'tshift(900)' ], loc = 2 ) ax [ 2 ] . get_xticklabels ()[ 1 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 2 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ); /tmp/ipykernel_66268/1389856076.py:6: FutureWarning: tshift is deprecated and will be removed in a future version. Please use shift instead. goog.tshift(900).plot(ax=ax[2]) ROI = 100 * ( goog . tshift ( - 365 ) / goog - 1 ) ROI . plot () plt . ylabel ( '% Return on Investment' ); /tmp/ipykernel_66268/2632432407.py:1: FutureWarning: tshift is deprecated and will be removed in a future version. Please use shift instead. ROI = 100 * (goog.tshift(-365) / goog - 1) Rolling windows \u00b6 Rolling statistics are a third type of time series\u2013specific operation implemented by Pandas. These can be accomplished via the rolling() attribute of Series and Data Frame objects, which returns a view similar to what we saw with the groupby opera\u2010 tion rolling = goog . rolling ( 365 , center = True ) data = pd . DataFrame ({ 'input' : goog , 'one-year rolling_mean' : rolling . mean (), 'one-year rolling_std' : rolling . std ()}) ax = data . plot ( style = [ '-' , '--' , ':' ]) ax . lines [ 0 ] . set_alpha ( 0.3 )","title":"13 Resampling Shifting and Windowing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#13-resampling-shifting-and-windowing","text":"Resampling, Shifting, and Windowing Resampling and converting frequencies Time-shifts Rolling windows","title":"13 - Resampling, Shifting and Windowing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#resampling-shifting-and-windowing","text":"The ability to use dates and times as indices to intuitively organize and access data is an important piece of the Pandas time series tools. The benefits of indexed data in general (automatic alignment during operations, intuitive data slicing and access, etc.) still apply, and Pandas provides several additional time series\u2013specific operations. ! conda install pandas - datareader - y Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /home/qalmaqihir/anaconda3 added / updated specs: - pandas-datareader The following packages will be downloaded: package | build ---------------------------|----------------- conda-4.14.0 | py39h06a4308_0 915 KB pandas-datareader-0.10.0 | pyhd3eb1b0_0 71 KB ------------------------------------------------------------ Total: 987 KB The following NEW packages will be INSTALLED: pandas-datareader pkgs/main/noarch::pandas-datareader-0.10.0-pyhd3eb1b0_0 The following packages will be UPDATED: conda 4.13.0-py39h06a4308_0 --> 4.14.0-py39h06a4308_0 Downloading and Extracting Packages pandas-datareader-0. | 71 KB | ##################################### | 100% conda-4.14.0 | 915 KB | ##################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done import pandas as pd from pandas_datareader import data google = data . DataReader ( 'GOOG' , start = '2004' , end = '2017' , data_source = 'yahoo' ) google . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } High Low Open Close Volume Adj Close Date 2004-08-19 2.591785 2.390042 2.490664 2.499133 897427216.0 2.499133 2004-08-20 2.716817 2.503118 2.515820 2.697639 458857488.0 2.697639 2004-08-23 2.826406 2.716070 2.758411 2.724787 366857939.0 2.724787 2004-08-24 2.779581 2.579581 2.770615 2.611960 306396159.0 2.611960 2004-08-25 2.689918 2.587302 2.614201 2.640104 184645512.0 2.640104 google . Close Date 2004-08-19 2.499133 2004-08-20 2.697639 2004-08-23 2.724787 2004-08-24 2.611960 2004-08-25 2.640104 ... 2016-12-23 39.495499 2016-12-27 39.577499 2016-12-28 39.252499 2016-12-29 39.139500 2016-12-30 38.591000 Name: Close, Length: 3115, dtype: float64 #google['Close'] % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () google [ 'Close' ] . plot ();","title":"Resampling, Shifting, and Windowing"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#resampling-and-converting-frequencies","text":"One common need for time series data is resampling at a higher or lower frequency. You can do this using the resample() method, or the much simpler asfreq() method. The primary difference between the two is that resample() is fundamentally a data aggregation, while asfreq() is fundamentally a data selection. google [ \"Close\" ] . plot ( alpha = 0.5 , style = '-' ) google [ \"Close\" ] . resample ( 'BA' ) . mean () . plot ( style = ':' ) google [ \"Close\" ] . asfreq ( 'BA' ) . plot ( style = '--' ); plt . legend ([ 'input' , 'resample' , 'asfreq' ], loc = 'upper left' ) <matplotlib.legend.Legend at 0x7fe1d96367c0> goog = google [ 'Close' ] fig , ax = plt . subplots ( 2 , sharex = True ) data = goog . iloc [: 10 ] data . asfreq ( 'D' ) . plot ( ax = ax [ 0 ], marker = 'o' ) data . asfreq ( 'D' , method = 'bfill' ) . plot ( ax = ax [ 1 ], style = '-o' ) data . asfreq ( 'D' , method = 'ffill' ) . plot ( ax = ax [ 1 ], style = '--o' ) ax [ 1 ] . legend ([ \"back-fill\" , \"forward-fill\" ]);","title":"Resampling and converting frequencies"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#time-shifts","text":"Another common time series\u2013specific operation is shifting of data in time. Pandas has two closely related methods for computing this: shift() and tshift(). In short, the difference between them is that shift() shifts the data, while tshift() shifts the index. In both cases, the shift is specified in multiples of the frequenc fig , ax = plt . subplots ( 3 , sharey = True ) # apply a frequency to the data goog = goog . asfreq ( 'D' , method = 'pad' ) goog . plot ( ax = ax [ 0 ]) goog . shift ( 900 ) . plot ( ax = ax [ 1 ]) goog . tshift ( 900 ) . plot ( ax = ax [ 2 ]) # legends and annotations local_max = pd . to_datetime ( '2007-11-05' ) offset = pd . Timedelta ( 900 , 'D' ) ax [ 0 ] . legend ([ 'input' ], loc = 2 ) ax [ 0 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 0 ] . axvline ( local_max , alpha = 0.3 , color = 'red' ) ax [ 1 ] . legend ([ 'shift(900)' ], loc = 2 ) ax [ 1 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 1 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ) ax [ 2 ] . legend ([ 'tshift(900)' ], loc = 2 ) ax [ 2 ] . get_xticklabels ()[ 1 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 2 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ); /tmp/ipykernel_66268/1389856076.py:6: FutureWarning: tshift is deprecated and will be removed in a future version. Please use shift instead. goog.tshift(900).plot(ax=ax[2]) ROI = 100 * ( goog . tshift ( - 365 ) / goog - 1 ) ROI . plot () plt . ylabel ( '% Return on Investment' ); /tmp/ipykernel_66268/2632432407.py:1: FutureWarning: tshift is deprecated and will be removed in a future version. Please use shift instead. ROI = 100 * (goog.tshift(-365) / goog - 1)","title":"Time-shifts"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/#rolling-windows","text":"Rolling statistics are a third type of time series\u2013specific operation implemented by Pandas. These can be accomplished via the rolling() attribute of Series and Data Frame objects, which returns a view similar to what we saw with the groupby opera\u2010 tion rolling = goog . rolling ( 365 , center = True ) data = pd . DataFrame ({ 'input' : goog , 'one-year rolling_mean' : rolling . mean (), 'one-year rolling_std' : rolling . std ()}) ax = data . plot ( style = [ '-' , '--' , ':' ]) ax . lines [ 0 ] . set_alpha ( 0.3 )","title":"Rolling windows"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 Example: Visualzing Seattle Bucycle Counts \u00b6 Example: Visualizing Seattle Bicycle Counts Visualizing the Data Digging into the data Example: Visualizing Seattle Bicycle Counts \u00b6 As a more involved example of working with some time series data, let\u2019s take a look at bicycle counts on Seattle\u2019s Fremont Bridge. This data comes from an automated bicy\u2010 cle counter, installed in late 2012, which has inductive sensors on the east and west sidewalks of the bridge. ! curl - o FremontBridge . csv https : // data . seattle . gov / api / views / 65 db - xm6k / rows . csv ? accessType = DOWNLOAD % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2679k 0 2679k 0 0 170k 0 --:--:-- 0:00:15 --:--:-- 296k import pandas as pd import numpy as np data = pd . read_csv ( 'FremontBridge.csv' , index_col = 'Date' , parse_dates = True ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fremont Bridge Total Fremont Bridge East Sidewalk Fremont Bridge West Sidewalk Date 2012-10-03 00:00:00 13.0 4.0 9.0 2012-10-03 01:00:00 10.0 4.0 6.0 2012-10-03 02:00:00 2.0 1.0 1.0 2012-10-03 03:00:00 5.0 2.0 3.0 2012-10-03 04:00:00 7.0 6.0 1.0 data . columns = [ 'West' , 'East' , 'Total' ] data [ 'Total' ] = data . eval ( 'West + East' ) data . dropna () . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } West East Total count 86122.000000 86122.000000 86122.000000 mean 106.798449 47.996238 154.794687 std 134.926536 61.795993 192.517894 min 0.000000 0.000000 0.000000 25% 13.000000 6.000000 19.000000 50% 59.000000 27.000000 86.000000 75% 143.000000 66.000000 209.000000 max 1097.000000 698.000000 1569.000000 Visualizing the Data \u00b6 % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set (); data . plot () plt . ylabel ( 'Hourly Bicycle Count' ); weekly = data . resample ( 'W' ) . sum () weekly . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'Weekly bicycle count' ) Text(0, 0.5, 'Weekly bicycle count') people bicycle more in the summer than in the winter, and even within a particular season the bicy\u2010 cle use varies from week to week (likely dependent on weather daily = data . resample ( 'D' ) . sum () daily . rolling ( 30 , center = True ) . sum () . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'Mean hourly count' ) Text(0, 0.5, 'Mean hourly count') The jaggedness of the result is due to the hard cutoff of the window. We can get a smoother version of a rolling mean using a window function\u2014for example, a Gaus\u2010 sian window. daily . rolling ( 50 , center = True , win_type = 'gaussian' ) . sum ( std = 10 ) . plot ( style = [ ':' , '--' , '-' ]) <AxesSubplot:xlabel='Date'> Digging into the data \u00b6 While the smoothed data views above are useful to get an idea of the general trend in the data, they hide much of the interesting structure. For example, we might want to look at the average traffic as a function of the time of day. We can do this using the GroupBy functionality by_time = data . groupby ( data . index . time ) . mean () hourly_ticks = 4 * 60 * np . arange ( 6 ) by_time . plot ( xticks = hourly_ticks , style = [ ':' , '--' , '-' ]) <AxesSubplot:xlabel='time'> by_weekday = data . groupby ( data . index . day_of_week ) . mean () by_weekday . index = [ 'Mon' , 'Tues' , 'Wed' , 'Thr' , 'Fri' , 'Sat' , 'Sun' ] by_weekday . plot ( style = [ ':' , '--' , '-' ]) <AxesSubplot:> This shows a strong distinction between weekday and weekend totals, with around twice as many average riders crossing the bridge on Monday through Friday than on Saturday and Sunday. With this in mind, let\u2019s do a compound groupby and look at the hourly trend on weekdays versus weekends. We\u2019ll start by grouping by both a flag marking the week\u2010 end, and the time of day: weekend = np . where ( data . index . weekday < 5 , 'Weekday' , 'Weekend' ) by_time = data . groupby ([ weekend , data . index . time ]) . mean () import matplotlib.pyplot as plt fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) by_time . loc [ 'Weekday' ] . plot ( ax = ax [ 0 ], title = 'Weekdays' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]) by_time . loc [ 'Weekend' ] . plot ( ax = ax [ 1 ], title = 'Weekends' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]);","title":"Example Visualizing Seattle Bicycle Counts"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#example-visualzing-seattle-bucycle-counts","text":"Example: Visualizing Seattle Bicycle Counts Visualizing the Data Digging into the data","title":"Example: Visualzing Seattle Bucycle Counts"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#example-visualizing-seattle-bicycle-counts","text":"As a more involved example of working with some time series data, let\u2019s take a look at bicycle counts on Seattle\u2019s Fremont Bridge. This data comes from an automated bicy\u2010 cle counter, installed in late 2012, which has inductive sensors on the east and west sidewalks of the bridge. ! curl - o FremontBridge . csv https : // data . seattle . gov / api / views / 65 db - xm6k / rows . csv ? accessType = DOWNLOAD % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2679k 0 2679k 0 0 170k 0 --:--:-- 0:00:15 --:--:-- 296k import pandas as pd import numpy as np data = pd . read_csv ( 'FremontBridge.csv' , index_col = 'Date' , parse_dates = True ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fremont Bridge Total Fremont Bridge East Sidewalk Fremont Bridge West Sidewalk Date 2012-10-03 00:00:00 13.0 4.0 9.0 2012-10-03 01:00:00 10.0 4.0 6.0 2012-10-03 02:00:00 2.0 1.0 1.0 2012-10-03 03:00:00 5.0 2.0 3.0 2012-10-03 04:00:00 7.0 6.0 1.0 data . columns = [ 'West' , 'East' , 'Total' ] data [ 'Total' ] = data . eval ( 'West + East' ) data . dropna () . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } West East Total count 86122.000000 86122.000000 86122.000000 mean 106.798449 47.996238 154.794687 std 134.926536 61.795993 192.517894 min 0.000000 0.000000 0.000000 25% 13.000000 6.000000 19.000000 50% 59.000000 27.000000 86.000000 75% 143.000000 66.000000 209.000000 max 1097.000000 698.000000 1569.000000","title":"Example: Visualizing Seattle Bicycle Counts"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#visualizing-the-data","text":"% matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set (); data . plot () plt . ylabel ( 'Hourly Bicycle Count' ); weekly = data . resample ( 'W' ) . sum () weekly . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'Weekly bicycle count' ) Text(0, 0.5, 'Weekly bicycle count') people bicycle more in the summer than in the winter, and even within a particular season the bicy\u2010 cle use varies from week to week (likely dependent on weather daily = data . resample ( 'D' ) . sum () daily . rolling ( 30 , center = True ) . sum () . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'Mean hourly count' ) Text(0, 0.5, 'Mean hourly count') The jaggedness of the result is due to the hard cutoff of the window. We can get a smoother version of a rolling mean using a window function\u2014for example, a Gaus\u2010 sian window. daily . rolling ( 50 , center = True , win_type = 'gaussian' ) . sum ( std = 10 ) . plot ( style = [ ':' , '--' , '-' ]) <AxesSubplot:xlabel='Date'>","title":"Visualizing the Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/#digging-into-the-data","text":"While the smoothed data views above are useful to get an idea of the general trend in the data, they hide much of the interesting structure. For example, we might want to look at the average traffic as a function of the time of day. We can do this using the GroupBy functionality by_time = data . groupby ( data . index . time ) . mean () hourly_ticks = 4 * 60 * np . arange ( 6 ) by_time . plot ( xticks = hourly_ticks , style = [ ':' , '--' , '-' ]) <AxesSubplot:xlabel='time'> by_weekday = data . groupby ( data . index . day_of_week ) . mean () by_weekday . index = [ 'Mon' , 'Tues' , 'Wed' , 'Thr' , 'Fri' , 'Sat' , 'Sun' ] by_weekday . plot ( style = [ ':' , '--' , '-' ]) <AxesSubplot:> This shows a strong distinction between weekday and weekend totals, with around twice as many average riders crossing the bridge on Monday through Friday than on Saturday and Sunday. With this in mind, let\u2019s do a compound groupby and look at the hourly trend on weekdays versus weekends. We\u2019ll start by grouping by both a flag marking the week\u2010 end, and the time of day: weekend = np . where ( data . index . weekday < 5 , 'Weekday' , 'Weekend' ) by_time = data . groupby ([ weekend , data . index . time ]) . mean () import matplotlib.pyplot as plt fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) by_time . loc [ 'Weekday' ] . plot ( ax = ax [ 0 ], title = 'Weekdays' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]) by_time . loc [ 'Weekend' ] . plot ( ax = ax [ 1 ], title = 'Weekends' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]);","title":"Digging into the data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 Example: Birthrate Data \u00b6 Example: Birthrate Data Further data exploration Example: Birthrate Data \u00b6 As a more interesting example, let\u2019s take a look at the freely available data on births in the United States, provided by the Centers for Disease Control (CDC). This data can be found at link births.csv (this dataset has been analyzed rather extensively by Andrew Gelman and his group; see, for example, this blog post) # shell command to download the data: # !curl -O https://raw.githubusercontent.com/jakevdp/data-CDCbirths/ # master/births.csv import numpy as np import pandas as pd births = pd . read_csv ( \"../data/births.csv\" ) births . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births 0 1969 1 1.0 F 4046 1 1969 1 1.0 M 4440 2 1969 1 2.0 F 4454 3 1969 1 2.0 M 4548 4 1969 1 3.0 F 4548 We can start to understand this data a bit more by using a pivot table. Let\u2019s add a dec\u2010 ade column, and take a look at male and female births as a function of decade: births [ 'decade' ] = 10 * ( births [ 'year' ] // 10 ) births . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1.0 F 4046 1960 1 1969 1 1.0 M 4440 1960 2 1969 1 2.0 F 4454 1960 3 1969 1 2.0 M 4548 1960 4 1969 1 3.0 F 4548 1960 births . pivot_table ( 'births' , index = 'decade' , columns = 'gender' , aggfunc = 'sum' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gender F M decade 1960 1753634 1846572 1970 16263075 17121550 1980 18310351 19243452 1990 19479454 20420553 2000 18229309 19106428 We immediately see that male births outnumber female births in every decade. To see this trend a bit more clearly, we can use the built-in plotting tools in Pandas to visual\u2010 ize the total number of births by year % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns sns . set () births . pivot_table ( 'births' , index = 'year' , columns = 'gender' , aggfunc = 'sum' ) . plot () plt . ylabel ( \"total birth per year\" ) Text(0, 0.5, 'total birth per year') Further data exploration \u00b6 Though this doesn\u2019t necessarily relate to the pivot table, there are a few more interest\u2010 ing features we can pull out of this dataset using the Pandas tools covered up to this point. We must start by cleaning the data a bit, removing outliers caused by mistyped dates (e.g., June 31 st ) or missing values (e.g., June 99 th ). One easy way to remove these all at once is to cut outliers; we\u2019ll do this via a robust sigma-clipping operation quartiles = np . percentile ( births [ 'births' ],[ 25 , 50 , 75 ]) mu = quartiles [ 1 ] sig = 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) mu 4814.0 sig 689.31 This final line is a robust estimate of the sample mean, where the 0.74 comes from the interquartile range of a Gaussian distribution. With this we can use the query() method to filter out rows with births outside these values: births = births . query ( '(births > @mu -5 * @sig) & (births<@mu + 5*@sig)' ) births .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1.0 F 4046 1960 1 1969 1 1.0 M 4440 1960 2 1969 1 2.0 F 4454 1960 3 1969 1 2.0 M 4548 1960 4 1969 1 3.0 F 4548 1960 ... ... ... ... ... ... ... 15062 1988 12 29.0 M 5944 1980 15063 1988 12 30.0 F 5742 1980 15064 1988 12 30.0 M 6095 1980 15065 1988 12 31.0 F 4435 1980 15066 1988 12 31.0 M 4698 1980 14610 rows \u00d7 6 columns Next we set the day column to integers; previously it had been a string because some columns in the dataset contained the value \u2018null\u2019: births [ 'day' ] = births [ 'day' ] . astype ( int ) /tmp/ipykernel_18639/3805690895.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy births['day']=births['day'].astype(int) births .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1 F 4046 1960 1 1969 1 1 M 4440 1960 2 1969 1 2 F 4454 1960 3 1969 1 2 M 4548 1960 4 1969 1 3 F 4548 1960 ... ... ... ... ... ... ... 15062 1988 12 29 M 5944 1980 15063 1988 12 30 F 5742 1980 15064 1988 12 30 M 6095 1980 15065 1988 12 31 F 4435 1980 15066 1988 12 31 M 4698 1980 14610 rows \u00d7 6 columns Finally, we can combine the day, month, and year to create a Date index This allows us to quickly compute the weekday corresponding to each row: # create a datetime index from the year, month, day births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = '%Y%m %d ' ) births [ 'dayofweek' ] = births . index . day_of_week # Using this we can plot the births by weekday for several decades import matplotlib.pyplot as plt import matplotlib as mpl births . pivot_table ( 'births' , index = 'dayofweek' , columns = 'decade' , aggfunc = 'mean' ) . plot () plt . gca () . set_xticklabels ([ 'Mon' , 'Tues' , 'Wed' , 'Thurs' , 'Fri' , 'Sat' , 'Sun' ]) plt . ylabel ( 'mean births by day' ); /tmp/ipykernel_18639/3967923407.py:6: UserWarning: FixedFormatter should only be used together with FixedLocator plt.gca().set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']) births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } births 1 1 4009.225 2 4247.400 3 4500.900 4 4571.350 5 4603.625 ... ... ... 12 27 4850.150 28 5044.200 29 5120.150 30 5172.350 31 4859.200 366 rows \u00d7 1 columns births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] /tmp/ipykernel_18639/1749910599.py:1: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead. births_by_date.index=[pd.datetime(2012,month,day) for (month,day)in births_by_date.index] births_by_date .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } births 2012-01-01 4009.225 2012-01-02 4247.400 2012-01-03 4500.900 2012-01-04 4571.350 2012-01-05 4603.625 ... ... 2012-12-27 4850.150 2012-12-28 5044.200 2012-12-29 5120.150 2012-12-30 5172.350 2012-12-31 4859.200 366 rows \u00d7 1 columns # Focusing on the month and day only, we now have a time series reflecting the average #number of births by date of the year. fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) <AxesSubplot:>","title":"Example  Birthrate Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/#example-birthrate-data","text":"Example: Birthrate Data Further data exploration","title":"Example: Birthrate Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/#example-birthrate-data_1","text":"As a more interesting example, let\u2019s take a look at the freely available data on births in the United States, provided by the Centers for Disease Control (CDC). This data can be found at link births.csv (this dataset has been analyzed rather extensively by Andrew Gelman and his group; see, for example, this blog post) # shell command to download the data: # !curl -O https://raw.githubusercontent.com/jakevdp/data-CDCbirths/ # master/births.csv import numpy as np import pandas as pd births = pd . read_csv ( \"../data/births.csv\" ) births . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births 0 1969 1 1.0 F 4046 1 1969 1 1.0 M 4440 2 1969 1 2.0 F 4454 3 1969 1 2.0 M 4548 4 1969 1 3.0 F 4548 We can start to understand this data a bit more by using a pivot table. Let\u2019s add a dec\u2010 ade column, and take a look at male and female births as a function of decade: births [ 'decade' ] = 10 * ( births [ 'year' ] // 10 ) births . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1.0 F 4046 1960 1 1969 1 1.0 M 4440 1960 2 1969 1 2.0 F 4454 1960 3 1969 1 2.0 M 4548 1960 4 1969 1 3.0 F 4548 1960 births . pivot_table ( 'births' , index = 'decade' , columns = 'gender' , aggfunc = 'sum' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gender F M decade 1960 1753634 1846572 1970 16263075 17121550 1980 18310351 19243452 1990 19479454 20420553 2000 18229309 19106428 We immediately see that male births outnumber female births in every decade. To see this trend a bit more clearly, we can use the built-in plotting tools in Pandas to visual\u2010 ize the total number of births by year % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns sns . set () births . pivot_table ( 'births' , index = 'year' , columns = 'gender' , aggfunc = 'sum' ) . plot () plt . ylabel ( \"total birth per year\" ) Text(0, 0.5, 'total birth per year')","title":"Example: Birthrate Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/#further-data-exploration","text":"Though this doesn\u2019t necessarily relate to the pivot table, there are a few more interest\u2010 ing features we can pull out of this dataset using the Pandas tools covered up to this point. We must start by cleaning the data a bit, removing outliers caused by mistyped dates (e.g., June 31 st ) or missing values (e.g., June 99 th ). One easy way to remove these all at once is to cut outliers; we\u2019ll do this via a robust sigma-clipping operation quartiles = np . percentile ( births [ 'births' ],[ 25 , 50 , 75 ]) mu = quartiles [ 1 ] sig = 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) mu 4814.0 sig 689.31 This final line is a robust estimate of the sample mean, where the 0.74 comes from the interquartile range of a Gaussian distribution. With this we can use the query() method to filter out rows with births outside these values: births = births . query ( '(births > @mu -5 * @sig) & (births<@mu + 5*@sig)' ) births .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1.0 F 4046 1960 1 1969 1 1.0 M 4440 1960 2 1969 1 2.0 F 4454 1960 3 1969 1 2.0 M 4548 1960 4 1969 1 3.0 F 4548 1960 ... ... ... ... ... ... ... 15062 1988 12 29.0 M 5944 1980 15063 1988 12 30.0 F 5742 1980 15064 1988 12 30.0 M 6095 1980 15065 1988 12 31.0 F 4435 1980 15066 1988 12 31.0 M 4698 1980 14610 rows \u00d7 6 columns Next we set the day column to integers; previously it had been a string because some columns in the dataset contained the value \u2018null\u2019: births [ 'day' ] = births [ 'day' ] . astype ( int ) /tmp/ipykernel_18639/3805690895.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy births['day']=births['day'].astype(int) births .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month day gender births decade 0 1969 1 1 F 4046 1960 1 1969 1 1 M 4440 1960 2 1969 1 2 F 4454 1960 3 1969 1 2 M 4548 1960 4 1969 1 3 F 4548 1960 ... ... ... ... ... ... ... 15062 1988 12 29 M 5944 1980 15063 1988 12 30 F 5742 1980 15064 1988 12 30 M 6095 1980 15065 1988 12 31 F 4435 1980 15066 1988 12 31 M 4698 1980 14610 rows \u00d7 6 columns Finally, we can combine the day, month, and year to create a Date index This allows us to quickly compute the weekday corresponding to each row: # create a datetime index from the year, month, day births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = '%Y%m %d ' ) births [ 'dayofweek' ] = births . index . day_of_week # Using this we can plot the births by weekday for several decades import matplotlib.pyplot as plt import matplotlib as mpl births . pivot_table ( 'births' , index = 'dayofweek' , columns = 'decade' , aggfunc = 'mean' ) . plot () plt . gca () . set_xticklabels ([ 'Mon' , 'Tues' , 'Wed' , 'Thurs' , 'Fri' , 'Sat' , 'Sun' ]) plt . ylabel ( 'mean births by day' ); /tmp/ipykernel_18639/3967923407.py:6: UserWarning: FixedFormatter should only be used together with FixedLocator plt.gca().set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']) births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } births 1 1 4009.225 2 4247.400 3 4500.900 4 4571.350 5 4603.625 ... ... ... 12 27 4850.150 28 5044.200 29 5120.150 30 5172.350 31 4859.200 366 rows \u00d7 1 columns births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] /tmp/ipykernel_18639/1749910599.py:1: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead. births_by_date.index=[pd.datetime(2012,month,day) for (month,day)in births_by_date.index] births_by_date .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } births 2012-01-01 4009.225 2012-01-02 4247.400 2012-01-03 4500.900 2012-01-04 4571.350 2012-01-05 4603.625 ... ... 2012-12-27 4850.150 2012-12-28 5044.200 2012-12-29 5120.150 2012-12-30 5172.350 2012-12-31 4859.200 366 rows \u00d7 1 columns # Focusing on the month and day only, we now have a time series reflecting the average #number of births by date of the year. fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) <AxesSubplot:>","title":"Further data exploration"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 Example: US States Data \u00b6 Example: US States Data Example: US States Data \u00b6 Merge and join operations come up most often when one is combining data from dif\u2010 ferent sources. Here we will consider an example of some data about US states and their populations. import numpy as np import pandas as pd pop = pd . read_csv ( \"../data/state-population.csv\" ) areas = pd . read_csv ( \"../data/state-areas.csv\" ) abbrevs = pd . read_csv ( \"../data/state-abbrevs.csv\" ) pop . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population 0 AL under18 2012 1117489.0 1 AL total 2012 4817528.0 2 AL under18 2010 1130966.0 3 AL total 2010 4785570.0 4 AL under18 2011 1125763.0 areas . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state area (sq. mi) 0 Alabama 52423 1 Alaska 656425 2 Arizona 114006 3 Arkansas 53182 4 California 163707 abbrevs . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state abbreviation 0 Alabama AL 1 Alaska AK 2 Arizona AZ 3 Arkansas AR 4 California CA Given this information, say we want to compute a relatively straightforward result:rank US states and territories by their 2010 population density merged = pd . merge ( pop , abbrevs , how = 'outer' , left_on = 'state/region' , right_on = 'abbreviation' ) merged .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state abbreviation 0 AL under18 2012 1117489.0 Alabama AL 1 AL total 2012 4817528.0 Alabama AL 2 AL under18 2010 1130966.0 Alabama AL 3 AL total 2010 4785570.0 Alabama AL 4 AL under18 2011 1125763.0 Alabama AL ... ... ... ... ... ... ... 2539 USA total 2010 309326295.0 NaN NaN 2540 USA under18 2011 73902222.0 NaN NaN 2541 USA total 2011 311582564.0 NaN NaN 2542 USA under18 2012 73708179.0 NaN NaN 2543 USA total 2012 313873685.0 NaN NaN 2544 rows \u00d7 6 columns merged = merged . drop ( 'abbreviation' , 1 ) /tmp/ipykernel_88168/2168094788.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only. merged=merged.drop('abbreviation',1) merged . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 0 AL under18 2012 1117489.0 Alabama 1 AL total 2012 4817528.0 Alabama 2 AL under18 2010 1130966.0 Alabama 3 AL total 2010 4785570.0 Alabama 4 AL under18 2011 1125763.0 Alabama # CHeck if there is any mismatch, merged . isnull () . any () state/region False ages False year False population True state True dtype: bool ## Some of the population and state info is null, lets check which one merged [ merged [ 'population' ] . isnull ()] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 2448 PR under18 1990 NaN NaN 2449 PR total 1990 NaN NaN 2450 PR total 1991 NaN NaN 2451 PR under18 1991 NaN NaN 2452 PR total 1993 NaN NaN merged [ merged [ 'state' ] . isnull ()] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 2448 PR under18 1990 NaN NaN 2449 PR total 1990 NaN NaN 2450 PR total 1991 NaN NaN 2451 PR under18 1991 NaN NaN 2452 PR total 1993 NaN NaN merged . loc [ merged [ 'state' ] . isnull (), 'state/region' ] . unique () array(['PR', 'USA'], dtype=object) # To fix the missing values of PR, USA merged . loc [ merged [ 'state/region' ] == 'PR' , 'state' ] = 'Puerto Rico' merged . loc [ merged [ 'state/region' ] == 'USA' , 'state' ] = 'United State' merged . isnull () . any () state/region False ages False year False population True state False dtype: bool #Now lets merged the result wiht the area dataset final = pd . merge ( merged , areas , on = 'state' , how = 'left' ) final . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 final . isnull () . any () state/region False ages False year False population True state False area (sq. mi) True dtype: bool # Lets check the regions which areas is null final [ 'state' ][ final [ 'area (sq. mi)' ] . isnull ()] . unique () array(['United State'], dtype=object) # No area value for USA; we can either insert it by suming all the areas or just drop it final . dropna ( inplace = True ) final . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 Now we have all the data we need. To answer the question of interest, let\u2019s first select the portion of the data corresponding with the year 2000, and the total population. We\u2019ll use the query() function to do this quickly (this requires the numexpr package to be installed; data2000 = final . query ( \"year==2000 & ages=='total'\" ) data2000 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 28 AL total 2000 4452173.0 Alabama 52423.0 68 AK total 2000 627963.0 Alaska 656425.0 124 AZ total 2000 5160586.0 Arizona 114006.0 162 AR total 2000 2678588.0 Arkansas 53182.0 220 CA total 2000 33987977.0 California 163707.0 Now let\u2019s compute the population density and display it in order. We\u2019ll start by rein\u2010 dexing our data on the state, and then compute the result: data2000 . set_index ( 'state' , inplace = True ) density = data2000 [ 'population' ] / data2000 [ 'area (sq. mi)' ] density . sort_values ( ascending = False , inplace = True ) density . head () state District of Columbia 8412.441176 Puerto Rico 1084.098151 New Jersey 966.592639 Rhode Island 679.785113 Connecticut 615.399892 dtype: float64 density . tail () state South Dakota 9.800755 North Dakota 9.080434 Montana 6.146192 Wyoming 5.053262 Alaska 0.956641 dtype: float64","title":"Example  US States Data (Merge and Join)"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/#example-us-states-data","text":"Example: US States Data","title":"Example: US States Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/#example-us-states-data_1","text":"Merge and join operations come up most often when one is combining data from dif\u2010 ferent sources. Here we will consider an example of some data about US states and their populations. import numpy as np import pandas as pd pop = pd . read_csv ( \"../data/state-population.csv\" ) areas = pd . read_csv ( \"../data/state-areas.csv\" ) abbrevs = pd . read_csv ( \"../data/state-abbrevs.csv\" ) pop . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population 0 AL under18 2012 1117489.0 1 AL total 2012 4817528.0 2 AL under18 2010 1130966.0 3 AL total 2010 4785570.0 4 AL under18 2011 1125763.0 areas . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state area (sq. mi) 0 Alabama 52423 1 Alaska 656425 2 Arizona 114006 3 Arkansas 53182 4 California 163707 abbrevs . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state abbreviation 0 Alabama AL 1 Alaska AK 2 Arizona AZ 3 Arkansas AR 4 California CA Given this information, say we want to compute a relatively straightforward result:rank US states and territories by their 2010 population density merged = pd . merge ( pop , abbrevs , how = 'outer' , left_on = 'state/region' , right_on = 'abbreviation' ) merged .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state abbreviation 0 AL under18 2012 1117489.0 Alabama AL 1 AL total 2012 4817528.0 Alabama AL 2 AL under18 2010 1130966.0 Alabama AL 3 AL total 2010 4785570.0 Alabama AL 4 AL under18 2011 1125763.0 Alabama AL ... ... ... ... ... ... ... 2539 USA total 2010 309326295.0 NaN NaN 2540 USA under18 2011 73902222.0 NaN NaN 2541 USA total 2011 311582564.0 NaN NaN 2542 USA under18 2012 73708179.0 NaN NaN 2543 USA total 2012 313873685.0 NaN NaN 2544 rows \u00d7 6 columns merged = merged . drop ( 'abbreviation' , 1 ) /tmp/ipykernel_88168/2168094788.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only. merged=merged.drop('abbreviation',1) merged . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 0 AL under18 2012 1117489.0 Alabama 1 AL total 2012 4817528.0 Alabama 2 AL under18 2010 1130966.0 Alabama 3 AL total 2010 4785570.0 Alabama 4 AL under18 2011 1125763.0 Alabama # CHeck if there is any mismatch, merged . isnull () . any () state/region False ages False year False population True state True dtype: bool ## Some of the population and state info is null, lets check which one merged [ merged [ 'population' ] . isnull ()] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 2448 PR under18 1990 NaN NaN 2449 PR total 1990 NaN NaN 2450 PR total 1991 NaN NaN 2451 PR under18 1991 NaN NaN 2452 PR total 1993 NaN NaN merged [ merged [ 'state' ] . isnull ()] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state 2448 PR under18 1990 NaN NaN 2449 PR total 1990 NaN NaN 2450 PR total 1991 NaN NaN 2451 PR under18 1991 NaN NaN 2452 PR total 1993 NaN NaN merged . loc [ merged [ 'state' ] . isnull (), 'state/region' ] . unique () array(['PR', 'USA'], dtype=object) # To fix the missing values of PR, USA merged . loc [ merged [ 'state/region' ] == 'PR' , 'state' ] = 'Puerto Rico' merged . loc [ merged [ 'state/region' ] == 'USA' , 'state' ] = 'United State' merged . isnull () . any () state/region False ages False year False population True state False dtype: bool #Now lets merged the result wiht the area dataset final = pd . merge ( merged , areas , on = 'state' , how = 'left' ) final . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 final . isnull () . any () state/region False ages False year False population True state False area (sq. mi) True dtype: bool # Lets check the regions which areas is null final [ 'state' ][ final [ 'area (sq. mi)' ] . isnull ()] . unique () array(['United State'], dtype=object) # No area value for USA; we can either insert it by suming all the areas or just drop it final . dropna ( inplace = True ) final . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 Now we have all the data we need. To answer the question of interest, let\u2019s first select the portion of the data corresponding with the year 2000, and the total population. We\u2019ll use the query() function to do this quickly (this requires the numexpr package to be installed; data2000 = final . query ( \"year==2000 & ages=='total'\" ) data2000 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state/region ages year population state area (sq. mi) 28 AL total 2000 4452173.0 Alabama 52423.0 68 AK total 2000 627963.0 Alaska 656425.0 124 AZ total 2000 5160586.0 Arizona 114006.0 162 AR total 2000 2678588.0 Arkansas 53182.0 220 CA total 2000 33987977.0 California 163707.0 Now let\u2019s compute the population density and display it in order. We\u2019ll start by rein\u2010 dexing our data on the state, and then compute the result: data2000 . set_index ( 'state' , inplace = True ) density = data2000 [ 'population' ] / data2000 [ 'area (sq. mi)' ] density . sort_values ( ascending = False , inplace = True ) density . head () state District of Columbia 8412.441176 Puerto Rico 1084.098151 New Jersey 966.592639 Rhode Island 679.785113 Connecticut 615.399892 dtype: float64 density . tail () state South Dakota 9.800755 North Dakota 9.080434 Montana 6.146192 Wyoming 5.053262 Alaska 0.956641 dtype: float64","title":"Example: US States Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/","text":"Notes [book] Data Science Handbook \u00b6 by Jawad Haider Chpt 2 - Data Manipulation with Pandas \u00b6 Example: Recipe Database \u00b6 Example: Recipe Database Unfortunately the dataset is not present to do all the other operations :( import numpy as np import pandas as pd Example: Recipe Database \u00b6 These vectorized string operations become most useful in the process of cleaning up messy, real-world data. Here I\u2019ll walk through an example of that, using an open recipe database compiled from various sources on the Web. Our goal will be to parse the recipe data into ingredient lists, so we can quickly find a recipe based on some ingredients we have on hand try : recipes = pd . read_json ( '../data/recipeitems-latest.json' ) except ValueError as e : print ( \"Value Error: \" , e ) Value Error: Expected object or value with open ( '../data/recipeitems-latest.json' ) as f : line = f . readline () pd . read_json ( line ) . shape ValueError: Expected object or value Unfortunately the dataset is not present to do all the other operations :( \u00b6","title":"Example Recipe Database"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/#chpt-2-data-manipulation-with-pandas","text":"","title":"Chpt 2 - Data Manipulation with Pandas"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/#example-recipe-database","text":"Example: Recipe Database Unfortunately the dataset is not present to do all the other operations :( import numpy as np import pandas as pd","title":"Example: Recipe Database"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/#example-recipe-database_1","text":"These vectorized string operations become most useful in the process of cleaning up messy, real-world data. Here I\u2019ll walk through an example of that, using an open recipe database compiled from various sources on the Web. Our goal will be to parse the recipe data into ingredient lists, so we can quickly find a recipe based on some ingredients we have on hand try : recipes = pd . read_json ( '../data/recipeitems-latest.json' ) except ValueError as e : print ( \"Value Error: \" , e ) Value Error: Expected object or value with open ( '../data/recipeitems-latest.json' ) as f : line = f . readline () pd . read_json ( line ) . shape ValueError: Expected object or value","title":"Example: Recipe Database"},{"location":"booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/#unfortunately-the-dataset-is-not-present-to-do-all-the-other-operations","text":"","title":"Unfortunately the dataset is not present to do all the other operations :("},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 01 - General Matplotlib Tips \u00b6 General Matplotlib Tips Importing matplotlib Setting Styles show() or No show()? How to Display Your Plots Saving Figures to File Two Interfaces for the Price of One MATLAB-style interface Object-oriented interface Mtplotlib is a multiplatform data visualization library built on NumPy arrays, and dsigned to work with the broader SciPy stack. One of Matplotlib\u2019s most important features is its ability to play well with many operating systems and graphics backends. Matplotlib supports dozens of backends and output types, which means you can count on it to work regardless of which operating system you are using or which output format you wish. This cross-platform, everything-to-everyone approach has been one of the great strengths of Matplotlib. It has led to a large userbase, which in turn has led to an active developer base and Matplotlib\u2019s powerful tools and ubiquity within the scientific Python world. General Matplotlib Tips \u00b6 Before we dive into the details of creating visualizations with Matplotlib, there are a few useful things you should know about using the package. Importing matplotlib \u00b6 Just as we use the np shorthand for NumPy and the pd shorthand for Pandas, we will use some standard shorthands for Matplotlib imports: import matplotlib as mpl import matplotlib.pyplot as plt Setting Styles \u00b6 We will use the plt.style directive to choose appropriate aesthetic styles for our fig\u2010 ures. Here we will set the classic style, which ensures that the plots we create use the classic Matplotlib style: plt . style . use ( 'classic' ) show() or No show()? How to Display Your Plots \u00b6 A visualization you can\u2019t see won\u2019t be of much use, but just how you view your Mat\u2010 plotlib plots depends on the context. The best use of Matplotlib differs depending on how you are using it; roughly, the three applicable contexts are using Matplotlib in a script, in an IPython terminal, or in an IPython notebook. #### Plotting from a script If you are using Matplotlib from within a script, the function plt.show() is your friend. plt.show() starts an event loop, looks for all currently active figure objects, and opens one or more interactive windows that display your figure or figures import numpy as np x = np . linspace ( 0 , 10 , 100 ) plt . plot ( x , np . cos ( x )) plt . plot ( x , np . sin ( x )) x = np . linspace ( 0 , 10 , 100 ) plt . plot ( x , np . cos ( x )) plt . plot ( x , np . sin ( x )) plt . show () You can then run this script from the command-line prompt, which will result in a window opening with your figure displayed: $ python myplot.py The plt.show() command does a lot under the hood, as it must interact with your system\u2019s interactive graphical backend. The details of this operation can vary greatly from system to system and even installation to installation, but Matplotlib does its best to hide all these details from you. \u2022 %matplotlib notebook will lead to interactive plots embedded within the notebook \u2022 %matplotlib inline will lead to static images of your plot embedded in the notebook fig = plt . figure () plt . plot ( x , np . sin ( x ), '--' ) plt . plot ( x , np . cos ( x ), '-' ); Saving Figures to File \u00b6 One nice feature of Matplotlib is the ability to save figures in a wide variety of for\u2010 mats. You can save a figure using the savefig() command. For example, to save the previous figure as a PNG file, you can run this: fig . savefig ( 'cos_sinplots.png' ) ! ls - lh total 168K -rw-rw-r-- 1 qalmaqihir qalmaqihir 128K \u0441\u0435\u043d 5 09:09 01_Intro.ipynb -rw-rw-r-- 1 qalmaqihir qalmaqihir 40K \u0441\u0435\u043d 5 09:10 cos_sinplots.png #lets display the image of our plots back from IPython.display import Image Image ( 'cos_sinplots.png' ) In savefig(), the file format is inferred from the extension of the given filename. Depending on what backends you have installed, many different file formats are available. You can find the list of supported file types for your system by using the following method of the figure canvas object: fig . canvas . get_supported_filetypes () {'eps': 'Encapsulated Postscript', 'jpg': 'Joint Photographic Experts Group', 'jpeg': 'Joint Photographic Experts Group', 'pdf': 'Portable Document Format', 'pgf': 'PGF code for LaTeX', 'png': 'Portable Network Graphics', 'ps': 'Postscript', 'raw': 'Raw RGBA bitmap', 'rgba': 'Raw RGBA bitmap', 'svg': 'Scalable Vector Graphics', 'svgz': 'Scalable Vector Graphics', 'tif': 'Tagged Image File Format', 'tiff': 'Tagged Image File Format'} Two Interfaces for the Price of One \u00b6 A potentially confusing feature of Matplotlib is its dual interfaces: a convenient MATLAB-style state-based interface, and a more powerful object-oriented interface. We\u2019ll quickly highlight the differences between the two here. MATLAB-style interface \u00b6 Matplotlib was originally written as a Python alternative for MATLAB users, and much of its syntax reflects that fact. The MATLAB-style tools are contained in the pyplot (plt) interface. plt . figure () # create a plot figure #create the first of two panels and set current axis plt . subplot ( 2 , 1 , 1 ) plt . plot ( x , np . sin ( x )) #create the 2nd of two panels and set current axis plt . subplot ( 2 , 1 , 2 ) plt . plot ( x , np . cos ( x )) plt . gcf () <Figure size 640x480 with 0 Axes> <Figure size 640x480 with 0 Axes> plt . gca () <AxesSubplot:> It\u2019s important to note that this interface is stateful: it keeps track of the \u201ccurrent\u201d figure and axes, which are where all plt commands are applied. You can get a reference to these using the plt.gcf() (get current figure) and plt.gca() (get current axes) routines. While this stateful interface is fast and convenient for simple plots, it is easy to run into problems. For example, once the second panel is created, how can we go back and add something to the first? This is possible within the MATLAB-style interface, but a bit clunky. Fortunately, there is a better way. Object-oriented interface \u00b6 The object-oriented interface is available for these more complicated situations, and for when you want more control over your figure. Rather than depending on some notion of an \u201cactive\u201d figure or axes, in the object-oriented interface the plotting func\u2010 tions are methods of explicit Figure and Axes objects. To re-create the previous plot using this style of plotting, you might do the following # first create a grid of plots # ax will be an array of two Axes Objects fig , ax = plt . subplots ( 2 ) #call the plot() method on the appropriate object ax [ 0 ] . plot ( x , np . cos ( x )) ax [ 0 ] . plot ( x , np . sin ( x )) ax [ 1 ] . plot ( x , np . cos ( x )) For more simple plots, the choice of which style to use is largely a matter of prefer\u2010 ence, but the object-oriented approach can become a necessity as plots become more complicated.","title":"01general Matplotlib tips"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#01-general-matplotlib-tips","text":"General Matplotlib Tips Importing matplotlib Setting Styles show() or No show()? How to Display Your Plots Saving Figures to File Two Interfaces for the Price of One MATLAB-style interface Object-oriented interface Mtplotlib is a multiplatform data visualization library built on NumPy arrays, and dsigned to work with the broader SciPy stack. One of Matplotlib\u2019s most important features is its ability to play well with many operating systems and graphics backends. Matplotlib supports dozens of backends and output types, which means you can count on it to work regardless of which operating system you are using or which output format you wish. This cross-platform, everything-to-everyone approach has been one of the great strengths of Matplotlib. It has led to a large userbase, which in turn has led to an active developer base and Matplotlib\u2019s powerful tools and ubiquity within the scientific Python world.","title":"01 - General Matplotlib Tips"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#general-matplotlib-tips","text":"Before we dive into the details of creating visualizations with Matplotlib, there are a few useful things you should know about using the package.","title":"General Matplotlib Tips"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#importing-matplotlib","text":"Just as we use the np shorthand for NumPy and the pd shorthand for Pandas, we will use some standard shorthands for Matplotlib imports: import matplotlib as mpl import matplotlib.pyplot as plt","title":"Importing matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#setting-styles","text":"We will use the plt.style directive to choose appropriate aesthetic styles for our fig\u2010 ures. Here we will set the classic style, which ensures that the plots we create use the classic Matplotlib style: plt . style . use ( 'classic' )","title":"Setting Styles"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#show-or-no-show-how-to-display-your-plots","text":"A visualization you can\u2019t see won\u2019t be of much use, but just how you view your Mat\u2010 plotlib plots depends on the context. The best use of Matplotlib differs depending on how you are using it; roughly, the three applicable contexts are using Matplotlib in a script, in an IPython terminal, or in an IPython notebook. #### Plotting from a script If you are using Matplotlib from within a script, the function plt.show() is your friend. plt.show() starts an event loop, looks for all currently active figure objects, and opens one or more interactive windows that display your figure or figures import numpy as np x = np . linspace ( 0 , 10 , 100 ) plt . plot ( x , np . cos ( x )) plt . plot ( x , np . sin ( x )) x = np . linspace ( 0 , 10 , 100 ) plt . plot ( x , np . cos ( x )) plt . plot ( x , np . sin ( x )) plt . show () You can then run this script from the command-line prompt, which will result in a window opening with your figure displayed: $ python myplot.py The plt.show() command does a lot under the hood, as it must interact with your system\u2019s interactive graphical backend. The details of this operation can vary greatly from system to system and even installation to installation, but Matplotlib does its best to hide all these details from you. \u2022 %matplotlib notebook will lead to interactive plots embedded within the notebook \u2022 %matplotlib inline will lead to static images of your plot embedded in the notebook fig = plt . figure () plt . plot ( x , np . sin ( x ), '--' ) plt . plot ( x , np . cos ( x ), '-' );","title":"show() or No show()? How to Display Your Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#saving-figures-to-file","text":"One nice feature of Matplotlib is the ability to save figures in a wide variety of for\u2010 mats. You can save a figure using the savefig() command. For example, to save the previous figure as a PNG file, you can run this: fig . savefig ( 'cos_sinplots.png' ) ! ls - lh total 168K -rw-rw-r-- 1 qalmaqihir qalmaqihir 128K \u0441\u0435\u043d 5 09:09 01_Intro.ipynb -rw-rw-r-- 1 qalmaqihir qalmaqihir 40K \u0441\u0435\u043d 5 09:10 cos_sinplots.png #lets display the image of our plots back from IPython.display import Image Image ( 'cos_sinplots.png' ) In savefig(), the file format is inferred from the extension of the given filename. Depending on what backends you have installed, many different file formats are available. You can find the list of supported file types for your system by using the following method of the figure canvas object: fig . canvas . get_supported_filetypes () {'eps': 'Encapsulated Postscript', 'jpg': 'Joint Photographic Experts Group', 'jpeg': 'Joint Photographic Experts Group', 'pdf': 'Portable Document Format', 'pgf': 'PGF code for LaTeX', 'png': 'Portable Network Graphics', 'ps': 'Postscript', 'raw': 'Raw RGBA bitmap', 'rgba': 'Raw RGBA bitmap', 'svg': 'Scalable Vector Graphics', 'svgz': 'Scalable Vector Graphics', 'tif': 'Tagged Image File Format', 'tiff': 'Tagged Image File Format'}","title":"Saving Figures to File"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#two-interfaces-for-the-price-of-one","text":"A potentially confusing feature of Matplotlib is its dual interfaces: a convenient MATLAB-style state-based interface, and a more powerful object-oriented interface. We\u2019ll quickly highlight the differences between the two here.","title":"Two Interfaces for the Price of One"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#matlab-style-interface","text":"Matplotlib was originally written as a Python alternative for MATLAB users, and much of its syntax reflects that fact. The MATLAB-style tools are contained in the pyplot (plt) interface. plt . figure () # create a plot figure #create the first of two panels and set current axis plt . subplot ( 2 , 1 , 1 ) plt . plot ( x , np . sin ( x )) #create the 2nd of two panels and set current axis plt . subplot ( 2 , 1 , 2 ) plt . plot ( x , np . cos ( x )) plt . gcf () <Figure size 640x480 with 0 Axes> <Figure size 640x480 with 0 Axes> plt . gca () <AxesSubplot:> It\u2019s important to note that this interface is stateful: it keeps track of the \u201ccurrent\u201d figure and axes, which are where all plt commands are applied. You can get a reference to these using the plt.gcf() (get current figure) and plt.gca() (get current axes) routines. While this stateful interface is fast and convenient for simple plots, it is easy to run into problems. For example, once the second panel is created, how can we go back and add something to the first? This is possible within the MATLAB-style interface, but a bit clunky. Fortunately, there is a better way.","title":"MATLAB-style interface"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/#object-oriented-interface","text":"The object-oriented interface is available for these more complicated situations, and for when you want more control over your figure. Rather than depending on some notion of an \u201cactive\u201d figure or axes, in the object-oriented interface the plotting func\u2010 tions are methods of explicit Figure and Axes objects. To re-create the previous plot using this style of plotting, you might do the following # first create a grid of plots # ax will be an array of two Axes Objects fig , ax = plt . subplots ( 2 ) #call the plot() method on the appropriate object ax [ 0 ] . plot ( x , np . cos ( x )) ax [ 0 ] . plot ( x , np . sin ( x )) ax [ 1 ] . plot ( x , np . cos ( x )) For more simple plots, the choice of which style to use is largely a matter of prefer\u2010 ence, but the object-oriented approach can become a necessity as plots become more complicated.","title":"Object-oriented interface"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 02 - Simple Line Plots \u00b6 Simple Line Plots Adjusting the Plot: Line Colors and Styles Adjusting the Plot: Axes Limits Labeling Plots Simple Line Plots \u00b6 Perhaps the simplest of all plots is the visualization of a single function y = f x . Here we will take a first look at creating a simple plot of this type. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( \"seaborn-whitegrid\" ) import numpy as np # all Matplotlib plots can be started by creating a figure and an axes # their simplist form fig = plt . figure () ax = plt . axes () In Matplotlib, the figure (an instance of the class plt.Figure) can be thought of as a single container that contains all the objects representing axes, graphics, text, and labels. The axes (an instance of the class plt.Axes) is what we see above: a bounding box with ticks and labels, which will eventually contain the plot elements that make up our visualization. #Once we have created an axes, we can use the ax.plot function to plot some data. fig = plt . figure () ax = plt . axes () x = np . linspace ( 0 , 10 , 100 ) ax . plot ( x , np . sin ( x )) Alternatively, we can use the pylab interface and let the figure and axes be created for us in the background fig = plt . figure () ax = plt . axes () plt . plot ( x , np . sin ( x )) If we want to create a single figure with multiple lines, we can simply call the plot function multiple times fig = plt . figure () ax = plt . axes () plt . plot ( x , np . sin ( x )) plt . plot ( x , np . cos ( x )) # plt.plot(x,np.tan(x)) plt . plot ( x , np . log ( x )) /tmp/ipykernel_58842/3301612854.py:6: RuntimeWarning: divide by zero encountered in log plt.plot(x,np.log(x)) Adjusting the Plot: Line Colors and Styles \u00b6 The first adjustment you might wish to make to a plot is to control the line colors and styles. The plt.plot() function takes additional arguments that can be used to spec\u2010 ify these. To adjust the color, you can use the color keyword, which accepts a string argument representing virtually any imaginable color. The color can be specified in a variety of ways plt . plot ( x , np . sin ( x - 0 ), color = 'blue' ) # specify color by name plt . plot ( x , np . sin ( x - 1 ), color = 'g' ) # short color code (rgbcmyk) plt . plot ( x , np . sin ( x - 2 ), color = '0.75' ) # Grayscale between 0 and 1 plt . plot ( x , np . sin ( x - 3 ), color = '#FFDD44' ) # Hex code (RRGGBB from 00 to FF) plt . plot ( x , np . sin ( x - 4 ), color = ( 1.0 , 0.2 , 0.3 )) # RGB tuple, values 0 and 1 plt . plot ( x , np . sin ( x - 5 ), color = 'chartreuse' ); # all HTML color names supported *If no color is specified, Matplotlib will automatically cycle through a set of default colors for multiple lines. similarly, you can adjust the line style using linestyle keyword* plt . plot ( x , x + 0 , linestyle = 'solid' ) plt . plot ( x , x + 1 , linestyle = 'dashed' ) plt . plot ( x , x + 2 , linestyle = 'dashdot' ) plt . plot ( x , x + 3 , linestyle = 'dotted' ); # For short, you can use the following codes: plt . plot ( x , x + 4 , linestyle = '-' ) # solid plt . plot ( x , x + 5 , linestyle = '--' ) # dashed plt . plot ( x , x + 6 , linestyle = '-.' ) # dashdot plt . plot ( x , x + 7 , linestyle = ':' ); # dotted If you would like to be extremely terse, these linestyle and color codes can be com\u2010 bined into a single nonkeyword argument to the plt.plot() function Adjusting the Plot: Axes Limits \u00b6 Matplotlib does a decent job of choosing default axes limits for your plot, but some\u2010 times it\u2019s nice to have finer control. The most basic way to adjust axis limits is to use the plt.xlim() and plt.ylim() methods plt . plot ( x , np . sin ( x )) plt . xlim ( - 1 , 11 ) plt . ylim ( - 1.5 , 1.5 ) (-1.5, 1.5) if for some reason you\u2019d like either axis to be displayed in reverse, you can simply reverse the order of the arguments plt . plot ( x , np . sin ( x )) plt . xlim ( 10 , 0 ) plt . ylim ( 1.2 , - 1.2 ) (1.2, -1.2) A useful related method is plt.axis() (note here the potential confusion between axes with an e, and axis with an i). The plt.axis() method allows you to set the x and y limits with a single call, by passing a list that specifies [xmin, xmax, ymin,ymax] plt . plot ( x , np . sin ( x )) plt . axis ([ - 1 , 11 , - 1.5 , 1.5 ]); The plt.axis() method goes even beyond this, allowing you to do things like auto\u2010 matically tighten the bounds around the current plot plt . plot ( x , np . sin ( x )) plt . axis ( 'tight' ); It allows even higher-level specifications, such as ensuring an equal aspect ratio so that on your screen, one unit in x is equal to one unit in y plt . plot ( x , np . sin ( x )) plt . axis ( 'equal' ); Labeling Plots \u00b6 As the last piece of this section, we\u2019ll briefly look at the labeling of plots: titles, axis labels, and simple legends. Titles and axis labels are the simplest such labels\u2014there are methods that can be used to quickly set them plt . plot ( x , np . sin ( x )) plt . title ( 'A sine curve' ) plt . xlabel ( 'x' ) plt . ylabel ( 'sin(x)' ) Text(0, 0.5, 'sin(x)') When multiple lines are being shown within a single axes, it can be useful to create a plot legend that labels each line type. Again, Matplotlib has a built-in way of quickly creating such a legend. It is done via the (you guessed it) plt.legend() method. Though there are several valid ways of using this, I find it easiest to specify the label of each line using the label keyword of the plot function plt . plot ( x , np . sin ( x ), '-g' , label = 'sin(x)' ) plt . plot ( x , np . cos ( x ), ':b' , label = 'cos(x)' ) plt . axis ( 'equal' ) plt . legend ();","title":"02simple lineplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#02-simple-line-plots","text":"Simple Line Plots Adjusting the Plot: Line Colors and Styles Adjusting the Plot: Axes Limits Labeling Plots","title":"02 -  Simple Line Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#simple-line-plots","text":"Perhaps the simplest of all plots is the visualization of a single function y = f x . Here we will take a first look at creating a simple plot of this type. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( \"seaborn-whitegrid\" ) import numpy as np # all Matplotlib plots can be started by creating a figure and an axes # their simplist form fig = plt . figure () ax = plt . axes () In Matplotlib, the figure (an instance of the class plt.Figure) can be thought of as a single container that contains all the objects representing axes, graphics, text, and labels. The axes (an instance of the class plt.Axes) is what we see above: a bounding box with ticks and labels, which will eventually contain the plot elements that make up our visualization. #Once we have created an axes, we can use the ax.plot function to plot some data. fig = plt . figure () ax = plt . axes () x = np . linspace ( 0 , 10 , 100 ) ax . plot ( x , np . sin ( x )) Alternatively, we can use the pylab interface and let the figure and axes be created for us in the background fig = plt . figure () ax = plt . axes () plt . plot ( x , np . sin ( x )) If we want to create a single figure with multiple lines, we can simply call the plot function multiple times fig = plt . figure () ax = plt . axes () plt . plot ( x , np . sin ( x )) plt . plot ( x , np . cos ( x )) # plt.plot(x,np.tan(x)) plt . plot ( x , np . log ( x )) /tmp/ipykernel_58842/3301612854.py:6: RuntimeWarning: divide by zero encountered in log plt.plot(x,np.log(x))","title":"Simple Line Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#adjusting-the-plot-line-colors-and-styles","text":"The first adjustment you might wish to make to a plot is to control the line colors and styles. The plt.plot() function takes additional arguments that can be used to spec\u2010 ify these. To adjust the color, you can use the color keyword, which accepts a string argument representing virtually any imaginable color. The color can be specified in a variety of ways plt . plot ( x , np . sin ( x - 0 ), color = 'blue' ) # specify color by name plt . plot ( x , np . sin ( x - 1 ), color = 'g' ) # short color code (rgbcmyk) plt . plot ( x , np . sin ( x - 2 ), color = '0.75' ) # Grayscale between 0 and 1 plt . plot ( x , np . sin ( x - 3 ), color = '#FFDD44' ) # Hex code (RRGGBB from 00 to FF) plt . plot ( x , np . sin ( x - 4 ), color = ( 1.0 , 0.2 , 0.3 )) # RGB tuple, values 0 and 1 plt . plot ( x , np . sin ( x - 5 ), color = 'chartreuse' ); # all HTML color names supported *If no color is specified, Matplotlib will automatically cycle through a set of default colors for multiple lines. similarly, you can adjust the line style using linestyle keyword* plt . plot ( x , x + 0 , linestyle = 'solid' ) plt . plot ( x , x + 1 , linestyle = 'dashed' ) plt . plot ( x , x + 2 , linestyle = 'dashdot' ) plt . plot ( x , x + 3 , linestyle = 'dotted' ); # For short, you can use the following codes: plt . plot ( x , x + 4 , linestyle = '-' ) # solid plt . plot ( x , x + 5 , linestyle = '--' ) # dashed plt . plot ( x , x + 6 , linestyle = '-.' ) # dashdot plt . plot ( x , x + 7 , linestyle = ':' ); # dotted If you would like to be extremely terse, these linestyle and color codes can be com\u2010 bined into a single nonkeyword argument to the plt.plot() function","title":"Adjusting the Plot: Line Colors and Styles"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#adjusting-the-plot-axes-limits","text":"Matplotlib does a decent job of choosing default axes limits for your plot, but some\u2010 times it\u2019s nice to have finer control. The most basic way to adjust axis limits is to use the plt.xlim() and plt.ylim() methods plt . plot ( x , np . sin ( x )) plt . xlim ( - 1 , 11 ) plt . ylim ( - 1.5 , 1.5 ) (-1.5, 1.5) if for some reason you\u2019d like either axis to be displayed in reverse, you can simply reverse the order of the arguments plt . plot ( x , np . sin ( x )) plt . xlim ( 10 , 0 ) plt . ylim ( 1.2 , - 1.2 ) (1.2, -1.2) A useful related method is plt.axis() (note here the potential confusion between axes with an e, and axis with an i). The plt.axis() method allows you to set the x and y limits with a single call, by passing a list that specifies [xmin, xmax, ymin,ymax] plt . plot ( x , np . sin ( x )) plt . axis ([ - 1 , 11 , - 1.5 , 1.5 ]); The plt.axis() method goes even beyond this, allowing you to do things like auto\u2010 matically tighten the bounds around the current plot plt . plot ( x , np . sin ( x )) plt . axis ( 'tight' ); It allows even higher-level specifications, such as ensuring an equal aspect ratio so that on your screen, one unit in x is equal to one unit in y plt . plot ( x , np . sin ( x )) plt . axis ( 'equal' );","title":"Adjusting the Plot: Axes Limits"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/#labeling-plots","text":"As the last piece of this section, we\u2019ll briefly look at the labeling of plots: titles, axis labels, and simple legends. Titles and axis labels are the simplest such labels\u2014there are methods that can be used to quickly set them plt . plot ( x , np . sin ( x )) plt . title ( 'A sine curve' ) plt . xlabel ( 'x' ) plt . ylabel ( 'sin(x)' ) Text(0, 0.5, 'sin(x)') When multiple lines are being shown within a single axes, it can be useful to create a plot legend that labels each line type. Again, Matplotlib has a built-in way of quickly creating such a legend. It is done via the (you guessed it) plt.legend() method. Though there are several valid ways of using this, I find it easiest to specify the label of each line using the label keyword of the plot function plt . plot ( x , np . sin ( x ), '-g' , label = 'sin(x)' ) plt . plot ( x , np . cos ( x ), ':b' , label = 'cos(x)' ) plt . axis ( 'equal' ) plt . legend ();","title":"Labeling Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 03 - Simple Scatter Plots \u00b6 Simple Scatter Plots Scatter Plots with plt.plot Scatter Plots with plt.scatter Simple Scatter Plots \u00b6 Another commonly used plot type is the simple scatter plot, a close cousin of the line plot. Instead of points being joined by line segments, here the points are represented individually with a dot, circle, or other shape. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np Scatter Plots with plt.plot \u00b6 In the previous section, we looked at plt.plot/ax.plot to produce line plots. It turns out that this same function can produce scatter plots as well x = np . linspace ( 0 , 10 , 30 ) y = np . sin ( x ) plt . plot ( x , y , 'o' , color = 'black' ); # possible markers are rng = np . random . RandomState ( 0 ) for marker in [ 'o' , '.' , ',' , 'x' , '+' , 'v' , '^' , '<' , '>' , 's' , 'd' ]: plt . plot ( rng . rand ( 5 ), rng . rand ( 5 ), marker , label = \"marker=' {0} '\" . format ( marker )) plt . legend ( numpoints = 1 ) plt . xlim ( 0 , 1.8 ); For even more possibilities, these character codes can be used together with line and color codes to plot points along with a line connecting them plt . plot ( x , y , '-*r' ); Additional keyword arguments to plt.plot specify a wide range of properties of the lines and markers plt . plot ( x , y , '-p' , color = 'gray' , markersize = 15 , linewidth = 4 , markerfacecolor = 'white' , markeredgecolor = 'gray' , markeredgewidth = 2 ) plt . ylim ( - 1.2 , 1.2 ) (-1.2, 1.2) Scatter Plots with plt.scatter \u00b6 A second, more powerful method of creating scatter plots is the plt.scatter func\u2010 tion, which can be used very similarly to the plt.plot function plt . scatter ( x , y , marker = 'o' ) <matplotlib.collections.PathCollection at 0x7fe4dcc12040> The primary difference of plt.scatter from plt.plot is that it can be used to create scatter plots where the properties of each individual point (size, face color, edge color, etc.) can be individually controlled or mapped to data. rng = np . random . RandomState ( 0 ) x = rng . randn ( 100 ) y = rng . randn ( 100 ) colors = rng . rand ( 100 ) sizes = 1000 * rng . rand ( 100 ) plt . scatter ( x , y , c = colors , s = sizes , alpha = 0.3 , cmap = 'viridis' ) plt . colorbar (); # show color scale /tmp/ipykernel_61416/429789784.py:8: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first. plt.colorbar(); # show color scale Notice that the color argument is automatically mapped to a color scale (shown here by the colorbar() command), and the size argument is given in pixels. In this way, the color and size of points can be used to convey information in the visualization, in order to illustrate multidimensional data. For example, we might use the Iris data from Scikit-Learn, where each sample is one of three types of flowers that has had the size of its petals and sepals carefully meas\u2010 ured from sklearn.datasets import load_iris iris = load_iris () features = iris . data . T plt . scatter ( features [ 0 ], feaatures [ 1 ], alpha = 0.2 , s = 100 * features [ 3 ], c = iris . target , cmap = 'viridis' ) plt . xlabel ( iris . feature_names [ 0 ]) plt . ylabel ( iris . feature_names [ 1 ]);","title":"03simple scatter plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#03-simple-scatter-plots","text":"Simple Scatter Plots Scatter Plots with plt.plot Scatter Plots with plt.scatter","title":"03 -  Simple Scatter Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#simple-scatter-plots","text":"Another commonly used plot type is the simple scatter plot, a close cousin of the line plot. Instead of points being joined by line segments, here the points are represented individually with a dot, circle, or other shape. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np","title":"Simple Scatter Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#scatter-plots-with-pltplot","text":"In the previous section, we looked at plt.plot/ax.plot to produce line plots. It turns out that this same function can produce scatter plots as well x = np . linspace ( 0 , 10 , 30 ) y = np . sin ( x ) plt . plot ( x , y , 'o' , color = 'black' ); # possible markers are rng = np . random . RandomState ( 0 ) for marker in [ 'o' , '.' , ',' , 'x' , '+' , 'v' , '^' , '<' , '>' , 's' , 'd' ]: plt . plot ( rng . rand ( 5 ), rng . rand ( 5 ), marker , label = \"marker=' {0} '\" . format ( marker )) plt . legend ( numpoints = 1 ) plt . xlim ( 0 , 1.8 ); For even more possibilities, these character codes can be used together with line and color codes to plot points along with a line connecting them plt . plot ( x , y , '-*r' ); Additional keyword arguments to plt.plot specify a wide range of properties of the lines and markers plt . plot ( x , y , '-p' , color = 'gray' , markersize = 15 , linewidth = 4 , markerfacecolor = 'white' , markeredgecolor = 'gray' , markeredgewidth = 2 ) plt . ylim ( - 1.2 , 1.2 ) (-1.2, 1.2)","title":"Scatter Plots with plt.plot"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/#scatter-plots-with-pltscatter","text":"A second, more powerful method of creating scatter plots is the plt.scatter func\u2010 tion, which can be used very similarly to the plt.plot function plt . scatter ( x , y , marker = 'o' ) <matplotlib.collections.PathCollection at 0x7fe4dcc12040> The primary difference of plt.scatter from plt.plot is that it can be used to create scatter plots where the properties of each individual point (size, face color, edge color, etc.) can be individually controlled or mapped to data. rng = np . random . RandomState ( 0 ) x = rng . randn ( 100 ) y = rng . randn ( 100 ) colors = rng . rand ( 100 ) sizes = 1000 * rng . rand ( 100 ) plt . scatter ( x , y , c = colors , s = sizes , alpha = 0.3 , cmap = 'viridis' ) plt . colorbar (); # show color scale /tmp/ipykernel_61416/429789784.py:8: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first. plt.colorbar(); # show color scale Notice that the color argument is automatically mapped to a color scale (shown here by the colorbar() command), and the size argument is given in pixels. In this way, the color and size of points can be used to convey information in the visualization, in order to illustrate multidimensional data. For example, we might use the Iris data from Scikit-Learn, where each sample is one of three types of flowers that has had the size of its petals and sepals carefully meas\u2010 ured from sklearn.datasets import load_iris iris = load_iris () features = iris . data . T plt . scatter ( features [ 0 ], feaatures [ 1 ], alpha = 0.2 , s = 100 * features [ 3 ], c = iris . target , cmap = 'viridis' ) plt . xlabel ( iris . feature_names [ 0 ]) plt . ylabel ( iris . feature_names [ 1 ]);","title":"Scatter Plots with plt.scatter"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 04 - Basic Errorbars \u00b6 Visualizing Errors Basic Errorbars Visualizing Errors \u00b6 For any scientific measurement, accurate accounting for errors is nearly as important, if not more important, than accurate reporting of the number itself. Basic Errorbars \u00b6 A basic errorbar can be created with a single Matplotlib function call % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np x = np . linspace ( 0 , 10 , 50 ) dy = 0.8 y = np . sin ( x ) + dy * np . random . randn ( 50 ) plt . errorbar ( x , y , yerr = dy , fmt = 'k' ); Here the fmt is a format code controlling the appearance of lines plt . errorbar ( x , y , yerr = dy , fmt = 'o' , color = 'black' , ecolor = 'lightgray' , elinewidth = 3 , capsize = 0 ); In addition to these options, you can also specify horizontal errorbars (xerr), one- sided errorbars, and many other variants. For more information on the options avail\u2010 able, refer to the docstring of plt.errorbar. ## Continuous Errors In some situations it is desirable to show errorbars on continuous quantities. Though Matplotlib does not have a built-in convenience routine for this type of application, it\u2019s relatively easy to combine primitives like plt.plot and plt.fill_between for a useful result. from sklearn.gaussian_process import GaussianProcessRegressor #define the model and draw some dataa model = lambda x : x * np . sin ( x ) xdata = np . array ([ 1 , 3 , 5 , 6 , 8 ]) ydata = model ( xdata ) # computer the Gaussain process fit gr = GaussianProcessRegressor () gr . fit ( xdata [:, np . newaxis ], ydata ) #sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;} GaussianProcessRegressor() In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. GaussianProcessRegressor GaussianProcessRegressor() xfit = np . linspace ( 0 , 10 , 1000 ) yfit = gr . predict ( xfit [:, np . newaxis ])","title":"04visualizing errors"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/#04-basic-errorbars","text":"Visualizing Errors Basic Errorbars","title":"04 -  Basic Errorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/#visualizing-errors","text":"For any scientific measurement, accurate accounting for errors is nearly as important, if not more important, than accurate reporting of the number itself.","title":"Visualizing Errors"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/#basic-errorbars","text":"A basic errorbar can be created with a single Matplotlib function call % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np x = np . linspace ( 0 , 10 , 50 ) dy = 0.8 y = np . sin ( x ) + dy * np . random . randn ( 50 ) plt . errorbar ( x , y , yerr = dy , fmt = 'k' ); Here the fmt is a format code controlling the appearance of lines plt . errorbar ( x , y , yerr = dy , fmt = 'o' , color = 'black' , ecolor = 'lightgray' , elinewidth = 3 , capsize = 0 ); In addition to these options, you can also specify horizontal errorbars (xerr), one- sided errorbars, and many other variants. For more information on the options avail\u2010 able, refer to the docstring of plt.errorbar. ## Continuous Errors In some situations it is desirable to show errorbars on continuous quantities. Though Matplotlib does not have a built-in convenience routine for this type of application, it\u2019s relatively easy to combine primitives like plt.plot and plt.fill_between for a useful result. from sklearn.gaussian_process import GaussianProcessRegressor #define the model and draw some dataa model = lambda x : x * np . sin ( x ) xdata = np . array ([ 1 , 3 , 5 , 6 , 8 ]) ydata = model ( xdata ) # computer the Gaussain process fit gr = GaussianProcessRegressor () gr . fit ( xdata [:, np . newaxis ], ydata ) #sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;} GaussianProcessRegressor() In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. GaussianProcessRegressor GaussianProcessRegressor() xfit = np . linspace ( 0 , 10 , 1000 ) yfit = gr . predict ( xfit [:, np . newaxis ])","title":"Basic Errorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 05 - Density and Contour Plots \u00b6 Density and Contour Plots Visualizing a Three-Dimensional Function Density and Contour Plots \u00b6 Sometimes it is useful to display three-dimensional data in two dimensions using contours or color-coded regions. There are three Matplotlib functions that can be helpful for this task: plt.contour for contour plots, plt.contourf for filled contour plots, and plt.imshow for showing images. This section looks at several examples of using these. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np Visualizing a Three-Dimensional Function \u00b6 We\u2019ll start by demonstrating a contour plot using a function z = f x, y , using the fol\u2010 lowing particular choice for f def f ( x , y ): return np . sin ( x ) ** 10 + np . cos ( 10 + y * x ) * np . cos ( x ) A contour plot can be created with the plt.contour function. It takes three argu\u2010 ments: a grid of x values, a grid of y values, and a grid of z values. The x and y values represent positions on the plot, and the z values will be represented by the contour levels. Perhaps the most straightforward way to prepare such data is to use the np.meshgrid function, which builds two-dimensional grids from one-dimensional arrays: x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 40 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) plt . contour ( X , Y , Z , colors = 'black' ) <matplotlib.contour.QuadContourSet at 0x7f5d78321eb0> Notice that by default when a single color is used, negative values are represented by dashed lines, and positive values by solid lines. Alternatively, you can color-code the lines by specifying a colormap with the cmap argument. Here, we\u2019ll also specify that we want more lines to be drawn\u201420 equally spaced intervals within the data range plt . contour ( X , Y , Z , 20 , cmap = 'RdGy' ) <matplotlib.contour.QuadContourSet at 0x7f5d78a0adc0> Here we chose the RdGy (short for Red-Gray) colormap, which is a good choice for centered data. Matplotlib has a wide range of colormaps available. Our plot is looking nicer, but the spaces between the lines may be a bit distracting. We can change this by switching to a filled contour plot using the plt.contourf() function (notice the f at the end), which uses largely the same syntax as plt.contour() .Additionally, we\u2019ll add a plt.colorbar() command, which automatically creates an additional axis with labeled color information for the plot plt . contourf ( X , Y , Z , 20 , cmap = 'RdGy' ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f5d7847d190> The colorbar makes it clear that the black regions are \u201cpeaks,\u201d while the red regions are \u201cvalleys.\u201d One potential issue with this plot is that it is a bit \u201csplotchy.\u201d That is, the color steps are discrete rather than continuous, which is not always what is desired. You could remedy this by setting the number of contours to a very high number, but this results in a rather inefficient plot: Matplotlib must render a new polygon for each step in the level. A better way to handle this is to use the plt.imshow() function, which inter\u2010 prets a two-dimensional grid of data as an image. plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' ) plt . colorbar () plt . axis ( aspect = 'image' ) TypeError: axis() got an unexpected keyword argument 'aspect' There are a few potential gotchas with imshow(), however: \u2022 plt.imshow() doesn\u2019t accept an x and y grid, so you must manually specify the extent [xmin, xmax, ymin, ymax] of the image on the plot. \u2022 plt.imshow() by default follows the standard image array definition where the origin is in the upper left, not in the lower left as in most contour plots. This must be changed when showing gridded data. \u2022 plt.imshow() will automatically adjust the axis aspect ratio to match the input data; you can change this by setting, for example, plt.axis(aspect=\u2018image\u2019) to make x and y units match. # A combined contours plots and image plots contours = plt . contour ( X , Y , Z , 3 , colors = 'black' ) plt . clabel ( contours , inline = True , fontsize = 8 ) plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' , alpha = 0.5 ) plt . colorbar ();","title":"05density and contour plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/#05-density-and-contour-plots","text":"Density and Contour Plots Visualizing a Three-Dimensional Function","title":"05 -  Density and Contour Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/#density-and-contour-plots","text":"Sometimes it is useful to display three-dimensional data in two dimensions using contours or color-coded regions. There are three Matplotlib functions that can be helpful for this task: plt.contour for contour plots, plt.contourf for filled contour plots, and plt.imshow for showing images. This section looks at several examples of using these. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np","title":"Density and Contour Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/#visualizing-a-three-dimensional-function","text":"We\u2019ll start by demonstrating a contour plot using a function z = f x, y , using the fol\u2010 lowing particular choice for f def f ( x , y ): return np . sin ( x ) ** 10 + np . cos ( 10 + y * x ) * np . cos ( x ) A contour plot can be created with the plt.contour function. It takes three argu\u2010 ments: a grid of x values, a grid of y values, and a grid of z values. The x and y values represent positions on the plot, and the z values will be represented by the contour levels. Perhaps the most straightforward way to prepare such data is to use the np.meshgrid function, which builds two-dimensional grids from one-dimensional arrays: x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 40 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) plt . contour ( X , Y , Z , colors = 'black' ) <matplotlib.contour.QuadContourSet at 0x7f5d78321eb0> Notice that by default when a single color is used, negative values are represented by dashed lines, and positive values by solid lines. Alternatively, you can color-code the lines by specifying a colormap with the cmap argument. Here, we\u2019ll also specify that we want more lines to be drawn\u201420 equally spaced intervals within the data range plt . contour ( X , Y , Z , 20 , cmap = 'RdGy' ) <matplotlib.contour.QuadContourSet at 0x7f5d78a0adc0> Here we chose the RdGy (short for Red-Gray) colormap, which is a good choice for centered data. Matplotlib has a wide range of colormaps available. Our plot is looking nicer, but the spaces between the lines may be a bit distracting. We can change this by switching to a filled contour plot using the plt.contourf() function (notice the f at the end), which uses largely the same syntax as plt.contour() .Additionally, we\u2019ll add a plt.colorbar() command, which automatically creates an additional axis with labeled color information for the plot plt . contourf ( X , Y , Z , 20 , cmap = 'RdGy' ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f5d7847d190> The colorbar makes it clear that the black regions are \u201cpeaks,\u201d while the red regions are \u201cvalleys.\u201d One potential issue with this plot is that it is a bit \u201csplotchy.\u201d That is, the color steps are discrete rather than continuous, which is not always what is desired. You could remedy this by setting the number of contours to a very high number, but this results in a rather inefficient plot: Matplotlib must render a new polygon for each step in the level. A better way to handle this is to use the plt.imshow() function, which inter\u2010 prets a two-dimensional grid of data as an image. plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' ) plt . colorbar () plt . axis ( aspect = 'image' ) TypeError: axis() got an unexpected keyword argument 'aspect' There are a few potential gotchas with imshow(), however: \u2022 plt.imshow() doesn\u2019t accept an x and y grid, so you must manually specify the extent [xmin, xmax, ymin, ymax] of the image on the plot. \u2022 plt.imshow() by default follows the standard image array definition where the origin is in the upper left, not in the lower left as in most contour plots. This must be changed when showing gridded data. \u2022 plt.imshow() will automatically adjust the axis aspect ratio to match the input data; you can change this by setting, for example, plt.axis(aspect=\u2018image\u2019) to make x and y units match. # A combined contours plots and image plots contours = plt . contour ( X , Y , Z , 3 , colors = 'black' ) plt . clabel ( contours , inline = True , fontsize = 8 ) plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' , alpha = 0.5 ) plt . colorbar ();","title":"Visualizing a Three-Dimensional Function"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 06 - Histograms, Binnings, and Density \u00b6 Histograms, Binnings, and Density Two-Dimensional Histograms and Binnings plt.hexbin: Hexagonal binnings Kernel density estimation Histograms, Binnings, and Density \u00b6 A simple histogram can be a great first step in understanding a dataset. Earlier, we saw a preview of Matplotlib\u2019s histogram function % matplotlib inline import numpy as np import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) data = np . random . randn ( 1000 ) plt . hist ( data ); # A more customized histogram plt . hist ( data , bins = 30 , alpha = 0.5 , histtype = 'stepfilled' , color = 'steelblue' , edgecolor = 'none' ); The plt.hist docstring has more information on other customization options avail\u2010 able. I find this combination of histtype=\u2018stepfilled\u2019 along with some transpar\u2010 ency alpha to be very useful when comparing histograms of several distributions x1 = np . random . normal ( 0 , 0.8 , 1000 ) x2 = np . random . normal ( - 2 , 1 , 1000 ) x3 = np . random . normal ( 2 , 3 , 1000 ) kwargs = dict ( histtype = 'stepfilled' , alpha = 0.3 , bins = 40 ) plt . hist ( x1 , ** kwargs ); plt . hist ( x2 , ** kwargs ); plt . hist ( x3 , ** kwargs ); If you would like to simply compute the histogram (that is, count the number of points in a given bin) and not display it, the np.histogram() function is available: counts , bin_edges = np . histogram ( data , bins = 5 ) counts array([ 17, 292, 555, 132, 4]) Two-Dimensional Histograms and Binnings \u00b6 Just as we create histograms in one dimension by dividing the number line into bins, we can also create histograms in two dimensions by dividing points among two- dimensional bins. We\u2019ll take a brief look at several ways to do this here. We\u2019ll start by defining some data\u2014an x and y array drawn from a multivariate Gaussian distribution: mean = [ 0 , 0 ] con = [[ 1 , 1 ],[ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , con , 10000 ) . T plt.hist2d: Two-dimensional histogram \u00b6 One straightforward way to plot a two-dimensional histogram is to use Matplotlib\u2019s plt.hist2d function plt . hist2d ( x , y , bins = 30 , cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( 'Counts in Bin' ) Just as with plt.hist, plt.hist2d has a number of extra options to fine-tune the plot and the binning, which are nicely outlined in the function docstring. Further, just as plt.hist has a counterpart in np.histogram, plt.hist2d has a counterpart in np.histogram2d, which can be used as follows: counts , xedges , yedges = np . histogram2d ( x , y , bins = 30 ) xedges array([-4.42186998, -4.13921105, -3.85655212, -3.57389318, -3.29123425, -3.00857531, -2.72591638, -2.44325744, -2.16059851, -1.87793957, -1.59528064, -1.31262171, -1.02996277, -0.74730384, -0.4646449 , -0.18198597, 0.10067297, 0.3833319 , 0.66599084, 0.94864977, 1.2313087 , 1.51396764, 1.79662657, 2.07928551, 2.36194444, 2.64460338, 2.92726231, 3.20992125, 3.49258018, 3.77523911, 4.05789805]) plt.hexbin: Hexagonal binnings \u00b6 The two-dimensional histogram creates a tessellation of squares across the axes. Another natural shape for such a tessellation is the regular hexagon. For this purpose, Matplotlib provides the plt.hexbin routine, which represents a two-dimensional dataset binned within a grid of hexagons plt . hexbin ( x , y , gridsize = 30 , cmap = 'Blues' ) cb = plt . colorbar ( label = 'Count in Bins' ) Kernel density estimation \u00b6 Another common method of evaluating densities in multiple dimensions is kernel density estimation (KDE). KDE can be thought of as a way to \u201csmear out\u201d the points in space and add up the result to obtain a smooth function. One extremely quick and simple KDE implementation exists in the scipy.stats package. Here is a quick example of using the KDE on this data from scipy.stats import gaussian_kde # fit an arry of size nxn data = np . vstack ([ x , y ]) kde = gaussian_kde ( data ) # evaluate on a regular grid xgrid = np . linspace ( - 3.5 , 3.5 , 40 ) ygrid = np . linspace ( - 6 , 6 , 40 ) Xgrid , Ygrid = np . meshgrid ( xgrid , ygrid ) Z = kde . evaluate ( np . vstack ([ Xgrid . ravel (), Ygrid . ravel ()])) # plot the result as an image plt . imshow ( Z . reshape ( Xgrid . shape ), origin = 'lower' , aspect = 'auto' , extent = [ - 3.5 , 3.5 , - 5 , 5 ], cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( 'Density' ) KDE has a smoothing length that effectively slides the knob between detail and smoothness (one example of the ubiquitous bias\u2013variance trade-off). The literature on choosing an appropriate smoothing length is vast: gaussian_kde uses a rule of thumb to attempt to find a nearly optimal smoothing length for the input data.","title":"06Histograms Binnings and Density"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#06-histograms-binnings-and-density","text":"Histograms, Binnings, and Density Two-Dimensional Histograms and Binnings plt.hexbin: Hexagonal binnings Kernel density estimation","title":"06 -  Histograms, Binnings, and Density"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#histograms-binnings-and-density","text":"A simple histogram can be a great first step in understanding a dataset. Earlier, we saw a preview of Matplotlib\u2019s histogram function % matplotlib inline import numpy as np import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) data = np . random . randn ( 1000 ) plt . hist ( data ); # A more customized histogram plt . hist ( data , bins = 30 , alpha = 0.5 , histtype = 'stepfilled' , color = 'steelblue' , edgecolor = 'none' ); The plt.hist docstring has more information on other customization options avail\u2010 able. I find this combination of histtype=\u2018stepfilled\u2019 along with some transpar\u2010 ency alpha to be very useful when comparing histograms of several distributions x1 = np . random . normal ( 0 , 0.8 , 1000 ) x2 = np . random . normal ( - 2 , 1 , 1000 ) x3 = np . random . normal ( 2 , 3 , 1000 ) kwargs = dict ( histtype = 'stepfilled' , alpha = 0.3 , bins = 40 ) plt . hist ( x1 , ** kwargs ); plt . hist ( x2 , ** kwargs ); plt . hist ( x3 , ** kwargs ); If you would like to simply compute the histogram (that is, count the number of points in a given bin) and not display it, the np.histogram() function is available: counts , bin_edges = np . histogram ( data , bins = 5 ) counts array([ 17, 292, 555, 132, 4])","title":"Histograms, Binnings, and Density"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#two-dimensional-histograms-and-binnings","text":"Just as we create histograms in one dimension by dividing the number line into bins, we can also create histograms in two dimensions by dividing points among two- dimensional bins. We\u2019ll take a brief look at several ways to do this here. We\u2019ll start by defining some data\u2014an x and y array drawn from a multivariate Gaussian distribution: mean = [ 0 , 0 ] con = [[ 1 , 1 ],[ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , con , 10000 ) . T","title":"Two-Dimensional Histograms and Binnings"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#plthist2d-two-dimensional-histogram","text":"One straightforward way to plot a two-dimensional histogram is to use Matplotlib\u2019s plt.hist2d function plt . hist2d ( x , y , bins = 30 , cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( 'Counts in Bin' ) Just as with plt.hist, plt.hist2d has a number of extra options to fine-tune the plot and the binning, which are nicely outlined in the function docstring. Further, just as plt.hist has a counterpart in np.histogram, plt.hist2d has a counterpart in np.histogram2d, which can be used as follows: counts , xedges , yedges = np . histogram2d ( x , y , bins = 30 ) xedges array([-4.42186998, -4.13921105, -3.85655212, -3.57389318, -3.29123425, -3.00857531, -2.72591638, -2.44325744, -2.16059851, -1.87793957, -1.59528064, -1.31262171, -1.02996277, -0.74730384, -0.4646449 , -0.18198597, 0.10067297, 0.3833319 , 0.66599084, 0.94864977, 1.2313087 , 1.51396764, 1.79662657, 2.07928551, 2.36194444, 2.64460338, 2.92726231, 3.20992125, 3.49258018, 3.77523911, 4.05789805])","title":"plt.hist2d: Two-dimensional histogram"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#plthexbin-hexagonal-binnings","text":"The two-dimensional histogram creates a tessellation of squares across the axes. Another natural shape for such a tessellation is the regular hexagon. For this purpose, Matplotlib provides the plt.hexbin routine, which represents a two-dimensional dataset binned within a grid of hexagons plt . hexbin ( x , y , gridsize = 30 , cmap = 'Blues' ) cb = plt . colorbar ( label = 'Count in Bins' )","title":"plt.hexbin: Hexagonal binnings"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/#kernel-density-estimation","text":"Another common method of evaluating densities in multiple dimensions is kernel density estimation (KDE). KDE can be thought of as a way to \u201csmear out\u201d the points in space and add up the result to obtain a smooth function. One extremely quick and simple KDE implementation exists in the scipy.stats package. Here is a quick example of using the KDE on this data from scipy.stats import gaussian_kde # fit an arry of size nxn data = np . vstack ([ x , y ]) kde = gaussian_kde ( data ) # evaluate on a regular grid xgrid = np . linspace ( - 3.5 , 3.5 , 40 ) ygrid = np . linspace ( - 6 , 6 , 40 ) Xgrid , Ygrid = np . meshgrid ( xgrid , ygrid ) Z = kde . evaluate ( np . vstack ([ Xgrid . ravel (), Ygrid . ravel ()])) # plot the result as an image plt . imshow ( Z . reshape ( Xgrid . shape ), origin = 'lower' , aspect = 'auto' , extent = [ - 3.5 , 3.5 , - 5 , 5 ], cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( 'Density' ) KDE has a smoothing length that effectively slides the knob between detail and smoothness (one example of the ubiquitous bias\u2013variance trade-off). The literature on choosing an appropriate smoothing length is vast: gaussian_kde uses a rule of thumb to attempt to find a nearly optimal smoothing length for the input data.","title":"Kernel density estimation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 07 - Customizing Plot Legends \u00b6 Customizing Plot Legends Choosing Elements for the Legend Legend for Size of Points Multiple Legends Customizing Plot Legends \u00b6 Plot legends give meaning to a visualization, assigning labels to the various plot ele\u2010 ments. We previously saw how to create a simple legend; here we\u2019ll take a look at cus\u2010 tomizing the placement and aesthetics of the legend in Matplotlib. The simplest legend can be created with the plt.legend() command, which auto\u2010 matically creates a legend for any labeled plot elements import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np x = np . linspace ( 0 , 10 , 10000 ) fig , ax = plt . subplots () ax . plot ( x , np . sin ( x ), '-b' , label = 'Sine' ) ax . plot ( x , np . cos ( x ), '--r' , label = 'Cosine' ) ax . axis ( 'equal' ) leg = ax . legend (); #many wasy we can customize the legend, # we can specify the location and turn off the frame ax . legend ( loc = 'upper left' , frameon = False ) fig #We can use the ncol command to specify the number of columns in the legend ax . legend ( frameon = False , loc = 'lower center' , ncol = 2 ) fig # We can use a rounded box (fancybox) or add a shadow, change the transparency #(alpha value) of the frame, or change the padding around the text ax . legend ( fancybox = True , framealpha = 0.8 , shadow = True , borderpad = 0.5 ) fig Choosing Elements for the Legend \u00b6 As we\u2019ve already seen, the legend includes all labeled elements by default. If this is not what is desired, we can fine-tune which elements and labels appear in the legend by using the objects returned by plot commands. The plt.plot() command is able to create multiple lines at once, and returns a list of created line instances. Passing any of these to plt.legend() will tell it which to identify, along with the labels we\u2019d like to specify y = np . sin ( x [:, np . newaxis ] + np . pi * np . arange ( 0 , 2 , 0.5 )) lines = plt . plot ( x , y ) # line is a list of plt.line2D instance plt . legend ( lines [: 2 ],[ 'first' , 'second' ]) <matplotlib.legend.Legend at 0x7fba9d7a2c10> I generally find in practice that it is clearer to use the first method, applying labels to the plot elements you\u2019d like to show on the legend plt . plot ( x , y [:, 0 ], label = 'first' ) plt . plot ( x , y [:, 1 ], label = 'second' ) plt . plot ( x , y [:, 2 :]) plt . legend ( framealpha = 1 , frameon = True ); Legend for Size of Points \u00b6 Sometimes the legend defaults are not sufficient for the given visualization. For exam\u2010 ple, perhaps you\u2019re using the size of points to mark certain features of the data, and want to create a legend reflecting this. Here is an example where we\u2019ll use the size of points to indicate populations of California cities. We\u2019d like a legend that specifies the scale of the sizes of the points, and we\u2019ll accomplish this by plotting some labeled data with no entries import pandas as pd cities = pd . read_csv ( '../data/california_cities.csv' ) # extract the data we'er interestd in lat , lon = cities [ 'latd' ], cities [ 'longd' ] population , area = cities [ 'population_total' ], cities [ 'area_total_km2' ] # scatter the points, using size and color but no label plt . scatter ( lon , lat , label = None , c = np . log10 ( population ), cmap = 'viridis' , s = area , linewidth = 0 , alpha = 0.5 ) #plt.axis(aspect='equal') plt . xlabel ( 'Longitude' ) plt . ylabel ( 'Latitude' ) plt . colorbar ( label = 'log$_ {10} $(population)' ) plt . clim ( 3 , 7 ) # scatter the points, using size and color but no label plt . scatter ( lon , lat , label = None , c = np . log10 ( population ), cmap = 'viridis' , s = area , linewidth = 0 , alpha = 0.5 ) #plt.axis(aspect='equal') plt . xlabel ( 'Longitude' ) plt . ylabel ( 'Latitude' ) plt . colorbar ( label = 'log$_ {10} $(population)' ) plt . clim ( 3 , 7 ) # here we created a legend: # we'll plot empty lists with the desired size and label for area in [ 100 , 300 , 500 ]: plt . scatter ([],[], c = 'k' , alpha = 0.3 , s = area , label = str ( area ) + ' km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , title = 'City Area' ) plt . title ( 'California cities: Area and Population' ) Text(0.5, 1.0, 'California cities: Area and Population') Multiple Legends \u00b6 Sometimes when designing a plot you\u2019d like to add multiple legends to the same axes. Unfortunately, Matplotlib does not make this easy: via the standard legend interface, it is only possible to create a single legend for the entire plot. If you try to create a second legend using plt.legend() or ax.legend(), it will simply override the first one. We can work around this by creating a new legend artist from scratch, and then using the lower-level ax.add_artist() method to manually add the second artist to the plot fig , ax = plt . subplots () lines = [] styles = [ '-' , '--' , '-.' , ':' ] x = np . linspace ( 0 , 10 , 1000 ) for i in range ( 4 ): lines += ax . plot ( x , np . sin ( x - i * np . pi / 2 ), styles [ i ], color = 'black' ) ax . axis ( 'equal' ) # specify the lines and labels of the first legend ax . legend ( lines [: 2 ], [ 'line A' , 'line B' ], loc = 'upper right' , frameon = False ) # Create the second legend and add the artist manually. from matplotlib.legend import Legend leg = Legend ( ax , lines [ 2 :], [ 'line C' , 'line D' ], loc = 'lower right' , frameon = False ) ax . add_artist ( leg );","title":"07customized plot legends"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#07-customizing-plot-legends","text":"Customizing Plot Legends Choosing Elements for the Legend Legend for Size of Points Multiple Legends","title":"07 -  Customizing Plot Legends"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#customizing-plot-legends","text":"Plot legends give meaning to a visualization, assigning labels to the various plot ele\u2010 ments. We previously saw how to create a simple legend; here we\u2019ll take a look at cus\u2010 tomizing the placement and aesthetics of the legend in Matplotlib. The simplest legend can be created with the plt.legend() command, which auto\u2010 matically creates a legend for any labeled plot elements import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np x = np . linspace ( 0 , 10 , 10000 ) fig , ax = plt . subplots () ax . plot ( x , np . sin ( x ), '-b' , label = 'Sine' ) ax . plot ( x , np . cos ( x ), '--r' , label = 'Cosine' ) ax . axis ( 'equal' ) leg = ax . legend (); #many wasy we can customize the legend, # we can specify the location and turn off the frame ax . legend ( loc = 'upper left' , frameon = False ) fig #We can use the ncol command to specify the number of columns in the legend ax . legend ( frameon = False , loc = 'lower center' , ncol = 2 ) fig # We can use a rounded box (fancybox) or add a shadow, change the transparency #(alpha value) of the frame, or change the padding around the text ax . legend ( fancybox = True , framealpha = 0.8 , shadow = True , borderpad = 0.5 ) fig","title":"Customizing Plot Legends"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#choosing-elements-for-the-legend","text":"As we\u2019ve already seen, the legend includes all labeled elements by default. If this is not what is desired, we can fine-tune which elements and labels appear in the legend by using the objects returned by plot commands. The plt.plot() command is able to create multiple lines at once, and returns a list of created line instances. Passing any of these to plt.legend() will tell it which to identify, along with the labels we\u2019d like to specify y = np . sin ( x [:, np . newaxis ] + np . pi * np . arange ( 0 , 2 , 0.5 )) lines = plt . plot ( x , y ) # line is a list of plt.line2D instance plt . legend ( lines [: 2 ],[ 'first' , 'second' ]) <matplotlib.legend.Legend at 0x7fba9d7a2c10> I generally find in practice that it is clearer to use the first method, applying labels to the plot elements you\u2019d like to show on the legend plt . plot ( x , y [:, 0 ], label = 'first' ) plt . plot ( x , y [:, 1 ], label = 'second' ) plt . plot ( x , y [:, 2 :]) plt . legend ( framealpha = 1 , frameon = True );","title":"Choosing Elements for the Legend"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#legend-for-size-of-points","text":"Sometimes the legend defaults are not sufficient for the given visualization. For exam\u2010 ple, perhaps you\u2019re using the size of points to mark certain features of the data, and want to create a legend reflecting this. Here is an example where we\u2019ll use the size of points to indicate populations of California cities. We\u2019d like a legend that specifies the scale of the sizes of the points, and we\u2019ll accomplish this by plotting some labeled data with no entries import pandas as pd cities = pd . read_csv ( '../data/california_cities.csv' ) # extract the data we'er interestd in lat , lon = cities [ 'latd' ], cities [ 'longd' ] population , area = cities [ 'population_total' ], cities [ 'area_total_km2' ] # scatter the points, using size and color but no label plt . scatter ( lon , lat , label = None , c = np . log10 ( population ), cmap = 'viridis' , s = area , linewidth = 0 , alpha = 0.5 ) #plt.axis(aspect='equal') plt . xlabel ( 'Longitude' ) plt . ylabel ( 'Latitude' ) plt . colorbar ( label = 'log$_ {10} $(population)' ) plt . clim ( 3 , 7 ) # scatter the points, using size and color but no label plt . scatter ( lon , lat , label = None , c = np . log10 ( population ), cmap = 'viridis' , s = area , linewidth = 0 , alpha = 0.5 ) #plt.axis(aspect='equal') plt . xlabel ( 'Longitude' ) plt . ylabel ( 'Latitude' ) plt . colorbar ( label = 'log$_ {10} $(population)' ) plt . clim ( 3 , 7 ) # here we created a legend: # we'll plot empty lists with the desired size and label for area in [ 100 , 300 , 500 ]: plt . scatter ([],[], c = 'k' , alpha = 0.3 , s = area , label = str ( area ) + ' km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , title = 'City Area' ) plt . title ( 'California cities: Area and Population' ) Text(0.5, 1.0, 'California cities: Area and Population')","title":"Legend for Size of Points"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/#multiple-legends","text":"Sometimes when designing a plot you\u2019d like to add multiple legends to the same axes. Unfortunately, Matplotlib does not make this easy: via the standard legend interface, it is only possible to create a single legend for the entire plot. If you try to create a second legend using plt.legend() or ax.legend(), it will simply override the first one. We can work around this by creating a new legend artist from scratch, and then using the lower-level ax.add_artist() method to manually add the second artist to the plot fig , ax = plt . subplots () lines = [] styles = [ '-' , '--' , '-.' , ':' ] x = np . linspace ( 0 , 10 , 1000 ) for i in range ( 4 ): lines += ax . plot ( x , np . sin ( x - i * np . pi / 2 ), styles [ i ], color = 'black' ) ax . axis ( 'equal' ) # specify the lines and labels of the first legend ax . legend ( lines [: 2 ], [ 'line A' , 'line B' ], loc = 'upper right' , frameon = False ) # Create the second legend and add the artist manually. from matplotlib.legend import Legend leg = Legend ( ax , lines [ 2 :], [ 'line C' , 'line D' ], loc = 'lower right' , frameon = False ) ax . add_artist ( leg );","title":"Multiple Legends"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 08 - Customizing Colorbars \u00b6 Customizing Colorbars Customizing Colorbars Choosing the colormap Color limits and extensions Discrete colorbars Customizing Colorbars \u00b6 Plot legends identify discrete labels of discrete points. For continuous labels based on the color of points, lines, or regions, a labeled colorbar can be a great tool. In Mat\u2010 plotlib, a colorbar is a separate axes that can provide a key for the meaning of colors in a plot. import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np x = np . linspace ( 0 , 10 , 1000 ) I = np . sin ( x ) * np . cos ( x [:, np . newaxis ]) np . cos ( x [:, np . newaxis ]) array([[ 1.00000000e+00], [ 9.99949900e-01], [ 9.99799606e-01], [ 9.99549133e-01], [ 9.99198505e-01], [ 9.98747758e-01], [ 9.98196937e-01], [ 9.97546097e-01], [ 9.96795304e-01], [ 9.95944632e-01], [ 9.94994167e-01], [ 9.93944004e-01], [ 9.92794249e-01], [ 9.91545016e-01], [ 9.90196431e-01], [ 9.88748629e-01], [ 9.87201754e-01], [ 9.85555963e-01], [ 9.83811419e-01], [ 9.81968298e-01], [ 9.80026785e-01], [ 9.77987073e-01], [ 9.75849367e-01], [ 9.73613882e-01], [ 9.71280841e-01], [ 9.68850478e-01], [ 9.66323038e-01], [ 9.63698772e-01], [ 9.60977944e-01], [ 9.58160826e-01], [ 9.55247701e-01], [ 9.52238861e-01], [ 9.49134607e-01], [ 9.45935251e-01], [ 9.42641112e-01], [ 9.39252521e-01], [ 9.35769817e-01], [ 9.32193350e-01], [ 9.28523478e-01], [ 9.24760567e-01], [ 9.20904997e-01], [ 9.16957152e-01], [ 9.12917429e-01], [ 9.08786232e-01], [ 9.04563975e-01], [ 9.00251081e-01], [ 8.95847982e-01], [ 8.91355120e-01], [ 8.86772945e-01], [ 8.82101915e-01], [ 8.77342499e-01], [ 8.72495174e-01], [ 8.67560426e-01], [ 8.62538748e-01], [ 8.57430645e-01], [ 8.52236627e-01], [ 8.46957216e-01], [ 8.41592940e-01], [ 8.36144337e-01], [ 8.30611953e-01], [ 8.24996341e-01], [ 8.19298066e-01], [ 8.13517698e-01], [ 8.07655815e-01], [ 8.01713006e-01], [ 7.95689865e-01], [ 7.89586997e-01], [ 7.83405013e-01], [ 7.77144531e-01], [ 7.70806181e-01], [ 7.64390596e-01], [ 7.57898419e-01], [ 7.51330302e-01], [ 7.44686901e-01], [ 7.37968884e-01], [ 7.31176922e-01], [ 7.24311697e-01], [ 7.17373896e-01], [ 7.10364214e-01], [ 7.03283355e-01], [ 6.96132027e-01], [ 6.88910947e-01], [ 6.81620838e-01], [ 6.74262431e-01], [ 6.66836464e-01], [ 6.59343679e-01], [ 6.51784829e-01], [ 6.44160671e-01], [ 6.36471968e-01], [ 6.28719491e-01], [ 6.20904016e-01], [ 6.13026327e-01], [ 6.05087214e-01], [ 5.97087471e-01], [ 5.89027900e-01], [ 5.80909308e-01], [ 5.72732510e-01], [ 5.64498325e-01], [ 5.56207577e-01], [ 5.47861097e-01], [ 5.39459722e-01], [ 5.31004293e-01], [ 5.22495658e-01], [ 5.13934670e-01], [ 5.05322185e-01], [ 4.96659067e-01], [ 4.87946184e-01], [ 4.79184410e-01], [ 4.70374621e-01], [ 4.61517701e-01], [ 4.52614537e-01], [ 4.43666022e-01], [ 4.34673051e-01], [ 4.25636526e-01], [ 4.16557353e-01], [ 4.07436441e-01], [ 3.98274704e-01], [ 3.89073060e-01], [ 3.79832432e-01], [ 3.70553744e-01], [ 3.61237927e-01], [ 3.51885914e-01], [ 3.42498642e-01], [ 3.33077052e-01], [ 3.23622088e-01], [ 3.14134698e-01], [ 3.04615831e-01], [ 2.95066442e-01], [ 2.85487487e-01], [ 2.75879926e-01], [ 2.66244723e-01], [ 2.56582842e-01], [ 2.46895251e-01], [ 2.37182922e-01], [ 2.27446827e-01], [ 2.17687942e-01], [ 2.07907245e-01], [ 1.98105716e-01], [ 1.88284337e-01], [ 1.78444091e-01], [ 1.68585966e-01], [ 1.58710948e-01], [ 1.48820028e-01], [ 1.38914196e-01], [ 1.28994445e-01], [ 1.19061768e-01], [ 1.09117162e-01], [ 9.91616222e-02], [ 8.91961465e-02], [ 7.92217334e-02], [ 6.92393823e-02], [ 5.92500934e-02], [ 4.92548678e-02], [ 3.92547068e-02], [ 2.92506125e-02], [ 1.92435873e-02], [ 9.23463398e-03], [-7.75244699e-04], [-1.07850457e-02], [-2.07937660e-02], [-3.08004029e-02], [-4.08039535e-02], [-5.08034156e-02], [-6.07977872e-02], [-7.07860669e-02], [-8.07672539e-02], [-9.07403481e-02], [-1.00704350e-01], [-1.10658262e-01], [-1.20601085e-01], [-1.30531825e-01], [-1.40449485e-01], [-1.50353072e-01], [-1.60241594e-01], [-1.70114060e-01], [-1.79969480e-01], [-1.89806868e-01], [-1.99625237e-01], [-2.09423604e-01], [-2.19200987e-01], [-2.28956405e-01], [-2.38688883e-01], [-2.48397444e-01], [-2.58081116e-01], [-2.67738928e-01], [-2.77369913e-01], [-2.86973105e-01], [-2.96547543e-01], [-3.06092267e-01], [-3.15606321e-01], [-3.25088751e-01], [-3.34538607e-01], [-3.43954943e-01], [-3.53336815e-01], [-3.62683283e-01], [-3.71993410e-01], [-3.81266263e-01], [-3.90500914e-01], [-3.99696437e-01], [-4.08851910e-01], [-4.17966417e-01], [-4.27039043e-01], [-4.36068881e-01], [-4.45055025e-01], [-4.53996574e-01], [-4.62892633e-01], [-4.71742311e-01], [-4.80544720e-01], [-4.89298979e-01], [-4.98004210e-01], [-5.06659542e-01], [-5.15264107e-01], [-5.23817042e-01], [-5.32317492e-01], [-5.40764603e-01], [-5.49157530e-01], [-5.57495432e-01], [-5.65777473e-01], [-5.74002823e-01], [-5.82170659e-01], [-5.90280161e-01], [-5.98330518e-01], [-6.06320922e-01], [-6.14250574e-01], [-6.22118677e-01], [-6.29924445e-01], [-6.37667095e-01], [-6.45345851e-01], [-6.52959943e-01], [-6.60508609e-01], [-6.67991093e-01], [-6.75406644e-01], [-6.82754520e-01], [-6.90033984e-01], [-6.97244307e-01], [-7.04384767e-01], [-7.11454648e-01], [-7.18453241e-01], [-7.25379846e-01], [-7.32233768e-01], [-7.39014320e-01], [-7.45720824e-01], [-7.52352607e-01], [-7.58909005e-01], [-7.65389360e-01], [-7.71793024e-01], [-7.78119354e-01], [-7.84367718e-01], [-7.90537488e-01], [-7.96628046e-01], [-8.02638783e-01], [-8.08569096e-01], [-8.14418391e-01], [-8.20186082e-01], [-8.25871590e-01], [-8.31474347e-01], [-8.36993790e-01], [-8.42429367e-01], [-8.47780532e-01], [-8.53046751e-01], [-8.58227495e-01], [-8.63322245e-01], [-8.68330490e-01], [-8.73251730e-01], [-8.78085470e-01], [-8.82831226e-01], [-8.87488523e-01], [-8.92056894e-01], [-8.96535881e-01], [-9.00925037e-01], [-9.05223919e-01], [-9.09432099e-01], [-9.13549155e-01], [-9.17574673e-01], [-9.21508251e-01], [-9.25349494e-01], [-9.29098017e-01], [-9.32753446e-01], [-9.36315413e-01], [-9.39783561e-01], [-9.43157544e-01], [-9.46437023e-01], [-9.49621670e-01], [-9.52711165e-01], [-9.55705199e-01], [-9.58603471e-01], [-9.61405692e-01], [-9.64111581e-01], [-9.66720867e-01], [-9.69233287e-01], [-9.71648591e-01], [-9.73966536e-01], [-9.76186890e-01], [-9.78309431e-01], [-9.80333945e-01], [-9.82260231e-01], [-9.84088095e-01], [-9.85817354e-01], [-9.87447834e-01], [-9.88979373e-01], [-9.90411816e-01], [-9.91745021e-01], [-9.92978853e-01], [-9.94113189e-01], [-9.95147916e-01], [-9.96082929e-01], [-9.96918136e-01], [-9.97653452e-01], [-9.98288803e-01], [-9.98824127e-01], [-9.99259369e-01], [-9.99594485e-01], [-9.99829443e-01], [-9.99964218e-01], [-9.99998798e-01], [-9.99933178e-01], [-9.99767366e-01], [-9.99501377e-01], [-9.99135239e-01], [-9.98668988e-01], [-9.98102670e-01], [-9.97436344e-01], [-9.96670075e-01], [-9.95803940e-01], [-9.94838026e-01], [-9.93772430e-01], [-9.92607258e-01], [-9.91342628e-01], [-9.89978665e-01], [-9.88515508e-01], [-9.86953301e-01], [-9.85292203e-01], [-9.83532378e-01], [-9.81674005e-01], [-9.79717268e-01], [-9.77662364e-01], [-9.75509498e-01], [-9.73258887e-01], [-9.70910757e-01], [-9.68465341e-01], [-9.65922886e-01], [-9.63283645e-01], [-9.60547884e-01], [-9.57715877e-01], [-9.54787907e-01], [-9.51764268e-01], [-9.48645263e-01], [-9.45431204e-01], [-9.42122413e-01], [-9.38719222e-01], [-9.35221972e-01], [-9.31631013e-01], [-9.27946706e-01], [-9.24169418e-01], [-9.20299529e-01], [-9.16337427e-01], [-9.12283508e-01], [-9.08138179e-01], [-9.03901855e-01], [-8.99574960e-01], [-8.95157929e-01], [-8.90651203e-01], [-8.86055234e-01], [-8.81370484e-01], [-8.76597420e-01], [-8.71736521e-01], [-8.66788276e-01], [-8.61753178e-01], [-8.56631733e-01], [-8.51424454e-01], [-8.46131863e-01], [-8.40754490e-01], [-8.35292874e-01], [-8.29747562e-01], [-8.24119109e-01], [-8.18408081e-01], [-8.12615048e-01], [-8.06740592e-01], [-8.00785301e-01], [-7.94749771e-01], [-7.88634608e-01], [-7.82440424e-01], [-7.76167840e-01], [-7.69817485e-01], [-7.63389994e-01], [-7.56886012e-01], [-7.50306190e-01], [-7.43651188e-01], [-7.36921673e-01], [-7.30118318e-01], [-7.23241806e-01], [-7.16292826e-01], [-7.09272073e-01], [-7.02180252e-01], [-6.95018073e-01], [-6.87786253e-01], [-6.80485517e-01], [-6.73116597e-01], [-6.65680231e-01], [-6.58177165e-01], [-6.50608149e-01], [-6.42973943e-01], [-6.35275311e-01], [-6.27513025e-01], [-6.19687862e-01], [-6.11800607e-01], [-6.03852050e-01], [-5.95842988e-01], [-5.87774222e-01], [-5.79646561e-01], [-5.71460820e-01], [-5.63217820e-01], [-5.54918385e-01], [-5.46563347e-01], [-5.38153544e-01], [-5.29689819e-01], [-5.21173018e-01], [-5.12603997e-01], [-5.03983613e-01], [-4.95312730e-01], [-4.86592217e-01], [-4.77822948e-01], [-4.69005801e-01], [-4.60141660e-01], [-4.51231412e-01], [-4.42275952e-01], [-4.33276176e-01], [-4.24232986e-01], [-4.15147288e-01], [-4.06019993e-01], [-3.96852014e-01], [-3.87644272e-01], [-3.78397687e-01], [-3.69113187e-01], [-3.59791703e-01], [-3.50434167e-01], [-3.41041518e-01], [-3.31614697e-01], [-3.22154648e-01], [-3.12662319e-01], [-3.03138662e-01], [-2.93584631e-01], [-2.84001182e-01], [-2.74389277e-01], [-2.64749878e-01], [-2.55083951e-01], [-2.45392466e-01], [-2.35676391e-01], [-2.25936703e-01], [-2.16174375e-01], [-2.06390387e-01], [-1.96585719e-01], [-1.86761353e-01], [-1.76918273e-01], [-1.67057466e-01], [-1.57179921e-01], [-1.47286626e-01], [-1.37378573e-01], [-1.27456754e-01], [-1.17522165e-01], [-1.07575800e-01], [-9.76186559e-02], [-8.76517305e-02], [-7.76760224e-02], [-6.76925312e-02], [-5.77022572e-02], [-4.77062016e-02], [-3.77053657e-02], [-2.77007519e-02], [-1.76933624e-02], [-7.68420006e-03], [ 2.32573223e-03], [ 1.23354315e-02], [ 2.23438947e-02], [ 3.23501191e-02], [ 4.23531021e-02], [ 5.23518413e-02], [ 6.23453348e-02], [ 7.23325814e-02], [ 8.23125803e-02], [ 9.22843315e-02], [ 1.02246836e-01], [ 1.12199095e-01], [ 1.22140112e-01], [ 1.32068891e-01], [ 1.41984436e-01], [ 1.51885755e-01], [ 1.61771855e-01], [ 1.71641745e-01], [ 1.81494437e-01], [ 1.91328943e-01], [ 2.01144278e-01], [ 2.10939459e-01], [ 2.20713504e-01], [ 2.30465433e-01], [ 2.40194270e-01], [ 2.49899039e-01], [ 2.59578769e-01], [ 2.69232489e-01], [ 2.78859232e-01], [ 2.88458033e-01], [ 2.98027932e-01], [ 3.07567967e-01], [ 3.17077185e-01], [ 3.26554632e-01], [ 3.35999358e-01], [ 3.45410418e-01], [ 3.54786867e-01], [ 3.64127767e-01], [ 3.73432181e-01], [ 3.82699178e-01], [ 3.91927829e-01], [ 4.01117208e-01], [ 4.10266396e-01], [ 4.19374475e-01], [ 4.28440534e-01], [ 4.37463662e-01], [ 4.46442957e-01], [ 4.55377519e-01], [ 4.64266452e-01], [ 4.73108866e-01], [ 4.81903875e-01], [ 4.90650597e-01], [ 4.99348156e-01], [ 5.07995681e-01], [ 5.16592305e-01], [ 5.25137167e-01], [ 5.33629410e-01], [ 5.42068184e-01], [ 5.50452643e-01], [ 5.58781947e-01], [ 5.67055261e-01], [ 5.75271756e-01], [ 5.83430610e-01], [ 5.91531004e-01], [ 5.99572127e-01], [ 6.07553173e-01], [ 6.15473343e-01], [ 6.23331843e-01], [ 6.31127885e-01], [ 6.38860689e-01], [ 6.46529479e-01], [ 6.54133487e-01], [ 6.61671951e-01], [ 6.69144116e-01], [ 6.76549233e-01], [ 6.83886561e-01], [ 6.91155363e-01], [ 6.98354912e-01], [ 7.05484486e-01], [ 7.12543371e-01], [ 7.19530859e-01], [ 7.26446251e-01], [ 7.33288853e-01], [ 7.40057981e-01], [ 7.46752954e-01], [ 7.53373104e-01], [ 7.59917766e-01], [ 7.66386284e-01], [ 7.72778011e-01], [ 7.79092307e-01], [ 7.85328537e-01], [ 7.91486078e-01], [ 7.97564313e-01], [ 8.03562632e-01], [ 8.09480434e-01], [ 8.15317127e-01], [ 8.21072126e-01], [ 8.26744853e-01], [ 8.32334742e-01], [ 8.37841230e-01], [ 8.43263768e-01], [ 8.48601811e-01], [ 8.53854824e-01], [ 8.59022282e-01], [ 8.64103666e-01], [ 8.69098468e-01], [ 8.74006186e-01], [ 8.78826329e-01], [ 8.83558414e-01], [ 8.88201968e-01], [ 8.92756523e-01], [ 8.97221626e-01], [ 9.01596827e-01], [ 9.05881688e-01], [ 9.10075781e-01], [ 9.14178684e-01], [ 9.18189988e-01], [ 9.22109289e-01], [ 9.25936195e-01], [ 9.29670323e-01], [ 9.33311299e-01], [ 9.36858757e-01], [ 9.40312343e-01], [ 9.43671709e-01], [ 9.46936521e-01], [ 9.50106449e-01], [ 9.53181178e-01], [ 9.56160398e-01], [ 9.59043812e-01], [ 9.61831130e-01], [ 9.64522073e-01], [ 9.67116371e-01], [ 9.69613765e-01], [ 9.72014004e-01], [ 9.74316848e-01], [ 9.76522066e-01], [ 9.78629437e-01], [ 9.80638749e-01], [ 9.82549803e-01], [ 9.84362405e-01], [ 9.86076374e-01], [ 9.87691540e-01], [ 9.89207739e-01], [ 9.90624820e-01], [ 9.91942641e-01], [ 9.93161070e-01], [ 9.94279984e-01], [ 9.95299273e-01], [ 9.96218833e-01], [ 9.97038572e-01], [ 9.97758408e-01], [ 9.98378270e-01], [ 9.98898095e-01], [ 9.99317830e-01], [ 9.99637435e-01], [ 9.99856876e-01], [ 9.99976133e-01], [ 9.99995192e-01], [ 9.99914052e-01], [ 9.99732722e-01], [ 9.99451218e-01], [ 9.99069571e-01], [ 9.98587817e-01], [ 9.98006005e-01], [ 9.97324193e-01], [ 9.96542450e-01], [ 9.95660854e-01], [ 9.94679493e-01], [ 9.93598466e-01], [ 9.92417881e-01], [ 9.91137856e-01], [ 9.89758520e-01], [ 9.88280011e-01], [ 9.86702476e-01], [ 9.85026074e-01], [ 9.83250973e-01], [ 9.81377351e-01], [ 9.79405396e-01], [ 9.77335304e-01], [ 9.75167284e-01], [ 9.72901553e-01], [ 9.70538338e-01], [ 9.68077875e-01], [ 9.65520411e-01], [ 9.62866203e-01], [ 9.60115516e-01], [ 9.57268626e-01], [ 9.54325818e-01], [ 9.51287388e-01], [ 9.48153638e-01], [ 9.44924885e-01], [ 9.41601450e-01], [ 9.38183667e-01], [ 9.34671879e-01], [ 9.31066437e-01], [ 9.27367703e-01], [ 9.23576047e-01], [ 9.19691849e-01], [ 9.15715499e-01], [ 9.11647395e-01], [ 9.07487943e-01], [ 9.03237562e-01], [ 8.98896678e-01], [ 8.94465724e-01], [ 8.89945145e-01], [ 8.85335394e-01], [ 8.80636933e-01], [ 8.75850233e-01], [ 8.70975773e-01], [ 8.66014042e-01], [ 8.60965536e-01], [ 8.55830762e-01], [ 8.50610235e-01], [ 8.45304477e-01], [ 8.39914019e-01], [ 8.34439403e-01], [ 8.28881176e-01], [ 8.23239896e-01], [ 8.17516128e-01], [ 8.11710445e-01], [ 8.05823429e-01], [ 7.99855670e-01], [ 7.93807766e-01], [ 7.87680323e-01], [ 7.81473955e-01], [ 7.75189283e-01], [ 7.68826938e-01], [ 7.62387557e-01], [ 7.55871785e-01], [ 7.49280275e-01], [ 7.42613687e-01], [ 7.35872690e-01], [ 7.29057959e-01], [ 7.22170177e-01], [ 7.15210034e-01], [ 7.08178227e-01], [ 7.01075461e-01], [ 6.93902448e-01], [ 6.86659906e-01], [ 6.79348561e-01], [ 6.71969145e-01], [ 6.64522399e-01], [ 6.57009068e-01], [ 6.49429905e-01], [ 6.41785669e-01], [ 6.34077127e-01], [ 6.26305051e-01], [ 6.18470219e-01], [ 6.10573417e-01], [ 6.02615435e-01], [ 5.94597072e-01], [ 5.86519131e-01], [ 5.78382421e-01], [ 5.70187757e-01], [ 5.61935960e-01], [ 5.53627858e-01], [ 5.45264283e-01], [ 5.36846073e-01], [ 5.28374071e-01], [ 5.19849126e-01], [ 5.11272092e-01], [ 5.02643829e-01], [ 4.93965202e-01], [ 4.85237080e-01], [ 4.76460337e-01], [ 4.67635853e-01], [ 4.58764512e-01], [ 4.49847203e-01], [ 4.40884820e-01], [ 4.31878260e-01], [ 4.22828426e-01], [ 4.13736226e-01], [ 4.04602569e-01], [ 3.95428371e-01], [ 3.86214551e-01], [ 3.76962033e-01], [ 3.67671743e-01], [ 3.58344613e-01], [ 3.48981577e-01], [ 3.39583574e-01], [ 3.30151544e-01], [ 3.20686433e-01], [ 3.11189189e-01], [ 3.01660765e-01], [ 2.92102114e-01], [ 2.82514195e-01], [ 2.72897968e-01], [ 2.63254397e-01], [ 2.53584448e-01], [ 2.43889090e-01], [ 2.34169294e-01], [ 2.24426035e-01], [ 2.14660288e-01], [ 2.04873032e-01], [ 1.95065249e-01], [ 1.85237919e-01], [ 1.75392030e-01], [ 1.65528565e-01], [ 1.55648516e-01], [ 1.45752870e-01], [ 1.35842619e-01], [ 1.25918758e-01], [ 1.15982279e-01], [ 1.06034179e-01], [ 9.60754549e-02], [ 8.61071037e-02], [ 7.61301246e-02], [ 6.61455173e-02], [ 5.61542823e-02], [ 4.61574206e-02], [ 3.61559340e-02], [ 2.61508246e-02], [ 1.61430949e-02], [ 6.13374766e-03], [-3.87621418e-03], [-1.38857876e-02], [-2.38939697e-02], [-3.38997577e-02], [-4.39021489e-02], [-5.39001411e-02], [-6.38927325e-02], [-7.38789220e-02], [-8.38577088e-02], [-9.38280931e-02], [-1.03789076e-01], [-1.13739659e-01], [-1.23678846e-01], [-1.33605640e-01], [-1.43519046e-01], [-1.53418073e-01], [-1.63301726e-01], [-1.73169017e-01], [-1.83018957e-01], [-1.92850558e-01], [-2.02662836e-01], [-2.12454807e-01], [-2.22225490e-01], [-2.31973906e-01], [-2.41699079e-01], [-2.51400034e-01], [-2.61075798e-01], [-2.70725403e-01], [-2.80347881e-01], [-2.89942268e-01], [-2.99507604e-01], [-3.09042928e-01], [-3.18547287e-01], [-3.28019728e-01], [-3.37459301e-01], [-3.46865061e-01], [-3.56236066e-01], [-3.65571375e-01], [-3.74870055e-01], [-3.84131173e-01], [-3.93353801e-01], [-4.02537015e-01], [-4.11679896e-01], [-4.20781526e-01], [-4.29840994e-01], [-4.38857392e-01], [-4.47829817e-01], [-4.56757370e-01], [-4.65639155e-01], [-4.74474285e-01], [-4.83261871e-01], [-4.92001036e-01], [-5.00690902e-01], [-5.09330599e-01], [-5.17919262e-01], [-5.26456029e-01], [-5.34940046e-01], [-5.43370462e-01], [-5.51746432e-01], [-5.60067118e-01], [-5.68331685e-01], [-5.76539306e-01], [-5.84689158e-01], [-5.92780425e-01], [-6.00812295e-01], [-6.08783964e-01], [-6.16694633e-01], [-6.24543510e-01], [-6.32329808e-01], [-6.40052747e-01], [-6.47711552e-01], [-6.55305458e-01], [-6.62833702e-01], [-6.70295531e-01], [-6.77690196e-01], [-6.85016957e-01], [-6.92275080e-01], [-6.99463837e-01], [-7.06582509e-01], [-7.13630381e-01], [-7.20606748e-01], [-7.27510910e-01], [-7.34342176e-01], [-7.41099862e-01], [-7.47783289e-01], [-7.54391789e-01], [-7.60924700e-01], [-7.67381366e-01], [-7.73761141e-01], [-7.80063386e-01], [-7.86287469e-01], [-7.92432766e-01], [-7.98498661e-01], [-8.04484548e-01], [-8.10389826e-01], [-8.16213903e-01], [-8.21956196e-01], [-8.27616129e-01], [-8.33193136e-01], [-8.38686657e-01], [-8.44096142e-01], [-8.49421049e-01], [-8.54660845e-01], [-8.59815004e-01], [-8.64883010e-01], [-8.69864355e-01], [-8.74758541e-01], [-8.79565076e-01], [-8.84283479e-01], [-8.88913277e-01], [-8.93454007e-01], [-8.97905213e-01], [-9.02266449e-01], [-9.06537279e-01], [-9.10717274e-01], [-9.14806016e-01], [-9.18803095e-01], [-9.22708110e-01], [-9.26520671e-01], [-9.30240394e-01], [-9.33866908e-01], [-9.37399849e-01], [-9.40838863e-01], [-9.44183606e-01], [-9.47433742e-01], [-9.50588945e-01], [-9.53648900e-01], [-9.56613300e-01], [-9.59481847e-01], [-9.62254255e-01], [-9.64930246e-01], [-9.67509551e-01], [-9.69991913e-01], [-9.72377081e-01], [-9.74664818e-01], [-9.76854895e-01], [-9.78947090e-01], [-9.80941196e-01], [-9.82837012e-01], [-9.84634348e-01], [-9.86333025e-01], [-9.87932871e-01], [-9.89433727e-01], [-9.90835442e-01], [-9.92137877e-01], [-9.93340899e-01], [-9.94444389e-01], [-9.95448237e-01], [-9.96352341e-01], [-9.97156611e-01], [-9.97860966e-01], [-9.98465337e-01], [-9.98969661e-01], [-9.99373890e-01], [-9.99677982e-01], [-9.99881906e-01], [-9.99985643e-01], [-9.99989182e-01], [-9.99892522e-01], [-9.99695674e-01], [-9.99398657e-01], [-9.99001501e-01], [-9.98504245e-01], [-9.97906940e-01], [-9.97209644e-01], [-9.96412429e-01], [-9.95515375e-01], [-9.94518569e-01], [-9.93422114e-01], [-9.92226118e-01], [-9.90930702e-01], [-9.89535995e-01], [-9.88042137e-01], [-9.86449278e-01], [-9.84757577e-01], [-9.82967204e-01], [-9.81078338e-01], [-9.79091169e-01], [-9.77005895e-01], [-9.74822726e-01], [-9.72541880e-01], [-9.70163586e-01], [-9.67688082e-01], [-9.65115616e-01], [-9.62446446e-01], [-9.59680840e-01], [-9.56819074e-01], [-9.53861435e-01], [-9.50808220e-01], [-9.47659734e-01], [-9.44416293e-01], [-9.41078223e-01], [-9.37645857e-01], [-9.34119539e-01], [-9.30499623e-01], [-9.26786471e-01], [-9.22980456e-01], [-9.19081959e-01], [-9.15091370e-01], [-9.11009089e-01], [-9.06835526e-01], [-9.02571099e-01], [-8.98216234e-01], [-8.93771368e-01], [-8.89236948e-01], [-8.84613426e-01], [-8.79901266e-01], [-8.75100941e-01], [-8.70212930e-01], [-8.65237726e-01], [-8.60175824e-01], [-8.55027734e-01], [-8.49793970e-01], [-8.44475058e-01], [-8.39071529e-01]]) plt . imshow ( I ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f5d4c0728b0> Customizing Colorbars \u00b6 We can specify the colormap using the cmap argument to the plotting function that is creating the visualization plt . imshow ( I , cmap = 'gray' ) <matplotlib.image.AxesImage at 0x7f5d3a3e2490> All the available colormaps are in the plt.cm namespace; using IPython\u2019s tab- completion feature will give you a full list of built-in possibilities: #plt.imshow(I, cmap='winter_r') But being able to choose a colormap is just the first step: more important is how to decide among the possibilities! The choice turns out to be much more subtle than you might initially expect. Choosing the colormap \u00b6 A full treatment of color choice within visualization is beyond the scope of this book, but for entertaining reading on this subject and others, Broadly, you should be aware of three different categories of colormaps: Sequential colormaps These consist of one continuous sequence of colors (e.g., binary or viridis). Divergent colormaps These usually contain two distinct colors, which show positive and negative deviations from a mean (e.g., RdBu or PuOr). Qualitative colormaps These mix colors with no particular sequence (e.g., rainbow or jet). The jet colormap, which was the default in Matplotlib prior to version 2.0, is an example of a qualitative colormap. Its status as the default was quite unfortunate, because qualitative maps are often a poor choice for representing quantitative data. Among the problems is the fact that qualitative maps usually do not display any uni\u2010 form progression in brightness as the scale increases. from matplotlib.colors import LinearSegmentedColormap def grayscale_cmap ( cmap ): \"\"\"Return a grayscale version of the given colormap\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) # convert RGBA to perceived grayscale luminance # cf. http://alienryderflex.com/hsp.html RGB_weight = [ 0.299 , 0.587 , 0.114 ] luminance = np . sqrt ( np . dot ( colors [:, : 3 ] ** 2 , RGB_weight )) colors [:, : 3 ] = luminance [:, np . newaxis ] return LinearSegmentedColormap . from_list ( cmap . name + \"_gray\" , colors , cmap . N ) def view_colormap ( cmap ): \"\"\"Plot a colormap with its grayscale equivalent\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) cmap = grayscale_cmap ( cmap ) grayscale = cmap ( np . arange ( cmap . N )) fig , ax = plt . subplots ( 2 , figsize = ( 6 , 2 ), subplot_kw = dict ( xticks = [], yticks = [])) ax [ 0 ] . imshow ([ colors ], extent = [ 0 , 10 , 0 , 1 ]) ax [ 1 ] . imshow ([ grayscale ], extent = [ 0 , 10 , 0 , 1 ]) view_colormap ( 'jet' ) Notice the bright stripes in the grayscale image. Even in full color, this uneven bright\u2010 ness means that the eye will be drawn to certain portions of the color range, which will potentially emphasize unimportant parts of the dataset. It\u2019s better to use a color\u2010 map such as viridis (the default as of Matplotlib 2.0), which is specifically construc\u2010 ted to have an even brightness variation across the range. Thus, it not only plays well with our color perception, but also will translate well to grayscale printing view_colormap ( 'viridis' ) view_colormap ( 'cubehelix' ) view_colormap ( 'RdBu' ) Color limits and extensions \u00b6 Matplotlib allows for a large range of colorbar customization. The colorbar itself is simply an instance of plt.Axes, so all of the axes and tick formatting tricks we\u2019ve learned are applicable. The colorbar has some interesting flexibility; for example, we can narrow the color limits and indicate the out-of-bounds values with a triangular arrow at the top and bottom by setting the extend property. This might come in handy, for example, if you\u2019re displaying an image that is subject to noise # make noise in 1% of the image pixel speckles = ( np . random . random ( I . shape ) < 0.01 ) I [ speckles ] = np . random . normal ( 0 , 3 , np . count_nonzero ( speckles )) plt . figure ( figsize = ( 10 , 3.5 )) plt . subplot ( 1 , 2 , 1 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar () plt . subplot ( 1 , 2 , 2 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar ( extend = 'both' ) plt . clim ( - 1 , 1 ) Discrete colorbars \u00b6 Colormaps are by default continuous, but sometimes you\u2019d like to represent discrete values. The easiest way to do this is to use the plt.cm.get_cmap() function, and pass the name of a suitable colormap along with the number of desired bins plt . imshow ( I , cmap = plt . cm . get_cmap ( 'Blues' , 6 )) plt . colorbar () plt . clim ( - 1 , 1 );","title":"08customizing colorbar"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#08-customizing-colorbars","text":"Customizing Colorbars Customizing Colorbars Choosing the colormap Color limits and extensions Discrete colorbars","title":"08 -  Customizing Colorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#customizing-colorbars","text":"Plot legends identify discrete labels of discrete points. For continuous labels based on the color of points, lines, or regions, a labeled colorbar can be a great tool. In Mat\u2010 plotlib, a colorbar is a separate axes that can provide a key for the meaning of colors in a plot. import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np x = np . linspace ( 0 , 10 , 1000 ) I = np . sin ( x ) * np . cos ( x [:, np . newaxis ]) np . cos ( x [:, np . newaxis ]) array([[ 1.00000000e+00], [ 9.99949900e-01], [ 9.99799606e-01], [ 9.99549133e-01], [ 9.99198505e-01], [ 9.98747758e-01], [ 9.98196937e-01], [ 9.97546097e-01], [ 9.96795304e-01], [ 9.95944632e-01], [ 9.94994167e-01], [ 9.93944004e-01], [ 9.92794249e-01], [ 9.91545016e-01], [ 9.90196431e-01], [ 9.88748629e-01], [ 9.87201754e-01], [ 9.85555963e-01], [ 9.83811419e-01], [ 9.81968298e-01], [ 9.80026785e-01], [ 9.77987073e-01], [ 9.75849367e-01], [ 9.73613882e-01], [ 9.71280841e-01], [ 9.68850478e-01], [ 9.66323038e-01], [ 9.63698772e-01], [ 9.60977944e-01], [ 9.58160826e-01], [ 9.55247701e-01], [ 9.52238861e-01], [ 9.49134607e-01], [ 9.45935251e-01], [ 9.42641112e-01], [ 9.39252521e-01], [ 9.35769817e-01], [ 9.32193350e-01], [ 9.28523478e-01], [ 9.24760567e-01], [ 9.20904997e-01], [ 9.16957152e-01], [ 9.12917429e-01], [ 9.08786232e-01], [ 9.04563975e-01], [ 9.00251081e-01], [ 8.95847982e-01], [ 8.91355120e-01], [ 8.86772945e-01], [ 8.82101915e-01], [ 8.77342499e-01], [ 8.72495174e-01], [ 8.67560426e-01], [ 8.62538748e-01], [ 8.57430645e-01], [ 8.52236627e-01], [ 8.46957216e-01], [ 8.41592940e-01], [ 8.36144337e-01], [ 8.30611953e-01], [ 8.24996341e-01], [ 8.19298066e-01], [ 8.13517698e-01], [ 8.07655815e-01], [ 8.01713006e-01], [ 7.95689865e-01], [ 7.89586997e-01], [ 7.83405013e-01], [ 7.77144531e-01], [ 7.70806181e-01], [ 7.64390596e-01], [ 7.57898419e-01], [ 7.51330302e-01], [ 7.44686901e-01], [ 7.37968884e-01], [ 7.31176922e-01], [ 7.24311697e-01], [ 7.17373896e-01], [ 7.10364214e-01], [ 7.03283355e-01], [ 6.96132027e-01], [ 6.88910947e-01], [ 6.81620838e-01], [ 6.74262431e-01], [ 6.66836464e-01], [ 6.59343679e-01], [ 6.51784829e-01], [ 6.44160671e-01], [ 6.36471968e-01], [ 6.28719491e-01], [ 6.20904016e-01], [ 6.13026327e-01], [ 6.05087214e-01], [ 5.97087471e-01], [ 5.89027900e-01], [ 5.80909308e-01], [ 5.72732510e-01], [ 5.64498325e-01], [ 5.56207577e-01], [ 5.47861097e-01], [ 5.39459722e-01], [ 5.31004293e-01], [ 5.22495658e-01], [ 5.13934670e-01], [ 5.05322185e-01], [ 4.96659067e-01], [ 4.87946184e-01], [ 4.79184410e-01], [ 4.70374621e-01], [ 4.61517701e-01], [ 4.52614537e-01], [ 4.43666022e-01], [ 4.34673051e-01], [ 4.25636526e-01], [ 4.16557353e-01], [ 4.07436441e-01], [ 3.98274704e-01], [ 3.89073060e-01], [ 3.79832432e-01], [ 3.70553744e-01], [ 3.61237927e-01], [ 3.51885914e-01], [ 3.42498642e-01], [ 3.33077052e-01], [ 3.23622088e-01], [ 3.14134698e-01], [ 3.04615831e-01], [ 2.95066442e-01], [ 2.85487487e-01], [ 2.75879926e-01], [ 2.66244723e-01], [ 2.56582842e-01], [ 2.46895251e-01], [ 2.37182922e-01], [ 2.27446827e-01], [ 2.17687942e-01], [ 2.07907245e-01], [ 1.98105716e-01], [ 1.88284337e-01], [ 1.78444091e-01], [ 1.68585966e-01], [ 1.58710948e-01], [ 1.48820028e-01], [ 1.38914196e-01], [ 1.28994445e-01], [ 1.19061768e-01], [ 1.09117162e-01], [ 9.91616222e-02], [ 8.91961465e-02], [ 7.92217334e-02], [ 6.92393823e-02], [ 5.92500934e-02], [ 4.92548678e-02], [ 3.92547068e-02], [ 2.92506125e-02], [ 1.92435873e-02], [ 9.23463398e-03], [-7.75244699e-04], [-1.07850457e-02], [-2.07937660e-02], [-3.08004029e-02], [-4.08039535e-02], [-5.08034156e-02], [-6.07977872e-02], [-7.07860669e-02], [-8.07672539e-02], [-9.07403481e-02], [-1.00704350e-01], [-1.10658262e-01], [-1.20601085e-01], [-1.30531825e-01], [-1.40449485e-01], [-1.50353072e-01], [-1.60241594e-01], [-1.70114060e-01], [-1.79969480e-01], [-1.89806868e-01], [-1.99625237e-01], [-2.09423604e-01], [-2.19200987e-01], [-2.28956405e-01], [-2.38688883e-01], [-2.48397444e-01], [-2.58081116e-01], [-2.67738928e-01], [-2.77369913e-01], [-2.86973105e-01], [-2.96547543e-01], [-3.06092267e-01], [-3.15606321e-01], [-3.25088751e-01], [-3.34538607e-01], [-3.43954943e-01], [-3.53336815e-01], [-3.62683283e-01], [-3.71993410e-01], [-3.81266263e-01], [-3.90500914e-01], [-3.99696437e-01], [-4.08851910e-01], [-4.17966417e-01], [-4.27039043e-01], [-4.36068881e-01], [-4.45055025e-01], [-4.53996574e-01], [-4.62892633e-01], [-4.71742311e-01], [-4.80544720e-01], [-4.89298979e-01], [-4.98004210e-01], [-5.06659542e-01], [-5.15264107e-01], [-5.23817042e-01], [-5.32317492e-01], [-5.40764603e-01], [-5.49157530e-01], [-5.57495432e-01], [-5.65777473e-01], [-5.74002823e-01], [-5.82170659e-01], [-5.90280161e-01], [-5.98330518e-01], [-6.06320922e-01], [-6.14250574e-01], [-6.22118677e-01], [-6.29924445e-01], [-6.37667095e-01], [-6.45345851e-01], [-6.52959943e-01], [-6.60508609e-01], [-6.67991093e-01], [-6.75406644e-01], [-6.82754520e-01], [-6.90033984e-01], [-6.97244307e-01], [-7.04384767e-01], [-7.11454648e-01], [-7.18453241e-01], [-7.25379846e-01], [-7.32233768e-01], [-7.39014320e-01], [-7.45720824e-01], [-7.52352607e-01], [-7.58909005e-01], [-7.65389360e-01], [-7.71793024e-01], [-7.78119354e-01], [-7.84367718e-01], [-7.90537488e-01], [-7.96628046e-01], [-8.02638783e-01], [-8.08569096e-01], [-8.14418391e-01], [-8.20186082e-01], [-8.25871590e-01], [-8.31474347e-01], [-8.36993790e-01], [-8.42429367e-01], [-8.47780532e-01], [-8.53046751e-01], [-8.58227495e-01], [-8.63322245e-01], [-8.68330490e-01], [-8.73251730e-01], [-8.78085470e-01], [-8.82831226e-01], [-8.87488523e-01], [-8.92056894e-01], [-8.96535881e-01], [-9.00925037e-01], [-9.05223919e-01], [-9.09432099e-01], [-9.13549155e-01], [-9.17574673e-01], [-9.21508251e-01], [-9.25349494e-01], [-9.29098017e-01], [-9.32753446e-01], [-9.36315413e-01], [-9.39783561e-01], [-9.43157544e-01], [-9.46437023e-01], [-9.49621670e-01], [-9.52711165e-01], [-9.55705199e-01], [-9.58603471e-01], [-9.61405692e-01], [-9.64111581e-01], [-9.66720867e-01], [-9.69233287e-01], [-9.71648591e-01], [-9.73966536e-01], [-9.76186890e-01], [-9.78309431e-01], [-9.80333945e-01], [-9.82260231e-01], [-9.84088095e-01], [-9.85817354e-01], [-9.87447834e-01], [-9.88979373e-01], [-9.90411816e-01], [-9.91745021e-01], [-9.92978853e-01], [-9.94113189e-01], [-9.95147916e-01], [-9.96082929e-01], [-9.96918136e-01], [-9.97653452e-01], [-9.98288803e-01], [-9.98824127e-01], [-9.99259369e-01], [-9.99594485e-01], [-9.99829443e-01], [-9.99964218e-01], [-9.99998798e-01], [-9.99933178e-01], [-9.99767366e-01], [-9.99501377e-01], [-9.99135239e-01], [-9.98668988e-01], [-9.98102670e-01], [-9.97436344e-01], [-9.96670075e-01], [-9.95803940e-01], [-9.94838026e-01], [-9.93772430e-01], [-9.92607258e-01], [-9.91342628e-01], [-9.89978665e-01], [-9.88515508e-01], [-9.86953301e-01], [-9.85292203e-01], [-9.83532378e-01], [-9.81674005e-01], [-9.79717268e-01], [-9.77662364e-01], [-9.75509498e-01], [-9.73258887e-01], [-9.70910757e-01], [-9.68465341e-01], [-9.65922886e-01], [-9.63283645e-01], [-9.60547884e-01], [-9.57715877e-01], [-9.54787907e-01], [-9.51764268e-01], [-9.48645263e-01], [-9.45431204e-01], [-9.42122413e-01], [-9.38719222e-01], [-9.35221972e-01], [-9.31631013e-01], [-9.27946706e-01], [-9.24169418e-01], [-9.20299529e-01], [-9.16337427e-01], [-9.12283508e-01], [-9.08138179e-01], [-9.03901855e-01], [-8.99574960e-01], [-8.95157929e-01], [-8.90651203e-01], [-8.86055234e-01], [-8.81370484e-01], [-8.76597420e-01], [-8.71736521e-01], [-8.66788276e-01], [-8.61753178e-01], [-8.56631733e-01], [-8.51424454e-01], [-8.46131863e-01], [-8.40754490e-01], [-8.35292874e-01], [-8.29747562e-01], [-8.24119109e-01], [-8.18408081e-01], [-8.12615048e-01], [-8.06740592e-01], [-8.00785301e-01], [-7.94749771e-01], [-7.88634608e-01], [-7.82440424e-01], [-7.76167840e-01], [-7.69817485e-01], [-7.63389994e-01], [-7.56886012e-01], [-7.50306190e-01], [-7.43651188e-01], [-7.36921673e-01], [-7.30118318e-01], [-7.23241806e-01], [-7.16292826e-01], [-7.09272073e-01], [-7.02180252e-01], [-6.95018073e-01], [-6.87786253e-01], [-6.80485517e-01], [-6.73116597e-01], [-6.65680231e-01], [-6.58177165e-01], [-6.50608149e-01], [-6.42973943e-01], [-6.35275311e-01], [-6.27513025e-01], [-6.19687862e-01], [-6.11800607e-01], [-6.03852050e-01], [-5.95842988e-01], [-5.87774222e-01], [-5.79646561e-01], [-5.71460820e-01], [-5.63217820e-01], [-5.54918385e-01], [-5.46563347e-01], [-5.38153544e-01], [-5.29689819e-01], [-5.21173018e-01], [-5.12603997e-01], [-5.03983613e-01], [-4.95312730e-01], [-4.86592217e-01], [-4.77822948e-01], [-4.69005801e-01], [-4.60141660e-01], [-4.51231412e-01], [-4.42275952e-01], [-4.33276176e-01], [-4.24232986e-01], [-4.15147288e-01], [-4.06019993e-01], [-3.96852014e-01], [-3.87644272e-01], [-3.78397687e-01], [-3.69113187e-01], [-3.59791703e-01], [-3.50434167e-01], [-3.41041518e-01], [-3.31614697e-01], [-3.22154648e-01], [-3.12662319e-01], [-3.03138662e-01], [-2.93584631e-01], [-2.84001182e-01], [-2.74389277e-01], [-2.64749878e-01], [-2.55083951e-01], [-2.45392466e-01], [-2.35676391e-01], [-2.25936703e-01], [-2.16174375e-01], [-2.06390387e-01], [-1.96585719e-01], [-1.86761353e-01], [-1.76918273e-01], [-1.67057466e-01], [-1.57179921e-01], [-1.47286626e-01], [-1.37378573e-01], [-1.27456754e-01], [-1.17522165e-01], [-1.07575800e-01], [-9.76186559e-02], [-8.76517305e-02], [-7.76760224e-02], [-6.76925312e-02], [-5.77022572e-02], [-4.77062016e-02], [-3.77053657e-02], [-2.77007519e-02], [-1.76933624e-02], [-7.68420006e-03], [ 2.32573223e-03], [ 1.23354315e-02], [ 2.23438947e-02], [ 3.23501191e-02], [ 4.23531021e-02], [ 5.23518413e-02], [ 6.23453348e-02], [ 7.23325814e-02], [ 8.23125803e-02], [ 9.22843315e-02], [ 1.02246836e-01], [ 1.12199095e-01], [ 1.22140112e-01], [ 1.32068891e-01], [ 1.41984436e-01], [ 1.51885755e-01], [ 1.61771855e-01], [ 1.71641745e-01], [ 1.81494437e-01], [ 1.91328943e-01], [ 2.01144278e-01], [ 2.10939459e-01], [ 2.20713504e-01], [ 2.30465433e-01], [ 2.40194270e-01], [ 2.49899039e-01], [ 2.59578769e-01], [ 2.69232489e-01], [ 2.78859232e-01], [ 2.88458033e-01], [ 2.98027932e-01], [ 3.07567967e-01], [ 3.17077185e-01], [ 3.26554632e-01], [ 3.35999358e-01], [ 3.45410418e-01], [ 3.54786867e-01], [ 3.64127767e-01], [ 3.73432181e-01], [ 3.82699178e-01], [ 3.91927829e-01], [ 4.01117208e-01], [ 4.10266396e-01], [ 4.19374475e-01], [ 4.28440534e-01], [ 4.37463662e-01], [ 4.46442957e-01], [ 4.55377519e-01], [ 4.64266452e-01], [ 4.73108866e-01], [ 4.81903875e-01], [ 4.90650597e-01], [ 4.99348156e-01], [ 5.07995681e-01], [ 5.16592305e-01], [ 5.25137167e-01], [ 5.33629410e-01], [ 5.42068184e-01], [ 5.50452643e-01], [ 5.58781947e-01], [ 5.67055261e-01], [ 5.75271756e-01], [ 5.83430610e-01], [ 5.91531004e-01], [ 5.99572127e-01], [ 6.07553173e-01], [ 6.15473343e-01], [ 6.23331843e-01], [ 6.31127885e-01], [ 6.38860689e-01], [ 6.46529479e-01], [ 6.54133487e-01], [ 6.61671951e-01], [ 6.69144116e-01], [ 6.76549233e-01], [ 6.83886561e-01], [ 6.91155363e-01], [ 6.98354912e-01], [ 7.05484486e-01], [ 7.12543371e-01], [ 7.19530859e-01], [ 7.26446251e-01], [ 7.33288853e-01], [ 7.40057981e-01], [ 7.46752954e-01], [ 7.53373104e-01], [ 7.59917766e-01], [ 7.66386284e-01], [ 7.72778011e-01], [ 7.79092307e-01], [ 7.85328537e-01], [ 7.91486078e-01], [ 7.97564313e-01], [ 8.03562632e-01], [ 8.09480434e-01], [ 8.15317127e-01], [ 8.21072126e-01], [ 8.26744853e-01], [ 8.32334742e-01], [ 8.37841230e-01], [ 8.43263768e-01], [ 8.48601811e-01], [ 8.53854824e-01], [ 8.59022282e-01], [ 8.64103666e-01], [ 8.69098468e-01], [ 8.74006186e-01], [ 8.78826329e-01], [ 8.83558414e-01], [ 8.88201968e-01], [ 8.92756523e-01], [ 8.97221626e-01], [ 9.01596827e-01], [ 9.05881688e-01], [ 9.10075781e-01], [ 9.14178684e-01], [ 9.18189988e-01], [ 9.22109289e-01], [ 9.25936195e-01], [ 9.29670323e-01], [ 9.33311299e-01], [ 9.36858757e-01], [ 9.40312343e-01], [ 9.43671709e-01], [ 9.46936521e-01], [ 9.50106449e-01], [ 9.53181178e-01], [ 9.56160398e-01], [ 9.59043812e-01], [ 9.61831130e-01], [ 9.64522073e-01], [ 9.67116371e-01], [ 9.69613765e-01], [ 9.72014004e-01], [ 9.74316848e-01], [ 9.76522066e-01], [ 9.78629437e-01], [ 9.80638749e-01], [ 9.82549803e-01], [ 9.84362405e-01], [ 9.86076374e-01], [ 9.87691540e-01], [ 9.89207739e-01], [ 9.90624820e-01], [ 9.91942641e-01], [ 9.93161070e-01], [ 9.94279984e-01], [ 9.95299273e-01], [ 9.96218833e-01], [ 9.97038572e-01], [ 9.97758408e-01], [ 9.98378270e-01], [ 9.98898095e-01], [ 9.99317830e-01], [ 9.99637435e-01], [ 9.99856876e-01], [ 9.99976133e-01], [ 9.99995192e-01], [ 9.99914052e-01], [ 9.99732722e-01], [ 9.99451218e-01], [ 9.99069571e-01], [ 9.98587817e-01], [ 9.98006005e-01], [ 9.97324193e-01], [ 9.96542450e-01], [ 9.95660854e-01], [ 9.94679493e-01], [ 9.93598466e-01], [ 9.92417881e-01], [ 9.91137856e-01], [ 9.89758520e-01], [ 9.88280011e-01], [ 9.86702476e-01], [ 9.85026074e-01], [ 9.83250973e-01], [ 9.81377351e-01], [ 9.79405396e-01], [ 9.77335304e-01], [ 9.75167284e-01], [ 9.72901553e-01], [ 9.70538338e-01], [ 9.68077875e-01], [ 9.65520411e-01], [ 9.62866203e-01], [ 9.60115516e-01], [ 9.57268626e-01], [ 9.54325818e-01], [ 9.51287388e-01], [ 9.48153638e-01], [ 9.44924885e-01], [ 9.41601450e-01], [ 9.38183667e-01], [ 9.34671879e-01], [ 9.31066437e-01], [ 9.27367703e-01], [ 9.23576047e-01], [ 9.19691849e-01], [ 9.15715499e-01], [ 9.11647395e-01], [ 9.07487943e-01], [ 9.03237562e-01], [ 8.98896678e-01], [ 8.94465724e-01], [ 8.89945145e-01], [ 8.85335394e-01], [ 8.80636933e-01], [ 8.75850233e-01], [ 8.70975773e-01], [ 8.66014042e-01], [ 8.60965536e-01], [ 8.55830762e-01], [ 8.50610235e-01], [ 8.45304477e-01], [ 8.39914019e-01], [ 8.34439403e-01], [ 8.28881176e-01], [ 8.23239896e-01], [ 8.17516128e-01], [ 8.11710445e-01], [ 8.05823429e-01], [ 7.99855670e-01], [ 7.93807766e-01], [ 7.87680323e-01], [ 7.81473955e-01], [ 7.75189283e-01], [ 7.68826938e-01], [ 7.62387557e-01], [ 7.55871785e-01], [ 7.49280275e-01], [ 7.42613687e-01], [ 7.35872690e-01], [ 7.29057959e-01], [ 7.22170177e-01], [ 7.15210034e-01], [ 7.08178227e-01], [ 7.01075461e-01], [ 6.93902448e-01], [ 6.86659906e-01], [ 6.79348561e-01], [ 6.71969145e-01], [ 6.64522399e-01], [ 6.57009068e-01], [ 6.49429905e-01], [ 6.41785669e-01], [ 6.34077127e-01], [ 6.26305051e-01], [ 6.18470219e-01], [ 6.10573417e-01], [ 6.02615435e-01], [ 5.94597072e-01], [ 5.86519131e-01], [ 5.78382421e-01], [ 5.70187757e-01], [ 5.61935960e-01], [ 5.53627858e-01], [ 5.45264283e-01], [ 5.36846073e-01], [ 5.28374071e-01], [ 5.19849126e-01], [ 5.11272092e-01], [ 5.02643829e-01], [ 4.93965202e-01], [ 4.85237080e-01], [ 4.76460337e-01], [ 4.67635853e-01], [ 4.58764512e-01], [ 4.49847203e-01], [ 4.40884820e-01], [ 4.31878260e-01], [ 4.22828426e-01], [ 4.13736226e-01], [ 4.04602569e-01], [ 3.95428371e-01], [ 3.86214551e-01], [ 3.76962033e-01], [ 3.67671743e-01], [ 3.58344613e-01], [ 3.48981577e-01], [ 3.39583574e-01], [ 3.30151544e-01], [ 3.20686433e-01], [ 3.11189189e-01], [ 3.01660765e-01], [ 2.92102114e-01], [ 2.82514195e-01], [ 2.72897968e-01], [ 2.63254397e-01], [ 2.53584448e-01], [ 2.43889090e-01], [ 2.34169294e-01], [ 2.24426035e-01], [ 2.14660288e-01], [ 2.04873032e-01], [ 1.95065249e-01], [ 1.85237919e-01], [ 1.75392030e-01], [ 1.65528565e-01], [ 1.55648516e-01], [ 1.45752870e-01], [ 1.35842619e-01], [ 1.25918758e-01], [ 1.15982279e-01], [ 1.06034179e-01], [ 9.60754549e-02], [ 8.61071037e-02], [ 7.61301246e-02], [ 6.61455173e-02], [ 5.61542823e-02], [ 4.61574206e-02], [ 3.61559340e-02], [ 2.61508246e-02], [ 1.61430949e-02], [ 6.13374766e-03], [-3.87621418e-03], [-1.38857876e-02], [-2.38939697e-02], [-3.38997577e-02], [-4.39021489e-02], [-5.39001411e-02], [-6.38927325e-02], [-7.38789220e-02], [-8.38577088e-02], [-9.38280931e-02], [-1.03789076e-01], [-1.13739659e-01], [-1.23678846e-01], [-1.33605640e-01], [-1.43519046e-01], [-1.53418073e-01], [-1.63301726e-01], [-1.73169017e-01], [-1.83018957e-01], [-1.92850558e-01], [-2.02662836e-01], [-2.12454807e-01], [-2.22225490e-01], [-2.31973906e-01], [-2.41699079e-01], [-2.51400034e-01], [-2.61075798e-01], [-2.70725403e-01], [-2.80347881e-01], [-2.89942268e-01], [-2.99507604e-01], [-3.09042928e-01], [-3.18547287e-01], [-3.28019728e-01], [-3.37459301e-01], [-3.46865061e-01], [-3.56236066e-01], [-3.65571375e-01], [-3.74870055e-01], [-3.84131173e-01], [-3.93353801e-01], [-4.02537015e-01], [-4.11679896e-01], [-4.20781526e-01], [-4.29840994e-01], [-4.38857392e-01], [-4.47829817e-01], [-4.56757370e-01], [-4.65639155e-01], [-4.74474285e-01], [-4.83261871e-01], [-4.92001036e-01], [-5.00690902e-01], [-5.09330599e-01], [-5.17919262e-01], [-5.26456029e-01], [-5.34940046e-01], [-5.43370462e-01], [-5.51746432e-01], [-5.60067118e-01], [-5.68331685e-01], [-5.76539306e-01], [-5.84689158e-01], [-5.92780425e-01], [-6.00812295e-01], [-6.08783964e-01], [-6.16694633e-01], [-6.24543510e-01], [-6.32329808e-01], [-6.40052747e-01], [-6.47711552e-01], [-6.55305458e-01], [-6.62833702e-01], [-6.70295531e-01], [-6.77690196e-01], [-6.85016957e-01], [-6.92275080e-01], [-6.99463837e-01], [-7.06582509e-01], [-7.13630381e-01], [-7.20606748e-01], [-7.27510910e-01], [-7.34342176e-01], [-7.41099862e-01], [-7.47783289e-01], [-7.54391789e-01], [-7.60924700e-01], [-7.67381366e-01], [-7.73761141e-01], [-7.80063386e-01], [-7.86287469e-01], [-7.92432766e-01], [-7.98498661e-01], [-8.04484548e-01], [-8.10389826e-01], [-8.16213903e-01], [-8.21956196e-01], [-8.27616129e-01], [-8.33193136e-01], [-8.38686657e-01], [-8.44096142e-01], [-8.49421049e-01], [-8.54660845e-01], [-8.59815004e-01], [-8.64883010e-01], [-8.69864355e-01], [-8.74758541e-01], [-8.79565076e-01], [-8.84283479e-01], [-8.88913277e-01], [-8.93454007e-01], [-8.97905213e-01], [-9.02266449e-01], [-9.06537279e-01], [-9.10717274e-01], [-9.14806016e-01], [-9.18803095e-01], [-9.22708110e-01], [-9.26520671e-01], [-9.30240394e-01], [-9.33866908e-01], [-9.37399849e-01], [-9.40838863e-01], [-9.44183606e-01], [-9.47433742e-01], [-9.50588945e-01], [-9.53648900e-01], [-9.56613300e-01], [-9.59481847e-01], [-9.62254255e-01], [-9.64930246e-01], [-9.67509551e-01], [-9.69991913e-01], [-9.72377081e-01], [-9.74664818e-01], [-9.76854895e-01], [-9.78947090e-01], [-9.80941196e-01], [-9.82837012e-01], [-9.84634348e-01], [-9.86333025e-01], [-9.87932871e-01], [-9.89433727e-01], [-9.90835442e-01], [-9.92137877e-01], [-9.93340899e-01], [-9.94444389e-01], [-9.95448237e-01], [-9.96352341e-01], [-9.97156611e-01], [-9.97860966e-01], [-9.98465337e-01], [-9.98969661e-01], [-9.99373890e-01], [-9.99677982e-01], [-9.99881906e-01], [-9.99985643e-01], [-9.99989182e-01], [-9.99892522e-01], [-9.99695674e-01], [-9.99398657e-01], [-9.99001501e-01], [-9.98504245e-01], [-9.97906940e-01], [-9.97209644e-01], [-9.96412429e-01], [-9.95515375e-01], [-9.94518569e-01], [-9.93422114e-01], [-9.92226118e-01], [-9.90930702e-01], [-9.89535995e-01], [-9.88042137e-01], [-9.86449278e-01], [-9.84757577e-01], [-9.82967204e-01], [-9.81078338e-01], [-9.79091169e-01], [-9.77005895e-01], [-9.74822726e-01], [-9.72541880e-01], [-9.70163586e-01], [-9.67688082e-01], [-9.65115616e-01], [-9.62446446e-01], [-9.59680840e-01], [-9.56819074e-01], [-9.53861435e-01], [-9.50808220e-01], [-9.47659734e-01], [-9.44416293e-01], [-9.41078223e-01], [-9.37645857e-01], [-9.34119539e-01], [-9.30499623e-01], [-9.26786471e-01], [-9.22980456e-01], [-9.19081959e-01], [-9.15091370e-01], [-9.11009089e-01], [-9.06835526e-01], [-9.02571099e-01], [-8.98216234e-01], [-8.93771368e-01], [-8.89236948e-01], [-8.84613426e-01], [-8.79901266e-01], [-8.75100941e-01], [-8.70212930e-01], [-8.65237726e-01], [-8.60175824e-01], [-8.55027734e-01], [-8.49793970e-01], [-8.44475058e-01], [-8.39071529e-01]]) plt . imshow ( I ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f5d4c0728b0>","title":"Customizing Colorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#customizing-colorbars_1","text":"We can specify the colormap using the cmap argument to the plotting function that is creating the visualization plt . imshow ( I , cmap = 'gray' ) <matplotlib.image.AxesImage at 0x7f5d3a3e2490> All the available colormaps are in the plt.cm namespace; using IPython\u2019s tab- completion feature will give you a full list of built-in possibilities: #plt.imshow(I, cmap='winter_r') But being able to choose a colormap is just the first step: more important is how to decide among the possibilities! The choice turns out to be much more subtle than you might initially expect.","title":"Customizing Colorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#choosing-the-colormap","text":"A full treatment of color choice within visualization is beyond the scope of this book, but for entertaining reading on this subject and others, Broadly, you should be aware of three different categories of colormaps: Sequential colormaps These consist of one continuous sequence of colors (e.g., binary or viridis). Divergent colormaps These usually contain two distinct colors, which show positive and negative deviations from a mean (e.g., RdBu or PuOr). Qualitative colormaps These mix colors with no particular sequence (e.g., rainbow or jet). The jet colormap, which was the default in Matplotlib prior to version 2.0, is an example of a qualitative colormap. Its status as the default was quite unfortunate, because qualitative maps are often a poor choice for representing quantitative data. Among the problems is the fact that qualitative maps usually do not display any uni\u2010 form progression in brightness as the scale increases. from matplotlib.colors import LinearSegmentedColormap def grayscale_cmap ( cmap ): \"\"\"Return a grayscale version of the given colormap\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) # convert RGBA to perceived grayscale luminance # cf. http://alienryderflex.com/hsp.html RGB_weight = [ 0.299 , 0.587 , 0.114 ] luminance = np . sqrt ( np . dot ( colors [:, : 3 ] ** 2 , RGB_weight )) colors [:, : 3 ] = luminance [:, np . newaxis ] return LinearSegmentedColormap . from_list ( cmap . name + \"_gray\" , colors , cmap . N ) def view_colormap ( cmap ): \"\"\"Plot a colormap with its grayscale equivalent\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) cmap = grayscale_cmap ( cmap ) grayscale = cmap ( np . arange ( cmap . N )) fig , ax = plt . subplots ( 2 , figsize = ( 6 , 2 ), subplot_kw = dict ( xticks = [], yticks = [])) ax [ 0 ] . imshow ([ colors ], extent = [ 0 , 10 , 0 , 1 ]) ax [ 1 ] . imshow ([ grayscale ], extent = [ 0 , 10 , 0 , 1 ]) view_colormap ( 'jet' ) Notice the bright stripes in the grayscale image. Even in full color, this uneven bright\u2010 ness means that the eye will be drawn to certain portions of the color range, which will potentially emphasize unimportant parts of the dataset. It\u2019s better to use a color\u2010 map such as viridis (the default as of Matplotlib 2.0), which is specifically construc\u2010 ted to have an even brightness variation across the range. Thus, it not only plays well with our color perception, but also will translate well to grayscale printing view_colormap ( 'viridis' ) view_colormap ( 'cubehelix' ) view_colormap ( 'RdBu' )","title":"Choosing the colormap"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#color-limits-and-extensions","text":"Matplotlib allows for a large range of colorbar customization. The colorbar itself is simply an instance of plt.Axes, so all of the axes and tick formatting tricks we\u2019ve learned are applicable. The colorbar has some interesting flexibility; for example, we can narrow the color limits and indicate the out-of-bounds values with a triangular arrow at the top and bottom by setting the extend property. This might come in handy, for example, if you\u2019re displaying an image that is subject to noise # make noise in 1% of the image pixel speckles = ( np . random . random ( I . shape ) < 0.01 ) I [ speckles ] = np . random . normal ( 0 , 3 , np . count_nonzero ( speckles )) plt . figure ( figsize = ( 10 , 3.5 )) plt . subplot ( 1 , 2 , 1 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar () plt . subplot ( 1 , 2 , 2 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar ( extend = 'both' ) plt . clim ( - 1 , 1 )","title":"Color limits and extensions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/#discrete-colorbars","text":"Colormaps are by default continuous, but sometimes you\u2019d like to represent discrete values. The easiest way to do this is to use the plt.cm.get_cmap() function, and pass the name of a suitable colormap along with the number of desired bins plt . imshow ( I , cmap = plt . cm . get_cmap ( 'Blues' , 6 )) plt . colorbar () plt . clim ( - 1 , 1 );","title":"Discrete colorbars"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 09 - Multiple Subplots \u00b6 Multiple Subplots plt.axes: Subplots by Hand plt.subplot: Simple Grids of Subplots plt.subplots: The Whole Grid in One Go plt.GridSpec: More Complicated Arrangements Multiple Subplots \u00b6 Sometimes it is helpful to compare different views of data side by side. To this end, Matplotlib has the concept of subplots: groups of smaller axes that can exist together within a single figure. These subplots might be insets, grids of plots, or other more complicated layouts. In this section, we\u2019ll explore four routines for creating subplots in Matplotlib. We\u2019ll start by setting up the notebook for plotting and importing the functions we will use: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np plt.axes: Subplots by Hand \u00b6 The most basic method of creating an axes is to use the plt.axes function. As we\u2019ve seen previously, by default this creates a standard axes object that fills the entire fig\u2010 ure. plt.axes also takes an optional argument that is a list of four numbers in the figure coordinate system. These numbers represent [bottom, left, width, height] in the figure coordinate system, which ranges from 0 at the bottom left of the figure to 1 at the top right of the figure. ax1 = plt . axes () ax2 = plt . axes ([ 0.65 , 0.65 , 0.2 , 0.2 ]) The equivalent of this command within the object-oriented interface is fig.add_axes(). Let\u2019s use this to create two vertically stacked axes fig = plt . figure () ax1 = fig . add_axes ([ 0.1 , 0.5 , 0.8 , 0.4 ], xticklabels = [], ylim = ( - 1.2 , 1.2 )) ax2 = fig . add_axes ([ 0.1 , 0.1 , 0.8 , 0.4 ], xticklabels = [], ylim = ( - 1.2 , 1.2 )) x = np . linspace ( - 0 , 10 ) ax1 . plot ( np . sin ( x )) ax2 . plot ( np . cos ( x )); plt.subplot: Simple Grids of Subplots \u00b6 Aligned columns or rows of subplots are a common enough need that Matplotlib has several convenience routines that make them easy to create. The lowest level of these is plt.subplot(), which creates a single subplot within a grid. As you can see, this command takes three integer arguments\u2014the number of rows, the number of col\u2010 umns, and the index of the plot to be created in this scheme, which runs from the upper left to the bottom right for i in range ( 1 , 7 ): plt . subplot ( 2 , 3 , i ) plt . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) The command plt.subplots_adjust can be used to adjust the spacing between these plots. object-oriented command, fig.add_subplot(): fig = plt . figure () fig . subplots_adjust ( hspace = 0.4 , wspace = 0.4 ) for i in range ( 1 , 7 ): ax = fig . add_subplot ( 2 , 3 , i ) ax . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) plt.subplots: The Whole Grid in One Go \u00b6 The approach just described can become quite tedious when you\u2019re creating a large grid of subplots, especially if you\u2019d like to hide the x- and y-axis labels on the inner plots. For this purpose, plt.subplots() is the easier tool to use (note the s at the end of subplots). Rather than creating a single subplot, this function creates a full grid of subplots in a single line, returning them in a NumPy array. The arguments are the number of rows and number of columns, along with optional keywords sharex and sharey, which allow you to specify the relationships between different axes. fig , ax = plt . subplots ( 2 , 3 , sharex = 'col' , sharey = 'row' ) for i in range ( 2 ): for j in range ( 3 ): ax [ i , j ] . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) fig plt.GridSpec: More Complicated Arrangements \u00b6 To go beyond a regular grid to subplots that span multiple rows and columns, plt.GridSpec() is the best tool. The plt.GridSpec() object does not create a plot by itself; it is simply a convenient interface that is recognized by the plt.subplot() command. grid = plt . GridSpec ( 2 , 3 , wspace = 0.4 , hspace = 0.3 ) plt . subplot ( grid [ 0 , 0 ]) plt . subplot ( grid [ 0 , 1 :]) plt . subplot ( grid [ 1 ,: 2 ]) plt . subplot ( grid [ 1 , 2 ]); # Create some normally distributed data mean = [ 0 , 0 ] cov = [[ 1 , 1 ],[ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , cov , 3000 ) . T # Set up the axes with gridspec fig = plt . figure ( figsize = ( 6 , 6 )) grid = plt . GridSpec ( 4 , 4 , wspace = 0.2 , hspace = 0.2 ) main_ax = fig . add_subplot ( grid [: - 1 , 1 :]) y_hist = fig . add_subplot ( grid [: - 1 , 0 ], xticklabels = [], sharey = main_ax ) x_hist = fig . add_subplot ( grid [ - 1 , 1 :], yticklabels = [], sharex = main_ax ) # scatter points on the main axes main_ax . plot ( x , y , 'ok' , markersize = 3 , alpha = 0.2 ) # historgram on the attached axes x_hist . hist ( x , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'gray' ) x_hist . invert_yaxis () y_hist . hist ( y , 40 , histtype = 'stepfilled' , orientation = 'horizontal' , color = 'gray' ) y_hist . invert_xaxis ()","title":"09multiple subplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#09-multiple-subplots","text":"Multiple Subplots plt.axes: Subplots by Hand plt.subplot: Simple Grids of Subplots plt.subplots: The Whole Grid in One Go plt.GridSpec: More Complicated Arrangements","title":"09 -  Multiple Subplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#multiple-subplots","text":"Sometimes it is helpful to compare different views of data side by side. To this end, Matplotlib has the concept of subplots: groups of smaller axes that can exist together within a single figure. These subplots might be insets, grids of plots, or other more complicated layouts. In this section, we\u2019ll explore four routines for creating subplots in Matplotlib. We\u2019ll start by setting up the notebook for plotting and importing the functions we will use: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np","title":"Multiple Subplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#pltaxes-subplots-by-hand","text":"The most basic method of creating an axes is to use the plt.axes function. As we\u2019ve seen previously, by default this creates a standard axes object that fills the entire fig\u2010 ure. plt.axes also takes an optional argument that is a list of four numbers in the figure coordinate system. These numbers represent [bottom, left, width, height] in the figure coordinate system, which ranges from 0 at the bottom left of the figure to 1 at the top right of the figure. ax1 = plt . axes () ax2 = plt . axes ([ 0.65 , 0.65 , 0.2 , 0.2 ]) The equivalent of this command within the object-oriented interface is fig.add_axes(). Let\u2019s use this to create two vertically stacked axes fig = plt . figure () ax1 = fig . add_axes ([ 0.1 , 0.5 , 0.8 , 0.4 ], xticklabels = [], ylim = ( - 1.2 , 1.2 )) ax2 = fig . add_axes ([ 0.1 , 0.1 , 0.8 , 0.4 ], xticklabels = [], ylim = ( - 1.2 , 1.2 )) x = np . linspace ( - 0 , 10 ) ax1 . plot ( np . sin ( x )) ax2 . plot ( np . cos ( x ));","title":"plt.axes: Subplots by Hand"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#pltsubplot-simple-grids-of-subplots","text":"Aligned columns or rows of subplots are a common enough need that Matplotlib has several convenience routines that make them easy to create. The lowest level of these is plt.subplot(), which creates a single subplot within a grid. As you can see, this command takes three integer arguments\u2014the number of rows, the number of col\u2010 umns, and the index of the plot to be created in this scheme, which runs from the upper left to the bottom right for i in range ( 1 , 7 ): plt . subplot ( 2 , 3 , i ) plt . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) The command plt.subplots_adjust can be used to adjust the spacing between these plots. object-oriented command, fig.add_subplot(): fig = plt . figure () fig . subplots_adjust ( hspace = 0.4 , wspace = 0.4 ) for i in range ( 1 , 7 ): ax = fig . add_subplot ( 2 , 3 , i ) ax . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' )","title":"plt.subplot: Simple Grids of Subplots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#pltsubplots-the-whole-grid-in-one-go","text":"The approach just described can become quite tedious when you\u2019re creating a large grid of subplots, especially if you\u2019d like to hide the x- and y-axis labels on the inner plots. For this purpose, plt.subplots() is the easier tool to use (note the s at the end of subplots). Rather than creating a single subplot, this function creates a full grid of subplots in a single line, returning them in a NumPy array. The arguments are the number of rows and number of columns, along with optional keywords sharex and sharey, which allow you to specify the relationships between different axes. fig , ax = plt . subplots ( 2 , 3 , sharex = 'col' , sharey = 'row' ) for i in range ( 2 ): for j in range ( 3 ): ax [ i , j ] . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) fig","title":"plt.subplots: The Whole Grid in One Go"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/#pltgridspec-more-complicated-arrangements","text":"To go beyond a regular grid to subplots that span multiple rows and columns, plt.GridSpec() is the best tool. The plt.GridSpec() object does not create a plot by itself; it is simply a convenient interface that is recognized by the plt.subplot() command. grid = plt . GridSpec ( 2 , 3 , wspace = 0.4 , hspace = 0.3 ) plt . subplot ( grid [ 0 , 0 ]) plt . subplot ( grid [ 0 , 1 :]) plt . subplot ( grid [ 1 ,: 2 ]) plt . subplot ( grid [ 1 , 2 ]); # Create some normally distributed data mean = [ 0 , 0 ] cov = [[ 1 , 1 ],[ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , cov , 3000 ) . T # Set up the axes with gridspec fig = plt . figure ( figsize = ( 6 , 6 )) grid = plt . GridSpec ( 4 , 4 , wspace = 0.2 , hspace = 0.2 ) main_ax = fig . add_subplot ( grid [: - 1 , 1 :]) y_hist = fig . add_subplot ( grid [: - 1 , 0 ], xticklabels = [], sharey = main_ax ) x_hist = fig . add_subplot ( grid [ - 1 , 1 :], yticklabels = [], sharex = main_ax ) # scatter points on the main axes main_ax . plot ( x , y , 'ok' , markersize = 3 , alpha = 0.2 ) # historgram on the attached axes x_hist . hist ( x , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'gray' ) x_hist . invert_yaxis () y_hist . hist ( y , 40 , histtype = 'stepfilled' , orientation = 'horizontal' , color = 'gray' ) y_hist . invert_xaxis ()","title":"plt.GridSpec: More Complicated Arrangements"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 10 - Text and Annotation \u00b6 Text and Annotation Transforms and Text Position Arrows and Annotation Text and Annotation \u00b6 Creating a good visualization involves guiding the reader so that the figure tells a story. In some cases, this story can be told in an entirely visual manner, without the need for added text, but in others, small textual cues and labels are necessary. Perhaps the most basic types of annotations you will use are axes labels and titles, but the options go beyond this. Let\u2019s take a look at some data and how we might visualize and annotate it to help convey interesting information. % matplotlib inline import matplotlib.pyplot as plt import matplotlib as mpl plt . style . use ( 'seaborn-whitegrid' ) import numpy as np import pandas as pd births = pd . read_csv ( '../data/births.csv' ) quartiles = np . percentile ( births [ 'births' ],[ 25 , 50 , 75 ]) mu , sig = quartiles [ 1 ], 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) births = births . query ( '(births > @mu - 5*@sig)& (births<@mu + 5* @sig)' ) births [ 'day' ] = births [ 'day' ] . astype ( int ) births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = \"%Y%m %d \" ) births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] /tmp/ipykernel_132909/914953373.py:15: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead. births_by_date.index=[pd.datetime(2012, month, day) for (month, day) in births_by_date.index] fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) <AxesSubplot:> fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot style = dict ( size = 10 , color = 'gray' ) ax . text ( '2012-1-1' , 3950 , \"New Year's Day\" , ** style ) ax . text ( '2012-7-4' , 4250 , \"Independence Day\" , ha = 'center' , ** style ) ax . text ( '2012-9-4' , 4850 , \"Labor Day\" , ha = 'center' , ** style ) ax . text ( '2012-10-31' , 4600 , \"Halloween\" , ha = 'right' , ** style ) ax . text ( '2012-11-25' , 4450 , \"Thanksgiving\" , ha = 'center' , ** style ) ax . text ( '2012-12-25' , 3850 , \"Christmas \" , ha = 'right' , ** style ) # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' )); Transforms and Text Position \u00b6 In the previous example, we anchored our text annotations to data locations. Some\u2010 times it\u2019s preferable to anchor the text to a position on the axes or figure, independent of the data. In Matplotlib, we do this by modifying the transform. Any graphics display framework needs some scheme for translating between coordinate systems. For example, a data point at x, y = 1, 1 needs to somehow be represented at a certain location on the figure, which in turn needs to be represented in pixels on the screen. Mathematically, such coordinate transformations are relatively straightforward, and Matplotlib has a well-developed set of tools that it uses internally to perform them (the tools can be explored in the Matplotlib.transforms submodule). The average user rarely needs to worry about the details of these transforms, but it is helpful knowledge to have when considering the placement of text on a figure. There are three predefined transforms that can be useful in this situation: ax.transData Transform associated with data coordinates ax.transAxes Transform associated with the axes (in units of axes dimensions) fig.transFigure Transform associated with the figure (in units of figure dimensions) # # %matplotlib inline % matplotlib notebook # import matplotlib.pyplot as plt # import matplotlib as mpl # plt.style.use('seaborn-whitegrid') # import numpy as np # import pandas as pd fig , ax = plt . subplots ( facecolor = 'lightgray' ) ax . axis ([ 0 , 10 , 0 , 10 ]) # transform=ax.transData is the default, ax . text ( 1 , 5 , \". Data: (1,5)\" , transform = ax . transData ) ax . text ( 5 , 3 , \". Data: (2,1.5)\" , transform = ax . transData ) ax . text ( 0.2 , 0.2 , \". Data: (0.2,0.2)\" , transform = ax . transData ) <IPython.core.display.Javascript object> Text(0.2, 0.2, '. Data: (0.2,0.2)') ax . set_xlim ( 0 , 2 ) ax . set_ylim ( - 6 , 6 ) fig <IPython.core.display.Javascript object> Arrows and Annotation \u00b6 Along with tick marks and text, another useful annotation mark is the simple arrow. Drawing arrows in Matplotlib is often much harder than you might hope. While there is a plt.arrow() function available, I wouldn\u2019t suggest using it; the arrows it creates are SVG objects that will be subject to the varying aspect ratio of your plots, and the result is rarely what the user intended. Instead, I\u2019d suggest using the plt.anno tate() function. This function creates some text and an arrow, and the arrows can be very flexibly specified. % matplotlib inline fig , ax = plt . subplots () x = np . linspace ( 0 , 20 , 1000 ) ax . plot ( x , np . cos ( x )) ax . axis ( 'equal' ) ax . annotate ( 'local maximum' , xy = ( 6.28 , 1 ), xytext = ( 10 , 4 ), arrowprops = dict ( facecolor = 'black' , shrink = 0.05 )) ax . annotate ( 'local minimum' , xy = ( 5 * np . pi , - 1 ), xytext = ( 2 , - 6 ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )); The arrow style is controlled through the arrowprops dictionary, which has numer\u2010 ous options available. These options are fairly well documented in Matplotlib\u2019s online documentation, fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot ax . annotate ( \"New Year's Day\" , xy = ( '2012-1-1' , 4100 ), xycoords = 'data' , xytext = ( 50 , - 30 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"arc3,rad=-0.2\" )) ax . annotate ( \"Independence Day\" , xy = ( '2012-7-4' , 4250 ), xycoords = 'data' , bbox = dict ( boxstyle = \"round\" , fc = \"none\" , ec = \"gray\" ), xytext = ( 10 , - 40 ), textcoords = 'offset points' , ha = 'center' , arrowprops = dict ( arrowstyle = \"->\" )) ax . annotate ( 'Labor Day' , xy = ( '2012-9-4' , 4850 ), xycoords = 'data' , ha = 'center' , xytext = ( 0 , - 20 ), textcoords = 'offset points' ) ax . annotate ( '' , xy = ( '2012-9-1' , 4850 ), xytext = ( '2012-9-7' , 4850 ), xycoords = 'data' , textcoords = 'data' , arrowprops = { 'arrowstyle' : '|-|,widthA=0.2,widthB=0.2' , }) ax . annotate ( 'Halloween' , xy = ( '2012-10-31' , 4600 ), xycoords = 'data' , xytext = ( - 80 , - 40 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"fancy\" , fc = \"0.6\" , ec = \"none\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )) ax . annotate ( 'Thanksgiving' , xy = ( '2012-11-25' , 4500 ), xycoords = 'data' , xytext = ( - 120 , - 60 ), textcoords = 'offset points' , bbox = dict ( boxstyle = \"round4,pad=.5\" , fc = \"0.9\" ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle,angleA=0,angleB=80,rad=20\" )) ax . annotate ( 'Christmas' , xy = ( '2012-12-25' , 3850 ), xycoords = 'data' , xytext = ( - 30 , 0 ), textcoords = 'offset points' , size = 13 , ha = 'right' , va = \"center\" , bbox = dict ( boxstyle = \"round\" , alpha = 0.1 ), arrowprops = dict ( arrowstyle = \"wedge,tail_width=0.5\" , alpha = 0.1 )); # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' )); ax . set_ylim ( 3600 , 5400 );","title":"10text and annotation Example"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#10-text-and-annotation","text":"Text and Annotation Transforms and Text Position Arrows and Annotation","title":"10 -  Text and Annotation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#text-and-annotation","text":"Creating a good visualization involves guiding the reader so that the figure tells a story. In some cases, this story can be told in an entirely visual manner, without the need for added text, but in others, small textual cues and labels are necessary. Perhaps the most basic types of annotations you will use are axes labels and titles, but the options go beyond this. Let\u2019s take a look at some data and how we might visualize and annotate it to help convey interesting information. % matplotlib inline import matplotlib.pyplot as plt import matplotlib as mpl plt . style . use ( 'seaborn-whitegrid' ) import numpy as np import pandas as pd births = pd . read_csv ( '../data/births.csv' ) quartiles = np . percentile ( births [ 'births' ],[ 25 , 50 , 75 ]) mu , sig = quartiles [ 1 ], 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) births = births . query ( '(births > @mu - 5*@sig)& (births<@mu + 5* @sig)' ) births [ 'day' ] = births [ 'day' ] . astype ( int ) births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = \"%Y%m %d \" ) births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] /tmp/ipykernel_132909/914953373.py:15: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead. births_by_date.index=[pd.datetime(2012, month, day) for (month, day) in births_by_date.index] fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) <AxesSubplot:> fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot style = dict ( size = 10 , color = 'gray' ) ax . text ( '2012-1-1' , 3950 , \"New Year's Day\" , ** style ) ax . text ( '2012-7-4' , 4250 , \"Independence Day\" , ha = 'center' , ** style ) ax . text ( '2012-9-4' , 4850 , \"Labor Day\" , ha = 'center' , ** style ) ax . text ( '2012-10-31' , 4600 , \"Halloween\" , ha = 'right' , ** style ) ax . text ( '2012-11-25' , 4450 , \"Thanksgiving\" , ha = 'center' , ** style ) ax . text ( '2012-12-25' , 3850 , \"Christmas \" , ha = 'right' , ** style ) # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' ));","title":"Text and Annotation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#transforms-and-text-position","text":"In the previous example, we anchored our text annotations to data locations. Some\u2010 times it\u2019s preferable to anchor the text to a position on the axes or figure, independent of the data. In Matplotlib, we do this by modifying the transform. Any graphics display framework needs some scheme for translating between coordinate systems. For example, a data point at x, y = 1, 1 needs to somehow be represented at a certain location on the figure, which in turn needs to be represented in pixels on the screen. Mathematically, such coordinate transformations are relatively straightforward, and Matplotlib has a well-developed set of tools that it uses internally to perform them (the tools can be explored in the Matplotlib.transforms submodule). The average user rarely needs to worry about the details of these transforms, but it is helpful knowledge to have when considering the placement of text on a figure. There are three predefined transforms that can be useful in this situation: ax.transData Transform associated with data coordinates ax.transAxes Transform associated with the axes (in units of axes dimensions) fig.transFigure Transform associated with the figure (in units of figure dimensions) # # %matplotlib inline % matplotlib notebook # import matplotlib.pyplot as plt # import matplotlib as mpl # plt.style.use('seaborn-whitegrid') # import numpy as np # import pandas as pd fig , ax = plt . subplots ( facecolor = 'lightgray' ) ax . axis ([ 0 , 10 , 0 , 10 ]) # transform=ax.transData is the default, ax . text ( 1 , 5 , \". Data: (1,5)\" , transform = ax . transData ) ax . text ( 5 , 3 , \". Data: (2,1.5)\" , transform = ax . transData ) ax . text ( 0.2 , 0.2 , \". Data: (0.2,0.2)\" , transform = ax . transData ) <IPython.core.display.Javascript object> Text(0.2, 0.2, '. Data: (0.2,0.2)') ax . set_xlim ( 0 , 2 ) ax . set_ylim ( - 6 , 6 ) fig <IPython.core.display.Javascript object>","title":"Transforms and Text Position"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/#arrows-and-annotation","text":"Along with tick marks and text, another useful annotation mark is the simple arrow. Drawing arrows in Matplotlib is often much harder than you might hope. While there is a plt.arrow() function available, I wouldn\u2019t suggest using it; the arrows it creates are SVG objects that will be subject to the varying aspect ratio of your plots, and the result is rarely what the user intended. Instead, I\u2019d suggest using the plt.anno tate() function. This function creates some text and an arrow, and the arrows can be very flexibly specified. % matplotlib inline fig , ax = plt . subplots () x = np . linspace ( 0 , 20 , 1000 ) ax . plot ( x , np . cos ( x )) ax . axis ( 'equal' ) ax . annotate ( 'local maximum' , xy = ( 6.28 , 1 ), xytext = ( 10 , 4 ), arrowprops = dict ( facecolor = 'black' , shrink = 0.05 )) ax . annotate ( 'local minimum' , xy = ( 5 * np . pi , - 1 ), xytext = ( 2 , - 6 ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )); The arrow style is controlled through the arrowprops dictionary, which has numer\u2010 ous options available. These options are fairly well documented in Matplotlib\u2019s online documentation, fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot ax . annotate ( \"New Year's Day\" , xy = ( '2012-1-1' , 4100 ), xycoords = 'data' , xytext = ( 50 , - 30 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"arc3,rad=-0.2\" )) ax . annotate ( \"Independence Day\" , xy = ( '2012-7-4' , 4250 ), xycoords = 'data' , bbox = dict ( boxstyle = \"round\" , fc = \"none\" , ec = \"gray\" ), xytext = ( 10 , - 40 ), textcoords = 'offset points' , ha = 'center' , arrowprops = dict ( arrowstyle = \"->\" )) ax . annotate ( 'Labor Day' , xy = ( '2012-9-4' , 4850 ), xycoords = 'data' , ha = 'center' , xytext = ( 0 , - 20 ), textcoords = 'offset points' ) ax . annotate ( '' , xy = ( '2012-9-1' , 4850 ), xytext = ( '2012-9-7' , 4850 ), xycoords = 'data' , textcoords = 'data' , arrowprops = { 'arrowstyle' : '|-|,widthA=0.2,widthB=0.2' , }) ax . annotate ( 'Halloween' , xy = ( '2012-10-31' , 4600 ), xycoords = 'data' , xytext = ( - 80 , - 40 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"fancy\" , fc = \"0.6\" , ec = \"none\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )) ax . annotate ( 'Thanksgiving' , xy = ( '2012-11-25' , 4500 ), xycoords = 'data' , xytext = ( - 120 , - 60 ), textcoords = 'offset points' , bbox = dict ( boxstyle = \"round4,pad=.5\" , fc = \"0.9\" ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle,angleA=0,angleB=80,rad=20\" )) ax . annotate ( 'Christmas' , xy = ( '2012-12-25' , 3850 ), xycoords = 'data' , xytext = ( - 30 , 0 ), textcoords = 'offset points' , size = 13 , ha = 'right' , va = \"center\" , bbox = dict ( boxstyle = \"round\" , alpha = 0.1 ), arrowprops = dict ( arrowstyle = \"wedge,tail_width=0.5\" , alpha = 0.1 )); # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' )); ax . set_ylim ( 3600 , 5400 );","title":"Arrows and Annotation"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 11 - Customizing Ticks \u00b6 Customizing Ticks Major and Minor Ticks Hiding Ticks or Labels Reducing or Increasing the Number of Ticks Fancy Tick Formats Customizing Ticks \u00b6 Matplotlib\u2019s default tick locators and formatters are designed to be generally sufficient in many common situations, but are in no way optimal for every plot. Major and Minor Ticks \u00b6 Within each axis, there is the concept of a major tick mark and a minor tick mark. As the names would imply, major ticks are usually bigger or more pronounced, while minor ticks are usually smaller. By default, Matplotlib rarely makes use of minor ticks, but one place you can see them is within logarithmic plots % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np ax = plt . axes ( xscale = 'log' , yscale = 'log' ) print ( ax . xaxis . get_major_locator ()) print ( ax . xaxis . get_minor_locator ()) <matplotlib.ticker.LogLocator object at 0x7faa7bb8f640> <matplotlib.ticker.LogLocator object at 0x7faa7be3baf0> print ( ax . xaxis . get_major_formatter ()) print ( ax . xaxis . get_minor_formatter ()) <matplotlib.ticker.LogFormatterSciNotation object at 0x7faa7bb7ca60> <matplotlib.ticker.LogFormatterSciNotation object at 0x7faab054b6d0> Hiding Ticks or Labels \u00b6 Perhaps the most common tick/label formatting operation is the act of hiding ticks or labels. We can do this using plt.NullLocator() and plt.NullFormatter(), as shown here ax = plt . axes () ax . plot ( np . random . rand ( 50 )) ax . yaxis . set_major_locator ( plt . NullLocator ()) ax . yaxis . set_major_formatter ( plt . NullFormatter ()) ax = plt . axes () ax . plot ( np . random . rand ( 50 )) fig , ax = plt . subplots ( 5 , 5 , figsize = ( 5 , 5 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) #Get some face images from scikit-learn from sklearn.datasets import fetch_olivetti_faces faces = fetch_olivetti_faces () . images for i in range ( 5 ): for j in range ( 5 ): ax [ i , j ] . xaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . yaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . imshow ( faces [ 10 * i + j ], cmap = 'bone' ) Reducing or Increasing the Number of Ticks \u00b6 One common problem with the default settings is that smaller subplots can end up with crowded labels. fig , ac = plt . subplots ( 4 , 4 , sharex = True , sharey = True ) Particularly for the x ticks, the numbers nearly overlap, making them quite difficult to decipher. We can fix this with the plt.MaxNLocator(), which allows us to specify the maximum number of ticks that will be displayed. Given this maximum number, Mat\u2010 plotlib will use internal logic to choose the particular tick locations for axi in ax . flat : axi . xaxis . set_major_locator ( plt . MaxNLocator ( 3 )) axi . yaxis . set_major_locator ( plt . MaxNLocator ( 3 )) fig Fancy Tick Formats \u00b6 Matplotlib\u2019s default tick formatting can leave a lot to be desired; it works well as a broad default, but sometimes you\u2019d like to do something more # Plot a sine and cosine curve fig , ax = plt . subplots () x = np . linspace ( 0 , 3 * np . pi , 1000 ) ax . plot ( x , np . sin ( x ), lw = 3 , label = 'Sine' ) ax . plot ( x , np . cos ( x ), lw = 3 , label = 'Cosine' ) # Set up grid, legend, and limits ax . grid ( True ) ax . legend ( frameon = False ) ax . axis ( 'equal' ) ax . set_xlim ( 0 , 3 * np . pi ); ax . xaxis . set_major_locator ( plt . MultipleLocator ( np . pi / 2 )) ax . xaxis . set_minor_locator ( plt . MultipleLocator ( np . pi / 4 )) fig def format_func ( value , tick_number ): # find number of multiples of pi/2 N = int ( np . round ( 2 * value / np . pi )) if N == 0 : return \"0\" elif N == 1 : return r \"$\\pi/2$\" elif N == 2 : return r \"$\\pi$\" elif N % 2 > 0 : return r \"$ {0} \\pi/2$\" . format ( N ) else : return r \"$ {0} \\pi$\" . format ( N // 2 ) ax . xaxis . set_major_formatter ( plt . FuncFormatter ( format_func )) fig This is much better! Notice that we\u2019ve made use of Matplotlib\u2019s LaTeX support, speci\u2010 fied by enclosing the string within dollar signs. This is very convenient for display of mathematical symbols and formulae; in this case, \u201c \\(\\pi\\) \u201d is rendered as the Greek character \u03c0. The plt.FuncFormatter() offers extremely fine-grained control over the appearance of your plot ticks, and comes in very handy when you\u2019re preparing plots for presenta\u2010 tion or publication.","title":"11customizing ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#11-customizing-ticks","text":"Customizing Ticks Major and Minor Ticks Hiding Ticks or Labels Reducing or Increasing the Number of Ticks Fancy Tick Formats","title":"11 -  Customizing Ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#customizing-ticks","text":"Matplotlib\u2019s default tick locators and formatters are designed to be generally sufficient in many common situations, but are in no way optimal for every plot.","title":"Customizing Ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#major-and-minor-ticks","text":"Within each axis, there is the concept of a major tick mark and a minor tick mark. As the names would imply, major ticks are usually bigger or more pronounced, while minor ticks are usually smaller. By default, Matplotlib rarely makes use of minor ticks, but one place you can see them is within logarithmic plots % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np ax = plt . axes ( xscale = 'log' , yscale = 'log' ) print ( ax . xaxis . get_major_locator ()) print ( ax . xaxis . get_minor_locator ()) <matplotlib.ticker.LogLocator object at 0x7faa7bb8f640> <matplotlib.ticker.LogLocator object at 0x7faa7be3baf0> print ( ax . xaxis . get_major_formatter ()) print ( ax . xaxis . get_minor_formatter ()) <matplotlib.ticker.LogFormatterSciNotation object at 0x7faa7bb7ca60> <matplotlib.ticker.LogFormatterSciNotation object at 0x7faab054b6d0>","title":"Major and Minor Ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#hiding-ticks-or-labels","text":"Perhaps the most common tick/label formatting operation is the act of hiding ticks or labels. We can do this using plt.NullLocator() and plt.NullFormatter(), as shown here ax = plt . axes () ax . plot ( np . random . rand ( 50 )) ax . yaxis . set_major_locator ( plt . NullLocator ()) ax . yaxis . set_major_formatter ( plt . NullFormatter ()) ax = plt . axes () ax . plot ( np . random . rand ( 50 )) fig , ax = plt . subplots ( 5 , 5 , figsize = ( 5 , 5 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) #Get some face images from scikit-learn from sklearn.datasets import fetch_olivetti_faces faces = fetch_olivetti_faces () . images for i in range ( 5 ): for j in range ( 5 ): ax [ i , j ] . xaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . yaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . imshow ( faces [ 10 * i + j ], cmap = 'bone' )","title":"Hiding Ticks or Labels"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#reducing-or-increasing-the-number-of-ticks","text":"One common problem with the default settings is that smaller subplots can end up with crowded labels. fig , ac = plt . subplots ( 4 , 4 , sharex = True , sharey = True ) Particularly for the x ticks, the numbers nearly overlap, making them quite difficult to decipher. We can fix this with the plt.MaxNLocator(), which allows us to specify the maximum number of ticks that will be displayed. Given this maximum number, Mat\u2010 plotlib will use internal logic to choose the particular tick locations for axi in ax . flat : axi . xaxis . set_major_locator ( plt . MaxNLocator ( 3 )) axi . yaxis . set_major_locator ( plt . MaxNLocator ( 3 )) fig","title":"Reducing or Increasing the Number of Ticks"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/#fancy-tick-formats","text":"Matplotlib\u2019s default tick formatting can leave a lot to be desired; it works well as a broad default, but sometimes you\u2019d like to do something more # Plot a sine and cosine curve fig , ax = plt . subplots () x = np . linspace ( 0 , 3 * np . pi , 1000 ) ax . plot ( x , np . sin ( x ), lw = 3 , label = 'Sine' ) ax . plot ( x , np . cos ( x ), lw = 3 , label = 'Cosine' ) # Set up grid, legend, and limits ax . grid ( True ) ax . legend ( frameon = False ) ax . axis ( 'equal' ) ax . set_xlim ( 0 , 3 * np . pi ); ax . xaxis . set_major_locator ( plt . MultipleLocator ( np . pi / 2 )) ax . xaxis . set_minor_locator ( plt . MultipleLocator ( np . pi / 4 )) fig def format_func ( value , tick_number ): # find number of multiples of pi/2 N = int ( np . round ( 2 * value / np . pi )) if N == 0 : return \"0\" elif N == 1 : return r \"$\\pi/2$\" elif N == 2 : return r \"$\\pi$\" elif N % 2 > 0 : return r \"$ {0} \\pi/2$\" . format ( N ) else : return r \"$ {0} \\pi$\" . format ( N // 2 ) ax . xaxis . set_major_formatter ( plt . FuncFormatter ( format_func )) fig This is much better! Notice that we\u2019ve made use of Matplotlib\u2019s LaTeX support, speci\u2010 fied by enclosing the string within dollar signs. This is very convenient for display of mathematical symbols and formulae; in this case, \u201c \\(\\pi\\) \u201d is rendered as the Greek character \u03c0. The plt.FuncFormatter() offers extremely fine-grained control over the appearance of your plot ticks, and comes in very handy when you\u2019re preparing plots for presenta\u2010 tion or publication.","title":"Fancy Tick Formats"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 12 - Customizing Matplotlib: Configurations and Stylesheets \u00b6 Customizing Matplotlib: Configurations and Stylesheets Plot Customization by Hand Changing the Defaults: rcParams Stylesheets Default style Customizing Matplotlib: Configurations and Stylesheets \u00b6 Matplotlib\u2019s default plot settings are often the subject of complaint among its users. While much is slated to change in the 2.0 Matplotlib release, the ability to customize default settings helps bring the package in line with your own aesthetic preferences. Here we\u2019ll walk through some of Matplotlib\u2019s runtime configuration (rc) options, and take a look at the newer stylesheets feature, which contains some nice sets of default configurations. Plot Customization by Hand \u00b6 Throughout this chapter, we\u2019ve seen how it is possible to tweak individual plot set\u2010 tings to end up with something that looks a little bit nicer than the default. It\u2019s possi\u2010 ble to do these customizations for each individual plot. For example, here is a fairly drab default histogram import matplotlib.pyplot as plt plt . style . use ( 'classic' ) import numpy as np % matplotlib inline x = np . random . randn ( 1000 ) plt . hist ( x ); # We can adjust this by hand to make it a much more visually pleasing plot # use a gray backgoung ax = plt . axes () ax . set_axisbelow ( True ) # draw solid white grid line plt . grid ( color = 'w' , linestyle = 'solid' ) # hide axis spines for spine in ax . spines . values (): spine . set_visible ( False ) #hide top and right ticks ax . xaxis . tick_bottom () ax . yaxis . tick_left () #lighten ticks and labels ax . tick_params ( color = 'gray' , direction = 'out' ) for tick in ax . get_xticklabels (): tick . set_color ( 'gray' ) for tick in ax . get_yticklabels (): tick . set_color ( 'gray' ) # control face and edge color of histogram ax . hist ( x , edgecolor = '#E6E6E6' , color = '#EE6666' ) (array([ 12., 37., 98., 173., 257., 213., 124., 59., 23., 4.]), array([-2.90788595, -2.30201003, -1.69613411, -1.09025818, -0.48438226, 0.12149367, 0.72736959, 1.33324551, 1.93912144, 2.54499736, 3.15087329]), <BarContainer object of 10 artists>) This looks better, and you may recognize the look as inspired by the look of the R language\u2019s ggplot visualization package. But this took a whole lot of effort! We defi\u2010 nitely do not want to have to do all that tweaking each time we create a plot. Fortu\u2010 nately, there is a way to adjust these defaults once in a way that will work for all plots. Changing the Defaults: rcParams \u00b6 Each time Matplotlib loads, it defines a runtime configuration (rc) containing the default styles for every plot element you create. You can adjust this configuration at any time using the plt.rc convenience routine. Let\u2019s see what it looks like to modify the rc parameters so that our default plot will look similar to what we did before. We\u2019ll start by saving a copy of the current rcParams dictionary, so we can easily reset these changes in the current session: Ipython_default = plt . rcParams . copy () from matplotlib import cycler colors = cycler ( 'color' , [ '#EE6666' , '#3388BB' , '#9988DD' , '#EECC55' , '#88BB44' , '#FFBBBB' ]) plt . rc ( 'axes' , facecolor = '#E6E6E6' , edgecolor = 'none' , axisbelow = True , grid = True , prop_cycle = colors ) plt . rc ( 'grid' , color = 'w' , linestyle = 'solid' ) plt . rc ( 'xtick' , direction = 'out' , color = 'gray' ) plt . rc ( 'ytick' , direction = 'out' , color = 'gray' ) plt . rc ( 'patch' , edgecolor = '#E6E6E6' ) plt . rc ( 'lines' , linewidth = 2 ) # With these settings defined, we can now create a plot and see our settings in action plt . hist ( x ); # Let\u2019s see what simple line plots look like with these rc parameters for i in range ( 4 ): plt . plot ( np . random . rand ( 10 )) I find this much more aesthetically pleasing than the default styling. If you disagree with my aesthetic sense, the good news is that you can adjust the rc parameters to suit your own tastes! These settings can be saved in a .matplotlibrc file, which you can read about in the Matplotlib documentation. That said, I prefer to customize Mat\u2010 plotlib using its stylesheets instead. Stylesheets \u00b6 Even if you don\u2019t create your own style, the stylesheets included by default are extremely useful. The available styles are listed in plt.style.available\u2014here I\u2019ll list only the first five for brevity: plt . style . available [: 5 ] ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh'] plt . style . use ( 'Solarize_Light2' ) with plt . style . context ( 'Solarize_Light2' ): make_a_plot () NameError: name 'make_a_plot' is not defined def hist_and_lines (): np . random . seed ( 0 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 11 , 4 )) ax [ 0 ] . hist ( np . random . randn ( 1000 )) for i in range ( 3 ): ax [ 1 ] . plot ( np . random . rand ( 100 )) ax [ 1 ] . legend ([ 'a' , 'b' , 'c' ], loc = 'lower left' ) Default style \u00b6 The default style is what we\u2019ve been seeing so far throughout the book; we\u2019ll start with that. First, let\u2019s reset our runtime configuration to the notebook default: # reset rcParams plt . rcParams . update ( Ipython_default ); # let's see how it looks hist_and_lines () # let use one of the styles avialable plt . style . available [:] ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10'] # Using fivethrityeight with plt . style . context ( 'fivethirtyeight' ): hist_and_lines () # Using ggplot with plt . style . context ( 'ggplot' ): hist_and_lines () Bayesian Methods for Hackers style There is a very nice short online book called Probabilistic Programming and Bayesian Methods for Hackers; it features figures created with Matplotlib, and uses a nice set of rc parameters to create a consistent and visually appealing style throughout the book. This style is reproduced in the bmh stylesheet # Using bmh with plt . style . context ( 'bmh' ): hist_and_lines () Dark background For figures used within presentations, it is often useful to have a dark rather than light background. The dark_background style provides this with plt . style . context ( 'dark_background' ): hist_and_lines () Grayscale Sometimes you might find yourself preparing figures for a print publication that does not accept color figures. with plt . style . context ( 'grayscale' ): hist_and_lines () Seaborn style Matplotlib also has stylesheets inspired by the Seaborn library (discussed more fully in \u201cVisualization with Seaborn\u201d on page 311). As we will see, these styles are loaded automatically when Seaborn is imported into a notebook. import seaborn hist_and_lines () with plt . style . context ( 'seaborn' ): hist_and_lines ()","title":"12customizing matplotlib configuration and stylesheets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#12-customizing-matplotlib-configurations-and-stylesheets","text":"Customizing Matplotlib: Configurations and Stylesheets Plot Customization by Hand Changing the Defaults: rcParams Stylesheets Default style","title":"12 -  Customizing Matplotlib: Configurations and Stylesheets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#customizing-matplotlib-configurations-and-stylesheets","text":"Matplotlib\u2019s default plot settings are often the subject of complaint among its users. While much is slated to change in the 2.0 Matplotlib release, the ability to customize default settings helps bring the package in line with your own aesthetic preferences. Here we\u2019ll walk through some of Matplotlib\u2019s runtime configuration (rc) options, and take a look at the newer stylesheets feature, which contains some nice sets of default configurations.","title":"Customizing Matplotlib: Configurations and Stylesheets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#plot-customization-by-hand","text":"Throughout this chapter, we\u2019ve seen how it is possible to tweak individual plot set\u2010 tings to end up with something that looks a little bit nicer than the default. It\u2019s possi\u2010 ble to do these customizations for each individual plot. For example, here is a fairly drab default histogram import matplotlib.pyplot as plt plt . style . use ( 'classic' ) import numpy as np % matplotlib inline x = np . random . randn ( 1000 ) plt . hist ( x ); # We can adjust this by hand to make it a much more visually pleasing plot # use a gray backgoung ax = plt . axes () ax . set_axisbelow ( True ) # draw solid white grid line plt . grid ( color = 'w' , linestyle = 'solid' ) # hide axis spines for spine in ax . spines . values (): spine . set_visible ( False ) #hide top and right ticks ax . xaxis . tick_bottom () ax . yaxis . tick_left () #lighten ticks and labels ax . tick_params ( color = 'gray' , direction = 'out' ) for tick in ax . get_xticklabels (): tick . set_color ( 'gray' ) for tick in ax . get_yticklabels (): tick . set_color ( 'gray' ) # control face and edge color of histogram ax . hist ( x , edgecolor = '#E6E6E6' , color = '#EE6666' ) (array([ 12., 37., 98., 173., 257., 213., 124., 59., 23., 4.]), array([-2.90788595, -2.30201003, -1.69613411, -1.09025818, -0.48438226, 0.12149367, 0.72736959, 1.33324551, 1.93912144, 2.54499736, 3.15087329]), <BarContainer object of 10 artists>) This looks better, and you may recognize the look as inspired by the look of the R language\u2019s ggplot visualization package. But this took a whole lot of effort! We defi\u2010 nitely do not want to have to do all that tweaking each time we create a plot. Fortu\u2010 nately, there is a way to adjust these defaults once in a way that will work for all plots.","title":"Plot Customization by Hand"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#changing-the-defaults-rcparams","text":"Each time Matplotlib loads, it defines a runtime configuration (rc) containing the default styles for every plot element you create. You can adjust this configuration at any time using the plt.rc convenience routine. Let\u2019s see what it looks like to modify the rc parameters so that our default plot will look similar to what we did before. We\u2019ll start by saving a copy of the current rcParams dictionary, so we can easily reset these changes in the current session: Ipython_default = plt . rcParams . copy () from matplotlib import cycler colors = cycler ( 'color' , [ '#EE6666' , '#3388BB' , '#9988DD' , '#EECC55' , '#88BB44' , '#FFBBBB' ]) plt . rc ( 'axes' , facecolor = '#E6E6E6' , edgecolor = 'none' , axisbelow = True , grid = True , prop_cycle = colors ) plt . rc ( 'grid' , color = 'w' , linestyle = 'solid' ) plt . rc ( 'xtick' , direction = 'out' , color = 'gray' ) plt . rc ( 'ytick' , direction = 'out' , color = 'gray' ) plt . rc ( 'patch' , edgecolor = '#E6E6E6' ) plt . rc ( 'lines' , linewidth = 2 ) # With these settings defined, we can now create a plot and see our settings in action plt . hist ( x ); # Let\u2019s see what simple line plots look like with these rc parameters for i in range ( 4 ): plt . plot ( np . random . rand ( 10 )) I find this much more aesthetically pleasing than the default styling. If you disagree with my aesthetic sense, the good news is that you can adjust the rc parameters to suit your own tastes! These settings can be saved in a .matplotlibrc file, which you can read about in the Matplotlib documentation. That said, I prefer to customize Mat\u2010 plotlib using its stylesheets instead.","title":"Changing the Defaults: rcParams"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#stylesheets","text":"Even if you don\u2019t create your own style, the stylesheets included by default are extremely useful. The available styles are listed in plt.style.available\u2014here I\u2019ll list only the first five for brevity: plt . style . available [: 5 ] ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh'] plt . style . use ( 'Solarize_Light2' ) with plt . style . context ( 'Solarize_Light2' ): make_a_plot () NameError: name 'make_a_plot' is not defined def hist_and_lines (): np . random . seed ( 0 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 11 , 4 )) ax [ 0 ] . hist ( np . random . randn ( 1000 )) for i in range ( 3 ): ax [ 1 ] . plot ( np . random . rand ( 100 )) ax [ 1 ] . legend ([ 'a' , 'b' , 'c' ], loc = 'lower left' )","title":"Stylesheets"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/#default-style","text":"The default style is what we\u2019ve been seeing so far throughout the book; we\u2019ll start with that. First, let\u2019s reset our runtime configuration to the notebook default: # reset rcParams plt . rcParams . update ( Ipython_default ); # let's see how it looks hist_and_lines () # let use one of the styles avialable plt . style . available [:] ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10'] # Using fivethrityeight with plt . style . context ( 'fivethirtyeight' ): hist_and_lines () # Using ggplot with plt . style . context ( 'ggplot' ): hist_and_lines () Bayesian Methods for Hackers style There is a very nice short online book called Probabilistic Programming and Bayesian Methods for Hackers; it features figures created with Matplotlib, and uses a nice set of rc parameters to create a consistent and visually appealing style throughout the book. This style is reproduced in the bmh stylesheet # Using bmh with plt . style . context ( 'bmh' ): hist_and_lines () Dark background For figures used within presentations, it is often useful to have a dark rather than light background. The dark_background style provides this with plt . style . context ( 'dark_background' ): hist_and_lines () Grayscale Sometimes you might find yourself preparing figures for a print publication that does not accept color figures. with plt . style . context ( 'grayscale' ): hist_and_lines () Seaborn style Matplotlib also has stylesheets inspired by the Seaborn library (discussed more fully in \u201cVisualization with Seaborn\u201d on page 311). As we will see, these styles are loaded automatically when Seaborn is imported into a notebook. import seaborn hist_and_lines () with plt . style . context ( 'seaborn' ): hist_and_lines ()","title":"Default style"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 13 - Three-Dimensional Plotting in Matplotlib \u00b6 Three-Dimensional Plotting in Matplotlib Three-Dimensional Points and Lines Three-Dimensional Contour Plots Wireframes and Surface Plots Surface Triangulations Three-Dimensional Plotting in Matplotlib \u00b6 Matplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib\u2019s two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. We enable three-dimensional plots by importing the mplot3d toolkit, included with the main Matplotlib installation from mpl_toolkits import mplot3d % matplotlib inline import numpy as np import matplotlib.pyplot as plt fig = plt . figure () ax = plt . axes ( projection = '3d' ) With this 3D axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code. % matplotlib notebook Three-Dimensional Points and Lines \u00b6 The most basic three-dimensional plot is a line or scatter plot created from sets of (x,y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, we can create these using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to \u201cSimple Line Plots\u201d and \u201cSimple Scatter Plots\u201d for more information on controlling the output. ax = plt . axes ( projection = '3d' ) # data for 3d line zline = np . linspace ( 0 , 15 , 1000 ) xline = np . sin ( zline ) yline = np . cos ( zline ) ax . plot3D ( xline , yline , zline , 'gray' ) # data for 3d scatter points zdata = 15 * np . random . random ( 100 ) xdata = np . sin ( zdata ) + 0.1 * np . random . randn ( 100 ) ydata = np . cos ( zdata ) + 0.1 * np . random . randn ( 100 ) ax . scatter3D ( xdata , ydata , zdata , c = zdata , cmap = 'Greens' ); <IPython.core.display.Javascript object> Three-Dimensional Contour Plots \u00b6 Analogous to the contour plots we explored in \u201cDensity and Contour Plots\u201d, mplot3d contains tools to create three-dimensional relief plots using the same inputs. Like two-dimensional ax.contour plots, ax.contour3D requires all the input data to be in the form of two-dimensional regular grids, with the Z data evaluated at each point. Here we\u2019ll show a three-dimensional contour diagram of a three-dimensional sinusoidal function def f ( x , y ): return np . sin ( np . sqrt ( x ** 2 + y ** 2 )) x = np . linspace ( - 6 , 6 , 30 ) y = np . linspace ( - 6 , 6 , 30 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . contour3D ( X , Y , Z , 50 , cmap = 'binary' ) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 'y' ) ax . set_zlabel ( 'z' ); <IPython.core.display.Javascript object> Sometimes the default viewing angle is not optimal, in which case we can use the view_init method to set the elevation and azimuthal angles. In this example (the result of which is shown below, we\u2019ll use an elevation of 60 degrees (that is, 60 degrees above the x-y plane) and an azimuth of 35 degrees (that is, rotated 35 degrees counter-clockwise about the z-axis): ax . view_init ( 60 , 35 ) fig <IPython.core.display.Javascript object> Wireframes and Surface Plots \u00b6 Two other types of three-dimensional plots that work on gridded data are wireframes and surface plots. These take a grid of values and project it onto the specified three- dimensional surface, and can make the resulting three-dimensional forms quite easy to visualize. Here\u2019s an example using a wireframe fit = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_wireframe ( X , Y , Z , color = 'black' ) ax . set_title ( 'WireFrame' ); <IPython.core.display.Javascript object> A surface plot is like a wireframe plot, but each face of the wireframe is a filled poly\u2010 gon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ) ax . set_title ( 'Surface' ); <IPython.core.display.Javascript object> Note that though the grid of values for a surface plot needs to be two-dimensional, it need not be rectilinear. Here is an example of creating a partial polar grid, which when used with the surface3D plot can give us a slice into the function we\u2019re visualiz\u2010 ing fig = plt . figure () r = np . linspace ( 0 , 6 , 20 ) theta = np . linspace ( - 0.9 * np . pi , 0.8 * np . pi , 40 ) r , theta = np . meshgrid ( r , theta ) X = r * np . sin ( theta ) Y = r * np . cos ( theta ) Z = f ( X , Y ) ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ); <IPython.core.display.Javascript object> Surface Triangulations \u00b6 For some applications, the evenly sampled grids required by the preceding routines are overly restrictive and inconvenient. In these situations, the triangulation-based plots can be very useful. What if rather than an even draw from a Cartesian or a polar grid, we instead have a set of random draws? theta = 2 * np . pi * np . random . random ( 1000 ) r = 6 * np . random . random ( 1000 ) x = np . ravel ( r * np . sin ( theta )) y = np . ravel ( r * np . cos ( theta )) z = f ( x , y ) fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . scatter ( x , y , z , c = z , cmap = 'viridis' , linewidth = 0.5 ) <IPython.core.display.Javascript object> <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f86f7843820> This leaves a lot to be desired. The function that will help us in this case is ax.plot_trisurf, which creates a surface by first finding a set of triangles formed between adjacent points fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , cmap = 'viridis' , edgecolor = 'none' ) <IPython.core.display.Javascript object> <mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x7f86f7206460> The result is certainly not as clean as when it is plotted with a grid, but the flexibility of such a triangulation allows for some really interesting three-dimensional plots. For example, it is actually possible to plot a three-dimensional M\u00f6bius strip using this, as we\u2019ll see next.","title":"13threedimensional plotting"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#13-three-dimensional-plotting-in-matplotlib","text":"Three-Dimensional Plotting in Matplotlib Three-Dimensional Points and Lines Three-Dimensional Contour Plots Wireframes and Surface Plots Surface Triangulations","title":"13 -  Three-Dimensional Plotting in Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#three-dimensional-plotting-in-matplotlib","text":"Matplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib\u2019s two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. We enable three-dimensional plots by importing the mplot3d toolkit, included with the main Matplotlib installation from mpl_toolkits import mplot3d % matplotlib inline import numpy as np import matplotlib.pyplot as plt fig = plt . figure () ax = plt . axes ( projection = '3d' ) With this 3D axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code. % matplotlib notebook","title":"Three-Dimensional Plotting in Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#three-dimensional-points-and-lines","text":"The most basic three-dimensional plot is a line or scatter plot created from sets of (x,y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, we can create these using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to \u201cSimple Line Plots\u201d and \u201cSimple Scatter Plots\u201d for more information on controlling the output. ax = plt . axes ( projection = '3d' ) # data for 3d line zline = np . linspace ( 0 , 15 , 1000 ) xline = np . sin ( zline ) yline = np . cos ( zline ) ax . plot3D ( xline , yline , zline , 'gray' ) # data for 3d scatter points zdata = 15 * np . random . random ( 100 ) xdata = np . sin ( zdata ) + 0.1 * np . random . randn ( 100 ) ydata = np . cos ( zdata ) + 0.1 * np . random . randn ( 100 ) ax . scatter3D ( xdata , ydata , zdata , c = zdata , cmap = 'Greens' ); <IPython.core.display.Javascript object>","title":"Three-Dimensional Points and Lines"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#three-dimensional-contour-plots","text":"Analogous to the contour plots we explored in \u201cDensity and Contour Plots\u201d, mplot3d contains tools to create three-dimensional relief plots using the same inputs. Like two-dimensional ax.contour plots, ax.contour3D requires all the input data to be in the form of two-dimensional regular grids, with the Z data evaluated at each point. Here we\u2019ll show a three-dimensional contour diagram of a three-dimensional sinusoidal function def f ( x , y ): return np . sin ( np . sqrt ( x ** 2 + y ** 2 )) x = np . linspace ( - 6 , 6 , 30 ) y = np . linspace ( - 6 , 6 , 30 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . contour3D ( X , Y , Z , 50 , cmap = 'binary' ) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 'y' ) ax . set_zlabel ( 'z' ); <IPython.core.display.Javascript object> Sometimes the default viewing angle is not optimal, in which case we can use the view_init method to set the elevation and azimuthal angles. In this example (the result of which is shown below, we\u2019ll use an elevation of 60 degrees (that is, 60 degrees above the x-y plane) and an azimuth of 35 degrees (that is, rotated 35 degrees counter-clockwise about the z-axis): ax . view_init ( 60 , 35 ) fig <IPython.core.display.Javascript object>","title":"Three-Dimensional Contour Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#wireframes-and-surface-plots","text":"Two other types of three-dimensional plots that work on gridded data are wireframes and surface plots. These take a grid of values and project it onto the specified three- dimensional surface, and can make the resulting three-dimensional forms quite easy to visualize. Here\u2019s an example using a wireframe fit = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_wireframe ( X , Y , Z , color = 'black' ) ax . set_title ( 'WireFrame' ); <IPython.core.display.Javascript object> A surface plot is like a wireframe plot, but each face of the wireframe is a filled poly\u2010 gon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ) ax . set_title ( 'Surface' ); <IPython.core.display.Javascript object> Note that though the grid of values for a surface plot needs to be two-dimensional, it need not be rectilinear. Here is an example of creating a partial polar grid, which when used with the surface3D plot can give us a slice into the function we\u2019re visualiz\u2010 ing fig = plt . figure () r = np . linspace ( 0 , 6 , 20 ) theta = np . linspace ( - 0.9 * np . pi , 0.8 * np . pi , 40 ) r , theta = np . meshgrid ( r , theta ) X = r * np . sin ( theta ) Y = r * np . cos ( theta ) Z = f ( X , Y ) ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ); <IPython.core.display.Javascript object>","title":"Wireframes and Surface Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/#surface-triangulations","text":"For some applications, the evenly sampled grids required by the preceding routines are overly restrictive and inconvenient. In these situations, the triangulation-based plots can be very useful. What if rather than an even draw from a Cartesian or a polar grid, we instead have a set of random draws? theta = 2 * np . pi * np . random . random ( 1000 ) r = 6 * np . random . random ( 1000 ) x = np . ravel ( r * np . sin ( theta )) y = np . ravel ( r * np . cos ( theta )) z = f ( x , y ) fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . scatter ( x , y , z , c = z , cmap = 'viridis' , linewidth = 0.5 ) <IPython.core.display.Javascript object> <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f86f7843820> This leaves a lot to be desired. The function that will help us in this case is ax.plot_trisurf, which creates a surface by first finding a set of triangles formed between adjacent points fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , cmap = 'viridis' , edgecolor = 'none' ) <IPython.core.display.Javascript object> <mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x7f86f7206460> The result is certainly not as clean as when it is plotted with a grid, but the flexibility of such a triangulation allows for some really interesting three-dimensional plots. For example, it is actually possible to plot a three-dimensional M\u00f6bius strip using this, as we\u2019ll see next.","title":"Surface Triangulations"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 14 - Geographic Data with Basemap \u00b6 Geographic Data with Basemap Map Projections Let\u2019s start by a convenience routine to draw our world map along with the longitude and latitude lines: Cylindrical projections Pseudo-cylindrical projections Perspective projections Conic projections Other projections Drawing a Map Background Plotting Data on Maps Geographic Data with Basemap \u00b6 One common type of visualization in data science is that of geographic data. Matplot\u2010 lib\u2019s main tool for this type of visualization is the Basemap toolkit, which is one of several Matplotlib toolkits that live under the mpl_toolkits namespace. Admittedly, Basemap feels a bit clunky to use, and often even simple visualizations take much longer to render than you might hope. More modern solutions, such as leaflet or the Google Maps API, may be a better choice for more intensive map visualizations. Still, Basemap is a useful tool for Python users to have in their virtual toolbelts. In this sec\u2010 tion, we\u2019ll show several examples of the type of map visualization that is possible with this toolkit. Installation of Basemap is straightforward; if you\u2019re using conda you can type this and the package will be downloaded: $ conda install basemap # !conda install basemap # !python -m pip install basemap % matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap from PIL import Image plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = 'i' , lat_0 = 50 , lon_0 =- 100 ) # m.bluemarble(scale=0.9); # plt.show(m) m . bluemarble () m . drawcoastlines () plt . show () Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). from mpl_toolkits.basemap import Basemap import matplotlib.pyplot as plt ma = Basemap ( llcrnrlon =- 10.5 , llcrnrlat = 33 , urcrnrlon = 10. , urcrnrlat = 46. , resolution = 'i' , projection = 'cass' , lat_0 = 39.5 , lon_0 = 0. ) ma . bluemarble () ma . drawcoastlines () plt . show () The useful thing is that the globe shown here is not a mere image; it is a fully func\u2010 tioning Matplotlib axes that understands spherical coordinates and allows us to easily over-plot data on the map! For example, we can use a different map projection, zoom in to North America, and plot the location of Seattle. We\u2019ll use an etopo image (which shows topographical features both on land and under the ocean) as the map back\u2010 ground fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . etopo ( scale = 0.5 , alpha = 0.5 ) # Map (long, lat) to (x,y) for plotting x , y = m ( - 122.3 , 47.6 ) plt . plot ( x , y , 'ok' , markersize = 5 ) plt . text ( x , y , 'Seattle' , fontsize = 12 ); plt . show () warning: width and height keywords ignored for Orthographic projection Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Map Projections \u00b6 The first thing to decide when you are using maps is which projection to use. You\u2019re probably familiar with the fact that it is impossible to project a spherical map, such as that of the Earth, onto a flat surface without somehow distorting it or breaking its continuity. These projections have been developed over the course of human history, and there are a lot of choices! Depending on the intended use of the map projection, there are certain map features (e.g., direction, area, distance, shape, or other consider\u2010 ations) that are useful to maintain. Let\u2019s start by a convenience routine to draw our world map along with the longitude and latitude lines: \u00b6 # from itertools import chain # def draw_map(m, scale=0.2): # #draw a shadded-relief image # m.shadedrelief(scale=scale) # # lats and longs are returned as a dictionary # lats = m.drawparallels(np.linspace(-90,90,13)) # lons = m.drawparallels(np.linspace(-180,180,13)) # # keys contain the plt.Line2D instances # lat_lines=chain(*(tup[1][0] for tup in lats.items())) # lon_lines = chain(*(tup[1][0] for tup in lons.items())) # all_lines = chain(lat_lines, lon_lines) # # cycle throught these lines and set the desird style # for line in all_lines: # line.set(linestyle='-', alpha=0.3, color='w') from itertools import chain def draw_map ( m , scale = 0.2 ): # draw a shaded-relief image m . shadedrelief ( scale = scale ) # lats and longs are returned as a dictionary lats = m . drawparallels ( np . linspace ( - 90 , 90 , 13 )) lons = m . drawmeridians ( np . linspace ( - 180 , 180 , 13 )) # keys contain the plt.Line2D instances lat_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lats . items ())) lon_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lons . items ())) all_lines = chain ( lat_lines , lon_lines ) # cycle through these lines and set the desired style for line in all_lines : line . set ( linestyle = '-' , alpha = 0.3 , color = 'w' ) Cylindrical projections \u00b6 The simplest of map projections are cylindrical projections, in which lines of constant latitude and longitude are mapped to horizontal and vertical lines, respectively. This type of mapping represents equatorial regions quite well, but results in extreme dis\u2010 tortions near the poles. The spacing of latitude lines varies between different cylindri\u2010 cal projections, leading to different conservation properties, and different distortion near the poles. we show an example of the equidistant cylindrical pro\u2010 jection, which chooses a latitude scaling that preserves distances along meridians. Other cylindrical projections are the Mercator (projection=\u2018merc\u2019) and the cylin\u2010 drical equal-area (projection=\u2018cea\u2019) projections. # An equidistant cylinderical projection fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'cyl' , resolution = None , llcrnrlat =- 90 , urcrnrlat = 90 , llcrnrlon =- 180 , urcrnrlon = 180 , ) draw_map ( m ) # The additional arguments to Basemap for this view specify the latitude (lat) and lon\u2010 # gitude (lon) of the lower-left corner (llcrnr) and upper-right corner (urcrnr) for the # desired map, in units of degrees # # An equidistant cylinderical projection # fig = plt.figure(figsize=(8, 6), edgecolor='w') # m = Basemap(projection='merc', resolution=None, # llcrnrlat=-90, urcrnrlat=90, # llcrnrlon=-180, urcrnrlon=180, ) # draw_map(m) # The additional arguments to Basemap for this view specify the latitude (lat) and lon\u2010 # gitude (lon) of the lower-left corner (llcrnr) and upper-right corner (urcrnr) for the # desired map, in units of degrees Pseudo-cylindrical projections \u00b6 Pseudo-cylindrical projections relax the requirement that meridians (lines of constant longitude) remain vertical; this can give better properties near the poles of the projec\u2010 tion. The Mollweide projection (projection=\u2018moll\u2019) is one common example of this, in which all meridians are elliptical arcs. It is constructed so as to preserve area across the map: though there are distortions near the poles, the area of small patches reflects the true area. Other pseudo-cylindrical projections are the sinusoidal (projection=\u2018sinu\u2019) and Robinson (projection=\u2018robin\u2019) projections. fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'moll' , resolution = None , lat_0 = 0 , lon_0 = 0 ) draw_map ( m ) # The extra arguments to Basemap here refer to the central latitude (lat_0) and longi\u2010 # tude (lon_0) for the desired map. Perspective projections \u00b6 Perspective projections are constructed using a particular choice of perspective point, similar to if you photographed the Earth from a particular point in space (a point which, for some projections, technically lies within the Earth!). One common exam\u2010 ple is the orthographic projection (projection=\u2018ortho\u2019), which shows one side of the globe as seen from a viewer at a very long distance. Thus, it can show only half the globe at a time. Other perspective-based projections include the gnomonic projection (projection=\u2018gnom\u2019) and stereographic projection (projection=\u2018stere\u2019). These are often the most useful for showing small portions of the map. fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , lat_0 = 50 , lon_0 = 0 ) draw_map ( m ) Conic projections \u00b6 A conic projection projects the map onto a single cone, which is then unrolled. This can lead to very good local properties, but regions far from the focus point of the cone may become very distorted. One example of this is the Lambert conformal conic projection (projection=\u2018lcc\u2019), which we saw earlier in the map of North America. It projects the map onto a cone arranged in such a way that two standard parallels (specified in Basemap by lat_1 and lat_2) have well-represented distances, with scale decreasing between them and increasing outside of them. Other useful conic projec\u2010 tions are the equidistant conic (projection=\u2018eqdc\u2019) and the Albers equal-area (pro jection=\u2018aea\u2019) projection. Conic projections, like perspective projections, tend to be good choices for representing small to medium patches of the globe fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = None , lon_0 = 0 , lat_0 = 50 , lat_1 = 45 , lat_2 = 55 , width = 1.6E7 , height = 1.2E7 ) draw_map ( m ) Other projections \u00b6 If you\u2019re going to do much with map-based visualizations, I encourage you to read up on other available projections, along with their properties, advantages, and disadvan\u2010 tages. Most likely, they are available in the Basemap package. If you dig deep enough into this topic, you\u2019ll find an incredible subculture of geo-viz geeks who will be ready to argue fervently in support of their favorite projection for any given application! Drawing a Map Background \u00b6 Earlier we saw the bluemarble() and shadedrelief() methods for projecting global images on the map, as well as the drawparallels() and drawmeridians() methods for drawing lines of constant latitude and longitude. The Basemap package contains a range of useful functions for drawing borders of physical features like continents, oceans, lakes, and rivers, as well as political boundaries such as countries and US states and counties. The following are some of the available drawing functions that you may wish to explore using IPython\u2019s help features: \u2022 Physical boundaries and bodies of water drawcoastlines() Draw continental coast lines drawlsmask() Draw a mask between the land and sea, for sea with projecting images on one or the other drawmapboundary() Draw the map boundary, including the fill color for oceans drawrivers() Draw rivers on the map fillcontinents() Fill the continents with a given color; optionally fill lakes with another color \u2022 Political boundaries drawcountries() Draw country boundaries drawstates() Draw US state boundaries drawcounties() Draw US county boundaries \u2022 Map features drawgreatcircle() Draw a great circle between two points drawparallels() Draw lines of constant latitude drawmeridians() Draw lines of constant longitude drawmapscale() Draw a linear scale on the map \u2022 Whole-globe images bluemarble() Project NASA\u2019s blue marble image onto the map shadedrelief() Project a shaded relief image onto the map etopo() Draw an etopo relief image onto the map warpimage() Project a user-provided image onto the map For the boundary-based features, you must set the desired resolution when creating a Basemap image. The resolution argument of the Basemap class sets the level of detail in boundaries, either \u2018c\u2019 (crude), \u2018l\u2019 (low), \u2018i\u2019 (intermediate), \u2018h\u2019 (high), \u2018f\u2019 (full), or None if no boundaries will be used. This choice is important: setting high- resolution boundaries on a global map, for example, can be very slow. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 8 )) for i , res in enumerate ([ 'l' , 'h' ]): m = Basemap ( projection = 'gnom' , lat_0 = 57.3 , lon_0 =- 6.2 , width = 90000 , height = 120000 , resolution = res , ax = ax [ i ]) m . fillcontinents ( color = '#FFDDCC' , lake_color = '#DDEEFF' ) m . drawmapboundary ( fill_color = '#DDEEFF' ) m . drawcoastlines () ax [ i ] . set_title ( \"resolution = ' {0} ''\" . format ( res )); Notice that the low-resolution coastlines are not suitable for this level of zoom, while high-resolution works just fine. The low level would work just fine for a global view, however, and would be much faster than loading the high-resolution border data for the entire globe! It might require some experimentation to find the correct resolution parameter for a given view; the best route is to start with a fast, low-resolution plot and increase the resolution as needed. Plotting Data on Maps \u00b6 Perhaps the most useful piece of the Basemap toolkit is the ability to over-plot a variety of data onto a map background. For simple plotting and text, any plt function works on the map; you can use the Basemap instance to project latitude and longitude coordinates to (x, y) coordinates for plotting with plt, as we saw earlier in the Seattle example. In addition to this, there are many map-specific functions available as methods of the Basemap instance. These work very similarly to their standard Matplotlib counterparts, but have an additional Boolean argument latlon, which if set to True allows you to pass raw latitudes and longitudes to the method, rather than projected (x, y) coordinates. Some of these map-specific methods are: contour()/contourf() Draw contour lines or filled contours imshow() Draw an image pcolor()/pcolormesh() Draw a pseudocolor plot for irregular/regular meshes plot() Draw lines and/or markers scatter() Draw points with markers quiver() Draw vectors barbs() Draw wind barbs drawgreatcircle() Draw a great circle","title":"14 geographic data with basemap"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#14-geographic-data-with-basemap","text":"Geographic Data with Basemap Map Projections Let\u2019s start by a convenience routine to draw our world map along with the longitude and latitude lines: Cylindrical projections Pseudo-cylindrical projections Perspective projections Conic projections Other projections Drawing a Map Background Plotting Data on Maps","title":"14 -  Geographic Data with Basemap"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#geographic-data-with-basemap","text":"One common type of visualization in data science is that of geographic data. Matplot\u2010 lib\u2019s main tool for this type of visualization is the Basemap toolkit, which is one of several Matplotlib toolkits that live under the mpl_toolkits namespace. Admittedly, Basemap feels a bit clunky to use, and often even simple visualizations take much longer to render than you might hope. More modern solutions, such as leaflet or the Google Maps API, may be a better choice for more intensive map visualizations. Still, Basemap is a useful tool for Python users to have in their virtual toolbelts. In this sec\u2010 tion, we\u2019ll show several examples of the type of map visualization that is possible with this toolkit. Installation of Basemap is straightforward; if you\u2019re using conda you can type this and the package will be downloaded: $ conda install basemap # !conda install basemap # !python -m pip install basemap % matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap from PIL import Image plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = 'i' , lat_0 = 50 , lon_0 =- 100 ) # m.bluemarble(scale=0.9); # plt.show(m) m . bluemarble () m . drawcoastlines () plt . show () Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). from mpl_toolkits.basemap import Basemap import matplotlib.pyplot as plt ma = Basemap ( llcrnrlon =- 10.5 , llcrnrlat = 33 , urcrnrlon = 10. , urcrnrlat = 46. , resolution = 'i' , projection = 'cass' , lat_0 = 39.5 , lon_0 = 0. ) ma . bluemarble () ma . drawcoastlines () plt . show () The useful thing is that the globe shown here is not a mere image; it is a fully func\u2010 tioning Matplotlib axes that understands spherical coordinates and allows us to easily over-plot data on the map! For example, we can use a different map projection, zoom in to North America, and plot the location of Seattle. We\u2019ll use an etopo image (which shows topographical features both on land and under the ocean) as the map back\u2010 ground fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . etopo ( scale = 0.5 , alpha = 0.5 ) # Map (long, lat) to (x,y) for plotting x , y = m ( - 122.3 , 47.6 ) plt . plot ( x , y , 'ok' , markersize = 5 ) plt . text ( x , y , 'Seattle' , fontsize = 12 ); plt . show () warning: width and height keywords ignored for Orthographic projection Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).","title":"Geographic Data with Basemap"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#map-projections","text":"The first thing to decide when you are using maps is which projection to use. You\u2019re probably familiar with the fact that it is impossible to project a spherical map, such as that of the Earth, onto a flat surface without somehow distorting it or breaking its continuity. These projections have been developed over the course of human history, and there are a lot of choices! Depending on the intended use of the map projection, there are certain map features (e.g., direction, area, distance, shape, or other consider\u2010 ations) that are useful to maintain.","title":"Map Projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#lets-start-by-a-convenience-routine-to-draw-our-world-map-along-with-the-longitude-and-latitude-lines","text":"# from itertools import chain # def draw_map(m, scale=0.2): # #draw a shadded-relief image # m.shadedrelief(scale=scale) # # lats and longs are returned as a dictionary # lats = m.drawparallels(np.linspace(-90,90,13)) # lons = m.drawparallels(np.linspace(-180,180,13)) # # keys contain the plt.Line2D instances # lat_lines=chain(*(tup[1][0] for tup in lats.items())) # lon_lines = chain(*(tup[1][0] for tup in lons.items())) # all_lines = chain(lat_lines, lon_lines) # # cycle throught these lines and set the desird style # for line in all_lines: # line.set(linestyle='-', alpha=0.3, color='w') from itertools import chain def draw_map ( m , scale = 0.2 ): # draw a shaded-relief image m . shadedrelief ( scale = scale ) # lats and longs are returned as a dictionary lats = m . drawparallels ( np . linspace ( - 90 , 90 , 13 )) lons = m . drawmeridians ( np . linspace ( - 180 , 180 , 13 )) # keys contain the plt.Line2D instances lat_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lats . items ())) lon_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lons . items ())) all_lines = chain ( lat_lines , lon_lines ) # cycle through these lines and set the desired style for line in all_lines : line . set ( linestyle = '-' , alpha = 0.3 , color = 'w' )","title":"Let\u2019s start by a convenience routine to draw our world map along with the longitude and latitude lines:"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#cylindrical-projections","text":"The simplest of map projections are cylindrical projections, in which lines of constant latitude and longitude are mapped to horizontal and vertical lines, respectively. This type of mapping represents equatorial regions quite well, but results in extreme dis\u2010 tortions near the poles. The spacing of latitude lines varies between different cylindri\u2010 cal projections, leading to different conservation properties, and different distortion near the poles. we show an example of the equidistant cylindrical pro\u2010 jection, which chooses a latitude scaling that preserves distances along meridians. Other cylindrical projections are the Mercator (projection=\u2018merc\u2019) and the cylin\u2010 drical equal-area (projection=\u2018cea\u2019) projections. # An equidistant cylinderical projection fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'cyl' , resolution = None , llcrnrlat =- 90 , urcrnrlat = 90 , llcrnrlon =- 180 , urcrnrlon = 180 , ) draw_map ( m ) # The additional arguments to Basemap for this view specify the latitude (lat) and lon\u2010 # gitude (lon) of the lower-left corner (llcrnr) and upper-right corner (urcrnr) for the # desired map, in units of degrees # # An equidistant cylinderical projection # fig = plt.figure(figsize=(8, 6), edgecolor='w') # m = Basemap(projection='merc', resolution=None, # llcrnrlat=-90, urcrnrlat=90, # llcrnrlon=-180, urcrnrlon=180, ) # draw_map(m) # The additional arguments to Basemap for this view specify the latitude (lat) and lon\u2010 # gitude (lon) of the lower-left corner (llcrnr) and upper-right corner (urcrnr) for the # desired map, in units of degrees","title":"Cylindrical projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#pseudo-cylindrical-projections","text":"Pseudo-cylindrical projections relax the requirement that meridians (lines of constant longitude) remain vertical; this can give better properties near the poles of the projec\u2010 tion. The Mollweide projection (projection=\u2018moll\u2019) is one common example of this, in which all meridians are elliptical arcs. It is constructed so as to preserve area across the map: though there are distortions near the poles, the area of small patches reflects the true area. Other pseudo-cylindrical projections are the sinusoidal (projection=\u2018sinu\u2019) and Robinson (projection=\u2018robin\u2019) projections. fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'moll' , resolution = None , lat_0 = 0 , lon_0 = 0 ) draw_map ( m ) # The extra arguments to Basemap here refer to the central latitude (lat_0) and longi\u2010 # tude (lon_0) for the desired map.","title":"Pseudo-cylindrical projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#perspective-projections","text":"Perspective projections are constructed using a particular choice of perspective point, similar to if you photographed the Earth from a particular point in space (a point which, for some projections, technically lies within the Earth!). One common exam\u2010 ple is the orthographic projection (projection=\u2018ortho\u2019), which shows one side of the globe as seen from a viewer at a very long distance. Thus, it can show only half the globe at a time. Other perspective-based projections include the gnomonic projection (projection=\u2018gnom\u2019) and stereographic projection (projection=\u2018stere\u2019). These are often the most useful for showing small portions of the map. fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , lat_0 = 50 , lon_0 = 0 ) draw_map ( m )","title":"Perspective projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#conic-projections","text":"A conic projection projects the map onto a single cone, which is then unrolled. This can lead to very good local properties, but regions far from the focus point of the cone may become very distorted. One example of this is the Lambert conformal conic projection (projection=\u2018lcc\u2019), which we saw earlier in the map of North America. It projects the map onto a cone arranged in such a way that two standard parallels (specified in Basemap by lat_1 and lat_2) have well-represented distances, with scale decreasing between them and increasing outside of them. Other useful conic projec\u2010 tions are the equidistant conic (projection=\u2018eqdc\u2019) and the Albers equal-area (pro jection=\u2018aea\u2019) projection. Conic projections, like perspective projections, tend to be good choices for representing small to medium patches of the globe fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = None , lon_0 = 0 , lat_0 = 50 , lat_1 = 45 , lat_2 = 55 , width = 1.6E7 , height = 1.2E7 ) draw_map ( m )","title":"Conic projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#other-projections","text":"If you\u2019re going to do much with map-based visualizations, I encourage you to read up on other available projections, along with their properties, advantages, and disadvan\u2010 tages. Most likely, they are available in the Basemap package. If you dig deep enough into this topic, you\u2019ll find an incredible subculture of geo-viz geeks who will be ready to argue fervently in support of their favorite projection for any given application!","title":"Other projections"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#drawing-a-map-background","text":"Earlier we saw the bluemarble() and shadedrelief() methods for projecting global images on the map, as well as the drawparallels() and drawmeridians() methods for drawing lines of constant latitude and longitude. The Basemap package contains a range of useful functions for drawing borders of physical features like continents, oceans, lakes, and rivers, as well as political boundaries such as countries and US states and counties. The following are some of the available drawing functions that you may wish to explore using IPython\u2019s help features: \u2022 Physical boundaries and bodies of water drawcoastlines() Draw continental coast lines drawlsmask() Draw a mask between the land and sea, for sea with projecting images on one or the other drawmapboundary() Draw the map boundary, including the fill color for oceans drawrivers() Draw rivers on the map fillcontinents() Fill the continents with a given color; optionally fill lakes with another color \u2022 Political boundaries drawcountries() Draw country boundaries drawstates() Draw US state boundaries drawcounties() Draw US county boundaries \u2022 Map features drawgreatcircle() Draw a great circle between two points drawparallels() Draw lines of constant latitude drawmeridians() Draw lines of constant longitude drawmapscale() Draw a linear scale on the map \u2022 Whole-globe images bluemarble() Project NASA\u2019s blue marble image onto the map shadedrelief() Project a shaded relief image onto the map etopo() Draw an etopo relief image onto the map warpimage() Project a user-provided image onto the map For the boundary-based features, you must set the desired resolution when creating a Basemap image. The resolution argument of the Basemap class sets the level of detail in boundaries, either \u2018c\u2019 (crude), \u2018l\u2019 (low), \u2018i\u2019 (intermediate), \u2018h\u2019 (high), \u2018f\u2019 (full), or None if no boundaries will be used. This choice is important: setting high- resolution boundaries on a global map, for example, can be very slow. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 8 )) for i , res in enumerate ([ 'l' , 'h' ]): m = Basemap ( projection = 'gnom' , lat_0 = 57.3 , lon_0 =- 6.2 , width = 90000 , height = 120000 , resolution = res , ax = ax [ i ]) m . fillcontinents ( color = '#FFDDCC' , lake_color = '#DDEEFF' ) m . drawmapboundary ( fill_color = '#DDEEFF' ) m . drawcoastlines () ax [ i ] . set_title ( \"resolution = ' {0} ''\" . format ( res )); Notice that the low-resolution coastlines are not suitable for this level of zoom, while high-resolution works just fine. The low level would work just fine for a global view, however, and would be much faster than loading the high-resolution border data for the entire globe! It might require some experimentation to find the correct resolution parameter for a given view; the best route is to start with a fast, low-resolution plot and increase the resolution as needed.","title":"Drawing a Map Background"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/#plotting-data-on-maps","text":"Perhaps the most useful piece of the Basemap toolkit is the ability to over-plot a variety of data onto a map background. For simple plotting and text, any plt function works on the map; you can use the Basemap instance to project latitude and longitude coordinates to (x, y) coordinates for plotting with plt, as we saw earlier in the Seattle example. In addition to this, there are many map-specific functions available as methods of the Basemap instance. These work very similarly to their standard Matplotlib counterparts, but have an additional Boolean argument latlon, which if set to True allows you to pass raw latitudes and longitudes to the method, rather than projected (x, y) coordinates. Some of these map-specific methods are: contour()/contourf() Draw contour lines or filled contours imshow() Draw an image pcolor()/pcolormesh() Draw a pseudocolor plot for irregular/regular meshes plot() Draw lines and/or markers scatter() Draw points with markers quiver() Draw vectors barbs() Draw wind barbs drawgreatcircle() Draw a great circle","title":"Plotting Data on Maps"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 15 - Visualization with Seaborn \u00b6 Visualization with Seaborn Seaborn Versus Matplotlib Exploring Seaborn Plots Histograms, KDE, and densities Pair plots Faceted histograms Factor plots Joint distributions Bar plots Visualization with Seaborn \u00b6 Matplotlib has proven to be an incredibly useful and popular visualization tool, but even avid users will admit it often leaves much to be desired. There are several valid complaints about Matplotlib that often come up: \u2022 Prior to version 2.0, Matplotlib\u2019s defaults are not exactly the best choices. It was based off of MATLAB circa 1999, and this often shows. \u2022 Matplotlib\u2019s API is relatively low level. Doing sophisticated statistical visualiza\u2010 tion is possible, but often requires a lot of boilerplate code. \u2022 Matplotlib predated Pandas by more than a decade, and thus is not designed for use with Pandas DataFrames. In order to visualize data from a Pandas DataFrame, you must extract each Series and often concatenate them together into the right format. It would be nicer to have a plotting library that can intelligently use the DataFrame labels in a plot. An answer to these problems is Seaborn. Seaborn provides an API on top of Matplot\u2010 lib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality pro\u2010 vided by Pandas DataFrames. Seaborn Versus Matplotlib \u00b6 Here is an example of a simple random-walk plot in Matplotlib, using its classic plot formatting and colors. import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np import pandas as pd # create some data rng = np . random . RandomState ( 0 ) x = np . linspace ( 0 , 10 , 500 ) y = np . cumsum ( rng . randn ( 500 , 6 ), 0 ) x array([ 0. , 0.02004008, 0.04008016, 0.06012024, 0.08016032, 0.1002004 , 0.12024048, 0.14028056, 0.16032064, 0.18036072, 0.2004008 , 0.22044088, 0.24048096, 0.26052104, 0.28056112, 0.3006012 , 0.32064128, 0.34068136, 0.36072144, 0.38076152, 0.4008016 , 0.42084168, 0.44088176, 0.46092184, 0.48096192, 0.501002 , 0.52104208, 0.54108216, 0.56112224, 0.58116232, 0.6012024 , 0.62124248, 0.64128257, 0.66132265, 0.68136273, 0.70140281, 0.72144289, 0.74148297, 0.76152305, 0.78156313, 0.80160321, 0.82164329, 0.84168337, 0.86172345, 0.88176353, 0.90180361, 0.92184369, 0.94188377, 0.96192385, 0.98196393, 1.00200401, 1.02204409, 1.04208417, 1.06212425, 1.08216433, 1.10220441, 1.12224449, 1.14228457, 1.16232465, 1.18236473, 1.20240481, 1.22244489, 1.24248497, 1.26252505, 1.28256513, 1.30260521, 1.32264529, 1.34268537, 1.36272545, 1.38276553, 1.40280561, 1.42284569, 1.44288577, 1.46292585, 1.48296593, 1.50300601, 1.52304609, 1.54308617, 1.56312625, 1.58316633, 1.60320641, 1.62324649, 1.64328657, 1.66332665, 1.68336673, 1.70340681, 1.72344689, 1.74348697, 1.76352705, 1.78356713, 1.80360721, 1.82364729, 1.84368737, 1.86372745, 1.88376754, 1.90380762, 1.9238477 , 1.94388778, 1.96392786, 1.98396794, 2.00400802, 2.0240481 , 2.04408818, 2.06412826, 2.08416834, 2.10420842, 2.1242485 , 2.14428858, 2.16432866, 2.18436874, 2.20440882, 2.2244489 , 2.24448898, 2.26452906, 2.28456914, 2.30460922, 2.3246493 , 2.34468938, 2.36472946, 2.38476954, 2.40480962, 2.4248497 , 2.44488978, 2.46492986, 2.48496994, 2.50501002, 2.5250501 , 2.54509018, 2.56513026, 2.58517034, 2.60521042, 2.6252505 , 2.64529058, 2.66533066, 2.68537074, 2.70541082, 2.7254509 , 2.74549098, 2.76553106, 2.78557114, 2.80561122, 2.8256513 , 2.84569138, 2.86573146, 2.88577154, 2.90581162, 2.9258517 , 2.94589178, 2.96593186, 2.98597194, 3.00601202, 3.0260521 , 3.04609218, 3.06613226, 3.08617234, 3.10621242, 3.12625251, 3.14629259, 3.16633267, 3.18637275, 3.20641283, 3.22645291, 3.24649299, 3.26653307, 3.28657315, 3.30661323, 3.32665331, 3.34669339, 3.36673347, 3.38677355, 3.40681363, 3.42685371, 3.44689379, 3.46693387, 3.48697395, 3.50701403, 3.52705411, 3.54709419, 3.56713427, 3.58717435, 3.60721443, 3.62725451, 3.64729459, 3.66733467, 3.68737475, 3.70741483, 3.72745491, 3.74749499, 3.76753507, 3.78757515, 3.80761523, 3.82765531, 3.84769539, 3.86773547, 3.88777555, 3.90781563, 3.92785571, 3.94789579, 3.96793587, 3.98797595, 4.00801603, 4.02805611, 4.04809619, 4.06813627, 4.08817635, 4.10821643, 4.12825651, 4.14829659, 4.16833667, 4.18837675, 4.20841683, 4.22845691, 4.24849699, 4.26853707, 4.28857715, 4.30861723, 4.32865731, 4.34869739, 4.36873747, 4.38877756, 4.40881764, 4.42885772, 4.4488978 , 4.46893788, 4.48897796, 4.50901804, 4.52905812, 4.5490982 , 4.56913828, 4.58917836, 4.60921844, 4.62925852, 4.6492986 , 4.66933868, 4.68937876, 4.70941884, 4.72945892, 4.749499 , 4.76953908, 4.78957916, 4.80961924, 4.82965932, 4.8496994 , 4.86973948, 4.88977956, 4.90981964, 4.92985972, 4.9498998 , 4.96993988, 4.98997996, 5.01002004, 5.03006012, 5.0501002 , 5.07014028, 5.09018036, 5.11022044, 5.13026052, 5.1503006 , 5.17034068, 5.19038076, 5.21042084, 5.23046092, 5.250501 , 5.27054108, 5.29058116, 5.31062124, 5.33066132, 5.3507014 , 5.37074148, 5.39078156, 5.41082164, 5.43086172, 5.4509018 , 5.47094188, 5.49098196, 5.51102204, 5.53106212, 5.5511022 , 5.57114228, 5.59118236, 5.61122244, 5.63126253, 5.65130261, 5.67134269, 5.69138277, 5.71142285, 5.73146293, 5.75150301, 5.77154309, 5.79158317, 5.81162325, 5.83166333, 5.85170341, 5.87174349, 5.89178357, 5.91182365, 5.93186373, 5.95190381, 5.97194389, 5.99198397, 6.01202405, 6.03206413, 6.05210421, 6.07214429, 6.09218437, 6.11222445, 6.13226453, 6.15230461, 6.17234469, 6.19238477, 6.21242485, 6.23246493, 6.25250501, 6.27254509, 6.29258517, 6.31262525, 6.33266533, 6.35270541, 6.37274549, 6.39278557, 6.41282565, 6.43286573, 6.45290581, 6.47294589, 6.49298597, 6.51302605, 6.53306613, 6.55310621, 6.57314629, 6.59318637, 6.61322645, 6.63326653, 6.65330661, 6.67334669, 6.69338677, 6.71342685, 6.73346693, 6.75350701, 6.77354709, 6.79358717, 6.81362725, 6.83366733, 6.85370741, 6.87374749, 6.89378758, 6.91382766, 6.93386774, 6.95390782, 6.9739479 , 6.99398798, 7.01402806, 7.03406814, 7.05410822, 7.0741483 , 7.09418838, 7.11422846, 7.13426854, 7.15430862, 7.1743487 , 7.19438878, 7.21442886, 7.23446894, 7.25450902, 7.2745491 , 7.29458918, 7.31462926, 7.33466934, 7.35470942, 7.3747495 , 7.39478958, 7.41482966, 7.43486974, 7.45490982, 7.4749499 , 7.49498998, 7.51503006, 7.53507014, 7.55511022, 7.5751503 , 7.59519038, 7.61523046, 7.63527054, 7.65531062, 7.6753507 , 7.69539078, 7.71543086, 7.73547094, 7.75551102, 7.7755511 , 7.79559118, 7.81563126, 7.83567134, 7.85571142, 7.8757515 , 7.89579158, 7.91583166, 7.93587174, 7.95591182, 7.9759519 , 7.99599198, 8.01603206, 8.03607214, 8.05611222, 8.0761523 , 8.09619238, 8.11623246, 8.13627255, 8.15631263, 8.17635271, 8.19639279, 8.21643287, 8.23647295, 8.25651303, 8.27655311, 8.29659319, 8.31663327, 8.33667335, 8.35671343, 8.37675351, 8.39679359, 8.41683367, 8.43687375, 8.45691383, 8.47695391, 8.49699399, 8.51703407, 8.53707415, 8.55711423, 8.57715431, 8.59719439, 8.61723447, 8.63727455, 8.65731463, 8.67735471, 8.69739479, 8.71743487, 8.73747495, 8.75751503, 8.77755511, 8.79759519, 8.81763527, 8.83767535, 8.85771543, 8.87775551, 8.89779559, 8.91783567, 8.93787575, 8.95791583, 8.97795591, 8.99799599, 9.01803607, 9.03807615, 9.05811623, 9.07815631, 9.09819639, 9.11823647, 9.13827655, 9.15831663, 9.17835671, 9.19839679, 9.21843687, 9.23847695, 9.25851703, 9.27855711, 9.29859719, 9.31863727, 9.33867735, 9.35871743, 9.37875752, 9.3987976 , 9.41883768, 9.43887776, 9.45891784, 9.47895792, 9.498998 , 9.51903808, 9.53907816, 9.55911824, 9.57915832, 9.5991984 , 9.61923848, 9.63927856, 9.65931864, 9.67935872, 9.6993988 , 9.71943888, 9.73947896, 9.75951904, 9.77955912, 9.7995992 , 9.81963928, 9.83967936, 9.85971944, 9.87975952, 9.8997996 , 9.91983968, 9.93987976, 9.95991984, 9.97995992, 10. ]) y array([[ 1.76405235, 0.40015721, 0.97873798, 2.2408932 , 1.86755799, -0.97727788], [ 2.71414076, 0.2488 , 0.87551913, 2.6514917 , 2.01160156, 0.47699563], [ 3.47517849, 0.37047502, 1.31938237, 2.98516603, 3.50568063, 0.27183736], ..., [-34.82533536, -44.37245964, -32.86660099, 31.93843765, 9.67250307, -9.16537805], [-35.4875268 , -45.95006671, -33.20716103, 30.63521756, 10.13925372, -9.00427173], [-35.16749487, -43.87089005, -34.11462701, 30.44281336, 8.92673797, -9.08487024]]) # plot the data usng matplotlib defaults plt . plot ( x , y ) plt . legend ( \"ABCDEG\" , ncol = 2 , loc = 'upper left' ) <matplotlib.legend.Legend at 0x7f9257fd7e50> Although the result contains all the information we\u2019d like it to convey, it does so in a way that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of 21 st -century data visualization. import seaborn as sns sns . set () # some plotting code as above plt . plot ( x , y ) plt . legend ( \"ABCDEF\" , ncol = 2 , loc = 'upper left' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2 warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\" <matplotlib.legend.Legend at 0x7f9253d88220> Exploring Seaborn Plots \u00b6 The main idea of Seaborn is that it provides high-level commands to create a variety of plot types useful for statistical data exploration, and even some statistical model fitting. Let\u2019s take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood), but the Seaborn API is much more convenient. Histograms, KDE, and densities \u00b6 Often in statistical data visualization, all you want is to plot histograms and joint dis\u2010 tributions of variables. We have seen that this is relatively straightforward in Matplot\u2010 lib ( data = np . random . multivariate_normal ([ 0 , 0 ],[[ 5 , 2 ],[ 2 , 2 ]], size = 2000 ) data = pd . DataFrame ( data , columns = [ 'x' , 'y' ]) for col in 'xy' : plt . hist ( data [ col ], alpha = 0.5 ) Rather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with sns.kdeplot for col in 'xy' : sns . kdeplot ( data [ col ], shade = True ) Histograms and KDE can be combined using displot sns . distplot ( data [ 'x' ]) sns . distplot ( data [ 'y' ]); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) If we pass the full two-dimensional dataset to kdeplot, we will get a two-dimensional visualization of the data sns . kdeplot ( data ); ValueError: If using all scalar values, you must pass an index We can see the joint distribution and the marginal distributions together using sns.jointplot. For this plot, we\u2019ll set the style to a white background with sns . axes_style ( 'white' ): sns . jointplot ( 'x' , 'y' , data , kind = 'kde' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, data. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( There are other parameters that can be passed to jointplot\u2014for example, we can use a hexagonally based histogram instead with sns . axes_style ( 'white' ): sns . jointplot ( \"x\" , \"y\" , data , kind = 'hex' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, data. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( Pair plots \u00b6 When you generalize joint plots to datasets of larger dimensions, you end up with pair plots. This is very useful for exploring correlations between multidimensional data, when you\u2019d like to plot all pairs of values against each other. We\u2019ll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species: iris = sns . load_dataset ( 'iris' ) iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa # visulizing the multidimensional relationships among the samples is # as easy sns . pairplot ( iris , hue = 'species' , size = 2.5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/axisgrid.py:2076: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) <seaborn.axisgrid.PairGrid at 0x7f923c0e5910> Faceted histograms \u00b6 Sometimes the best way to view data is via histograms of subsets. Seaborn\u2019s FacetGrid makes this extremely simple. We\u2019ll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data tips = sns . load_dataset ( 'tips' ) tips . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 tips [ 'tip_pct' ] = 100 * tips [ 'tip' ] / tips [ 'total_bill' ] tips [ 'tip_pct' ] 0 5.944673 1 16.054159 2 16.658734 3 13.978041 4 14.680765 ... 239 20.392697 240 7.358352 241 8.822232 242 9.820426 243 15.974441 Name: tip_pct, Length: 244, dtype: float64 grid = sns . FacetGrid ( tips , row = 'sex' , col = 'time' , margin_titles = True ) grid . map ( plt . hist , \"tip_pct\" , bins = np . linspace ( 0 , 40 , 15 )); Factor plots \u00b6 Factor plots can be useful for this kind of visualization as well. This allows you to view the distribution of a parameter within bins defined by any other parameter with sns . axes_style ( style = 'ticks' ): g = sns . factorplot ( 'day' , 'total_bill' , 'sex' , data = tips , kind = 'box' ) g . set_axis_labels ( 'Day' , 'Total Bill' ); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, hue. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( Joint distributions \u00b6 Similar to the pair plot we saw earlier, we can use sns.jointplot to show the joint distribution between different datasets, along with the associated marginal distribu\u2010 tions with sns . axes_style ( 'white' ): sns . jointplot ( 'total_bill' , 'tip' , data = tips , kind = 'hex' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( # the joint plot can also do some automatic kernel density nd regression sns . jointplot ( 'total_bill' , 'tip' , data = tips , kind = 'reg' ); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( Bar plots \u00b6 Time series can be plotted with sns.factorplot planets = sns . load_dataset ( 'planets' ) planets . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 with sns . axes_style ( 'white' ): g = sns . factorplot ( 'year' , data = planets , aspect = 2 , kind = 'count' , color = 'steelblue' ) g . set_xticklabels ( step = 5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( # we can learn more by looking at the method of discovery of each of these planets with sns . axes_style ( 'white' ): g = sns . factorplot ( 'year' , data = planets , aspect = 2 , kind = 'count' , color = 'steelblue' , hue = 'method' , order = range ( 2001 , 2015 )) g . set_ylabels ( 'Number of Planets Discovered' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"15visualiztion with seaborn"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#15-visualization-with-seaborn","text":"Visualization with Seaborn Seaborn Versus Matplotlib Exploring Seaborn Plots Histograms, KDE, and densities Pair plots Faceted histograms Factor plots Joint distributions Bar plots","title":"15 -  Visualization with Seaborn"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#visualization-with-seaborn","text":"Matplotlib has proven to be an incredibly useful and popular visualization tool, but even avid users will admit it often leaves much to be desired. There are several valid complaints about Matplotlib that often come up: \u2022 Prior to version 2.0, Matplotlib\u2019s defaults are not exactly the best choices. It was based off of MATLAB circa 1999, and this often shows. \u2022 Matplotlib\u2019s API is relatively low level. Doing sophisticated statistical visualiza\u2010 tion is possible, but often requires a lot of boilerplate code. \u2022 Matplotlib predated Pandas by more than a decade, and thus is not designed for use with Pandas DataFrames. In order to visualize data from a Pandas DataFrame, you must extract each Series and often concatenate them together into the right format. It would be nicer to have a plotting library that can intelligently use the DataFrame labels in a plot. An answer to these problems is Seaborn. Seaborn provides an API on top of Matplot\u2010 lib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality pro\u2010 vided by Pandas DataFrames.","title":"Visualization with Seaborn"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#seaborn-versus-matplotlib","text":"Here is an example of a simple random-walk plot in Matplotlib, using its classic plot formatting and colors. import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np import pandas as pd # create some data rng = np . random . RandomState ( 0 ) x = np . linspace ( 0 , 10 , 500 ) y = np . cumsum ( rng . randn ( 500 , 6 ), 0 ) x array([ 0. , 0.02004008, 0.04008016, 0.06012024, 0.08016032, 0.1002004 , 0.12024048, 0.14028056, 0.16032064, 0.18036072, 0.2004008 , 0.22044088, 0.24048096, 0.26052104, 0.28056112, 0.3006012 , 0.32064128, 0.34068136, 0.36072144, 0.38076152, 0.4008016 , 0.42084168, 0.44088176, 0.46092184, 0.48096192, 0.501002 , 0.52104208, 0.54108216, 0.56112224, 0.58116232, 0.6012024 , 0.62124248, 0.64128257, 0.66132265, 0.68136273, 0.70140281, 0.72144289, 0.74148297, 0.76152305, 0.78156313, 0.80160321, 0.82164329, 0.84168337, 0.86172345, 0.88176353, 0.90180361, 0.92184369, 0.94188377, 0.96192385, 0.98196393, 1.00200401, 1.02204409, 1.04208417, 1.06212425, 1.08216433, 1.10220441, 1.12224449, 1.14228457, 1.16232465, 1.18236473, 1.20240481, 1.22244489, 1.24248497, 1.26252505, 1.28256513, 1.30260521, 1.32264529, 1.34268537, 1.36272545, 1.38276553, 1.40280561, 1.42284569, 1.44288577, 1.46292585, 1.48296593, 1.50300601, 1.52304609, 1.54308617, 1.56312625, 1.58316633, 1.60320641, 1.62324649, 1.64328657, 1.66332665, 1.68336673, 1.70340681, 1.72344689, 1.74348697, 1.76352705, 1.78356713, 1.80360721, 1.82364729, 1.84368737, 1.86372745, 1.88376754, 1.90380762, 1.9238477 , 1.94388778, 1.96392786, 1.98396794, 2.00400802, 2.0240481 , 2.04408818, 2.06412826, 2.08416834, 2.10420842, 2.1242485 , 2.14428858, 2.16432866, 2.18436874, 2.20440882, 2.2244489 , 2.24448898, 2.26452906, 2.28456914, 2.30460922, 2.3246493 , 2.34468938, 2.36472946, 2.38476954, 2.40480962, 2.4248497 , 2.44488978, 2.46492986, 2.48496994, 2.50501002, 2.5250501 , 2.54509018, 2.56513026, 2.58517034, 2.60521042, 2.6252505 , 2.64529058, 2.66533066, 2.68537074, 2.70541082, 2.7254509 , 2.74549098, 2.76553106, 2.78557114, 2.80561122, 2.8256513 , 2.84569138, 2.86573146, 2.88577154, 2.90581162, 2.9258517 , 2.94589178, 2.96593186, 2.98597194, 3.00601202, 3.0260521 , 3.04609218, 3.06613226, 3.08617234, 3.10621242, 3.12625251, 3.14629259, 3.16633267, 3.18637275, 3.20641283, 3.22645291, 3.24649299, 3.26653307, 3.28657315, 3.30661323, 3.32665331, 3.34669339, 3.36673347, 3.38677355, 3.40681363, 3.42685371, 3.44689379, 3.46693387, 3.48697395, 3.50701403, 3.52705411, 3.54709419, 3.56713427, 3.58717435, 3.60721443, 3.62725451, 3.64729459, 3.66733467, 3.68737475, 3.70741483, 3.72745491, 3.74749499, 3.76753507, 3.78757515, 3.80761523, 3.82765531, 3.84769539, 3.86773547, 3.88777555, 3.90781563, 3.92785571, 3.94789579, 3.96793587, 3.98797595, 4.00801603, 4.02805611, 4.04809619, 4.06813627, 4.08817635, 4.10821643, 4.12825651, 4.14829659, 4.16833667, 4.18837675, 4.20841683, 4.22845691, 4.24849699, 4.26853707, 4.28857715, 4.30861723, 4.32865731, 4.34869739, 4.36873747, 4.38877756, 4.40881764, 4.42885772, 4.4488978 , 4.46893788, 4.48897796, 4.50901804, 4.52905812, 4.5490982 , 4.56913828, 4.58917836, 4.60921844, 4.62925852, 4.6492986 , 4.66933868, 4.68937876, 4.70941884, 4.72945892, 4.749499 , 4.76953908, 4.78957916, 4.80961924, 4.82965932, 4.8496994 , 4.86973948, 4.88977956, 4.90981964, 4.92985972, 4.9498998 , 4.96993988, 4.98997996, 5.01002004, 5.03006012, 5.0501002 , 5.07014028, 5.09018036, 5.11022044, 5.13026052, 5.1503006 , 5.17034068, 5.19038076, 5.21042084, 5.23046092, 5.250501 , 5.27054108, 5.29058116, 5.31062124, 5.33066132, 5.3507014 , 5.37074148, 5.39078156, 5.41082164, 5.43086172, 5.4509018 , 5.47094188, 5.49098196, 5.51102204, 5.53106212, 5.5511022 , 5.57114228, 5.59118236, 5.61122244, 5.63126253, 5.65130261, 5.67134269, 5.69138277, 5.71142285, 5.73146293, 5.75150301, 5.77154309, 5.79158317, 5.81162325, 5.83166333, 5.85170341, 5.87174349, 5.89178357, 5.91182365, 5.93186373, 5.95190381, 5.97194389, 5.99198397, 6.01202405, 6.03206413, 6.05210421, 6.07214429, 6.09218437, 6.11222445, 6.13226453, 6.15230461, 6.17234469, 6.19238477, 6.21242485, 6.23246493, 6.25250501, 6.27254509, 6.29258517, 6.31262525, 6.33266533, 6.35270541, 6.37274549, 6.39278557, 6.41282565, 6.43286573, 6.45290581, 6.47294589, 6.49298597, 6.51302605, 6.53306613, 6.55310621, 6.57314629, 6.59318637, 6.61322645, 6.63326653, 6.65330661, 6.67334669, 6.69338677, 6.71342685, 6.73346693, 6.75350701, 6.77354709, 6.79358717, 6.81362725, 6.83366733, 6.85370741, 6.87374749, 6.89378758, 6.91382766, 6.93386774, 6.95390782, 6.9739479 , 6.99398798, 7.01402806, 7.03406814, 7.05410822, 7.0741483 , 7.09418838, 7.11422846, 7.13426854, 7.15430862, 7.1743487 , 7.19438878, 7.21442886, 7.23446894, 7.25450902, 7.2745491 , 7.29458918, 7.31462926, 7.33466934, 7.35470942, 7.3747495 , 7.39478958, 7.41482966, 7.43486974, 7.45490982, 7.4749499 , 7.49498998, 7.51503006, 7.53507014, 7.55511022, 7.5751503 , 7.59519038, 7.61523046, 7.63527054, 7.65531062, 7.6753507 , 7.69539078, 7.71543086, 7.73547094, 7.75551102, 7.7755511 , 7.79559118, 7.81563126, 7.83567134, 7.85571142, 7.8757515 , 7.89579158, 7.91583166, 7.93587174, 7.95591182, 7.9759519 , 7.99599198, 8.01603206, 8.03607214, 8.05611222, 8.0761523 , 8.09619238, 8.11623246, 8.13627255, 8.15631263, 8.17635271, 8.19639279, 8.21643287, 8.23647295, 8.25651303, 8.27655311, 8.29659319, 8.31663327, 8.33667335, 8.35671343, 8.37675351, 8.39679359, 8.41683367, 8.43687375, 8.45691383, 8.47695391, 8.49699399, 8.51703407, 8.53707415, 8.55711423, 8.57715431, 8.59719439, 8.61723447, 8.63727455, 8.65731463, 8.67735471, 8.69739479, 8.71743487, 8.73747495, 8.75751503, 8.77755511, 8.79759519, 8.81763527, 8.83767535, 8.85771543, 8.87775551, 8.89779559, 8.91783567, 8.93787575, 8.95791583, 8.97795591, 8.99799599, 9.01803607, 9.03807615, 9.05811623, 9.07815631, 9.09819639, 9.11823647, 9.13827655, 9.15831663, 9.17835671, 9.19839679, 9.21843687, 9.23847695, 9.25851703, 9.27855711, 9.29859719, 9.31863727, 9.33867735, 9.35871743, 9.37875752, 9.3987976 , 9.41883768, 9.43887776, 9.45891784, 9.47895792, 9.498998 , 9.51903808, 9.53907816, 9.55911824, 9.57915832, 9.5991984 , 9.61923848, 9.63927856, 9.65931864, 9.67935872, 9.6993988 , 9.71943888, 9.73947896, 9.75951904, 9.77955912, 9.7995992 , 9.81963928, 9.83967936, 9.85971944, 9.87975952, 9.8997996 , 9.91983968, 9.93987976, 9.95991984, 9.97995992, 10. ]) y array([[ 1.76405235, 0.40015721, 0.97873798, 2.2408932 , 1.86755799, -0.97727788], [ 2.71414076, 0.2488 , 0.87551913, 2.6514917 , 2.01160156, 0.47699563], [ 3.47517849, 0.37047502, 1.31938237, 2.98516603, 3.50568063, 0.27183736], ..., [-34.82533536, -44.37245964, -32.86660099, 31.93843765, 9.67250307, -9.16537805], [-35.4875268 , -45.95006671, -33.20716103, 30.63521756, 10.13925372, -9.00427173], [-35.16749487, -43.87089005, -34.11462701, 30.44281336, 8.92673797, -9.08487024]]) # plot the data usng matplotlib defaults plt . plot ( x , y ) plt . legend ( \"ABCDEG\" , ncol = 2 , loc = 'upper left' ) <matplotlib.legend.Legend at 0x7f9257fd7e50> Although the result contains all the information we\u2019d like it to convey, it does so in a way that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of 21 st -century data visualization. import seaborn as sns sns . set () # some plotting code as above plt . plot ( x , y ) plt . legend ( \"ABCDEF\" , ncol = 2 , loc = 'upper left' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2 warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\" <matplotlib.legend.Legend at 0x7f9253d88220>","title":"Seaborn Versus Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#exploring-seaborn-plots","text":"The main idea of Seaborn is that it provides high-level commands to create a variety of plot types useful for statistical data exploration, and even some statistical model fitting. Let\u2019s take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood), but the Seaborn API is much more convenient.","title":"Exploring Seaborn Plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#histograms-kde-and-densities","text":"Often in statistical data visualization, all you want is to plot histograms and joint dis\u2010 tributions of variables. We have seen that this is relatively straightforward in Matplot\u2010 lib ( data = np . random . multivariate_normal ([ 0 , 0 ],[[ 5 , 2 ],[ 2 , 2 ]], size = 2000 ) data = pd . DataFrame ( data , columns = [ 'x' , 'y' ]) for col in 'xy' : plt . hist ( data [ col ], alpha = 0.5 ) Rather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with sns.kdeplot for col in 'xy' : sns . kdeplot ( data [ col ], shade = True ) Histograms and KDE can be combined using displot sns . distplot ( data [ 'x' ]) sns . distplot ( data [ 'y' ]); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) If we pass the full two-dimensional dataset to kdeplot, we will get a two-dimensional visualization of the data sns . kdeplot ( data ); ValueError: If using all scalar values, you must pass an index We can see the joint distribution and the marginal distributions together using sns.jointplot. For this plot, we\u2019ll set the style to a white background with sns . axes_style ( 'white' ): sns . jointplot ( 'x' , 'y' , data , kind = 'kde' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, data. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( There are other parameters that can be passed to jointplot\u2014for example, we can use a hexagonally based histogram instead with sns . axes_style ( 'white' ): sns . jointplot ( \"x\" , \"y\" , data , kind = 'hex' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, data. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"Histograms, KDE, and densities"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#pair-plots","text":"When you generalize joint plots to datasets of larger dimensions, you end up with pair plots. This is very useful for exploring correlations between multidimensional data, when you\u2019d like to plot all pairs of values against each other. We\u2019ll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species: iris = sns . load_dataset ( 'iris' ) iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa # visulizing the multidimensional relationships among the samples is # as easy sns . pairplot ( iris , hue = 'species' , size = 2.5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/axisgrid.py:2076: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) <seaborn.axisgrid.PairGrid at 0x7f923c0e5910>","title":"Pair plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#faceted-histograms","text":"Sometimes the best way to view data is via histograms of subsets. Seaborn\u2019s FacetGrid makes this extremely simple. We\u2019ll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data tips = sns . load_dataset ( 'tips' ) tips . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 tips [ 'tip_pct' ] = 100 * tips [ 'tip' ] / tips [ 'total_bill' ] tips [ 'tip_pct' ] 0 5.944673 1 16.054159 2 16.658734 3 13.978041 4 14.680765 ... 239 20.392697 240 7.358352 241 8.822232 242 9.820426 243 15.974441 Name: tip_pct, Length: 244, dtype: float64 grid = sns . FacetGrid ( tips , row = 'sex' , col = 'time' , margin_titles = True ) grid . map ( plt . hist , \"tip_pct\" , bins = np . linspace ( 0 , 40 , 15 ));","title":"Faceted histograms"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#factor-plots","text":"Factor plots can be useful for this kind of visualization as well. This allows you to view the distribution of a parameter within bins defined by any other parameter with sns . axes_style ( style = 'ticks' ): g = sns . factorplot ( 'day' , 'total_bill' , 'sex' , data = tips , kind = 'box' ) g . set_axis_labels ( 'Day' , 'Total Bill' ); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, hue. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"Factor plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#joint-distributions","text":"Similar to the pair plot we saw earlier, we can use sns.jointplot to show the joint distribution between different datasets, along with the associated marginal distribu\u2010 tions with sns . axes_style ( 'white' ): sns . jointplot ( 'total_bill' , 'tip' , data = tips , kind = 'hex' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( # the joint plot can also do some automatic kernel density nd regression sns . jointplot ( 'total_bill' , 'tip' , data = tips , kind = 'reg' ); /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"Joint distributions"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/#bar-plots","text":"Time series can be plotted with sns.factorplot planets = sns . load_dataset ( 'planets' ) planets . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 with sns . axes_style ( 'white' ): g = sns . factorplot ( 'year' , data = planets , aspect = 2 , kind = 'count' , color = 'steelblue' ) g . set_xticklabels ( step = 5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( # we can learn more by looking at the method of discovery of each of these planets with sns . axes_style ( 'white' ): g = sns . factorplot ( 'year' , data = planets , aspect = 2 , kind = 'count' , color = 'steelblue' , hue = 'method' , order = range ( 2001 , 2015 )) g . set_ylabels ( 'Number of Planets Discovered' ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`. warnings.warn(msg) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn(","title":"Bar plots"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: Exploring Marathon Finishing Times \u00b6 Example: Exploring Marathon Finishing Times Example: Exploring Marathon Finishing Times \u00b6 Here we\u2019ll look at using Seaborn to help visualize and understand finishing results from a marathon. I\u2019ve scraped the data from sources on the Web, aggregated it and removed any identifying information, and put it on GitHub where it can be downloa\u2010 ded (if you are interested in using Python for web scraping, I would recommend Web Scraping with Python by Ryan Mitchell). We will start by downloading the data from the Web, and loading it into Pandas: # !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 836k 100 836k 0 0 9808 0 0:01:27 0:01:27 --:--:-- 11084 13626 0 0:01:02 0:00:11 0:00:51 10234 import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np import pandas as pd ! ls '01general Matplotlib tips.ipynb' 02simple_lineplots.ipynb '03simple scatter plots.ipynb' '04visualizing errors.ipynb' '05density and contour plots.ipynb' '06Histograms Binnings and Density.ipynb' '07customized plot legends.ipynb' '08customizing colorbar.ipynb' '09multiple subplots.ipynb' '10text and annotation Example.ipynb' '11customizing ticks.ipynb' '12customizing matplotlib configuration and stylesheets.ipynb' '13threedimensional plotting.ipynb' '14_geographic data with basemap.ipynb' '15visualiztion with seaborn.ipynb' cos_sinplots.png 'example California cities.ipynb' 'Example Exploring Marathon Finishing times.ipynb' 'Example Handwritten Digits.ipynb' 'example surface temperature data.ipynb' 'Example Visualizing a Mobius Strip.ipynb' gistemp250.nc.gz marathon-data.csv data = pd . read_csv ( 'marathon-data.csv' ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age gender split final 0 33 M 01:05:38 02:08:51 1 32 M 01:06:26 02:09:28 2 31 M 01:06:49 02:10:42 3 38 M 01:06:16 02:13:45 4 31 M 01:06:32 02:13:59 data . dtypes age int64 gender object split object final object dtype: object # lets convert split and final to times def convert_time ( s ): h , m , s = map ( int , s . split ( ':' )) return pd . datetools . timedelta ( hours = h , minutes = m , seconds = s ) data = pd . read_csv ( 'marathon-data.csv' , converters = { 'split' : convert_time , 'final' : convert_time }) data . head () AttributeError: module 'pandas' has no attribute 'datetools'","title":"Example Exploring Marathon Finishing times"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/#example-exploring-marathon-finishing-times","text":"Example: Exploring Marathon Finishing Times","title":"Example: Exploring Marathon Finishing Times"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/#example-exploring-marathon-finishing-times_1","text":"Here we\u2019ll look at using Seaborn to help visualize and understand finishing results from a marathon. I\u2019ve scraped the data from sources on the Web, aggregated it and removed any identifying information, and put it on GitHub where it can be downloa\u2010 ded (if you are interested in using Python for web scraping, I would recommend Web Scraping with Python by Ryan Mitchell). We will start by downloading the data from the Web, and loading it into Pandas: # !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 836k 100 836k 0 0 9808 0 0:01:27 0:01:27 --:--:-- 11084 13626 0 0:01:02 0:00:11 0:00:51 10234 import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np import pandas as pd ! ls '01general Matplotlib tips.ipynb' 02simple_lineplots.ipynb '03simple scatter plots.ipynb' '04visualizing errors.ipynb' '05density and contour plots.ipynb' '06Histograms Binnings and Density.ipynb' '07customized plot legends.ipynb' '08customizing colorbar.ipynb' '09multiple subplots.ipynb' '10text and annotation Example.ipynb' '11customizing ticks.ipynb' '12customizing matplotlib configuration and stylesheets.ipynb' '13threedimensional plotting.ipynb' '14_geographic data with basemap.ipynb' '15visualiztion with seaborn.ipynb' cos_sinplots.png 'example California cities.ipynb' 'Example Exploring Marathon Finishing times.ipynb' 'Example Handwritten Digits.ipynb' 'example surface temperature data.ipynb' 'Example Visualizing a Mobius Strip.ipynb' gistemp250.nc.gz marathon-data.csv data = pd . read_csv ( 'marathon-data.csv' ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age gender split final 0 33 M 01:05:38 02:08:51 1 32 M 01:06:26 02:09:28 2 31 M 01:06:49 02:10:42 3 38 M 01:06:16 02:13:45 4 31 M 01:06:32 02:13:59 data . dtypes age int64 gender object split object final object dtype: object # lets convert split and final to times def convert_time ( s ): h , m , s = map ( int , s . split ( ':' )) return pd . datetools . timedelta ( hours = h , minutes = m , seconds = s ) data = pd . read_csv ( 'marathon-data.csv' , converters = { 'split' : convert_time , 'final' : convert_time }) data . head () AttributeError: module 'pandas' has no attribute 'datetools'","title":"Example: Exploring Marathon Finishing Times"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: Handwritten Digits \u00b6 Example: Handwritten Digits Example: Handwritten Digits \u00b6 For an example of where this might be useful, let\u2019s look at an interesting visualization of some handwritten digits data. This data is included in Scikit-Learn, and consists of nearly 2,000 8\u00d78 thumbnails showing various handwritten digits. For now, let\u2019s start by downloading the digits data and visualizing several of the exam\u2010 ple images with plt.imshow() import matplotlib.pyplot as plt # load images of the digit 0 through 5 and visualize several of them from sklearn.datasets import load_digits digits = load_digits ( n_class = 6 ) fig , ax = plt . subplots ( 8 , 8 , figsize = ( 6 , 6 )) for i , axi in enumerate ( ax . flat ): axi . imshow ( digits . images [ i ], cmap = 'binary' ) axi . set ( xticks = [], yticks = []) Because each digit is defined by the hue of its 64 pixels, we can consider each digit to be a point lying in 64-dimensional space: each dimension represents the brightness of one pixel. But visualizing relationships in such high-dimensional spaces can be extremely difficult. One way to approach this is to use a dimensionality reduction technique such as manifold learning to reduce the dimensionality of the data while maintaining the relationships of interest. Dimensionality reduction is an example of unsupervised machine learning Deferring the discussion of these details, let\u2019s take a look at a two-dimensional mani\u2010 fold learning projection of this digits data from sklearn.manifold import Isomap iso = Isomap ( n_components = 2 ) projection = iso . fit_transform ( digits . data ) # plot the results plt . scatter ( projection [:, 0 ], projection [:, 1 ], lw = 0.1 , c = digits . target , cmap = plt . cm . get_cmap ( 'cubehelix' , 6 )) plt . colorbar ( ticks = range ( 6 ), label = 'digit value' ) plt . clim ( - 0.5 , 5.5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:348: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue. self._fit_transform(X) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient. self._set_intXint(row, col, x.flat[0]) The projection also gives us some interesting insights on the relationships within the dataset: for example, the ranges of 5 and 3 nearly overlap in this projection, indicating that some handwritten fives and threes are difficult to distinguish, and therefore more likely to be confused by an automated classification algorithm. Other values, like 0 and 1, are more distantly separated, and therefore much less likely to be con\u2010 fused. This observation agrees with our intuition, because 5 and 3 look much more similar than do 0 and 1.","title":"Example Handwritten Digits"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/#example-handwritten-digits","text":"Example: Handwritten Digits","title":"Example: Handwritten Digits"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/#example-handwritten-digits_1","text":"For an example of where this might be useful, let\u2019s look at an interesting visualization of some handwritten digits data. This data is included in Scikit-Learn, and consists of nearly 2,000 8\u00d78 thumbnails showing various handwritten digits. For now, let\u2019s start by downloading the digits data and visualizing several of the exam\u2010 ple images with plt.imshow() import matplotlib.pyplot as plt # load images of the digit 0 through 5 and visualize several of them from sklearn.datasets import load_digits digits = load_digits ( n_class = 6 ) fig , ax = plt . subplots ( 8 , 8 , figsize = ( 6 , 6 )) for i , axi in enumerate ( ax . flat ): axi . imshow ( digits . images [ i ], cmap = 'binary' ) axi . set ( xticks = [], yticks = []) Because each digit is defined by the hue of its 64 pixels, we can consider each digit to be a point lying in 64-dimensional space: each dimension represents the brightness of one pixel. But visualizing relationships in such high-dimensional spaces can be extremely difficult. One way to approach this is to use a dimensionality reduction technique such as manifold learning to reduce the dimensionality of the data while maintaining the relationships of interest. Dimensionality reduction is an example of unsupervised machine learning Deferring the discussion of these details, let\u2019s take a look at a two-dimensional mani\u2010 fold learning projection of this digits data from sklearn.manifold import Isomap iso = Isomap ( n_components = 2 ) projection = iso . fit_transform ( digits . data ) # plot the results plt . scatter ( projection [:, 0 ], projection [:, 1 ], lw = 0.1 , c = digits . target , cmap = plt . cm . get_cmap ( 'cubehelix' , 6 )) plt . colorbar ( ticks = range ( 6 ), label = 'digit value' ) plt . clim ( - 0.5 , 5.5 ) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:348: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue. self._fit_transform(X) /home/qalmaqihir/anaconda3/lib/python3.9/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient. self._set_intXint(row, col, x.flat[0]) The projection also gives us some interesting insights on the relationships within the dataset: for example, the ranges of 5 and 3 nearly overlap in this projection, indicating that some handwritten fives and threes are difficult to distinguish, and therefore more likely to be confused by an automated classification algorithm. Other values, like 0 and 1, are more distantly separated, and therefore much less likely to be con\u2010 fused. This observation agrees with our intuition, because 5 and 3 look much more similar than do 0 and 1.","title":"Example: Handwritten Digits"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: Visualizing a M\u00f6bius strip \u00b6 Example: Visualizing a M\u00f6bius strip Example: Visualizing a M\u00f6bius strip \u00b6 A M\u00f6bius strip is similar to a strip of paper glued into a loop with a half-twist. Topo\u2010 logically, it\u2019s quite interesting because despite appearances it has only a single side! Here we will visualize such an object using Matplotlib\u2019s three-dimensional tools. The key to creating the M\u00f6bius strip is to think about its parameterization: it\u2019s a two-dimensional strip, so we need two intrinsic dimensions. Let\u2019s call them \u03b8, which ranges from 0 to 2\u03c0 around the loop, and w which ranges from \u20131 to 1 across the width of the strip: from mpl_toolkits import mplot3d % matplotlib notebook import numpy as np import matplotlib.pyplot as plt theta = np . linspace ( 0 , 2 * np . pi , 30 ) w = np . linspace ( - 0.25 , 0.25 , 8 ) w , theta = np . meshgrid ( w , theta ) Now from this parameterization, we must determine the (x, y, z) positions of the embedded strip. Thinking about it, we might realize that there are two rotations happening: one is the position of the loop about its center (what we\u2019ve called \u03b8), while the other is the twist\u2010 ing of the strip about its axis (we\u2019ll call this \u03d5). For a M\u00f6bius strip, we must have the strip make half a twist during a full loop, or \u0394\u03d5 = \u0394\u03b8/2. phi = 0.5 * theta Now we use our recollection of trigonometry to derive the three-dimensional embed\u2010 ding. We\u2019ll define r, the distance of each point from the center, and use this to find the embedded x, y, z coordinates: # radius in x-y plane r = 1 + w * np . cos ( phi ) x = np . ravel ( r * np . cos ( theta )) y = np . ravel ( r * np . sin ( theta )) z = np . ravel ( w * np . sin ( theta )) Finally, to plot the object, we must make sure the triangulation is correct. The best way to do this is to define the triangulation within the underlying parameterization, and then let Matplotlib project this triangulation into the three-dimensional space of the M\u00f6bius strip. This can be accomplished as follows # triangulate in the underlying parametrization from matplotlib.tri import Triangulation tri = Triangulation ( np . ravel ( w ), np . ravel ( theta )) ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , triangles = tri . triangles , cmap = 'viridis' , linewidth = 0.2 ) ax . set_xlim ( - 1 , 1 ); ax . set_ylim ( - 1 , 1 ); ax . set_zlim ( - 1 , 1 ); <IPython.core.display.Javascript object> Combining all of these techniques, it is possible to create and display a wide variety of three-dimensional objects and patterns in Matplotlib.","title":"Example Visualizing a Mobius Strip"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/#example-visualizing-a-mobius-strip","text":"Example: Visualizing a M\u00f6bius strip","title":"Example: Visualizing a M\u00f6bius strip"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/#example-visualizing-a-mobius-strip_1","text":"A M\u00f6bius strip is similar to a strip of paper glued into a loop with a half-twist. Topo\u2010 logically, it\u2019s quite interesting because despite appearances it has only a single side! Here we will visualize such an object using Matplotlib\u2019s three-dimensional tools. The key to creating the M\u00f6bius strip is to think about its parameterization: it\u2019s a two-dimensional strip, so we need two intrinsic dimensions. Let\u2019s call them \u03b8, which ranges from 0 to 2\u03c0 around the loop, and w which ranges from \u20131 to 1 across the width of the strip: from mpl_toolkits import mplot3d % matplotlib notebook import numpy as np import matplotlib.pyplot as plt theta = np . linspace ( 0 , 2 * np . pi , 30 ) w = np . linspace ( - 0.25 , 0.25 , 8 ) w , theta = np . meshgrid ( w , theta ) Now from this parameterization, we must determine the (x, y, z) positions of the embedded strip. Thinking about it, we might realize that there are two rotations happening: one is the position of the loop about its center (what we\u2019ve called \u03b8), while the other is the twist\u2010 ing of the strip about its axis (we\u2019ll call this \u03d5). For a M\u00f6bius strip, we must have the strip make half a twist during a full loop, or \u0394\u03d5 = \u0394\u03b8/2. phi = 0.5 * theta Now we use our recollection of trigonometry to derive the three-dimensional embed\u2010 ding. We\u2019ll define r, the distance of each point from the center, and use this to find the embedded x, y, z coordinates: # radius in x-y plane r = 1 + w * np . cos ( phi ) x = np . ravel ( r * np . cos ( theta )) y = np . ravel ( r * np . sin ( theta )) z = np . ravel ( w * np . sin ( theta )) Finally, to plot the object, we must make sure the triangulation is correct. The best way to do this is to define the triangulation within the underlying parameterization, and then let Matplotlib project this triangulation into the three-dimensional space of the M\u00f6bius strip. This can be accomplished as follows # triangulate in the underlying parametrization from matplotlib.tri import Triangulation tri = Triangulation ( np . ravel ( w ), np . ravel ( theta )) ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , triangles = tri . triangles , cmap = 'viridis' , linewidth = 0.2 ) ax . set_xlim ( - 1 , 1 ); ax . set_ylim ( - 1 , 1 ); ax . set_zlim ( - 1 , 1 ); <IPython.core.display.Javascript object> Combining all of these techniques, it is possible to create and display a wide variety of three-dimensional objects and patterns in Matplotlib.","title":"Example: Visualizing a M\u00f6bius strip"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: California Cities \u00b6 Example: California Cities Example: California Cities \u00b6 Recall that in \u201cCustomizing Plot Legends\u201d, we demonstrated the use of size and color in a scatter plot to convey information about the location, size, and population of California cities. Here, we\u2019ll create this plot again, but using Basemap to put the data in context. import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap cities = pd . read_csv ( '../data/california_cities.csv' ) cities . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 city latd longd elevation_m elevation_ft population_total area_total_sq_mi area_land_sq_mi area_water_sq_mi area_total_km2 area_land_km2 area_water_km2 area_water_percent 0 0 Adelanto 34.576111 -117.432778 875.0 2871.0 31765 56.027 56.009 0.018 145.107 145.062 0.046 0.03 1 1 AgouraHills 34.153333 -118.761667 281.0 922.0 20330 7.822 7.793 0.029 20.260 20.184 0.076 0.37 2 2 Alameda 37.756111 -122.274444 NaN 33.0 75467 22.960 10.611 12.349 59.465 27.482 31.983 53.79 3 3 Albany 37.886944 -122.297778 NaN 43.0 18969 5.465 1.788 3.677 14.155 4.632 9.524 67.28 4 4 Alhambra 34.081944 -118.135000 150.0 492.0 83089 7.632 7.631 0.001 19.766 19.763 0.003 0.01 # extract the data we're interested in lat = cities [ 'latd' ] . values lon = cities [ 'longd' ] . values population = cities [ 'population_total' ] . values area = cities [ 'area_total_km2' ] . values # 1. Draw the map background fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'h' , lat_0 = 37.5 , lon_0 =- 119 , width = 1E6 , height = 1.2E6 ) m . shadedrelief () m . drawcoastlines ( color = 'gray' ) m . drawcountries ( color = 'gray' ) m . drawstates ( color = 'gray' ) # 2. scatter city data, with color reflecting population and size reflecting area m . scatter ( lon , lat , latlon = True , c = np . log10 ( population ), s = area , cmap = 'Reds' , alpha = 0.5 ) # 3. Create colobar and legned plt . colorbar ( label = r '$log_ {10} ({\\rm population})$' ) plt . clim ( 3 , 7 ) # 4. make legend with dummy points for a in [ 100 , 300 , 500 ]: plt . scatter ([],[], c = 'k' , alpha = 0.5 , s = a , label = str ( a ) + 'km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , labelspacing = 1 , loc = 'lower left' ) <matplotlib.legend.Legend at 0x7f0308093b50>","title":"example California cities"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/#example-california-cities","text":"Example: California Cities","title":"Example: California Cities"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/#example-california-cities_1","text":"Recall that in \u201cCustomizing Plot Legends\u201d, we demonstrated the use of size and color in a scatter plot to convey information about the location, size, and population of California cities. Here, we\u2019ll create this plot again, but using Basemap to put the data in context. import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap cities = pd . read_csv ( '../data/california_cities.csv' ) cities . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 city latd longd elevation_m elevation_ft population_total area_total_sq_mi area_land_sq_mi area_water_sq_mi area_total_km2 area_land_km2 area_water_km2 area_water_percent 0 0 Adelanto 34.576111 -117.432778 875.0 2871.0 31765 56.027 56.009 0.018 145.107 145.062 0.046 0.03 1 1 AgouraHills 34.153333 -118.761667 281.0 922.0 20330 7.822 7.793 0.029 20.260 20.184 0.076 0.37 2 2 Alameda 37.756111 -122.274444 NaN 33.0 75467 22.960 10.611 12.349 59.465 27.482 31.983 53.79 3 3 Albany 37.886944 -122.297778 NaN 43.0 18969 5.465 1.788 3.677 14.155 4.632 9.524 67.28 4 4 Alhambra 34.081944 -118.135000 150.0 492.0 83089 7.632 7.631 0.001 19.766 19.763 0.003 0.01 # extract the data we're interested in lat = cities [ 'latd' ] . values lon = cities [ 'longd' ] . values population = cities [ 'population_total' ] . values area = cities [ 'area_total_km2' ] . values # 1. Draw the map background fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'h' , lat_0 = 37.5 , lon_0 =- 119 , width = 1E6 , height = 1.2E6 ) m . shadedrelief () m . drawcoastlines ( color = 'gray' ) m . drawcountries ( color = 'gray' ) m . drawstates ( color = 'gray' ) # 2. scatter city data, with color reflecting population and size reflecting area m . scatter ( lon , lat , latlon = True , c = np . log10 ( population ), s = area , cmap = 'Reds' , alpha = 0.5 ) # 3. Create colobar and legned plt . colorbar ( label = r '$log_ {10} ({\\rm population})$' ) plt . clim ( 3 , 7 ) # 4. make legend with dummy points for a in [ 100 , 300 , 500 ]: plt . scatter ([],[], c = 'k' , alpha = 0.5 , s = a , label = str ( a ) + 'km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , labelspacing = 1 , loc = 'lower left' ) <matplotlib.legend.Legend at 0x7f0308093b50>","title":"Example: California Cities"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/","text":"Notes [Book] Data Science Handbook \u00b6 by Jawad Haider Chpt 4 - Visualization with Matplotlib \u00b6 Example: Surface Temperature Data \u00b6 Example: Surface Temperature Data Unfortunately, no dataset found ! Example: Surface Temperature Data \u00b6 As an example of visualizing some more continuous geographic data, let\u2019s consider the \u201cpolar vortex\u201d that hit the eastern half of the United States in January 2014. A great source for any sort of climatic data is NASA\u2019s Goddard Institute for Space Stud\u2010 ies. Here we\u2019ll use the GIS 250 temperature data, which we can download using shell commands (these commands may have to be modified on Windows machines). The data used here was downloaded on 6/12/2016, and the file size is approximately 9 MB: ! curl - O http : // data . giss . nasa . gov / pub / gistemp / gistemp250 . nc . gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 263 100 263 0 0 468 0 --:--:-- --:--:-- --:--:-- 467 ! gunzip gistemp250 . nc . gz gzip: gistemp250.nc.gz: not in gzip format The data comes in NetCDF format, which can be read in Python by the netCDF4 library. You can install this library as shown here: # ! conda install netcdf4 % matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap from PIL import Image Unfortunately, no dataset found ! \u00b6 from netCDF4 import Dataset data = Dataset ( 'gistemp250.nc' ) from netCDF4 import date2index from datetime import datetime timeindex = date2index ( datetime ( 2014 , 1 , 15 ), data . variables [ 'time' ]) lat = data . variables [ 'lat' ][:] lon = data . variables [ 'lon' ][:] lon , lat = np . meshgrid ( lon , lat ) temp_anomaly = data . variables [ 'tempanomaly' ][ timeindex ] fig = plt . figure ( figsize = ( 10 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'c' , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . shadedrelief ( scale = 0.5 ) m . pcolormesh ( lon , lat , temp_anomaly , latlon = True , cmap = 'RdBu_r' ) plt . clim ( - 8 , 8 ) m . drawcoastlines ( color = 'lightgray' ) plt . title ( 'January 2014 Temperature Anomaly' ) plt . colorbar ( label = 'temperature anomaly (\u00b0C)' );","title":"Example surface temperature data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/#notes-book-data-science-handbook","text":"by Jawad Haider","title":"Notes [Book] Data Science Handbook"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/#chpt-4-visualization-with-matplotlib","text":"","title":"Chpt 4 - Visualization with Matplotlib"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/#example-surface-temperature-data","text":"Example: Surface Temperature Data Unfortunately, no dataset found !","title":"Example: Surface Temperature Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/#example-surface-temperature-data_1","text":"As an example of visualizing some more continuous geographic data, let\u2019s consider the \u201cpolar vortex\u201d that hit the eastern half of the United States in January 2014. A great source for any sort of climatic data is NASA\u2019s Goddard Institute for Space Stud\u2010 ies. Here we\u2019ll use the GIS 250 temperature data, which we can download using shell commands (these commands may have to be modified on Windows machines). The data used here was downloaded on 6/12/2016, and the file size is approximately 9 MB: ! curl - O http : // data . giss . nasa . gov / pub / gistemp / gistemp250 . nc . gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 263 100 263 0 0 468 0 --:--:-- --:--:-- --:--:-- 467 ! gunzip gistemp250 . nc . gz gzip: gistemp250.nc.gz: not in gzip format The data comes in NetCDF format, which can be read in Python by the netCDF4 library. You can install this library as shown here: # ! conda install netcdf4 % matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap from PIL import Image","title":"Example: Surface Temperature Data"},{"location":"booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/#unfortunately-no-dataset-found","text":"from netCDF4 import Dataset data = Dataset ( 'gistemp250.nc' ) from netCDF4 import date2index from datetime import datetime timeindex = date2index ( datetime ( 2014 , 1 , 15 ), data . variables [ 'time' ]) lat = data . variables [ 'lat' ][:] lon = data . variables [ 'lon' ][:] lon , lat = np . meshgrid ( lon , lat ) temp_anomaly = data . variables [ 'tempanomaly' ][ timeindex ] fig = plt . figure ( figsize = ( 10 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'c' , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . shadedrelief ( scale = 0.5 ) m . pcolormesh ( lon , lat , temp_anomaly , latlon = True , cmap = 'RdBu_r' ) plt . clim ( - 8 , 8 ) m . drawcoastlines ( color = 'lightgray' ) plt . title ( 'January 2014 Temperature Anomaly' ) plt . colorbar ( label = 'temperature anomaly (\u00b0C)' );","title":"Unfortunately, no dataset found !"},{"location":"bootcampsnotes/","text":"BootCamps Notes \u00b6 The structure of this directory is as follow: Here you will find notes related to: \u00b6 Numpy Pandas Pytorch Basics Pytorch for Deeplearning Bootcamp a. ANN b. CNN c. RNN d. NLP Tensorflow BootCamp: a. Colab Basics 1. Installing Tensorflow 2. Loading Data b. Machine Learning Basics 1. Linear Classification 2. Linear Regression c. ANN 1. ANN MNIST 2. ANN Regression d. CNN 1. Fashion MNIST 2. CIFAR 3. Improved CIFAR e. RNN Autoregressive Model Simple RNN sine RNN Shape LSTM Non-linear Long Distance RNN MNIST Stock Returns f. NLP Text Preprocessing Spam Detection CNN g. Recommendar Systems h. Transfer Learning i. GANs j. Advance Tensorflow Tensorflow serving Mirror Strategy TFLite TPU k. Low-Level Tensorflow Basic Computation Variables and Gradients Build your own Model soon to be added... \u00b6 matplotlib seaborn Python for DataSceince and Machine Learning","title":"BootCamps Notes :boot::camping::notes:"},{"location":"bootcampsnotes/#bootcamps-notes","text":"The structure of this directory is as follow:","title":"BootCamps Notes"},{"location":"bootcampsnotes/#here-you-will-find-notes-related-to","text":"Numpy Pandas Pytorch Basics Pytorch for Deeplearning Bootcamp a. ANN b. CNN c. RNN d. NLP Tensorflow BootCamp: a. Colab Basics 1. Installing Tensorflow 2. Loading Data b. Machine Learning Basics 1. Linear Classification 2. Linear Regression c. ANN 1. ANN MNIST 2. ANN Regression d. CNN 1. Fashion MNIST 2. CIFAR 3. Improved CIFAR e. RNN Autoregressive Model Simple RNN sine RNN Shape LSTM Non-linear Long Distance RNN MNIST Stock Returns f. NLP Text Preprocessing Spam Detection CNN g. Recommendar Systems h. Transfer Learning i. GANs j. Advance Tensorflow Tensorflow serving Mirror Strategy TFLite TPU k. Low-Level Tensorflow Basic Computation Variables and Gradients Build your own Model","title":"Here you will find notes related to:"},{"location":"bootcampsnotes/#soon-to-be-added","text":"matplotlib seaborn Python for DataSceince and Machine Learning","title":"soon to be added..."},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_MNIST/","text":"Tensorflow BootCamp - ANN MNIST \u00b6 by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348.9MB 52kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 49.1MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 54.6MB/s 2.0.0-beta1 # Load in the data mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) # Compile the model model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # Train the model r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 10 ) WARNING: Logging before flag parsing goes to stderr. W0718 16:30:06.976897 139639418169216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 7s 111us/sample - loss: 0.2949 - accuracy: 0.9134 - val_loss: 0.1339 - val_accuracy: 0.9594 Epoch 2/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.1452 - accuracy: 0.9564 - val_loss: 0.1063 - val_accuracy: 0.9681 Epoch 3/10 60000/60000 [==============================] - 6s 95us/sample - loss: 0.1100 - accuracy: 0.9664 - val_loss: 0.0879 - val_accuracy: 0.9722 Epoch 4/10 60000/60000 [==============================] - 6s 94us/sample - loss: 0.0902 - accuracy: 0.9721 - val_loss: 0.0737 - val_accuracy: 0.9766 Epoch 5/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0765 - accuracy: 0.9758 - val_loss: 0.0747 - val_accuracy: 0.9748 Epoch 6/10 60000/60000 [==============================] - 6s 94us/sample - loss: 0.0672 - accuracy: 0.9789 - val_loss: 0.0699 - val_accuracy: 0.9772 Epoch 7/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0586 - accuracy: 0.9820 - val_loss: 0.0729 - val_accuracy: 0.9781 Epoch 8/10 60000/60000 [==============================] - 6s 94us/sample - loss: 0.0544 - accuracy: 0.9815 - val_loss: 0.0697 - val_accuracy: 0.9780 Epoch 9/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0505 - accuracy: 0.9834 - val_loss: 0.0690 - val_accuracy: 0.9789 Epoch 10/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0447 - accuracy: 0.9850 - val_loss: 0.0692 - val_accuracy: 0.9806 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7effe00f2ac8> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7effe0092518> # Evaluate the model print ( model . evaluate ( x_test , y_test )) 10000/10000 [==============================] - 1s 55us/sample - loss: 0.0692 - accuracy: 0.9806 [0.06924617666350968, 0.9806] # Plot confusion matrix from sklearn.metrics import confusion_matrix import numpy as np import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) # Do these results make sense? # It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc. Confusion matrix, without normalization [[ 972 0 1 1 0 0 2 1 2 1] [ 0 1126 2 2 0 0 2 0 3 0] [ 2 1 1015 3 1 0 1 4 4 1] [ 1 0 1 996 0 2 0 3 3 4] [ 1 1 2 1 959 0 5 3 1 9] [ 2 1 0 12 1 868 2 2 3 1] [ 5 3 1 1 1 3 939 0 5 0] [ 2 10 12 3 0 0 0 994 3 4] [ 6 0 3 2 3 1 0 2 953 4] [ 2 2 0 3 12 3 0 3 0 984]] # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( y_test [ i ], p_test [ i ])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 ANN MNIST"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_MNIST/#tensorflow-bootcamp-ann-mnist","text":"by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348.9MB 52kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 49.1MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 54.6MB/s 2.0.0-beta1 # Load in the data mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) # Compile the model model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # Train the model r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 10 ) WARNING: Logging before flag parsing goes to stderr. W0718 16:30:06.976897 139639418169216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 7s 111us/sample - loss: 0.2949 - accuracy: 0.9134 - val_loss: 0.1339 - val_accuracy: 0.9594 Epoch 2/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.1452 - accuracy: 0.9564 - val_loss: 0.1063 - val_accuracy: 0.9681 Epoch 3/10 60000/60000 [==============================] - 6s 95us/sample - loss: 0.1100 - accuracy: 0.9664 - val_loss: 0.0879 - val_accuracy: 0.9722 Epoch 4/10 60000/60000 [==============================] - 6s 94us/sample - loss: 0.0902 - accuracy: 0.9721 - val_loss: 0.0737 - val_accuracy: 0.9766 Epoch 5/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0765 - accuracy: 0.9758 - val_loss: 0.0747 - val_accuracy: 0.9748 Epoch 6/10 60000/60000 [==============================] - 6s 94us/sample - loss: 0.0672 - accuracy: 0.9789 - val_loss: 0.0699 - val_accuracy: 0.9772 Epoch 7/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0586 - accuracy: 0.9820 - val_loss: 0.0729 - val_accuracy: 0.9781 Epoch 8/10 60000/60000 [==============================] - 6s 94us/sample - loss: 0.0544 - accuracy: 0.9815 - val_loss: 0.0697 - val_accuracy: 0.9780 Epoch 9/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0505 - accuracy: 0.9834 - val_loss: 0.0690 - val_accuracy: 0.9789 Epoch 10/10 60000/60000 [==============================] - 6s 93us/sample - loss: 0.0447 - accuracy: 0.9850 - val_loss: 0.0692 - val_accuracy: 0.9806 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7effe00f2ac8> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7effe0092518> # Evaluate the model print ( model . evaluate ( x_test , y_test )) 10000/10000 [==============================] - 1s 55us/sample - loss: 0.0692 - accuracy: 0.9806 [0.06924617666350968, 0.9806] # Plot confusion matrix from sklearn.metrics import confusion_matrix import numpy as np import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) # Do these results make sense? # It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc. Confusion matrix, without normalization [[ 972 0 1 1 0 0 2 1 2 1] [ 0 1126 2 2 0 0 2 0 3 0] [ 2 1 1015 3 1 0 1 4 4 1] [ 1 0 1 996 0 2 0 3 3 4] [ 1 1 2 1 959 0 5 3 1 9] [ 2 1 0 12 1 868 2 2 3 1] [ 5 3 1 1 1 3 939 0 5 0] [ 2 10 12 3 0 0 0 994 3 4] [ 6 0 3 2 3 1 0 2 953 4] [ 2 2 0 3 12 3 0 3 0 984]] # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( y_test [ i ], p_test [ i ])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Tensorflow BootCamp - ANN MNIST"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_Regression/","text":"Tensorflow BootCamp - ANN MNIST Regression \u00b6 by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # Other imports import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D # Make the dataset N = 1000 X = np . random . random (( N , 2 )) * 6 - 3 # uniformly distributed between (-3, +3) Y = np . cos ( 2 * X [:, 0 ]) + np . cos ( 3 * X [:, 1 ]) This implements the function: \\[ y = \\cos(2x_1) + cos(3x_2) \\] # Plot it fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X [:, 0 ], X [:, 1 ], Y ) # plt.show() <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f2e953549e8> # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( 128 , input_shape = ( 2 ,), activation = 'relu' ), tf . keras . layers . Dense ( 1 ) ]) # Compile and fit opt = tf . keras . optimizers . Adam ( 0.01 ) model . compile ( optimizer = opt , loss = 'mse' ) r = model . fit ( X , Y , epochs = 100 ) Train on 1000 samples Epoch 1/100 1000/1000 [==============================] - 0s 493us/sample - loss: 0.9276 Epoch 2/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.9060 Epoch 3/100 1000/1000 [==============================] - 0s 68us/sample - loss: 0.8808 Epoch 4/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.8457 Epoch 5/100 1000/1000 [==============================] - 0s 57us/sample - loss: 0.8115 Epoch 6/100 1000/1000 [==============================] - 0s 59us/sample - loss: 0.7682 Epoch 7/100 1000/1000 [==============================] - 0s 59us/sample - loss: 0.6904 Epoch 8/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.6319 Epoch 9/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.5543 Epoch 10/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.5207 Epoch 11/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.4973 Epoch 12/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.4889 Epoch 13/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.4715 Epoch 14/100 1000/1000 [==============================] - 0s 55us/sample - loss: 0.4714 Epoch 15/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4387 Epoch 16/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.4519 Epoch 17/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4498 Epoch 18/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4180 Epoch 19/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4163 Epoch 20/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.4128 Epoch 21/100 1000/1000 [==============================] - 0s 67us/sample - loss: 0.4110 Epoch 22/100 1000/1000 [==============================] - 0s 62us/sample - loss: 0.3984 Epoch 23/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.3825 Epoch 24/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.3734 Epoch 25/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.4398 Epoch 26/100 1000/1000 [==============================] - 0s 57us/sample - loss: 0.3796 Epoch 27/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.3603 Epoch 28/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.3120 Epoch 29/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.2797 Epoch 30/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.2815 Epoch 31/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.2600 Epoch 32/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.2221 Epoch 33/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.2036 Epoch 34/100 1000/1000 [==============================] - 0s 59us/sample - loss: 0.1905 Epoch 35/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.1502 Epoch 36/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.1368 Epoch 37/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.1300 Epoch 38/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.1043 Epoch 39/100 1000/1000 [==============================] - 0s 66us/sample - loss: 0.1022 Epoch 40/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0823 Epoch 41/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0787 Epoch 42/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0593 Epoch 43/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0528 Epoch 44/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0616 Epoch 45/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0351 Epoch 46/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0273 Epoch 47/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0288 Epoch 48/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0281 Epoch 49/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0362 Epoch 50/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0585 Epoch 51/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0842 Epoch 52/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.0780 Epoch 53/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0700 Epoch 54/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0232 Epoch 55/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0149 Epoch 56/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0159 Epoch 57/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0140 Epoch 58/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0108 Epoch 59/100 1000/1000 [==============================] - 0s 66us/sample - loss: 0.0088 Epoch 60/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0077 Epoch 61/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0132 Epoch 62/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0108 Epoch 63/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0105 Epoch 64/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0077 Epoch 65/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0084 Epoch 66/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0101 Epoch 67/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0093 Epoch 68/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0082 Epoch 69/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0076 Epoch 70/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.0145 Epoch 71/100 1000/1000 [==============================] - 0s 47us/sample - loss: 0.0127 Epoch 72/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0101 Epoch 73/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0058 Epoch 74/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0051 Epoch 75/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0053 Epoch 76/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0073 Epoch 77/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0065 Epoch 78/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0066 Epoch 79/100 1000/1000 [==============================] - 0s 60us/sample - loss: 0.0202 Epoch 80/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0130 Epoch 81/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0093 Epoch 82/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0149 Epoch 83/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0147 Epoch 84/100 1000/1000 [==============================] - 0s 46us/sample - loss: 0.0085 Epoch 85/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0048 Epoch 86/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0041 Epoch 87/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0041 Epoch 88/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0106 Epoch 89/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0112 Epoch 90/100 1000/1000 [==============================] - 0s 47us/sample - loss: 0.0051 Epoch 91/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0121 Epoch 92/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0384 Epoch 93/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0463 Epoch 94/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0190 Epoch 95/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0340 Epoch 96/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0133 Epoch 97/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0062 Epoch 98/100 1000/1000 [==============================] - 0s 67us/sample - loss: 0.0043 Epoch 99/100 1000/1000 [==============================] - 0s 67us/sample - loss: 0.0050 Epoch 100/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0045 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) # Plot the prediction surface fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X [:, 0 ], X [:, 1 ], Y ) # surface plot line = np . linspace ( - 3 , 3 , 50 ) xx , yy = np . meshgrid ( line , line ) Xgrid = np . vstack (( xx . flatten (), yy . flatten ())) . T Yhat = model . predict ( Xgrid ) . flatten () ax . plot_trisurf ( Xgrid [:, 0 ], Xgrid [:, 1 ], Yhat , linewidth = 0.2 , antialiased = True ) plt . show () # Can it extrapolate? # Plot the prediction surface fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X [:, 0 ], X [:, 1 ], Y ) # surface plot line = np . linspace ( - 5 , 5 , 50 ) xx , yy = np . meshgrid ( line , line ) Xgrid = np . vstack (( xx . flatten (), yy . flatten ())) . T Yhat = model . predict ( Xgrid ) . flatten () ax . plot_trisurf ( Xgrid [:, 0 ], Xgrid [:, 1 ], Yhat , linewidth = 0.2 , antialiased = True ) plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 ANN Regression"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_Regression/#tensorflow-bootcamp-ann-mnist-regression","text":"by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # Other imports import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D # Make the dataset N = 1000 X = np . random . random (( N , 2 )) * 6 - 3 # uniformly distributed between (-3, +3) Y = np . cos ( 2 * X [:, 0 ]) + np . cos ( 3 * X [:, 1 ]) This implements the function: \\[ y = \\cos(2x_1) + cos(3x_2) \\] # Plot it fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X [:, 0 ], X [:, 1 ], Y ) # plt.show() <mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f2e953549e8> # Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( 128 , input_shape = ( 2 ,), activation = 'relu' ), tf . keras . layers . Dense ( 1 ) ]) # Compile and fit opt = tf . keras . optimizers . Adam ( 0.01 ) model . compile ( optimizer = opt , loss = 'mse' ) r = model . fit ( X , Y , epochs = 100 ) Train on 1000 samples Epoch 1/100 1000/1000 [==============================] - 0s 493us/sample - loss: 0.9276 Epoch 2/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.9060 Epoch 3/100 1000/1000 [==============================] - 0s 68us/sample - loss: 0.8808 Epoch 4/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.8457 Epoch 5/100 1000/1000 [==============================] - 0s 57us/sample - loss: 0.8115 Epoch 6/100 1000/1000 [==============================] - 0s 59us/sample - loss: 0.7682 Epoch 7/100 1000/1000 [==============================] - 0s 59us/sample - loss: 0.6904 Epoch 8/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.6319 Epoch 9/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.5543 Epoch 10/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.5207 Epoch 11/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.4973 Epoch 12/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.4889 Epoch 13/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.4715 Epoch 14/100 1000/1000 [==============================] - 0s 55us/sample - loss: 0.4714 Epoch 15/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4387 Epoch 16/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.4519 Epoch 17/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4498 Epoch 18/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4180 Epoch 19/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.4163 Epoch 20/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.4128 Epoch 21/100 1000/1000 [==============================] - 0s 67us/sample - loss: 0.4110 Epoch 22/100 1000/1000 [==============================] - 0s 62us/sample - loss: 0.3984 Epoch 23/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.3825 Epoch 24/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.3734 Epoch 25/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.4398 Epoch 26/100 1000/1000 [==============================] - 0s 57us/sample - loss: 0.3796 Epoch 27/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.3603 Epoch 28/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.3120 Epoch 29/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.2797 Epoch 30/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.2815 Epoch 31/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.2600 Epoch 32/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.2221 Epoch 33/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.2036 Epoch 34/100 1000/1000 [==============================] - 0s 59us/sample - loss: 0.1905 Epoch 35/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.1502 Epoch 36/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.1368 Epoch 37/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.1300 Epoch 38/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.1043 Epoch 39/100 1000/1000 [==============================] - 0s 66us/sample - loss: 0.1022 Epoch 40/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0823 Epoch 41/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0787 Epoch 42/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0593 Epoch 43/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0528 Epoch 44/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0616 Epoch 45/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0351 Epoch 46/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0273 Epoch 47/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0288 Epoch 48/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0281 Epoch 49/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0362 Epoch 50/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0585 Epoch 51/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0842 Epoch 52/100 1000/1000 [==============================] - 0s 54us/sample - loss: 0.0780 Epoch 53/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0700 Epoch 54/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0232 Epoch 55/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0149 Epoch 56/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0159 Epoch 57/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0140 Epoch 58/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0108 Epoch 59/100 1000/1000 [==============================] - 0s 66us/sample - loss: 0.0088 Epoch 60/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0077 Epoch 61/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0132 Epoch 62/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0108 Epoch 63/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0105 Epoch 64/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0077 Epoch 65/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0084 Epoch 66/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0101 Epoch 67/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0093 Epoch 68/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0082 Epoch 69/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0076 Epoch 70/100 1000/1000 [==============================] - 0s 58us/sample - loss: 0.0145 Epoch 71/100 1000/1000 [==============================] - 0s 47us/sample - loss: 0.0127 Epoch 72/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0101 Epoch 73/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0058 Epoch 74/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0051 Epoch 75/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0053 Epoch 76/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0073 Epoch 77/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0065 Epoch 78/100 1000/1000 [==============================] - 0s 53us/sample - loss: 0.0066 Epoch 79/100 1000/1000 [==============================] - 0s 60us/sample - loss: 0.0202 Epoch 80/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0130 Epoch 81/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0093 Epoch 82/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0149 Epoch 83/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0147 Epoch 84/100 1000/1000 [==============================] - 0s 46us/sample - loss: 0.0085 Epoch 85/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0048 Epoch 86/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0041 Epoch 87/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0041 Epoch 88/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0106 Epoch 89/100 1000/1000 [==============================] - 0s 50us/sample - loss: 0.0112 Epoch 90/100 1000/1000 [==============================] - 0s 47us/sample - loss: 0.0051 Epoch 91/100 1000/1000 [==============================] - 0s 48us/sample - loss: 0.0121 Epoch 92/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0384 Epoch 93/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0463 Epoch 94/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0190 Epoch 95/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0340 Epoch 96/100 1000/1000 [==============================] - 0s 52us/sample - loss: 0.0133 Epoch 97/100 1000/1000 [==============================] - 0s 49us/sample - loss: 0.0062 Epoch 98/100 1000/1000 [==============================] - 0s 67us/sample - loss: 0.0043 Epoch 99/100 1000/1000 [==============================] - 0s 67us/sample - loss: 0.0050 Epoch 100/100 1000/1000 [==============================] - 0s 51us/sample - loss: 0.0045 # Plot the loss plt . plot ( r . history [ 'loss' ], label = 'loss' ) # Plot the prediction surface fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X [:, 0 ], X [:, 1 ], Y ) # surface plot line = np . linspace ( - 3 , 3 , 50 ) xx , yy = np . meshgrid ( line , line ) Xgrid = np . vstack (( xx . flatten (), yy . flatten ())) . T Yhat = model . predict ( Xgrid ) . flatten () ax . plot_trisurf ( Xgrid [:, 0 ], Xgrid [:, 1 ], Yhat , linewidth = 0.2 , antialiased = True ) plt . show () # Can it extrapolate? # Plot the prediction surface fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X [:, 0 ], X [:, 1 ], Y ) # surface plot line = np . linspace ( - 5 , 5 , 50 ) xx , yy = np . meshgrid ( line , line ) Xgrid = np . vstack (( xx . flatten (), yy . flatten ())) . T Yhat = model . predict ( Xgrid ) . flatten () ax . plot_trisurf ( Xgrid [:, 0 ], Xgrid [:, 1 ], Yhat , linewidth = 0.2 , antialiased = True ) plt . show () Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Tensorflow BootCamp - ANN MNIST Regression"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR/","text":"Tensorflow BootCamp - CNN CIFAR \u00b6 by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , GlobalMaxPooling2D from tensorflow.keras.models import Model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Conv2D ( 64 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Conv2D ( 128 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Flatten ()( x ) x = Dropout ( 0.5 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile and fit # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) Train on 50000 samples, validate on 10000 samples Epoch 1/15 50000/50000 [==============================] - 17s 336us/sample - loss: 1.5736 - accuracy: 0.4255 - val_loss: 1.3048 - val_accuracy: 0.5317 Epoch 2/15 50000/50000 [==============================] - 16s 320us/sample - loss: 1.2732 - accuracy: 0.5435 - val_loss: 1.1139 - val_accuracy: 0.6004 Epoch 3/15 50000/50000 [==============================] - 16s 321us/sample - loss: 1.1454 - accuracy: 0.5909 - val_loss: 1.0765 - val_accuracy: 0.6176 Epoch 4/15 50000/50000 [==============================] - 16s 325us/sample - loss: 1.0497 - accuracy: 0.6259 - val_loss: 0.9637 - val_accuracy: 0.6603 Epoch 5/15 50000/50000 [==============================] - 16s 325us/sample - loss: 0.9764 - accuracy: 0.6546 - val_loss: 0.9363 - val_accuracy: 0.6728 Epoch 6/15 50000/50000 [==============================] - 16s 323us/sample - loss: 0.9164 - accuracy: 0.6735 - val_loss: 0.9153 - val_accuracy: 0.6772 Epoch 7/15 50000/50000 [==============================] - 16s 322us/sample - loss: 0.8660 - accuracy: 0.6944 - val_loss: 0.8996 - val_accuracy: 0.6818 Epoch 8/15 50000/50000 [==============================] - 17s 330us/sample - loss: 0.8213 - accuracy: 0.7083 - val_loss: 0.8806 - val_accuracy: 0.6954 Epoch 9/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.7807 - accuracy: 0.7233 - val_loss: 0.8833 - val_accuracy: 0.6951 Epoch 10/15 50000/50000 [==============================] - 16s 327us/sample - loss: 0.7470 - accuracy: 0.7359 - val_loss: 0.8368 - val_accuracy: 0.7058 Epoch 11/15 50000/50000 [==============================] - 16s 325us/sample - loss: 0.7121 - accuracy: 0.7462 - val_loss: 0.8376 - val_accuracy: 0.7092 Epoch 12/15 50000/50000 [==============================] - 17s 330us/sample - loss: 0.6808 - accuracy: 0.7560 - val_loss: 0.8430 - val_accuracy: 0.7115 Epoch 13/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.6576 - accuracy: 0.7671 - val_loss: 0.8284 - val_accuracy: 0.7110 Epoch 14/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.6402 - accuracy: 0.7702 - val_loss: 0.8487 - val_accuracy: 0.7067 Epoch 15/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.6232 - accuracy: 0.7769 - val_loss: 0.8320 - val_accuracy: 0.7136 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f04d6f0db00> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f04d6b34208> # Plot confusion matrix from sklearn.metrics import confusion_matrix import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) Confusion matrix, without normalization [[741 16 36 26 12 2 6 11 113 37] [ 14 781 9 9 2 4 6 2 47 126] [ 62 9 545 91 102 66 50 29 28 18] [ 20 9 51 548 51 171 58 33 30 29] [ 40 10 62 79 617 43 47 78 19 5] [ 15 2 52 214 38 572 13 55 21 18] [ 6 9 45 101 43 44 711 7 18 16] [ 24 3 38 48 59 64 5 726 6 27] [ 49 30 9 17 6 3 1 3 865 17] [ 35 105 16 19 4 7 1 11 46 756]] # label mapping labels = '''airplane automobile bird cat deer dog frog horse ship truck''' . split () # Show some misclassified examples # TODO: add label names misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( labels [ y_test [ i ]], labels [ p_test [ i ]])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 CIFAR"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR/#tensorflow-bootcamp-cnn-cifar","text":"by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) 2.0.0-beta1 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , GlobalMaxPooling2D from tensorflow.keras.models import Model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Conv2D ( 64 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Conv2D ( 128 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Flatten ()( x ) x = Dropout ( 0.5 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile and fit # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) Train on 50000 samples, validate on 10000 samples Epoch 1/15 50000/50000 [==============================] - 17s 336us/sample - loss: 1.5736 - accuracy: 0.4255 - val_loss: 1.3048 - val_accuracy: 0.5317 Epoch 2/15 50000/50000 [==============================] - 16s 320us/sample - loss: 1.2732 - accuracy: 0.5435 - val_loss: 1.1139 - val_accuracy: 0.6004 Epoch 3/15 50000/50000 [==============================] - 16s 321us/sample - loss: 1.1454 - accuracy: 0.5909 - val_loss: 1.0765 - val_accuracy: 0.6176 Epoch 4/15 50000/50000 [==============================] - 16s 325us/sample - loss: 1.0497 - accuracy: 0.6259 - val_loss: 0.9637 - val_accuracy: 0.6603 Epoch 5/15 50000/50000 [==============================] - 16s 325us/sample - loss: 0.9764 - accuracy: 0.6546 - val_loss: 0.9363 - val_accuracy: 0.6728 Epoch 6/15 50000/50000 [==============================] - 16s 323us/sample - loss: 0.9164 - accuracy: 0.6735 - val_loss: 0.9153 - val_accuracy: 0.6772 Epoch 7/15 50000/50000 [==============================] - 16s 322us/sample - loss: 0.8660 - accuracy: 0.6944 - val_loss: 0.8996 - val_accuracy: 0.6818 Epoch 8/15 50000/50000 [==============================] - 17s 330us/sample - loss: 0.8213 - accuracy: 0.7083 - val_loss: 0.8806 - val_accuracy: 0.6954 Epoch 9/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.7807 - accuracy: 0.7233 - val_loss: 0.8833 - val_accuracy: 0.6951 Epoch 10/15 50000/50000 [==============================] - 16s 327us/sample - loss: 0.7470 - accuracy: 0.7359 - val_loss: 0.8368 - val_accuracy: 0.7058 Epoch 11/15 50000/50000 [==============================] - 16s 325us/sample - loss: 0.7121 - accuracy: 0.7462 - val_loss: 0.8376 - val_accuracy: 0.7092 Epoch 12/15 50000/50000 [==============================] - 17s 330us/sample - loss: 0.6808 - accuracy: 0.7560 - val_loss: 0.8430 - val_accuracy: 0.7115 Epoch 13/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.6576 - accuracy: 0.7671 - val_loss: 0.8284 - val_accuracy: 0.7110 Epoch 14/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.6402 - accuracy: 0.7702 - val_loss: 0.8487 - val_accuracy: 0.7067 Epoch 15/15 50000/50000 [==============================] - 16s 328us/sample - loss: 0.6232 - accuracy: 0.7769 - val_loss: 0.8320 - val_accuracy: 0.7136 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f04d6f0db00> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f04d6b34208> # Plot confusion matrix from sklearn.metrics import confusion_matrix import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) Confusion matrix, without normalization [[741 16 36 26 12 2 6 11 113 37] [ 14 781 9 9 2 4 6 2 47 126] [ 62 9 545 91 102 66 50 29 28 18] [ 20 9 51 548 51 171 58 33 30 29] [ 40 10 62 79 617 43 47 78 19 5] [ 15 2 52 214 38 572 13 55 21 18] [ 6 9 45 101 43 44 711 7 18 16] [ 24 3 38 48 59 64 5 726 6 27] [ 49 30 9 17 6 3 1 3 865 17] [ 35 105 16 19 4 7 1 11 46 756]] # label mapping labels = '''airplane automobile bird cat deer dog frog horse ship truck''' . split () # Show some misclassified examples # TODO: add label names misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( labels [ y_test [ i ]], labels [ p_test [ i ]])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Tensorflow BootCamp - CNN CIFAR"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR_Improved/","text":"Tensorflow BootCamp - CNN CIFAR Improved \u00b6 by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.2.0-rc2 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , GlobalMaxPooling2D , MaxPooling2D , BatchNormalization from tensorflow.keras.models import Model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 13s 0us/step x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) # x = Conv2D(32, (3, 3), strides=2, activation='relu')(i) # x = Conv2D(64, (3, 3), strides=2, activation='relu')(x) # x = Conv2D(128, (3, 3), strides=2, activation='relu')(x) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( i ) x = BatchNormalization ()( x ) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) # x = Dropout(0.2)(x) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) # x = Dropout(0.2)(x) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) # x = Dropout(0.2)(x) # x = GlobalMaxPooling2D()(x) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # Fit r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 50 ) Epoch 1/50 1465/1563 [===========================>..] - ETA: 0s - loss: 1.2974 - accuracy: 0.5487 KeyboardInterrupt: ignored # Fit with data augmentation # Note: if you run this AFTER calling the previous model.fit(), it will CONTINUE training where it left off batch_size = 32 data_generator = tf . keras . preprocessing . image . ImageDataGenerator ( width_shift_range = 0.1 , height_shift_range = 0.1 , horizontal_flip = True ) train_generator = data_generator . flow ( x_train , y_train , batch_size ) steps_per_epoch = x_train . shape [ 0 ] // batch_size r = model . fit ( train_generator , validation_data = ( x_test , y_test ), steps_per_epoch = steps_per_epoch , epochs = 50 ) Epoch 1/50 1562/1562 [==============================] - 27s 17ms/step - loss: 0.9854 - accuracy: 0.6597 - val_loss: 0.9380 - val_accuracy: 0.6898 Epoch 2/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.8444 - accuracy: 0.7101 - val_loss: 0.8461 - val_accuracy: 0.7158 Epoch 3/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.7506 - accuracy: 0.7444 - val_loss: 0.7562 - val_accuracy: 0.7485 Epoch 4/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.6825 - accuracy: 0.7695 - val_loss: 0.6062 - val_accuracy: 0.7959 Epoch 5/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.6324 - accuracy: 0.7856 - val_loss: 0.6161 - val_accuracy: 0.7971 Epoch 6/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.5944 - accuracy: 0.7997 - val_loss: 0.6473 - val_accuracy: 0.7832 Epoch 7/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.5502 - accuracy: 0.8109 - val_loss: 0.6116 - val_accuracy: 0.8007 Epoch 8/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.5210 - accuracy: 0.8227 - val_loss: 0.6694 - val_accuracy: 0.7861 Epoch 9/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4947 - accuracy: 0.8300 - val_loss: 0.4850 - val_accuracy: 0.8358 Epoch 10/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4690 - accuracy: 0.8407 - val_loss: 0.5492 - val_accuracy: 0.8174 Epoch 11/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4480 - accuracy: 0.8479 - val_loss: 0.5357 - val_accuracy: 0.8212 Epoch 12/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4283 - accuracy: 0.8523 - val_loss: 0.5085 - val_accuracy: 0.8319 Epoch 13/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4125 - accuracy: 0.8577 - val_loss: 0.5201 - val_accuracy: 0.8308 Epoch 14/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.3929 - accuracy: 0.8662 - val_loss: 0.4446 - val_accuracy: 0.8510 Epoch 15/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3779 - accuracy: 0.8694 - val_loss: 0.4738 - val_accuracy: 0.8506 Epoch 16/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3706 - accuracy: 0.8721 - val_loss: 0.4617 - val_accuracy: 0.8504 Epoch 17/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3470 - accuracy: 0.8812 - val_loss: 0.4172 - val_accuracy: 0.8627 Epoch 18/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3405 - accuracy: 0.8818 - val_loss: 0.4572 - val_accuracy: 0.8587 Epoch 19/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3312 - accuracy: 0.8868 - val_loss: 0.4150 - val_accuracy: 0.8654 Epoch 20/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3203 - accuracy: 0.8880 - val_loss: 0.5443 - val_accuracy: 0.8273 Epoch 21/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3108 - accuracy: 0.8919 - val_loss: 0.4421 - val_accuracy: 0.8605 Epoch 22/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3017 - accuracy: 0.8964 - val_loss: 0.4778 - val_accuracy: 0.8537 Epoch 23/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2978 - accuracy: 0.8975 - val_loss: 0.4370 - val_accuracy: 0.8621 Epoch 24/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2830 - accuracy: 0.9023 - val_loss: 0.4270 - val_accuracy: 0.8676 Epoch 25/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2796 - accuracy: 0.9035 - val_loss: 0.4009 - val_accuracy: 0.8748 Epoch 26/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2714 - accuracy: 0.9069 - val_loss: 0.4017 - val_accuracy: 0.8719 Epoch 27/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2663 - accuracy: 0.9080 - val_loss: 0.4199 - val_accuracy: 0.8669 Epoch 28/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2547 - accuracy: 0.9121 - val_loss: 0.4094 - val_accuracy: 0.8703 Epoch 29/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2574 - accuracy: 0.9110 - val_loss: 0.4227 - val_accuracy: 0.8698 Epoch 30/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2464 - accuracy: 0.9141 - val_loss: 0.4117 - val_accuracy: 0.8649 Epoch 31/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2439 - accuracy: 0.9157 - val_loss: 0.4096 - val_accuracy: 0.8758 Epoch 32/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2411 - accuracy: 0.9159 - val_loss: 0.4118 - val_accuracy: 0.8705 Epoch 33/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2330 - accuracy: 0.9194 - val_loss: 0.3841 - val_accuracy: 0.8764 Epoch 34/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2312 - accuracy: 0.9201 - val_loss: 0.4127 - val_accuracy: 0.8708 Epoch 35/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2272 - accuracy: 0.9209 - val_loss: 0.4259 - val_accuracy: 0.8762 Epoch 36/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2204 - accuracy: 0.9241 - val_loss: 0.4246 - val_accuracy: 0.8769 Epoch 37/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2150 - accuracy: 0.9251 - val_loss: 0.3939 - val_accuracy: 0.8797 Epoch 38/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2156 - accuracy: 0.9261 - val_loss: 0.4005 - val_accuracy: 0.8790 Epoch 39/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2135 - accuracy: 0.9268 - val_loss: 0.3959 - val_accuracy: 0.8773 Epoch 40/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2121 - accuracy: 0.9263 - val_loss: 0.4072 - val_accuracy: 0.8742 Epoch 41/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2087 - accuracy: 0.9277 - val_loss: 0.4234 - val_accuracy: 0.8769 Epoch 42/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2023 - accuracy: 0.9310 - val_loss: 0.3904 - val_accuracy: 0.8812 Epoch 43/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1986 - accuracy: 0.9311 - val_loss: 0.3859 - val_accuracy: 0.8806 Epoch 44/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1936 - accuracy: 0.9341 - val_loss: 0.4627 - val_accuracy: 0.8703 Epoch 45/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1907 - accuracy: 0.9342 - val_loss: 0.4460 - val_accuracy: 0.8646 Epoch 46/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1884 - accuracy: 0.9341 - val_loss: 0.4511 - val_accuracy: 0.8658 Epoch 47/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1877 - accuracy: 0.9344 - val_loss: 0.3790 - val_accuracy: 0.8831 Epoch 48/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1851 - accuracy: 0.9366 - val_loss: 0.4208 - val_accuracy: 0.8770 Epoch 49/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1869 - accuracy: 0.9364 - val_loss: 0.3946 - val_accuracy: 0.8841 Epoch 50/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1767 - accuracy: 0.9397 - val_loss: 0.4432 - val_accuracy: 0.8781 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f1d5a2856d8> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f1d5a26ada0> # Plot confusion matrix from sklearn.metrics import confusion_matrix import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) Confusion matrix, without normalization [[908 14 12 2 1 0 2 2 33 26] [ 5 957 0 0 0 1 3 0 4 30] [ 42 4 817 18 28 18 43 11 7 12] [ 22 18 27 742 29 62 48 19 13 20] [ 16 2 39 22 847 16 34 15 5 4] [ 5 10 22 94 23 790 21 23 4 8] [ 4 2 16 18 4 2 946 1 4 3] [ 15 5 6 15 27 6 5 908 6 7] [ 37 12 1 3 0 0 1 2 921 23] [ 9 37 0 0 1 1 2 1 4 945]] # label mapping labels = '''airplane automobile bird cat deer dog frog horse ship truck''' . split () # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( labels [ y_test [ i ]], labels [ p_test [ i ]])); # Now that the model is so large, it's useful to summarize it model . summary () Model: \"model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 32, 32, 3)] 0 _________________________________________________________________ conv2d (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ batch_normalization (BatchNo (None, 32, 32, 32) 128 _________________________________________________________________ conv2d_1 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 32) 128 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ batch_normalization_2 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ batch_normalization_3 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 8, 8, 128) 73856 _________________________________________________________________ batch_normalization_4 (Batch (None, 8, 8, 128) 512 _________________________________________________________________ conv2d_5 (Conv2D) (None, 8, 8, 128) 147584 _________________________________________________________________ batch_normalization_5 (Batch (None, 8, 8, 128) 512 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 2048) 0 _________________________________________________________________ dropout (Dropout) (None, 2048) 0 _________________________________________________________________ dense (Dense) (None, 1024) 2098176 _________________________________________________________________ dropout_1 (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 10) 10250 ================================================================= Total params: 2,397,226 Trainable params: 2,396,330 Non-trainable params: 896 _________________________________________________________________ Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 CIFAR Improved"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR_Improved/#tensorflow-bootcamp-cnn-cifar-improved","text":"by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.2.0-rc2 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout , GlobalMaxPooling2D , MaxPooling2D , BatchNormalization from tensorflow.keras.models import Model # Load in the data cifar10 = tf . keras . datasets . cifar10 ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 y_train , y_test = y_train . flatten (), y_test . flatten () print ( \"x_train.shape:\" , x_train . shape ) print ( \"y_train.shape\" , y_train . shape ) Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 13s 0us/step x_train.shape: (50000, 32, 32, 3) y_train.shape (50000,) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) # x = Conv2D(32, (3, 3), strides=2, activation='relu')(i) # x = Conv2D(64, (3, 3), strides=2, activation='relu')(x) # x = Conv2D(128, (3, 3), strides=2, activation='relu')(x) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( i ) x = BatchNormalization ()( x ) x = Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) # x = Dropout(0.2)(x) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) # x = Dropout(0.2)(x) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )( x ) x = BatchNormalization ()( x ) x = MaxPooling2D (( 2 , 2 ))( x ) # x = Dropout(0.2)(x) # x = GlobalMaxPooling2D()(x) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 1024 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # Fit r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 50 ) Epoch 1/50 1465/1563 [===========================>..] - ETA: 0s - loss: 1.2974 - accuracy: 0.5487 KeyboardInterrupt: ignored # Fit with data augmentation # Note: if you run this AFTER calling the previous model.fit(), it will CONTINUE training where it left off batch_size = 32 data_generator = tf . keras . preprocessing . image . ImageDataGenerator ( width_shift_range = 0.1 , height_shift_range = 0.1 , horizontal_flip = True ) train_generator = data_generator . flow ( x_train , y_train , batch_size ) steps_per_epoch = x_train . shape [ 0 ] // batch_size r = model . fit ( train_generator , validation_data = ( x_test , y_test ), steps_per_epoch = steps_per_epoch , epochs = 50 ) Epoch 1/50 1562/1562 [==============================] - 27s 17ms/step - loss: 0.9854 - accuracy: 0.6597 - val_loss: 0.9380 - val_accuracy: 0.6898 Epoch 2/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.8444 - accuracy: 0.7101 - val_loss: 0.8461 - val_accuracy: 0.7158 Epoch 3/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.7506 - accuracy: 0.7444 - val_loss: 0.7562 - val_accuracy: 0.7485 Epoch 4/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.6825 - accuracy: 0.7695 - val_loss: 0.6062 - val_accuracy: 0.7959 Epoch 5/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.6324 - accuracy: 0.7856 - val_loss: 0.6161 - val_accuracy: 0.7971 Epoch 6/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.5944 - accuracy: 0.7997 - val_loss: 0.6473 - val_accuracy: 0.7832 Epoch 7/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.5502 - accuracy: 0.8109 - val_loss: 0.6116 - val_accuracy: 0.8007 Epoch 8/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.5210 - accuracy: 0.8227 - val_loss: 0.6694 - val_accuracy: 0.7861 Epoch 9/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4947 - accuracy: 0.8300 - val_loss: 0.4850 - val_accuracy: 0.8358 Epoch 10/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4690 - accuracy: 0.8407 - val_loss: 0.5492 - val_accuracy: 0.8174 Epoch 11/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4480 - accuracy: 0.8479 - val_loss: 0.5357 - val_accuracy: 0.8212 Epoch 12/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4283 - accuracy: 0.8523 - val_loss: 0.5085 - val_accuracy: 0.8319 Epoch 13/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.4125 - accuracy: 0.8577 - val_loss: 0.5201 - val_accuracy: 0.8308 Epoch 14/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.3929 - accuracy: 0.8662 - val_loss: 0.4446 - val_accuracy: 0.8510 Epoch 15/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3779 - accuracy: 0.8694 - val_loss: 0.4738 - val_accuracy: 0.8506 Epoch 16/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3706 - accuracy: 0.8721 - val_loss: 0.4617 - val_accuracy: 0.8504 Epoch 17/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3470 - accuracy: 0.8812 - val_loss: 0.4172 - val_accuracy: 0.8627 Epoch 18/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3405 - accuracy: 0.8818 - val_loss: 0.4572 - val_accuracy: 0.8587 Epoch 19/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3312 - accuracy: 0.8868 - val_loss: 0.4150 - val_accuracy: 0.8654 Epoch 20/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3203 - accuracy: 0.8880 - val_loss: 0.5443 - val_accuracy: 0.8273 Epoch 21/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3108 - accuracy: 0.8919 - val_loss: 0.4421 - val_accuracy: 0.8605 Epoch 22/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.3017 - accuracy: 0.8964 - val_loss: 0.4778 - val_accuracy: 0.8537 Epoch 23/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2978 - accuracy: 0.8975 - val_loss: 0.4370 - val_accuracy: 0.8621 Epoch 24/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2830 - accuracy: 0.9023 - val_loss: 0.4270 - val_accuracy: 0.8676 Epoch 25/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2796 - accuracy: 0.9035 - val_loss: 0.4009 - val_accuracy: 0.8748 Epoch 26/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2714 - accuracy: 0.9069 - val_loss: 0.4017 - val_accuracy: 0.8719 Epoch 27/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2663 - accuracy: 0.9080 - val_loss: 0.4199 - val_accuracy: 0.8669 Epoch 28/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2547 - accuracy: 0.9121 - val_loss: 0.4094 - val_accuracy: 0.8703 Epoch 29/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2574 - accuracy: 0.9110 - val_loss: 0.4227 - val_accuracy: 0.8698 Epoch 30/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2464 - accuracy: 0.9141 - val_loss: 0.4117 - val_accuracy: 0.8649 Epoch 31/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2439 - accuracy: 0.9157 - val_loss: 0.4096 - val_accuracy: 0.8758 Epoch 32/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2411 - accuracy: 0.9159 - val_loss: 0.4118 - val_accuracy: 0.8705 Epoch 33/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2330 - accuracy: 0.9194 - val_loss: 0.3841 - val_accuracy: 0.8764 Epoch 34/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2312 - accuracy: 0.9201 - val_loss: 0.4127 - val_accuracy: 0.8708 Epoch 35/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2272 - accuracy: 0.9209 - val_loss: 0.4259 - val_accuracy: 0.8762 Epoch 36/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2204 - accuracy: 0.9241 - val_loss: 0.4246 - val_accuracy: 0.8769 Epoch 37/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2150 - accuracy: 0.9251 - val_loss: 0.3939 - val_accuracy: 0.8797 Epoch 38/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2156 - accuracy: 0.9261 - val_loss: 0.4005 - val_accuracy: 0.8790 Epoch 39/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2135 - accuracy: 0.9268 - val_loss: 0.3959 - val_accuracy: 0.8773 Epoch 40/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2121 - accuracy: 0.9263 - val_loss: 0.4072 - val_accuracy: 0.8742 Epoch 41/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2087 - accuracy: 0.9277 - val_loss: 0.4234 - val_accuracy: 0.8769 Epoch 42/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.2023 - accuracy: 0.9310 - val_loss: 0.3904 - val_accuracy: 0.8812 Epoch 43/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1986 - accuracy: 0.9311 - val_loss: 0.3859 - val_accuracy: 0.8806 Epoch 44/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1936 - accuracy: 0.9341 - val_loss: 0.4627 - val_accuracy: 0.8703 Epoch 45/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1907 - accuracy: 0.9342 - val_loss: 0.4460 - val_accuracy: 0.8646 Epoch 46/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1884 - accuracy: 0.9341 - val_loss: 0.4511 - val_accuracy: 0.8658 Epoch 47/50 1562/1562 [==============================] - 26s 17ms/step - loss: 0.1877 - accuracy: 0.9344 - val_loss: 0.3790 - val_accuracy: 0.8831 Epoch 48/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1851 - accuracy: 0.9366 - val_loss: 0.4208 - val_accuracy: 0.8770 Epoch 49/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1869 - accuracy: 0.9364 - val_loss: 0.3946 - val_accuracy: 0.8841 Epoch 50/50 1562/1562 [==============================] - 26s 16ms/step - loss: 0.1767 - accuracy: 0.9397 - val_loss: 0.4432 - val_accuracy: 0.8781 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f1d5a2856d8> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f1d5a26ada0> # Plot confusion matrix from sklearn.metrics import confusion_matrix import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) Confusion matrix, without normalization [[908 14 12 2 1 0 2 2 33 26] [ 5 957 0 0 0 1 3 0 4 30] [ 42 4 817 18 28 18 43 11 7 12] [ 22 18 27 742 29 62 48 19 13 20] [ 16 2 39 22 847 16 34 15 5 4] [ 5 10 22 94 23 790 21 23 4 8] [ 4 2 16 18 4 2 946 1 4 3] [ 15 5 6 15 27 6 5 908 6 7] [ 37 12 1 3 0 0 1 2 921 23] [ 9 37 0 0 1 1 2 1 4 945]] # label mapping labels = '''airplane automobile bird cat deer dog frog horse ship truck''' . split () # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ], cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( labels [ y_test [ i ]], labels [ p_test [ i ]])); # Now that the model is so large, it's useful to summarize it model . summary () Model: \"model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 32, 32, 3)] 0 _________________________________________________________________ conv2d (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ batch_normalization (BatchNo (None, 32, 32, 32) 128 _________________________________________________________________ conv2d_1 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 32) 128 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ batch_normalization_2 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ batch_normalization_3 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 8, 8, 128) 73856 _________________________________________________________________ batch_normalization_4 (Batch (None, 8, 8, 128) 512 _________________________________________________________________ conv2d_5 (Conv2D) (None, 8, 8, 128) 147584 _________________________________________________________________ batch_normalization_5 (Batch (None, 8, 8, 128) 512 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 2048) 0 _________________________________________________________________ dropout (Dropout) (None, 2048) 0 _________________________________________________________________ dense (Dense) (None, 1024) 2098176 _________________________________________________________________ dropout_1 (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 10) 10250 ================================================================= Total params: 2,397,226 Trainable params: 2,396,330 Non-trainable params: 896 _________________________________________________________________ Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Tensorflow BootCamp - CNN CIFAR Improved"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_Fashion_MNIST/","text":"Tensorflow BootCamp - CNN Fashion_MNIST \u00b6 by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.1.0-rc1 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout from tensorflow.keras.models import Model # Load in the data fashion_mnist = tf . keras . datasets . fashion_mnist ( x_train , y_train ), ( x_test , y_test ) = fashion_mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # the data is only 2D! # convolution expects height x width x color x_train = np . expand_dims ( x_train , - 1 ) x_test = np . expand_dims ( x_test , - 1 ) print ( x_train . shape ) (60000, 28, 28, 1) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Conv2D ( 64 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Conv2D ( 128 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 512 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile and fit # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) Train on 60000 samples, validate on 10000 samples Epoch 1/15 60000/60000 [==============================] - 11s 175us/sample - loss: 0.5238 - accuracy: 0.8055 - val_loss: 0.3963 - val_accuracy: 0.8485 Epoch 2/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.3679 - accuracy: 0.8616 - val_loss: 0.3406 - val_accuracy: 0.8744 Epoch 3/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.3158 - accuracy: 0.8802 - val_loss: 0.3336 - val_accuracy: 0.8733 Epoch 4/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.2813 - accuracy: 0.8942 - val_loss: 0.3047 - val_accuracy: 0.8878 Epoch 5/15 60000/60000 [==============================] - 7s 109us/sample - loss: 0.2563 - accuracy: 0.9026 - val_loss: 0.3044 - val_accuracy: 0.8916 Epoch 6/15 60000/60000 [==============================] - 7s 110us/sample - loss: 0.2363 - accuracy: 0.9104 - val_loss: 0.2950 - val_accuracy: 0.8955 Epoch 7/15 60000/60000 [==============================] - 7s 108us/sample - loss: 0.2166 - accuracy: 0.9181 - val_loss: 0.2928 - val_accuracy: 0.8985 Epoch 8/15 60000/60000 [==============================] - 7s 111us/sample - loss: 0.1986 - accuracy: 0.9243 - val_loss: 0.2932 - val_accuracy: 0.9001 Epoch 9/15 60000/60000 [==============================] - 7s 112us/sample - loss: 0.1863 - accuracy: 0.9294 - val_loss: 0.3185 - val_accuracy: 0.8923 Epoch 10/15 60000/60000 [==============================] - 7s 109us/sample - loss: 0.1712 - accuracy: 0.9355 - val_loss: 0.3151 - val_accuracy: 0.8970 Epoch 11/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1582 - accuracy: 0.9398 - val_loss: 0.3154 - val_accuracy: 0.8998 Epoch 12/15 60000/60000 [==============================] - 6s 108us/sample - loss: 0.1509 - accuracy: 0.9414 - val_loss: 0.3235 - val_accuracy: 0.8979 Epoch 13/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1412 - accuracy: 0.9461 - val_loss: 0.3347 - val_accuracy: 0.9013 Epoch 14/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1326 - accuracy: 0.9489 - val_loss: 0.3652 - val_accuracy: 0.8961 Epoch 15/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1266 - accuracy: 0.9520 - val_loss: 0.3516 - val_accuracy: 0.9001 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f23402c8860> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f23243183c8> # Plot confusion matrix from sklearn.metrics import confusion_matrix import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) Confusion matrix, without normalization [[858 0 27 15 3 1 91 0 5 0] [ 1 975 2 14 3 0 3 0 2 0] [ 13 1 881 12 51 0 41 0 1 0] [ 12 6 20 887 53 1 20 0 1 0] [ 0 1 74 10 875 0 38 0 2 0] [ 0 0 1 0 0 977 0 12 1 9] [115 0 87 26 123 0 641 0 8 0] [ 0 0 0 0 0 10 0 962 0 28] [ 1 0 4 2 8 1 5 2 975 2] [ 0 0 0 0 0 4 1 25 0 970]] # Label mapping labels = '''T-shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot''' . split ( \" \\n \" ) # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ] . reshape ( 28 , 28 ), cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( labels [ y_test [ i ]], labels [ p_test [ i ]])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Fashion MNIST"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_Fashion_MNIST/#tensorflow-bootcamp-cnn-fashion_mnist","text":"by Jawad Haider # Install TensorFlow # !pip install -q tensorflow-gpu==2.0.0-beta1 try : % tensorflow_version 2. x # Colab only. except Exception : pass import tensorflow as tf print ( tf . __version__ ) `%tensorflow_version` only switches the major version: `1.x` or `2.x`. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. 2.1.0-rc1 # additional imports import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Input , Conv2D , Dense , Flatten , Dropout from tensorflow.keras.models import Model # Load in the data fashion_mnist = tf . keras . datasets . fashion_mnist ( x_train , y_train ), ( x_test , y_test ) = fashion_mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( \"x_train.shape:\" , x_train . shape ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step x_train.shape: (60000, 28, 28) # the data is only 2D! # convolution expects height x width x color x_train = np . expand_dims ( x_train , - 1 ) x_test = np . expand_dims ( x_test , - 1 ) print ( x_train . shape ) (60000, 28, 28, 1) # number of classes K = len ( set ( y_train )) print ( \"number of classes:\" , K ) number of classes: 10 # Build the model using the functional API i = Input ( shape = x_train [ 0 ] . shape ) x = Conv2D ( 32 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( i ) x = Conv2D ( 64 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Conv2D ( 128 , ( 3 , 3 ), strides = 2 , activation = 'relu' )( x ) x = Flatten ()( x ) x = Dropout ( 0.2 )( x ) x = Dense ( 512 , activation = 'relu' )( x ) x = Dropout ( 0.2 )( x ) x = Dense ( K , activation = 'softmax' )( x ) model = Model ( i , x ) # Compile and fit # Note: make sure you are using the GPU for this! model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) r = model . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 15 ) Train on 60000 samples, validate on 10000 samples Epoch 1/15 60000/60000 [==============================] - 11s 175us/sample - loss: 0.5238 - accuracy: 0.8055 - val_loss: 0.3963 - val_accuracy: 0.8485 Epoch 2/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.3679 - accuracy: 0.8616 - val_loss: 0.3406 - val_accuracy: 0.8744 Epoch 3/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.3158 - accuracy: 0.8802 - val_loss: 0.3336 - val_accuracy: 0.8733 Epoch 4/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.2813 - accuracy: 0.8942 - val_loss: 0.3047 - val_accuracy: 0.8878 Epoch 5/15 60000/60000 [==============================] - 7s 109us/sample - loss: 0.2563 - accuracy: 0.9026 - val_loss: 0.3044 - val_accuracy: 0.8916 Epoch 6/15 60000/60000 [==============================] - 7s 110us/sample - loss: 0.2363 - accuracy: 0.9104 - val_loss: 0.2950 - val_accuracy: 0.8955 Epoch 7/15 60000/60000 [==============================] - 7s 108us/sample - loss: 0.2166 - accuracy: 0.9181 - val_loss: 0.2928 - val_accuracy: 0.8985 Epoch 8/15 60000/60000 [==============================] - 7s 111us/sample - loss: 0.1986 - accuracy: 0.9243 - val_loss: 0.2932 - val_accuracy: 0.9001 Epoch 9/15 60000/60000 [==============================] - 7s 112us/sample - loss: 0.1863 - accuracy: 0.9294 - val_loss: 0.3185 - val_accuracy: 0.8923 Epoch 10/15 60000/60000 [==============================] - 7s 109us/sample - loss: 0.1712 - accuracy: 0.9355 - val_loss: 0.3151 - val_accuracy: 0.8970 Epoch 11/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1582 - accuracy: 0.9398 - val_loss: 0.3154 - val_accuracy: 0.8998 Epoch 12/15 60000/60000 [==============================] - 6s 108us/sample - loss: 0.1509 - accuracy: 0.9414 - val_loss: 0.3235 - val_accuracy: 0.8979 Epoch 13/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1412 - accuracy: 0.9461 - val_loss: 0.3347 - val_accuracy: 0.9013 Epoch 14/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1326 - accuracy: 0.9489 - val_loss: 0.3652 - val_accuracy: 0.8961 Epoch 15/15 60000/60000 [==============================] - 6s 107us/sample - loss: 0.1266 - accuracy: 0.9520 - val_loss: 0.3516 - val_accuracy: 0.9001 # Plot loss per iteration import matplotlib.pyplot as plt plt . plot ( r . history [ 'loss' ], label = 'loss' ) plt . plot ( r . history [ 'val_loss' ], label = 'val_loss' ) plt . legend () <matplotlib.legend.Legend at 0x7f23402c8860> # Plot accuracy per iteration plt . plot ( r . history [ 'accuracy' ], label = 'acc' ) plt . plot ( r . history [ 'val_accuracy' ], label = 'val_acc' ) plt . legend () <matplotlib.legend.Legend at 0x7f23243183c8> # Plot confusion matrix from sklearn.metrics import confusion_matrix import itertools def plot_confusion_matrix ( cm , classes , normalize = False , title = 'Confusion matrix' , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () tick_marks = np . arange ( len ( classes )) plt . xticks ( tick_marks , classes , rotation = 45 ) plt . yticks ( tick_marks , classes ) fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): plt . text ( j , i , format ( cm [ i , j ], fmt ), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label' ) plt . show () p_test = model . predict ( x_test ) . argmax ( axis = 1 ) cm = confusion_matrix ( y_test , p_test ) plot_confusion_matrix ( cm , list ( range ( 10 ))) Confusion matrix, without normalization [[858 0 27 15 3 1 91 0 5 0] [ 1 975 2 14 3 0 3 0 2 0] [ 13 1 881 12 51 0 41 0 1 0] [ 12 6 20 887 53 1 20 0 1 0] [ 0 1 74 10 875 0 38 0 2 0] [ 0 0 1 0 0 977 0 12 1 9] [115 0 87 26 123 0 641 0 8 0] [ 0 0 0 0 0 10 0 962 0 28] [ 1 0 4 2 8 1 5 2 975 2] [ 0 0 0 0 0 4 1 25 0 970]] # Label mapping labels = '''T-shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot''' . split ( \" \\n \" ) # Show some misclassified examples misclassified_idx = np . where ( p_test != y_test )[ 0 ] i = np . random . choice ( misclassified_idx ) plt . imshow ( x_test [ i ] . reshape ( 28 , 28 ), cmap = 'gray' ) plt . title ( \"True label: %s Predicted: %s \" % ( labels [ y_test [ i ]], labels [ p_test [ i ]])); Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Tensorflow BootCamp - CNN Fashion_MNIST"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Demo/","text":"Tensorflow BootCamp - Colab Basics \u00b6 by Jawad Haider This is my title import numpy as np import matplotlib.pyplot as plt x = np . linspace ( 0 , 10 * np . pi , 1000 ) y = np . sin ( x ) plt . plot ( x , y ) This is my title \u00b6 Here is some regular text. import numpy as np import sklearn print ( sklearn . __version__ ) import numpy print ( numpy . __version__ ) import scipy print ( scipy . __version__ ) import matplotlib print ( matplotlib . __version__ ) import pandas print ( pandas . __version__ ) import torch print ( torch . __version__ ) import seaborn print ( seaborn . __version__ ) import wordcloud print ( wordcloud . __version__ ) import bs4 print ( bs4 . __version__ ) import requests print ( requests . __version__ ) import theano print ( theano . __version__ ) import networkx print ( networkx . __version__ ) import cv2 print ( cv2 . __version__ ) import gym print ( gym . __version__ ) 0.21.2 1.16.4 1.3.0 3.0.3 0.24.2 1.1.0 0.9.0 1.5.0 4.6.3 2.21.0 1.0.4 2.3 3.4.3 0.10.11 ! ls sample_data ! ls sample_data anscombe.json mnist_test.csv california_housing_test.csv mnist_train_small.csv california_housing_train.csv README.md import json json . loads ( open ( 'sample_data/anscombe.json' ) . read ()) [{'Series': 'I', 'X': 10.0, 'Y': 8.04}, {'Series': 'I', 'X': 8.0, 'Y': 6.95}, {'Series': 'I', 'X': 13.0, 'Y': 7.58}, {'Series': 'I', 'X': 9.0, 'Y': 8.81}, {'Series': 'I', 'X': 11.0, 'Y': 8.33}, {'Series': 'I', 'X': 14.0, 'Y': 9.96}, {'Series': 'I', 'X': 6.0, 'Y': 7.24}, {'Series': 'I', 'X': 4.0, 'Y': 4.26}, {'Series': 'I', 'X': 12.0, 'Y': 10.84}, {'Series': 'I', 'X': 7.0, 'Y': 4.81}, {'Series': 'I', 'X': 5.0, 'Y': 5.68}, {'Series': 'II', 'X': 10.0, 'Y': 9.14}, {'Series': 'II', 'X': 8.0, 'Y': 8.14}, {'Series': 'II', 'X': 13.0, 'Y': 8.74}, {'Series': 'II', 'X': 9.0, 'Y': 8.77}, {'Series': 'II', 'X': 11.0, 'Y': 9.26}, {'Series': 'II', 'X': 14.0, 'Y': 8.1}, {'Series': 'II', 'X': 6.0, 'Y': 6.13}, {'Series': 'II', 'X': 4.0, 'Y': 3.1}, {'Series': 'II', 'X': 12.0, 'Y': 9.13}, {'Series': 'II', 'X': 7.0, 'Y': 7.26}, {'Series': 'II', 'X': 5.0, 'Y': 4.74}, {'Series': 'III', 'X': 10.0, 'Y': 7.46}, {'Series': 'III', 'X': 8.0, 'Y': 6.77}, {'Series': 'III', 'X': 13.0, 'Y': 12.74}, {'Series': 'III', 'X': 9.0, 'Y': 7.11}, {'Series': 'III', 'X': 11.0, 'Y': 7.81}, {'Series': 'III', 'X': 14.0, 'Y': 8.84}, {'Series': 'III', 'X': 6.0, 'Y': 6.08}, {'Series': 'III', 'X': 4.0, 'Y': 5.39}, {'Series': 'III', 'X': 12.0, 'Y': 8.15}, {'Series': 'III', 'X': 7.0, 'Y': 6.42}, {'Series': 'III', 'X': 5.0, 'Y': 5.73}, {'Series': 'IV', 'X': 8.0, 'Y': 6.58}, {'Series': 'IV', 'X': 8.0, 'Y': 5.76}, {'Series': 'IV', 'X': 8.0, 'Y': 7.71}, {'Series': 'IV', 'X': 8.0, 'Y': 8.84}, {'Series': 'IV', 'X': 8.0, 'Y': 8.47}, {'Series': 'IV', 'X': 8.0, 'Y': 7.04}, {'Series': 'IV', 'X': 8.0, 'Y': 5.25}, {'Series': 'IV', 'X': 19.0, 'Y': 12.5}, {'Series': 'IV', 'X': 8.0, 'Y': 5.56}, {'Series': 'IV', 'X': 8.0, 'Y': 7.91}, {'Series': 'IV', 'X': 8.0, 'Y': 6.89}] a = 5 print ( a ) NameError: ignored Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Demo"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Demo/#tensorflow-bootcamp-colab-basics","text":"by Jawad Haider This is my title import numpy as np import matplotlib.pyplot as plt x = np . linspace ( 0 , 10 * np . pi , 1000 ) y = np . sin ( x ) plt . plot ( x , y )","title":"Tensorflow BootCamp - Colab Basics"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Demo/#this-is-my-title","text":"Here is some regular text. import numpy as np import sklearn print ( sklearn . __version__ ) import numpy print ( numpy . __version__ ) import scipy print ( scipy . __version__ ) import matplotlib print ( matplotlib . __version__ ) import pandas print ( pandas . __version__ ) import torch print ( torch . __version__ ) import seaborn print ( seaborn . __version__ ) import wordcloud print ( wordcloud . __version__ ) import bs4 print ( bs4 . __version__ ) import requests print ( requests . __version__ ) import theano print ( theano . __version__ ) import networkx print ( networkx . __version__ ) import cv2 print ( cv2 . __version__ ) import gym print ( gym . __version__ ) 0.21.2 1.16.4 1.3.0 3.0.3 0.24.2 1.1.0 0.9.0 1.5.0 4.6.3 2.21.0 1.0.4 2.3 3.4.3 0.10.11 ! ls sample_data ! ls sample_data anscombe.json mnist_test.csv california_housing_test.csv mnist_train_small.csv california_housing_train.csv README.md import json json . loads ( open ( 'sample_data/anscombe.json' ) . read ()) [{'Series': 'I', 'X': 10.0, 'Y': 8.04}, {'Series': 'I', 'X': 8.0, 'Y': 6.95}, {'Series': 'I', 'X': 13.0, 'Y': 7.58}, {'Series': 'I', 'X': 9.0, 'Y': 8.81}, {'Series': 'I', 'X': 11.0, 'Y': 8.33}, {'Series': 'I', 'X': 14.0, 'Y': 9.96}, {'Series': 'I', 'X': 6.0, 'Y': 7.24}, {'Series': 'I', 'X': 4.0, 'Y': 4.26}, {'Series': 'I', 'X': 12.0, 'Y': 10.84}, {'Series': 'I', 'X': 7.0, 'Y': 4.81}, {'Series': 'I', 'X': 5.0, 'Y': 5.68}, {'Series': 'II', 'X': 10.0, 'Y': 9.14}, {'Series': 'II', 'X': 8.0, 'Y': 8.14}, {'Series': 'II', 'X': 13.0, 'Y': 8.74}, {'Series': 'II', 'X': 9.0, 'Y': 8.77}, {'Series': 'II', 'X': 11.0, 'Y': 9.26}, {'Series': 'II', 'X': 14.0, 'Y': 8.1}, {'Series': 'II', 'X': 6.0, 'Y': 6.13}, {'Series': 'II', 'X': 4.0, 'Y': 3.1}, {'Series': 'II', 'X': 12.0, 'Y': 9.13}, {'Series': 'II', 'X': 7.0, 'Y': 7.26}, {'Series': 'II', 'X': 5.0, 'Y': 4.74}, {'Series': 'III', 'X': 10.0, 'Y': 7.46}, {'Series': 'III', 'X': 8.0, 'Y': 6.77}, {'Series': 'III', 'X': 13.0, 'Y': 12.74}, {'Series': 'III', 'X': 9.0, 'Y': 7.11}, {'Series': 'III', 'X': 11.0, 'Y': 7.81}, {'Series': 'III', 'X': 14.0, 'Y': 8.84}, {'Series': 'III', 'X': 6.0, 'Y': 6.08}, {'Series': 'III', 'X': 4.0, 'Y': 5.39}, {'Series': 'III', 'X': 12.0, 'Y': 8.15}, {'Series': 'III', 'X': 7.0, 'Y': 6.42}, {'Series': 'III', 'X': 5.0, 'Y': 5.73}, {'Series': 'IV', 'X': 8.0, 'Y': 6.58}, {'Series': 'IV', 'X': 8.0, 'Y': 5.76}, {'Series': 'IV', 'X': 8.0, 'Y': 7.71}, {'Series': 'IV', 'X': 8.0, 'Y': 8.84}, {'Series': 'IV', 'X': 8.0, 'Y': 8.47}, {'Series': 'IV', 'X': 8.0, 'Y': 7.04}, {'Series': 'IV', 'X': 8.0, 'Y': 5.25}, {'Series': 'IV', 'X': 19.0, 'Y': 12.5}, {'Series': 'IV', 'X': 8.0, 'Y': 5.56}, {'Series': 'IV', 'X': 8.0, 'Y': 7.91}, {'Series': 'IV', 'X': 8.0, 'Y': 6.89}] a = 5 print ( a ) NameError: ignored Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"This is my title"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Installing_Tensorflow/","text":"Tensorflow BootCamp - Colab Basics Installing Tensorflow \u00b6 by Jawad Haider # What's already installed? # import tensorflow as tf # print(tf.__version__) 1.14.0 # Install TensorFlow 2.0 # You can run regular shell commands by prepending ! # !pip install -q tensorflow==2.0.0-beta1 # GPU version # !pip install -q tensorflow-gpu==2.0.0-beta1 ##### UPDATE 2020 ##### # new feature of colab - you can just use this try : % tensorflow_version 2. x # Colab only. except Exception : pass `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. # Check Tensorflow version again import tensorflow as tf print ( tf . __version__ ) 2.2.0-rc2 # How to install a library permanently? # https://stackoverflow.com/questions/55253498/how-do-i-install-a-library-permanently-in-colab # More fun with ! ! ls sample_data # More fun with ! # Nice! Looks like we already have some useful data to work with ! ls sample_data anscombe.json mnist_test.csv california_housing_test.csv mnist_train_small.csv california_housing_train.csv README.md Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ s","title":"TF2 0 Installing Tensorflow"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Installing_Tensorflow/#tensorflow-bootcamp-colab-basics-installing-tensorflow","text":"by Jawad Haider # What's already installed? # import tensorflow as tf # print(tf.__version__) 1.14.0 # Install TensorFlow 2.0 # You can run regular shell commands by prepending ! # !pip install -q tensorflow==2.0.0-beta1 # GPU version # !pip install -q tensorflow-gpu==2.0.0-beta1 ##### UPDATE 2020 ##### # new feature of colab - you can just use this try : % tensorflow_version 2. x # Colab only. except Exception : pass `%tensorflow_version` only switches the major version: 1.x or 2.x. You set: `2.x # Colab only.`. This will be interpreted as: `2.x`. TensorFlow 2.x selected. # Check Tensorflow version again import tensorflow as tf print ( tf . __version__ ) 2.2.0-rc2 # How to install a library permanently? # https://stackoverflow.com/questions/55253498/how-do-i-install-a-library-permanently-in-colab # More fun with ! ! ls sample_data # More fun with ! # Nice! Looks like we already have some useful data to work with ! ls sample_data anscombe.json mnist_test.csv california_housing_test.csv mnist_train_small.csv california_housing_train.csv README.md Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ s","title":"Tensorflow BootCamp - Colab Basics Installing Tensorflow"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/","text":"Tensorflow BootCamp - Colab Basics Loading Data \u00b6 by Jawad Haider Part 1: Using wget Part 2: Using tf.keras Part 3: Upload the file yourself Part 4: Access files from Google Drive Part 1: Using wget \u00b6 # download the data from a URL # source: https://archive.ics.uci.edu/ml/datasets/Arrhythmia # alternate URL: https://lazyprogrammer.me/course_files/arrhythmia.data #!wget --no-check-certificate https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data ! wget https : // lazyprogrammer . me / course_files / arrhythmia . data --2020-04-26 07:33:37-- https://lazyprogrammer.me/course_files/arrhythmia.data Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.31.80.48, 104.31.81.48, 2606:4700:3035::681f:5130, ... Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.31.80.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 402355 (393K) Saving to: \u2018arrhythmia.data\u2019 arrhythmia.data 100%[===================>] 392.92K 1.09MB/s in 0.4s 2020-04-26 07:33:38 (1.09 MB/s) - \u2018arrhythmia.data\u2019 saved [402355/402355] # list files in current directory ! ls arrhythmia.data sample_data # check if the data has a header ! head arrhythmia . data 75,0,190,80,91,193,371,174,121,-16,13,64,-2,?,63,0,52,44,0,0,32,0,0,0,0,0,0,0,44,20,36,0,28,0,0,0,0,0,0,52,40,0,0,0,60,0,0,0,0,0,0,52,0,0,0,0,0,0,0,0,0,0,0,0,56,36,0,0,32,0,0,0,0,0,0,48,32,0,0,0,56,0,0,0,0,0,0,80,0,0,0,0,0,0,0,0,0,0,0,0,40,52,0,0,28,0,0,0,0,0,0,0,48,48,0,0,32,0,0,0,0,0,0,0,52,52,0,0,36,0,0,0,0,0,0,0,52,48,0,0,32,0,0,0,0,0,0,0,56,44,0,0,32,0,0,0,0,0,0,-0.2,0.0,6.1,-1.0,0.0,0.0,0.6,2.1,13.6,30.8,0.0,0.0,1.7,-1.0,0.6,0.0,1.3,1.5,3.7,14.5,0.1,-5.2,1.4,0.0,0.0,0.0,0.8,-0.6,-10.7,-15.6,0.4,-3.9,0.0,0.0,0.0,0.0,-0.8,-1.7,-10.1,-22.0,0.0,0.0,5.7,-1.0,0.0,0.0,-0.1,1.2,14.1,22.5,0.0,-2.5,0.8,0.0,0.0,0.0,1.0,0.4,-4.8,-2.7,0.1,-6.0,0.0,0.0,0.0,0.0,-0.8,-0.6,-24.0,-29.7,0.0,0.0,2.0,-6.4,0.0,0.0,0.2,2.9,-12.6,15.2,-0.1,0.0,8.4,-10.0,0.0,0.0,0.6,5.9,-3.9,52.7,-0.3,0.0,15.2,-8.4,0.0,0.0,0.9,5.1,17.7,70.7,-0.4,0.0,13.5,-4.0,0.0,0.0,0.9,3.9,25.5,62.9,-0.3,0.0,9.0,-0.9,0.0,0.0,0.9,2.9,23.3,49.4,8 56,1,165,64,81,174,401,149,39,25,37,-17,31,?,53,0,48,0,0,0,24,0,0,0,0,0,0,0,64,0,0,0,24,0,0,0,0,0,0,32,24,0,0,0,40,0,0,0,0,0,0,48,0,0,0,0,0,0,0,0,0,0,0,0,44,20,0,0,24,0,0,0,0,0,0,0,60,0,0,0,20,0,0,0,0,0,0,0,24,52,0,0,16,0,0,0,0,0,0,0,32,52,0,0,20,0,0,0,0,0,0,0,44,48,0,0,32,0,0,0,0,0,0,0,48,44,0,0,32,0,0,0,0,0,0,0,48,40,0,0,28,0,0,0,0,0,0,0,48,0,0,0,28,0,0,0,0,0,0,-0.6,0.0,7.2,0.0,0.0,0.0,0.4,1.5,17.2,26.5,0.0,0.0,5.5,0.0,0.0,0.0,0.1,1.7,17.6,29.5,0.3,-1.6,0.9,0.0,0.0,0.0,-0.3,0.4,-1.5,1.3,0.1,-6.4,0.0,0.0,0.0,0.0,-0.3,-1.6,-15.3,-25.5,-0.3,0.0,4.2,-0.9,0.0,0.0,0.4,0.7,8.3,12.3,0.2,0.0,2.2,0.0,0.0,0.0,-0.2,0.8,6.6,11.7,0.4,0.0,1.0,-8.8,0.0,0.0,0.5,-0.6,-21.6,-26.8,0.4,0.0,2.6,-7.9,0.0,0.0,0.8,2.0,-16.4,1.2,0.0,0.0,5.8,-7.7,0.0,0.0,0.9,3.8,-5.7,27.7,-0.2,0.0,9.5,-5.0,0.0,0.0,0.5,2.6,11.8,34.6,-0.4,0.0,11.0,-2.4,0.0,0.0,0.4,2.6,21.6,43.4,-0.5,0.0,8.5,0.0,0.0,0.0,0.2,2.1,20.4,38.8,6 54,0,172,95,138,163,386,185,102,96,34,70,66,23,75,0,40,80,0,0,24,0,0,0,0,0,0,20,56,52,0,0,40,0,0,0,0,0,0,28,116,0,0,0,52,0,0,0,0,0,0,52,64,0,0,0,88,0,0,0,0,0,0,0,36,92,0,0,24,0,0,0,0,0,0,0,128,0,0,0,24,0,1,0,0,0,0,0,24,36,76,0,100,0,0,0,0,0,0,0,40,28,60,0,96,0,0,0,0,0,0,0,48,20,56,24,32,0,0,0,0,0,0,0,44,88,0,0,28,0,0,0,0,0,0,0,44,76,0,0,28,0,0,0,0,0,0,0,44,72,0,0,24,0,0,0,0,0,0,1.0,0.0,4.5,-2.8,0.0,0.0,0.3,2.5,-2.2,19.8,0.8,-0.4,6.4,-1.3,0.0,0.0,0.7,2.7,14.2,37.9,-0.2,-0.6,4.4,0.0,0.0,0.0,0.5,0.2,24.7,26.2,-1.0,-5.3,1.8,0.0,0.0,0.0,-0.5,-2.5,-8.0,-28.5,0.5,0.0,1.7,-2.7,0.0,0.0,-0.2,1.0,-9.4,-1.2,0.4,0.0,4.9,0.0,0.0,0.0,0.6,1.4,31.3,42.7,-0.8,0.0,0.7,-3.8,6.5,0.0,0.3,-3.3,18.7,-13.6,-0.9,0.0,2.2,-4.1,7.4,0.0,0.5,-2.4,20.9,-2.6,0.0,0.0,5.8,-4.1,4.0,-0.5,0.4,0.3,20.4,23.3,0.7,0.0,10.0,-5.7,0.0,0.0,0.5,2.2,-3.0,20.7,1.3,0.0,11.1,-3.4,0.0,0.0,0.4,3.4,11.5,48.2,0.9,0.0,9.5,-2.4,0.0,0.0,0.3,3.4,12.3,49.0,10 55,0,175,94,100,202,380,179,143,28,11,-5,20,?,71,0,72,20,0,0,48,0,0,0,0,0,0,0,64,36,0,0,36,0,0,0,0,0,0,20,52,48,0,0,56,0,0,0,0,0,0,64,32,0,0,0,72,0,0,0,0,0,0,0,60,12,0,0,44,0,0,0,0,0,0,0,60,44,0,0,32,0,0,0,0,0,0,56,0,0,0,0,0,0,0,0,0,0,0,0,40,44,0,0,20,0,0,0,0,0,0,0,52,40,0,0,32,0,0,0,0,0,0,0,56,48,0,0,36,0,0,0,0,0,0,0,60,48,0,0,36,0,0,0,0,0,0,0,64,40,0,0,40,0,0,0,0,0,0,0.9,0.0,7.8,-0.7,0.0,0.0,1.1,1.9,27.3,45.1,0.1,0.0,9.1,-2.6,0.0,0.0,0.4,1.5,24.5,36.8,-0.4,-0.4,1.6,-2.2,0.0,0.0,-1.0,-0.9,-1.5,-9.2,-0.4,-8.2,1.8,0.0,0.0,0.0,-0.7,-1.7,-23.4,-35.6,0.9,0.0,3.2,-0.4,0.0,0.0,0.7,1.2,9.4,18.0,-0.1,0.0,5.1,-2.5,0.0,0.0,0.3,0.6,9.8,12.6,1.6,-6.5,0.0,0.0,0.0,0.0,-0.4,-0.4,-18.2,-22.4,2.1,0.0,1.2,-6.9,0.0,0.0,-0.5,2.9,-12.7,18.0,0.7,0.0,9.0,-7.9,0.0,0.0,0.1,4.1,7.6,51.0,0.4,0.0,15.0,-5.5,0.0,0.0,0.1,3.3,28.8,63.1,0.1,0.0,15.2,-3.7,0.0,0.0,0.6,3.0,36.8,68.0,0.1,0.0,12.2,-2.2,0.0,0.0,0.4,2.6,34.6,61.6,1 75,0,190,80,88,181,360,177,103,-16,13,61,3,?,?,0,48,40,0,0,28,0,0,0,0,0,0,0,40,24,0,0,24,0,0,0,0,0,0,52,36,0,0,0,60,0,0,0,0,0,0,48,28,0,0,0,56,0,0,0,0,0,0,0,48,36,0,0,28,0,0,0,0,0,0,44,0,0,0,0,0,0,0,0,0,0,0,88,0,0,0,0,0,0,0,0,0,0,0,0,40,52,0,0,28,0,0,0,0,0,0,0,48,48,0,0,32,0,0,0,0,0,0,0,48,52,0,0,32,0,0,0,0,0,0,0,52,44,0,0,28,0,0,0,0,0,0,0,52,48,0,0,32,0,0,0,0,0,0,0.0,0.0,5.2,-1.4,0.0,0.0,0.9,2.3,9.6,31.6,0.1,0.0,1.6,-0.5,0.0,0.0,1.9,1.7,2.6,18.9,0.2,-3.8,1.2,0.0,0.0,0.0,1.0,-0.6,-7.7,-13.4,-0.1,-3.4,0.8,0.0,0.0,0.0,-1.4,-1.5,-7.0,-17.8,-0.1,0.0,4.4,-1.3,0.0,0.0,-0.1,1.1,8.2,16.5,0.6,-1.6,0.0,0.0,0.0,0.0,1.4,0.3,-3.5,-1.9,0.0,-5.7,0.0,0.0,0.0,0.0,-0.4,-0.5,-25.0,-30.0,-0.2,0.0,1.6,-6.0,0.0,0.0,-0.7,2.1,-12.4,8.6,-0.5,0.0,8.5,-10.2,0.0,0.0,-1.0,4.7,-4.0,43.0,-0.2,0.0,15.2,-7.8,0.0,0.0,-0.1,4.9,16.2,63.2,-0.2,0.0,9.1,-0.9,0.0,0.0,-0.2,2.9,21.7,48.9,-0.4,0.0,13.1,-3.6,0.0,0.0,-0.1,3.9,25.4,62.8,7 13,0,169,51,100,167,321,174,91,107,66,52,88,?,84,0,36,48,0,0,20,0,0,0,0,0,0,20,44,36,0,0,44,0,0,0,0,0,0,24,64,0,0,0,48,0,0,0,0,0,0,44,36,0,0,0,52,0,0,0,0,0,0,0,28,64,0,0,16,0,0,0,0,0,0,24,44,40,0,0,44,0,0,0,0,0,0,0,36,60,0,0,24,0,0,0,0,0,0,20,32,60,0,0,40,0,0,0,0,0,0,24,32,60,0,0,44,0,0,0,0,0,0,0,52,40,0,0,36,0,0,0,0,0,0,0,44,40,0,0,32,0,0,0,0,0,0,20,36,56,0,0,40,0,0,0,0,0,0,0.5,0.0,2.7,-6.4,0.0,0.0,0.9,1.7,-10.5,7.1,0.1,-1.2,19.1,-2.3,0.0,0.0,1.4,4.3,36.7,84.8,-0.4,-2.3,21.7,0.0,0.0,0.0,0.7,2.6,66.7,95.8,-0.2,-9.0,3.2,0.0,0.0,0.0,-1.1,-2.9,-14.1,-39.0,0.5,0.0,1.8,-12.9,0.0,0.0,0.4,-0.4,-38.7,-42.1,-0.1,-1.6,19.9,-0.7,0.0,0.0,1.0,3.3,40.4,65.4,0.4,0.0,6.7,-24.4,0.0,0.0,-1.2,0.4,-61.2,-59.9,0.9,-0.5,11.9,-43.3,0.0,0.0,0.8,3.4,-111.4,-95.1,2.0,-0.8,19.8,-48.4,0.0,0.0,1.6,8.7,-114.5,-72.8,2.0,0.0,31.0,-25.7,0.0,0.0,0.8,5.9,29.2,85.8,0.6,0.0,19.5,-11.4,0.0,0.0,0.8,3.3,20.1,49.1,0.0,-0.6,12.2,-2.8,0.0,0.0,0.9,2.2,13.5,31.1,14 40,1,160,52,77,129,377,133,77,77,49,75,65,?,70,0,44,0,0,0,24,0,0,0,0,0,0,0,40,32,0,0,24,0,0,0,0,0,0,0,44,28,0,0,24,0,0,0,0,0,0,44,16,0,0,0,48,0,0,0,0,0,0,36,0,0,0,0,0,0,0,0,0,0,0,0,44,16,0,0,24,0,0,0,0,0,0,0,20,56,0,0,12,0,0,0,0,0,0,0,24,56,0,0,16,0,0,0,0,0,0,0,36,48,0,0,24,0,0,0,0,0,0,0,40,44,0,0,28,0,0,0,0,0,0,0,40,44,0,0,24,0,0,0,0,0,0,0,44,0,0,0,24,0,0,0,0,0,0,-0.5,0.0,1.8,0.0,0.0,0.0,0.2,1.0,3.9,10.5,-0.1,0.0,7.6,-1.1,0.0,0.0,0.5,1.4,13.5,22.7,0.0,0.0,5.9,-0.5,0.0,0.0,0.3,0.6,12.2,15.0,0.1,-4.6,0.6,0.0,0.0,0.0,-0.4,-0.9,-9.7,-14.7,0.2,-2.1,0.0,0.0,0.0,0.0,-0.3,0.4,-3.7,-1.4,-0.2,0.0,6.8,-0.9,0.0,0.0,0.7,0.7,14.2,17.1,1.3,0.0,1.3,-11.5,0.0,0.0,-0.3,1.7,-30.9,-13.9,1.7,0.0,2.3,-17.5,0.0,0.0,-0.6,4.5,-46.3,-1.3,1.1,0.0,3.7,-11.0,0.0,0.0,-0.5,4.1,-19.8,21.2,0.1,0.0,7.7,-6.4,0.0,0.0,0.4,1.9,1.4,15.4,0.0,0.0,7.4,-2.5,0.0,0.0,0.4,1.3,9.3,18.9,-0.4,0.0,6.5,0.0,0.0,0.0,0.4,1.0,14.3,20.5,1 49,1,162,54,78,0,376,157,70,67,7,8,51,?,67,0,44,36,0,0,24,0,0,0,0,0,0,0,52,32,0,0,28,0,0,0,0,0,0,0,56,28,0,0,24,0,0,0,0,0,0,48,32,0,0,0,56,0,0,0,0,0,0,52,0,0,0,0,0,0,0,0,0,0,0,0,52,28,0,0,28,0,0,0,0,0,0,0,20,44,0,0,8,0,0,0,0,0,0,0,24,48,0,0,16,0,0,0,0,0,0,0,36,44,0,0,24,0,0,0,0,0,0,0,44,48,0,0,28,0,0,0,0,0,0,0,48,44,0,0,28,0,0,0,0,0,0,0,48,40,0,0,24,0,0,0,0,0,0,-0.3,0.0,4.1,-1.1,0.0,0.0,0.8,1.0,7.1,13.7,-0.3,0.0,8.4,-1.5,0.0,0.0,0.6,0.7,19.4,22.9,0.0,0.0,4.4,-0.8,0.0,0.0,-0.3,-0.6,11.2,6.9,0.1,-6.3,1.3,0.0,0.0,0.0,-0.6,-0.8,-13.1,-17.9,0.1,-0.8,0.0,0.0,0.0,0.0,0.6,0.7,-2.0,2.9,-0.2,0.0,6.3,-1.2,0.0,0.0,0.2,0.3,14.7,16.8,0.7,0.0,0.5,-7.3,0.0,0.0,0.2,-0.1,-15.5,-16.4,0.9,0.0,0.7,-8.9,0.0,0.0,0.6,2.5,-20.5,4.0,0.8,0.0,2.1,-9.0,0.0,0.0,0.6,3.8,-16.1,21.1,0.1,0.0,6.6,-4.1,0.0,0.0,0.3,1.4,4.7,14.2,-0.2,0.0,8.5,-2.7,0.0,0.0,0.1,0.8,14.5,20.9,-0.3,0.0,8.2,-1.9,0.0,0.0,0.1,0.5,15.8,19.8,1 44,0,168,56,84,118,354,160,63,61,69,78,66,84,64,0,40,0,0,0,20,0,0,0,0,0,0,0,44,12,0,0,28,0,0,0,0,0,0,0,36,8,0,0,20,0,0,0,0,0,0,40,12,0,0,0,44,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,36,12,0,0,20,0,0,0,0,0,0,0,20,56,0,0,12,0,0,0,0,0,0,0,24,48,0,0,12,0,0,0,0,0,0,0,28,44,0,0,16,0,0,0,0,0,0,0,44,32,0,0,32,0,0,0,0,0,0,0,44,28,0,0,28,0,0,0,0,0,0,0,40,24,0,0,24,0,0,0,0,0,0,0.1,0.0,2.3,0.0,0.0,0.0,0.4,1.0,4.6,11.6,1.2,0.0,5.4,-0.7,0.0,0.0,1.8,2.8,11.4,31.0,1.1,0.0,3.0,-0.4,0.0,0.0,1.4,1.8,5.3,17.9,-0.7,-3.9,0.5,0.0,0.0,0.0,-1.1,-1.9,-7.5,-20.4,-0.5,0.0,0.0,0.0,0.0,0.0,-0.6,-0.5,0.0,-3.4,1.1,0.0,4.2,-0.5,0.0,0.0,1.6,2.3,7.2,22.8,0.5,0.0,0.9,-5.5,0.0,0.0,-0.7,1.0,-14.5,-5.3,0.7,0.0,1.2,-6.4,0.0,0.0,-0.5,2.6,-13.9,10.0,1.5,0.0,2.4,-10.3,0.0,0.0,0.3,6.8,-19.3,43.2,0.8,0.0,7.9,-7.3,0.0,0.0,0.9,6.5,5.7,62.9,0.1,0.0,9.3,-3.8,0.0,0.0,0.8,3.8,15.1,48.5,0.1,0.0,7.0,-1.3,0.0,0.0,0.6,2.1,12.5,30.9,1 50,1,167,67,89,130,383,156,73,85,34,70,71,?,63,0,44,40,0,0,28,0,0,0,0,0,0,0,56,24,0,0,32,0,0,0,0,0,0,0,72,0,0,0,28,0,0,0,0,0,0,56,28,0,0,0,60,0,0,0,0,0,0,0,28,56,0,0,16,0,0,0,0,0,0,0,60,0,0,0,32,0,0,0,0,0,0,0,24,36,32,0,68,0,0,0,0,0,0,0,36,44,0,0,20,0,0,0,0,0,0,0,40,48,0,0,24,0,0,0,0,0,0,0,56,40,0,0,40,0,0,0,0,0,0,0,52,36,0,0,32,0,0,0,0,0,0,20,44,36,0,0,44,0,0,0,0,0,0,-0.1,0.0,3.5,-2.0,0.0,0.0,0.4,1.3,3.7,13.5,0.0,0.0,9.9,-0.8,0.0,0.0,1.2,1.2,26.8,35.2,0.0,0.0,8.3,0.0,0.0,0.0,0.8,0.3,29.8,32.0,0.1,-6.1,1.1,0.0,0.0,0.0,-0.6,-1.2,-15.5,-24.1,0.0,0.0,0.6,-4.1,0.0,0.0,-0.1,0.8,-10.6,-4.9,-0.2,0.0,8.9,0.0,0.0,0.0,0.8,0.7,26.7,30.2,0.1,0.0,1.3,-5.4,1.9,0.0,0.2,0.8,-5.2,2.1,0.8,0.0,4.4,-8.5,0.0,0.0,0.8,3.9,-10.8,25.0,0.4,0.0,4.3,-7.3,0.0,0.0,1.1,4.0,-8.9,27.9,-0.5,0.0,7.0,-3.2,0.0,0.0,1.1,1.3,13.2,22.3,-0.5,0.0,10.9,-2.5,0.0,0.0,1.0,1.0,23.8,29.6,-0.5,-0.6,10.8,-1.7,0.0,0.0,0.8,0.9,20.1,25.1,10 # check the data import pandas as pd df = pd . read_csv ( 'arrhythmia.data' , header = None ) # since the data has many columns, take just the first few and name them (as per the documentation) data = df [[ 0 , 1 , 2 , 3 , 4 , 5 ]] data . columns = [ 'age' , 'sex' , 'height' , 'weight' , 'QRS duration' , 'P-R interval' ] import matplotlib.pyplot as plt plt . rcParams [ 'figure.figsize' ] = [ 15 , 15 ] # make the plot bigger so the subplots don't overlap data . hist (); # use a semicolon to supress return value from pandas.plotting import scatter_matrix scatter_matrix ( data ); Part 2: Using tf.keras \u00b6 # use keras get_file to download the auto MPG dataset # source: https://archive.ics.uci.edu/ml/datasets/Auto+MPG #url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data' ### alternate URL url = 'https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/auto-mpg.data' # Install TensorFlow import tensorflow as tf print ( tf . __version__ ) 2.2.0-rc2 # check out the documentation for other arguments tf . keras . utils . get_file ( 'auto-mpg.data' , url ) Downloading data from https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/auto-mpg.data 32768/30286 [================================] - 0s 1us/step '/root/.keras/datasets/auto-mpg.data' ! head / root /. keras / datasets / auto - mpg . data 18.0 8 307.0 130.0 3504. 12.0 70 1 \"chevrolet chevelle malibu\" 15.0 8 350.0 165.0 3693. 11.5 70 1 \"buick skylark 320\" 18.0 8 318.0 150.0 3436. 11.0 70 1 \"plymouth satellite\" 16.0 8 304.0 150.0 3433. 12.0 70 1 \"amc rebel sst\" 17.0 8 302.0 140.0 3449. 10.5 70 1 \"ford torino\" 15.0 8 429.0 198.0 4341. 10.0 70 1 \"ford galaxie 500\" 14.0 8 454.0 220.0 4354. 9.0 70 1 \"chevrolet impala\" 14.0 8 440.0 215.0 4312. 8.5 70 1 \"plymouth fury iii\" 14.0 8 455.0 225.0 4425. 10.0 70 1 \"pontiac catalina\" 15.0 8 390.0 190.0 3850. 8.5 70 1 \"amc ambassador dpl\" # unless you specify an alternative path, the data will go into /root/.keras/datasets/ df = pd . read_csv ( '/root/.keras/datasets/auto-mpg.data' , header = None , delim_whitespace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 0 18.0 8 307.0 130.0 3504.0 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165.0 3693.0 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150.0 3436.0 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150.0 3433.0 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140.0 3449.0 10.5 70 1 ford torino Part 3: Upload the file yourself \u00b6 # another method: upload your own file ##### PLEASE NOTE: IT DOES NOT MATTER WHICH FILE YOU UPLOAD ##### YOU CAN UPLOAD ANY FILE YOU WANT ##### IN FACT, YOU ARE ENCOURAGED TO EXPLORE ON YOUR OWN # if you must, then get the file from here: # https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/daily-minimum-temperatures-in-me.csv from google.colab import files uploaded = files . upload () <input type=\"file\" id=\"files-f85a97ca-0bfe-4c55-8369-d830289c8925\" name=\"files[]\" multiple disabled /> <output id=\"result-f85a97ca-0bfe-4c55-8369-d830289c8925\"> Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. </output> <script src=\"/nbextensions/google.colab/files.js\"></script> Saving daily-minimum-temperatures-in-me.csv to daily-minimum-temperatures-in-me.csv uploaded {'daily-minimum-temperatures-in-me.csv': b'\"Date\",\"Daily minimum temperatures in Melbourne, Australia, 1981-1990\"\\r\\n\"1981-01-01\",20.7\\r\\n\"1981-01-02\",17.9\\r\\n\"1981-01-03\",18.8\\r\\n\"1981-01-04\",14.6\\r\\n\"1981-01-05\",15.8\\r\\n\"1981-01-06\",15.8\\r\\n\"1981-01-07\",15.8\\r\\n\"1981-01-08\",17.4\\r\\n\"1981-01-09\",21.8\\r\\n\"1981-01-10\",20.0\\r\\n\"1981-01-11\",16.2\\r\\n\"1981-01-12\",13.3\\r\\n\"1981-01-13\",16.7\\r\\n\"1981-01-14\",21.5\\r\\n\"1981-01-15\",25.0\\r\\n\"1981-01-16\",20.7\\r\\n\"1981-01-17\",20.6\\r\\n\"1981-01-18\",24.8\\r\\n\"1981-01-19\",17.7\\r\\n\"1981-01-20\",15.5\\r\\n\"1981-01-21\",18.2\\r\\n\"1981-01-22\",12.1\\r\\n\"1981-01-23\",14.4\\r\\n\"1981-01-24\",16.0\\r\\n\"1981-01-25\",16.5\\r\\n\"1981-01-26\",18.7\\r\\n\"1981-01-27\",19.4\\r\\n\"1981-01-28\",17.2\\r\\n\"1981-01-29\",15.5\\r\\n\"1981-01-30\",15.1\\r\\n\"1981-01-31\",15.4\\r\\n\"1981-02-01\",15.3\\r\\n\"1981-02-02\",18.8\\r\\n\"1981-02-03\",21.9\\r\\n\"1981-02-04\",19.9\\r\\n\"1981-02-05\",16.6\\r\\n\"1981-02-06\",16.8\\r\\n\"1981-02-07\",14.6\\r\\n\"1981-02-08\",17.1\\r\\n\"1981-02-09\",25.0\\r\\n\"1981-02-10\",15.0\\r\\n\"1981-02-11\",13.7\\r\\n\"1981-02-12\",13.9\\r\\n\"1981-02-13\",18.3\\r\\n\"1981-02-14\",22.0\\r\\n\"1981-02-15\",22.1\\r\\n\"1981-02-16\",21.2\\r\\n\"1981-02-17\",18.4\\r\\n\"1981-02-18\",16.6\\r\\n\"1981-02-19\",16.1\\r\\n\"1981-02-20\",15.7\\r\\n\"1981-02-21\",16.6\\r\\n\"1981-02-22\",16.5\\r\\n\"1981-02-23\",14.4\\r\\n\"1981-02-24\",14.4\\r\\n\"1981-02-25\",18.5\\r\\n\"1981-02-26\",16.9\\r\\n\"1981-02-27\",17.5\\r\\n\"1981-02-28\",21.2\\r\\n\"1981-03-01\",17.8\\r\\n\"1981-03-02\",18.6\\r\\n\"1981-03-03\",17.0\\r\\n\"1981-03-04\",16.0\\r\\n\"1981-03-05\",13.3\\r\\n\"1981-03-06\",14.3\\r\\n\"1981-03-07\",11.4\\r\\n\"1981-03-08\",16.3\\r\\n\"1981-03-09\",16.1\\r\\n\"1981-03-10\",11.8\\r\\n\"1981-03-11\",12.2\\r\\n\"1981-03-12\",14.7\\r\\n\"1981-03-13\",11.8\\r\\n\"1981-03-14\",11.3\\r\\n\"1981-03-15\",10.6\\r\\n\"1981-03-16\",11.7\\r\\n\"1981-03-17\",14.2\\r\\n\"1981-03-18\",11.2\\r\\n\"1981-03-19\",16.9\\r\\n\"1981-03-20\",16.7\\r\\n\"1981-03-21\",8.1\\r\\n\"1981-03-22\",8.0\\r\\n\"1981-03-23\",8.8\\r\\n\"1981-03-24\",13.4\\r\\n\"1981-03-25\",10.9\\r\\n\"1981-03-26\",13.4\\r\\n\"1981-03-27\",11.0\\r\\n\"1981-03-28\",15.0\\r\\n\"1981-03-29\",15.7\\r\\n\"1981-03-30\",14.5\\r\\n\"1981-03-31\",15.8\\r\\n\"1981-04-01\",16.7\\r\\n\"1981-04-02\",16.8\\r\\n\"1981-04-03\",17.5\\r\\n\"1981-04-04\",17.1\\r\\n\"1981-04-05\",18.1\\r\\n\"1981-04-06\",16.6\\r\\n\"1981-04-07\",10.0\\r\\n\"1981-04-08\",14.9\\r\\n\"1981-04-09\",15.9\\r\\n\"1981-04-10\",13.0\\r\\n\"1981-04-11\",7.6\\r\\n\"1981-04-12\",11.5\\r\\n\"1981-04-13\",13.5\\r\\n\"1981-04-14\",13.0\\r\\n\"1981-04-15\",13.3\\r\\n\"1981-04-16\",12.1\\r\\n\"1981-04-17\",12.4\\r\\n\"1981-04-18\",13.2\\r\\n\"1981-04-19\",13.8\\r\\n\"1981-04-20\",10.6\\r\\n\"1981-04-21\",9.0\\r\\n\"1981-04-22\",10.0\\r\\n\"1981-04-23\",9.8\\r\\n\"1981-04-24\",11.5\\r\\n\"1981-04-25\",8.9\\r\\n\"1981-04-26\",7.4\\r\\n\"1981-04-27\",9.9\\r\\n\"1981-04-28\",9.3\\r\\n\"1981-04-29\",9.9\\r\\n\"1981-04-30\",7.4\\r\\n\"1981-05-01\",8.6\\r\\n\"1981-05-02\",11.9\\r\\n\"1981-05-03\",14.0\\r\\n\"1981-05-04\",8.6\\r\\n\"1981-05-05\",10.0\\r\\n\"1981-05-06\",13.5\\r\\n\"1981-05-07\",12.0\\r\\n\"1981-05-08\",10.5\\r\\n\"1981-05-09\",10.7\\r\\n\"1981-05-10\",8.1\\r\\n\"1981-05-11\",10.1\\r\\n\"1981-05-12\",10.6\\r\\n\"1981-05-13\",5.3\\r\\n\"1981-05-14\",6.6\\r\\n\"1981-05-15\",8.5\\r\\n\"1981-05-16\",11.2\\r\\n\"1981-05-17\",9.8\\r\\n\"1981-05-18\",5.9\\r\\n\"1981-05-19\",3.2\\r\\n\"1981-05-20\",2.1\\r\\n\"1981-05-21\",3.4\\r\\n\"1981-05-22\",5.4\\r\\n\"1981-05-23\",9.6\\r\\n\"1981-05-24\",11.5\\r\\n\"1981-05-25\",12.3\\r\\n\"1981-05-26\",12.6\\r\\n\"1981-05-27\",11.0\\r\\n\"1981-05-28\",11.2\\r\\n\"1981-05-29\",11.4\\r\\n\"1981-05-30\",11.8\\r\\n\"1981-05-31\",12.8\\r\\n\"1981-06-01\",11.6\\r\\n\"1981-06-02\",10.6\\r\\n\"1981-06-03\",9.8\\r\\n\"1981-06-04\",11.2\\r\\n\"1981-06-05\",5.7\\r\\n\"1981-06-06\",7.1\\r\\n\"1981-06-07\",2.5\\r\\n\"1981-06-08\",3.5\\r\\n\"1981-06-09\",4.6\\r\\n\"1981-06-10\",11.0\\r\\n\"1981-06-11\",5.7\\r\\n\"1981-06-12\",7.7\\r\\n\"1981-06-13\",10.4\\r\\n\"1981-06-14\",11.4\\r\\n\"1981-06-15\",9.2\\r\\n\"1981-06-16\",6.1\\r\\n\"1981-06-17\",2.7\\r\\n\"1981-06-18\",4.3\\r\\n\"1981-06-19\",6.3\\r\\n\"1981-06-20\",3.8\\r\\n\"1981-06-21\",4.4\\r\\n\"1981-06-22\",7.1\\r\\n\"1981-06-23\",4.8\\r\\n\"1981-06-24\",5.8\\r\\n\"1981-06-25\",6.2\\r\\n\"1981-06-26\",7.3\\r\\n\"1981-06-27\",9.2\\r\\n\"1981-06-28\",10.2\\r\\n\"1981-06-29\",9.5\\r\\n\"1981-06-30\",9.5\\r\\n\"1981-07-01\",10.7\\r\\n\"1981-07-02\",10.0\\r\\n\"1981-07-03\",6.5\\r\\n\"1981-07-04\",7.0\\r\\n\"1981-07-05\",7.4\\r\\n\"1981-07-06\",8.1\\r\\n\"1981-07-07\",6.6\\r\\n\"1981-07-08\",8.3\\r\\n\"1981-07-09\",8.9\\r\\n\"1981-07-10\",4.6\\r\\n\"1981-07-11\",6.8\\r\\n\"1981-07-12\",5.7\\r\\n\"1981-07-13\",6.1\\r\\n\"1981-07-14\",7.0\\r\\n\"1981-07-15\",7.2\\r\\n\"1981-07-16\",6.3\\r\\n\"1981-07-17\",8.8\\r\\n\"1981-07-18\",5.0\\r\\n\"1981-07-19\",7.4\\r\\n\"1981-07-20\",10.1\\r\\n\"1981-07-21\",12.0\\r\\n\"1981-07-22\",9.0\\r\\n\"1981-07-23\",8.9\\r\\n\"1981-07-24\",9.8\\r\\n\"1981-07-25\",9.0\\r\\n\"1981-07-26\",9.2\\r\\n\"1981-07-27\",7.7\\r\\n\"1981-07-28\",8.0\\r\\n\"1981-07-29\",6.1\\r\\n\"1981-07-30\",3.5\\r\\n\"1981-07-31\",3.2\\r\\n\"1981-08-01\",5.7\\r\\n\"1981-08-02\",7.7\\r\\n\"1981-08-03\",9.0\\r\\n\"1981-08-04\",10.0\\r\\n\"1981-08-05\",6.2\\r\\n\"1981-08-06\",6.9\\r\\n\"1981-08-07\",6.5\\r\\n\"1981-08-08\",6.8\\r\\n\"1981-08-09\",7.0\\r\\n\"1981-08-10\",5.2\\r\\n\"1981-08-11\",3.0\\r\\n\"1981-08-12\",5.6\\r\\n\"1981-08-13\",7.9\\r\\n\"1981-08-14\",9.0\\r\\n\"1981-08-15\",8.6\\r\\n\"1981-08-16\",10.3\\r\\n\"1981-08-17\",10.5\\r\\n\"1981-08-18\",7.6\\r\\n\"1981-08-19\",9.7\\r\\n\"1981-08-20\",12.5\\r\\n\"1981-08-21\",7.4\\r\\n\"1981-08-22\",7.9\\r\\n\"1981-08-23\",3.9\\r\\n\"1981-08-24\",6.6\\r\\n\"1981-08-25\",4.6\\r\\n\"1981-08-26\",7.0\\r\\n\"1981-08-27\",6.0\\r\\n\"1981-08-28\",5.5\\r\\n\"1981-08-29\",8.1\\r\\n\"1981-08-30\",5.5\\r\\n\"1981-08-31\",6.2\\r\\n\"1981-09-01\",8.0\\r\\n\"1981-09-02\",10.3\\r\\n\"1981-09-03\",9.8\\r\\n\"1981-09-04\",9.6\\r\\n\"1981-09-05\",8.5\\r\\n\"1981-09-06\",7.5\\r\\n\"1981-09-07\",11.2\\r\\n\"1981-09-08\",14.6\\r\\n\"1981-09-09\",11.7\\r\\n\"1981-09-10\",7.8\\r\\n\"1981-09-11\",12.3\\r\\n\"1981-09-12\",10.1\\r\\n\"1981-09-13\",11.5\\r\\n\"1981-09-14\",7.3\\r\\n\"1981-09-15\",10.9\\r\\n\"1981-09-16\",14.1\\r\\n\"1981-09-17\",10.7\\r\\n\"1981-09-18\",16.9\\r\\n\"1981-09-19\",10.5\\r\\n\"1981-09-20\",6.5\\r\\n\"1981-09-21\",11.0\\r\\n\"1981-09-22\",6.3\\r\\n\"1981-09-23\",10.5\\r\\n\"1981-09-24\",7.2\\r\\n\"1981-09-25\",7.6\\r\\n\"1981-09-26\",10.7\\r\\n\"1981-09-27\",7.8\\r\\n\"1981-09-28\",9.6\\r\\n\"1981-09-29\",11.4\\r\\n\"1981-09-30\",12.4\\r\\n\"1981-10-01\",8.9\\r\\n\"1981-10-02\",13.2\\r\\n\"1981-10-03\",8.6\\r\\n\"1981-10-04\",6.2\\r\\n\"1981-10-05\",11.4\\r\\n\"1981-10-06\",13.2\\r\\n\"1981-10-07\",14.3\\r\\n\"1981-10-08\",7.3\\r\\n\"1981-10-09\",12.9\\r\\n\"1981-10-10\",7.8\\r\\n\"1981-10-11\",6.2\\r\\n\"1981-10-12\",5.6\\r\\n\"1981-10-13\",10.0\\r\\n\"1981-10-14\",13.3\\r\\n\"1981-10-15\",8.3\\r\\n\"1981-10-16\",10.2\\r\\n\"1981-10-17\",8.6\\r\\n\"1981-10-18\",7.3\\r\\n\"1981-10-19\",10.4\\r\\n\"1981-10-20\",11.2\\r\\n\"1981-10-21\",13.2\\r\\n\"1981-10-22\",11.4\\r\\n\"1981-10-23\",9.1\\r\\n\"1981-10-24\",6.6\\r\\n\"1981-10-25\",8.4\\r\\n\"1981-10-26\",9.7\\r\\n\"1981-10-27\",13.2\\r\\n\"1981-10-28\",12.5\\r\\n\"1981-10-29\",11.0\\r\\n\"1981-10-30\",11.0\\r\\n\"1981-10-31\",11.7\\r\\n\"1981-11-01\",9.2\\r\\n\"1981-11-02\",11.5\\r\\n\"1981-11-03\",13.6\\r\\n\"1981-11-04\",13.7\\r\\n\"1981-11-05\",10.4\\r\\n\"1981-11-06\",11.5\\r\\n\"1981-11-07\",7.6\\r\\n\"1981-11-08\",9.6\\r\\n\"1981-11-09\",14.2\\r\\n\"1981-11-10\",15.7\\r\\n\"1981-11-11\",10.5\\r\\n\"1981-11-12\",10.5\\r\\n\"1981-11-13\",9.7\\r\\n\"1981-11-14\",9.5\\r\\n\"1981-11-15\",11.3\\r\\n\"1981-11-16\",8.9\\r\\n\"1981-11-17\",9.4\\r\\n\"1981-11-18\",11.9\\r\\n\"1981-11-19\",11.7\\r\\n\"1981-11-20\",13.4\\r\\n\"1981-11-21\",12.6\\r\\n\"1981-11-22\",10.1\\r\\n\"1981-11-23\",15.8\\r\\n\"1981-11-24\",13.6\\r\\n\"1981-11-25\",11.9\\r\\n\"1981-11-26\",9.9\\r\\n\"1981-11-27\",12.6\\r\\n\"1981-11-28\",17.8\\r\\n\"1981-11-29\",15.0\\r\\n\"1981-11-30\",13.6\\r\\n\"1981-12-01\",13.4\\r\\n\"1981-12-02\",10.5\\r\\n\"1981-12-03\",14.2\\r\\n\"1981-12-04\",11.5\\r\\n\"1981-12-05\",13.0\\r\\n\"1981-12-06\",15.0\\r\\n\"1981-12-07\",14.7\\r\\n\"1981-12-08\",12.6\\r\\n\"1981-12-09\",12.5\\r\\n\"1981-12-10\",13.5\\r\\n\"1981-12-11\",14.8\\r\\n\"1981-12-12\",17.2\\r\\n\"1981-12-13\",9.7\\r\\n\"1981-12-14\",12.1\\r\\n\"1981-12-15\",12.8\\r\\n\"1981-12-16\",11.2\\r\\n\"1981-12-17\",16.4\\r\\n\"1981-12-18\",15.6\\r\\n\"1981-12-19\",13.3\\r\\n\"1981-12-20\",11.0\\r\\n\"1981-12-21\",11.1\\r\\n\"1981-12-22\",15.0\\r\\n\"1981-12-23\",12.8\\r\\n\"1981-12-24\",15.0\\r\\n\"1981-12-25\",14.2\\r\\n\"1981-12-26\",14.0\\r\\n\"1981-12-27\",15.5\\r\\n\"1981-12-28\",13.3\\r\\n\"1981-12-29\",15.6\\r\\n\"1981-12-30\",15.2\\r\\n\"1981-12-31\",17.4\\r\\n\"1982-01-01\",17.0\\r\\n\"1982-01-02\",15.0\\r\\n\"1982-01-03\",13.5\\r\\n\"1982-01-04\",15.2\\r\\n\"1982-01-05\",13.0\\r\\n\"1982-01-06\",12.5\\r\\n\"1982-01-07\",14.1\\r\\n\"1982-01-08\",14.8\\r\\n\"1982-01-09\",16.2\\r\\n\"1982-01-10\",15.8\\r\\n\"1982-01-11\",19.1\\r\\n\"1982-01-12\",22.2\\r\\n\"1982-01-13\",15.9\\r\\n\"1982-01-14\",13.0\\r\\n\"1982-01-15\",14.1\\r\\n\"1982-01-16\",15.8\\r\\n\"1982-01-17\",24.0\\r\\n\"1982-01-18\",18.0\\r\\n\"1982-01-19\",19.7\\r\\n\"1982-01-20\",25.2\\r\\n\"1982-01-21\",20.5\\r\\n\"1982-01-22\",19.3\\r\\n\"1982-01-23\",15.8\\r\\n\"1982-01-24\",17.0\\r\\n\"1982-01-25\",18.4\\r\\n\"1982-01-26\",13.3\\r\\n\"1982-01-27\",14.6\\r\\n\"1982-01-28\",12.5\\r\\n\"1982-01-29\",17.0\\r\\n\"1982-01-30\",17.1\\r\\n\"1982-01-31\",14.0\\r\\n\"1982-02-01\",14.6\\r\\n\"1982-02-02\",13.3\\r\\n\"1982-02-03\",14.8\\r\\n\"1982-02-04\",15.1\\r\\n\"1982-02-05\",13.1\\r\\n\"1982-02-06\",13.6\\r\\n\"1982-02-07\",19.5\\r\\n\"1982-02-08\",22.7\\r\\n\"1982-02-09\",17.2\\r\\n\"1982-02-10\",13.5\\r\\n\"1982-02-11\",15.4\\r\\n\"1982-02-12\",17.0\\r\\n\"1982-02-13\",19.2\\r\\n\"1982-02-14\",22.8\\r\\n\"1982-02-15\",26.3\\r\\n\"1982-02-16\",18.2\\r\\n\"1982-02-17\",17.0\\r\\n\"1982-02-18\",14.8\\r\\n\"1982-02-19\",12.8\\r\\n\"1982-02-20\",15.5\\r\\n\"1982-02-21\",15.6\\r\\n\"1982-02-22\",13.1\\r\\n\"1982-02-23\",15.2\\r\\n\"1982-02-24\",14.1\\r\\n\"1982-02-25\",12.5\\r\\n\"1982-02-26\",14.6\\r\\n\"1982-02-27\",10.4\\r\\n\"1982-02-28\",13.9\\r\\n\"1982-03-01\",11.9\\r\\n\"1982-03-02\",13.5\\r\\n\"1982-03-03\",9.8\\r\\n\"1982-03-04\",14.0\\r\\n\"1982-03-05\",21.5\\r\\n\"1982-03-06\",19.5\\r\\n\"1982-03-07\",16.7\\r\\n\"1982-03-08\",19.1\\r\\n\"1982-03-09\",11.0\\r\\n\"1982-03-10\",9.0\\r\\n\"1982-03-11\",10.0\\r\\n\"1982-03-12\",14.6\\r\\n\"1982-03-13\",12.5\\r\\n\"1982-03-14\",17.2\\r\\n\"1982-03-15\",19.2\\r\\n\"1982-03-16\",22.2\\r\\n\"1982-03-17\",15.7\\r\\n\"1982-03-18\",14.2\\r\\n\"1982-03-19\",9.8\\r\\n\"1982-03-20\",14.0\\r\\n\"1982-03-21\",17.5\\r\\n\"1982-03-22\",20.7\\r\\n\"1982-03-23\",15.6\\r\\n\"1982-03-24\",13.2\\r\\n\"1982-03-25\",14.5\\r\\n\"1982-03-26\",16.8\\r\\n\"1982-03-27\",17.2\\r\\n\"1982-03-28\",13.4\\r\\n\"1982-03-29\",14.2\\r\\n\"1982-03-30\",14.3\\r\\n\"1982-03-31\",10.2\\r\\n\"1982-04-01\",10.4\\r\\n\"1982-04-02\",12.3\\r\\n\"1982-04-03\",11.9\\r\\n\"1982-04-04\",11.2\\r\\n\"1982-04-05\",8.5\\r\\n\"1982-04-06\",12.0\\r\\n\"1982-04-07\",12.4\\r\\n\"1982-04-08\",12.9\\r\\n\"1982-04-09\",10.1\\r\\n\"1982-04-10\",15.0\\r\\n\"1982-04-11\",13.6\\r\\n\"1982-04-12\",12.4\\r\\n\"1982-04-13\",13.6\\r\\n\"1982-04-14\",16.1\\r\\n\"1982-04-15\",19.5\\r\\n\"1982-04-16\",14.2\\r\\n\"1982-04-17\",9.3\\r\\n\"1982-04-18\",10.1\\r\\n\"1982-04-19\",7.4\\r\\n\"1982-04-20\",8.6\\r\\n\"1982-04-21\",7.8\\r\\n\"1982-04-22\",9.1\\r\\n\"1982-04-23\",13.0\\r\\n\"1982-04-24\",16.5\\r\\n\"1982-04-25\",12.9\\r\\n\"1982-04-26\",6.9\\r\\n\"1982-04-27\",6.9\\r\\n\"1982-04-28\",8.7\\r\\n\"1982-04-29\",10.0\\r\\n\"1982-04-30\",10.8\\r\\n\"1982-05-01\",7.5\\r\\n\"1982-05-02\",6.3\\r\\n\"1982-05-03\",11.9\\r\\n\"1982-05-04\",13.8\\r\\n\"1982-05-05\",11.8\\r\\n\"1982-05-06\",11.0\\r\\n\"1982-05-07\",10.1\\r\\n\"1982-05-08\",8.5\\r\\n\"1982-05-09\",5.5\\r\\n\"1982-05-10\",7.6\\r\\n\"1982-05-11\",8.7\\r\\n\"1982-05-12\",10.8\\r\\n\"1982-05-13\",11.2\\r\\n\"1982-05-14\",9.1\\r\\n\"1982-05-15\",3.7\\r\\n\"1982-05-16\",4.6\\r\\n\"1982-05-17\",6.6\\r\\n\"1982-05-18\",13.2\\r\\n\"1982-05-19\",15.2\\r\\n\"1982-05-20\",7.6\\r\\n\"1982-05-21\",8.4\\r\\n\"1982-05-22\",6.0\\r\\n\"1982-05-23\",8.3\\r\\n\"1982-05-24\",8.6\\r\\n\"1982-05-25\",11.1\\r\\n\"1982-05-26\",12.1\\r\\n\"1982-05-27\",12.9\\r\\n\"1982-05-28\",14.0\\r\\n\"1982-05-29\",12.5\\r\\n\"1982-05-30\",11.5\\r\\n\"1982-05-31\",7.0\\r\\n\"1982-06-01\",7.1\\r\\n\"1982-06-02\",9.0\\r\\n\"1982-06-03\",3.1\\r\\n\"1982-06-04\",2.5\\r\\n\"1982-06-05\",0.0\\r\\n\"1982-06-06\",1.6\\r\\n\"1982-06-07\",2.6\\r\\n\"1982-06-08\",5.7\\r\\n\"1982-06-09\",2.3\\r\\n\"1982-06-10\",4.5\\r\\n\"1982-06-11\",8.2\\r\\n\"1982-06-12\",6.9\\r\\n\"1982-06-13\",7.3\\r\\n\"1982-06-14\",6.0\\r\\n\"1982-06-15\",7.3\\r\\n\"1982-06-16\",7.6\\r\\n\"1982-06-17\",8.0\\r\\n\"1982-06-18\",8.0\\r\\n\"1982-06-19\",6.8\\r\\n\"1982-06-20\",7.3\\r\\n\"1982-06-21\",6.2\\r\\n\"1982-06-22\",6.9\\r\\n\"1982-06-23\",8.9\\r\\n\"1982-06-24\",4.0\\r\\n\"1982-06-25\",1.3\\r\\n\"1982-06-26\",0.8\\r\\n\"1982-06-27\",4.3\\r\\n\"1982-06-28\",7.3\\r\\n\"1982-06-29\",7.7\\r\\n\"1982-06-30\",9.0\\r\\n\"1982-07-01\",4.2\\r\\n\"1982-07-02\",1.6\\r\\n\"1982-07-03\",2.6\\r\\n\"1982-07-04\",3.4\\r\\n\"1982-07-05\",3.9\\r\\n\"1982-07-06\",7.0\\r\\n\"1982-07-07\",7.8\\r\\n\"1982-07-08\",5.3\\r\\n\"1982-07-09\",2.4\\r\\n\"1982-07-10\",2.8\\r\\n\"1982-07-11\",4.0\\r\\n\"1982-07-12\",7.5\\r\\n\"1982-07-13\",7.8\\r\\n\"1982-07-14\",5.6\\r\\n\"1982-07-15\",3.3\\r\\n\"1982-07-16\",5.0\\r\\n\"1982-07-17\",3.7\\r\\n\"1982-07-18\",3.9\\r\\n\"1982-07-19\",5.2\\r\\n\"1982-07-20\",?0.2\\r\\n\"1982-07-21\",?0.8\\r\\n\"1982-07-22\",0.9\\r\\n\"1982-07-23\",3.5\\r\\n\"1982-07-24\",6.6\\r\\n\"1982-07-25\",9.5\\r\\n\"1982-07-26\",9.0\\r\\n\"1982-07-27\",3.5\\r\\n\"1982-07-28\",4.5\\r\\n\"1982-07-29\",5.7\\r\\n\"1982-07-30\",5.6\\r\\n\"1982-07-31\",7.1\\r\\n\"1982-08-01\",9.7\\r\\n\"1982-08-02\",8.3\\r\\n\"1982-08-03\",9.1\\r\\n\"1982-08-04\",2.8\\r\\n\"1982-08-05\",2.2\\r\\n\"1982-08-06\",4.5\\r\\n\"1982-08-07\",3.8\\r\\n\"1982-08-08\",3.8\\r\\n\"1982-08-09\",6.2\\r\\n\"1982-08-10\",11.5\\r\\n\"1982-08-11\",10.2\\r\\n\"1982-08-12\",7.9\\r\\n\"1982-08-13\",9.0\\r\\n\"1982-08-14\",9.5\\r\\n\"1982-08-15\",6.0\\r\\n\"1982-08-16\",8.2\\r\\n\"1982-08-17\",9.2\\r\\n\"1982-08-18\",4.3\\r\\n\"1982-08-19\",6.6\\r\\n\"1982-08-20\",9.4\\r\\n\"1982-08-21\",13.2\\r\\n\"1982-08-22\",6.6\\r\\n\"1982-08-23\",5.1\\r\\n\"1982-08-24\",12.1\\r\\n\"1982-08-25\",11.2\\r\\n\"1982-08-26\",8.5\\r\\n\"1982-08-27\",4.6\\r\\n\"1982-08-28\",7.0\\r\\n\"1982-08-29\",14.2\\r\\n\"1982-08-30\",12.7\\r\\n\"1982-08-31\",7.6\\r\\n\"1982-09-01\",4.0\\r\\n\"1982-09-02\",10.0\\r\\n\"1982-09-03\",10.5\\r\\n\"1982-09-04\",5.0\\r\\n\"1982-09-05\",4.5\\r\\n\"1982-09-06\",8.2\\r\\n\"1982-09-07\",4.3\\r\\n\"1982-09-08\",9.8\\r\\n\"1982-09-09\",5.8\\r\\n\"1982-09-10\",5.0\\r\\n\"1982-09-11\",8.5\\r\\n\"1982-09-12\",9.0\\r\\n\"1982-09-13\",3.6\\r\\n\"1982-09-14\",6.7\\r\\n\"1982-09-15\",6.7\\r\\n\"1982-09-16\",10.1\\r\\n\"1982-09-17\",15.0\\r\\n\"1982-09-18\",8.9\\r\\n\"1982-09-19\",5.7\\r\\n\"1982-09-20\",4.2\\r\\n\"1982-09-21\",4.0\\r\\n\"1982-09-22\",5.3\\r\\n\"1982-09-23\",6.3\\r\\n\"1982-09-24\",8.5\\r\\n\"1982-09-25\",11.5\\r\\n\"1982-09-26\",7.7\\r\\n\"1982-09-27\",9.2\\r\\n\"1982-09-28\",7.8\\r\\n\"1982-09-29\",6.3\\r\\n\"1982-09-30\",6.3\\r\\n\"1982-10-01\",8.6\\r\\n\"1982-10-02\",6.1\\r\\n\"1982-10-03\",13.2\\r\\n\"1982-10-04\",9.9\\r\\n\"1982-10-05\",4.7\\r\\n\"1982-10-06\",5.8\\r\\n\"1982-10-07\",14.9\\r\\n\"1982-10-08\",10.7\\r\\n\"1982-10-09\",8.6\\r\\n\"1982-10-10\",9.4\\r\\n\"1982-10-11\",5.7\\r\\n\"1982-10-12\",10.9\\r\\n\"1982-10-13\",13.1\\r\\n\"1982-10-14\",10.4\\r\\n\"1982-10-15\",8.2\\r\\n\"1982-10-16\",9.8\\r\\n\"1982-10-17\",7.5\\r\\n\"1982-10-18\",5.8\\r\\n\"1982-10-19\",9.8\\r\\n\"1982-10-20\",7.9\\r\\n\"1982-10-21\",8.7\\r\\n\"1982-10-22\",10.0\\r\\n\"1982-10-23\",10.6\\r\\n\"1982-10-24\",8.0\\r\\n\"1982-10-25\",10.2\\r\\n\"1982-10-26\",15.1\\r\\n\"1982-10-27\",13.9\\r\\n\"1982-10-28\",9.2\\r\\n\"1982-10-29\",9.0\\r\\n\"1982-10-30\",13.2\\r\\n\"1982-10-31\",7.0\\r\\n\"1982-11-01\",10.6\\r\\n\"1982-11-02\",6.9\\r\\n\"1982-11-03\",9.5\\r\\n\"1982-11-04\",12.5\\r\\n\"1982-11-05\",13.6\\r\\n\"1982-11-06\",17.7\\r\\n\"1982-11-07\",16.0\\r\\n\"1982-11-08\",11.3\\r\\n\"1982-11-09\",10.5\\r\\n\"1982-11-10\",14.4\\r\\n\"1982-11-11\",10.3\\r\\n\"1982-11-12\",9.0\\r\\n\"1982-11-13\",11.1\\r\\n\"1982-11-14\",14.5\\r\\n\"1982-11-15\",18.0\\r\\n\"1982-11-16\",12.8\\r\\n\"1982-11-17\",10.7\\r\\n\"1982-11-18\",9.1\\r\\n\"1982-11-19\",8.7\\r\\n\"1982-11-20\",12.4\\r\\n\"1982-11-21\",12.6\\r\\n\"1982-11-22\",10.3\\r\\n\"1982-11-23\",13.7\\r\\n\"1982-11-24\",16.0\\r\\n\"1982-11-25\",15.8\\r\\n\"1982-11-26\",12.1\\r\\n\"1982-11-27\",12.5\\r\\n\"1982-11-28\",12.2\\r\\n\"1982-11-29\",13.7\\r\\n\"1982-11-30\",16.1\\r\\n\"1982-12-01\",15.5\\r\\n\"1982-12-02\",10.3\\r\\n\"1982-12-03\",10.5\\r\\n\"1982-12-04\",11.0\\r\\n\"1982-12-05\",11.9\\r\\n\"1982-12-06\",13.0\\r\\n\"1982-12-07\",12.2\\r\\n\"1982-12-08\",10.6\\r\\n\"1982-12-09\",13.0\\r\\n\"1982-12-10\",13.0\\r\\n\"1982-12-11\",12.2\\r\\n\"1982-12-12\",12.6\\r\\n\"1982-12-13\",18.7\\r\\n\"1982-12-14\",15.2\\r\\n\"1982-12-15\",15.3\\r\\n\"1982-12-16\",13.9\\r\\n\"1982-12-17\",15.8\\r\\n\"1982-12-18\",13.0\\r\\n\"1982-12-19\",13.0\\r\\n\"1982-12-20\",13.7\\r\\n\"1982-12-21\",12.0\\r\\n\"1982-12-22\",10.8\\r\\n\"1982-12-23\",15.6\\r\\n\"1982-12-24\",15.3\\r\\n\"1982-12-25\",13.9\\r\\n\"1982-12-26\",13.0\\r\\n\"1982-12-27\",15.3\\r\\n\"1982-12-28\",16.3\\r\\n\"1982-12-29\",15.8\\r\\n\"1982-12-30\",17.7\\r\\n\"1982-12-31\",16.3\\r\\n\"1983-01-01\",18.4\\r\\n\"1983-01-02\",15.0\\r\\n\"1983-01-03\",10.9\\r\\n\"1983-01-04\",11.4\\r\\n\"1983-01-05\",14.8\\r\\n\"1983-01-06\",12.1\\r\\n\"1983-01-07\",12.8\\r\\n\"1983-01-08\",16.2\\r\\n\"1983-01-09\",15.5\\r\\n\"1983-01-10\",13.0\\r\\n\"1983-01-11\",10.5\\r\\n\"1983-01-12\",9.1\\r\\n\"1983-01-13\",10.5\\r\\n\"1983-01-14\",11.8\\r\\n\"1983-01-15\",12.7\\r\\n\"1983-01-16\",12.7\\r\\n\"1983-01-17\",11.5\\r\\n\"1983-01-18\",13.8\\r\\n\"1983-01-19\",13.3\\r\\n\"1983-01-20\",11.6\\r\\n\"1983-01-21\",15.4\\r\\n\"1983-01-22\",12.4\\r\\n\"1983-01-23\",16.9\\r\\n\"1983-01-24\",14.7\\r\\n\"1983-01-25\",10.6\\r\\n\"1983-01-26\",15.6\\r\\n\"1983-01-27\",10.7\\r\\n\"1983-01-28\",12.6\\r\\n\"1983-01-29\",13.8\\r\\n\"1983-01-30\",14.3\\r\\n\"1983-01-31\",14.0\\r\\n\"1983-02-01\",18.1\\r\\n\"1983-02-02\",17.3\\r\\n\"1983-02-03\",13.0\\r\\n\"1983-02-04\",16.0\\r\\n\"1983-02-05\",14.9\\r\\n\"1983-02-06\",16.2\\r\\n\"1983-02-07\",20.3\\r\\n\"1983-02-08\",22.5\\r\\n\"1983-02-09\",17.2\\r\\n\"1983-02-10\",15.9\\r\\n\"1983-02-11\",16.8\\r\\n\"1983-02-12\",13.8\\r\\n\"1983-02-13\",12.8\\r\\n\"1983-02-14\",14.0\\r\\n\"1983-02-15\",17.5\\r\\n\"1983-02-16\",21.5\\r\\n\"1983-02-17\",16.8\\r\\n\"1983-02-18\",13.6\\r\\n\"1983-02-19\",14.5\\r\\n\"1983-02-20\",14.2\\r\\n\"1983-02-21\",15.7\\r\\n\"1983-02-22\",19.7\\r\\n\"1983-02-23\",17.4\\r\\n\"1983-02-24\",14.4\\r\\n\"1983-02-25\",16.9\\r\\n\"1983-02-26\",19.1\\r\\n\"1983-02-27\",20.4\\r\\n\"1983-02-28\",20.1\\r\\n\"1983-03-01\",19.9\\r\\n\"1983-03-02\",22.0\\r\\n\"1983-03-03\",20.5\\r\\n\"1983-03-04\",22.1\\r\\n\"1983-03-05\",20.6\\r\\n\"1983-03-06\",15.0\\r\\n\"1983-03-07\",20.6\\r\\n\"1983-03-08\",21.5\\r\\n\"1983-03-09\",16.2\\r\\n\"1983-03-10\",14.1\\r\\n\"1983-03-11\",14.5\\r\\n\"1983-03-12\",21.1\\r\\n\"1983-03-13\",15.9\\r\\n\"1983-03-14\",15.2\\r\\n\"1983-03-15\",13.1\\r\\n\"1983-03-16\",13.2\\r\\n\"1983-03-17\",12.5\\r\\n\"1983-03-18\",15.2\\r\\n\"1983-03-19\",17.6\\r\\n\"1983-03-20\",15.5\\r\\n\"1983-03-21\",16.7\\r\\n\"1983-03-22\",16.3\\r\\n\"1983-03-23\",15.1\\r\\n\"1983-03-24\",12.7\\r\\n\"1983-03-25\",10.0\\r\\n\"1983-03-26\",11.4\\r\\n\"1983-03-27\",12.6\\r\\n\"1983-03-28\",10.7\\r\\n\"1983-03-29\",10.0\\r\\n\"1983-03-30\",13.9\\r\\n\"1983-03-31\",13.4\\r\\n\"1983-04-01\",12.5\\r\\n\"1983-04-02\",12.8\\r\\n\"1983-04-03\",7.8\\r\\n\"1983-04-04\",11.1\\r\\n\"1983-04-05\",10.7\\r\\n\"1983-04-06\",7.1\\r\\n\"1983-04-07\",6.7\\r\\n\"1983-04-08\",5.7\\r\\n\"1983-04-09\",9.1\\r\\n\"1983-04-10\",15.2\\r\\n\"1983-04-11\",15.5\\r\\n\"1983-04-12\",11.1\\r\\n\"1983-04-13\",11.7\\r\\n\"1983-04-14\",11.5\\r\\n\"1983-04-15\",9.8\\r\\n\"1983-04-16\",6.2\\r\\n\"1983-04-17\",6.7\\r\\n\"1983-04-18\",7.5\\r\\n\"1983-04-19\",8.8\\r\\n\"1983-04-20\",8.0\\r\\n\"1983-04-21\",10.4\\r\\n\"1983-04-22\",14.5\\r\\n\"1983-04-23\",16.5\\r\\n\"1983-04-24\",14.1\\r\\n\"1983-04-25\",10.5\\r\\n\"1983-04-26\",12.6\\r\\n\"1983-04-27\",13.0\\r\\n\"1983-04-28\",8.7\\r\\n\"1983-04-29\",10.1\\r\\n\"1983-04-30\",12.0\\r\\n\"1983-05-01\",12.5\\r\\n\"1983-05-02\",13.5\\r\\n\"1983-05-03\",13.7\\r\\n\"1983-05-04\",13.5\\r\\n\"1983-05-05\",10.7\\r\\n\"1983-05-06\",13.0\\r\\n\"1983-05-07\",11.6\\r\\n\"1983-05-08\",13.0\\r\\n\"1983-05-09\",11.2\\r\\n\"1983-05-10\",13.5\\r\\n\"1983-05-11\",12.9\\r\\n\"1983-05-12\",6.8\\r\\n\"1983-05-13\",10.0\\r\\n\"1983-05-14\",14.5\\r\\n\"1983-05-15\",11.7\\r\\n\"1983-05-16\",6.7\\r\\n\"1983-05-17\",4.6\\r\\n\"1983-05-18\",4.9\\r\\n\"1983-05-19\",7.4\\r\\n\"1983-05-20\",8.3\\r\\n\"1983-05-21\",7.5\\r\\n\"1983-05-22\",6.2\\r\\n\"1983-05-23\",7.8\\r\\n\"1983-05-24\",13.2\\r\\n\"1983-05-25\",11.9\\r\\n\"1983-05-26\",6.5\\r\\n\"1983-05-27\",8.3\\r\\n\"1983-05-28\",12.1\\r\\n\"1983-05-29\",9.3\\r\\n\"1983-05-30\",7.5\\r\\n\"1983-05-31\",9.3\\r\\n\"1983-06-01\",11.0\\r\\n\"1983-06-02\",10.8\\r\\n\"1983-06-03\",5.3\\r\\n\"1983-06-04\",7.6\\r\\n\"1983-06-05\",5.6\\r\\n\"1983-06-06\",7.2\\r\\n\"1983-06-07\",9.6\\r\\n\"1983-06-08\",7.0\\r\\n\"1983-06-09\",8.3\\r\\n\"1983-06-10\",7.8\\r\\n\"1983-06-11\",4.7\\r\\n\"1983-06-12\",6.8\\r\\n\"1983-06-13\",7.2\\r\\n\"1983-06-14\",8.3\\r\\n\"1983-06-15\",9.5\\r\\n\"1983-06-16\",4.7\\r\\n\"1983-06-17\",3.0\\r\\n\"1983-06-18\",1.5\\r\\n\"1983-06-19\",2.5\\r\\n\"1983-06-20\",6.2\\r\\n\"1983-06-21\",11.6\\r\\n\"1983-06-22\",6.6\\r\\n\"1983-06-23\",6.6\\r\\n\"1983-06-24\",8.0\\r\\n\"1983-06-25\",7.9\\r\\n\"1983-06-26\",3.3\\r\\n\"1983-06-27\",3.9\\r\\n\"1983-06-28\",6.0\\r\\n\"1983-06-29\",4.0\\r\\n\"1983-06-30\",5.5\\r\\n\"1983-07-01\",8.5\\r\\n\"1983-07-02\",9.8\\r\\n\"1983-07-03\",9.5\\r\\n\"1983-07-04\",7.2\\r\\n\"1983-07-05\",8.1\\r\\n\"1983-07-06\",8.0\\r\\n\"1983-07-07\",8.5\\r\\n\"1983-07-08\",8.8\\r\\n\"1983-07-09\",8.3\\r\\n\"1983-07-10\",2.4\\r\\n\"1983-07-11\",4.9\\r\\n\"1983-07-12\",5.9\\r\\n\"1983-07-13\",6.7\\r\\n\"1983-07-14\",8.4\\r\\n\"1983-07-15\",6.5\\r\\n\"1983-07-16\",7.9\\r\\n\"1983-07-17\",4.1\\r\\n\"1983-07-18\",5.4\\r\\n\"1983-07-19\",7.5\\r\\n\"1983-07-20\",3.9\\r\\n\"1983-07-21\",2.5\\r\\n\"1983-07-22\",5.3\\r\\n\"1983-07-23\",6.6\\r\\n\"1983-07-24\",0.0\\r\\n\"1983-07-25\",0.7\\r\\n\"1983-07-26\",7.6\\r\\n\"1983-07-27\",12.3\\r\\n\"1983-07-28\",9.2\\r\\n\"1983-07-29\",9.6\\r\\n\"1983-07-30\",9.5\\r\\n\"1983-07-31\",10.0\\r\\n\"1983-08-01\",7.7\\r\\n\"1983-08-02\",8.0\\r\\n\"1983-08-03\",8.3\\r\\n\"1983-08-04\",8.3\\r\\n\"1983-08-05\",4.5\\r\\n\"1983-08-06\",6.5\\r\\n\"1983-08-07\",9.4\\r\\n\"1983-08-08\",9.4\\r\\n\"1983-08-09\",10.5\\r\\n\"1983-08-10\",10.7\\r\\n\"1983-08-11\",9.9\\r\\n\"1983-08-12\",7.6\\r\\n\"1983-08-13\",5.8\\r\\n\"1983-08-14\",8.5\\r\\n\"1983-08-15\",13.8\\r\\n\"1983-08-16\",14.3\\r\\n\"1983-08-17\",8.3\\r\\n\"1983-08-18\",5.3\\r\\n\"1983-08-19\",3.0\\r\\n\"1983-08-20\",5.2\\r\\n\"1983-08-21\",10.3\\r\\n\"1983-08-22\",11.1\\r\\n\"1983-08-23\",10.5\\r\\n\"1983-08-24\",9.0\\r\\n\"1983-08-25\",13.0\\r\\n\"1983-08-26\",6.4\\r\\n\"1983-08-27\",8.4\\r\\n\"1983-08-28\",6.7\\r\\n\"1983-08-29\",8.3\\r\\n\"1983-08-30\",11.2\\r\\n\"1983-08-31\",10.0\\r\\n\"1983-09-01\",10.1\\r\\n\"1983-09-02\",10.6\\r\\n\"1983-09-03\",10.9\\r\\n\"1983-09-04\",5.7\\r\\n\"1983-09-05\",9.5\\r\\n\"1983-09-06\",10.4\\r\\n\"1983-09-07\",11.1\\r\\n\"1983-09-08\",12.2\\r\\n\"1983-09-09\",10.6\\r\\n\"1983-09-10\",8.8\\r\\n\"1983-09-11\",9.2\\r\\n\"1983-09-12\",5.5\\r\\n\"1983-09-13\",7.1\\r\\n\"1983-09-14\",6.5\\r\\n\"1983-09-15\",4.3\\r\\n\"1983-09-16\",5.0\\r\\n\"1983-09-17\",11.2\\r\\n\"1983-09-18\",7.5\\r\\n\"1983-09-19\",12.0\\r\\n\"1983-09-20\",13.6\\r\\n\"1983-09-21\",8.3\\r\\n\"1983-09-22\",8.5\\r\\n\"1983-09-23\",12.9\\r\\n\"1983-09-24\",7.7\\r\\n\"1983-09-25\",7.6\\r\\n\"1983-09-26\",3.5\\r\\n\"1983-09-27\",10.4\\r\\n\"1983-09-28\",15.4\\r\\n\"1983-09-29\",10.6\\r\\n\"1983-09-30\",9.6\\r\\n\"1983-10-01\",9.3\\r\\n\"1983-10-02\",13.9\\r\\n\"1983-10-03\",7.7\\r\\n\"1983-10-04\",9.5\\r\\n\"1983-10-05\",7.6\\r\\n\"1983-10-06\",6.9\\r\\n\"1983-10-07\",6.8\\r\\n\"1983-10-08\",5.8\\r\\n\"1983-10-09\",6.0\\r\\n\"1983-10-10\",8.3\\r\\n\"1983-10-11\",9.1\\r\\n\"1983-10-12\",12.5\\r\\n\"1983-10-13\",13.2\\r\\n\"1983-10-14\",16.2\\r\\n\"1983-10-15\",12.5\\r\\n\"1983-10-16\",11.8\\r\\n\"1983-10-17\",10.6\\r\\n\"1983-10-18\",10.0\\r\\n\"1983-10-19\",12.2\\r\\n\"1983-10-20\",8.9\\r\\n\"1983-10-21\",10.3\\r\\n\"1983-10-22\",7.5\\r\\n\"1983-10-23\",11.6\\r\\n\"1983-10-24\",12.6\\r\\n\"1983-10-25\",12.9\\r\\n\"1983-10-26\",11.7\\r\\n\"1983-10-27\",14.0\\r\\n\"1983-10-28\",12.3\\r\\n\"1983-10-29\",9.0\\r\\n\"1983-10-30\",9.2\\r\\n\"1983-10-31\",9.8\\r\\n\"1983-11-01\",11.8\\r\\n\"1983-11-02\",10.6\\r\\n\"1983-11-03\",12.6\\r\\n\"1983-11-04\",11.0\\r\\n\"1983-11-05\",8.2\\r\\n\"1983-11-06\",7.5\\r\\n\"1983-11-07\",13.6\\r\\n\"1983-11-08\",14.8\\r\\n\"1983-11-09\",10.9\\r\\n\"1983-11-10\",7.7\\r\\n\"1983-11-11\",10.2\\r\\n\"1983-11-12\",10.8\\r\\n\"1983-11-13\",10.8\\r\\n\"1983-11-14\",12.5\\r\\n\"1983-11-15\",13.2\\r\\n\"1983-11-16\",8.7\\r\\n\"1983-11-17\",5.7\\r\\n\"1983-11-18\",9.8\\r\\n\"1983-11-19\",7.3\\r\\n\"1983-11-20\",10.8\\r\\n\"1983-11-21\",10.0\\r\\n\"1983-11-22\",16.2\\r\\n\"1983-11-23\",15.0\\r\\n\"1983-11-24\",14.5\\r\\n\"1983-11-25\",15.9\\r\\n\"1983-11-26\",14.9\\r\\n\"1983-11-27\",14.2\\r\\n\"1983-11-28\",15.8\\r\\n\"1983-11-29\",17.2\\r\\n\"1983-11-30\",17.6\\r\\n\"1983-12-01\",12.1\\r\\n\"1983-12-02\",11.4\\r\\n\"1983-12-03\",13.0\\r\\n\"1983-12-04\",13.2\\r\\n\"1983-12-05\",12.0\\r\\n\"1983-12-06\",15.3\\r\\n\"1983-12-07\",12.7\\r\\n\"1983-12-08\",12.1\\r\\n\"1983-12-09\",13.8\\r\\n\"1983-12-10\",10.9\\r\\n\"1983-12-11\",12.0\\r\\n\"1983-12-12\",16.5\\r\\n\"1983-12-13\",15.0\\r\\n\"1983-12-14\",11.2\\r\\n\"1983-12-15\",13.9\\r\\n\"1983-12-16\",15.0\\r\\n\"1983-12-17\",14.8\\r\\n\"1983-12-18\",15.0\\r\\n\"1983-12-19\",13.3\\r\\n\"1983-12-20\",20.4\\r\\n\"1983-12-21\",18.0\\r\\n\"1983-12-22\",12.2\\r\\n\"1983-12-23\",16.7\\r\\n\"1983-12-24\",13.8\\r\\n\"1983-12-25\",17.5\\r\\n\"1983-12-26\",15.0\\r\\n\"1983-12-27\",13.9\\r\\n\"1983-12-28\",11.1\\r\\n\"1983-12-29\",16.1\\r\\n\"1983-12-30\",20.4\\r\\n\"1983-12-31\",18.0\\r\\n\"1984-01-01\",19.5\\r\\n\"1984-01-02\",17.1\\r\\n\"1984-01-03\",17.1\\r\\n\"1984-01-04\",12.0\\r\\n\"1984-01-05\",11.0\\r\\n\"1984-01-06\",16.3\\r\\n\"1984-01-07\",16.1\\r\\n\"1984-01-08\",13.0\\r\\n\"1984-01-09\",13.4\\r\\n\"1984-01-10\",15.2\\r\\n\"1984-01-11\",12.5\\r\\n\"1984-01-12\",14.3\\r\\n\"1984-01-13\",16.5\\r\\n\"1984-01-14\",18.6\\r\\n\"1984-01-15\",18.0\\r\\n\"1984-01-16\",18.2\\r\\n\"1984-01-17\",11.4\\r\\n\"1984-01-18\",11.9\\r\\n\"1984-01-19\",12.2\\r\\n\"1984-01-20\",14.8\\r\\n\"1984-01-21\",13.1\\r\\n\"1984-01-22\",12.7\\r\\n\"1984-01-23\",10.5\\r\\n\"1984-01-24\",13.8\\r\\n\"1984-01-25\",18.8\\r\\n\"1984-01-26\",13.9\\r\\n\"1984-01-27\",11.2\\r\\n\"1984-01-28\",10.6\\r\\n\"1984-01-29\",14.7\\r\\n\"1984-01-30\",13.1\\r\\n\"1984-01-31\",12.1\\r\\n\"1984-02-01\",14.7\\r\\n\"1984-02-02\",11.1\\r\\n\"1984-02-03\",13.0\\r\\n\"1984-02-04\",15.6\\r\\n\"1984-02-05\",14.2\\r\\n\"1984-02-06\",15.5\\r\\n\"1984-02-07\",18.0\\r\\n\"1984-02-08\",15.0\\r\\n\"1984-02-09\",15.9\\r\\n\"1984-02-10\",15.5\\r\\n\"1984-02-11\",15.8\\r\\n\"1984-02-12\",16.6\\r\\n\"1984-02-13\",13.6\\r\\n\"1984-02-14\",13.8\\r\\n\"1984-02-15\",14.6\\r\\n\"1984-02-16\",15.6\\r\\n\"1984-02-17\",16.6\\r\\n\"1984-02-18\",14.3\\r\\n\"1984-02-19\",16.3\\r\\n\"1984-02-20\",18.9\\r\\n\"1984-02-21\",18.7\\r\\n\"1984-02-22\",14.5\\r\\n\"1984-02-23\",16.5\\r\\n\"1984-02-24\",14.1\\r\\n\"1984-02-25\",13.5\\r\\n\"1984-02-26\",11.7\\r\\n\"1984-02-27\",15.1\\r\\n\"1984-02-28\",11.2\\r\\n\"1984-02-29\",13.5\\r\\n\"1984-03-01\",12.6\\r\\n\"1984-03-02\",8.8\\r\\n\"1984-03-03\",10.5\\r\\n\"1984-03-04\",12.1\\r\\n\"1984-03-05\",14.5\\r\\n\"1984-03-06\",19.5\\r\\n\"1984-03-07\",14.0\\r\\n\"1984-03-08\",13.8\\r\\n\"1984-03-09\",10.5\\r\\n\"1984-03-10\",13.8\\r\\n\"1984-03-11\",11.4\\r\\n\"1984-03-12\",15.6\\r\\n\"1984-03-13\",11.1\\r\\n\"1984-03-14\",12.1\\r\\n\"1984-03-15\",14.2\\r\\n\"1984-03-16\",10.9\\r\\n\"1984-03-17\",14.2\\r\\n\"1984-03-18\",13.8\\r\\n\"1984-03-19\",15.1\\r\\n\"1984-03-20\",14.0\\r\\n\"1984-03-21\",12.1\\r\\n\"1984-03-22\",13.8\\r\\n\"1984-03-23\",16.6\\r\\n\"1984-03-24\",17.8\\r\\n\"1984-03-25\",9.4\\r\\n\"1984-03-26\",10.2\\r\\n\"1984-03-27\",7.4\\r\\n\"1984-03-28\",8.7\\r\\n\"1984-03-29\",14.0\\r\\n\"1984-03-30\",15.3\\r\\n\"1984-03-31\",11.1\\r\\n\"1984-04-01\",9.7\\r\\n\"1984-04-02\",10.3\\r\\n\"1984-04-03\",9.2\\r\\n\"1984-04-04\",8.2\\r\\n\"1984-04-05\",9.7\\r\\n\"1984-04-06\",12.4\\r\\n\"1984-04-07\",12.5\\r\\n\"1984-04-08\",9.0\\r\\n\"1984-04-09\",9.7\\r\\n\"1984-04-10\",10.1\\r\\n\"1984-04-11\",11.2\\r\\n\"1984-04-12\",12.0\\r\\n\"1984-04-13\",11.1\\r\\n\"1984-04-14\",10.8\\r\\n\"1984-04-15\",12.8\\r\\n\"1984-04-16\",9.8\\r\\n\"1984-04-17\",13.7\\r\\n\"1984-04-18\",11.0\\r\\n\"1984-04-19\",13.2\\r\\n\"1984-04-20\",13.0\\r\\n\"1984-04-21\",10.2\\r\\n\"1984-04-22\",13.2\\r\\n\"1984-04-23\",9.3\\r\\n\"1984-04-24\",11.1\\r\\n\"1984-04-25\",10.3\\r\\n\"1984-04-26\",8.7\\r\\n\"1984-04-27\",11.7\\r\\n\"1984-04-28\",12.5\\r\\n\"1984-04-29\",6.5\\r\\n\"1984-04-30\",9.6\\r\\n\"1984-05-01\",13.8\\r\\n\"1984-05-02\",14.7\\r\\n\"1984-05-03\",9.1\\r\\n\"1984-05-04\",4.8\\r\\n\"1984-05-05\",3.3\\r\\n\"1984-05-06\",3.5\\r\\n\"1984-05-07\",5.7\\r\\n\"1984-05-08\",5.5\\r\\n\"1984-05-09\",7.0\\r\\n\"1984-05-10\",9.5\\r\\n\"1984-05-11\",9.9\\r\\n\"1984-05-12\",4.9\\r\\n\"1984-05-13\",6.3\\r\\n\"1984-05-14\",4.8\\r\\n\"1984-05-15\",6.2\\r\\n\"1984-05-16\",7.1\\r\\n\"1984-05-17\",7.5\\r\\n\"1984-05-18\",9.4\\r\\n\"1984-05-19\",8.7\\r\\n\"1984-05-20\",9.5\\r\\n\"1984-05-21\",12.1\\r\\n\"1984-05-22\",9.5\\r\\n\"1984-05-23\",9.3\\r\\n\"1984-05-24\",8.5\\r\\n\"1984-05-25\",8.0\\r\\n\"1984-05-26\",9.8\\r\\n\"1984-05-27\",6.2\\r\\n\"1984-05-28\",7.3\\r\\n\"1984-05-29\",10.9\\r\\n\"1984-05-30\",10.0\\r\\n\"1984-05-31\",8.7\\r\\n\"1984-06-01\",9.0\\r\\n\"1984-06-02\",10.8\\r\\n\"1984-06-03\",12.4\\r\\n\"1984-06-04\",7.2\\r\\n\"1984-06-05\",7.2\\r\\n\"1984-06-06\",11.1\\r\\n\"1984-06-07\",9.3\\r\\n\"1984-06-08\",10.1\\r\\n\"1984-06-09\",3.9\\r\\n\"1984-06-10\",5.0\\r\\n\"1984-06-11\",8.2\\r\\n\"1984-06-12\",2.8\\r\\n\"1984-06-13\",4.3\\r\\n\"1984-06-14\",8.1\\r\\n\"1984-06-15\",11.1\\r\\n\"1984-06-16\",4.7\\r\\n\"1984-06-17\",5.3\\r\\n\"1984-06-18\",10.0\\r\\n\"1984-06-19\",5.6\\r\\n\"1984-06-20\",2.2\\r\\n\"1984-06-21\",7.1\\r\\n\"1984-06-22\",8.3\\r\\n\"1984-06-23\",8.6\\r\\n\"1984-06-24\",10.1\\r\\n\"1984-06-25\",8.3\\r\\n\"1984-06-26\",7.2\\r\\n\"1984-06-27\",7.7\\r\\n\"1984-06-28\",7.8\\r\\n\"1984-06-29\",9.1\\r\\n\"1984-06-30\",9.4\\r\\n\"1984-07-01\",7.8\\r\\n\"1984-07-02\",2.6\\r\\n\"1984-07-03\",2.4\\r\\n\"1984-07-04\",3.9\\r\\n\"1984-07-05\",1.3\\r\\n\"1984-07-06\",2.1\\r\\n\"1984-07-07\",7.4\\r\\n\"1984-07-08\",7.2\\r\\n\"1984-07-09\",8.8\\r\\n\"1984-07-10\",8.9\\r\\n\"1984-07-11\",8.8\\r\\n\"1984-07-12\",8.0\\r\\n\"1984-07-13\",0.7\\r\\n\"1984-07-14\",?0.1\\r\\n\"1984-07-15\",0.9\\r\\n\"1984-07-16\",7.8\\r\\n\"1984-07-17\",7.2\\r\\n\"1984-07-18\",8.0\\r\\n\"1984-07-19\",4.6\\r\\n\"1984-07-20\",5.2\\r\\n\"1984-07-21\",5.8\\r\\n\"1984-07-22\",6.8\\r\\n\"1984-07-23\",8.1\\r\\n\"1984-07-24\",7.5\\r\\n\"1984-07-25\",5.4\\r\\n\"1984-07-26\",4.6\\r\\n\"1984-07-27\",6.4\\r\\n\"1984-07-28\",9.7\\r\\n\"1984-07-29\",7.0\\r\\n\"1984-07-30\",10.0\\r\\n\"1984-07-31\",10.6\\r\\n\"1984-08-01\",11.5\\r\\n\"1984-08-02\",10.2\\r\\n\"1984-08-03\",11.1\\r\\n\"1984-08-04\",11.0\\r\\n\"1984-08-05\",8.9\\r\\n\"1984-08-06\",9.9\\r\\n\"1984-08-07\",11.7\\r\\n\"1984-08-08\",11.6\\r\\n\"1984-08-09\",9.0\\r\\n\"1984-08-10\",6.3\\r\\n\"1984-08-11\",8.7\\r\\n\"1984-08-12\",8.5\\r\\n\"1984-08-13\",8.5\\r\\n\"1984-08-14\",8.0\\r\\n\"1984-08-15\",6.0\\r\\n\"1984-08-16\",8.0\\r\\n\"1984-08-17\",8.5\\r\\n\"1984-08-18\",7.7\\r\\n\"1984-08-19\",8.4\\r\\n\"1984-08-20\",9.0\\r\\n\"1984-08-21\",8.3\\r\\n\"1984-08-22\",6.8\\r\\n\"1984-08-23\",9.3\\r\\n\"1984-08-24\",6.7\\r\\n\"1984-08-25\",9.0\\r\\n\"1984-08-26\",7.3\\r\\n\"1984-08-27\",6.3\\r\\n\"1984-08-28\",7.9\\r\\n\"1984-08-29\",5.2\\r\\n\"1984-08-30\",9.0\\r\\n\"1984-08-31\",11.3\\r\\n\"1984-09-01\",9.2\\r\\n\"1984-09-02\",11.3\\r\\n\"1984-09-03\",7.0\\r\\n\"1984-09-04\",8.0\\r\\n\"1984-09-05\",4.6\\r\\n\"1984-09-06\",8.5\\r\\n\"1984-09-07\",9.5\\r\\n\"1984-09-08\",9.4\\r\\n\"1984-09-09\",10.5\\r\\n\"1984-09-10\",9.7\\r\\n\"1984-09-11\",4.9\\r\\n\"1984-09-12\",8.0\\r\\n\"1984-09-13\",5.8\\r\\n\"1984-09-14\",5.5\\r\\n\"1984-09-15\",10.9\\r\\n\"1984-09-16\",11.7\\r\\n\"1984-09-17\",9.2\\r\\n\"1984-09-18\",8.9\\r\\n\"1984-09-19\",11.3\\r\\n\"1984-09-20\",8.6\\r\\n\"1984-09-21\",6.2\\r\\n\"1984-09-22\",6.6\\r\\n\"1984-09-23\",9.1\\r\\n\"1984-09-24\",6.1\\r\\n\"1984-09-25\",7.5\\r\\n\"1984-09-26\",10.7\\r\\n\"1984-09-27\",6.3\\r\\n\"1984-09-28\",5.5\\r\\n\"1984-09-29\",6.7\\r\\n\"1984-09-30\",4.2\\r\\n\"1984-10-01\",11.3\\r\\n\"1984-10-02\",16.3\\r\\n\"1984-10-03\",10.5\\r\\n\"1984-10-04\",10.3\\r\\n\"1984-10-05\",7.9\\r\\n\"1984-10-06\",7.7\\r\\n\"1984-10-07\",16.0\\r\\n\"1984-10-08\",14.6\\r\\n\"1984-10-09\",12.5\\r\\n\"1984-10-10\",8.1\\r\\n\"1984-10-11\",12.2\\r\\n\"1984-10-12\",17.2\\r\\n\"1984-10-13\",9.4\\r\\n\"1984-10-14\",8.7\\r\\n\"1984-10-15\",5.9\\r\\n\"1984-10-16\",4.8\\r\\n\"1984-10-17\",7.4\\r\\n\"1984-10-18\",9.4\\r\\n\"1984-10-19\",9.7\\r\\n\"1984-10-20\",9.9\\r\\n\"1984-10-21\",6.5\\r\\n\"1984-10-22\",9.8\\r\\n\"1984-10-23\",18.2\\r\\n\"1984-10-24\",11.3\\r\\n\"1984-10-25\",9.1\\r\\n\"1984-10-26\",9.6\\r\\n\"1984-10-27\",13.5\\r\\n\"1984-10-28\",10.7\\r\\n\"1984-10-29\",10.0\\r\\n\"1984-10-30\",8.5\\r\\n\"1984-10-31\",12.6\\r\\n\"1984-11-01\",16.6\\r\\n\"1984-11-02\",11.6\\r\\n\"1984-11-03\",12.2\\r\\n\"1984-11-04\",11.2\\r\\n\"1984-11-05\",9.2\\r\\n\"1984-11-06\",9.9\\r\\n\"1984-11-07\",11.9\\r\\n\"1984-11-08\",15.6\\r\\n\"1984-11-09\",19.0\\r\\n\"1984-11-10\",12.8\\r\\n\"1984-11-11\",12.2\\r\\n\"1984-11-12\",12.0\\r\\n\"1984-11-13\",11.1\\r\\n\"1984-11-14\",11.8\\r\\n\"1984-11-15\",7.6\\r\\n\"1984-11-16\",13.0\\r\\n\"1984-11-17\",12.7\\r\\n\"1984-11-18\",16.0\\r\\n\"1984-11-19\",14.8\\r\\n\"1984-11-20\",14.2\\r\\n\"1984-11-21\",10.0\\r\\n\"1984-11-22\",8.8\\r\\n\"1984-11-23\",11.6\\r\\n\"1984-11-24\",8.6\\r\\n\"1984-11-25\",14.6\\r\\n\"1984-11-26\",24.3\\r\\n\"1984-11-27\",11.6\\r\\n\"1984-11-28\",10.8\\r\\n\"1984-11-29\",12.0\\r\\n\"1984-11-30\",11.0\\r\\n\"1984-12-01\",12.6\\r\\n\"1984-12-02\",10.8\\r\\n\"1984-12-03\",9.1\\r\\n\"1984-12-04\",11.0\\r\\n\"1984-12-05\",13.0\\r\\n\"1984-12-06\",12.8\\r\\n\"1984-12-07\",9.9\\r\\n\"1984-12-08\",11.6\\r\\n\"1984-12-09\",10.5\\r\\n\"1984-12-10\",15.9\\r\\n\"1984-12-11\",12.2\\r\\n\"1984-12-12\",13.0\\r\\n\"1984-12-13\",12.5\\r\\n\"1984-12-14\",12.5\\r\\n\"1984-12-15\",11.4\\r\\n\"1984-12-16\",12.1\\r\\n\"1984-12-17\",16.8\\r\\n\"1984-12-18\",12.1\\r\\n\"1984-12-19\",11.3\\r\\n\"1984-12-20\",10.4\\r\\n\"1984-12-21\",14.2\\r\\n\"1984-12-22\",11.4\\r\\n\"1984-12-23\",13.7\\r\\n\"1984-12-24\",16.5\\r\\n\"1984-12-25\",12.8\\r\\n\"1984-12-26\",12.2\\r\\n\"1984-12-27\",12.0\\r\\n\"1984-12-28\",12.6\\r\\n\"1984-12-29\",16.0\\r\\n\"1984-12-30\",16.4\\r\\n\"1985-01-01\",13.3\\r\\n\"1985-01-02\",15.2\\r\\n\"1985-01-03\",13.1\\r\\n\"1985-01-04\",12.7\\r\\n\"1985-01-05\",14.6\\r\\n\"1985-01-06\",11.0\\r\\n\"1985-01-07\",13.2\\r\\n\"1985-01-08\",12.2\\r\\n\"1985-01-09\",14.4\\r\\n\"1985-01-10\",13.7\\r\\n\"1985-01-11\",14.5\\r\\n\"1985-01-12\",14.1\\r\\n\"1985-01-13\",14.4\\r\\n\"1985-01-14\",19.7\\r\\n\"1985-01-15\",16.5\\r\\n\"1985-01-16\",15.9\\r\\n\"1985-01-17\",11.8\\r\\n\"1985-01-18\",12.0\\r\\n\"1985-01-19\",11.4\\r\\n\"1985-01-20\",14.4\\r\\n\"1985-01-21\",12.4\\r\\n\"1985-01-22\",15.1\\r\\n\"1985-01-23\",15.6\\r\\n\"1985-01-24\",15.2\\r\\n\"1985-01-25\",12.8\\r\\n\"1985-01-26\",13.3\\r\\n\"1985-01-27\",17.5\\r\\n\"1985-01-28\",15.4\\r\\n\"1985-01-29\",13.5\\r\\n\"1985-01-30\",16.7\\r\\n\"1985-01-31\",15.2\\r\\n\"1985-02-01\",14.9\\r\\n\"1985-02-02\",10.2\\r\\n\"1985-02-03\",13.6\\r\\n\"1985-02-04\",19.0\\r\\n\"1985-02-05\",15.7\\r\\n\"1985-02-06\",18.0\\r\\n\"1985-02-07\",14.8\\r\\n\"1985-02-08\",13.9\\r\\n\"1985-02-09\",13.0\\r\\n\"1985-02-10\",15.3\\r\\n\"1985-02-11\",14.3\\r\\n\"1985-02-12\",15.6\\r\\n\"1985-02-13\",16.0\\r\\n\"1985-02-14\",14.9\\r\\n\"1985-02-15\",11.1\\r\\n\"1985-02-16\",14.8\\r\\n\"1985-02-17\",13.0\\r\\n\"1985-02-18\",12.2\\r\\n\"1985-02-19\",10.9\\r\\n\"1985-02-20\",14.6\\r\\n\"1985-02-21\",16.6\\r\\n\"1985-02-22\",18.1\\r\\n\"1985-02-23\",13.4\\r\\n\"1985-02-24\",10.3\\r\\n\"1985-02-25\",13.6\\r\\n\"1985-02-26\",13.8\\r\\n\"1985-02-27\",10.3\\r\\n\"1985-02-28\",11.0\\r\\n\"1985-03-01\",14.3\\r\\n\"1985-03-02\",15.5\\r\\n\"1985-03-03\",14.7\\r\\n\"1985-03-04\",12.7\\r\\n\"1985-03-05\",10.7\\r\\n\"1985-03-06\",12.6\\r\\n\"1985-03-07\",9.8\\r\\n\"1985-03-08\",13.2\\r\\n\"1985-03-09\",15.2\\r\\n\"1985-03-10\",16.6\\r\\n\"1985-03-11\",21.0\\r\\n\"1985-03-12\",22.4\\r\\n\"1985-03-13\",17.0\\r\\n\"1985-03-14\",21.7\\r\\n\"1985-03-15\",21.4\\r\\n\"1985-03-16\",18.6\\r\\n\"1985-03-17\",16.2\\r\\n\"1985-03-18\",16.8\\r\\n\"1985-03-19\",17.0\\r\\n\"1985-03-20\",18.4\\r\\n\"1985-03-21\",17.2\\r\\n\"1985-03-22\",18.4\\r\\n\"1985-03-23\",18.8\\r\\n\"1985-03-24\",16.5\\r\\n\"1985-03-25\",13.3\\r\\n\"1985-03-26\",12.2\\r\\n\"1985-03-27\",11.3\\r\\n\"1985-03-28\",13.8\\r\\n\"1985-03-29\",16.6\\r\\n\"1985-03-30\",14.0\\r\\n\"1985-03-31\",14.3\\r\\n\"1985-04-01\",16.4\\r\\n\"1985-04-02\",11.9\\r\\n\"1985-04-03\",15.7\\r\\n\"1985-04-04\",17.6\\r\\n\"1985-04-05\",17.5\\r\\n\"1985-04-06\",15.9\\r\\n\"1985-04-07\",16.2\\r\\n\"1985-04-08\",16.0\\r\\n\"1985-04-09\",15.9\\r\\n\"1985-04-10\",16.2\\r\\n\"1985-04-11\",16.2\\r\\n\"1985-04-12\",19.5\\r\\n\"1985-04-13\",18.2\\r\\n\"1985-04-14\",21.8\\r\\n\"1985-04-15\",15.1\\r\\n\"1985-04-16\",11.0\\r\\n\"1985-04-17\",8.1\\r\\n\"1985-04-18\",9.5\\r\\n\"1985-04-19\",9.3\\r\\n\"1985-04-20\",10.6\\r\\n\"1985-04-21\",6.3\\r\\n\"1985-04-22\",8.6\\r\\n\"1985-04-23\",6.8\\r\\n\"1985-04-24\",8.7\\r\\n\"1985-04-25\",8.4\\r\\n\"1985-04-26\",9.3\\r\\n\"1985-04-27\",10.0\\r\\n\"1985-04-28\",10.5\\r\\n\"1985-04-29\",12.0\\r\\n\"1985-04-30\",10.1\\r\\n\"1985-05-01\",9.4\\r\\n\"1985-05-02\",10.1\\r\\n\"1985-05-03\",8.0\\r\\n\"1985-05-04\",10.6\\r\\n\"1985-05-05\",13.6\\r\\n\"1985-05-06\",15.4\\r\\n\"1985-05-07\",9.0\\r\\n\"1985-05-08\",10.4\\r\\n\"1985-05-09\",11.0\\r\\n\"1985-05-10\",12.1\\r\\n\"1985-05-11\",13.4\\r\\n\"1985-05-12\",11.3\\r\\n\"1985-05-13\",6.7\\r\\n\"1985-05-14\",9.8\\r\\n\"1985-05-15\",10.8\\r\\n\"1985-05-16\",7.8\\r\\n\"1985-05-17\",4.5\\r\\n\"1985-05-18\",7.6\\r\\n\"1985-05-19\",6.9\\r\\n\"1985-05-20\",7.5\\r\\n\"1985-05-21\",8.5\\r\\n\"1985-05-22\",5.5\\r\\n\"1985-05-23\",9.5\\r\\n\"1985-05-24\",7.3\\r\\n\"1985-05-25\",5.4\\r\\n\"1985-05-26\",5.5\\r\\n\"1985-05-27\",8.1\\r\\n\"1985-05-28\",11.2\\r\\n\"1985-05-29\",13.4\\r\\n\"1985-05-30\",11.6\\r\\n\"1985-05-31\",10.1\\r\\n\"1985-06-01\",4.3\\r\\n\"1985-06-02\",5.5\\r\\n\"1985-06-03\",4.4\\r\\n\"1985-06-04\",5.9\\r\\n\"1985-06-05\",5.7\\r\\n\"1985-06-06\",8.2\\r\\n\"1985-06-07\",8.2\\r\\n\"1985-06-08\",4.2\\r\\n\"1985-06-09\",6.5\\r\\n\"1985-06-10\",10.0\\r\\n\"1985-06-11\",8.8\\r\\n\"1985-06-12\",6.6\\r\\n\"1985-06-13\",7.8\\r\\n\"1985-06-14\",10.1\\r\\n\"1985-06-15\",7.1\\r\\n\"1985-06-16\",7.7\\r\\n\"1985-06-17\",8.5\\r\\n\"1985-06-18\",7.3\\r\\n\"1985-06-19\",6.9\\r\\n\"1985-06-20\",8.4\\r\\n\"1985-06-21\",7.1\\r\\n\"1985-06-22\",6.3\\r\\n\"1985-06-23\",0.6\\r\\n\"1985-06-24\",1.6\\r\\n\"1985-06-25\",7.0\\r\\n\"1985-06-26\",8.3\\r\\n\"1985-06-27\",8.0\\r\\n\"1985-06-28\",10.2\\r\\n\"1985-06-29\",10.6\\r\\n\"1985-06-30\",10.4\\r\\n\"1985-07-01\",11.6\\r\\n\"1985-07-02\",11.0\\r\\n\"1985-07-03\",10.7\\r\\n\"1985-07-04\",7.3\\r\\n\"1985-07-05\",4.2\\r\\n\"1985-07-06\",4.7\\r\\n\"1985-07-07\",5.6\\r\\n\"1985-07-08\",7.7\\r\\n\"1985-07-09\",7.5\\r\\n\"1985-07-10\",4.9\\r\\n\"1985-07-11\",5.9\\r\\n\"1985-07-12\",7.8\\r\\n\"1985-07-13\",5.8\\r\\n\"1985-07-14\",7.0\\r\\n\"1985-07-15\",8.4\\r\\n\"1985-07-16\",6.2\\r\\n\"1985-07-17\",7.5\\r\\n\"1985-07-18\",4.8\\r\\n\"1985-07-19\",3.3\\r\\n\"1985-07-20\",3.2\\r\\n\"1985-07-21\",7.0\\r\\n\"1985-07-22\",8.4\\r\\n\"1985-07-23\",0.3\\r\\n\"1985-07-24\",0.3\\r\\n\"1985-07-25\",2.1\\r\\n\"1985-07-26\",8.5\\r\\n\"1985-07-27\",1.4\\r\\n\"1985-07-28\",4.1\\r\\n\"1985-07-29\",10.3\\r\\n\"1985-07-30\",6.6\\r\\n\"1985-07-31\",6.1\\r\\n\"1985-08-01\",7.0\\r\\n\"1985-08-02\",5.1\\r\\n\"1985-08-03\",6.3\\r\\n\"1985-08-04\",6.9\\r\\n\"1985-08-05\",11.4\\r\\n\"1985-08-06\",10.4\\r\\n\"1985-08-07\",10.3\\r\\n\"1985-08-08\",9.2\\r\\n\"1985-08-09\",7.2\\r\\n\"1985-08-10\",7.5\\r\\n\"1985-08-11\",4.0\\r\\n\"1985-08-12\",5.6\\r\\n\"1985-08-13\",6.7\\r\\n\"1985-08-14\",8.4\\r\\n\"1985-08-15\",11.0\\r\\n\"1985-08-16\",8.4\\r\\n\"1985-08-17\",8.8\\r\\n\"1985-08-18\",8.6\\r\\n\"1985-08-19\",8.3\\r\\n\"1985-08-20\",4.0\\r\\n\"1985-08-21\",3.6\\r\\n\"1985-08-22\",5.7\\r\\n\"1985-08-23\",10.6\\r\\n\"1985-08-24\",6.9\\r\\n\"1985-08-25\",10.0\\r\\n\"1985-08-26\",9.8\\r\\n\"1985-08-27\",7.2\\r\\n\"1985-08-28\",10.5\\r\\n\"1985-08-29\",3.6\\r\\n\"1985-08-30\",5.3\\r\\n\"1985-08-31\",8.4\\r\\n\"1985-09-01\",10.3\\r\\n\"1985-09-02\",7.9\\r\\n\"1985-09-03\",8.5\\r\\n\"1985-09-04\",7.9\\r\\n\"1985-09-05\",8.0\\r\\n\"1985-09-06\",9.8\\r\\n\"1985-09-07\",6.7\\r\\n\"1985-09-08\",4.8\\r\\n\"1985-09-09\",9.9\\r\\n\"1985-09-10\",12.8\\r\\n\"1985-09-11\",10.9\\r\\n\"1985-09-12\",11.7\\r\\n\"1985-09-13\",11.7\\r\\n\"1985-09-14\",11.0\\r\\n\"1985-09-15\",8.2\\r\\n\"1985-09-16\",7.5\\r\\n\"1985-09-17\",5.4\\r\\n\"1985-09-18\",7.2\\r\\n\"1985-09-19\",9.7\\r\\n\"1985-09-20\",8.4\\r\\n\"1985-09-21\",9.0\\r\\n\"1985-09-22\",8.7\\r\\n\"1985-09-23\",6.6\\r\\n\"1985-09-24\",11.6\\r\\n\"1985-09-25\",13.1\\r\\n\"1985-09-26\",6.7\\r\\n\"1985-09-27\",6.5\\r\\n\"1985-09-28\",7.7\\r\\n\"1985-09-29\",8.7\\r\\n\"1985-09-30\",7.2\\r\\n\"1985-10-01\",10.5\\r\\n\"1985-10-02\",8.6\\r\\n\"1985-10-03\",7.2\\r\\n\"1985-10-04\",11.4\\r\\n\"1985-10-05\",16.2\\r\\n\"1985-10-06\",6.1\\r\\n\"1985-10-07\",9.6\\r\\n\"1985-10-08\",11.1\\r\\n\"1985-10-09\",13.6\\r\\n\"1985-10-10\",10.7\\r\\n\"1985-10-11\",14.7\\r\\n\"1985-10-12\",11.6\\r\\n\"1985-10-13\",7.3\\r\\n\"1985-10-14\",8.0\\r\\n\"1985-10-15\",9.6\\r\\n\"1985-10-16\",16.0\\r\\n\"1985-10-17\",15.1\\r\\n\"1985-10-18\",12.8\\r\\n\"1985-10-19\",6.2\\r\\n\"1985-10-20\",7.1\\r\\n\"1985-10-21\",8.4\\r\\n\"1985-10-22\",10.0\\r\\n\"1985-10-23\",12.7\\r\\n\"1985-10-24\",10.0\\r\\n\"1985-10-25\",10.2\\r\\n\"1985-10-26\",6.5\\r\\n\"1985-10-27\",9.2\\r\\n\"1985-10-28\",11.9\\r\\n\"1985-10-29\",14.7\\r\\n\"1985-10-30\",11.4\\r\\n\"1985-10-31\",6.8\\r\\n\"1985-11-01\",7.4\\r\\n\"1985-11-02\",11.2\\r\\n\"1985-11-03\",9.2\\r\\n\"1985-11-04\",12.6\\r\\n\"1985-11-05\",16.0\\r\\n\"1985-11-06\",17.1\\r\\n\"1985-11-07\",15.3\\r\\n\"1985-11-08\",13.3\\r\\n\"1985-11-09\",15.4\\r\\n\"1985-11-10\",13.2\\r\\n\"1985-11-11\",14.4\\r\\n\"1985-11-12\",14.0\\r\\n\"1985-11-13\",15.5\\r\\n\"1985-11-14\",21.0\\r\\n\"1985-11-15\",10.0\\r\\n\"1985-11-16\",9.6\\r\\n\"1985-11-17\",12.0\\r\\n\"1985-11-18\",12.2\\r\\n\"1985-11-19\",11.3\\r\\n\"1985-11-20\",13.2\\r\\n\"1985-11-21\",10.5\\r\\n\"1985-11-22\",10.1\\r\\n\"1985-11-23\",8.8\\r\\n\"1985-11-24\",13.7\\r\\n\"1985-11-25\",16.2\\r\\n\"1985-11-26\",16.0\\r\\n\"1985-11-27\",14.0\\r\\n\"1985-11-28\",13.7\\r\\n\"1985-11-29\",12.5\\r\\n\"1985-11-30\",12.8\\r\\n\"1985-12-01\",12.3\\r\\n\"1985-12-02\",15.2\\r\\n\"1985-12-03\",15.0\\r\\n\"1985-12-04\",16.4\\r\\n\"1985-12-05\",16.1\\r\\n\"1985-12-06\",14.6\\r\\n\"1985-12-07\",18.2\\r\\n\"1985-12-08\",16.4\\r\\n\"1985-12-09\",16.6\\r\\n\"1985-12-10\",14.7\\r\\n\"1985-12-11\",15.8\\r\\n\"1985-12-12\",14.1\\r\\n\"1985-12-13\",13.5\\r\\n\"1985-12-14\",13.6\\r\\n\"1985-12-15\",13.7\\r\\n\"1985-12-16\",13.6\\r\\n\"1985-12-17\",12.1\\r\\n\"1985-12-18\",12.7\\r\\n\"1985-12-19\",13.3\\r\\n\"1985-12-20\",14.2\\r\\n\"1985-12-21\",15.0\\r\\n\"1985-12-22\",13.7\\r\\n\"1985-12-23\",12.0\\r\\n\"1985-12-24\",13.1\\r\\n\"1985-12-25\",13.2\\r\\n\"1985-12-26\",13.3\\r\\n\"1985-12-27\",11.5\\r\\n\"1985-12-28\",10.8\\r\\n\"1985-12-29\",12.0\\r\\n\"1985-12-30\",16.3\\r\\n\"1985-12-31\",14.4\\r\\n\"1986-01-01\",12.9\\r\\n\"1986-01-02\",13.8\\r\\n\"1986-01-03\",10.6\\r\\n\"1986-01-04\",12.6\\r\\n\"1986-01-05\",13.7\\r\\n\"1986-01-06\",12.6\\r\\n\"1986-01-07\",13.1\\r\\n\"1986-01-08\",15.4\\r\\n\"1986-01-09\",11.9\\r\\n\"1986-01-10\",13.8\\r\\n\"1986-01-11\",14.4\\r\\n\"1986-01-12\",15.2\\r\\n\"1986-01-13\",12.5\\r\\n\"1986-01-14\",12.2\\r\\n\"1986-01-15\",16.1\\r\\n\"1986-01-16\",14.6\\r\\n\"1986-01-17\",11.6\\r\\n\"1986-01-18\",13.1\\r\\n\"1986-01-19\",12.8\\r\\n\"1986-01-20\",15.2\\r\\n\"1986-01-21\",13.8\\r\\n\"1986-01-22\",15.0\\r\\n\"1986-01-23\",13.5\\r\\n\"1986-01-24\",11.8\\r\\n\"1986-01-25\",15.3\\r\\n\"1986-01-26\",13.5\\r\\n\"1986-01-27\",15.3\\r\\n\"1986-01-28\",13.8\\r\\n\"1986-01-29\",15.8\\r\\n\"1986-01-30\",17.4\\r\\n\"1986-01-31\",15.3\\r\\n\"1986-02-01\",14.6\\r\\n\"1986-02-02\",14.8\\r\\n\"1986-02-03\",10.7\\r\\n\"1986-02-04\",11.6\\r\\n\"1986-02-05\",13.6\\r\\n\"1986-02-06\",14.4\\r\\n\"1986-02-07\",11.8\\r\\n\"1986-02-08\",15.8\\r\\n\"1986-02-09\",16.0\\r\\n\"1986-02-10\",11.8\\r\\n\"1986-02-11\",14.5\\r\\n\"1986-02-12\",10.7\\r\\n\"1986-02-13\",14.2\\r\\n\"1986-02-14\",19.5\\r\\n\"1986-02-15\",21.4\\r\\n\"1986-02-16\",17.9\\r\\n\"1986-02-17\",17.4\\r\\n\"1986-02-18\",12.7\\r\\n\"1986-02-19\",13.8\\r\\n\"1986-02-20\",14.0\\r\\n\"1986-02-21\",15.0\\r\\n\"1986-02-22\",14.5\\r\\n\"1986-02-23\",13.1\\r\\n\"1986-02-24\",11.4\\r\\n\"1986-02-25\",12.5\\r\\n\"1986-02-26\",12.0\\r\\n\"1986-02-27\",13.4\\r\\n\"1986-02-28\",14.4\\r\\n\"1986-03-01\",17.7\\r\\n\"1986-03-02\",13.9\\r\\n\"1986-03-03\",13.3\\r\\n\"1986-03-04\",14.6\\r\\n\"1986-03-05\",16.4\\r\\n\"1986-03-06\",16.8\\r\\n\"1986-03-07\",20.0\\r\\n\"1986-03-08\",12.5\\r\\n\"1986-03-09\",12.7\\r\\n\"1986-03-10\",11.7\\r\\n\"1986-03-11\",12.7\\r\\n\"1986-03-12\",8.6\\r\\n\"1986-03-13\",11.9\\r\\n\"1986-03-14\",16.0\\r\\n\"1986-03-15\",15.2\\r\\n\"1986-03-16\",13.4\\r\\n\"1986-03-17\",11.6\\r\\n\"1986-03-18\",11.1\\r\\n\"1986-03-19\",15.6\\r\\n\"1986-03-20\",17.0\\r\\n\"1986-03-21\",18.5\\r\\n\"1986-03-22\",17.4\\r\\n\"1986-03-23\",16.5\\r\\n\"1986-03-24\",16.2\\r\\n\"1986-03-25\",16.1\\r\\n\"1986-03-26\",13.2\\r\\n\"1986-03-27\",18.0\\r\\n\"1986-03-28\",12.8\\r\\n\"1986-03-29\",11.7\\r\\n\"1986-03-30\",16.7\\r\\n\"1986-03-31\",15.6\\r\\n\"1986-04-01\",10.2\\r\\n\"1986-04-02\",10.3\\r\\n\"1986-04-03\",15.0\\r\\n\"1986-04-04\",18.0\\r\\n\"1986-04-05\",13.8\\r\\n\"1986-04-06\",10.5\\r\\n\"1986-04-07\",11.8\\r\\n\"1986-04-08\",7.2\\r\\n\"1986-04-09\",11.6\\r\\n\"1986-04-10\",7.4\\r\\n\"1986-04-11\",14.2\\r\\n\"1986-04-12\",12.2\\r\\n\"1986-04-13\",9.0\\r\\n\"1986-04-14\",12.3\\r\\n\"1986-04-15\",19.7\\r\\n\"1986-04-16\",12.8\\r\\n\"1986-04-17\",12.4\\r\\n\"1986-04-18\",12.0\\r\\n\"1986-04-19\",12.0\\r\\n\"1986-04-20\",11.1\\r\\n\"1986-04-21\",12.7\\r\\n\"1986-04-22\",14.2\\r\\n\"1986-04-23\",11.6\\r\\n\"1986-04-24\",12.0\\r\\n\"1986-04-25\",11.5\\r\\n\"1986-04-26\",8.3\\r\\n\"1986-04-27\",10.5\\r\\n\"1986-04-28\",9.0\\r\\n\"1986-04-29\",6.9\\r\\n\"1986-04-30\",9.4\\r\\n\"1986-05-01\",11.1\\r\\n\"1986-05-02\",9.1\\r\\n\"1986-05-03\",7.7\\r\\n\"1986-05-04\",10.0\\r\\n\"1986-05-05\",10.4\\r\\n\"1986-05-06\",8.0\\r\\n\"1986-05-07\",9.8\\r\\n\"1986-05-08\",12.4\\r\\n\"1986-05-09\",12.9\\r\\n\"1986-05-10\",12.3\\r\\n\"1986-05-11\",6.9\\r\\n\"1986-05-12\",10.5\\r\\n\"1986-05-13\",11.0\\r\\n\"1986-05-14\",9.7\\r\\n\"1986-05-15\",11.1\\r\\n\"1986-05-16\",11.5\\r\\n\"1986-05-17\",13.4\\r\\n\"1986-05-18\",10.9\\r\\n\"1986-05-19\",12.0\\r\\n\"1986-05-20\",12.1\\r\\n\"1986-05-21\",10.4\\r\\n\"1986-05-22\",10.0\\r\\n\"1986-05-23\",9.6\\r\\n\"1986-05-24\",11.3\\r\\n\"1986-05-25\",8.5\\r\\n\"1986-05-26\",6.3\\r\\n\"1986-05-27\",8.2\\r\\n\"1986-05-28\",10.7\\r\\n\"1986-05-29\",10.3\\r\\n\"1986-05-30\",9.5\\r\\n\"1986-05-31\",10.9\\r\\n\"1986-06-01\",10.9\\r\\n\"1986-06-02\",4.3\\r\\n\"1986-06-03\",5.2\\r\\n\"1986-06-04\",11.0\\r\\n\"1986-06-05\",11.6\\r\\n\"1986-06-06\",10.6\\r\\n\"1986-06-07\",9.4\\r\\n\"1986-06-08\",10.0\\r\\n\"1986-06-09\",9.6\\r\\n\"1986-06-10\",9.5\\r\\n\"1986-06-11\",9.7\\r\\n\"1986-06-12\",9.6\\r\\n\"1986-06-13\",7.0\\r\\n\"1986-06-14\",7.0\\r\\n\"1986-06-15\",6.8\\r\\n\"1986-06-16\",6.9\\r\\n\"1986-06-17\",8.0\\r\\n\"1986-06-18\",7.6\\r\\n\"1986-06-19\",8.6\\r\\n\"1986-06-20\",5.7\\r\\n\"1986-06-21\",5.5\\r\\n\"1986-06-22\",5.7\\r\\n\"1986-06-23\",5.7\\r\\n\"1986-06-24\",6.6\\r\\n\"1986-06-25\",6.0\\r\\n\"1986-06-26\",6.9\\r\\n\"1986-06-27\",7.7\\r\\n\"1986-06-28\",8.0\\r\\n\"1986-06-29\",3.9\\r\\n\"1986-06-30\",0.8\\r\\n\"1986-07-01\",2.8\\r\\n\"1986-07-02\",8.0\\r\\n\"1986-07-03\",9.8\\r\\n\"1986-07-04\",11.4\\r\\n\"1986-07-05\",8.6\\r\\n\"1986-07-06\",5.2\\r\\n\"1986-07-07\",6.6\\r\\n\"1986-07-08\",5.7\\r\\n\"1986-07-09\",4.6\\r\\n\"1986-07-10\",5.8\\r\\n\"1986-07-11\",7.0\\r\\n\"1986-07-12\",4.8\\r\\n\"1986-07-13\",4.4\\r\\n\"1986-07-14\",4.4\\r\\n\"1986-07-15\",7.9\\r\\n\"1986-07-16\",10.6\\r\\n\"1986-07-17\",5.0\\r\\n\"1986-07-18\",7.6\\r\\n\"1986-07-19\",9.2\\r\\n\"1986-07-20\",9.7\\r\\n\"1986-07-21\",8.8\\r\\n\"1986-07-22\",6.8\\r\\n\"1986-07-23\",9.4\\r\\n\"1986-07-24\",11.0\\r\\n\"1986-07-25\",2.5\\r\\n\"1986-07-26\",2.1\\r\\n\"1986-07-27\",5.4\\r\\n\"1986-07-28\",6.2\\r\\n\"1986-07-29\",7.8\\r\\n\"1986-07-30\",7.4\\r\\n\"1986-07-31\",9.3\\r\\n\"1986-08-01\",9.3\\r\\n\"1986-08-02\",9.5\\r\\n\"1986-08-03\",8.5\\r\\n\"1986-08-04\",10.0\\r\\n\"1986-08-05\",7.7\\r\\n\"1986-08-06\",9.3\\r\\n\"1986-08-07\",9.1\\r\\n\"1986-08-08\",3.5\\r\\n\"1986-08-09\",3.6\\r\\n\"1986-08-10\",2.5\\r\\n\"1986-08-11\",1.7\\r\\n\"1986-08-12\",2.7\\r\\n\"1986-08-13\",2.9\\r\\n\"1986-08-14\",5.3\\r\\n\"1986-08-15\",7.7\\r\\n\"1986-08-16\",9.1\\r\\n\"1986-08-17\",9.4\\r\\n\"1986-08-18\",7.3\\r\\n\"1986-08-19\",8.4\\r\\n\"1986-08-20\",9.2\\r\\n\"1986-08-21\",6.6\\r\\n\"1986-08-22\",9.7\\r\\n\"1986-08-23\",12.4\\r\\n\"1986-08-24\",10.2\\r\\n\"1986-08-25\",5.9\\r\\n\"1986-08-26\",7.1\\r\\n\"1986-08-27\",7.5\\r\\n\"1986-08-28\",9.7\\r\\n\"1986-08-29\",12.2\\r\\n\"1986-08-30\",5.6\\r\\n\"1986-08-31\",5.4\\r\\n\"1986-09-01\",8.3\\r\\n\"1986-09-02\",10.6\\r\\n\"1986-09-03\",9.1\\r\\n\"1986-09-04\",11.3\\r\\n\"1986-09-05\",10.9\\r\\n\"1986-09-06\",8.9\\r\\n\"1986-09-07\",6.3\\r\\n\"1986-09-08\",9.0\\r\\n\"1986-09-09\",6.1\\r\\n\"1986-09-10\",9.1\\r\\n\"1986-09-11\",9.6\\r\\n\"1986-09-12\",6.0\\r\\n\"1986-09-13\",10.0\\r\\n\"1986-09-14\",11.0\\r\\n\"1986-09-15\",6.2\\r\\n\"1986-09-16\",8.3\\r\\n\"1986-09-17\",11.3\\r\\n\"1986-09-18\",11.3\\r\\n\"1986-09-19\",6.7\\r\\n\"1986-09-20\",6.6\\r\\n\"1986-09-21\",11.4\\r\\n\"1986-09-22\",6.9\\r\\n\"1986-09-23\",10.6\\r\\n\"1986-09-24\",8.6\\r\\n\"1986-09-25\",11.3\\r\\n\"1986-09-26\",12.5\\r\\n\"1986-09-27\",9.9\\r\\n\"1986-09-28\",6.9\\r\\n\"1986-09-29\",5.5\\r\\n\"1986-09-30\",7.8\\r\\n\"1986-10-01\",11.0\\r\\n\"1986-10-02\",16.2\\r\\n\"1986-10-03\",9.9\\r\\n\"1986-10-04\",8.7\\r\\n\"1986-10-05\",10.5\\r\\n\"1986-10-06\",12.2\\r\\n\"1986-10-07\",10.6\\r\\n\"1986-10-08\",8.3\\r\\n\"1986-10-09\",5.5\\r\\n\"1986-10-10\",9.0\\r\\n\"1986-10-11\",6.4\\r\\n\"1986-10-12\",7.2\\r\\n\"1986-10-13\",12.9\\r\\n\"1986-10-14\",12.0\\r\\n\"1986-10-15\",7.3\\r\\n\"1986-10-16\",9.7\\r\\n\"1986-10-17\",8.4\\r\\n\"1986-10-18\",14.7\\r\\n\"1986-10-19\",9.5\\r\\n\"1986-10-20\",7.9\\r\\n\"1986-10-21\",6.8\\r\\n\"1986-10-22\",12.6\\r\\n\"1986-10-23\",5.2\\r\\n\"1986-10-24\",7.5\\r\\n\"1986-10-25\",8.7\\r\\n\"1986-10-26\",7.6\\r\\n\"1986-10-27\",9.0\\r\\n\"1986-10-28\",7.2\\r\\n\"1986-10-29\",10.7\\r\\n\"1986-10-30\",13.1\\r\\n\"1986-10-31\",13.9\\r\\n\"1986-11-01\",10.8\\r\\n\"1986-11-02\",10.4\\r\\n\"1986-11-03\",9.1\\r\\n\"1986-11-04\",16.0\\r\\n\"1986-11-05\",21.0\\r\\n\"1986-11-06\",16.2\\r\\n\"1986-11-07\",8.6\\r\\n\"1986-11-08\",9.2\\r\\n\"1986-11-09\",12.5\\r\\n\"1986-11-10\",9.7\\r\\n\"1986-11-11\",12.5\\r\\n\"1986-11-12\",10.3\\r\\n\"1986-11-13\",12.0\\r\\n\"1986-11-14\",11.0\\r\\n\"1986-11-15\",14.8\\r\\n\"1986-11-16\",15.0\\r\\n\"1986-11-17\",15.3\\r\\n\"1986-11-18\",10.3\\r\\n\"1986-11-19\",10.7\\r\\n\"1986-11-20\",10.5\\r\\n\"1986-11-21\",8.9\\r\\n\"1986-11-22\",8.1\\r\\n\"1986-11-23\",11.5\\r\\n\"1986-11-24\",12.8\\r\\n\"1986-11-25\",9.1\\r\\n\"1986-11-26\",14.6\\r\\n\"1986-11-27\",11.6\\r\\n\"1986-11-28\",11.2\\r\\n\"1986-11-29\",12.6\\r\\n\"1986-11-30\",7.5\\r\\n\"1986-12-01\",11.0\\r\\n\"1986-12-02\",14.5\\r\\n\"1986-12-03\",18.5\\r\\n\"1986-12-04\",15.4\\r\\n\"1986-12-05\",13.1\\r\\n\"1986-12-06\",16.3\\r\\n\"1986-12-07\",20.2\\r\\n\"1986-12-08\",11.5\\r\\n\"1986-12-09\",12.4\\r\\n\"1986-12-10\",10.9\\r\\n\"1986-12-11\",12.7\\r\\n\"1986-12-12\",12.2\\r\\n\"1986-12-13\",12.4\\r\\n\"1986-12-14\",9.8\\r\\n\"1986-12-15\",8.5\\r\\n\"1986-12-16\",14.7\\r\\n\"1986-12-17\",12.0\\r\\n\"1986-12-18\",10.3\\r\\n\"1986-12-19\",11.0\\r\\n\"1986-12-20\",10.2\\r\\n\"1986-12-21\",12.6\\r\\n\"1986-12-22\",11.6\\r\\n\"1986-12-23\",9.7\\r\\n\"1986-12-24\",13.4\\r\\n\"1986-12-25\",10.5\\r\\n\"1986-12-26\",14.7\\r\\n\"1986-12-27\",14.6\\r\\n\"1986-12-28\",14.2\\r\\n\"1986-12-29\",13.2\\r\\n\"1986-12-30\",11.7\\r\\n\"1986-12-31\",17.2\\r\\n\"1987-01-01\",12.3\\r\\n\"1987-01-02\",13.8\\r\\n\"1987-01-03\",15.3\\r\\n\"1987-01-04\",15.6\\r\\n\"1987-01-05\",16.2\\r\\n\"1987-01-06\",16.3\\r\\n\"1987-01-07\",16.8\\r\\n\"1987-01-08\",11.0\\r\\n\"1987-01-09\",8.5\\r\\n\"1987-01-10\",13.2\\r\\n\"1987-01-11\",13.0\\r\\n\"1987-01-12\",12.4\\r\\n\"1987-01-13\",13.0\\r\\n\"1987-01-14\",16.6\\r\\n\"1987-01-15\",12.0\\r\\n\"1987-01-16\",12.4\\r\\n\"1987-01-17\",15.0\\r\\n\"1987-01-18\",11.8\\r\\n\"1987-01-19\",11.6\\r\\n\"1987-01-20\",12.2\\r\\n\"1987-01-21\",13.7\\r\\n\"1987-01-22\",11.2\\r\\n\"1987-01-23\",12.4\\r\\n\"1987-01-24\",11.5\\r\\n\"1987-01-25\",13.8\\r\\n\"1987-01-26\",15.7\\r\\n\"1987-01-27\",12.9\\r\\n\"1987-01-28\",11.5\\r\\n\"1987-01-29\",11.0\\r\\n\"1987-01-30\",12.7\\r\\n\"1987-01-31\",14.9\\r\\n\"1987-02-01\",16.5\\r\\n\"1987-02-02\",12.8\\r\\n\"1987-02-03\",12.7\\r\\n\"1987-02-04\",12.7\\r\\n\"1987-02-05\",11.6\\r\\n\"1987-02-06\",13.3\\r\\n\"1987-02-07\",15.2\\r\\n\"1987-02-08\",16.4\\r\\n\"1987-02-09\",11.9\\r\\n\"1987-02-10\",15.1\\r\\n\"1987-02-11\",10.6\\r\\n\"1987-02-12\",13.6\\r\\n\"1987-02-13\",12.1\\r\\n\"1987-02-14\",16.0\\r\\n\"1987-02-15\",16.8\\r\\n\"1987-02-16\",16.6\\r\\n\"1987-02-17\",15.6\\r\\n\"1987-02-18\",15.2\\r\\n\"1987-02-19\",17.7\\r\\n\"1987-02-20\",21.0\\r\\n\"1987-02-21\",13.4\\r\\n\"1987-02-22\",10.5\\r\\n\"1987-02-23\",9.5\\r\\n\"1987-02-24\",12.0\\r\\n\"1987-02-25\",10.4\\r\\n\"1987-02-26\",11.5\\r\\n\"1987-02-27\",13.2\\r\\n\"1987-02-28\",15.0\\r\\n\"1987-03-01\",14.1\\r\\n\"1987-03-02\",12.4\\r\\n\"1987-03-03\",13.4\\r\\n\"1987-03-04\",12.5\\r\\n\"1987-03-05\",14.3\\r\\n\"1987-03-06\",17.6\\r\\n\"1987-03-07\",10.4\\r\\n\"1987-03-08\",9.9\\r\\n\"1987-03-09\",10.2\\r\\n\"1987-03-10\",11.3\\r\\n\"1987-03-11\",9.5\\r\\n\"1987-03-12\",11.8\\r\\n\"1987-03-13\",11.5\\r\\n\"1987-03-14\",10.5\\r\\n\"1987-03-15\",10.8\\r\\n\"1987-03-16\",13.0\\r\\n\"1987-03-17\",18.5\\r\\n\"1987-03-18\",18.7\\r\\n\"1987-03-19\",15.0\\r\\n\"1987-03-20\",13.0\\r\\n\"1987-03-21\",11.3\\r\\n\"1987-03-22\",13.0\\r\\n\"1987-03-23\",13.3\\r\\n\"1987-03-24\",11.0\\r\\n\"1987-03-25\",10.3\\r\\n\"1987-03-26\",13.0\\r\\n\"1987-03-27\",12.3\\r\\n\"1987-03-28\",15.6\\r\\n\"1987-03-29\",10.2\\r\\n\"1987-03-30\",10.8\\r\\n\"1987-03-31\",12.0\\r\\n\"1987-04-01\",13.3\\r\\n\"1987-04-02\",11.7\\r\\n\"1987-04-03\",12.5\\r\\n\"1987-04-04\",13.7\\r\\n\"1987-04-05\",14.9\\r\\n\"1987-04-06\",20.2\\r\\n\"1987-04-07\",16.3\\r\\n\"1987-04-08\",13.9\\r\\n\"1987-04-09\",10.1\\r\\n\"1987-04-10\",7.3\\r\\n\"1987-04-11\",14.0\\r\\n\"1987-04-12\",17.7\\r\\n\"1987-04-13\",16.3\\r\\n\"1987-04-14\",10.6\\r\\n\"1987-04-15\",9.7\\r\\n\"1987-04-16\",7.8\\r\\n\"1987-04-17\",10.4\\r\\n\"1987-04-18\",10.4\\r\\n\"1987-04-19\",14.1\\r\\n\"1987-04-20\",7.1\\r\\n\"1987-04-21\",8.1\\r\\n\"1987-04-22\",7.8\\r\\n\"1987-04-23\",10.6\\r\\n\"1987-04-24\",9.1\\r\\n\"1987-04-25\",9.0\\r\\n\"1987-04-26\",11.9\\r\\n\"1987-04-27\",17.1\\r\\n\"1987-04-28\",16.8\\r\\n\"1987-04-29\",13.5\\r\\n\"1987-04-30\",11.6\\r\\n\"1987-05-01\",7.0\\r\\n\"1987-05-02\",9.7\\r\\n\"1987-05-03\",9.9\\r\\n\"1987-05-04\",11.2\\r\\n\"1987-05-05\",11.3\\r\\n\"1987-05-06\",11.8\\r\\n\"1987-05-07\",9.9\\r\\n\"1987-05-08\",7.1\\r\\n\"1987-05-09\",9.6\\r\\n\"1987-05-10\",9.8\\r\\n\"1987-05-11\",10.6\\r\\n\"1987-05-12\",12.8\\r\\n\"1987-05-13\",16.5\\r\\n\"1987-05-14\",11.7\\r\\n\"1987-05-15\",12.3\\r\\n\"1987-05-16\",12.2\\r\\n\"1987-05-17\",11.8\\r\\n\"1987-05-18\",10.7\\r\\n\"1987-05-19\",10.2\\r\\n\"1987-05-20\",10.0\\r\\n\"1987-05-21\",8.3\\r\\n\"1987-05-22\",6.6\\r\\n\"1987-05-23\",9.5\\r\\n\"1987-05-24\",12.3\\r\\n\"1987-05-25\",7.6\\r\\n\"1987-05-26\",9.3\\r\\n\"1987-05-27\",5.0\\r\\n\"1987-05-28\",4.3\\r\\n\"1987-05-29\",6.4\\r\\n\"1987-05-30\",10.8\\r\\n\"1987-05-31\",7.8\\r\\n\"1987-06-01\",8.5\\r\\n\"1987-06-02\",9.7\\r\\n\"1987-06-03\",10.0\\r\\n\"1987-06-04\",11.0\\r\\n\"1987-06-05\",10.2\\r\\n\"1987-06-06\",6.6\\r\\n\"1987-06-07\",6.1\\r\\n\"1987-06-08\",5.9\\r\\n\"1987-06-09\",8.9\\r\\n\"1987-06-10\",13.0\\r\\n\"1987-06-11\",12.6\\r\\n\"1987-06-12\",5.4\\r\\n\"1987-06-13\",6.0\\r\\n\"1987-06-14\",7.8\\r\\n\"1987-06-15\",9.0\\r\\n\"1987-06-16\",4.2\\r\\n\"1987-06-17\",3.0\\r\\n\"1987-06-18\",4.5\\r\\n\"1987-06-19\",6.2\\r\\n\"1987-06-20\",11.9\\r\\n\"1987-06-21\",11.8\\r\\n\"1987-06-22\",9.4\\r\\n\"1987-06-23\",9.6\\r\\n\"1987-06-24\",9.4\\r\\n\"1987-06-25\",7.0\\r\\n\"1987-06-26\",8.9\\r\\n\"1987-06-27\",9.3\\r\\n\"1987-06-28\",6.8\\r\\n\"1987-06-29\",7.5\\r\\n\"1987-06-30\",8.0\\r\\n\"1987-07-01\",8.3\\r\\n\"1987-07-02\",2.7\\r\\n\"1987-07-03\",3.9\\r\\n\"1987-07-04\",4.1\\r\\n\"1987-07-05\",5.0\\r\\n\"1987-07-06\",5.8\\r\\n\"1987-07-07\",4.4\\r\\n\"1987-07-08\",4.1\\r\\n\"1987-07-09\",5.8\\r\\n\"1987-07-10\",9.1\\r\\n\"1987-07-11\",7.9\\r\\n\"1987-07-12\",5.0\\r\\n\"1987-07-13\",2.8\\r\\n\"1987-07-14\",4.7\\r\\n\"1987-07-15\",8.9\\r\\n\"1987-07-16\",5.4\\r\\n\"1987-07-17\",7.1\\r\\n\"1987-07-18\",9.0\\r\\n\"1987-07-19\",9.4\\r\\n\"1987-07-20\",6.3\\r\\n\"1987-07-21\",7.0\\r\\n\"1987-07-22\",6.4\\r\\n\"1987-07-23\",6.7\\r\\n\"1987-07-24\",1.5\\r\\n\"1987-07-25\",2.9\\r\\n\"1987-07-26\",4.8\\r\\n\"1987-07-27\",6.3\\r\\n\"1987-07-28\",5.7\\r\\n\"1987-07-29\",7.0\\r\\n\"1987-07-30\",8.8\\r\\n\"1987-07-31\",8.7\\r\\n\"1987-08-01\",9.0\\r\\n\"1987-08-02\",9.6\\r\\n\"1987-08-03\",8.0\\r\\n\"1987-08-04\",8.4\\r\\n\"1987-08-05\",8.1\\r\\n\"1987-08-06\",9.0\\r\\n\"1987-08-07\",5.3\\r\\n\"1987-08-08\",8.9\\r\\n\"1987-08-09\",8.7\\r\\n\"1987-08-10\",4.9\\r\\n\"1987-08-11\",7.0\\r\\n\"1987-08-12\",7.5\\r\\n\"1987-08-13\",7.0\\r\\n\"1987-08-14\",9.1\\r\\n\"1987-08-15\",11.8\\r\\n\"1987-08-16\",9.9\\r\\n\"1987-08-17\",5.6\\r\\n\"1987-08-18\",4.2\\r\\n\"1987-08-19\",4.3\\r\\n\"1987-08-20\",8.0\\r\\n\"1987-08-21\",5.1\\r\\n\"1987-08-22\",9.4\\r\\n\"1987-08-23\",9.1\\r\\n\"1987-08-24\",9.7\\r\\n\"1987-08-25\",10.6\\r\\n\"1987-08-26\",8.6\\r\\n\"1987-08-27\",10.1\\r\\n\"1987-08-28\",11.0\\r\\n\"1987-08-29\",9.7\\r\\n\"1987-08-30\",5.0\\r\\n\"1987-08-31\",6.1\\r\\n\"1987-09-01\",5.4\\r\\n\"1987-09-02\",5.8\\r\\n\"1987-09-03\",7.3\\r\\n\"1987-09-04\",6.3\\r\\n\"1987-09-05\",4.8\\r\\n\"1987-09-06\",7.6\\r\\n\"1987-09-07\",8.1\\r\\n\"1987-09-08\",9.5\\r\\n\"1987-09-09\",10.3\\r\\n\"1987-09-10\",7.0\\r\\n\"1987-09-11\",9.0\\r\\n\"1987-09-12\",10.2\\r\\n\"1987-09-13\",6.8\\r\\n\"1987-09-14\",9.3\\r\\n\"1987-09-15\",9.8\\r\\n\"1987-09-16\",10.7\\r\\n\"1987-09-17\",7.8\\r\\n\"1987-09-18\",9.2\\r\\n\"1987-09-19\",15.0\\r\\n\"1987-09-20\",7.8\\r\\n\"1987-09-21\",5.3\\r\\n\"1987-09-22\",9.5\\r\\n\"1987-09-23\",7.6\\r\\n\"1987-09-24\",14.0\\r\\n\"1987-09-25\",14.9\\r\\n\"1987-09-26\",14.9\\r\\n\"1987-09-27\",19.2\\r\\n\"1987-09-28\",17.0\\r\\n\"1987-09-29\",13.0\\r\\n\"1987-09-30\",11.2\\r\\n\"1987-10-01\",9.5\\r\\n\"1987-10-02\",10.3\\r\\n\"1987-10-03\",9.3\\r\\n\"1987-10-04\",11.3\\r\\n\"1987-10-05\",6.5\\r\\n\"1987-10-06\",12.0\\r\\n\"1987-10-07\",8.3\\r\\n\"1987-10-08\",8.7\\r\\n\"1987-10-09\",8.7\\r\\n\"1987-10-10\",10.2\\r\\n\"1987-10-11\",6.9\\r\\n\"1987-10-12\",4.9\\r\\n\"1987-10-13\",10.0\\r\\n\"1987-10-14\",7.6\\r\\n\"1987-10-15\",14.5\\r\\n\"1987-10-16\",13.2\\r\\n\"1987-10-17\",9.9\\r\\n\"1987-10-18\",10.1\\r\\n\"1987-10-19\",11.3\\r\\n\"1987-10-20\",10.4\\r\\n\"1987-10-21\",10.9\\r\\n\"1987-10-22\",9.2\\r\\n\"1987-10-23\",10.5\\r\\n\"1987-10-24\",11.4\\r\\n\"1987-10-25\",13.5\\r\\n\"1987-10-26\",9.8\\r\\n\"1987-10-27\",13.1\\r\\n\"1987-10-28\",9.7\\r\\n\"1987-10-29\",11.4\\r\\n\"1987-10-30\",9.9\\r\\n\"1987-10-31\",14.4\\r\\n\"1987-11-01\",19.0\\r\\n\"1987-11-02\",23.0\\r\\n\"1987-11-03\",15.4\\r\\n\"1987-11-04\",9.6\\r\\n\"1987-11-05\",10.8\\r\\n\"1987-11-06\",12.1\\r\\n\"1987-11-07\",11.0\\r\\n\"1987-11-08\",12.6\\r\\n\"1987-11-09\",14.7\\r\\n\"1987-11-10\",11.1\\r\\n\"1987-11-11\",10.1\\r\\n\"1987-11-12\",11.4\\r\\n\"1987-11-13\",13.0\\r\\n\"1987-11-14\",11.9\\r\\n\"1987-11-15\",9.5\\r\\n\"1987-11-16\",13.5\\r\\n\"1987-11-17\",15.2\\r\\n\"1987-11-18\",18.4\\r\\n\"1987-11-19\",24.1\\r\\n\"1987-11-20\",14.1\\r\\n\"1987-11-21\",10.7\\r\\n\"1987-11-22\",8.7\\r\\n\"1987-11-23\",13.3\\r\\n\"1987-11-24\",11.6\\r\\n\"1987-11-25\",9.9\\r\\n\"1987-11-26\",10.8\\r\\n\"1987-11-27\",11.5\\r\\n\"1987-11-28\",10.0\\r\\n\"1987-11-29\",13.9\\r\\n\"1987-11-30\",13.6\\r\\n\"1987-12-01\",11.9\\r\\n\"1987-12-02\",11.1\\r\\n\"1987-12-03\",8.2\\r\\n\"1987-12-04\",9.4\\r\\n\"1987-12-05\",12.7\\r\\n\"1987-12-06\",11.6\\r\\n\"1987-12-07\",11.0\\r\\n\"1987-12-08\",11.3\\r\\n\"1987-12-09\",13.4\\r\\n\"1987-12-10\",14.9\\r\\n\"1987-12-11\",15.2\\r\\n\"1987-12-12\",13.9\\r\\n\"1987-12-13\",15.0\\r\\n\"1987-12-14\",16.2\\r\\n\"1987-12-15\",17.7\\r\\n\"1987-12-16\",20.5\\r\\n\"1987-12-17\",14.7\\r\\n\"1987-12-18\",12.5\\r\\n\"1987-12-19\",10.9\\r\\n\"1987-12-20\",12.8\\r\\n\"1987-12-21\",12.7\\r\\n\"1987-12-22\",11.2\\r\\n\"1987-12-23\",11.4\\r\\n\"1987-12-24\",11.2\\r\\n\"1987-12-25\",12.1\\r\\n\"1987-12-26\",12.7\\r\\n\"1987-12-27\",16.2\\r\\n\"1987-12-28\",14.2\\r\\n\"1987-12-29\",14.3\\r\\n\"1987-12-30\",13.3\\r\\n\"1987-12-31\",16.7\\r\\n\"1988-01-01\",15.3\\r\\n\"1988-01-02\",14.3\\r\\n\"1988-01-03\",13.5\\r\\n\"1988-01-04\",15.0\\r\\n\"1988-01-05\",13.6\\r\\n\"1988-01-06\",15.2\\r\\n\"1988-01-07\",17.0\\r\\n\"1988-01-08\",18.7\\r\\n\"1988-01-09\",16.5\\r\\n\"1988-01-10\",17.4\\r\\n\"1988-01-11\",18.3\\r\\n\"1988-01-12\",18.3\\r\\n\"1988-01-13\",22.4\\r\\n\"1988-01-14\",21.4\\r\\n\"1988-01-15\",20.9\\r\\n\"1988-01-16\",17.6\\r\\n\"1988-01-17\",15.5\\r\\n\"1988-01-18\",16.6\\r\\n\"1988-01-19\",16.2\\r\\n\"1988-01-20\",15.6\\r\\n\"1988-01-21\",14.5\\r\\n\"1988-01-22\",14.0\\r\\n\"1988-01-23\",15.6\\r\\n\"1988-01-24\",12.3\\r\\n\"1988-01-25\",11.6\\r\\n\"1988-01-26\",12.6\\r\\n\"1988-01-27\",14.9\\r\\n\"1988-01-28\",17.3\\r\\n\"1988-01-29\",21.4\\r\\n\"1988-01-30\",23.4\\r\\n\"1988-01-31\",14.4\\r\\n\"1988-02-01\",14.1\\r\\n\"1988-02-02\",15.0\\r\\n\"1988-02-03\",14.5\\r\\n\"1988-02-04\",15.1\\r\\n\"1988-02-05\",13.9\\r\\n\"1988-02-06\",13.4\\r\\n\"1988-02-07\",9.2\\r\\n\"1988-02-08\",12.5\\r\\n\"1988-02-09\",15.1\\r\\n\"1988-02-10\",12.1\\r\\n\"1988-02-11\",14.5\\r\\n\"1988-02-12\",16.3\\r\\n\"1988-02-13\",16.5\\r\\n\"1988-02-14\",14.9\\r\\n\"1988-02-15\",13.2\\r\\n\"1988-02-16\",11.8\\r\\n\"1988-02-17\",13.6\\r\\n\"1988-02-18\",16.2\\r\\n\"1988-02-19\",14.1\\r\\n\"1988-02-20\",13.5\\r\\n\"1988-02-21\",15.0\\r\\n\"1988-02-22\",14.8\\r\\n\"1988-02-23\",16.2\\r\\n\"1988-02-24\",16.2\\r\\n\"1988-02-25\",13.3\\r\\n\"1988-02-26\",15.3\\r\\n\"1988-02-27\",18.4\\r\\n\"1988-02-28\",16.2\\r\\n\"1988-02-29\",16.3\\r\\n\"1988-03-01\",12.4\\r\\n\"1988-03-02\",15.6\\r\\n\"1988-03-03\",14.9\\r\\n\"1988-03-04\",14.8\\r\\n\"1988-03-05\",12.7\\r\\n\"1988-03-06\",14.2\\r\\n\"1988-03-07\",16.8\\r\\n\"1988-03-08\",16.7\\r\\n\"1988-03-09\",16.2\\r\\n\"1988-03-10\",14.5\\r\\n\"1988-03-11\",10.0\\r\\n\"1988-03-12\",12.6\\r\\n\"1988-03-13\",11.9\\r\\n\"1988-03-14\",11.8\\r\\n\"1988-03-15\",13.4\\r\\n\"1988-03-16\",14.5\\r\\n\"1988-03-17\",15.7\\r\\n\"1988-03-18\",15.3\\r\\n\"1988-03-19\",13.9\\r\\n\"1988-03-20\",13.7\\r\\n\"1988-03-21\",15.1\\r\\n\"1988-03-22\",15.6\\r\\n\"1988-03-23\",14.4\\r\\n\"1988-03-24\",13.9\\r\\n\"1988-03-25\",16.2\\r\\n\"1988-03-26\",16.7\\r\\n\"1988-03-27\",15.5\\r\\n\"1988-03-28\",16.4\\r\\n\"1988-03-29\",17.5\\r\\n\"1988-03-30\",18.2\\r\\n\"1988-03-31\",16.1\\r\\n\"1988-04-01\",16.5\\r\\n\"1988-04-02\",14.6\\r\\n\"1988-04-03\",16.4\\r\\n\"1988-04-04\",13.6\\r\\n\"1988-04-05\",15.9\\r\\n\"1988-04-06\",11.9\\r\\n\"1988-04-07\",14.7\\r\\n\"1988-04-08\",9.4\\r\\n\"1988-04-09\",6.6\\r\\n\"1988-04-10\",7.9\\r\\n\"1988-04-11\",11.0\\r\\n\"1988-04-12\",15.7\\r\\n\"1988-04-13\",15.2\\r\\n\"1988-04-14\",15.9\\r\\n\"1988-04-15\",10.6\\r\\n\"1988-04-16\",8.3\\r\\n\"1988-04-17\",8.6\\r\\n\"1988-04-18\",12.7\\r\\n\"1988-04-19\",10.5\\r\\n\"1988-04-20\",12.0\\r\\n\"1988-04-21\",11.1\\r\\n\"1988-04-22\",13.0\\r\\n\"1988-04-23\",12.4\\r\\n\"1988-04-24\",13.3\\r\\n\"1988-04-25\",15.9\\r\\n\"1988-04-26\",12.0\\r\\n\"1988-04-27\",13.7\\r\\n\"1988-04-28\",17.6\\r\\n\"1988-04-29\",14.3\\r\\n\"1988-04-30\",13.7\\r\\n\"1988-05-01\",15.2\\r\\n\"1988-05-02\",14.5\\r\\n\"1988-05-03\",14.9\\r\\n\"1988-05-04\",15.5\\r\\n\"1988-05-05\",16.4\\r\\n\"1988-05-06\",14.5\\r\\n\"1988-05-07\",12.6\\r\\n\"1988-05-08\",13.6\\r\\n\"1988-05-09\",11.2\\r\\n\"1988-05-10\",11.0\\r\\n\"1988-05-11\",12.0\\r\\n\"1988-05-12\",6.8\\r\\n\"1988-05-13\",10.6\\r\\n\"1988-05-14\",13.1\\r\\n\"1988-05-15\",13.5\\r\\n\"1988-05-16\",11.7\\r\\n\"1988-05-17\",13.2\\r\\n\"1988-05-18\",12.0\\r\\n\"1988-05-19\",10.4\\r\\n\"1988-05-20\",10.0\\r\\n\"1988-05-21\",8.2\\r\\n\"1988-05-22\",9.4\\r\\n\"1988-05-23\",10.3\\r\\n\"1988-05-24\",8.1\\r\\n\"1988-05-25\",8.7\\r\\n\"1988-05-26\",12.6\\r\\n\"1988-05-27\",10.9\\r\\n\"1988-05-28\",8.7\\r\\n\"1988-05-29\",9.3\\r\\n\"1988-05-30\",6.3\\r\\n\"1988-05-31\",7.8\\r\\n\"1988-06-01\",10.0\\r\\n\"1988-06-02\",11.0\\r\\n\"1988-06-03\",11.1\\r\\n\"1988-06-04\",12.6\\r\\n\"1988-06-05\",10.2\\r\\n\"1988-06-06\",11.1\\r\\n\"1988-06-07\",8.7\\r\\n\"1988-06-08\",9.5\\r\\n\"1988-06-09\",9.7\\r\\n\"1988-06-10\",8.2\\r\\n\"1988-06-11\",5.0\\r\\n\"1988-06-12\",6.5\\r\\n\"1988-06-13\",12.1\\r\\n\"1988-06-14\",8.9\\r\\n\"1988-06-15\",6.1\\r\\n\"1988-06-16\",2.8\\r\\n\"1988-06-17\",3.7\\r\\n\"1988-06-18\",6.8\\r\\n\"1988-06-19\",6.6\\r\\n\"1988-06-20\",7.0\\r\\n\"1988-06-21\",7.3\\r\\n\"1988-06-22\",7.9\\r\\n\"1988-06-23\",10.6\\r\\n\"1988-06-24\",8.1\\r\\n\"1988-06-25\",6.7\\r\\n\"1988-06-26\",8.0\\r\\n\"1988-06-27\",10.0\\r\\n\"1988-06-28\",6.7\\r\\n\"1988-06-29\",9.4\\r\\n\"1988-06-30\",9.3\\r\\n\"1988-07-01\",6.0\\r\\n\"1988-07-02\",5.8\\r\\n\"1988-07-03\",4.9\\r\\n\"1988-07-04\",5.0\\r\\n\"1988-07-05\",8.4\\r\\n\"1988-07-06\",12.3\\r\\n\"1988-07-07\",13.0\\r\\n\"1988-07-08\",11.4\\r\\n\"1988-07-09\",6.8\\r\\n\"1988-07-10\",7.6\\r\\n\"1988-07-11\",12.4\\r\\n\"1988-07-12\",7.1\\r\\n\"1988-07-13\",7.5\\r\\n\"1988-07-14\",10.0\\r\\n\"1988-07-15\",5.3\\r\\n\"1988-07-16\",6.3\\r\\n\"1988-07-17\",8.0\\r\\n\"1988-07-18\",8.3\\r\\n\"1988-07-19\",9.3\\r\\n\"1988-07-20\",9.5\\r\\n\"1988-07-21\",5.6\\r\\n\"1988-07-22\",7.0\\r\\n\"1988-07-23\",8.5\\r\\n\"1988-07-24\",8.5\\r\\n\"1988-07-25\",8.2\\r\\n\"1988-07-26\",8.5\\r\\n\"1988-07-27\",9.6\\r\\n\"1988-07-28\",9.7\\r\\n\"1988-07-29\",7.1\\r\\n\"1988-07-30\",8.4\\r\\n\"1988-07-31\",9.2\\r\\n\"1988-08-01\",9.8\\r\\n\"1988-08-02\",8.1\\r\\n\"1988-08-03\",9.4\\r\\n\"1988-08-04\",10.0\\r\\n\"1988-08-05\",5.1\\r\\n\"1988-08-06\",6.7\\r\\n\"1988-08-07\",6.9\\r\\n\"1988-08-08\",6.8\\r\\n\"1988-08-09\",8.6\\r\\n\"1988-08-10\",9.1\\r\\n\"1988-08-11\",3.9\\r\\n\"1988-08-12\",4.8\\r\\n\"1988-08-13\",8.4\\r\\n\"1988-08-14\",11.6\\r\\n\"1988-08-15\",12.1\\r\\n\"1988-08-16\",12.4\\r\\n\"1988-08-17\",10.0\\r\\n\"1988-08-18\",10.1\\r\\n\"1988-08-19\",9.7\\r\\n\"1988-08-20\",11.7\\r\\n\"1988-08-21\",7.9\\r\\n\"1988-08-22\",8.6\\r\\n\"1988-08-23\",7.7\\r\\n\"1988-08-24\",5.8\\r\\n\"1988-08-25\",8.7\\r\\n\"1988-08-26\",10.6\\r\\n\"1988-08-27\",6.7\\r\\n\"1988-08-28\",8.8\\r\\n\"1988-08-29\",9.7\\r\\n\"1988-08-30\",9.0\\r\\n\"1988-08-31\",11.8\\r\\n\"1988-09-01\",15.2\\r\\n\"1988-09-02\",10.0\\r\\n\"1988-09-03\",10.5\\r\\n\"1988-09-04\",5.5\\r\\n\"1988-09-05\",9.4\\r\\n\"1988-09-06\",8.8\\r\\n\"1988-09-07\",5.3\\r\\n\"1988-09-08\",13.0\\r\\n\"1988-09-09\",15.2\\r\\n\"1988-09-10\",13.2\\r\\n\"1988-09-11\",11.5\\r\\n\"1988-09-12\",6.8\\r\\n\"1988-09-13\",4.7\\r\\n\"1988-09-14\",5.2\\r\\n\"1988-09-15\",6.8\\r\\n\"1988-09-16\",10.7\\r\\n\"1988-09-17\",10.1\\r\\n\"1988-09-18\",10.0\\r\\n\"1988-09-19\",9.8\\r\\n\"1988-09-20\",5.5\\r\\n\"1988-09-21\",13.5\\r\\n\"1988-09-22\",16.6\\r\\n\"1988-09-23\",8.4\\r\\n\"1988-09-24\",8.2\\r\\n\"1988-09-25\",11.1\\r\\n\"1988-09-26\",10.8\\r\\n\"1988-09-27\",8.8\\r\\n\"1988-09-28\",10.8\\r\\n\"1988-09-29\",8.7\\r\\n\"1988-09-30\",12.4\\r\\n\"1988-10-01\",9.0\\r\\n\"1988-10-02\",13.5\\r\\n\"1988-10-03\",14.7\\r\\n\"1988-10-04\",10.9\\r\\n\"1988-10-05\",8.5\\r\\n\"1988-10-06\",6.0\\r\\n\"1988-10-07\",12.7\\r\\n\"1988-10-08\",11.1\\r\\n\"1988-10-09\",8.7\\r\\n\"1988-10-10\",12.3\\r\\n\"1988-10-11\",13.3\\r\\n\"1988-10-12\",5.6\\r\\n\"1988-10-13\",13.7\\r\\n\"1988-10-14\",8.5\\r\\n\"1988-10-15\",11.2\\r\\n\"1988-10-16\",8.7\\r\\n\"1988-10-17\",11.7\\r\\n\"1988-10-18\",12.5\\r\\n\"1988-10-19\",8.2\\r\\n\"1988-10-20\",15.6\\r\\n\"1988-10-21\",10.3\\r\\n\"1988-10-22\",11.4\\r\\n\"1988-10-23\",9.7\\r\\n\"1988-10-24\",6.3\\r\\n\"1988-10-25\",14.3\\r\\n\"1988-10-26\",11.3\\r\\n\"1988-10-27\",7.3\\r\\n\"1988-10-28\",12.8\\r\\n\"1988-10-29\",11.9\\r\\n\"1988-10-30\",14.3\\r\\n\"1988-10-31\",11.6\\r\\n\"1988-11-01\",13.2\\r\\n\"1988-11-02\",15.5\\r\\n\"1988-11-03\",14.1\\r\\n\"1988-11-04\",9.5\\r\\n\"1988-11-05\",7.2\\r\\n\"1988-11-06\",11.8\\r\\n\"1988-11-07\",16.8\\r\\n\"1988-11-08\",12.5\\r\\n\"1988-11-09\",9.4\\r\\n\"1988-11-10\",11.9\\r\\n\"1988-11-11\",10.3\\r\\n\"1988-11-12\",16.9\\r\\n\"1988-11-13\",17.5\\r\\n\"1988-11-14\",7.5\\r\\n\"1988-11-15\",8.6\\r\\n\"1988-11-16\",11.1\\r\\n\"1988-11-17\",11.5\\r\\n\"1988-11-18\",10.7\\r\\n\"1988-11-19\",15.7\\r\\n\"1988-11-20\",12.8\\r\\n\"1988-11-21\",13.0\\r\\n\"1988-11-22\",12.9\\r\\n\"1988-11-23\",14.3\\r\\n\"1988-11-24\",13.7\\r\\n\"1988-11-25\",12.1\\r\\n\"1988-11-26\",11.9\\r\\n\"1988-11-27\",11.8\\r\\n\"1988-11-28\",11.4\\r\\n\"1988-11-29\",10.3\\r\\n\"1988-11-30\",11.7\\r\\n\"1988-12-01\",12.0\\r\\n\"1988-12-02\",17.4\\r\\n\"1988-12-03\",16.8\\r\\n\"1988-12-04\",16.2\\r\\n\"1988-12-05\",13.0\\r\\n\"1988-12-06\",12.5\\r\\n\"1988-12-07\",12.4\\r\\n\"1988-12-08\",16.1\\r\\n\"1988-12-09\",20.2\\r\\n\"1988-12-10\",14.3\\r\\n\"1988-12-11\",11.0\\r\\n\"1988-12-12\",14.4\\r\\n\"1988-12-13\",15.7\\r\\n\"1988-12-14\",19.7\\r\\n\"1988-12-15\",20.7\\r\\n\"1988-12-16\",23.9\\r\\n\"1988-12-17\",16.6\\r\\n\"1988-12-18\",17.5\\r\\n\"1988-12-19\",14.9\\r\\n\"1988-12-20\",13.6\\r\\n\"1988-12-21\",11.9\\r\\n\"1988-12-22\",15.2\\r\\n\"1988-12-23\",17.3\\r\\n\"1988-12-24\",19.8\\r\\n\"1988-12-25\",15.8\\r\\n\"1988-12-26\",9.5\\r\\n\"1988-12-27\",12.9\\r\\n\"1988-12-28\",12.9\\r\\n\"1988-12-29\",14.8\\r\\n\"1988-12-30\",14.1\\r\\n\"1989-01-01\",14.3\\r\\n\"1989-01-02\",17.4\\r\\n\"1989-01-03\",18.5\\r\\n\"1989-01-04\",16.8\\r\\n\"1989-01-05\",11.5\\r\\n\"1989-01-06\",9.5\\r\\n\"1989-01-07\",12.2\\r\\n\"1989-01-08\",15.7\\r\\n\"1989-01-09\",16.3\\r\\n\"1989-01-10\",13.6\\r\\n\"1989-01-11\",12.6\\r\\n\"1989-01-12\",13.8\\r\\n\"1989-01-13\",12.1\\r\\n\"1989-01-14\",13.4\\r\\n\"1989-01-15\",17.3\\r\\n\"1989-01-16\",19.4\\r\\n\"1989-01-17\",16.6\\r\\n\"1989-01-18\",13.9\\r\\n\"1989-01-19\",13.1\\r\\n\"1989-01-20\",16.0\\r\\n\"1989-01-21\",14.5\\r\\n\"1989-01-22\",15.0\\r\\n\"1989-01-23\",12.6\\r\\n\"1989-01-24\",12.5\\r\\n\"1989-01-25\",15.2\\r\\n\"1989-01-26\",16.2\\r\\n\"1989-01-27\",16.5\\r\\n\"1989-01-28\",20.1\\r\\n\"1989-01-29\",20.6\\r\\n\"1989-01-30\",16.9\\r\\n\"1989-01-31\",16.5\\r\\n\"1989-02-01\",16.1\\r\\n\"1989-02-02\",14.4\\r\\n\"1989-02-03\",16.3\\r\\n\"1989-02-04\",15.7\\r\\n\"1989-02-05\",14.2\\r\\n\"1989-02-06\",13.2\\r\\n\"1989-02-07\",16.8\\r\\n\"1989-02-08\",18.5\\r\\n\"1989-02-09\",16.7\\r\\n\"1989-02-10\",15.3\\r\\n\"1989-02-11\",15.9\\r\\n\"1989-02-12\",15.2\\r\\n\"1989-02-13\",17.5\\r\\n\"1989-02-14\",18.3\\r\\n\"1989-02-15\",19.4\\r\\n\"1989-02-16\",19.4\\r\\n\"1989-02-17\",19.5\\r\\n\"1989-02-18\",20.5\\r\\n\"1989-02-19\",15.7\\r\\n\"1989-02-20\",15.0\\r\\n\"1989-02-21\",16.1\\r\\n\"1989-02-22\",14.3\\r\\n\"1989-02-23\",13.0\\r\\n\"1989-02-24\",16.2\\r\\n\"1989-02-25\",17.7\\r\\n\"1989-02-26\",13.2\\r\\n\"1989-02-27\",15.8\\r\\n\"1989-02-28\",18.5\\r\\n\"1989-03-01\",20.4\\r\\n\"1989-03-02\",22.0\\r\\n\"1989-03-03\",19.7\\r\\n\"1989-03-04\",19.6\\r\\n\"1989-03-05\",20.3\\r\\n\"1989-03-06\",18.3\\r\\n\"1989-03-07\",18.9\\r\\n\"1989-03-08\",20.3\\r\\n\"1989-03-09\",21.4\\r\\n\"1989-03-10\",18.3\\r\\n\"1989-03-11\",17.8\\r\\n\"1989-03-12\",17.7\\r\\n\"1989-03-13\",12.8\\r\\n\"1989-03-14\",15.1\\r\\n\"1989-03-15\",15.0\\r\\n\"1989-03-16\",14.8\\r\\n\"1989-03-17\",12.0\\r\\n\"1989-03-18\",12.5\\r\\n\"1989-03-19\",15.0\\r\\n\"1989-03-20\",17.1\\r\\n\"1989-03-21\",17.3\\r\\n\"1989-03-22\",16.9\\r\\n\"1989-03-23\",16.5\\r\\n\"1989-03-24\",13.6\\r\\n\"1989-03-25\",13.2\\r\\n\"1989-03-26\",9.4\\r\\n\"1989-03-27\",9.5\\r\\n\"1989-03-28\",11.8\\r\\n\"1989-03-29\",10.4\\r\\n\"1989-03-30\",9.7\\r\\n\"1989-03-31\",12.6\\r\\n\"1989-04-01\",13.3\\r\\n\"1989-04-02\",15.1\\r\\n\"1989-04-03\",14.2\\r\\n\"1989-04-04\",14.2\\r\\n\"1989-04-05\",19.2\\r\\n\"1989-04-06\",12.6\\r\\n\"1989-04-07\",14.2\\r\\n\"1989-04-08\",11.9\\r\\n\"1989-04-09\",13.9\\r\\n\"1989-04-10\",13.5\\r\\n\"1989-04-11\",15.3\\r\\n\"1989-04-12\",13.9\\r\\n\"1989-04-13\",14.0\\r\\n\"1989-04-14\",12.9\\r\\n\"1989-04-15\",8.5\\r\\n\"1989-04-16\",11.4\\r\\n\"1989-04-17\",10.9\\r\\n\"1989-04-18\",12.0\\r\\n\"1989-04-19\",8.6\\r\\n\"1989-04-20\",9.0\\r\\n\"1989-04-21\",9.6\\r\\n\"1989-04-22\",10.2\\r\\n\"1989-04-23\",9.8\\r\\n\"1989-04-24\",8.3\\r\\n\"1989-04-25\",11.0\\r\\n\"1989-04-26\",11.9\\r\\n\"1989-04-27\",14.0\\r\\n\"1989-04-28\",15.8\\r\\n\"1989-04-29\",14.5\\r\\n\"1989-04-30\",13.2\\r\\n\"1989-05-01\",14.2\\r\\n\"1989-05-02\",14.6\\r\\n\"1989-05-03\",11.8\\r\\n\"1989-05-04\",14.4\\r\\n\"1989-05-05\",10.4\\r\\n\"1989-05-06\",10.3\\r\\n\"1989-05-07\",10.8\\r\\n\"1989-05-08\",10.5\\r\\n\"1989-05-09\",9.5\\r\\n\"1989-05-10\",12.5\\r\\n\"1989-05-11\",13.7\\r\\n\"1989-05-12\",12.7\\r\\n\"1989-05-13\",11.9\\r\\n\"1989-05-14\",11.4\\r\\n\"1989-05-15\",9.7\\r\\n\"1989-05-16\",8.3\\r\\n\"1989-05-17\",8.1\\r\\n\"1989-05-18\",11.7\\r\\n\"1989-05-19\",11.6\\r\\n\"1989-05-20\",7.4\\r\\n\"1989-05-21\",5.2\\r\\n\"1989-05-22\",11.0\\r\\n\"1989-05-23\",9.5\\r\\n\"1989-05-24\",9.2\\r\\n\"1989-05-25\",10.7\\r\\n\"1989-05-26\",9.0\\r\\n\"1989-05-27\",10.2\\r\\n\"1989-05-28\",10.3\\r\\n\"1989-05-29\",12.1\\r\\n\"1989-05-30\",13.2\\r\\n\"1989-05-31\",6.6\\r\\n\"1989-06-01\",2.3\\r\\n\"1989-06-02\",1.4\\r\\n\"1989-06-03\",2.1\\r\\n\"1989-06-04\",6.6\\r\\n\"1989-06-05\",8.9\\r\\n\"1989-06-06\",7.8\\r\\n\"1989-06-07\",9.0\\r\\n\"1989-06-08\",10.3\\r\\n\"1989-06-09\",7.9\\r\\n\"1989-06-10\",7.2\\r\\n\"1989-06-11\",8.6\\r\\n\"1989-06-12\",8.8\\r\\n\"1989-06-13\",6.2\\r\\n\"1989-06-14\",9.5\\r\\n\"1989-06-15\",10.2\\r\\n\"1989-06-16\",9.7\\r\\n\"1989-06-17\",11.2\\r\\n\"1989-06-18\",10.2\\r\\n\"1989-06-19\",10.1\\r\\n\"1989-06-20\",8.1\\r\\n\"1989-06-21\",6.6\\r\\n\"1989-06-22\",5.0\\r\\n\"1989-06-23\",4.7\\r\\n\"1989-06-24\",5.3\\r\\n\"1989-06-25\",4.5\\r\\n\"1989-06-26\",2.3\\r\\n\"1989-06-27\",1.4\\r\\n\"1989-06-28\",0.5\\r\\n\"1989-06-29\",2.4\\r\\n\"1989-06-30\",8.0\\r\\n\"1989-07-01\",6.0\\r\\n\"1989-07-02\",7.1\\r\\n\"1989-07-03\",9.7\\r\\n\"1989-07-04\",6.9\\r\\n\"1989-07-05\",5.3\\r\\n\"1989-07-06\",7.0\\r\\n\"1989-07-07\",6.2\\r\\n\"1989-07-08\",7.0\\r\\n\"1989-07-09\",9.7\\r\\n\"1989-07-10\",8.0\\r\\n\"1989-07-11\",8.5\\r\\n\"1989-07-12\",7.1\\r\\n\"1989-07-13\",7.5\\r\\n\"1989-07-14\",3.3\\r\\n\"1989-07-15\",1.8\\r\\n\"1989-07-16\",2.6\\r\\n\"1989-07-17\",5.3\\r\\n\"1989-07-18\",5.8\\r\\n\"1989-07-19\",5.8\\r\\n\"1989-07-20\",7.2\\r\\n\"1989-07-21\",5.3\\r\\n\"1989-07-22\",1.6\\r\\n\"1989-07-23\",3.1\\r\\n\"1989-07-24\",5.3\\r\\n\"1989-07-25\",7.7\\r\\n\"1989-07-26\",4.2\\r\\n\"1989-07-27\",5.5\\r\\n\"1989-07-28\",9.0\\r\\n\"1989-07-29\",11.2\\r\\n\"1989-07-30\",8.0\\r\\n\"1989-07-31\",7.6\\r\\n\"1989-08-01\",3.7\\r\\n\"1989-08-02\",7.5\\r\\n\"1989-08-03\",8.1\\r\\n\"1989-08-04\",8.4\\r\\n\"1989-08-05\",7.1\\r\\n\"1989-08-06\",7.6\\r\\n\"1989-08-07\",7.6\\r\\n\"1989-08-08\",5.6\\r\\n\"1989-08-09\",7.0\\r\\n\"1989-08-10\",10.5\\r\\n\"1989-08-11\",7.3\\r\\n\"1989-08-12\",7.8\\r\\n\"1989-08-13\",5.8\\r\\n\"1989-08-14\",3.8\\r\\n\"1989-08-15\",5.8\\r\\n\"1989-08-16\",6.7\\r\\n\"1989-08-17\",6.6\\r\\n\"1989-08-18\",6.6\\r\\n\"1989-08-19\",9.0\\r\\n\"1989-08-20\",8.1\\r\\n\"1989-08-21\",5.1\\r\\n\"1989-08-22\",8.6\\r\\n\"1989-08-23\",7.0\\r\\n\"1989-08-24\",5.5\\r\\n\"1989-08-25\",7.4\\r\\n\"1989-08-26\",6.2\\r\\n\"1989-08-27\",4.2\\r\\n\"1989-08-28\",6.3\\r\\n\"1989-08-29\",7.0\\r\\n\"1989-08-30\",4.0\\r\\n\"1989-08-31\",8.0\\r\\n\"1989-09-01\",8.8\\r\\n\"1989-09-02\",8.8\\r\\n\"1989-09-03\",6.1\\r\\n\"1989-09-04\",8.6\\r\\n\"1989-09-05\",8.9\\r\\n\"1989-09-06\",7.8\\r\\n\"1989-09-07\",5.0\\r\\n\"1989-09-08\",7.0\\r\\n\"1989-09-09\",13.3\\r\\n\"1989-09-10\",7.9\\r\\n\"1989-09-11\",7.5\\r\\n\"1989-09-12\",8.3\\r\\n\"1989-09-13\",7.2\\r\\n\"1989-09-14\",6.5\\r\\n\"1989-09-15\",8.9\\r\\n\"1989-09-16\",7.4\\r\\n\"1989-09-17\",9.9\\r\\n\"1989-09-18\",9.3\\r\\n\"1989-09-19\",10.6\\r\\n\"1989-09-20\",8.6\\r\\n\"1989-09-21\",7.2\\r\\n\"1989-09-22\",12.6\\r\\n\"1989-09-23\",7.8\\r\\n\"1989-09-24\",6.3\\r\\n\"1989-09-25\",9.2\\r\\n\"1989-09-26\",5.8\\r\\n\"1989-09-27\",9.0\\r\\n\"1989-09-28\",5.0\\r\\n\"1989-09-29\",11.9\\r\\n\"1989-09-30\",13.4\\r\\n\"1989-10-01\",10.5\\r\\n\"1989-10-02\",6.2\\r\\n\"1989-10-03\",5.1\\r\\n\"1989-10-04\",9.5\\r\\n\"1989-10-05\",11.7\\r\\n\"1989-10-06\",9.2\\r\\n\"1989-10-07\",7.3\\r\\n\"1989-10-08\",9.7\\r\\n\"1989-10-09\",9.4\\r\\n\"1989-10-10\",10.0\\r\\n\"1989-10-11\",10.9\\r\\n\"1989-10-12\",11.0\\r\\n\"1989-10-13\",10.9\\r\\n\"1989-10-14\",8.0\\r\\n\"1989-10-15\",11.2\\r\\n\"1989-10-16\",7.5\\r\\n\"1989-10-17\",7.2\\r\\n\"1989-10-18\",13.2\\r\\n\"1989-10-19\",12.9\\r\\n\"1989-10-20\",9.4\\r\\n\"1989-10-21\",10.2\\r\\n\"1989-10-22\",9.5\\r\\n\"1989-10-23\",12.4\\r\\n\"1989-10-24\",10.2\\r\\n\"1989-10-25\",13.4\\r\\n\"1989-10-26\",11.6\\r\\n\"1989-10-27\",8.0\\r\\n\"1989-10-28\",9.0\\r\\n\"1989-10-29\",9.3\\r\\n\"1989-10-30\",13.5\\r\\n\"1989-10-31\",8.0\\r\\n\"1989-11-01\",8.1\\r\\n\"1989-11-02\",10.0\\r\\n\"1989-11-03\",8.5\\r\\n\"1989-11-04\",12.5\\r\\n\"1989-11-05\",15.0\\r\\n\"1989-11-06\",13.3\\r\\n\"1989-11-07\",11.0\\r\\n\"1989-11-08\",11.9\\r\\n\"1989-11-09\",8.3\\r\\n\"1989-11-10\",9.7\\r\\n\"1989-11-11\",11.3\\r\\n\"1989-11-12\",12.5\\r\\n\"1989-11-13\",9.4\\r\\n\"1989-11-14\",11.4\\r\\n\"1989-11-15\",13.2\\r\\n\"1989-11-16\",13.8\\r\\n\"1989-11-17\",16.0\\r\\n\"1989-11-18\",10.9\\r\\n\"1989-11-19\",11.9\\r\\n\"1989-11-20\",12.4\\r\\n\"1989-11-21\",13.2\\r\\n\"1989-11-22\",15.5\\r\\n\"1989-11-23\",21.6\\r\\n\"1989-11-24\",14.9\\r\\n\"1989-11-25\",14.4\\r\\n\"1989-11-26\",12.9\\r\\n\"1989-11-27\",13.1\\r\\n\"1989-11-28\",14.0\\r\\n\"1989-11-29\",17.9\\r\\n\"1989-11-30\",17.7\\r\\n\"1989-12-01\",16.3\\r\\n\"1989-12-02\",18.3\\r\\n\"1989-12-03\",13.7\\r\\n\"1989-12-04\",13.3\\r\\n\"1989-12-05\",10.6\\r\\n\"1989-12-06\",14.1\\r\\n\"1989-12-07\",16.0\\r\\n\"1989-12-08\",16.5\\r\\n\"1989-12-09\",14.1\\r\\n\"1989-12-10\",18.7\\r\\n\"1989-12-11\",16.2\\r\\n\"1989-12-12\",14.8\\r\\n\"1989-12-13\",12.6\\r\\n\"1989-12-14\",10.4\\r\\n\"1989-12-15\",12.2\\r\\n\"1989-12-16\",12.6\\r\\n\"1989-12-17\",12.1\\r\\n\"1989-12-18\",17.3\\r\\n\"1989-12-19\",16.4\\r\\n\"1989-12-20\",12.6\\r\\n\"1989-12-21\",12.3\\r\\n\"1989-12-22\",11.8\\r\\n\"1989-12-23\",12.0\\r\\n\"1989-12-24\",12.7\\r\\n\"1989-12-25\",16.4\\r\\n\"1989-12-26\",16.0\\r\\n\"1989-12-27\",13.3\\r\\n\"1989-12-28\",11.7\\r\\n\"1989-12-29\",10.4\\r\\n\"1989-12-30\",14.4\\r\\n\"1989-12-31\",12.7\\r\\n\"1990-01-01\",14.8\\r\\n\"1990-01-02\",13.3\\r\\n\"1990-01-03\",15.6\\r\\n\"1990-01-04\",14.5\\r\\n\"1990-01-05\",14.3\\r\\n\"1990-01-06\",15.3\\r\\n\"1990-01-07\",16.4\\r\\n\"1990-01-08\",14.8\\r\\n\"1990-01-09\",17.4\\r\\n\"1990-01-10\",18.8\\r\\n\"1990-01-11\",22.1\\r\\n\"1990-01-12\",19.0\\r\\n\"1990-01-13\",15.5\\r\\n\"1990-01-14\",15.8\\r\\n\"1990-01-15\",14.7\\r\\n\"1990-01-16\",10.7\\r\\n\"1990-01-17\",11.5\\r\\n\"1990-01-18\",15.0\\r\\n\"1990-01-19\",14.5\\r\\n\"1990-01-20\",14.5\\r\\n\"1990-01-21\",13.3\\r\\n\"1990-01-22\",14.3\\r\\n\"1990-01-23\",14.3\\r\\n\"1990-01-24\",20.5\\r\\n\"1990-01-25\",15.0\\r\\n\"1990-01-26\",17.1\\r\\n\"1990-01-27\",16.9\\r\\n\"1990-01-28\",16.9\\r\\n\"1990-01-29\",13.6\\r\\n\"1990-01-30\",16.4\\r\\n\"1990-01-31\",16.1\\r\\n\"1990-02-01\",12.0\\r\\n\"1990-02-02\",12.2\\r\\n\"1990-02-03\",14.8\\r\\n\"1990-02-04\",14.8\\r\\n\"1990-02-05\",14.4\\r\\n\"1990-02-06\",12.9\\r\\n\"1990-02-07\",13.4\\r\\n\"1990-02-08\",15.9\\r\\n\"1990-02-09\",16.1\\r\\n\"1990-02-10\",17.6\\r\\n\"1990-02-11\",15.6\\r\\n\"1990-02-12\",15.0\\r\\n\"1990-02-13\",13.0\\r\\n\"1990-02-14\",14.1\\r\\n\"1990-02-15\",17.3\\r\\n\"1990-02-16\",15.7\\r\\n\"1990-02-17\",18.6\\r\\n\"1990-02-18\",12.7\\r\\n\"1990-02-19\",14.0\\r\\n\"1990-02-20\",13.7\\r\\n\"1990-02-21\",16.3\\r\\n\"1990-02-22\",20.0\\r\\n\"1990-02-23\",17.0\\r\\n\"1990-02-24\",15.2\\r\\n\"1990-02-25\",16.5\\r\\n\"1990-02-26\",16.5\\r\\n\"1990-02-27\",17.3\\r\\n\"1990-02-28\",19.1\\r\\n\"1990-03-01\",19.3\\r\\n\"1990-03-02\",17.3\\r\\n\"1990-03-03\",19.0\\r\\n\"1990-03-04\",19.8\\r\\n\"1990-03-05\",19.3\\r\\n\"1990-03-06\",17.2\\r\\n\"1990-03-07\",14.2\\r\\n\"1990-03-08\",10.3\\r\\n\"1990-03-09\",13.0\\r\\n\"1990-03-10\",15.3\\r\\n\"1990-03-11\",15.0\\r\\n\"1990-03-12\",12.1\\r\\n\"1990-03-13\",9.2\\r\\n\"1990-03-14\",11.0\\r\\n\"1990-03-15\",15.0\\r\\n\"1990-03-16\",11.6\\r\\n\"1990-03-17\",11.6\\r\\n\"1990-03-18\",15.1\\r\\n\"1990-03-19\",15.0\\r\\n\"1990-03-20\",13.6\\r\\n\"1990-03-21\",12.5\\r\\n\"1990-03-22\",14.3\\r\\n\"1990-03-23\",16.0\\r\\n\"1990-03-24\",17.4\\r\\n\"1990-03-25\",16.9\\r\\n\"1990-03-26\",18.0\\r\\n\"1990-03-27\",20.6\\r\\n\"1990-03-28\",14.2\\r\\n\"1990-03-29\",10.9\\r\\n\"1990-03-30\",11.9\\r\\n\"1990-03-31\",13.3\\r\\n\"1990-04-01\",15.3\\r\\n\"1990-04-02\",14.7\\r\\n\"1990-04-03\",11.0\\r\\n\"1990-04-04\",12.2\\r\\n\"1990-04-05\",14.2\\r\\n\"1990-04-06\",17.0\\r\\n\"1990-04-07\",15.8\\r\\n\"1990-04-08\",15.2\\r\\n\"1990-04-09\",15.1\\r\\n\"1990-04-10\",14.7\\r\\n\"1990-04-11\",18.5\\r\\n\"1990-04-12\",16.4\\r\\n\"1990-04-13\",18.4\\r\\n\"1990-04-14\",15.1\\r\\n\"1990-04-15\",9.9\\r\\n\"1990-04-16\",10.2\\r\\n\"1990-04-17\",12.6\\r\\n\"1990-04-18\",13.2\\r\\n\"1990-04-19\",11.5\\r\\n\"1990-04-20\",13.8\\r\\n\"1990-04-21\",14.5\\r\\n\"1990-04-22\",14.7\\r\\n\"1990-04-23\",11.2\\r\\n\"1990-04-24\",12.7\\r\\n\"1990-04-25\",13.7\\r\\n\"1990-04-26\",11.5\\r\\n\"1990-04-27\",10.4\\r\\n\"1990-04-28\",8.9\\r\\n\"1990-04-29\",11.1\\r\\n\"1990-04-30\",9.5\\r\\n\"1990-05-01\",13.0\\r\\n\"1990-05-02\",13.9\\r\\n\"1990-05-03\",12.6\\r\\n\"1990-05-04\",14.3\\r\\n\"1990-05-05\",16.0\\r\\n\"1990-05-06\",13.3\\r\\n\"1990-05-07\",7.0\\r\\n\"1990-05-08\",4.9\\r\\n\"1990-05-09\",6.9\\r\\n\"1990-05-10\",13.7\\r\\n\"1990-05-11\",10.6\\r\\n\"1990-05-12\",12.3\\r\\n\"1990-05-13\",11.1\\r\\n\"1990-05-14\",10.2\\r\\n\"1990-05-15\",9.5\\r\\n\"1990-05-16\",8.9\\r\\n\"1990-05-17\",13.4\\r\\n\"1990-05-18\",9.1\\r\\n\"1990-05-19\",9.4\\r\\n\"1990-05-20\",8.7\\r\\n\"1990-05-21\",5.8\\r\\n\"1990-05-22\",4.5\\r\\n\"1990-05-23\",7.2\\r\\n\"1990-05-24\",10.0\\r\\n\"1990-05-25\",10.5\\r\\n\"1990-05-26\",10.7\\r\\n\"1990-05-27\",8.2\\r\\n\"1990-05-28\",6.1\\r\\n\"1990-05-29\",4.5\\r\\n\"1990-05-30\",6.1\\r\\n\"1990-05-31\",9.8\\r\\n\"1990-06-01\",9.7\\r\\n\"1990-06-02\",8.2\\r\\n\"1990-06-03\",8.4\\r\\n\"1990-06-04\",8.5\\r\\n\"1990-06-05\",10.4\\r\\n\"1990-06-06\",6.8\\r\\n\"1990-06-07\",6.0\\r\\n\"1990-06-08\",6.6\\r\\n\"1990-06-09\",7.8\\r\\n\"1990-06-10\",10.3\\r\\n\"1990-06-11\",7.2\\r\\n\"1990-06-12\",7.4\\r\\n\"1990-06-13\",11.4\\r\\n\"1990-06-14\",5.4\\r\\n\"1990-06-15\",4.4\\r\\n\"1990-06-16\",6.4\\r\\n\"1990-06-17\",9.3\\r\\n\"1990-06-18\",7.7\\r\\n\"1990-06-19\",8.1\\r\\n\"1990-06-20\",8.3\\r\\n\"1990-06-21\",9.1\\r\\n\"1990-06-22\",7.7\\r\\n\"1990-06-23\",10.6\\r\\n\"1990-06-24\",8.2\\r\\n\"1990-06-25\",7.9\\r\\n\"1990-06-26\",5.2\\r\\n\"1990-06-27\",5.9\\r\\n\"1990-06-28\",3.7\\r\\n\"1990-06-29\",5.6\\r\\n\"1990-06-30\",9.4\\r\\n\"1990-07-01\",7.4\\r\\n\"1990-07-02\",7.3\\r\\n\"1990-07-03\",7.7\\r\\n\"1990-07-04\",7.7\\r\\n\"1990-07-05\",9.3\\r\\n\"1990-07-06\",4.4\\r\\n\"1990-07-07\",5.7\\r\\n\"1990-07-08\",10.2\\r\\n\"1990-07-09\",10.2\\r\\n\"1990-07-10\",9.3\\r\\n\"1990-07-11\",5.4\\r\\n\"1990-07-12\",5.0\\r\\n\"1990-07-13\",7.6\\r\\n\"1990-07-14\",9.6\\r\\n\"1990-07-15\",10.4\\r\\n\"1990-07-16\",11.2\\r\\n\"1990-07-17\",9.1\\r\\n\"1990-07-18\",11.2\\r\\n\"1990-07-19\",6.8\\r\\n\"1990-07-20\",8.3\\r\\n\"1990-07-21\",9.7\\r\\n\"1990-07-22\",9.6\\r\\n\"1990-07-23\",9.8\\r\\n\"1990-07-24\",10.8\\r\\n\"1990-07-25\",9.2\\r\\n\"1990-07-26\",6.5\\r\\n\"1990-07-27\",8.1\\r\\n\"1990-07-28\",7.3\\r\\n\"1990-07-29\",7.9\\r\\n\"1990-07-30\",6.0\\r\\n\"1990-07-31\",5.0\\r\\n\"1990-08-01\",6.8\\r\\n\"1990-08-02\",9.8\\r\\n\"1990-08-03\",5.7\\r\\n\"1990-08-04\",8.6\\r\\n\"1990-08-05\",10.6\\r\\n\"1990-08-06\",7.8\\r\\n\"1990-08-07\",7.7\\r\\n\"1990-08-08\",8.6\\r\\n\"1990-08-09\",6.5\\r\\n\"1990-08-10\",6.9\\r\\n\"1990-08-11\",6.4\\r\\n\"1990-08-12\",8.5\\r\\n\"1990-08-13\",7.8\\r\\n\"1990-08-14\",9.3\\r\\n\"1990-08-15\",8.4\\r\\n\"1990-08-16\",7.8\\r\\n\"1990-08-17\",7.4\\r\\n\"1990-08-18\",7.7\\r\\n\"1990-08-19\",8.9\\r\\n\"1990-08-20\",9.7\\r\\n\"1990-08-21\",9.9\\r\\n\"1990-08-22\",6.1\\r\\n\"1990-08-23\",6.6\\r\\n\"1990-08-24\",7.6\\r\\n\"1990-08-25\",7.4\\r\\n\"1990-08-26\",8.0\\r\\n\"1990-08-27\",2.1\\r\\n\"1990-08-28\",5.9\\r\\n\"1990-08-29\",11.6\\r\\n\"1990-08-30\",8.6\\r\\n\"1990-08-31\",7.9\\r\\n\"1990-09-01\",6.0\\r\\n\"1990-09-02\",9.5\\r\\n\"1990-09-03\",8.6\\r\\n\"1990-09-04\",7.6\\r\\n\"1990-09-05\",10.4\\r\\n\"1990-09-06\",10.3\\r\\n\"1990-09-07\",7.5\\r\\n\"1990-09-08\",3.0\\r\\n\"1990-09-09\",5.3\\r\\n\"1990-09-10\",10.5\\r\\n\"1990-09-11\",14.6\\r\\n\"1990-09-12\",12.6\\r\\n\"1990-09-13\",9.8\\r\\n\"1990-09-14\",7.2\\r\\n\"1990-09-15\",10.1\\r\\n\"1990-09-16\",10.4\\r\\n\"1990-09-17\",3.7\\r\\n\"1990-09-18\",7.3\\r\\n\"1990-09-19\",11.6\\r\\n\"1990-09-20\",16.3\\r\\n\"1990-09-21\",9.6\\r\\n\"1990-09-22\",6.8\\r\\n\"1990-09-23\",5.2\\r\\n\"1990-09-24\",10.6\\r\\n\"1990-09-25\",16.3\\r\\n\"1990-09-26\",9.8\\r\\n\"1990-09-27\",4.6\\r\\n\"1990-09-28\",11.1\\r\\n\"1990-09-29\",8.7\\r\\n\"1990-09-30\",10.0\\r\\n\"1990-10-01\",11.3\\r\\n\"1990-10-02\",10.5\\r\\n\"1990-10-03\",9.9\\r\\n\"1990-10-04\",11.0\\r\\n\"1990-10-05\",14.0\\r\\n\"1990-10-06\",9.2\\r\\n\"1990-10-07\",9.8\\r\\n\"1990-10-08\",6.0\\r\\n\"1990-10-09\",9.8\\r\\n\"1990-10-10\",9.2\\r\\n\"1990-10-11\",11.8\\r\\n\"1990-10-12\",10.3\\r\\n\"1990-10-13\",7.5\\r\\n\"1990-10-14\",7.7\\r\\n\"1990-10-15\",15.8\\r\\n\"1990-10-16\",14.6\\r\\n\"1990-10-17\",10.5\\r\\n\"1990-10-18\",11.3\\r\\n\"1990-10-19\",10.9\\r\\n\"1990-10-20\",6.4\\r\\n\"1990-10-21\",10.9\\r\\n\"1990-10-22\",9.0\\r\\n\"1990-10-23\",10.9\\r\\n\"1990-10-24\",12.4\\r\\n\"1990-10-25\",11.6\\r\\n\"1990-10-26\",13.3\\r\\n\"1990-10-27\",14.4\\r\\n\"1990-10-28\",18.4\\r\\n\"1990-10-29\",13.6\\r\\n\"1990-10-30\",14.9\\r\\n\"1990-10-31\",14.8\\r\\n\"1990-11-01\",15.4\\r\\n\"1990-11-02\",11.8\\r\\n\"1990-11-03\",13.0\\r\\n\"1990-11-04\",11.1\\r\\n\"1990-11-05\",12.5\\r\\n\"1990-11-06\",18.3\\r\\n\"1990-11-07\",19.2\\r\\n\"1990-11-08\",15.4\\r\\n\"1990-11-09\",13.1\\r\\n\"1990-11-10\",11.5\\r\\n\"1990-11-11\",8.6\\r\\n\"1990-11-12\",12.6\\r\\n\"1990-11-13\",13.8\\r\\n\"1990-11-14\",14.6\\r\\n\"1990-11-15\",13.2\\r\\n\"1990-11-16\",12.3\\r\\n\"1990-11-17\",8.8\\r\\n\"1990-11-18\",10.7\\r\\n\"1990-11-19\",9.9\\r\\n\"1990-11-20\",8.3\\r\\n\"1990-11-21\",15.0\\r\\n\"1990-11-22\",12.2\\r\\n\"1990-11-23\",10.5\\r\\n\"1990-11-24\",11.1\\r\\n\"1990-11-25\",13.0\\r\\n\"1990-11-26\",12.9\\r\\n\"1990-11-27\",8.8\\r\\n\"1990-11-28\",14.7\\r\\n\"1990-11-29\",14.7\\r\\n\"1990-11-30\",12.7\\r\\n\"1990-12-01\",13.3\\r\\n\"1990-12-02\",13.2\\r\\n\"1990-12-03\",16.2\\r\\n\"1990-12-04\",17.3\\r\\n\"1990-12-05\",20.5\\r\\n\"1990-12-06\",20.2\\r\\n\"1990-12-07\",19.4\\r\\n\"1990-12-08\",15.5\\r\\n\"1990-12-09\",14.1\\r\\n\"1990-12-10\",11.0\\r\\n\"1990-12-11\",11.1\\r\\n\"1990-12-12\",14.0\\r\\n\"1990-12-13\",11.4\\r\\n\"1990-12-14\",12.5\\r\\n\"1990-12-15\",13.4\\r\\n\"1990-12-16\",13.6\\r\\n\"1990-12-17\",13.9\\r\\n\"1990-12-18\",17.2\\r\\n\"1990-12-19\",14.7\\r\\n\"1990-12-20\",15.4\\r\\n\"1990-12-21\",13.1\\r\\n\"1990-12-22\",13.2\\r\\n\"1990-12-23\",13.9\\r\\n\"1990-12-24\",10.0\\r\\n\"1990-12-25\",12.9\\r\\n\"1990-12-26\",14.6\\r\\n\"1990-12-27\",14.0\\r\\n\"1990-12-28\",13.6\\r\\n\"1990-12-29\",13.5\\r\\n\"1990-12-30\",15.7\\r\\n\"1990-12-31\",13.0\\r\\n\\r\\nDaily minimum temperatures in Melbourne, Australia, 1981-1990\\r\\n\\r\\n'} # file is uploaded to the current directory ! ls arrhythmia.data daily-minimum-temperatures-in-me.csv sample_data # open the file # the last few lines are junk df = pd . read_csv ( 'daily-minimum-temperatures-in-me.csv' , error_bad_lines = False ) df . head () b'Skipping line 3653: expected 2 fields, saw 3\\n' .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Daily minimum temperatures in Melbourne, Australia, 1981-1990 0 1981-01-01 20.7 1 1981-01-02 17.9 2 1981-01-03 18.8 3 1981-01-04 14.6 4 1981-01-05 15.8 # upload a Python file with some useful functions (meant for fake_util.py) from google.colab import files uploaded = files . upload () <input type=\"file\" id=\"files-c590b543-ffcc-4177-91d3-62725b363169\" name=\"files[]\" multiple disabled /> <output id=\"result-c590b543-ffcc-4177-91d3-62725b363169\"> Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. </output> <script src=\"/nbextensions/google.colab/files.js\"></script> Saving fake_util.py to fake_util.py from fake_util import my_useful_function my_useful_function () hello world ! pwd /content Part 4: Access files from Google Drive \u00b6 # Access files from your Google Drive from google.colab import drive drive . mount ( '/content/gdrive' ) # Check current directory - now gdrive is there ! ls # What's in gdrive? ! ls gdrive # Whoa! Look at all this great VIP content! ! ls '/content/gdrive/My Drive/' Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"TF2 0 Loading Data"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/#tensorflow-bootcamp-colab-basics-loading-data","text":"by Jawad Haider Part 1: Using wget Part 2: Using tf.keras Part 3: Upload the file yourself Part 4: Access files from Google Drive","title":"Tensorflow BootCamp - Colab Basics Loading Data"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/#part-1-using-wget","text":"# download the data from a URL # source: https://archive.ics.uci.edu/ml/datasets/Arrhythmia # alternate URL: https://lazyprogrammer.me/course_files/arrhythmia.data #!wget --no-check-certificate https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data ! wget https : // lazyprogrammer . me / course_files / arrhythmia . data --2020-04-26 07:33:37-- https://lazyprogrammer.me/course_files/arrhythmia.data Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.31.80.48, 104.31.81.48, 2606:4700:3035::681f:5130, ... Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.31.80.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 402355 (393K) Saving to: \u2018arrhythmia.data\u2019 arrhythmia.data 100%[===================>] 392.92K 1.09MB/s in 0.4s 2020-04-26 07:33:38 (1.09 MB/s) - \u2018arrhythmia.data\u2019 saved [402355/402355] # list files in current directory ! ls arrhythmia.data sample_data # check if the data has a header ! head arrhythmia . data 75,0,190,80,91,193,371,174,121,-16,13,64,-2,?,63,0,52,44,0,0,32,0,0,0,0,0,0,0,44,20,36,0,28,0,0,0,0,0,0,52,40,0,0,0,60,0,0,0,0,0,0,52,0,0,0,0,0,0,0,0,0,0,0,0,56,36,0,0,32,0,0,0,0,0,0,48,32,0,0,0,56,0,0,0,0,0,0,80,0,0,0,0,0,0,0,0,0,0,0,0,40,52,0,0,28,0,0,0,0,0,0,0,48,48,0,0,32,0,0,0,0,0,0,0,52,52,0,0,36,0,0,0,0,0,0,0,52,48,0,0,32,0,0,0,0,0,0,0,56,44,0,0,32,0,0,0,0,0,0,-0.2,0.0,6.1,-1.0,0.0,0.0,0.6,2.1,13.6,30.8,0.0,0.0,1.7,-1.0,0.6,0.0,1.3,1.5,3.7,14.5,0.1,-5.2,1.4,0.0,0.0,0.0,0.8,-0.6,-10.7,-15.6,0.4,-3.9,0.0,0.0,0.0,0.0,-0.8,-1.7,-10.1,-22.0,0.0,0.0,5.7,-1.0,0.0,0.0,-0.1,1.2,14.1,22.5,0.0,-2.5,0.8,0.0,0.0,0.0,1.0,0.4,-4.8,-2.7,0.1,-6.0,0.0,0.0,0.0,0.0,-0.8,-0.6,-24.0,-29.7,0.0,0.0,2.0,-6.4,0.0,0.0,0.2,2.9,-12.6,15.2,-0.1,0.0,8.4,-10.0,0.0,0.0,0.6,5.9,-3.9,52.7,-0.3,0.0,15.2,-8.4,0.0,0.0,0.9,5.1,17.7,70.7,-0.4,0.0,13.5,-4.0,0.0,0.0,0.9,3.9,25.5,62.9,-0.3,0.0,9.0,-0.9,0.0,0.0,0.9,2.9,23.3,49.4,8 56,1,165,64,81,174,401,149,39,25,37,-17,31,?,53,0,48,0,0,0,24,0,0,0,0,0,0,0,64,0,0,0,24,0,0,0,0,0,0,32,24,0,0,0,40,0,0,0,0,0,0,48,0,0,0,0,0,0,0,0,0,0,0,0,44,20,0,0,24,0,0,0,0,0,0,0,60,0,0,0,20,0,0,0,0,0,0,0,24,52,0,0,16,0,0,0,0,0,0,0,32,52,0,0,20,0,0,0,0,0,0,0,44,48,0,0,32,0,0,0,0,0,0,0,48,44,0,0,32,0,0,0,0,0,0,0,48,40,0,0,28,0,0,0,0,0,0,0,48,0,0,0,28,0,0,0,0,0,0,-0.6,0.0,7.2,0.0,0.0,0.0,0.4,1.5,17.2,26.5,0.0,0.0,5.5,0.0,0.0,0.0,0.1,1.7,17.6,29.5,0.3,-1.6,0.9,0.0,0.0,0.0,-0.3,0.4,-1.5,1.3,0.1,-6.4,0.0,0.0,0.0,0.0,-0.3,-1.6,-15.3,-25.5,-0.3,0.0,4.2,-0.9,0.0,0.0,0.4,0.7,8.3,12.3,0.2,0.0,2.2,0.0,0.0,0.0,-0.2,0.8,6.6,11.7,0.4,0.0,1.0,-8.8,0.0,0.0,0.5,-0.6,-21.6,-26.8,0.4,0.0,2.6,-7.9,0.0,0.0,0.8,2.0,-16.4,1.2,0.0,0.0,5.8,-7.7,0.0,0.0,0.9,3.8,-5.7,27.7,-0.2,0.0,9.5,-5.0,0.0,0.0,0.5,2.6,11.8,34.6,-0.4,0.0,11.0,-2.4,0.0,0.0,0.4,2.6,21.6,43.4,-0.5,0.0,8.5,0.0,0.0,0.0,0.2,2.1,20.4,38.8,6 54,0,172,95,138,163,386,185,102,96,34,70,66,23,75,0,40,80,0,0,24,0,0,0,0,0,0,20,56,52,0,0,40,0,0,0,0,0,0,28,116,0,0,0,52,0,0,0,0,0,0,52,64,0,0,0,88,0,0,0,0,0,0,0,36,92,0,0,24,0,0,0,0,0,0,0,128,0,0,0,24,0,1,0,0,0,0,0,24,36,76,0,100,0,0,0,0,0,0,0,40,28,60,0,96,0,0,0,0,0,0,0,48,20,56,24,32,0,0,0,0,0,0,0,44,88,0,0,28,0,0,0,0,0,0,0,44,76,0,0,28,0,0,0,0,0,0,0,44,72,0,0,24,0,0,0,0,0,0,1.0,0.0,4.5,-2.8,0.0,0.0,0.3,2.5,-2.2,19.8,0.8,-0.4,6.4,-1.3,0.0,0.0,0.7,2.7,14.2,37.9,-0.2,-0.6,4.4,0.0,0.0,0.0,0.5,0.2,24.7,26.2,-1.0,-5.3,1.8,0.0,0.0,0.0,-0.5,-2.5,-8.0,-28.5,0.5,0.0,1.7,-2.7,0.0,0.0,-0.2,1.0,-9.4,-1.2,0.4,0.0,4.9,0.0,0.0,0.0,0.6,1.4,31.3,42.7,-0.8,0.0,0.7,-3.8,6.5,0.0,0.3,-3.3,18.7,-13.6,-0.9,0.0,2.2,-4.1,7.4,0.0,0.5,-2.4,20.9,-2.6,0.0,0.0,5.8,-4.1,4.0,-0.5,0.4,0.3,20.4,23.3,0.7,0.0,10.0,-5.7,0.0,0.0,0.5,2.2,-3.0,20.7,1.3,0.0,11.1,-3.4,0.0,0.0,0.4,3.4,11.5,48.2,0.9,0.0,9.5,-2.4,0.0,0.0,0.3,3.4,12.3,49.0,10 55,0,175,94,100,202,380,179,143,28,11,-5,20,?,71,0,72,20,0,0,48,0,0,0,0,0,0,0,64,36,0,0,36,0,0,0,0,0,0,20,52,48,0,0,56,0,0,0,0,0,0,64,32,0,0,0,72,0,0,0,0,0,0,0,60,12,0,0,44,0,0,0,0,0,0,0,60,44,0,0,32,0,0,0,0,0,0,56,0,0,0,0,0,0,0,0,0,0,0,0,40,44,0,0,20,0,0,0,0,0,0,0,52,40,0,0,32,0,0,0,0,0,0,0,56,48,0,0,36,0,0,0,0,0,0,0,60,48,0,0,36,0,0,0,0,0,0,0,64,40,0,0,40,0,0,0,0,0,0,0.9,0.0,7.8,-0.7,0.0,0.0,1.1,1.9,27.3,45.1,0.1,0.0,9.1,-2.6,0.0,0.0,0.4,1.5,24.5,36.8,-0.4,-0.4,1.6,-2.2,0.0,0.0,-1.0,-0.9,-1.5,-9.2,-0.4,-8.2,1.8,0.0,0.0,0.0,-0.7,-1.7,-23.4,-35.6,0.9,0.0,3.2,-0.4,0.0,0.0,0.7,1.2,9.4,18.0,-0.1,0.0,5.1,-2.5,0.0,0.0,0.3,0.6,9.8,12.6,1.6,-6.5,0.0,0.0,0.0,0.0,-0.4,-0.4,-18.2,-22.4,2.1,0.0,1.2,-6.9,0.0,0.0,-0.5,2.9,-12.7,18.0,0.7,0.0,9.0,-7.9,0.0,0.0,0.1,4.1,7.6,51.0,0.4,0.0,15.0,-5.5,0.0,0.0,0.1,3.3,28.8,63.1,0.1,0.0,15.2,-3.7,0.0,0.0,0.6,3.0,36.8,68.0,0.1,0.0,12.2,-2.2,0.0,0.0,0.4,2.6,34.6,61.6,1 75,0,190,80,88,181,360,177,103,-16,13,61,3,?,?,0,48,40,0,0,28,0,0,0,0,0,0,0,40,24,0,0,24,0,0,0,0,0,0,52,36,0,0,0,60,0,0,0,0,0,0,48,28,0,0,0,56,0,0,0,0,0,0,0,48,36,0,0,28,0,0,0,0,0,0,44,0,0,0,0,0,0,0,0,0,0,0,88,0,0,0,0,0,0,0,0,0,0,0,0,40,52,0,0,28,0,0,0,0,0,0,0,48,48,0,0,32,0,0,0,0,0,0,0,48,52,0,0,32,0,0,0,0,0,0,0,52,44,0,0,28,0,0,0,0,0,0,0,52,48,0,0,32,0,0,0,0,0,0,0.0,0.0,5.2,-1.4,0.0,0.0,0.9,2.3,9.6,31.6,0.1,0.0,1.6,-0.5,0.0,0.0,1.9,1.7,2.6,18.9,0.2,-3.8,1.2,0.0,0.0,0.0,1.0,-0.6,-7.7,-13.4,-0.1,-3.4,0.8,0.0,0.0,0.0,-1.4,-1.5,-7.0,-17.8,-0.1,0.0,4.4,-1.3,0.0,0.0,-0.1,1.1,8.2,16.5,0.6,-1.6,0.0,0.0,0.0,0.0,1.4,0.3,-3.5,-1.9,0.0,-5.7,0.0,0.0,0.0,0.0,-0.4,-0.5,-25.0,-30.0,-0.2,0.0,1.6,-6.0,0.0,0.0,-0.7,2.1,-12.4,8.6,-0.5,0.0,8.5,-10.2,0.0,0.0,-1.0,4.7,-4.0,43.0,-0.2,0.0,15.2,-7.8,0.0,0.0,-0.1,4.9,16.2,63.2,-0.2,0.0,9.1,-0.9,0.0,0.0,-0.2,2.9,21.7,48.9,-0.4,0.0,13.1,-3.6,0.0,0.0,-0.1,3.9,25.4,62.8,7 13,0,169,51,100,167,321,174,91,107,66,52,88,?,84,0,36,48,0,0,20,0,0,0,0,0,0,20,44,36,0,0,44,0,0,0,0,0,0,24,64,0,0,0,48,0,0,0,0,0,0,44,36,0,0,0,52,0,0,0,0,0,0,0,28,64,0,0,16,0,0,0,0,0,0,24,44,40,0,0,44,0,0,0,0,0,0,0,36,60,0,0,24,0,0,0,0,0,0,20,32,60,0,0,40,0,0,0,0,0,0,24,32,60,0,0,44,0,0,0,0,0,0,0,52,40,0,0,36,0,0,0,0,0,0,0,44,40,0,0,32,0,0,0,0,0,0,20,36,56,0,0,40,0,0,0,0,0,0,0.5,0.0,2.7,-6.4,0.0,0.0,0.9,1.7,-10.5,7.1,0.1,-1.2,19.1,-2.3,0.0,0.0,1.4,4.3,36.7,84.8,-0.4,-2.3,21.7,0.0,0.0,0.0,0.7,2.6,66.7,95.8,-0.2,-9.0,3.2,0.0,0.0,0.0,-1.1,-2.9,-14.1,-39.0,0.5,0.0,1.8,-12.9,0.0,0.0,0.4,-0.4,-38.7,-42.1,-0.1,-1.6,19.9,-0.7,0.0,0.0,1.0,3.3,40.4,65.4,0.4,0.0,6.7,-24.4,0.0,0.0,-1.2,0.4,-61.2,-59.9,0.9,-0.5,11.9,-43.3,0.0,0.0,0.8,3.4,-111.4,-95.1,2.0,-0.8,19.8,-48.4,0.0,0.0,1.6,8.7,-114.5,-72.8,2.0,0.0,31.0,-25.7,0.0,0.0,0.8,5.9,29.2,85.8,0.6,0.0,19.5,-11.4,0.0,0.0,0.8,3.3,20.1,49.1,0.0,-0.6,12.2,-2.8,0.0,0.0,0.9,2.2,13.5,31.1,14 40,1,160,52,77,129,377,133,77,77,49,75,65,?,70,0,44,0,0,0,24,0,0,0,0,0,0,0,40,32,0,0,24,0,0,0,0,0,0,0,44,28,0,0,24,0,0,0,0,0,0,44,16,0,0,0,48,0,0,0,0,0,0,36,0,0,0,0,0,0,0,0,0,0,0,0,44,16,0,0,24,0,0,0,0,0,0,0,20,56,0,0,12,0,0,0,0,0,0,0,24,56,0,0,16,0,0,0,0,0,0,0,36,48,0,0,24,0,0,0,0,0,0,0,40,44,0,0,28,0,0,0,0,0,0,0,40,44,0,0,24,0,0,0,0,0,0,0,44,0,0,0,24,0,0,0,0,0,0,-0.5,0.0,1.8,0.0,0.0,0.0,0.2,1.0,3.9,10.5,-0.1,0.0,7.6,-1.1,0.0,0.0,0.5,1.4,13.5,22.7,0.0,0.0,5.9,-0.5,0.0,0.0,0.3,0.6,12.2,15.0,0.1,-4.6,0.6,0.0,0.0,0.0,-0.4,-0.9,-9.7,-14.7,0.2,-2.1,0.0,0.0,0.0,0.0,-0.3,0.4,-3.7,-1.4,-0.2,0.0,6.8,-0.9,0.0,0.0,0.7,0.7,14.2,17.1,1.3,0.0,1.3,-11.5,0.0,0.0,-0.3,1.7,-30.9,-13.9,1.7,0.0,2.3,-17.5,0.0,0.0,-0.6,4.5,-46.3,-1.3,1.1,0.0,3.7,-11.0,0.0,0.0,-0.5,4.1,-19.8,21.2,0.1,0.0,7.7,-6.4,0.0,0.0,0.4,1.9,1.4,15.4,0.0,0.0,7.4,-2.5,0.0,0.0,0.4,1.3,9.3,18.9,-0.4,0.0,6.5,0.0,0.0,0.0,0.4,1.0,14.3,20.5,1 49,1,162,54,78,0,376,157,70,67,7,8,51,?,67,0,44,36,0,0,24,0,0,0,0,0,0,0,52,32,0,0,28,0,0,0,0,0,0,0,56,28,0,0,24,0,0,0,0,0,0,48,32,0,0,0,56,0,0,0,0,0,0,52,0,0,0,0,0,0,0,0,0,0,0,0,52,28,0,0,28,0,0,0,0,0,0,0,20,44,0,0,8,0,0,0,0,0,0,0,24,48,0,0,16,0,0,0,0,0,0,0,36,44,0,0,24,0,0,0,0,0,0,0,44,48,0,0,28,0,0,0,0,0,0,0,48,44,0,0,28,0,0,0,0,0,0,0,48,40,0,0,24,0,0,0,0,0,0,-0.3,0.0,4.1,-1.1,0.0,0.0,0.8,1.0,7.1,13.7,-0.3,0.0,8.4,-1.5,0.0,0.0,0.6,0.7,19.4,22.9,0.0,0.0,4.4,-0.8,0.0,0.0,-0.3,-0.6,11.2,6.9,0.1,-6.3,1.3,0.0,0.0,0.0,-0.6,-0.8,-13.1,-17.9,0.1,-0.8,0.0,0.0,0.0,0.0,0.6,0.7,-2.0,2.9,-0.2,0.0,6.3,-1.2,0.0,0.0,0.2,0.3,14.7,16.8,0.7,0.0,0.5,-7.3,0.0,0.0,0.2,-0.1,-15.5,-16.4,0.9,0.0,0.7,-8.9,0.0,0.0,0.6,2.5,-20.5,4.0,0.8,0.0,2.1,-9.0,0.0,0.0,0.6,3.8,-16.1,21.1,0.1,0.0,6.6,-4.1,0.0,0.0,0.3,1.4,4.7,14.2,-0.2,0.0,8.5,-2.7,0.0,0.0,0.1,0.8,14.5,20.9,-0.3,0.0,8.2,-1.9,0.0,0.0,0.1,0.5,15.8,19.8,1 44,0,168,56,84,118,354,160,63,61,69,78,66,84,64,0,40,0,0,0,20,0,0,0,0,0,0,0,44,12,0,0,28,0,0,0,0,0,0,0,36,8,0,0,20,0,0,0,0,0,0,40,12,0,0,0,44,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,36,12,0,0,20,0,0,0,0,0,0,0,20,56,0,0,12,0,0,0,0,0,0,0,24,48,0,0,12,0,0,0,0,0,0,0,28,44,0,0,16,0,0,0,0,0,0,0,44,32,0,0,32,0,0,0,0,0,0,0,44,28,0,0,28,0,0,0,0,0,0,0,40,24,0,0,24,0,0,0,0,0,0,0.1,0.0,2.3,0.0,0.0,0.0,0.4,1.0,4.6,11.6,1.2,0.0,5.4,-0.7,0.0,0.0,1.8,2.8,11.4,31.0,1.1,0.0,3.0,-0.4,0.0,0.0,1.4,1.8,5.3,17.9,-0.7,-3.9,0.5,0.0,0.0,0.0,-1.1,-1.9,-7.5,-20.4,-0.5,0.0,0.0,0.0,0.0,0.0,-0.6,-0.5,0.0,-3.4,1.1,0.0,4.2,-0.5,0.0,0.0,1.6,2.3,7.2,22.8,0.5,0.0,0.9,-5.5,0.0,0.0,-0.7,1.0,-14.5,-5.3,0.7,0.0,1.2,-6.4,0.0,0.0,-0.5,2.6,-13.9,10.0,1.5,0.0,2.4,-10.3,0.0,0.0,0.3,6.8,-19.3,43.2,0.8,0.0,7.9,-7.3,0.0,0.0,0.9,6.5,5.7,62.9,0.1,0.0,9.3,-3.8,0.0,0.0,0.8,3.8,15.1,48.5,0.1,0.0,7.0,-1.3,0.0,0.0,0.6,2.1,12.5,30.9,1 50,1,167,67,89,130,383,156,73,85,34,70,71,?,63,0,44,40,0,0,28,0,0,0,0,0,0,0,56,24,0,0,32,0,0,0,0,0,0,0,72,0,0,0,28,0,0,0,0,0,0,56,28,0,0,0,60,0,0,0,0,0,0,0,28,56,0,0,16,0,0,0,0,0,0,0,60,0,0,0,32,0,0,0,0,0,0,0,24,36,32,0,68,0,0,0,0,0,0,0,36,44,0,0,20,0,0,0,0,0,0,0,40,48,0,0,24,0,0,0,0,0,0,0,56,40,0,0,40,0,0,0,0,0,0,0,52,36,0,0,32,0,0,0,0,0,0,20,44,36,0,0,44,0,0,0,0,0,0,-0.1,0.0,3.5,-2.0,0.0,0.0,0.4,1.3,3.7,13.5,0.0,0.0,9.9,-0.8,0.0,0.0,1.2,1.2,26.8,35.2,0.0,0.0,8.3,0.0,0.0,0.0,0.8,0.3,29.8,32.0,0.1,-6.1,1.1,0.0,0.0,0.0,-0.6,-1.2,-15.5,-24.1,0.0,0.0,0.6,-4.1,0.0,0.0,-0.1,0.8,-10.6,-4.9,-0.2,0.0,8.9,0.0,0.0,0.0,0.8,0.7,26.7,30.2,0.1,0.0,1.3,-5.4,1.9,0.0,0.2,0.8,-5.2,2.1,0.8,0.0,4.4,-8.5,0.0,0.0,0.8,3.9,-10.8,25.0,0.4,0.0,4.3,-7.3,0.0,0.0,1.1,4.0,-8.9,27.9,-0.5,0.0,7.0,-3.2,0.0,0.0,1.1,1.3,13.2,22.3,-0.5,0.0,10.9,-2.5,0.0,0.0,1.0,1.0,23.8,29.6,-0.5,-0.6,10.8,-1.7,0.0,0.0,0.8,0.9,20.1,25.1,10 # check the data import pandas as pd df = pd . read_csv ( 'arrhythmia.data' , header = None ) # since the data has many columns, take just the first few and name them (as per the documentation) data = df [[ 0 , 1 , 2 , 3 , 4 , 5 ]] data . columns = [ 'age' , 'sex' , 'height' , 'weight' , 'QRS duration' , 'P-R interval' ] import matplotlib.pyplot as plt plt . rcParams [ 'figure.figsize' ] = [ 15 , 15 ] # make the plot bigger so the subplots don't overlap data . hist (); # use a semicolon to supress return value from pandas.plotting import scatter_matrix scatter_matrix ( data );","title":"Part 1: Using wget"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/#part-2-using-tfkeras","text":"# use keras get_file to download the auto MPG dataset # source: https://archive.ics.uci.edu/ml/datasets/Auto+MPG #url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data' ### alternate URL url = 'https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/auto-mpg.data' # Install TensorFlow import tensorflow as tf print ( tf . __version__ ) 2.2.0-rc2 # check out the documentation for other arguments tf . keras . utils . get_file ( 'auto-mpg.data' , url ) Downloading data from https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/auto-mpg.data 32768/30286 [================================] - 0s 1us/step '/root/.keras/datasets/auto-mpg.data' ! head / root /. keras / datasets / auto - mpg . data 18.0 8 307.0 130.0 3504. 12.0 70 1 \"chevrolet chevelle malibu\" 15.0 8 350.0 165.0 3693. 11.5 70 1 \"buick skylark 320\" 18.0 8 318.0 150.0 3436. 11.0 70 1 \"plymouth satellite\" 16.0 8 304.0 150.0 3433. 12.0 70 1 \"amc rebel sst\" 17.0 8 302.0 140.0 3449. 10.5 70 1 \"ford torino\" 15.0 8 429.0 198.0 4341. 10.0 70 1 \"ford galaxie 500\" 14.0 8 454.0 220.0 4354. 9.0 70 1 \"chevrolet impala\" 14.0 8 440.0 215.0 4312. 8.5 70 1 \"plymouth fury iii\" 14.0 8 455.0 225.0 4425. 10.0 70 1 \"pontiac catalina\" 15.0 8 390.0 190.0 3850. 8.5 70 1 \"amc ambassador dpl\" # unless you specify an alternative path, the data will go into /root/.keras/datasets/ df = pd . read_csv ( '/root/.keras/datasets/auto-mpg.data' , header = None , delim_whitespace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 0 18.0 8 307.0 130.0 3504.0 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165.0 3693.0 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150.0 3436.0 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150.0 3433.0 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140.0 3449.0 10.5 70 1 ford torino","title":"Part 2: Using tf.keras"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/#part-3-upload-the-file-yourself","text":"# another method: upload your own file ##### PLEASE NOTE: IT DOES NOT MATTER WHICH FILE YOU UPLOAD ##### YOU CAN UPLOAD ANY FILE YOU WANT ##### IN FACT, YOU ARE ENCOURAGED TO EXPLORE ON YOUR OWN # if you must, then get the file from here: # https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/daily-minimum-temperatures-in-me.csv from google.colab import files uploaded = files . upload () <input type=\"file\" id=\"files-f85a97ca-0bfe-4c55-8369-d830289c8925\" name=\"files[]\" multiple disabled /> <output id=\"result-f85a97ca-0bfe-4c55-8369-d830289c8925\"> Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. </output> <script src=\"/nbextensions/google.colab/files.js\"></script> Saving daily-minimum-temperatures-in-me.csv to daily-minimum-temperatures-in-me.csv uploaded {'daily-minimum-temperatures-in-me.csv': b'\"Date\",\"Daily minimum temperatures in Melbourne, Australia, 1981-1990\"\\r\\n\"1981-01-01\",20.7\\r\\n\"1981-01-02\",17.9\\r\\n\"1981-01-03\",18.8\\r\\n\"1981-01-04\",14.6\\r\\n\"1981-01-05\",15.8\\r\\n\"1981-01-06\",15.8\\r\\n\"1981-01-07\",15.8\\r\\n\"1981-01-08\",17.4\\r\\n\"1981-01-09\",21.8\\r\\n\"1981-01-10\",20.0\\r\\n\"1981-01-11\",16.2\\r\\n\"1981-01-12\",13.3\\r\\n\"1981-01-13\",16.7\\r\\n\"1981-01-14\",21.5\\r\\n\"1981-01-15\",25.0\\r\\n\"1981-01-16\",20.7\\r\\n\"1981-01-17\",20.6\\r\\n\"1981-01-18\",24.8\\r\\n\"1981-01-19\",17.7\\r\\n\"1981-01-20\",15.5\\r\\n\"1981-01-21\",18.2\\r\\n\"1981-01-22\",12.1\\r\\n\"1981-01-23\",14.4\\r\\n\"1981-01-24\",16.0\\r\\n\"1981-01-25\",16.5\\r\\n\"1981-01-26\",18.7\\r\\n\"1981-01-27\",19.4\\r\\n\"1981-01-28\",17.2\\r\\n\"1981-01-29\",15.5\\r\\n\"1981-01-30\",15.1\\r\\n\"1981-01-31\",15.4\\r\\n\"1981-02-01\",15.3\\r\\n\"1981-02-02\",18.8\\r\\n\"1981-02-03\",21.9\\r\\n\"1981-02-04\",19.9\\r\\n\"1981-02-05\",16.6\\r\\n\"1981-02-06\",16.8\\r\\n\"1981-02-07\",14.6\\r\\n\"1981-02-08\",17.1\\r\\n\"1981-02-09\",25.0\\r\\n\"1981-02-10\",15.0\\r\\n\"1981-02-11\",13.7\\r\\n\"1981-02-12\",13.9\\r\\n\"1981-02-13\",18.3\\r\\n\"1981-02-14\",22.0\\r\\n\"1981-02-15\",22.1\\r\\n\"1981-02-16\",21.2\\r\\n\"1981-02-17\",18.4\\r\\n\"1981-02-18\",16.6\\r\\n\"1981-02-19\",16.1\\r\\n\"1981-02-20\",15.7\\r\\n\"1981-02-21\",16.6\\r\\n\"1981-02-22\",16.5\\r\\n\"1981-02-23\",14.4\\r\\n\"1981-02-24\",14.4\\r\\n\"1981-02-25\",18.5\\r\\n\"1981-02-26\",16.9\\r\\n\"1981-02-27\",17.5\\r\\n\"1981-02-28\",21.2\\r\\n\"1981-03-01\",17.8\\r\\n\"1981-03-02\",18.6\\r\\n\"1981-03-03\",17.0\\r\\n\"1981-03-04\",16.0\\r\\n\"1981-03-05\",13.3\\r\\n\"1981-03-06\",14.3\\r\\n\"1981-03-07\",11.4\\r\\n\"1981-03-08\",16.3\\r\\n\"1981-03-09\",16.1\\r\\n\"1981-03-10\",11.8\\r\\n\"1981-03-11\",12.2\\r\\n\"1981-03-12\",14.7\\r\\n\"1981-03-13\",11.8\\r\\n\"1981-03-14\",11.3\\r\\n\"1981-03-15\",10.6\\r\\n\"1981-03-16\",11.7\\r\\n\"1981-03-17\",14.2\\r\\n\"1981-03-18\",11.2\\r\\n\"1981-03-19\",16.9\\r\\n\"1981-03-20\",16.7\\r\\n\"1981-03-21\",8.1\\r\\n\"1981-03-22\",8.0\\r\\n\"1981-03-23\",8.8\\r\\n\"1981-03-24\",13.4\\r\\n\"1981-03-25\",10.9\\r\\n\"1981-03-26\",13.4\\r\\n\"1981-03-27\",11.0\\r\\n\"1981-03-28\",15.0\\r\\n\"1981-03-29\",15.7\\r\\n\"1981-03-30\",14.5\\r\\n\"1981-03-31\",15.8\\r\\n\"1981-04-01\",16.7\\r\\n\"1981-04-02\",16.8\\r\\n\"1981-04-03\",17.5\\r\\n\"1981-04-04\",17.1\\r\\n\"1981-04-05\",18.1\\r\\n\"1981-04-06\",16.6\\r\\n\"1981-04-07\",10.0\\r\\n\"1981-04-08\",14.9\\r\\n\"1981-04-09\",15.9\\r\\n\"1981-04-10\",13.0\\r\\n\"1981-04-11\",7.6\\r\\n\"1981-04-12\",11.5\\r\\n\"1981-04-13\",13.5\\r\\n\"1981-04-14\",13.0\\r\\n\"1981-04-15\",13.3\\r\\n\"1981-04-16\",12.1\\r\\n\"1981-04-17\",12.4\\r\\n\"1981-04-18\",13.2\\r\\n\"1981-04-19\",13.8\\r\\n\"1981-04-20\",10.6\\r\\n\"1981-04-21\",9.0\\r\\n\"1981-04-22\",10.0\\r\\n\"1981-04-23\",9.8\\r\\n\"1981-04-24\",11.5\\r\\n\"1981-04-25\",8.9\\r\\n\"1981-04-26\",7.4\\r\\n\"1981-04-27\",9.9\\r\\n\"1981-04-28\",9.3\\r\\n\"1981-04-29\",9.9\\r\\n\"1981-04-30\",7.4\\r\\n\"1981-05-01\",8.6\\r\\n\"1981-05-02\",11.9\\r\\n\"1981-05-03\",14.0\\r\\n\"1981-05-04\",8.6\\r\\n\"1981-05-05\",10.0\\r\\n\"1981-05-06\",13.5\\r\\n\"1981-05-07\",12.0\\r\\n\"1981-05-08\",10.5\\r\\n\"1981-05-09\",10.7\\r\\n\"1981-05-10\",8.1\\r\\n\"1981-05-11\",10.1\\r\\n\"1981-05-12\",10.6\\r\\n\"1981-05-13\",5.3\\r\\n\"1981-05-14\",6.6\\r\\n\"1981-05-15\",8.5\\r\\n\"1981-05-16\",11.2\\r\\n\"1981-05-17\",9.8\\r\\n\"1981-05-18\",5.9\\r\\n\"1981-05-19\",3.2\\r\\n\"1981-05-20\",2.1\\r\\n\"1981-05-21\",3.4\\r\\n\"1981-05-22\",5.4\\r\\n\"1981-05-23\",9.6\\r\\n\"1981-05-24\",11.5\\r\\n\"1981-05-25\",12.3\\r\\n\"1981-05-26\",12.6\\r\\n\"1981-05-27\",11.0\\r\\n\"1981-05-28\",11.2\\r\\n\"1981-05-29\",11.4\\r\\n\"1981-05-30\",11.8\\r\\n\"1981-05-31\",12.8\\r\\n\"1981-06-01\",11.6\\r\\n\"1981-06-02\",10.6\\r\\n\"1981-06-03\",9.8\\r\\n\"1981-06-04\",11.2\\r\\n\"1981-06-05\",5.7\\r\\n\"1981-06-06\",7.1\\r\\n\"1981-06-07\",2.5\\r\\n\"1981-06-08\",3.5\\r\\n\"1981-06-09\",4.6\\r\\n\"1981-06-10\",11.0\\r\\n\"1981-06-11\",5.7\\r\\n\"1981-06-12\",7.7\\r\\n\"1981-06-13\",10.4\\r\\n\"1981-06-14\",11.4\\r\\n\"1981-06-15\",9.2\\r\\n\"1981-06-16\",6.1\\r\\n\"1981-06-17\",2.7\\r\\n\"1981-06-18\",4.3\\r\\n\"1981-06-19\",6.3\\r\\n\"1981-06-20\",3.8\\r\\n\"1981-06-21\",4.4\\r\\n\"1981-06-22\",7.1\\r\\n\"1981-06-23\",4.8\\r\\n\"1981-06-24\",5.8\\r\\n\"1981-06-25\",6.2\\r\\n\"1981-06-26\",7.3\\r\\n\"1981-06-27\",9.2\\r\\n\"1981-06-28\",10.2\\r\\n\"1981-06-29\",9.5\\r\\n\"1981-06-30\",9.5\\r\\n\"1981-07-01\",10.7\\r\\n\"1981-07-02\",10.0\\r\\n\"1981-07-03\",6.5\\r\\n\"1981-07-04\",7.0\\r\\n\"1981-07-05\",7.4\\r\\n\"1981-07-06\",8.1\\r\\n\"1981-07-07\",6.6\\r\\n\"1981-07-08\",8.3\\r\\n\"1981-07-09\",8.9\\r\\n\"1981-07-10\",4.6\\r\\n\"1981-07-11\",6.8\\r\\n\"1981-07-12\",5.7\\r\\n\"1981-07-13\",6.1\\r\\n\"1981-07-14\",7.0\\r\\n\"1981-07-15\",7.2\\r\\n\"1981-07-16\",6.3\\r\\n\"1981-07-17\",8.8\\r\\n\"1981-07-18\",5.0\\r\\n\"1981-07-19\",7.4\\r\\n\"1981-07-20\",10.1\\r\\n\"1981-07-21\",12.0\\r\\n\"1981-07-22\",9.0\\r\\n\"1981-07-23\",8.9\\r\\n\"1981-07-24\",9.8\\r\\n\"1981-07-25\",9.0\\r\\n\"1981-07-26\",9.2\\r\\n\"1981-07-27\",7.7\\r\\n\"1981-07-28\",8.0\\r\\n\"1981-07-29\",6.1\\r\\n\"1981-07-30\",3.5\\r\\n\"1981-07-31\",3.2\\r\\n\"1981-08-01\",5.7\\r\\n\"1981-08-02\",7.7\\r\\n\"1981-08-03\",9.0\\r\\n\"1981-08-04\",10.0\\r\\n\"1981-08-05\",6.2\\r\\n\"1981-08-06\",6.9\\r\\n\"1981-08-07\",6.5\\r\\n\"1981-08-08\",6.8\\r\\n\"1981-08-09\",7.0\\r\\n\"1981-08-10\",5.2\\r\\n\"1981-08-11\",3.0\\r\\n\"1981-08-12\",5.6\\r\\n\"1981-08-13\",7.9\\r\\n\"1981-08-14\",9.0\\r\\n\"1981-08-15\",8.6\\r\\n\"1981-08-16\",10.3\\r\\n\"1981-08-17\",10.5\\r\\n\"1981-08-18\",7.6\\r\\n\"1981-08-19\",9.7\\r\\n\"1981-08-20\",12.5\\r\\n\"1981-08-21\",7.4\\r\\n\"1981-08-22\",7.9\\r\\n\"1981-08-23\",3.9\\r\\n\"1981-08-24\",6.6\\r\\n\"1981-08-25\",4.6\\r\\n\"1981-08-26\",7.0\\r\\n\"1981-08-27\",6.0\\r\\n\"1981-08-28\",5.5\\r\\n\"1981-08-29\",8.1\\r\\n\"1981-08-30\",5.5\\r\\n\"1981-08-31\",6.2\\r\\n\"1981-09-01\",8.0\\r\\n\"1981-09-02\",10.3\\r\\n\"1981-09-03\",9.8\\r\\n\"1981-09-04\",9.6\\r\\n\"1981-09-05\",8.5\\r\\n\"1981-09-06\",7.5\\r\\n\"1981-09-07\",11.2\\r\\n\"1981-09-08\",14.6\\r\\n\"1981-09-09\",11.7\\r\\n\"1981-09-10\",7.8\\r\\n\"1981-09-11\",12.3\\r\\n\"1981-09-12\",10.1\\r\\n\"1981-09-13\",11.5\\r\\n\"1981-09-14\",7.3\\r\\n\"1981-09-15\",10.9\\r\\n\"1981-09-16\",14.1\\r\\n\"1981-09-17\",10.7\\r\\n\"1981-09-18\",16.9\\r\\n\"1981-09-19\",10.5\\r\\n\"1981-09-20\",6.5\\r\\n\"1981-09-21\",11.0\\r\\n\"1981-09-22\",6.3\\r\\n\"1981-09-23\",10.5\\r\\n\"1981-09-24\",7.2\\r\\n\"1981-09-25\",7.6\\r\\n\"1981-09-26\",10.7\\r\\n\"1981-09-27\",7.8\\r\\n\"1981-09-28\",9.6\\r\\n\"1981-09-29\",11.4\\r\\n\"1981-09-30\",12.4\\r\\n\"1981-10-01\",8.9\\r\\n\"1981-10-02\",13.2\\r\\n\"1981-10-03\",8.6\\r\\n\"1981-10-04\",6.2\\r\\n\"1981-10-05\",11.4\\r\\n\"1981-10-06\",13.2\\r\\n\"1981-10-07\",14.3\\r\\n\"1981-10-08\",7.3\\r\\n\"1981-10-09\",12.9\\r\\n\"1981-10-10\",7.8\\r\\n\"1981-10-11\",6.2\\r\\n\"1981-10-12\",5.6\\r\\n\"1981-10-13\",10.0\\r\\n\"1981-10-14\",13.3\\r\\n\"1981-10-15\",8.3\\r\\n\"1981-10-16\",10.2\\r\\n\"1981-10-17\",8.6\\r\\n\"1981-10-18\",7.3\\r\\n\"1981-10-19\",10.4\\r\\n\"1981-10-20\",11.2\\r\\n\"1981-10-21\",13.2\\r\\n\"1981-10-22\",11.4\\r\\n\"1981-10-23\",9.1\\r\\n\"1981-10-24\",6.6\\r\\n\"1981-10-25\",8.4\\r\\n\"1981-10-26\",9.7\\r\\n\"1981-10-27\",13.2\\r\\n\"1981-10-28\",12.5\\r\\n\"1981-10-29\",11.0\\r\\n\"1981-10-30\",11.0\\r\\n\"1981-10-31\",11.7\\r\\n\"1981-11-01\",9.2\\r\\n\"1981-11-02\",11.5\\r\\n\"1981-11-03\",13.6\\r\\n\"1981-11-04\",13.7\\r\\n\"1981-11-05\",10.4\\r\\n\"1981-11-06\",11.5\\r\\n\"1981-11-07\",7.6\\r\\n\"1981-11-08\",9.6\\r\\n\"1981-11-09\",14.2\\r\\n\"1981-11-10\",15.7\\r\\n\"1981-11-11\",10.5\\r\\n\"1981-11-12\",10.5\\r\\n\"1981-11-13\",9.7\\r\\n\"1981-11-14\",9.5\\r\\n\"1981-11-15\",11.3\\r\\n\"1981-11-16\",8.9\\r\\n\"1981-11-17\",9.4\\r\\n\"1981-11-18\",11.9\\r\\n\"1981-11-19\",11.7\\r\\n\"1981-11-20\",13.4\\r\\n\"1981-11-21\",12.6\\r\\n\"1981-11-22\",10.1\\r\\n\"1981-11-23\",15.8\\r\\n\"1981-11-24\",13.6\\r\\n\"1981-11-25\",11.9\\r\\n\"1981-11-26\",9.9\\r\\n\"1981-11-27\",12.6\\r\\n\"1981-11-28\",17.8\\r\\n\"1981-11-29\",15.0\\r\\n\"1981-11-30\",13.6\\r\\n\"1981-12-01\",13.4\\r\\n\"1981-12-02\",10.5\\r\\n\"1981-12-03\",14.2\\r\\n\"1981-12-04\",11.5\\r\\n\"1981-12-05\",13.0\\r\\n\"1981-12-06\",15.0\\r\\n\"1981-12-07\",14.7\\r\\n\"1981-12-08\",12.6\\r\\n\"1981-12-09\",12.5\\r\\n\"1981-12-10\",13.5\\r\\n\"1981-12-11\",14.8\\r\\n\"1981-12-12\",17.2\\r\\n\"1981-12-13\",9.7\\r\\n\"1981-12-14\",12.1\\r\\n\"1981-12-15\",12.8\\r\\n\"1981-12-16\",11.2\\r\\n\"1981-12-17\",16.4\\r\\n\"1981-12-18\",15.6\\r\\n\"1981-12-19\",13.3\\r\\n\"1981-12-20\",11.0\\r\\n\"1981-12-21\",11.1\\r\\n\"1981-12-22\",15.0\\r\\n\"1981-12-23\",12.8\\r\\n\"1981-12-24\",15.0\\r\\n\"1981-12-25\",14.2\\r\\n\"1981-12-26\",14.0\\r\\n\"1981-12-27\",15.5\\r\\n\"1981-12-28\",13.3\\r\\n\"1981-12-29\",15.6\\r\\n\"1981-12-30\",15.2\\r\\n\"1981-12-31\",17.4\\r\\n\"1982-01-01\",17.0\\r\\n\"1982-01-02\",15.0\\r\\n\"1982-01-03\",13.5\\r\\n\"1982-01-04\",15.2\\r\\n\"1982-01-05\",13.0\\r\\n\"1982-01-06\",12.5\\r\\n\"1982-01-07\",14.1\\r\\n\"1982-01-08\",14.8\\r\\n\"1982-01-09\",16.2\\r\\n\"1982-01-10\",15.8\\r\\n\"1982-01-11\",19.1\\r\\n\"1982-01-12\",22.2\\r\\n\"1982-01-13\",15.9\\r\\n\"1982-01-14\",13.0\\r\\n\"1982-01-15\",14.1\\r\\n\"1982-01-16\",15.8\\r\\n\"1982-01-17\",24.0\\r\\n\"1982-01-18\",18.0\\r\\n\"1982-01-19\",19.7\\r\\n\"1982-01-20\",25.2\\r\\n\"1982-01-21\",20.5\\r\\n\"1982-01-22\",19.3\\r\\n\"1982-01-23\",15.8\\r\\n\"1982-01-24\",17.0\\r\\n\"1982-01-25\",18.4\\r\\n\"1982-01-26\",13.3\\r\\n\"1982-01-27\",14.6\\r\\n\"1982-01-28\",12.5\\r\\n\"1982-01-29\",17.0\\r\\n\"1982-01-30\",17.1\\r\\n\"1982-01-31\",14.0\\r\\n\"1982-02-01\",14.6\\r\\n\"1982-02-02\",13.3\\r\\n\"1982-02-03\",14.8\\r\\n\"1982-02-04\",15.1\\r\\n\"1982-02-05\",13.1\\r\\n\"1982-02-06\",13.6\\r\\n\"1982-02-07\",19.5\\r\\n\"1982-02-08\",22.7\\r\\n\"1982-02-09\",17.2\\r\\n\"1982-02-10\",13.5\\r\\n\"1982-02-11\",15.4\\r\\n\"1982-02-12\",17.0\\r\\n\"1982-02-13\",19.2\\r\\n\"1982-02-14\",22.8\\r\\n\"1982-02-15\",26.3\\r\\n\"1982-02-16\",18.2\\r\\n\"1982-02-17\",17.0\\r\\n\"1982-02-18\",14.8\\r\\n\"1982-02-19\",12.8\\r\\n\"1982-02-20\",15.5\\r\\n\"1982-02-21\",15.6\\r\\n\"1982-02-22\",13.1\\r\\n\"1982-02-23\",15.2\\r\\n\"1982-02-24\",14.1\\r\\n\"1982-02-25\",12.5\\r\\n\"1982-02-26\",14.6\\r\\n\"1982-02-27\",10.4\\r\\n\"1982-02-28\",13.9\\r\\n\"1982-03-01\",11.9\\r\\n\"1982-03-02\",13.5\\r\\n\"1982-03-03\",9.8\\r\\n\"1982-03-04\",14.0\\r\\n\"1982-03-05\",21.5\\r\\n\"1982-03-06\",19.5\\r\\n\"1982-03-07\",16.7\\r\\n\"1982-03-08\",19.1\\r\\n\"1982-03-09\",11.0\\r\\n\"1982-03-10\",9.0\\r\\n\"1982-03-11\",10.0\\r\\n\"1982-03-12\",14.6\\r\\n\"1982-03-13\",12.5\\r\\n\"1982-03-14\",17.2\\r\\n\"1982-03-15\",19.2\\r\\n\"1982-03-16\",22.2\\r\\n\"1982-03-17\",15.7\\r\\n\"1982-03-18\",14.2\\r\\n\"1982-03-19\",9.8\\r\\n\"1982-03-20\",14.0\\r\\n\"1982-03-21\",17.5\\r\\n\"1982-03-22\",20.7\\r\\n\"1982-03-23\",15.6\\r\\n\"1982-03-24\",13.2\\r\\n\"1982-03-25\",14.5\\r\\n\"1982-03-26\",16.8\\r\\n\"1982-03-27\",17.2\\r\\n\"1982-03-28\",13.4\\r\\n\"1982-03-29\",14.2\\r\\n\"1982-03-30\",14.3\\r\\n\"1982-03-31\",10.2\\r\\n\"1982-04-01\",10.4\\r\\n\"1982-04-02\",12.3\\r\\n\"1982-04-03\",11.9\\r\\n\"1982-04-04\",11.2\\r\\n\"1982-04-05\",8.5\\r\\n\"1982-04-06\",12.0\\r\\n\"1982-04-07\",12.4\\r\\n\"1982-04-08\",12.9\\r\\n\"1982-04-09\",10.1\\r\\n\"1982-04-10\",15.0\\r\\n\"1982-04-11\",13.6\\r\\n\"1982-04-12\",12.4\\r\\n\"1982-04-13\",13.6\\r\\n\"1982-04-14\",16.1\\r\\n\"1982-04-15\",19.5\\r\\n\"1982-04-16\",14.2\\r\\n\"1982-04-17\",9.3\\r\\n\"1982-04-18\",10.1\\r\\n\"1982-04-19\",7.4\\r\\n\"1982-04-20\",8.6\\r\\n\"1982-04-21\",7.8\\r\\n\"1982-04-22\",9.1\\r\\n\"1982-04-23\",13.0\\r\\n\"1982-04-24\",16.5\\r\\n\"1982-04-25\",12.9\\r\\n\"1982-04-26\",6.9\\r\\n\"1982-04-27\",6.9\\r\\n\"1982-04-28\",8.7\\r\\n\"1982-04-29\",10.0\\r\\n\"1982-04-30\",10.8\\r\\n\"1982-05-01\",7.5\\r\\n\"1982-05-02\",6.3\\r\\n\"1982-05-03\",11.9\\r\\n\"1982-05-04\",13.8\\r\\n\"1982-05-05\",11.8\\r\\n\"1982-05-06\",11.0\\r\\n\"1982-05-07\",10.1\\r\\n\"1982-05-08\",8.5\\r\\n\"1982-05-09\",5.5\\r\\n\"1982-05-10\",7.6\\r\\n\"1982-05-11\",8.7\\r\\n\"1982-05-12\",10.8\\r\\n\"1982-05-13\",11.2\\r\\n\"1982-05-14\",9.1\\r\\n\"1982-05-15\",3.7\\r\\n\"1982-05-16\",4.6\\r\\n\"1982-05-17\",6.6\\r\\n\"1982-05-18\",13.2\\r\\n\"1982-05-19\",15.2\\r\\n\"1982-05-20\",7.6\\r\\n\"1982-05-21\",8.4\\r\\n\"1982-05-22\",6.0\\r\\n\"1982-05-23\",8.3\\r\\n\"1982-05-24\",8.6\\r\\n\"1982-05-25\",11.1\\r\\n\"1982-05-26\",12.1\\r\\n\"1982-05-27\",12.9\\r\\n\"1982-05-28\",14.0\\r\\n\"1982-05-29\",12.5\\r\\n\"1982-05-30\",11.5\\r\\n\"1982-05-31\",7.0\\r\\n\"1982-06-01\",7.1\\r\\n\"1982-06-02\",9.0\\r\\n\"1982-06-03\",3.1\\r\\n\"1982-06-04\",2.5\\r\\n\"1982-06-05\",0.0\\r\\n\"1982-06-06\",1.6\\r\\n\"1982-06-07\",2.6\\r\\n\"1982-06-08\",5.7\\r\\n\"1982-06-09\",2.3\\r\\n\"1982-06-10\",4.5\\r\\n\"1982-06-11\",8.2\\r\\n\"1982-06-12\",6.9\\r\\n\"1982-06-13\",7.3\\r\\n\"1982-06-14\",6.0\\r\\n\"1982-06-15\",7.3\\r\\n\"1982-06-16\",7.6\\r\\n\"1982-06-17\",8.0\\r\\n\"1982-06-18\",8.0\\r\\n\"1982-06-19\",6.8\\r\\n\"1982-06-20\",7.3\\r\\n\"1982-06-21\",6.2\\r\\n\"1982-06-22\",6.9\\r\\n\"1982-06-23\",8.9\\r\\n\"1982-06-24\",4.0\\r\\n\"1982-06-25\",1.3\\r\\n\"1982-06-26\",0.8\\r\\n\"1982-06-27\",4.3\\r\\n\"1982-06-28\",7.3\\r\\n\"1982-06-29\",7.7\\r\\n\"1982-06-30\",9.0\\r\\n\"1982-07-01\",4.2\\r\\n\"1982-07-02\",1.6\\r\\n\"1982-07-03\",2.6\\r\\n\"1982-07-04\",3.4\\r\\n\"1982-07-05\",3.9\\r\\n\"1982-07-06\",7.0\\r\\n\"1982-07-07\",7.8\\r\\n\"1982-07-08\",5.3\\r\\n\"1982-07-09\",2.4\\r\\n\"1982-07-10\",2.8\\r\\n\"1982-07-11\",4.0\\r\\n\"1982-07-12\",7.5\\r\\n\"1982-07-13\",7.8\\r\\n\"1982-07-14\",5.6\\r\\n\"1982-07-15\",3.3\\r\\n\"1982-07-16\",5.0\\r\\n\"1982-07-17\",3.7\\r\\n\"1982-07-18\",3.9\\r\\n\"1982-07-19\",5.2\\r\\n\"1982-07-20\",?0.2\\r\\n\"1982-07-21\",?0.8\\r\\n\"1982-07-22\",0.9\\r\\n\"1982-07-23\",3.5\\r\\n\"1982-07-24\",6.6\\r\\n\"1982-07-25\",9.5\\r\\n\"1982-07-26\",9.0\\r\\n\"1982-07-27\",3.5\\r\\n\"1982-07-28\",4.5\\r\\n\"1982-07-29\",5.7\\r\\n\"1982-07-30\",5.6\\r\\n\"1982-07-31\",7.1\\r\\n\"1982-08-01\",9.7\\r\\n\"1982-08-02\",8.3\\r\\n\"1982-08-03\",9.1\\r\\n\"1982-08-04\",2.8\\r\\n\"1982-08-05\",2.2\\r\\n\"1982-08-06\",4.5\\r\\n\"1982-08-07\",3.8\\r\\n\"1982-08-08\",3.8\\r\\n\"1982-08-09\",6.2\\r\\n\"1982-08-10\",11.5\\r\\n\"1982-08-11\",10.2\\r\\n\"1982-08-12\",7.9\\r\\n\"1982-08-13\",9.0\\r\\n\"1982-08-14\",9.5\\r\\n\"1982-08-15\",6.0\\r\\n\"1982-08-16\",8.2\\r\\n\"1982-08-17\",9.2\\r\\n\"1982-08-18\",4.3\\r\\n\"1982-08-19\",6.6\\r\\n\"1982-08-20\",9.4\\r\\n\"1982-08-21\",13.2\\r\\n\"1982-08-22\",6.6\\r\\n\"1982-08-23\",5.1\\r\\n\"1982-08-24\",12.1\\r\\n\"1982-08-25\",11.2\\r\\n\"1982-08-26\",8.5\\r\\n\"1982-08-27\",4.6\\r\\n\"1982-08-28\",7.0\\r\\n\"1982-08-29\",14.2\\r\\n\"1982-08-30\",12.7\\r\\n\"1982-08-31\",7.6\\r\\n\"1982-09-01\",4.0\\r\\n\"1982-09-02\",10.0\\r\\n\"1982-09-03\",10.5\\r\\n\"1982-09-04\",5.0\\r\\n\"1982-09-05\",4.5\\r\\n\"1982-09-06\",8.2\\r\\n\"1982-09-07\",4.3\\r\\n\"1982-09-08\",9.8\\r\\n\"1982-09-09\",5.8\\r\\n\"1982-09-10\",5.0\\r\\n\"1982-09-11\",8.5\\r\\n\"1982-09-12\",9.0\\r\\n\"1982-09-13\",3.6\\r\\n\"1982-09-14\",6.7\\r\\n\"1982-09-15\",6.7\\r\\n\"1982-09-16\",10.1\\r\\n\"1982-09-17\",15.0\\r\\n\"1982-09-18\",8.9\\r\\n\"1982-09-19\",5.7\\r\\n\"1982-09-20\",4.2\\r\\n\"1982-09-21\",4.0\\r\\n\"1982-09-22\",5.3\\r\\n\"1982-09-23\",6.3\\r\\n\"1982-09-24\",8.5\\r\\n\"1982-09-25\",11.5\\r\\n\"1982-09-26\",7.7\\r\\n\"1982-09-27\",9.2\\r\\n\"1982-09-28\",7.8\\r\\n\"1982-09-29\",6.3\\r\\n\"1982-09-30\",6.3\\r\\n\"1982-10-01\",8.6\\r\\n\"1982-10-02\",6.1\\r\\n\"1982-10-03\",13.2\\r\\n\"1982-10-04\",9.9\\r\\n\"1982-10-05\",4.7\\r\\n\"1982-10-06\",5.8\\r\\n\"1982-10-07\",14.9\\r\\n\"1982-10-08\",10.7\\r\\n\"1982-10-09\",8.6\\r\\n\"1982-10-10\",9.4\\r\\n\"1982-10-11\",5.7\\r\\n\"1982-10-12\",10.9\\r\\n\"1982-10-13\",13.1\\r\\n\"1982-10-14\",10.4\\r\\n\"1982-10-15\",8.2\\r\\n\"1982-10-16\",9.8\\r\\n\"1982-10-17\",7.5\\r\\n\"1982-10-18\",5.8\\r\\n\"1982-10-19\",9.8\\r\\n\"1982-10-20\",7.9\\r\\n\"1982-10-21\",8.7\\r\\n\"1982-10-22\",10.0\\r\\n\"1982-10-23\",10.6\\r\\n\"1982-10-24\",8.0\\r\\n\"1982-10-25\",10.2\\r\\n\"1982-10-26\",15.1\\r\\n\"1982-10-27\",13.9\\r\\n\"1982-10-28\",9.2\\r\\n\"1982-10-29\",9.0\\r\\n\"1982-10-30\",13.2\\r\\n\"1982-10-31\",7.0\\r\\n\"1982-11-01\",10.6\\r\\n\"1982-11-02\",6.9\\r\\n\"1982-11-03\",9.5\\r\\n\"1982-11-04\",12.5\\r\\n\"1982-11-05\",13.6\\r\\n\"1982-11-06\",17.7\\r\\n\"1982-11-07\",16.0\\r\\n\"1982-11-08\",11.3\\r\\n\"1982-11-09\",10.5\\r\\n\"1982-11-10\",14.4\\r\\n\"1982-11-11\",10.3\\r\\n\"1982-11-12\",9.0\\r\\n\"1982-11-13\",11.1\\r\\n\"1982-11-14\",14.5\\r\\n\"1982-11-15\",18.0\\r\\n\"1982-11-16\",12.8\\r\\n\"1982-11-17\",10.7\\r\\n\"1982-11-18\",9.1\\r\\n\"1982-11-19\",8.7\\r\\n\"1982-11-20\",12.4\\r\\n\"1982-11-21\",12.6\\r\\n\"1982-11-22\",10.3\\r\\n\"1982-11-23\",13.7\\r\\n\"1982-11-24\",16.0\\r\\n\"1982-11-25\",15.8\\r\\n\"1982-11-26\",12.1\\r\\n\"1982-11-27\",12.5\\r\\n\"1982-11-28\",12.2\\r\\n\"1982-11-29\",13.7\\r\\n\"1982-11-30\",16.1\\r\\n\"1982-12-01\",15.5\\r\\n\"1982-12-02\",10.3\\r\\n\"1982-12-03\",10.5\\r\\n\"1982-12-04\",11.0\\r\\n\"1982-12-05\",11.9\\r\\n\"1982-12-06\",13.0\\r\\n\"1982-12-07\",12.2\\r\\n\"1982-12-08\",10.6\\r\\n\"1982-12-09\",13.0\\r\\n\"1982-12-10\",13.0\\r\\n\"1982-12-11\",12.2\\r\\n\"1982-12-12\",12.6\\r\\n\"1982-12-13\",18.7\\r\\n\"1982-12-14\",15.2\\r\\n\"1982-12-15\",15.3\\r\\n\"1982-12-16\",13.9\\r\\n\"1982-12-17\",15.8\\r\\n\"1982-12-18\",13.0\\r\\n\"1982-12-19\",13.0\\r\\n\"1982-12-20\",13.7\\r\\n\"1982-12-21\",12.0\\r\\n\"1982-12-22\",10.8\\r\\n\"1982-12-23\",15.6\\r\\n\"1982-12-24\",15.3\\r\\n\"1982-12-25\",13.9\\r\\n\"1982-12-26\",13.0\\r\\n\"1982-12-27\",15.3\\r\\n\"1982-12-28\",16.3\\r\\n\"1982-12-29\",15.8\\r\\n\"1982-12-30\",17.7\\r\\n\"1982-12-31\",16.3\\r\\n\"1983-01-01\",18.4\\r\\n\"1983-01-02\",15.0\\r\\n\"1983-01-03\",10.9\\r\\n\"1983-01-04\",11.4\\r\\n\"1983-01-05\",14.8\\r\\n\"1983-01-06\",12.1\\r\\n\"1983-01-07\",12.8\\r\\n\"1983-01-08\",16.2\\r\\n\"1983-01-09\",15.5\\r\\n\"1983-01-10\",13.0\\r\\n\"1983-01-11\",10.5\\r\\n\"1983-01-12\",9.1\\r\\n\"1983-01-13\",10.5\\r\\n\"1983-01-14\",11.8\\r\\n\"1983-01-15\",12.7\\r\\n\"1983-01-16\",12.7\\r\\n\"1983-01-17\",11.5\\r\\n\"1983-01-18\",13.8\\r\\n\"1983-01-19\",13.3\\r\\n\"1983-01-20\",11.6\\r\\n\"1983-01-21\",15.4\\r\\n\"1983-01-22\",12.4\\r\\n\"1983-01-23\",16.9\\r\\n\"1983-01-24\",14.7\\r\\n\"1983-01-25\",10.6\\r\\n\"1983-01-26\",15.6\\r\\n\"1983-01-27\",10.7\\r\\n\"1983-01-28\",12.6\\r\\n\"1983-01-29\",13.8\\r\\n\"1983-01-30\",14.3\\r\\n\"1983-01-31\",14.0\\r\\n\"1983-02-01\",18.1\\r\\n\"1983-02-02\",17.3\\r\\n\"1983-02-03\",13.0\\r\\n\"1983-02-04\",16.0\\r\\n\"1983-02-05\",14.9\\r\\n\"1983-02-06\",16.2\\r\\n\"1983-02-07\",20.3\\r\\n\"1983-02-08\",22.5\\r\\n\"1983-02-09\",17.2\\r\\n\"1983-02-10\",15.9\\r\\n\"1983-02-11\",16.8\\r\\n\"1983-02-12\",13.8\\r\\n\"1983-02-13\",12.8\\r\\n\"1983-02-14\",14.0\\r\\n\"1983-02-15\",17.5\\r\\n\"1983-02-16\",21.5\\r\\n\"1983-02-17\",16.8\\r\\n\"1983-02-18\",13.6\\r\\n\"1983-02-19\",14.5\\r\\n\"1983-02-20\",14.2\\r\\n\"1983-02-21\",15.7\\r\\n\"1983-02-22\",19.7\\r\\n\"1983-02-23\",17.4\\r\\n\"1983-02-24\",14.4\\r\\n\"1983-02-25\",16.9\\r\\n\"1983-02-26\",19.1\\r\\n\"1983-02-27\",20.4\\r\\n\"1983-02-28\",20.1\\r\\n\"1983-03-01\",19.9\\r\\n\"1983-03-02\",22.0\\r\\n\"1983-03-03\",20.5\\r\\n\"1983-03-04\",22.1\\r\\n\"1983-03-05\",20.6\\r\\n\"1983-03-06\",15.0\\r\\n\"1983-03-07\",20.6\\r\\n\"1983-03-08\",21.5\\r\\n\"1983-03-09\",16.2\\r\\n\"1983-03-10\",14.1\\r\\n\"1983-03-11\",14.5\\r\\n\"1983-03-12\",21.1\\r\\n\"1983-03-13\",15.9\\r\\n\"1983-03-14\",15.2\\r\\n\"1983-03-15\",13.1\\r\\n\"1983-03-16\",13.2\\r\\n\"1983-03-17\",12.5\\r\\n\"1983-03-18\",15.2\\r\\n\"1983-03-19\",17.6\\r\\n\"1983-03-20\",15.5\\r\\n\"1983-03-21\",16.7\\r\\n\"1983-03-22\",16.3\\r\\n\"1983-03-23\",15.1\\r\\n\"1983-03-24\",12.7\\r\\n\"1983-03-25\",10.0\\r\\n\"1983-03-26\",11.4\\r\\n\"1983-03-27\",12.6\\r\\n\"1983-03-28\",10.7\\r\\n\"1983-03-29\",10.0\\r\\n\"1983-03-30\",13.9\\r\\n\"1983-03-31\",13.4\\r\\n\"1983-04-01\",12.5\\r\\n\"1983-04-02\",12.8\\r\\n\"1983-04-03\",7.8\\r\\n\"1983-04-04\",11.1\\r\\n\"1983-04-05\",10.7\\r\\n\"1983-04-06\",7.1\\r\\n\"1983-04-07\",6.7\\r\\n\"1983-04-08\",5.7\\r\\n\"1983-04-09\",9.1\\r\\n\"1983-04-10\",15.2\\r\\n\"1983-04-11\",15.5\\r\\n\"1983-04-12\",11.1\\r\\n\"1983-04-13\",11.7\\r\\n\"1983-04-14\",11.5\\r\\n\"1983-04-15\",9.8\\r\\n\"1983-04-16\",6.2\\r\\n\"1983-04-17\",6.7\\r\\n\"1983-04-18\",7.5\\r\\n\"1983-04-19\",8.8\\r\\n\"1983-04-20\",8.0\\r\\n\"1983-04-21\",10.4\\r\\n\"1983-04-22\",14.5\\r\\n\"1983-04-23\",16.5\\r\\n\"1983-04-24\",14.1\\r\\n\"1983-04-25\",10.5\\r\\n\"1983-04-26\",12.6\\r\\n\"1983-04-27\",13.0\\r\\n\"1983-04-28\",8.7\\r\\n\"1983-04-29\",10.1\\r\\n\"1983-04-30\",12.0\\r\\n\"1983-05-01\",12.5\\r\\n\"1983-05-02\",13.5\\r\\n\"1983-05-03\",13.7\\r\\n\"1983-05-04\",13.5\\r\\n\"1983-05-05\",10.7\\r\\n\"1983-05-06\",13.0\\r\\n\"1983-05-07\",11.6\\r\\n\"1983-05-08\",13.0\\r\\n\"1983-05-09\",11.2\\r\\n\"1983-05-10\",13.5\\r\\n\"1983-05-11\",12.9\\r\\n\"1983-05-12\",6.8\\r\\n\"1983-05-13\",10.0\\r\\n\"1983-05-14\",14.5\\r\\n\"1983-05-15\",11.7\\r\\n\"1983-05-16\",6.7\\r\\n\"1983-05-17\",4.6\\r\\n\"1983-05-18\",4.9\\r\\n\"1983-05-19\",7.4\\r\\n\"1983-05-20\",8.3\\r\\n\"1983-05-21\",7.5\\r\\n\"1983-05-22\",6.2\\r\\n\"1983-05-23\",7.8\\r\\n\"1983-05-24\",13.2\\r\\n\"1983-05-25\",11.9\\r\\n\"1983-05-26\",6.5\\r\\n\"1983-05-27\",8.3\\r\\n\"1983-05-28\",12.1\\r\\n\"1983-05-29\",9.3\\r\\n\"1983-05-30\",7.5\\r\\n\"1983-05-31\",9.3\\r\\n\"1983-06-01\",11.0\\r\\n\"1983-06-02\",10.8\\r\\n\"1983-06-03\",5.3\\r\\n\"1983-06-04\",7.6\\r\\n\"1983-06-05\",5.6\\r\\n\"1983-06-06\",7.2\\r\\n\"1983-06-07\",9.6\\r\\n\"1983-06-08\",7.0\\r\\n\"1983-06-09\",8.3\\r\\n\"1983-06-10\",7.8\\r\\n\"1983-06-11\",4.7\\r\\n\"1983-06-12\",6.8\\r\\n\"1983-06-13\",7.2\\r\\n\"1983-06-14\",8.3\\r\\n\"1983-06-15\",9.5\\r\\n\"1983-06-16\",4.7\\r\\n\"1983-06-17\",3.0\\r\\n\"1983-06-18\",1.5\\r\\n\"1983-06-19\",2.5\\r\\n\"1983-06-20\",6.2\\r\\n\"1983-06-21\",11.6\\r\\n\"1983-06-22\",6.6\\r\\n\"1983-06-23\",6.6\\r\\n\"1983-06-24\",8.0\\r\\n\"1983-06-25\",7.9\\r\\n\"1983-06-26\",3.3\\r\\n\"1983-06-27\",3.9\\r\\n\"1983-06-28\",6.0\\r\\n\"1983-06-29\",4.0\\r\\n\"1983-06-30\",5.5\\r\\n\"1983-07-01\",8.5\\r\\n\"1983-07-02\",9.8\\r\\n\"1983-07-03\",9.5\\r\\n\"1983-07-04\",7.2\\r\\n\"1983-07-05\",8.1\\r\\n\"1983-07-06\",8.0\\r\\n\"1983-07-07\",8.5\\r\\n\"1983-07-08\",8.8\\r\\n\"1983-07-09\",8.3\\r\\n\"1983-07-10\",2.4\\r\\n\"1983-07-11\",4.9\\r\\n\"1983-07-12\",5.9\\r\\n\"1983-07-13\",6.7\\r\\n\"1983-07-14\",8.4\\r\\n\"1983-07-15\",6.5\\r\\n\"1983-07-16\",7.9\\r\\n\"1983-07-17\",4.1\\r\\n\"1983-07-18\",5.4\\r\\n\"1983-07-19\",7.5\\r\\n\"1983-07-20\",3.9\\r\\n\"1983-07-21\",2.5\\r\\n\"1983-07-22\",5.3\\r\\n\"1983-07-23\",6.6\\r\\n\"1983-07-24\",0.0\\r\\n\"1983-07-25\",0.7\\r\\n\"1983-07-26\",7.6\\r\\n\"1983-07-27\",12.3\\r\\n\"1983-07-28\",9.2\\r\\n\"1983-07-29\",9.6\\r\\n\"1983-07-30\",9.5\\r\\n\"1983-07-31\",10.0\\r\\n\"1983-08-01\",7.7\\r\\n\"1983-08-02\",8.0\\r\\n\"1983-08-03\",8.3\\r\\n\"1983-08-04\",8.3\\r\\n\"1983-08-05\",4.5\\r\\n\"1983-08-06\",6.5\\r\\n\"1983-08-07\",9.4\\r\\n\"1983-08-08\",9.4\\r\\n\"1983-08-09\",10.5\\r\\n\"1983-08-10\",10.7\\r\\n\"1983-08-11\",9.9\\r\\n\"1983-08-12\",7.6\\r\\n\"1983-08-13\",5.8\\r\\n\"1983-08-14\",8.5\\r\\n\"1983-08-15\",13.8\\r\\n\"1983-08-16\",14.3\\r\\n\"1983-08-17\",8.3\\r\\n\"1983-08-18\",5.3\\r\\n\"1983-08-19\",3.0\\r\\n\"1983-08-20\",5.2\\r\\n\"1983-08-21\",10.3\\r\\n\"1983-08-22\",11.1\\r\\n\"1983-08-23\",10.5\\r\\n\"1983-08-24\",9.0\\r\\n\"1983-08-25\",13.0\\r\\n\"1983-08-26\",6.4\\r\\n\"1983-08-27\",8.4\\r\\n\"1983-08-28\",6.7\\r\\n\"1983-08-29\",8.3\\r\\n\"1983-08-30\",11.2\\r\\n\"1983-08-31\",10.0\\r\\n\"1983-09-01\",10.1\\r\\n\"1983-09-02\",10.6\\r\\n\"1983-09-03\",10.9\\r\\n\"1983-09-04\",5.7\\r\\n\"1983-09-05\",9.5\\r\\n\"1983-09-06\",10.4\\r\\n\"1983-09-07\",11.1\\r\\n\"1983-09-08\",12.2\\r\\n\"1983-09-09\",10.6\\r\\n\"1983-09-10\",8.8\\r\\n\"1983-09-11\",9.2\\r\\n\"1983-09-12\",5.5\\r\\n\"1983-09-13\",7.1\\r\\n\"1983-09-14\",6.5\\r\\n\"1983-09-15\",4.3\\r\\n\"1983-09-16\",5.0\\r\\n\"1983-09-17\",11.2\\r\\n\"1983-09-18\",7.5\\r\\n\"1983-09-19\",12.0\\r\\n\"1983-09-20\",13.6\\r\\n\"1983-09-21\",8.3\\r\\n\"1983-09-22\",8.5\\r\\n\"1983-09-23\",12.9\\r\\n\"1983-09-24\",7.7\\r\\n\"1983-09-25\",7.6\\r\\n\"1983-09-26\",3.5\\r\\n\"1983-09-27\",10.4\\r\\n\"1983-09-28\",15.4\\r\\n\"1983-09-29\",10.6\\r\\n\"1983-09-30\",9.6\\r\\n\"1983-10-01\",9.3\\r\\n\"1983-10-02\",13.9\\r\\n\"1983-10-03\",7.7\\r\\n\"1983-10-04\",9.5\\r\\n\"1983-10-05\",7.6\\r\\n\"1983-10-06\",6.9\\r\\n\"1983-10-07\",6.8\\r\\n\"1983-10-08\",5.8\\r\\n\"1983-10-09\",6.0\\r\\n\"1983-10-10\",8.3\\r\\n\"1983-10-11\",9.1\\r\\n\"1983-10-12\",12.5\\r\\n\"1983-10-13\",13.2\\r\\n\"1983-10-14\",16.2\\r\\n\"1983-10-15\",12.5\\r\\n\"1983-10-16\",11.8\\r\\n\"1983-10-17\",10.6\\r\\n\"1983-10-18\",10.0\\r\\n\"1983-10-19\",12.2\\r\\n\"1983-10-20\",8.9\\r\\n\"1983-10-21\",10.3\\r\\n\"1983-10-22\",7.5\\r\\n\"1983-10-23\",11.6\\r\\n\"1983-10-24\",12.6\\r\\n\"1983-10-25\",12.9\\r\\n\"1983-10-26\",11.7\\r\\n\"1983-10-27\",14.0\\r\\n\"1983-10-28\",12.3\\r\\n\"1983-10-29\",9.0\\r\\n\"1983-10-30\",9.2\\r\\n\"1983-10-31\",9.8\\r\\n\"1983-11-01\",11.8\\r\\n\"1983-11-02\",10.6\\r\\n\"1983-11-03\",12.6\\r\\n\"1983-11-04\",11.0\\r\\n\"1983-11-05\",8.2\\r\\n\"1983-11-06\",7.5\\r\\n\"1983-11-07\",13.6\\r\\n\"1983-11-08\",14.8\\r\\n\"1983-11-09\",10.9\\r\\n\"1983-11-10\",7.7\\r\\n\"1983-11-11\",10.2\\r\\n\"1983-11-12\",10.8\\r\\n\"1983-11-13\",10.8\\r\\n\"1983-11-14\",12.5\\r\\n\"1983-11-15\",13.2\\r\\n\"1983-11-16\",8.7\\r\\n\"1983-11-17\",5.7\\r\\n\"1983-11-18\",9.8\\r\\n\"1983-11-19\",7.3\\r\\n\"1983-11-20\",10.8\\r\\n\"1983-11-21\",10.0\\r\\n\"1983-11-22\",16.2\\r\\n\"1983-11-23\",15.0\\r\\n\"1983-11-24\",14.5\\r\\n\"1983-11-25\",15.9\\r\\n\"1983-11-26\",14.9\\r\\n\"1983-11-27\",14.2\\r\\n\"1983-11-28\",15.8\\r\\n\"1983-11-29\",17.2\\r\\n\"1983-11-30\",17.6\\r\\n\"1983-12-01\",12.1\\r\\n\"1983-12-02\",11.4\\r\\n\"1983-12-03\",13.0\\r\\n\"1983-12-04\",13.2\\r\\n\"1983-12-05\",12.0\\r\\n\"1983-12-06\",15.3\\r\\n\"1983-12-07\",12.7\\r\\n\"1983-12-08\",12.1\\r\\n\"1983-12-09\",13.8\\r\\n\"1983-12-10\",10.9\\r\\n\"1983-12-11\",12.0\\r\\n\"1983-12-12\",16.5\\r\\n\"1983-12-13\",15.0\\r\\n\"1983-12-14\",11.2\\r\\n\"1983-12-15\",13.9\\r\\n\"1983-12-16\",15.0\\r\\n\"1983-12-17\",14.8\\r\\n\"1983-12-18\",15.0\\r\\n\"1983-12-19\",13.3\\r\\n\"1983-12-20\",20.4\\r\\n\"1983-12-21\",18.0\\r\\n\"1983-12-22\",12.2\\r\\n\"1983-12-23\",16.7\\r\\n\"1983-12-24\",13.8\\r\\n\"1983-12-25\",17.5\\r\\n\"1983-12-26\",15.0\\r\\n\"1983-12-27\",13.9\\r\\n\"1983-12-28\",11.1\\r\\n\"1983-12-29\",16.1\\r\\n\"1983-12-30\",20.4\\r\\n\"1983-12-31\",18.0\\r\\n\"1984-01-01\",19.5\\r\\n\"1984-01-02\",17.1\\r\\n\"1984-01-03\",17.1\\r\\n\"1984-01-04\",12.0\\r\\n\"1984-01-05\",11.0\\r\\n\"1984-01-06\",16.3\\r\\n\"1984-01-07\",16.1\\r\\n\"1984-01-08\",13.0\\r\\n\"1984-01-09\",13.4\\r\\n\"1984-01-10\",15.2\\r\\n\"1984-01-11\",12.5\\r\\n\"1984-01-12\",14.3\\r\\n\"1984-01-13\",16.5\\r\\n\"1984-01-14\",18.6\\r\\n\"1984-01-15\",18.0\\r\\n\"1984-01-16\",18.2\\r\\n\"1984-01-17\",11.4\\r\\n\"1984-01-18\",11.9\\r\\n\"1984-01-19\",12.2\\r\\n\"1984-01-20\",14.8\\r\\n\"1984-01-21\",13.1\\r\\n\"1984-01-22\",12.7\\r\\n\"1984-01-23\",10.5\\r\\n\"1984-01-24\",13.8\\r\\n\"1984-01-25\",18.8\\r\\n\"1984-01-26\",13.9\\r\\n\"1984-01-27\",11.2\\r\\n\"1984-01-28\",10.6\\r\\n\"1984-01-29\",14.7\\r\\n\"1984-01-30\",13.1\\r\\n\"1984-01-31\",12.1\\r\\n\"1984-02-01\",14.7\\r\\n\"1984-02-02\",11.1\\r\\n\"1984-02-03\",13.0\\r\\n\"1984-02-04\",15.6\\r\\n\"1984-02-05\",14.2\\r\\n\"1984-02-06\",15.5\\r\\n\"1984-02-07\",18.0\\r\\n\"1984-02-08\",15.0\\r\\n\"1984-02-09\",15.9\\r\\n\"1984-02-10\",15.5\\r\\n\"1984-02-11\",15.8\\r\\n\"1984-02-12\",16.6\\r\\n\"1984-02-13\",13.6\\r\\n\"1984-02-14\",13.8\\r\\n\"1984-02-15\",14.6\\r\\n\"1984-02-16\",15.6\\r\\n\"1984-02-17\",16.6\\r\\n\"1984-02-18\",14.3\\r\\n\"1984-02-19\",16.3\\r\\n\"1984-02-20\",18.9\\r\\n\"1984-02-21\",18.7\\r\\n\"1984-02-22\",14.5\\r\\n\"1984-02-23\",16.5\\r\\n\"1984-02-24\",14.1\\r\\n\"1984-02-25\",13.5\\r\\n\"1984-02-26\",11.7\\r\\n\"1984-02-27\",15.1\\r\\n\"1984-02-28\",11.2\\r\\n\"1984-02-29\",13.5\\r\\n\"1984-03-01\",12.6\\r\\n\"1984-03-02\",8.8\\r\\n\"1984-03-03\",10.5\\r\\n\"1984-03-04\",12.1\\r\\n\"1984-03-05\",14.5\\r\\n\"1984-03-06\",19.5\\r\\n\"1984-03-07\",14.0\\r\\n\"1984-03-08\",13.8\\r\\n\"1984-03-09\",10.5\\r\\n\"1984-03-10\",13.8\\r\\n\"1984-03-11\",11.4\\r\\n\"1984-03-12\",15.6\\r\\n\"1984-03-13\",11.1\\r\\n\"1984-03-14\",12.1\\r\\n\"1984-03-15\",14.2\\r\\n\"1984-03-16\",10.9\\r\\n\"1984-03-17\",14.2\\r\\n\"1984-03-18\",13.8\\r\\n\"1984-03-19\",15.1\\r\\n\"1984-03-20\",14.0\\r\\n\"1984-03-21\",12.1\\r\\n\"1984-03-22\",13.8\\r\\n\"1984-03-23\",16.6\\r\\n\"1984-03-24\",17.8\\r\\n\"1984-03-25\",9.4\\r\\n\"1984-03-26\",10.2\\r\\n\"1984-03-27\",7.4\\r\\n\"1984-03-28\",8.7\\r\\n\"1984-03-29\",14.0\\r\\n\"1984-03-30\",15.3\\r\\n\"1984-03-31\",11.1\\r\\n\"1984-04-01\",9.7\\r\\n\"1984-04-02\",10.3\\r\\n\"1984-04-03\",9.2\\r\\n\"1984-04-04\",8.2\\r\\n\"1984-04-05\",9.7\\r\\n\"1984-04-06\",12.4\\r\\n\"1984-04-07\",12.5\\r\\n\"1984-04-08\",9.0\\r\\n\"1984-04-09\",9.7\\r\\n\"1984-04-10\",10.1\\r\\n\"1984-04-11\",11.2\\r\\n\"1984-04-12\",12.0\\r\\n\"1984-04-13\",11.1\\r\\n\"1984-04-14\",10.8\\r\\n\"1984-04-15\",12.8\\r\\n\"1984-04-16\",9.8\\r\\n\"1984-04-17\",13.7\\r\\n\"1984-04-18\",11.0\\r\\n\"1984-04-19\",13.2\\r\\n\"1984-04-20\",13.0\\r\\n\"1984-04-21\",10.2\\r\\n\"1984-04-22\",13.2\\r\\n\"1984-04-23\",9.3\\r\\n\"1984-04-24\",11.1\\r\\n\"1984-04-25\",10.3\\r\\n\"1984-04-26\",8.7\\r\\n\"1984-04-27\",11.7\\r\\n\"1984-04-28\",12.5\\r\\n\"1984-04-29\",6.5\\r\\n\"1984-04-30\",9.6\\r\\n\"1984-05-01\",13.8\\r\\n\"1984-05-02\",14.7\\r\\n\"1984-05-03\",9.1\\r\\n\"1984-05-04\",4.8\\r\\n\"1984-05-05\",3.3\\r\\n\"1984-05-06\",3.5\\r\\n\"1984-05-07\",5.7\\r\\n\"1984-05-08\",5.5\\r\\n\"1984-05-09\",7.0\\r\\n\"1984-05-10\",9.5\\r\\n\"1984-05-11\",9.9\\r\\n\"1984-05-12\",4.9\\r\\n\"1984-05-13\",6.3\\r\\n\"1984-05-14\",4.8\\r\\n\"1984-05-15\",6.2\\r\\n\"1984-05-16\",7.1\\r\\n\"1984-05-17\",7.5\\r\\n\"1984-05-18\",9.4\\r\\n\"1984-05-19\",8.7\\r\\n\"1984-05-20\",9.5\\r\\n\"1984-05-21\",12.1\\r\\n\"1984-05-22\",9.5\\r\\n\"1984-05-23\",9.3\\r\\n\"1984-05-24\",8.5\\r\\n\"1984-05-25\",8.0\\r\\n\"1984-05-26\",9.8\\r\\n\"1984-05-27\",6.2\\r\\n\"1984-05-28\",7.3\\r\\n\"1984-05-29\",10.9\\r\\n\"1984-05-30\",10.0\\r\\n\"1984-05-31\",8.7\\r\\n\"1984-06-01\",9.0\\r\\n\"1984-06-02\",10.8\\r\\n\"1984-06-03\",12.4\\r\\n\"1984-06-04\",7.2\\r\\n\"1984-06-05\",7.2\\r\\n\"1984-06-06\",11.1\\r\\n\"1984-06-07\",9.3\\r\\n\"1984-06-08\",10.1\\r\\n\"1984-06-09\",3.9\\r\\n\"1984-06-10\",5.0\\r\\n\"1984-06-11\",8.2\\r\\n\"1984-06-12\",2.8\\r\\n\"1984-06-13\",4.3\\r\\n\"1984-06-14\",8.1\\r\\n\"1984-06-15\",11.1\\r\\n\"1984-06-16\",4.7\\r\\n\"1984-06-17\",5.3\\r\\n\"1984-06-18\",10.0\\r\\n\"1984-06-19\",5.6\\r\\n\"1984-06-20\",2.2\\r\\n\"1984-06-21\",7.1\\r\\n\"1984-06-22\",8.3\\r\\n\"1984-06-23\",8.6\\r\\n\"1984-06-24\",10.1\\r\\n\"1984-06-25\",8.3\\r\\n\"1984-06-26\",7.2\\r\\n\"1984-06-27\",7.7\\r\\n\"1984-06-28\",7.8\\r\\n\"1984-06-29\",9.1\\r\\n\"1984-06-30\",9.4\\r\\n\"1984-07-01\",7.8\\r\\n\"1984-07-02\",2.6\\r\\n\"1984-07-03\",2.4\\r\\n\"1984-07-04\",3.9\\r\\n\"1984-07-05\",1.3\\r\\n\"1984-07-06\",2.1\\r\\n\"1984-07-07\",7.4\\r\\n\"1984-07-08\",7.2\\r\\n\"1984-07-09\",8.8\\r\\n\"1984-07-10\",8.9\\r\\n\"1984-07-11\",8.8\\r\\n\"1984-07-12\",8.0\\r\\n\"1984-07-13\",0.7\\r\\n\"1984-07-14\",?0.1\\r\\n\"1984-07-15\",0.9\\r\\n\"1984-07-16\",7.8\\r\\n\"1984-07-17\",7.2\\r\\n\"1984-07-18\",8.0\\r\\n\"1984-07-19\",4.6\\r\\n\"1984-07-20\",5.2\\r\\n\"1984-07-21\",5.8\\r\\n\"1984-07-22\",6.8\\r\\n\"1984-07-23\",8.1\\r\\n\"1984-07-24\",7.5\\r\\n\"1984-07-25\",5.4\\r\\n\"1984-07-26\",4.6\\r\\n\"1984-07-27\",6.4\\r\\n\"1984-07-28\",9.7\\r\\n\"1984-07-29\",7.0\\r\\n\"1984-07-30\",10.0\\r\\n\"1984-07-31\",10.6\\r\\n\"1984-08-01\",11.5\\r\\n\"1984-08-02\",10.2\\r\\n\"1984-08-03\",11.1\\r\\n\"1984-08-04\",11.0\\r\\n\"1984-08-05\",8.9\\r\\n\"1984-08-06\",9.9\\r\\n\"1984-08-07\",11.7\\r\\n\"1984-08-08\",11.6\\r\\n\"1984-08-09\",9.0\\r\\n\"1984-08-10\",6.3\\r\\n\"1984-08-11\",8.7\\r\\n\"1984-08-12\",8.5\\r\\n\"1984-08-13\",8.5\\r\\n\"1984-08-14\",8.0\\r\\n\"1984-08-15\",6.0\\r\\n\"1984-08-16\",8.0\\r\\n\"1984-08-17\",8.5\\r\\n\"1984-08-18\",7.7\\r\\n\"1984-08-19\",8.4\\r\\n\"1984-08-20\",9.0\\r\\n\"1984-08-21\",8.3\\r\\n\"1984-08-22\",6.8\\r\\n\"1984-08-23\",9.3\\r\\n\"1984-08-24\",6.7\\r\\n\"1984-08-25\",9.0\\r\\n\"1984-08-26\",7.3\\r\\n\"1984-08-27\",6.3\\r\\n\"1984-08-28\",7.9\\r\\n\"1984-08-29\",5.2\\r\\n\"1984-08-30\",9.0\\r\\n\"1984-08-31\",11.3\\r\\n\"1984-09-01\",9.2\\r\\n\"1984-09-02\",11.3\\r\\n\"1984-09-03\",7.0\\r\\n\"1984-09-04\",8.0\\r\\n\"1984-09-05\",4.6\\r\\n\"1984-09-06\",8.5\\r\\n\"1984-09-07\",9.5\\r\\n\"1984-09-08\",9.4\\r\\n\"1984-09-09\",10.5\\r\\n\"1984-09-10\",9.7\\r\\n\"1984-09-11\",4.9\\r\\n\"1984-09-12\",8.0\\r\\n\"1984-09-13\",5.8\\r\\n\"1984-09-14\",5.5\\r\\n\"1984-09-15\",10.9\\r\\n\"1984-09-16\",11.7\\r\\n\"1984-09-17\",9.2\\r\\n\"1984-09-18\",8.9\\r\\n\"1984-09-19\",11.3\\r\\n\"1984-09-20\",8.6\\r\\n\"1984-09-21\",6.2\\r\\n\"1984-09-22\",6.6\\r\\n\"1984-09-23\",9.1\\r\\n\"1984-09-24\",6.1\\r\\n\"1984-09-25\",7.5\\r\\n\"1984-09-26\",10.7\\r\\n\"1984-09-27\",6.3\\r\\n\"1984-09-28\",5.5\\r\\n\"1984-09-29\",6.7\\r\\n\"1984-09-30\",4.2\\r\\n\"1984-10-01\",11.3\\r\\n\"1984-10-02\",16.3\\r\\n\"1984-10-03\",10.5\\r\\n\"1984-10-04\",10.3\\r\\n\"1984-10-05\",7.9\\r\\n\"1984-10-06\",7.7\\r\\n\"1984-10-07\",16.0\\r\\n\"1984-10-08\",14.6\\r\\n\"1984-10-09\",12.5\\r\\n\"1984-10-10\",8.1\\r\\n\"1984-10-11\",12.2\\r\\n\"1984-10-12\",17.2\\r\\n\"1984-10-13\",9.4\\r\\n\"1984-10-14\",8.7\\r\\n\"1984-10-15\",5.9\\r\\n\"1984-10-16\",4.8\\r\\n\"1984-10-17\",7.4\\r\\n\"1984-10-18\",9.4\\r\\n\"1984-10-19\",9.7\\r\\n\"1984-10-20\",9.9\\r\\n\"1984-10-21\",6.5\\r\\n\"1984-10-22\",9.8\\r\\n\"1984-10-23\",18.2\\r\\n\"1984-10-24\",11.3\\r\\n\"1984-10-25\",9.1\\r\\n\"1984-10-26\",9.6\\r\\n\"1984-10-27\",13.5\\r\\n\"1984-10-28\",10.7\\r\\n\"1984-10-29\",10.0\\r\\n\"1984-10-30\",8.5\\r\\n\"1984-10-31\",12.6\\r\\n\"1984-11-01\",16.6\\r\\n\"1984-11-02\",11.6\\r\\n\"1984-11-03\",12.2\\r\\n\"1984-11-04\",11.2\\r\\n\"1984-11-05\",9.2\\r\\n\"1984-11-06\",9.9\\r\\n\"1984-11-07\",11.9\\r\\n\"1984-11-08\",15.6\\r\\n\"1984-11-09\",19.0\\r\\n\"1984-11-10\",12.8\\r\\n\"1984-11-11\",12.2\\r\\n\"1984-11-12\",12.0\\r\\n\"1984-11-13\",11.1\\r\\n\"1984-11-14\",11.8\\r\\n\"1984-11-15\",7.6\\r\\n\"1984-11-16\",13.0\\r\\n\"1984-11-17\",12.7\\r\\n\"1984-11-18\",16.0\\r\\n\"1984-11-19\",14.8\\r\\n\"1984-11-20\",14.2\\r\\n\"1984-11-21\",10.0\\r\\n\"1984-11-22\",8.8\\r\\n\"1984-11-23\",11.6\\r\\n\"1984-11-24\",8.6\\r\\n\"1984-11-25\",14.6\\r\\n\"1984-11-26\",24.3\\r\\n\"1984-11-27\",11.6\\r\\n\"1984-11-28\",10.8\\r\\n\"1984-11-29\",12.0\\r\\n\"1984-11-30\",11.0\\r\\n\"1984-12-01\",12.6\\r\\n\"1984-12-02\",10.8\\r\\n\"1984-12-03\",9.1\\r\\n\"1984-12-04\",11.0\\r\\n\"1984-12-05\",13.0\\r\\n\"1984-12-06\",12.8\\r\\n\"1984-12-07\",9.9\\r\\n\"1984-12-08\",11.6\\r\\n\"1984-12-09\",10.5\\r\\n\"1984-12-10\",15.9\\r\\n\"1984-12-11\",12.2\\r\\n\"1984-12-12\",13.0\\r\\n\"1984-12-13\",12.5\\r\\n\"1984-12-14\",12.5\\r\\n\"1984-12-15\",11.4\\r\\n\"1984-12-16\",12.1\\r\\n\"1984-12-17\",16.8\\r\\n\"1984-12-18\",12.1\\r\\n\"1984-12-19\",11.3\\r\\n\"1984-12-20\",10.4\\r\\n\"1984-12-21\",14.2\\r\\n\"1984-12-22\",11.4\\r\\n\"1984-12-23\",13.7\\r\\n\"1984-12-24\",16.5\\r\\n\"1984-12-25\",12.8\\r\\n\"1984-12-26\",12.2\\r\\n\"1984-12-27\",12.0\\r\\n\"1984-12-28\",12.6\\r\\n\"1984-12-29\",16.0\\r\\n\"1984-12-30\",16.4\\r\\n\"1985-01-01\",13.3\\r\\n\"1985-01-02\",15.2\\r\\n\"1985-01-03\",13.1\\r\\n\"1985-01-04\",12.7\\r\\n\"1985-01-05\",14.6\\r\\n\"1985-01-06\",11.0\\r\\n\"1985-01-07\",13.2\\r\\n\"1985-01-08\",12.2\\r\\n\"1985-01-09\",14.4\\r\\n\"1985-01-10\",13.7\\r\\n\"1985-01-11\",14.5\\r\\n\"1985-01-12\",14.1\\r\\n\"1985-01-13\",14.4\\r\\n\"1985-01-14\",19.7\\r\\n\"1985-01-15\",16.5\\r\\n\"1985-01-16\",15.9\\r\\n\"1985-01-17\",11.8\\r\\n\"1985-01-18\",12.0\\r\\n\"1985-01-19\",11.4\\r\\n\"1985-01-20\",14.4\\r\\n\"1985-01-21\",12.4\\r\\n\"1985-01-22\",15.1\\r\\n\"1985-01-23\",15.6\\r\\n\"1985-01-24\",15.2\\r\\n\"1985-01-25\",12.8\\r\\n\"1985-01-26\",13.3\\r\\n\"1985-01-27\",17.5\\r\\n\"1985-01-28\",15.4\\r\\n\"1985-01-29\",13.5\\r\\n\"1985-01-30\",16.7\\r\\n\"1985-01-31\",15.2\\r\\n\"1985-02-01\",14.9\\r\\n\"1985-02-02\",10.2\\r\\n\"1985-02-03\",13.6\\r\\n\"1985-02-04\",19.0\\r\\n\"1985-02-05\",15.7\\r\\n\"1985-02-06\",18.0\\r\\n\"1985-02-07\",14.8\\r\\n\"1985-02-08\",13.9\\r\\n\"1985-02-09\",13.0\\r\\n\"1985-02-10\",15.3\\r\\n\"1985-02-11\",14.3\\r\\n\"1985-02-12\",15.6\\r\\n\"1985-02-13\",16.0\\r\\n\"1985-02-14\",14.9\\r\\n\"1985-02-15\",11.1\\r\\n\"1985-02-16\",14.8\\r\\n\"1985-02-17\",13.0\\r\\n\"1985-02-18\",12.2\\r\\n\"1985-02-19\",10.9\\r\\n\"1985-02-20\",14.6\\r\\n\"1985-02-21\",16.6\\r\\n\"1985-02-22\",18.1\\r\\n\"1985-02-23\",13.4\\r\\n\"1985-02-24\",10.3\\r\\n\"1985-02-25\",13.6\\r\\n\"1985-02-26\",13.8\\r\\n\"1985-02-27\",10.3\\r\\n\"1985-02-28\",11.0\\r\\n\"1985-03-01\",14.3\\r\\n\"1985-03-02\",15.5\\r\\n\"1985-03-03\",14.7\\r\\n\"1985-03-04\",12.7\\r\\n\"1985-03-05\",10.7\\r\\n\"1985-03-06\",12.6\\r\\n\"1985-03-07\",9.8\\r\\n\"1985-03-08\",13.2\\r\\n\"1985-03-09\",15.2\\r\\n\"1985-03-10\",16.6\\r\\n\"1985-03-11\",21.0\\r\\n\"1985-03-12\",22.4\\r\\n\"1985-03-13\",17.0\\r\\n\"1985-03-14\",21.7\\r\\n\"1985-03-15\",21.4\\r\\n\"1985-03-16\",18.6\\r\\n\"1985-03-17\",16.2\\r\\n\"1985-03-18\",16.8\\r\\n\"1985-03-19\",17.0\\r\\n\"1985-03-20\",18.4\\r\\n\"1985-03-21\",17.2\\r\\n\"1985-03-22\",18.4\\r\\n\"1985-03-23\",18.8\\r\\n\"1985-03-24\",16.5\\r\\n\"1985-03-25\",13.3\\r\\n\"1985-03-26\",12.2\\r\\n\"1985-03-27\",11.3\\r\\n\"1985-03-28\",13.8\\r\\n\"1985-03-29\",16.6\\r\\n\"1985-03-30\",14.0\\r\\n\"1985-03-31\",14.3\\r\\n\"1985-04-01\",16.4\\r\\n\"1985-04-02\",11.9\\r\\n\"1985-04-03\",15.7\\r\\n\"1985-04-04\",17.6\\r\\n\"1985-04-05\",17.5\\r\\n\"1985-04-06\",15.9\\r\\n\"1985-04-07\",16.2\\r\\n\"1985-04-08\",16.0\\r\\n\"1985-04-09\",15.9\\r\\n\"1985-04-10\",16.2\\r\\n\"1985-04-11\",16.2\\r\\n\"1985-04-12\",19.5\\r\\n\"1985-04-13\",18.2\\r\\n\"1985-04-14\",21.8\\r\\n\"1985-04-15\",15.1\\r\\n\"1985-04-16\",11.0\\r\\n\"1985-04-17\",8.1\\r\\n\"1985-04-18\",9.5\\r\\n\"1985-04-19\",9.3\\r\\n\"1985-04-20\",10.6\\r\\n\"1985-04-21\",6.3\\r\\n\"1985-04-22\",8.6\\r\\n\"1985-04-23\",6.8\\r\\n\"1985-04-24\",8.7\\r\\n\"1985-04-25\",8.4\\r\\n\"1985-04-26\",9.3\\r\\n\"1985-04-27\",10.0\\r\\n\"1985-04-28\",10.5\\r\\n\"1985-04-29\",12.0\\r\\n\"1985-04-30\",10.1\\r\\n\"1985-05-01\",9.4\\r\\n\"1985-05-02\",10.1\\r\\n\"1985-05-03\",8.0\\r\\n\"1985-05-04\",10.6\\r\\n\"1985-05-05\",13.6\\r\\n\"1985-05-06\",15.4\\r\\n\"1985-05-07\",9.0\\r\\n\"1985-05-08\",10.4\\r\\n\"1985-05-09\",11.0\\r\\n\"1985-05-10\",12.1\\r\\n\"1985-05-11\",13.4\\r\\n\"1985-05-12\",11.3\\r\\n\"1985-05-13\",6.7\\r\\n\"1985-05-14\",9.8\\r\\n\"1985-05-15\",10.8\\r\\n\"1985-05-16\",7.8\\r\\n\"1985-05-17\",4.5\\r\\n\"1985-05-18\",7.6\\r\\n\"1985-05-19\",6.9\\r\\n\"1985-05-20\",7.5\\r\\n\"1985-05-21\",8.5\\r\\n\"1985-05-22\",5.5\\r\\n\"1985-05-23\",9.5\\r\\n\"1985-05-24\",7.3\\r\\n\"1985-05-25\",5.4\\r\\n\"1985-05-26\",5.5\\r\\n\"1985-05-27\",8.1\\r\\n\"1985-05-28\",11.2\\r\\n\"1985-05-29\",13.4\\r\\n\"1985-05-30\",11.6\\r\\n\"1985-05-31\",10.1\\r\\n\"1985-06-01\",4.3\\r\\n\"1985-06-02\",5.5\\r\\n\"1985-06-03\",4.4\\r\\n\"1985-06-04\",5.9\\r\\n\"1985-06-05\",5.7\\r\\n\"1985-06-06\",8.2\\r\\n\"1985-06-07\",8.2\\r\\n\"1985-06-08\",4.2\\r\\n\"1985-06-09\",6.5\\r\\n\"1985-06-10\",10.0\\r\\n\"1985-06-11\",8.8\\r\\n\"1985-06-12\",6.6\\r\\n\"1985-06-13\",7.8\\r\\n\"1985-06-14\",10.1\\r\\n\"1985-06-15\",7.1\\r\\n\"1985-06-16\",7.7\\r\\n\"1985-06-17\",8.5\\r\\n\"1985-06-18\",7.3\\r\\n\"1985-06-19\",6.9\\r\\n\"1985-06-20\",8.4\\r\\n\"1985-06-21\",7.1\\r\\n\"1985-06-22\",6.3\\r\\n\"1985-06-23\",0.6\\r\\n\"1985-06-24\",1.6\\r\\n\"1985-06-25\",7.0\\r\\n\"1985-06-26\",8.3\\r\\n\"1985-06-27\",8.0\\r\\n\"1985-06-28\",10.2\\r\\n\"1985-06-29\",10.6\\r\\n\"1985-06-30\",10.4\\r\\n\"1985-07-01\",11.6\\r\\n\"1985-07-02\",11.0\\r\\n\"1985-07-03\",10.7\\r\\n\"1985-07-04\",7.3\\r\\n\"1985-07-05\",4.2\\r\\n\"1985-07-06\",4.7\\r\\n\"1985-07-07\",5.6\\r\\n\"1985-07-08\",7.7\\r\\n\"1985-07-09\",7.5\\r\\n\"1985-07-10\",4.9\\r\\n\"1985-07-11\",5.9\\r\\n\"1985-07-12\",7.8\\r\\n\"1985-07-13\",5.8\\r\\n\"1985-07-14\",7.0\\r\\n\"1985-07-15\",8.4\\r\\n\"1985-07-16\",6.2\\r\\n\"1985-07-17\",7.5\\r\\n\"1985-07-18\",4.8\\r\\n\"1985-07-19\",3.3\\r\\n\"1985-07-20\",3.2\\r\\n\"1985-07-21\",7.0\\r\\n\"1985-07-22\",8.4\\r\\n\"1985-07-23\",0.3\\r\\n\"1985-07-24\",0.3\\r\\n\"1985-07-25\",2.1\\r\\n\"1985-07-26\",8.5\\r\\n\"1985-07-27\",1.4\\r\\n\"1985-07-28\",4.1\\r\\n\"1985-07-29\",10.3\\r\\n\"1985-07-30\",6.6\\r\\n\"1985-07-31\",6.1\\r\\n\"1985-08-01\",7.0\\r\\n\"1985-08-02\",5.1\\r\\n\"1985-08-03\",6.3\\r\\n\"1985-08-04\",6.9\\r\\n\"1985-08-05\",11.4\\r\\n\"1985-08-06\",10.4\\r\\n\"1985-08-07\",10.3\\r\\n\"1985-08-08\",9.2\\r\\n\"1985-08-09\",7.2\\r\\n\"1985-08-10\",7.5\\r\\n\"1985-08-11\",4.0\\r\\n\"1985-08-12\",5.6\\r\\n\"1985-08-13\",6.7\\r\\n\"1985-08-14\",8.4\\r\\n\"1985-08-15\",11.0\\r\\n\"1985-08-16\",8.4\\r\\n\"1985-08-17\",8.8\\r\\n\"1985-08-18\",8.6\\r\\n\"1985-08-19\",8.3\\r\\n\"1985-08-20\",4.0\\r\\n\"1985-08-21\",3.6\\r\\n\"1985-08-22\",5.7\\r\\n\"1985-08-23\",10.6\\r\\n\"1985-08-24\",6.9\\r\\n\"1985-08-25\",10.0\\r\\n\"1985-08-26\",9.8\\r\\n\"1985-08-27\",7.2\\r\\n\"1985-08-28\",10.5\\r\\n\"1985-08-29\",3.6\\r\\n\"1985-08-30\",5.3\\r\\n\"1985-08-31\",8.4\\r\\n\"1985-09-01\",10.3\\r\\n\"1985-09-02\",7.9\\r\\n\"1985-09-03\",8.5\\r\\n\"1985-09-04\",7.9\\r\\n\"1985-09-05\",8.0\\r\\n\"1985-09-06\",9.8\\r\\n\"1985-09-07\",6.7\\r\\n\"1985-09-08\",4.8\\r\\n\"1985-09-09\",9.9\\r\\n\"1985-09-10\",12.8\\r\\n\"1985-09-11\",10.9\\r\\n\"1985-09-12\",11.7\\r\\n\"1985-09-13\",11.7\\r\\n\"1985-09-14\",11.0\\r\\n\"1985-09-15\",8.2\\r\\n\"1985-09-16\",7.5\\r\\n\"1985-09-17\",5.4\\r\\n\"1985-09-18\",7.2\\r\\n\"1985-09-19\",9.7\\r\\n\"1985-09-20\",8.4\\r\\n\"1985-09-21\",9.0\\r\\n\"1985-09-22\",8.7\\r\\n\"1985-09-23\",6.6\\r\\n\"1985-09-24\",11.6\\r\\n\"1985-09-25\",13.1\\r\\n\"1985-09-26\",6.7\\r\\n\"1985-09-27\",6.5\\r\\n\"1985-09-28\",7.7\\r\\n\"1985-09-29\",8.7\\r\\n\"1985-09-30\",7.2\\r\\n\"1985-10-01\",10.5\\r\\n\"1985-10-02\",8.6\\r\\n\"1985-10-03\",7.2\\r\\n\"1985-10-04\",11.4\\r\\n\"1985-10-05\",16.2\\r\\n\"1985-10-06\",6.1\\r\\n\"1985-10-07\",9.6\\r\\n\"1985-10-08\",11.1\\r\\n\"1985-10-09\",13.6\\r\\n\"1985-10-10\",10.7\\r\\n\"1985-10-11\",14.7\\r\\n\"1985-10-12\",11.6\\r\\n\"1985-10-13\",7.3\\r\\n\"1985-10-14\",8.0\\r\\n\"1985-10-15\",9.6\\r\\n\"1985-10-16\",16.0\\r\\n\"1985-10-17\",15.1\\r\\n\"1985-10-18\",12.8\\r\\n\"1985-10-19\",6.2\\r\\n\"1985-10-20\",7.1\\r\\n\"1985-10-21\",8.4\\r\\n\"1985-10-22\",10.0\\r\\n\"1985-10-23\",12.7\\r\\n\"1985-10-24\",10.0\\r\\n\"1985-10-25\",10.2\\r\\n\"1985-10-26\",6.5\\r\\n\"1985-10-27\",9.2\\r\\n\"1985-10-28\",11.9\\r\\n\"1985-10-29\",14.7\\r\\n\"1985-10-30\",11.4\\r\\n\"1985-10-31\",6.8\\r\\n\"1985-11-01\",7.4\\r\\n\"1985-11-02\",11.2\\r\\n\"1985-11-03\",9.2\\r\\n\"1985-11-04\",12.6\\r\\n\"1985-11-05\",16.0\\r\\n\"1985-11-06\",17.1\\r\\n\"1985-11-07\",15.3\\r\\n\"1985-11-08\",13.3\\r\\n\"1985-11-09\",15.4\\r\\n\"1985-11-10\",13.2\\r\\n\"1985-11-11\",14.4\\r\\n\"1985-11-12\",14.0\\r\\n\"1985-11-13\",15.5\\r\\n\"1985-11-14\",21.0\\r\\n\"1985-11-15\",10.0\\r\\n\"1985-11-16\",9.6\\r\\n\"1985-11-17\",12.0\\r\\n\"1985-11-18\",12.2\\r\\n\"1985-11-19\",11.3\\r\\n\"1985-11-20\",13.2\\r\\n\"1985-11-21\",10.5\\r\\n\"1985-11-22\",10.1\\r\\n\"1985-11-23\",8.8\\r\\n\"1985-11-24\",13.7\\r\\n\"1985-11-25\",16.2\\r\\n\"1985-11-26\",16.0\\r\\n\"1985-11-27\",14.0\\r\\n\"1985-11-28\",13.7\\r\\n\"1985-11-29\",12.5\\r\\n\"1985-11-30\",12.8\\r\\n\"1985-12-01\",12.3\\r\\n\"1985-12-02\",15.2\\r\\n\"1985-12-03\",15.0\\r\\n\"1985-12-04\",16.4\\r\\n\"1985-12-05\",16.1\\r\\n\"1985-12-06\",14.6\\r\\n\"1985-12-07\",18.2\\r\\n\"1985-12-08\",16.4\\r\\n\"1985-12-09\",16.6\\r\\n\"1985-12-10\",14.7\\r\\n\"1985-12-11\",15.8\\r\\n\"1985-12-12\",14.1\\r\\n\"1985-12-13\",13.5\\r\\n\"1985-12-14\",13.6\\r\\n\"1985-12-15\",13.7\\r\\n\"1985-12-16\",13.6\\r\\n\"1985-12-17\",12.1\\r\\n\"1985-12-18\",12.7\\r\\n\"1985-12-19\",13.3\\r\\n\"1985-12-20\",14.2\\r\\n\"1985-12-21\",15.0\\r\\n\"1985-12-22\",13.7\\r\\n\"1985-12-23\",12.0\\r\\n\"1985-12-24\",13.1\\r\\n\"1985-12-25\",13.2\\r\\n\"1985-12-26\",13.3\\r\\n\"1985-12-27\",11.5\\r\\n\"1985-12-28\",10.8\\r\\n\"1985-12-29\",12.0\\r\\n\"1985-12-30\",16.3\\r\\n\"1985-12-31\",14.4\\r\\n\"1986-01-01\",12.9\\r\\n\"1986-01-02\",13.8\\r\\n\"1986-01-03\",10.6\\r\\n\"1986-01-04\",12.6\\r\\n\"1986-01-05\",13.7\\r\\n\"1986-01-06\",12.6\\r\\n\"1986-01-07\",13.1\\r\\n\"1986-01-08\",15.4\\r\\n\"1986-01-09\",11.9\\r\\n\"1986-01-10\",13.8\\r\\n\"1986-01-11\",14.4\\r\\n\"1986-01-12\",15.2\\r\\n\"1986-01-13\",12.5\\r\\n\"1986-01-14\",12.2\\r\\n\"1986-01-15\",16.1\\r\\n\"1986-01-16\",14.6\\r\\n\"1986-01-17\",11.6\\r\\n\"1986-01-18\",13.1\\r\\n\"1986-01-19\",12.8\\r\\n\"1986-01-20\",15.2\\r\\n\"1986-01-21\",13.8\\r\\n\"1986-01-22\",15.0\\r\\n\"1986-01-23\",13.5\\r\\n\"1986-01-24\",11.8\\r\\n\"1986-01-25\",15.3\\r\\n\"1986-01-26\",13.5\\r\\n\"1986-01-27\",15.3\\r\\n\"1986-01-28\",13.8\\r\\n\"1986-01-29\",15.8\\r\\n\"1986-01-30\",17.4\\r\\n\"1986-01-31\",15.3\\r\\n\"1986-02-01\",14.6\\r\\n\"1986-02-02\",14.8\\r\\n\"1986-02-03\",10.7\\r\\n\"1986-02-04\",11.6\\r\\n\"1986-02-05\",13.6\\r\\n\"1986-02-06\",14.4\\r\\n\"1986-02-07\",11.8\\r\\n\"1986-02-08\",15.8\\r\\n\"1986-02-09\",16.0\\r\\n\"1986-02-10\",11.8\\r\\n\"1986-02-11\",14.5\\r\\n\"1986-02-12\",10.7\\r\\n\"1986-02-13\",14.2\\r\\n\"1986-02-14\",19.5\\r\\n\"1986-02-15\",21.4\\r\\n\"1986-02-16\",17.9\\r\\n\"1986-02-17\",17.4\\r\\n\"1986-02-18\",12.7\\r\\n\"1986-02-19\",13.8\\r\\n\"1986-02-20\",14.0\\r\\n\"1986-02-21\",15.0\\r\\n\"1986-02-22\",14.5\\r\\n\"1986-02-23\",13.1\\r\\n\"1986-02-24\",11.4\\r\\n\"1986-02-25\",12.5\\r\\n\"1986-02-26\",12.0\\r\\n\"1986-02-27\",13.4\\r\\n\"1986-02-28\",14.4\\r\\n\"1986-03-01\",17.7\\r\\n\"1986-03-02\",13.9\\r\\n\"1986-03-03\",13.3\\r\\n\"1986-03-04\",14.6\\r\\n\"1986-03-05\",16.4\\r\\n\"1986-03-06\",16.8\\r\\n\"1986-03-07\",20.0\\r\\n\"1986-03-08\",12.5\\r\\n\"1986-03-09\",12.7\\r\\n\"1986-03-10\",11.7\\r\\n\"1986-03-11\",12.7\\r\\n\"1986-03-12\",8.6\\r\\n\"1986-03-13\",11.9\\r\\n\"1986-03-14\",16.0\\r\\n\"1986-03-15\",15.2\\r\\n\"1986-03-16\",13.4\\r\\n\"1986-03-17\",11.6\\r\\n\"1986-03-18\",11.1\\r\\n\"1986-03-19\",15.6\\r\\n\"1986-03-20\",17.0\\r\\n\"1986-03-21\",18.5\\r\\n\"1986-03-22\",17.4\\r\\n\"1986-03-23\",16.5\\r\\n\"1986-03-24\",16.2\\r\\n\"1986-03-25\",16.1\\r\\n\"1986-03-26\",13.2\\r\\n\"1986-03-27\",18.0\\r\\n\"1986-03-28\",12.8\\r\\n\"1986-03-29\",11.7\\r\\n\"1986-03-30\",16.7\\r\\n\"1986-03-31\",15.6\\r\\n\"1986-04-01\",10.2\\r\\n\"1986-04-02\",10.3\\r\\n\"1986-04-03\",15.0\\r\\n\"1986-04-04\",18.0\\r\\n\"1986-04-05\",13.8\\r\\n\"1986-04-06\",10.5\\r\\n\"1986-04-07\",11.8\\r\\n\"1986-04-08\",7.2\\r\\n\"1986-04-09\",11.6\\r\\n\"1986-04-10\",7.4\\r\\n\"1986-04-11\",14.2\\r\\n\"1986-04-12\",12.2\\r\\n\"1986-04-13\",9.0\\r\\n\"1986-04-14\",12.3\\r\\n\"1986-04-15\",19.7\\r\\n\"1986-04-16\",12.8\\r\\n\"1986-04-17\",12.4\\r\\n\"1986-04-18\",12.0\\r\\n\"1986-04-19\",12.0\\r\\n\"1986-04-20\",11.1\\r\\n\"1986-04-21\",12.7\\r\\n\"1986-04-22\",14.2\\r\\n\"1986-04-23\",11.6\\r\\n\"1986-04-24\",12.0\\r\\n\"1986-04-25\",11.5\\r\\n\"1986-04-26\",8.3\\r\\n\"1986-04-27\",10.5\\r\\n\"1986-04-28\",9.0\\r\\n\"1986-04-29\",6.9\\r\\n\"1986-04-30\",9.4\\r\\n\"1986-05-01\",11.1\\r\\n\"1986-05-02\",9.1\\r\\n\"1986-05-03\",7.7\\r\\n\"1986-05-04\",10.0\\r\\n\"1986-05-05\",10.4\\r\\n\"1986-05-06\",8.0\\r\\n\"1986-05-07\",9.8\\r\\n\"1986-05-08\",12.4\\r\\n\"1986-05-09\",12.9\\r\\n\"1986-05-10\",12.3\\r\\n\"1986-05-11\",6.9\\r\\n\"1986-05-12\",10.5\\r\\n\"1986-05-13\",11.0\\r\\n\"1986-05-14\",9.7\\r\\n\"1986-05-15\",11.1\\r\\n\"1986-05-16\",11.5\\r\\n\"1986-05-17\",13.4\\r\\n\"1986-05-18\",10.9\\r\\n\"1986-05-19\",12.0\\r\\n\"1986-05-20\",12.1\\r\\n\"1986-05-21\",10.4\\r\\n\"1986-05-22\",10.0\\r\\n\"1986-05-23\",9.6\\r\\n\"1986-05-24\",11.3\\r\\n\"1986-05-25\",8.5\\r\\n\"1986-05-26\",6.3\\r\\n\"1986-05-27\",8.2\\r\\n\"1986-05-28\",10.7\\r\\n\"1986-05-29\",10.3\\r\\n\"1986-05-30\",9.5\\r\\n\"1986-05-31\",10.9\\r\\n\"1986-06-01\",10.9\\r\\n\"1986-06-02\",4.3\\r\\n\"1986-06-03\",5.2\\r\\n\"1986-06-04\",11.0\\r\\n\"1986-06-05\",11.6\\r\\n\"1986-06-06\",10.6\\r\\n\"1986-06-07\",9.4\\r\\n\"1986-06-08\",10.0\\r\\n\"1986-06-09\",9.6\\r\\n\"1986-06-10\",9.5\\r\\n\"1986-06-11\",9.7\\r\\n\"1986-06-12\",9.6\\r\\n\"1986-06-13\",7.0\\r\\n\"1986-06-14\",7.0\\r\\n\"1986-06-15\",6.8\\r\\n\"1986-06-16\",6.9\\r\\n\"1986-06-17\",8.0\\r\\n\"1986-06-18\",7.6\\r\\n\"1986-06-19\",8.6\\r\\n\"1986-06-20\",5.7\\r\\n\"1986-06-21\",5.5\\r\\n\"1986-06-22\",5.7\\r\\n\"1986-06-23\",5.7\\r\\n\"1986-06-24\",6.6\\r\\n\"1986-06-25\",6.0\\r\\n\"1986-06-26\",6.9\\r\\n\"1986-06-27\",7.7\\r\\n\"1986-06-28\",8.0\\r\\n\"1986-06-29\",3.9\\r\\n\"1986-06-30\",0.8\\r\\n\"1986-07-01\",2.8\\r\\n\"1986-07-02\",8.0\\r\\n\"1986-07-03\",9.8\\r\\n\"1986-07-04\",11.4\\r\\n\"1986-07-05\",8.6\\r\\n\"1986-07-06\",5.2\\r\\n\"1986-07-07\",6.6\\r\\n\"1986-07-08\",5.7\\r\\n\"1986-07-09\",4.6\\r\\n\"1986-07-10\",5.8\\r\\n\"1986-07-11\",7.0\\r\\n\"1986-07-12\",4.8\\r\\n\"1986-07-13\",4.4\\r\\n\"1986-07-14\",4.4\\r\\n\"1986-07-15\",7.9\\r\\n\"1986-07-16\",10.6\\r\\n\"1986-07-17\",5.0\\r\\n\"1986-07-18\",7.6\\r\\n\"1986-07-19\",9.2\\r\\n\"1986-07-20\",9.7\\r\\n\"1986-07-21\",8.8\\r\\n\"1986-07-22\",6.8\\r\\n\"1986-07-23\",9.4\\r\\n\"1986-07-24\",11.0\\r\\n\"1986-07-25\",2.5\\r\\n\"1986-07-26\",2.1\\r\\n\"1986-07-27\",5.4\\r\\n\"1986-07-28\",6.2\\r\\n\"1986-07-29\",7.8\\r\\n\"1986-07-30\",7.4\\r\\n\"1986-07-31\",9.3\\r\\n\"1986-08-01\",9.3\\r\\n\"1986-08-02\",9.5\\r\\n\"1986-08-03\",8.5\\r\\n\"1986-08-04\",10.0\\r\\n\"1986-08-05\",7.7\\r\\n\"1986-08-06\",9.3\\r\\n\"1986-08-07\",9.1\\r\\n\"1986-08-08\",3.5\\r\\n\"1986-08-09\",3.6\\r\\n\"1986-08-10\",2.5\\r\\n\"1986-08-11\",1.7\\r\\n\"1986-08-12\",2.7\\r\\n\"1986-08-13\",2.9\\r\\n\"1986-08-14\",5.3\\r\\n\"1986-08-15\",7.7\\r\\n\"1986-08-16\",9.1\\r\\n\"1986-08-17\",9.4\\r\\n\"1986-08-18\",7.3\\r\\n\"1986-08-19\",8.4\\r\\n\"1986-08-20\",9.2\\r\\n\"1986-08-21\",6.6\\r\\n\"1986-08-22\",9.7\\r\\n\"1986-08-23\",12.4\\r\\n\"1986-08-24\",10.2\\r\\n\"1986-08-25\",5.9\\r\\n\"1986-08-26\",7.1\\r\\n\"1986-08-27\",7.5\\r\\n\"1986-08-28\",9.7\\r\\n\"1986-08-29\",12.2\\r\\n\"1986-08-30\",5.6\\r\\n\"1986-08-31\",5.4\\r\\n\"1986-09-01\",8.3\\r\\n\"1986-09-02\",10.6\\r\\n\"1986-09-03\",9.1\\r\\n\"1986-09-04\",11.3\\r\\n\"1986-09-05\",10.9\\r\\n\"1986-09-06\",8.9\\r\\n\"1986-09-07\",6.3\\r\\n\"1986-09-08\",9.0\\r\\n\"1986-09-09\",6.1\\r\\n\"1986-09-10\",9.1\\r\\n\"1986-09-11\",9.6\\r\\n\"1986-09-12\",6.0\\r\\n\"1986-09-13\",10.0\\r\\n\"1986-09-14\",11.0\\r\\n\"1986-09-15\",6.2\\r\\n\"1986-09-16\",8.3\\r\\n\"1986-09-17\",11.3\\r\\n\"1986-09-18\",11.3\\r\\n\"1986-09-19\",6.7\\r\\n\"1986-09-20\",6.6\\r\\n\"1986-09-21\",11.4\\r\\n\"1986-09-22\",6.9\\r\\n\"1986-09-23\",10.6\\r\\n\"1986-09-24\",8.6\\r\\n\"1986-09-25\",11.3\\r\\n\"1986-09-26\",12.5\\r\\n\"1986-09-27\",9.9\\r\\n\"1986-09-28\",6.9\\r\\n\"1986-09-29\",5.5\\r\\n\"1986-09-30\",7.8\\r\\n\"1986-10-01\",11.0\\r\\n\"1986-10-02\",16.2\\r\\n\"1986-10-03\",9.9\\r\\n\"1986-10-04\",8.7\\r\\n\"1986-10-05\",10.5\\r\\n\"1986-10-06\",12.2\\r\\n\"1986-10-07\",10.6\\r\\n\"1986-10-08\",8.3\\r\\n\"1986-10-09\",5.5\\r\\n\"1986-10-10\",9.0\\r\\n\"1986-10-11\",6.4\\r\\n\"1986-10-12\",7.2\\r\\n\"1986-10-13\",12.9\\r\\n\"1986-10-14\",12.0\\r\\n\"1986-10-15\",7.3\\r\\n\"1986-10-16\",9.7\\r\\n\"1986-10-17\",8.4\\r\\n\"1986-10-18\",14.7\\r\\n\"1986-10-19\",9.5\\r\\n\"1986-10-20\",7.9\\r\\n\"1986-10-21\",6.8\\r\\n\"1986-10-22\",12.6\\r\\n\"1986-10-23\",5.2\\r\\n\"1986-10-24\",7.5\\r\\n\"1986-10-25\",8.7\\r\\n\"1986-10-26\",7.6\\r\\n\"1986-10-27\",9.0\\r\\n\"1986-10-28\",7.2\\r\\n\"1986-10-29\",10.7\\r\\n\"1986-10-30\",13.1\\r\\n\"1986-10-31\",13.9\\r\\n\"1986-11-01\",10.8\\r\\n\"1986-11-02\",10.4\\r\\n\"1986-11-03\",9.1\\r\\n\"1986-11-04\",16.0\\r\\n\"1986-11-05\",21.0\\r\\n\"1986-11-06\",16.2\\r\\n\"1986-11-07\",8.6\\r\\n\"1986-11-08\",9.2\\r\\n\"1986-11-09\",12.5\\r\\n\"1986-11-10\",9.7\\r\\n\"1986-11-11\",12.5\\r\\n\"1986-11-12\",10.3\\r\\n\"1986-11-13\",12.0\\r\\n\"1986-11-14\",11.0\\r\\n\"1986-11-15\",14.8\\r\\n\"1986-11-16\",15.0\\r\\n\"1986-11-17\",15.3\\r\\n\"1986-11-18\",10.3\\r\\n\"1986-11-19\",10.7\\r\\n\"1986-11-20\",10.5\\r\\n\"1986-11-21\",8.9\\r\\n\"1986-11-22\",8.1\\r\\n\"1986-11-23\",11.5\\r\\n\"1986-11-24\",12.8\\r\\n\"1986-11-25\",9.1\\r\\n\"1986-11-26\",14.6\\r\\n\"1986-11-27\",11.6\\r\\n\"1986-11-28\",11.2\\r\\n\"1986-11-29\",12.6\\r\\n\"1986-11-30\",7.5\\r\\n\"1986-12-01\",11.0\\r\\n\"1986-12-02\",14.5\\r\\n\"1986-12-03\",18.5\\r\\n\"1986-12-04\",15.4\\r\\n\"1986-12-05\",13.1\\r\\n\"1986-12-06\",16.3\\r\\n\"1986-12-07\",20.2\\r\\n\"1986-12-08\",11.5\\r\\n\"1986-12-09\",12.4\\r\\n\"1986-12-10\",10.9\\r\\n\"1986-12-11\",12.7\\r\\n\"1986-12-12\",12.2\\r\\n\"1986-12-13\",12.4\\r\\n\"1986-12-14\",9.8\\r\\n\"1986-12-15\",8.5\\r\\n\"1986-12-16\",14.7\\r\\n\"1986-12-17\",12.0\\r\\n\"1986-12-18\",10.3\\r\\n\"1986-12-19\",11.0\\r\\n\"1986-12-20\",10.2\\r\\n\"1986-12-21\",12.6\\r\\n\"1986-12-22\",11.6\\r\\n\"1986-12-23\",9.7\\r\\n\"1986-12-24\",13.4\\r\\n\"1986-12-25\",10.5\\r\\n\"1986-12-26\",14.7\\r\\n\"1986-12-27\",14.6\\r\\n\"1986-12-28\",14.2\\r\\n\"1986-12-29\",13.2\\r\\n\"1986-12-30\",11.7\\r\\n\"1986-12-31\",17.2\\r\\n\"1987-01-01\",12.3\\r\\n\"1987-01-02\",13.8\\r\\n\"1987-01-03\",15.3\\r\\n\"1987-01-04\",15.6\\r\\n\"1987-01-05\",16.2\\r\\n\"1987-01-06\",16.3\\r\\n\"1987-01-07\",16.8\\r\\n\"1987-01-08\",11.0\\r\\n\"1987-01-09\",8.5\\r\\n\"1987-01-10\",13.2\\r\\n\"1987-01-11\",13.0\\r\\n\"1987-01-12\",12.4\\r\\n\"1987-01-13\",13.0\\r\\n\"1987-01-14\",16.6\\r\\n\"1987-01-15\",12.0\\r\\n\"1987-01-16\",12.4\\r\\n\"1987-01-17\",15.0\\r\\n\"1987-01-18\",11.8\\r\\n\"1987-01-19\",11.6\\r\\n\"1987-01-20\",12.2\\r\\n\"1987-01-21\",13.7\\r\\n\"1987-01-22\",11.2\\r\\n\"1987-01-23\",12.4\\r\\n\"1987-01-24\",11.5\\r\\n\"1987-01-25\",13.8\\r\\n\"1987-01-26\",15.7\\r\\n\"1987-01-27\",12.9\\r\\n\"1987-01-28\",11.5\\r\\n\"1987-01-29\",11.0\\r\\n\"1987-01-30\",12.7\\r\\n\"1987-01-31\",14.9\\r\\n\"1987-02-01\",16.5\\r\\n\"1987-02-02\",12.8\\r\\n\"1987-02-03\",12.7\\r\\n\"1987-02-04\",12.7\\r\\n\"1987-02-05\",11.6\\r\\n\"1987-02-06\",13.3\\r\\n\"1987-02-07\",15.2\\r\\n\"1987-02-08\",16.4\\r\\n\"1987-02-09\",11.9\\r\\n\"1987-02-10\",15.1\\r\\n\"1987-02-11\",10.6\\r\\n\"1987-02-12\",13.6\\r\\n\"1987-02-13\",12.1\\r\\n\"1987-02-14\",16.0\\r\\n\"1987-02-15\",16.8\\r\\n\"1987-02-16\",16.6\\r\\n\"1987-02-17\",15.6\\r\\n\"1987-02-18\",15.2\\r\\n\"1987-02-19\",17.7\\r\\n\"1987-02-20\",21.0\\r\\n\"1987-02-21\",13.4\\r\\n\"1987-02-22\",10.5\\r\\n\"1987-02-23\",9.5\\r\\n\"1987-02-24\",12.0\\r\\n\"1987-02-25\",10.4\\r\\n\"1987-02-26\",11.5\\r\\n\"1987-02-27\",13.2\\r\\n\"1987-02-28\",15.0\\r\\n\"1987-03-01\",14.1\\r\\n\"1987-03-02\",12.4\\r\\n\"1987-03-03\",13.4\\r\\n\"1987-03-04\",12.5\\r\\n\"1987-03-05\",14.3\\r\\n\"1987-03-06\",17.6\\r\\n\"1987-03-07\",10.4\\r\\n\"1987-03-08\",9.9\\r\\n\"1987-03-09\",10.2\\r\\n\"1987-03-10\",11.3\\r\\n\"1987-03-11\",9.5\\r\\n\"1987-03-12\",11.8\\r\\n\"1987-03-13\",11.5\\r\\n\"1987-03-14\",10.5\\r\\n\"1987-03-15\",10.8\\r\\n\"1987-03-16\",13.0\\r\\n\"1987-03-17\",18.5\\r\\n\"1987-03-18\",18.7\\r\\n\"1987-03-19\",15.0\\r\\n\"1987-03-20\",13.0\\r\\n\"1987-03-21\",11.3\\r\\n\"1987-03-22\",13.0\\r\\n\"1987-03-23\",13.3\\r\\n\"1987-03-24\",11.0\\r\\n\"1987-03-25\",10.3\\r\\n\"1987-03-26\",13.0\\r\\n\"1987-03-27\",12.3\\r\\n\"1987-03-28\",15.6\\r\\n\"1987-03-29\",10.2\\r\\n\"1987-03-30\",10.8\\r\\n\"1987-03-31\",12.0\\r\\n\"1987-04-01\",13.3\\r\\n\"1987-04-02\",11.7\\r\\n\"1987-04-03\",12.5\\r\\n\"1987-04-04\",13.7\\r\\n\"1987-04-05\",14.9\\r\\n\"1987-04-06\",20.2\\r\\n\"1987-04-07\",16.3\\r\\n\"1987-04-08\",13.9\\r\\n\"1987-04-09\",10.1\\r\\n\"1987-04-10\",7.3\\r\\n\"1987-04-11\",14.0\\r\\n\"1987-04-12\",17.7\\r\\n\"1987-04-13\",16.3\\r\\n\"1987-04-14\",10.6\\r\\n\"1987-04-15\",9.7\\r\\n\"1987-04-16\",7.8\\r\\n\"1987-04-17\",10.4\\r\\n\"1987-04-18\",10.4\\r\\n\"1987-04-19\",14.1\\r\\n\"1987-04-20\",7.1\\r\\n\"1987-04-21\",8.1\\r\\n\"1987-04-22\",7.8\\r\\n\"1987-04-23\",10.6\\r\\n\"1987-04-24\",9.1\\r\\n\"1987-04-25\",9.0\\r\\n\"1987-04-26\",11.9\\r\\n\"1987-04-27\",17.1\\r\\n\"1987-04-28\",16.8\\r\\n\"1987-04-29\",13.5\\r\\n\"1987-04-30\",11.6\\r\\n\"1987-05-01\",7.0\\r\\n\"1987-05-02\",9.7\\r\\n\"1987-05-03\",9.9\\r\\n\"1987-05-04\",11.2\\r\\n\"1987-05-05\",11.3\\r\\n\"1987-05-06\",11.8\\r\\n\"1987-05-07\",9.9\\r\\n\"1987-05-08\",7.1\\r\\n\"1987-05-09\",9.6\\r\\n\"1987-05-10\",9.8\\r\\n\"1987-05-11\",10.6\\r\\n\"1987-05-12\",12.8\\r\\n\"1987-05-13\",16.5\\r\\n\"1987-05-14\",11.7\\r\\n\"1987-05-15\",12.3\\r\\n\"1987-05-16\",12.2\\r\\n\"1987-05-17\",11.8\\r\\n\"1987-05-18\",10.7\\r\\n\"1987-05-19\",10.2\\r\\n\"1987-05-20\",10.0\\r\\n\"1987-05-21\",8.3\\r\\n\"1987-05-22\",6.6\\r\\n\"1987-05-23\",9.5\\r\\n\"1987-05-24\",12.3\\r\\n\"1987-05-25\",7.6\\r\\n\"1987-05-26\",9.3\\r\\n\"1987-05-27\",5.0\\r\\n\"1987-05-28\",4.3\\r\\n\"1987-05-29\",6.4\\r\\n\"1987-05-30\",10.8\\r\\n\"1987-05-31\",7.8\\r\\n\"1987-06-01\",8.5\\r\\n\"1987-06-02\",9.7\\r\\n\"1987-06-03\",10.0\\r\\n\"1987-06-04\",11.0\\r\\n\"1987-06-05\",10.2\\r\\n\"1987-06-06\",6.6\\r\\n\"1987-06-07\",6.1\\r\\n\"1987-06-08\",5.9\\r\\n\"1987-06-09\",8.9\\r\\n\"1987-06-10\",13.0\\r\\n\"1987-06-11\",12.6\\r\\n\"1987-06-12\",5.4\\r\\n\"1987-06-13\",6.0\\r\\n\"1987-06-14\",7.8\\r\\n\"1987-06-15\",9.0\\r\\n\"1987-06-16\",4.2\\r\\n\"1987-06-17\",3.0\\r\\n\"1987-06-18\",4.5\\r\\n\"1987-06-19\",6.2\\r\\n\"1987-06-20\",11.9\\r\\n\"1987-06-21\",11.8\\r\\n\"1987-06-22\",9.4\\r\\n\"1987-06-23\",9.6\\r\\n\"1987-06-24\",9.4\\r\\n\"1987-06-25\",7.0\\r\\n\"1987-06-26\",8.9\\r\\n\"1987-06-27\",9.3\\r\\n\"1987-06-28\",6.8\\r\\n\"1987-06-29\",7.5\\r\\n\"1987-06-30\",8.0\\r\\n\"1987-07-01\",8.3\\r\\n\"1987-07-02\",2.7\\r\\n\"1987-07-03\",3.9\\r\\n\"1987-07-04\",4.1\\r\\n\"1987-07-05\",5.0\\r\\n\"1987-07-06\",5.8\\r\\n\"1987-07-07\",4.4\\r\\n\"1987-07-08\",4.1\\r\\n\"1987-07-09\",5.8\\r\\n\"1987-07-10\",9.1\\r\\n\"1987-07-11\",7.9\\r\\n\"1987-07-12\",5.0\\r\\n\"1987-07-13\",2.8\\r\\n\"1987-07-14\",4.7\\r\\n\"1987-07-15\",8.9\\r\\n\"1987-07-16\",5.4\\r\\n\"1987-07-17\",7.1\\r\\n\"1987-07-18\",9.0\\r\\n\"1987-07-19\",9.4\\r\\n\"1987-07-20\",6.3\\r\\n\"1987-07-21\",7.0\\r\\n\"1987-07-22\",6.4\\r\\n\"1987-07-23\",6.7\\r\\n\"1987-07-24\",1.5\\r\\n\"1987-07-25\",2.9\\r\\n\"1987-07-26\",4.8\\r\\n\"1987-07-27\",6.3\\r\\n\"1987-07-28\",5.7\\r\\n\"1987-07-29\",7.0\\r\\n\"1987-07-30\",8.8\\r\\n\"1987-07-31\",8.7\\r\\n\"1987-08-01\",9.0\\r\\n\"1987-08-02\",9.6\\r\\n\"1987-08-03\",8.0\\r\\n\"1987-08-04\",8.4\\r\\n\"1987-08-05\",8.1\\r\\n\"1987-08-06\",9.0\\r\\n\"1987-08-07\",5.3\\r\\n\"1987-08-08\",8.9\\r\\n\"1987-08-09\",8.7\\r\\n\"1987-08-10\",4.9\\r\\n\"1987-08-11\",7.0\\r\\n\"1987-08-12\",7.5\\r\\n\"1987-08-13\",7.0\\r\\n\"1987-08-14\",9.1\\r\\n\"1987-08-15\",11.8\\r\\n\"1987-08-16\",9.9\\r\\n\"1987-08-17\",5.6\\r\\n\"1987-08-18\",4.2\\r\\n\"1987-08-19\",4.3\\r\\n\"1987-08-20\",8.0\\r\\n\"1987-08-21\",5.1\\r\\n\"1987-08-22\",9.4\\r\\n\"1987-08-23\",9.1\\r\\n\"1987-08-24\",9.7\\r\\n\"1987-08-25\",10.6\\r\\n\"1987-08-26\",8.6\\r\\n\"1987-08-27\",10.1\\r\\n\"1987-08-28\",11.0\\r\\n\"1987-08-29\",9.7\\r\\n\"1987-08-30\",5.0\\r\\n\"1987-08-31\",6.1\\r\\n\"1987-09-01\",5.4\\r\\n\"1987-09-02\",5.8\\r\\n\"1987-09-03\",7.3\\r\\n\"1987-09-04\",6.3\\r\\n\"1987-09-05\",4.8\\r\\n\"1987-09-06\",7.6\\r\\n\"1987-09-07\",8.1\\r\\n\"1987-09-08\",9.5\\r\\n\"1987-09-09\",10.3\\r\\n\"1987-09-10\",7.0\\r\\n\"1987-09-11\",9.0\\r\\n\"1987-09-12\",10.2\\r\\n\"1987-09-13\",6.8\\r\\n\"1987-09-14\",9.3\\r\\n\"1987-09-15\",9.8\\r\\n\"1987-09-16\",10.7\\r\\n\"1987-09-17\",7.8\\r\\n\"1987-09-18\",9.2\\r\\n\"1987-09-19\",15.0\\r\\n\"1987-09-20\",7.8\\r\\n\"1987-09-21\",5.3\\r\\n\"1987-09-22\",9.5\\r\\n\"1987-09-23\",7.6\\r\\n\"1987-09-24\",14.0\\r\\n\"1987-09-25\",14.9\\r\\n\"1987-09-26\",14.9\\r\\n\"1987-09-27\",19.2\\r\\n\"1987-09-28\",17.0\\r\\n\"1987-09-29\",13.0\\r\\n\"1987-09-30\",11.2\\r\\n\"1987-10-01\",9.5\\r\\n\"1987-10-02\",10.3\\r\\n\"1987-10-03\",9.3\\r\\n\"1987-10-04\",11.3\\r\\n\"1987-10-05\",6.5\\r\\n\"1987-10-06\",12.0\\r\\n\"1987-10-07\",8.3\\r\\n\"1987-10-08\",8.7\\r\\n\"1987-10-09\",8.7\\r\\n\"1987-10-10\",10.2\\r\\n\"1987-10-11\",6.9\\r\\n\"1987-10-12\",4.9\\r\\n\"1987-10-13\",10.0\\r\\n\"1987-10-14\",7.6\\r\\n\"1987-10-15\",14.5\\r\\n\"1987-10-16\",13.2\\r\\n\"1987-10-17\",9.9\\r\\n\"1987-10-18\",10.1\\r\\n\"1987-10-19\",11.3\\r\\n\"1987-10-20\",10.4\\r\\n\"1987-10-21\",10.9\\r\\n\"1987-10-22\",9.2\\r\\n\"1987-10-23\",10.5\\r\\n\"1987-10-24\",11.4\\r\\n\"1987-10-25\",13.5\\r\\n\"1987-10-26\",9.8\\r\\n\"1987-10-27\",13.1\\r\\n\"1987-10-28\",9.7\\r\\n\"1987-10-29\",11.4\\r\\n\"1987-10-30\",9.9\\r\\n\"1987-10-31\",14.4\\r\\n\"1987-11-01\",19.0\\r\\n\"1987-11-02\",23.0\\r\\n\"1987-11-03\",15.4\\r\\n\"1987-11-04\",9.6\\r\\n\"1987-11-05\",10.8\\r\\n\"1987-11-06\",12.1\\r\\n\"1987-11-07\",11.0\\r\\n\"1987-11-08\",12.6\\r\\n\"1987-11-09\",14.7\\r\\n\"1987-11-10\",11.1\\r\\n\"1987-11-11\",10.1\\r\\n\"1987-11-12\",11.4\\r\\n\"1987-11-13\",13.0\\r\\n\"1987-11-14\",11.9\\r\\n\"1987-11-15\",9.5\\r\\n\"1987-11-16\",13.5\\r\\n\"1987-11-17\",15.2\\r\\n\"1987-11-18\",18.4\\r\\n\"1987-11-19\",24.1\\r\\n\"1987-11-20\",14.1\\r\\n\"1987-11-21\",10.7\\r\\n\"1987-11-22\",8.7\\r\\n\"1987-11-23\",13.3\\r\\n\"1987-11-24\",11.6\\r\\n\"1987-11-25\",9.9\\r\\n\"1987-11-26\",10.8\\r\\n\"1987-11-27\",11.5\\r\\n\"1987-11-28\",10.0\\r\\n\"1987-11-29\",13.9\\r\\n\"1987-11-30\",13.6\\r\\n\"1987-12-01\",11.9\\r\\n\"1987-12-02\",11.1\\r\\n\"1987-12-03\",8.2\\r\\n\"1987-12-04\",9.4\\r\\n\"1987-12-05\",12.7\\r\\n\"1987-12-06\",11.6\\r\\n\"1987-12-07\",11.0\\r\\n\"1987-12-08\",11.3\\r\\n\"1987-12-09\",13.4\\r\\n\"1987-12-10\",14.9\\r\\n\"1987-12-11\",15.2\\r\\n\"1987-12-12\",13.9\\r\\n\"1987-12-13\",15.0\\r\\n\"1987-12-14\",16.2\\r\\n\"1987-12-15\",17.7\\r\\n\"1987-12-16\",20.5\\r\\n\"1987-12-17\",14.7\\r\\n\"1987-12-18\",12.5\\r\\n\"1987-12-19\",10.9\\r\\n\"1987-12-20\",12.8\\r\\n\"1987-12-21\",12.7\\r\\n\"1987-12-22\",11.2\\r\\n\"1987-12-23\",11.4\\r\\n\"1987-12-24\",11.2\\r\\n\"1987-12-25\",12.1\\r\\n\"1987-12-26\",12.7\\r\\n\"1987-12-27\",16.2\\r\\n\"1987-12-28\",14.2\\r\\n\"1987-12-29\",14.3\\r\\n\"1987-12-30\",13.3\\r\\n\"1987-12-31\",16.7\\r\\n\"1988-01-01\",15.3\\r\\n\"1988-01-02\",14.3\\r\\n\"1988-01-03\",13.5\\r\\n\"1988-01-04\",15.0\\r\\n\"1988-01-05\",13.6\\r\\n\"1988-01-06\",15.2\\r\\n\"1988-01-07\",17.0\\r\\n\"1988-01-08\",18.7\\r\\n\"1988-01-09\",16.5\\r\\n\"1988-01-10\",17.4\\r\\n\"1988-01-11\",18.3\\r\\n\"1988-01-12\",18.3\\r\\n\"1988-01-13\",22.4\\r\\n\"1988-01-14\",21.4\\r\\n\"1988-01-15\",20.9\\r\\n\"1988-01-16\",17.6\\r\\n\"1988-01-17\",15.5\\r\\n\"1988-01-18\",16.6\\r\\n\"1988-01-19\",16.2\\r\\n\"1988-01-20\",15.6\\r\\n\"1988-01-21\",14.5\\r\\n\"1988-01-22\",14.0\\r\\n\"1988-01-23\",15.6\\r\\n\"1988-01-24\",12.3\\r\\n\"1988-01-25\",11.6\\r\\n\"1988-01-26\",12.6\\r\\n\"1988-01-27\",14.9\\r\\n\"1988-01-28\",17.3\\r\\n\"1988-01-29\",21.4\\r\\n\"1988-01-30\",23.4\\r\\n\"1988-01-31\",14.4\\r\\n\"1988-02-01\",14.1\\r\\n\"1988-02-02\",15.0\\r\\n\"1988-02-03\",14.5\\r\\n\"1988-02-04\",15.1\\r\\n\"1988-02-05\",13.9\\r\\n\"1988-02-06\",13.4\\r\\n\"1988-02-07\",9.2\\r\\n\"1988-02-08\",12.5\\r\\n\"1988-02-09\",15.1\\r\\n\"1988-02-10\",12.1\\r\\n\"1988-02-11\",14.5\\r\\n\"1988-02-12\",16.3\\r\\n\"1988-02-13\",16.5\\r\\n\"1988-02-14\",14.9\\r\\n\"1988-02-15\",13.2\\r\\n\"1988-02-16\",11.8\\r\\n\"1988-02-17\",13.6\\r\\n\"1988-02-18\",16.2\\r\\n\"1988-02-19\",14.1\\r\\n\"1988-02-20\",13.5\\r\\n\"1988-02-21\",15.0\\r\\n\"1988-02-22\",14.8\\r\\n\"1988-02-23\",16.2\\r\\n\"1988-02-24\",16.2\\r\\n\"1988-02-25\",13.3\\r\\n\"1988-02-26\",15.3\\r\\n\"1988-02-27\",18.4\\r\\n\"1988-02-28\",16.2\\r\\n\"1988-02-29\",16.3\\r\\n\"1988-03-01\",12.4\\r\\n\"1988-03-02\",15.6\\r\\n\"1988-03-03\",14.9\\r\\n\"1988-03-04\",14.8\\r\\n\"1988-03-05\",12.7\\r\\n\"1988-03-06\",14.2\\r\\n\"1988-03-07\",16.8\\r\\n\"1988-03-08\",16.7\\r\\n\"1988-03-09\",16.2\\r\\n\"1988-03-10\",14.5\\r\\n\"1988-03-11\",10.0\\r\\n\"1988-03-12\",12.6\\r\\n\"1988-03-13\",11.9\\r\\n\"1988-03-14\",11.8\\r\\n\"1988-03-15\",13.4\\r\\n\"1988-03-16\",14.5\\r\\n\"1988-03-17\",15.7\\r\\n\"1988-03-18\",15.3\\r\\n\"1988-03-19\",13.9\\r\\n\"1988-03-20\",13.7\\r\\n\"1988-03-21\",15.1\\r\\n\"1988-03-22\",15.6\\r\\n\"1988-03-23\",14.4\\r\\n\"1988-03-24\",13.9\\r\\n\"1988-03-25\",16.2\\r\\n\"1988-03-26\",16.7\\r\\n\"1988-03-27\",15.5\\r\\n\"1988-03-28\",16.4\\r\\n\"1988-03-29\",17.5\\r\\n\"1988-03-30\",18.2\\r\\n\"1988-03-31\",16.1\\r\\n\"1988-04-01\",16.5\\r\\n\"1988-04-02\",14.6\\r\\n\"1988-04-03\",16.4\\r\\n\"1988-04-04\",13.6\\r\\n\"1988-04-05\",15.9\\r\\n\"1988-04-06\",11.9\\r\\n\"1988-04-07\",14.7\\r\\n\"1988-04-08\",9.4\\r\\n\"1988-04-09\",6.6\\r\\n\"1988-04-10\",7.9\\r\\n\"1988-04-11\",11.0\\r\\n\"1988-04-12\",15.7\\r\\n\"1988-04-13\",15.2\\r\\n\"1988-04-14\",15.9\\r\\n\"1988-04-15\",10.6\\r\\n\"1988-04-16\",8.3\\r\\n\"1988-04-17\",8.6\\r\\n\"1988-04-18\",12.7\\r\\n\"1988-04-19\",10.5\\r\\n\"1988-04-20\",12.0\\r\\n\"1988-04-21\",11.1\\r\\n\"1988-04-22\",13.0\\r\\n\"1988-04-23\",12.4\\r\\n\"1988-04-24\",13.3\\r\\n\"1988-04-25\",15.9\\r\\n\"1988-04-26\",12.0\\r\\n\"1988-04-27\",13.7\\r\\n\"1988-04-28\",17.6\\r\\n\"1988-04-29\",14.3\\r\\n\"1988-04-30\",13.7\\r\\n\"1988-05-01\",15.2\\r\\n\"1988-05-02\",14.5\\r\\n\"1988-05-03\",14.9\\r\\n\"1988-05-04\",15.5\\r\\n\"1988-05-05\",16.4\\r\\n\"1988-05-06\",14.5\\r\\n\"1988-05-07\",12.6\\r\\n\"1988-05-08\",13.6\\r\\n\"1988-05-09\",11.2\\r\\n\"1988-05-10\",11.0\\r\\n\"1988-05-11\",12.0\\r\\n\"1988-05-12\",6.8\\r\\n\"1988-05-13\",10.6\\r\\n\"1988-05-14\",13.1\\r\\n\"1988-05-15\",13.5\\r\\n\"1988-05-16\",11.7\\r\\n\"1988-05-17\",13.2\\r\\n\"1988-05-18\",12.0\\r\\n\"1988-05-19\",10.4\\r\\n\"1988-05-20\",10.0\\r\\n\"1988-05-21\",8.2\\r\\n\"1988-05-22\",9.4\\r\\n\"1988-05-23\",10.3\\r\\n\"1988-05-24\",8.1\\r\\n\"1988-05-25\",8.7\\r\\n\"1988-05-26\",12.6\\r\\n\"1988-05-27\",10.9\\r\\n\"1988-05-28\",8.7\\r\\n\"1988-05-29\",9.3\\r\\n\"1988-05-30\",6.3\\r\\n\"1988-05-31\",7.8\\r\\n\"1988-06-01\",10.0\\r\\n\"1988-06-02\",11.0\\r\\n\"1988-06-03\",11.1\\r\\n\"1988-06-04\",12.6\\r\\n\"1988-06-05\",10.2\\r\\n\"1988-06-06\",11.1\\r\\n\"1988-06-07\",8.7\\r\\n\"1988-06-08\",9.5\\r\\n\"1988-06-09\",9.7\\r\\n\"1988-06-10\",8.2\\r\\n\"1988-06-11\",5.0\\r\\n\"1988-06-12\",6.5\\r\\n\"1988-06-13\",12.1\\r\\n\"1988-06-14\",8.9\\r\\n\"1988-06-15\",6.1\\r\\n\"1988-06-16\",2.8\\r\\n\"1988-06-17\",3.7\\r\\n\"1988-06-18\",6.8\\r\\n\"1988-06-19\",6.6\\r\\n\"1988-06-20\",7.0\\r\\n\"1988-06-21\",7.3\\r\\n\"1988-06-22\",7.9\\r\\n\"1988-06-23\",10.6\\r\\n\"1988-06-24\",8.1\\r\\n\"1988-06-25\",6.7\\r\\n\"1988-06-26\",8.0\\r\\n\"1988-06-27\",10.0\\r\\n\"1988-06-28\",6.7\\r\\n\"1988-06-29\",9.4\\r\\n\"1988-06-30\",9.3\\r\\n\"1988-07-01\",6.0\\r\\n\"1988-07-02\",5.8\\r\\n\"1988-07-03\",4.9\\r\\n\"1988-07-04\",5.0\\r\\n\"1988-07-05\",8.4\\r\\n\"1988-07-06\",12.3\\r\\n\"1988-07-07\",13.0\\r\\n\"1988-07-08\",11.4\\r\\n\"1988-07-09\",6.8\\r\\n\"1988-07-10\",7.6\\r\\n\"1988-07-11\",12.4\\r\\n\"1988-07-12\",7.1\\r\\n\"1988-07-13\",7.5\\r\\n\"1988-07-14\",10.0\\r\\n\"1988-07-15\",5.3\\r\\n\"1988-07-16\",6.3\\r\\n\"1988-07-17\",8.0\\r\\n\"1988-07-18\",8.3\\r\\n\"1988-07-19\",9.3\\r\\n\"1988-07-20\",9.5\\r\\n\"1988-07-21\",5.6\\r\\n\"1988-07-22\",7.0\\r\\n\"1988-07-23\",8.5\\r\\n\"1988-07-24\",8.5\\r\\n\"1988-07-25\",8.2\\r\\n\"1988-07-26\",8.5\\r\\n\"1988-07-27\",9.6\\r\\n\"1988-07-28\",9.7\\r\\n\"1988-07-29\",7.1\\r\\n\"1988-07-30\",8.4\\r\\n\"1988-07-31\",9.2\\r\\n\"1988-08-01\",9.8\\r\\n\"1988-08-02\",8.1\\r\\n\"1988-08-03\",9.4\\r\\n\"1988-08-04\",10.0\\r\\n\"1988-08-05\",5.1\\r\\n\"1988-08-06\",6.7\\r\\n\"1988-08-07\",6.9\\r\\n\"1988-08-08\",6.8\\r\\n\"1988-08-09\",8.6\\r\\n\"1988-08-10\",9.1\\r\\n\"1988-08-11\",3.9\\r\\n\"1988-08-12\",4.8\\r\\n\"1988-08-13\",8.4\\r\\n\"1988-08-14\",11.6\\r\\n\"1988-08-15\",12.1\\r\\n\"1988-08-16\",12.4\\r\\n\"1988-08-17\",10.0\\r\\n\"1988-08-18\",10.1\\r\\n\"1988-08-19\",9.7\\r\\n\"1988-08-20\",11.7\\r\\n\"1988-08-21\",7.9\\r\\n\"1988-08-22\",8.6\\r\\n\"1988-08-23\",7.7\\r\\n\"1988-08-24\",5.8\\r\\n\"1988-08-25\",8.7\\r\\n\"1988-08-26\",10.6\\r\\n\"1988-08-27\",6.7\\r\\n\"1988-08-28\",8.8\\r\\n\"1988-08-29\",9.7\\r\\n\"1988-08-30\",9.0\\r\\n\"1988-08-31\",11.8\\r\\n\"1988-09-01\",15.2\\r\\n\"1988-09-02\",10.0\\r\\n\"1988-09-03\",10.5\\r\\n\"1988-09-04\",5.5\\r\\n\"1988-09-05\",9.4\\r\\n\"1988-09-06\",8.8\\r\\n\"1988-09-07\",5.3\\r\\n\"1988-09-08\",13.0\\r\\n\"1988-09-09\",15.2\\r\\n\"1988-09-10\",13.2\\r\\n\"1988-09-11\",11.5\\r\\n\"1988-09-12\",6.8\\r\\n\"1988-09-13\",4.7\\r\\n\"1988-09-14\",5.2\\r\\n\"1988-09-15\",6.8\\r\\n\"1988-09-16\",10.7\\r\\n\"1988-09-17\",10.1\\r\\n\"1988-09-18\",10.0\\r\\n\"1988-09-19\",9.8\\r\\n\"1988-09-20\",5.5\\r\\n\"1988-09-21\",13.5\\r\\n\"1988-09-22\",16.6\\r\\n\"1988-09-23\",8.4\\r\\n\"1988-09-24\",8.2\\r\\n\"1988-09-25\",11.1\\r\\n\"1988-09-26\",10.8\\r\\n\"1988-09-27\",8.8\\r\\n\"1988-09-28\",10.8\\r\\n\"1988-09-29\",8.7\\r\\n\"1988-09-30\",12.4\\r\\n\"1988-10-01\",9.0\\r\\n\"1988-10-02\",13.5\\r\\n\"1988-10-03\",14.7\\r\\n\"1988-10-04\",10.9\\r\\n\"1988-10-05\",8.5\\r\\n\"1988-10-06\",6.0\\r\\n\"1988-10-07\",12.7\\r\\n\"1988-10-08\",11.1\\r\\n\"1988-10-09\",8.7\\r\\n\"1988-10-10\",12.3\\r\\n\"1988-10-11\",13.3\\r\\n\"1988-10-12\",5.6\\r\\n\"1988-10-13\",13.7\\r\\n\"1988-10-14\",8.5\\r\\n\"1988-10-15\",11.2\\r\\n\"1988-10-16\",8.7\\r\\n\"1988-10-17\",11.7\\r\\n\"1988-10-18\",12.5\\r\\n\"1988-10-19\",8.2\\r\\n\"1988-10-20\",15.6\\r\\n\"1988-10-21\",10.3\\r\\n\"1988-10-22\",11.4\\r\\n\"1988-10-23\",9.7\\r\\n\"1988-10-24\",6.3\\r\\n\"1988-10-25\",14.3\\r\\n\"1988-10-26\",11.3\\r\\n\"1988-10-27\",7.3\\r\\n\"1988-10-28\",12.8\\r\\n\"1988-10-29\",11.9\\r\\n\"1988-10-30\",14.3\\r\\n\"1988-10-31\",11.6\\r\\n\"1988-11-01\",13.2\\r\\n\"1988-11-02\",15.5\\r\\n\"1988-11-03\",14.1\\r\\n\"1988-11-04\",9.5\\r\\n\"1988-11-05\",7.2\\r\\n\"1988-11-06\",11.8\\r\\n\"1988-11-07\",16.8\\r\\n\"1988-11-08\",12.5\\r\\n\"1988-11-09\",9.4\\r\\n\"1988-11-10\",11.9\\r\\n\"1988-11-11\",10.3\\r\\n\"1988-11-12\",16.9\\r\\n\"1988-11-13\",17.5\\r\\n\"1988-11-14\",7.5\\r\\n\"1988-11-15\",8.6\\r\\n\"1988-11-16\",11.1\\r\\n\"1988-11-17\",11.5\\r\\n\"1988-11-18\",10.7\\r\\n\"1988-11-19\",15.7\\r\\n\"1988-11-20\",12.8\\r\\n\"1988-11-21\",13.0\\r\\n\"1988-11-22\",12.9\\r\\n\"1988-11-23\",14.3\\r\\n\"1988-11-24\",13.7\\r\\n\"1988-11-25\",12.1\\r\\n\"1988-11-26\",11.9\\r\\n\"1988-11-27\",11.8\\r\\n\"1988-11-28\",11.4\\r\\n\"1988-11-29\",10.3\\r\\n\"1988-11-30\",11.7\\r\\n\"1988-12-01\",12.0\\r\\n\"1988-12-02\",17.4\\r\\n\"1988-12-03\",16.8\\r\\n\"1988-12-04\",16.2\\r\\n\"1988-12-05\",13.0\\r\\n\"1988-12-06\",12.5\\r\\n\"1988-12-07\",12.4\\r\\n\"1988-12-08\",16.1\\r\\n\"1988-12-09\",20.2\\r\\n\"1988-12-10\",14.3\\r\\n\"1988-12-11\",11.0\\r\\n\"1988-12-12\",14.4\\r\\n\"1988-12-13\",15.7\\r\\n\"1988-12-14\",19.7\\r\\n\"1988-12-15\",20.7\\r\\n\"1988-12-16\",23.9\\r\\n\"1988-12-17\",16.6\\r\\n\"1988-12-18\",17.5\\r\\n\"1988-12-19\",14.9\\r\\n\"1988-12-20\",13.6\\r\\n\"1988-12-21\",11.9\\r\\n\"1988-12-22\",15.2\\r\\n\"1988-12-23\",17.3\\r\\n\"1988-12-24\",19.8\\r\\n\"1988-12-25\",15.8\\r\\n\"1988-12-26\",9.5\\r\\n\"1988-12-27\",12.9\\r\\n\"1988-12-28\",12.9\\r\\n\"1988-12-29\",14.8\\r\\n\"1988-12-30\",14.1\\r\\n\"1989-01-01\",14.3\\r\\n\"1989-01-02\",17.4\\r\\n\"1989-01-03\",18.5\\r\\n\"1989-01-04\",16.8\\r\\n\"1989-01-05\",11.5\\r\\n\"1989-01-06\",9.5\\r\\n\"1989-01-07\",12.2\\r\\n\"1989-01-08\",15.7\\r\\n\"1989-01-09\",16.3\\r\\n\"1989-01-10\",13.6\\r\\n\"1989-01-11\",12.6\\r\\n\"1989-01-12\",13.8\\r\\n\"1989-01-13\",12.1\\r\\n\"1989-01-14\",13.4\\r\\n\"1989-01-15\",17.3\\r\\n\"1989-01-16\",19.4\\r\\n\"1989-01-17\",16.6\\r\\n\"1989-01-18\",13.9\\r\\n\"1989-01-19\",13.1\\r\\n\"1989-01-20\",16.0\\r\\n\"1989-01-21\",14.5\\r\\n\"1989-01-22\",15.0\\r\\n\"1989-01-23\",12.6\\r\\n\"1989-01-24\",12.5\\r\\n\"1989-01-25\",15.2\\r\\n\"1989-01-26\",16.2\\r\\n\"1989-01-27\",16.5\\r\\n\"1989-01-28\",20.1\\r\\n\"1989-01-29\",20.6\\r\\n\"1989-01-30\",16.9\\r\\n\"1989-01-31\",16.5\\r\\n\"1989-02-01\",16.1\\r\\n\"1989-02-02\",14.4\\r\\n\"1989-02-03\",16.3\\r\\n\"1989-02-04\",15.7\\r\\n\"1989-02-05\",14.2\\r\\n\"1989-02-06\",13.2\\r\\n\"1989-02-07\",16.8\\r\\n\"1989-02-08\",18.5\\r\\n\"1989-02-09\",16.7\\r\\n\"1989-02-10\",15.3\\r\\n\"1989-02-11\",15.9\\r\\n\"1989-02-12\",15.2\\r\\n\"1989-02-13\",17.5\\r\\n\"1989-02-14\",18.3\\r\\n\"1989-02-15\",19.4\\r\\n\"1989-02-16\",19.4\\r\\n\"1989-02-17\",19.5\\r\\n\"1989-02-18\",20.5\\r\\n\"1989-02-19\",15.7\\r\\n\"1989-02-20\",15.0\\r\\n\"1989-02-21\",16.1\\r\\n\"1989-02-22\",14.3\\r\\n\"1989-02-23\",13.0\\r\\n\"1989-02-24\",16.2\\r\\n\"1989-02-25\",17.7\\r\\n\"1989-02-26\",13.2\\r\\n\"1989-02-27\",15.8\\r\\n\"1989-02-28\",18.5\\r\\n\"1989-03-01\",20.4\\r\\n\"1989-03-02\",22.0\\r\\n\"1989-03-03\",19.7\\r\\n\"1989-03-04\",19.6\\r\\n\"1989-03-05\",20.3\\r\\n\"1989-03-06\",18.3\\r\\n\"1989-03-07\",18.9\\r\\n\"1989-03-08\",20.3\\r\\n\"1989-03-09\",21.4\\r\\n\"1989-03-10\",18.3\\r\\n\"1989-03-11\",17.8\\r\\n\"1989-03-12\",17.7\\r\\n\"1989-03-13\",12.8\\r\\n\"1989-03-14\",15.1\\r\\n\"1989-03-15\",15.0\\r\\n\"1989-03-16\",14.8\\r\\n\"1989-03-17\",12.0\\r\\n\"1989-03-18\",12.5\\r\\n\"1989-03-19\",15.0\\r\\n\"1989-03-20\",17.1\\r\\n\"1989-03-21\",17.3\\r\\n\"1989-03-22\",16.9\\r\\n\"1989-03-23\",16.5\\r\\n\"1989-03-24\",13.6\\r\\n\"1989-03-25\",13.2\\r\\n\"1989-03-26\",9.4\\r\\n\"1989-03-27\",9.5\\r\\n\"1989-03-28\",11.8\\r\\n\"1989-03-29\",10.4\\r\\n\"1989-03-30\",9.7\\r\\n\"1989-03-31\",12.6\\r\\n\"1989-04-01\",13.3\\r\\n\"1989-04-02\",15.1\\r\\n\"1989-04-03\",14.2\\r\\n\"1989-04-04\",14.2\\r\\n\"1989-04-05\",19.2\\r\\n\"1989-04-06\",12.6\\r\\n\"1989-04-07\",14.2\\r\\n\"1989-04-08\",11.9\\r\\n\"1989-04-09\",13.9\\r\\n\"1989-04-10\",13.5\\r\\n\"1989-04-11\",15.3\\r\\n\"1989-04-12\",13.9\\r\\n\"1989-04-13\",14.0\\r\\n\"1989-04-14\",12.9\\r\\n\"1989-04-15\",8.5\\r\\n\"1989-04-16\",11.4\\r\\n\"1989-04-17\",10.9\\r\\n\"1989-04-18\",12.0\\r\\n\"1989-04-19\",8.6\\r\\n\"1989-04-20\",9.0\\r\\n\"1989-04-21\",9.6\\r\\n\"1989-04-22\",10.2\\r\\n\"1989-04-23\",9.8\\r\\n\"1989-04-24\",8.3\\r\\n\"1989-04-25\",11.0\\r\\n\"1989-04-26\",11.9\\r\\n\"1989-04-27\",14.0\\r\\n\"1989-04-28\",15.8\\r\\n\"1989-04-29\",14.5\\r\\n\"1989-04-30\",13.2\\r\\n\"1989-05-01\",14.2\\r\\n\"1989-05-02\",14.6\\r\\n\"1989-05-03\",11.8\\r\\n\"1989-05-04\",14.4\\r\\n\"1989-05-05\",10.4\\r\\n\"1989-05-06\",10.3\\r\\n\"1989-05-07\",10.8\\r\\n\"1989-05-08\",10.5\\r\\n\"1989-05-09\",9.5\\r\\n\"1989-05-10\",12.5\\r\\n\"1989-05-11\",13.7\\r\\n\"1989-05-12\",12.7\\r\\n\"1989-05-13\",11.9\\r\\n\"1989-05-14\",11.4\\r\\n\"1989-05-15\",9.7\\r\\n\"1989-05-16\",8.3\\r\\n\"1989-05-17\",8.1\\r\\n\"1989-05-18\",11.7\\r\\n\"1989-05-19\",11.6\\r\\n\"1989-05-20\",7.4\\r\\n\"1989-05-21\",5.2\\r\\n\"1989-05-22\",11.0\\r\\n\"1989-05-23\",9.5\\r\\n\"1989-05-24\",9.2\\r\\n\"1989-05-25\",10.7\\r\\n\"1989-05-26\",9.0\\r\\n\"1989-05-27\",10.2\\r\\n\"1989-05-28\",10.3\\r\\n\"1989-05-29\",12.1\\r\\n\"1989-05-30\",13.2\\r\\n\"1989-05-31\",6.6\\r\\n\"1989-06-01\",2.3\\r\\n\"1989-06-02\",1.4\\r\\n\"1989-06-03\",2.1\\r\\n\"1989-06-04\",6.6\\r\\n\"1989-06-05\",8.9\\r\\n\"1989-06-06\",7.8\\r\\n\"1989-06-07\",9.0\\r\\n\"1989-06-08\",10.3\\r\\n\"1989-06-09\",7.9\\r\\n\"1989-06-10\",7.2\\r\\n\"1989-06-11\",8.6\\r\\n\"1989-06-12\",8.8\\r\\n\"1989-06-13\",6.2\\r\\n\"1989-06-14\",9.5\\r\\n\"1989-06-15\",10.2\\r\\n\"1989-06-16\",9.7\\r\\n\"1989-06-17\",11.2\\r\\n\"1989-06-18\",10.2\\r\\n\"1989-06-19\",10.1\\r\\n\"1989-06-20\",8.1\\r\\n\"1989-06-21\",6.6\\r\\n\"1989-06-22\",5.0\\r\\n\"1989-06-23\",4.7\\r\\n\"1989-06-24\",5.3\\r\\n\"1989-06-25\",4.5\\r\\n\"1989-06-26\",2.3\\r\\n\"1989-06-27\",1.4\\r\\n\"1989-06-28\",0.5\\r\\n\"1989-06-29\",2.4\\r\\n\"1989-06-30\",8.0\\r\\n\"1989-07-01\",6.0\\r\\n\"1989-07-02\",7.1\\r\\n\"1989-07-03\",9.7\\r\\n\"1989-07-04\",6.9\\r\\n\"1989-07-05\",5.3\\r\\n\"1989-07-06\",7.0\\r\\n\"1989-07-07\",6.2\\r\\n\"1989-07-08\",7.0\\r\\n\"1989-07-09\",9.7\\r\\n\"1989-07-10\",8.0\\r\\n\"1989-07-11\",8.5\\r\\n\"1989-07-12\",7.1\\r\\n\"1989-07-13\",7.5\\r\\n\"1989-07-14\",3.3\\r\\n\"1989-07-15\",1.8\\r\\n\"1989-07-16\",2.6\\r\\n\"1989-07-17\",5.3\\r\\n\"1989-07-18\",5.8\\r\\n\"1989-07-19\",5.8\\r\\n\"1989-07-20\",7.2\\r\\n\"1989-07-21\",5.3\\r\\n\"1989-07-22\",1.6\\r\\n\"1989-07-23\",3.1\\r\\n\"1989-07-24\",5.3\\r\\n\"1989-07-25\",7.7\\r\\n\"1989-07-26\",4.2\\r\\n\"1989-07-27\",5.5\\r\\n\"1989-07-28\",9.0\\r\\n\"1989-07-29\",11.2\\r\\n\"1989-07-30\",8.0\\r\\n\"1989-07-31\",7.6\\r\\n\"1989-08-01\",3.7\\r\\n\"1989-08-02\",7.5\\r\\n\"1989-08-03\",8.1\\r\\n\"1989-08-04\",8.4\\r\\n\"1989-08-05\",7.1\\r\\n\"1989-08-06\",7.6\\r\\n\"1989-08-07\",7.6\\r\\n\"1989-08-08\",5.6\\r\\n\"1989-08-09\",7.0\\r\\n\"1989-08-10\",10.5\\r\\n\"1989-08-11\",7.3\\r\\n\"1989-08-12\",7.8\\r\\n\"1989-08-13\",5.8\\r\\n\"1989-08-14\",3.8\\r\\n\"1989-08-15\",5.8\\r\\n\"1989-08-16\",6.7\\r\\n\"1989-08-17\",6.6\\r\\n\"1989-08-18\",6.6\\r\\n\"1989-08-19\",9.0\\r\\n\"1989-08-20\",8.1\\r\\n\"1989-08-21\",5.1\\r\\n\"1989-08-22\",8.6\\r\\n\"1989-08-23\",7.0\\r\\n\"1989-08-24\",5.5\\r\\n\"1989-08-25\",7.4\\r\\n\"1989-08-26\",6.2\\r\\n\"1989-08-27\",4.2\\r\\n\"1989-08-28\",6.3\\r\\n\"1989-08-29\",7.0\\r\\n\"1989-08-30\",4.0\\r\\n\"1989-08-31\",8.0\\r\\n\"1989-09-01\",8.8\\r\\n\"1989-09-02\",8.8\\r\\n\"1989-09-03\",6.1\\r\\n\"1989-09-04\",8.6\\r\\n\"1989-09-05\",8.9\\r\\n\"1989-09-06\",7.8\\r\\n\"1989-09-07\",5.0\\r\\n\"1989-09-08\",7.0\\r\\n\"1989-09-09\",13.3\\r\\n\"1989-09-10\",7.9\\r\\n\"1989-09-11\",7.5\\r\\n\"1989-09-12\",8.3\\r\\n\"1989-09-13\",7.2\\r\\n\"1989-09-14\",6.5\\r\\n\"1989-09-15\",8.9\\r\\n\"1989-09-16\",7.4\\r\\n\"1989-09-17\",9.9\\r\\n\"1989-09-18\",9.3\\r\\n\"1989-09-19\",10.6\\r\\n\"1989-09-20\",8.6\\r\\n\"1989-09-21\",7.2\\r\\n\"1989-09-22\",12.6\\r\\n\"1989-09-23\",7.8\\r\\n\"1989-09-24\",6.3\\r\\n\"1989-09-25\",9.2\\r\\n\"1989-09-26\",5.8\\r\\n\"1989-09-27\",9.0\\r\\n\"1989-09-28\",5.0\\r\\n\"1989-09-29\",11.9\\r\\n\"1989-09-30\",13.4\\r\\n\"1989-10-01\",10.5\\r\\n\"1989-10-02\",6.2\\r\\n\"1989-10-03\",5.1\\r\\n\"1989-10-04\",9.5\\r\\n\"1989-10-05\",11.7\\r\\n\"1989-10-06\",9.2\\r\\n\"1989-10-07\",7.3\\r\\n\"1989-10-08\",9.7\\r\\n\"1989-10-09\",9.4\\r\\n\"1989-10-10\",10.0\\r\\n\"1989-10-11\",10.9\\r\\n\"1989-10-12\",11.0\\r\\n\"1989-10-13\",10.9\\r\\n\"1989-10-14\",8.0\\r\\n\"1989-10-15\",11.2\\r\\n\"1989-10-16\",7.5\\r\\n\"1989-10-17\",7.2\\r\\n\"1989-10-18\",13.2\\r\\n\"1989-10-19\",12.9\\r\\n\"1989-10-20\",9.4\\r\\n\"1989-10-21\",10.2\\r\\n\"1989-10-22\",9.5\\r\\n\"1989-10-23\",12.4\\r\\n\"1989-10-24\",10.2\\r\\n\"1989-10-25\",13.4\\r\\n\"1989-10-26\",11.6\\r\\n\"1989-10-27\",8.0\\r\\n\"1989-10-28\",9.0\\r\\n\"1989-10-29\",9.3\\r\\n\"1989-10-30\",13.5\\r\\n\"1989-10-31\",8.0\\r\\n\"1989-11-01\",8.1\\r\\n\"1989-11-02\",10.0\\r\\n\"1989-11-03\",8.5\\r\\n\"1989-11-04\",12.5\\r\\n\"1989-11-05\",15.0\\r\\n\"1989-11-06\",13.3\\r\\n\"1989-11-07\",11.0\\r\\n\"1989-11-08\",11.9\\r\\n\"1989-11-09\",8.3\\r\\n\"1989-11-10\",9.7\\r\\n\"1989-11-11\",11.3\\r\\n\"1989-11-12\",12.5\\r\\n\"1989-11-13\",9.4\\r\\n\"1989-11-14\",11.4\\r\\n\"1989-11-15\",13.2\\r\\n\"1989-11-16\",13.8\\r\\n\"1989-11-17\",16.0\\r\\n\"1989-11-18\",10.9\\r\\n\"1989-11-19\",11.9\\r\\n\"1989-11-20\",12.4\\r\\n\"1989-11-21\",13.2\\r\\n\"1989-11-22\",15.5\\r\\n\"1989-11-23\",21.6\\r\\n\"1989-11-24\",14.9\\r\\n\"1989-11-25\",14.4\\r\\n\"1989-11-26\",12.9\\r\\n\"1989-11-27\",13.1\\r\\n\"1989-11-28\",14.0\\r\\n\"1989-11-29\",17.9\\r\\n\"1989-11-30\",17.7\\r\\n\"1989-12-01\",16.3\\r\\n\"1989-12-02\",18.3\\r\\n\"1989-12-03\",13.7\\r\\n\"1989-12-04\",13.3\\r\\n\"1989-12-05\",10.6\\r\\n\"1989-12-06\",14.1\\r\\n\"1989-12-07\",16.0\\r\\n\"1989-12-08\",16.5\\r\\n\"1989-12-09\",14.1\\r\\n\"1989-12-10\",18.7\\r\\n\"1989-12-11\",16.2\\r\\n\"1989-12-12\",14.8\\r\\n\"1989-12-13\",12.6\\r\\n\"1989-12-14\",10.4\\r\\n\"1989-12-15\",12.2\\r\\n\"1989-12-16\",12.6\\r\\n\"1989-12-17\",12.1\\r\\n\"1989-12-18\",17.3\\r\\n\"1989-12-19\",16.4\\r\\n\"1989-12-20\",12.6\\r\\n\"1989-12-21\",12.3\\r\\n\"1989-12-22\",11.8\\r\\n\"1989-12-23\",12.0\\r\\n\"1989-12-24\",12.7\\r\\n\"1989-12-25\",16.4\\r\\n\"1989-12-26\",16.0\\r\\n\"1989-12-27\",13.3\\r\\n\"1989-12-28\",11.7\\r\\n\"1989-12-29\",10.4\\r\\n\"1989-12-30\",14.4\\r\\n\"1989-12-31\",12.7\\r\\n\"1990-01-01\",14.8\\r\\n\"1990-01-02\",13.3\\r\\n\"1990-01-03\",15.6\\r\\n\"1990-01-04\",14.5\\r\\n\"1990-01-05\",14.3\\r\\n\"1990-01-06\",15.3\\r\\n\"1990-01-07\",16.4\\r\\n\"1990-01-08\",14.8\\r\\n\"1990-01-09\",17.4\\r\\n\"1990-01-10\",18.8\\r\\n\"1990-01-11\",22.1\\r\\n\"1990-01-12\",19.0\\r\\n\"1990-01-13\",15.5\\r\\n\"1990-01-14\",15.8\\r\\n\"1990-01-15\",14.7\\r\\n\"1990-01-16\",10.7\\r\\n\"1990-01-17\",11.5\\r\\n\"1990-01-18\",15.0\\r\\n\"1990-01-19\",14.5\\r\\n\"1990-01-20\",14.5\\r\\n\"1990-01-21\",13.3\\r\\n\"1990-01-22\",14.3\\r\\n\"1990-01-23\",14.3\\r\\n\"1990-01-24\",20.5\\r\\n\"1990-01-25\",15.0\\r\\n\"1990-01-26\",17.1\\r\\n\"1990-01-27\",16.9\\r\\n\"1990-01-28\",16.9\\r\\n\"1990-01-29\",13.6\\r\\n\"1990-01-30\",16.4\\r\\n\"1990-01-31\",16.1\\r\\n\"1990-02-01\",12.0\\r\\n\"1990-02-02\",12.2\\r\\n\"1990-02-03\",14.8\\r\\n\"1990-02-04\",14.8\\r\\n\"1990-02-05\",14.4\\r\\n\"1990-02-06\",12.9\\r\\n\"1990-02-07\",13.4\\r\\n\"1990-02-08\",15.9\\r\\n\"1990-02-09\",16.1\\r\\n\"1990-02-10\",17.6\\r\\n\"1990-02-11\",15.6\\r\\n\"1990-02-12\",15.0\\r\\n\"1990-02-13\",13.0\\r\\n\"1990-02-14\",14.1\\r\\n\"1990-02-15\",17.3\\r\\n\"1990-02-16\",15.7\\r\\n\"1990-02-17\",18.6\\r\\n\"1990-02-18\",12.7\\r\\n\"1990-02-19\",14.0\\r\\n\"1990-02-20\",13.7\\r\\n\"1990-02-21\",16.3\\r\\n\"1990-02-22\",20.0\\r\\n\"1990-02-23\",17.0\\r\\n\"1990-02-24\",15.2\\r\\n\"1990-02-25\",16.5\\r\\n\"1990-02-26\",16.5\\r\\n\"1990-02-27\",17.3\\r\\n\"1990-02-28\",19.1\\r\\n\"1990-03-01\",19.3\\r\\n\"1990-03-02\",17.3\\r\\n\"1990-03-03\",19.0\\r\\n\"1990-03-04\",19.8\\r\\n\"1990-03-05\",19.3\\r\\n\"1990-03-06\",17.2\\r\\n\"1990-03-07\",14.2\\r\\n\"1990-03-08\",10.3\\r\\n\"1990-03-09\",13.0\\r\\n\"1990-03-10\",15.3\\r\\n\"1990-03-11\",15.0\\r\\n\"1990-03-12\",12.1\\r\\n\"1990-03-13\",9.2\\r\\n\"1990-03-14\",11.0\\r\\n\"1990-03-15\",15.0\\r\\n\"1990-03-16\",11.6\\r\\n\"1990-03-17\",11.6\\r\\n\"1990-03-18\",15.1\\r\\n\"1990-03-19\",15.0\\r\\n\"1990-03-20\",13.6\\r\\n\"1990-03-21\",12.5\\r\\n\"1990-03-22\",14.3\\r\\n\"1990-03-23\",16.0\\r\\n\"1990-03-24\",17.4\\r\\n\"1990-03-25\",16.9\\r\\n\"1990-03-26\",18.0\\r\\n\"1990-03-27\",20.6\\r\\n\"1990-03-28\",14.2\\r\\n\"1990-03-29\",10.9\\r\\n\"1990-03-30\",11.9\\r\\n\"1990-03-31\",13.3\\r\\n\"1990-04-01\",15.3\\r\\n\"1990-04-02\",14.7\\r\\n\"1990-04-03\",11.0\\r\\n\"1990-04-04\",12.2\\r\\n\"1990-04-05\",14.2\\r\\n\"1990-04-06\",17.0\\r\\n\"1990-04-07\",15.8\\r\\n\"1990-04-08\",15.2\\r\\n\"1990-04-09\",15.1\\r\\n\"1990-04-10\",14.7\\r\\n\"1990-04-11\",18.5\\r\\n\"1990-04-12\",16.4\\r\\n\"1990-04-13\",18.4\\r\\n\"1990-04-14\",15.1\\r\\n\"1990-04-15\",9.9\\r\\n\"1990-04-16\",10.2\\r\\n\"1990-04-17\",12.6\\r\\n\"1990-04-18\",13.2\\r\\n\"1990-04-19\",11.5\\r\\n\"1990-04-20\",13.8\\r\\n\"1990-04-21\",14.5\\r\\n\"1990-04-22\",14.7\\r\\n\"1990-04-23\",11.2\\r\\n\"1990-04-24\",12.7\\r\\n\"1990-04-25\",13.7\\r\\n\"1990-04-26\",11.5\\r\\n\"1990-04-27\",10.4\\r\\n\"1990-04-28\",8.9\\r\\n\"1990-04-29\",11.1\\r\\n\"1990-04-30\",9.5\\r\\n\"1990-05-01\",13.0\\r\\n\"1990-05-02\",13.9\\r\\n\"1990-05-03\",12.6\\r\\n\"1990-05-04\",14.3\\r\\n\"1990-05-05\",16.0\\r\\n\"1990-05-06\",13.3\\r\\n\"1990-05-07\",7.0\\r\\n\"1990-05-08\",4.9\\r\\n\"1990-05-09\",6.9\\r\\n\"1990-05-10\",13.7\\r\\n\"1990-05-11\",10.6\\r\\n\"1990-05-12\",12.3\\r\\n\"1990-05-13\",11.1\\r\\n\"1990-05-14\",10.2\\r\\n\"1990-05-15\",9.5\\r\\n\"1990-05-16\",8.9\\r\\n\"1990-05-17\",13.4\\r\\n\"1990-05-18\",9.1\\r\\n\"1990-05-19\",9.4\\r\\n\"1990-05-20\",8.7\\r\\n\"1990-05-21\",5.8\\r\\n\"1990-05-22\",4.5\\r\\n\"1990-05-23\",7.2\\r\\n\"1990-05-24\",10.0\\r\\n\"1990-05-25\",10.5\\r\\n\"1990-05-26\",10.7\\r\\n\"1990-05-27\",8.2\\r\\n\"1990-05-28\",6.1\\r\\n\"1990-05-29\",4.5\\r\\n\"1990-05-30\",6.1\\r\\n\"1990-05-31\",9.8\\r\\n\"1990-06-01\",9.7\\r\\n\"1990-06-02\",8.2\\r\\n\"1990-06-03\",8.4\\r\\n\"1990-06-04\",8.5\\r\\n\"1990-06-05\",10.4\\r\\n\"1990-06-06\",6.8\\r\\n\"1990-06-07\",6.0\\r\\n\"1990-06-08\",6.6\\r\\n\"1990-06-09\",7.8\\r\\n\"1990-06-10\",10.3\\r\\n\"1990-06-11\",7.2\\r\\n\"1990-06-12\",7.4\\r\\n\"1990-06-13\",11.4\\r\\n\"1990-06-14\",5.4\\r\\n\"1990-06-15\",4.4\\r\\n\"1990-06-16\",6.4\\r\\n\"1990-06-17\",9.3\\r\\n\"1990-06-18\",7.7\\r\\n\"1990-06-19\",8.1\\r\\n\"1990-06-20\",8.3\\r\\n\"1990-06-21\",9.1\\r\\n\"1990-06-22\",7.7\\r\\n\"1990-06-23\",10.6\\r\\n\"1990-06-24\",8.2\\r\\n\"1990-06-25\",7.9\\r\\n\"1990-06-26\",5.2\\r\\n\"1990-06-27\",5.9\\r\\n\"1990-06-28\",3.7\\r\\n\"1990-06-29\",5.6\\r\\n\"1990-06-30\",9.4\\r\\n\"1990-07-01\",7.4\\r\\n\"1990-07-02\",7.3\\r\\n\"1990-07-03\",7.7\\r\\n\"1990-07-04\",7.7\\r\\n\"1990-07-05\",9.3\\r\\n\"1990-07-06\",4.4\\r\\n\"1990-07-07\",5.7\\r\\n\"1990-07-08\",10.2\\r\\n\"1990-07-09\",10.2\\r\\n\"1990-07-10\",9.3\\r\\n\"1990-07-11\",5.4\\r\\n\"1990-07-12\",5.0\\r\\n\"1990-07-13\",7.6\\r\\n\"1990-07-14\",9.6\\r\\n\"1990-07-15\",10.4\\r\\n\"1990-07-16\",11.2\\r\\n\"1990-07-17\",9.1\\r\\n\"1990-07-18\",11.2\\r\\n\"1990-07-19\",6.8\\r\\n\"1990-07-20\",8.3\\r\\n\"1990-07-21\",9.7\\r\\n\"1990-07-22\",9.6\\r\\n\"1990-07-23\",9.8\\r\\n\"1990-07-24\",10.8\\r\\n\"1990-07-25\",9.2\\r\\n\"1990-07-26\",6.5\\r\\n\"1990-07-27\",8.1\\r\\n\"1990-07-28\",7.3\\r\\n\"1990-07-29\",7.9\\r\\n\"1990-07-30\",6.0\\r\\n\"1990-07-31\",5.0\\r\\n\"1990-08-01\",6.8\\r\\n\"1990-08-02\",9.8\\r\\n\"1990-08-03\",5.7\\r\\n\"1990-08-04\",8.6\\r\\n\"1990-08-05\",10.6\\r\\n\"1990-08-06\",7.8\\r\\n\"1990-08-07\",7.7\\r\\n\"1990-08-08\",8.6\\r\\n\"1990-08-09\",6.5\\r\\n\"1990-08-10\",6.9\\r\\n\"1990-08-11\",6.4\\r\\n\"1990-08-12\",8.5\\r\\n\"1990-08-13\",7.8\\r\\n\"1990-08-14\",9.3\\r\\n\"1990-08-15\",8.4\\r\\n\"1990-08-16\",7.8\\r\\n\"1990-08-17\",7.4\\r\\n\"1990-08-18\",7.7\\r\\n\"1990-08-19\",8.9\\r\\n\"1990-08-20\",9.7\\r\\n\"1990-08-21\",9.9\\r\\n\"1990-08-22\",6.1\\r\\n\"1990-08-23\",6.6\\r\\n\"1990-08-24\",7.6\\r\\n\"1990-08-25\",7.4\\r\\n\"1990-08-26\",8.0\\r\\n\"1990-08-27\",2.1\\r\\n\"1990-08-28\",5.9\\r\\n\"1990-08-29\",11.6\\r\\n\"1990-08-30\",8.6\\r\\n\"1990-08-31\",7.9\\r\\n\"1990-09-01\",6.0\\r\\n\"1990-09-02\",9.5\\r\\n\"1990-09-03\",8.6\\r\\n\"1990-09-04\",7.6\\r\\n\"1990-09-05\",10.4\\r\\n\"1990-09-06\",10.3\\r\\n\"1990-09-07\",7.5\\r\\n\"1990-09-08\",3.0\\r\\n\"1990-09-09\",5.3\\r\\n\"1990-09-10\",10.5\\r\\n\"1990-09-11\",14.6\\r\\n\"1990-09-12\",12.6\\r\\n\"1990-09-13\",9.8\\r\\n\"1990-09-14\",7.2\\r\\n\"1990-09-15\",10.1\\r\\n\"1990-09-16\",10.4\\r\\n\"1990-09-17\",3.7\\r\\n\"1990-09-18\",7.3\\r\\n\"1990-09-19\",11.6\\r\\n\"1990-09-20\",16.3\\r\\n\"1990-09-21\",9.6\\r\\n\"1990-09-22\",6.8\\r\\n\"1990-09-23\",5.2\\r\\n\"1990-09-24\",10.6\\r\\n\"1990-09-25\",16.3\\r\\n\"1990-09-26\",9.8\\r\\n\"1990-09-27\",4.6\\r\\n\"1990-09-28\",11.1\\r\\n\"1990-09-29\",8.7\\r\\n\"1990-09-30\",10.0\\r\\n\"1990-10-01\",11.3\\r\\n\"1990-10-02\",10.5\\r\\n\"1990-10-03\",9.9\\r\\n\"1990-10-04\",11.0\\r\\n\"1990-10-05\",14.0\\r\\n\"1990-10-06\",9.2\\r\\n\"1990-10-07\",9.8\\r\\n\"1990-10-08\",6.0\\r\\n\"1990-10-09\",9.8\\r\\n\"1990-10-10\",9.2\\r\\n\"1990-10-11\",11.8\\r\\n\"1990-10-12\",10.3\\r\\n\"1990-10-13\",7.5\\r\\n\"1990-10-14\",7.7\\r\\n\"1990-10-15\",15.8\\r\\n\"1990-10-16\",14.6\\r\\n\"1990-10-17\",10.5\\r\\n\"1990-10-18\",11.3\\r\\n\"1990-10-19\",10.9\\r\\n\"1990-10-20\",6.4\\r\\n\"1990-10-21\",10.9\\r\\n\"1990-10-22\",9.0\\r\\n\"1990-10-23\",10.9\\r\\n\"1990-10-24\",12.4\\r\\n\"1990-10-25\",11.6\\r\\n\"1990-10-26\",13.3\\r\\n\"1990-10-27\",14.4\\r\\n\"1990-10-28\",18.4\\r\\n\"1990-10-29\",13.6\\r\\n\"1990-10-30\",14.9\\r\\n\"1990-10-31\",14.8\\r\\n\"1990-11-01\",15.4\\r\\n\"1990-11-02\",11.8\\r\\n\"1990-11-03\",13.0\\r\\n\"1990-11-04\",11.1\\r\\n\"1990-11-05\",12.5\\r\\n\"1990-11-06\",18.3\\r\\n\"1990-11-07\",19.2\\r\\n\"1990-11-08\",15.4\\r\\n\"1990-11-09\",13.1\\r\\n\"1990-11-10\",11.5\\r\\n\"1990-11-11\",8.6\\r\\n\"1990-11-12\",12.6\\r\\n\"1990-11-13\",13.8\\r\\n\"1990-11-14\",14.6\\r\\n\"1990-11-15\",13.2\\r\\n\"1990-11-16\",12.3\\r\\n\"1990-11-17\",8.8\\r\\n\"1990-11-18\",10.7\\r\\n\"1990-11-19\",9.9\\r\\n\"1990-11-20\",8.3\\r\\n\"1990-11-21\",15.0\\r\\n\"1990-11-22\",12.2\\r\\n\"1990-11-23\",10.5\\r\\n\"1990-11-24\",11.1\\r\\n\"1990-11-25\",13.0\\r\\n\"1990-11-26\",12.9\\r\\n\"1990-11-27\",8.8\\r\\n\"1990-11-28\",14.7\\r\\n\"1990-11-29\",14.7\\r\\n\"1990-11-30\",12.7\\r\\n\"1990-12-01\",13.3\\r\\n\"1990-12-02\",13.2\\r\\n\"1990-12-03\",16.2\\r\\n\"1990-12-04\",17.3\\r\\n\"1990-12-05\",20.5\\r\\n\"1990-12-06\",20.2\\r\\n\"1990-12-07\",19.4\\r\\n\"1990-12-08\",15.5\\r\\n\"1990-12-09\",14.1\\r\\n\"1990-12-10\",11.0\\r\\n\"1990-12-11\",11.1\\r\\n\"1990-12-12\",14.0\\r\\n\"1990-12-13\",11.4\\r\\n\"1990-12-14\",12.5\\r\\n\"1990-12-15\",13.4\\r\\n\"1990-12-16\",13.6\\r\\n\"1990-12-17\",13.9\\r\\n\"1990-12-18\",17.2\\r\\n\"1990-12-19\",14.7\\r\\n\"1990-12-20\",15.4\\r\\n\"1990-12-21\",13.1\\r\\n\"1990-12-22\",13.2\\r\\n\"1990-12-23\",13.9\\r\\n\"1990-12-24\",10.0\\r\\n\"1990-12-25\",12.9\\r\\n\"1990-12-26\",14.6\\r\\n\"1990-12-27\",14.0\\r\\n\"1990-12-28\",13.6\\r\\n\"1990-12-29\",13.5\\r\\n\"1990-12-30\",15.7\\r\\n\"1990-12-31\",13.0\\r\\n\\r\\nDaily minimum temperatures in Melbourne, Australia, 1981-1990\\r\\n\\r\\n'} # file is uploaded to the current directory ! ls arrhythmia.data daily-minimum-temperatures-in-me.csv sample_data # open the file # the last few lines are junk df = pd . read_csv ( 'daily-minimum-temperatures-in-me.csv' , error_bad_lines = False ) df . head () b'Skipping line 3653: expected 2 fields, saw 3\\n' .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Daily minimum temperatures in Melbourne, Australia, 1981-1990 0 1981-01-01 20.7 1 1981-01-02 17.9 2 1981-01-03 18.8 3 1981-01-04 14.6 4 1981-01-05 15.8 # upload a Python file with some useful functions (meant for fake_util.py) from google.colab import files uploaded = files . upload () <input type=\"file\" id=\"files-c590b543-ffcc-4177-91d3-62725b363169\" name=\"files[]\" multiple disabled /> <output id=\"result-c590b543-ffcc-4177-91d3-62725b363169\"> Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. </output> <script src=\"/nbextensions/google.colab/files.js\"></script> Saving fake_util.py to fake_util.py from fake_util import my_useful_function my_useful_function () hello world ! pwd /content","title":"Part 3: Upload the file yourself"},{"location":"bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/#part-4-access-files-from-google-drive","text":"# Access files from your Google Drive from google.colab import drive drive . mount ( '/content/gdrive' ) # Check current directory - now gdrive is there ! ls # What's in gdrive? ! ls gdrive # Whoa! Look at all this great VIP content! ! ls '/content/gdrive/My Drive/' Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Part 4: Access files from Google Drive"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/","text":"Crash Course: Numpy \u00b6 by Jawad Haider 00 - Numpy Arrays \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy 1.1 Installation Instructions 1.2 Using NumPy 2 NumPy Arrays 2.1 Creating NumPy Arrays 2.1.1 1. From a Python List 2.1.2 2. Built-in Methods 2.1.3 arange 2.1.4 zeros and ones 2.1.5 linspace 2.1.6 eye 2.2 Random 2.2.1 1. rand 2.2.2 2. randn 2.2.3 3. randint 2.2.4 4. seed 2.3 Array Attributes and Methods 2.3.1 1. Reshape 2.3.2 2. max, min, argmax, argmin 2.3.3 3. Shape 2.3.4 4. dtype 3 Great Job! Thats the end of this part. NumPy \u00b6 NumPy is a powerful linear algebra library for Python. What makes it so important is that almost all of the libraries in the PyData ecosystem (pandas, scipy, scikit-learn, etc.) rely on NumPy as one of their main building blocks. Plus we will use it to generate data for our analysis examples later on! NumPy is also incredibly fast, as it has bindings to C libraries. For more info on why you would want to use arrays instead of lists, check out this great StackOverflow post . We will only learn the basics of NumPy. To get started we need to install it! Installation Instructions \u00b6 NumPy is already included in your environment! You are good to go if you are using the course environment! For those not using the provided environment: It is highly recommended you install Python using the Anaconda distribution to make sure all underlying dependencies (such as Linear Algebra libraries) all sync up with the use of a conda install. If you have Anaconda, install NumPy by going to your terminal or command prompt and typing: conda install numpy If you do not have Anaconda and can not install it, please refer to Numpy\u2019s official documentation on various installation instructions. Using NumPy \u00b6 Once you\u2019ve installed NumPy you can import it as a library: import numpy as np NumPy has many built-in functions and capabilities. We won\u2019t cover them all but instead we will focus on some of the most important aspects of NumPy: vectors, arrays, matrices and number generation. Let\u2019s start by discussing arrays. NumPy Arrays \u00b6 NumPy arrays are the main way we will use NumPy throughout the course. NumPy arrays essentially come in two flavors: vectors and matrices. Vectors are strictly 1-dimensional (1D) arrays and matrices are 2D (but you should note a matrix can still have only one row or one column). Let\u2019s begin our introduction by exploring how to create NumPy arrays. Creating NumPy Arrays \u00b6 1. From a Python List \u00b6 We can create an array by directly converting a list or list of lists: my_list = [ 1 , 2 , 3 ] my_list [1, 2, 3] np . array ( my_list ) array([1, 2, 3]) my_matrix = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] my_matrix [[1, 2, 3], [4, 5, 6], [7, 8, 9]] np . array ( my_matrix ) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 2. Built-in Methods \u00b6 There are lots of built-in ways to generate arrays. arange \u00b6 Return evenly spaced values within a given interval.[ Numpy ndarray arange ] np . arange ( 0 , 10 ) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) np . arange ( 0 , 11 , 2 ) array([ 0, 2, 4, 6, 8, 10]) zeros and ones \u00b6 Generate arrays of zeros or ones.[ Numpy ndarray zeros ] np.zeros(3) np . zeros (( 5 , 5 )) array([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) np . ones ( 3 ) array([1., 1., 1.]) np . ones (( 3 , 3 )) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) linspace \u00b6 Return evenly spaced numbers over a specified interval.[ Numpy ndarray linspace ] np . linspace ( 0 , 10 , 3 ) array([ 0., 5., 10.]) np . linspace ( 0 , 5 , 20 ) array([0. , 0.26315789, 0.52631579, 0.78947368, 1.05263158, 1.31578947, 1.57894737, 1.84210526, 2.10526316, 2.36842105, 2.63157895, 2.89473684, 3.15789474, 3.42105263, 3.68421053, 3.94736842, 4.21052632, 4.47368421, 4.73684211, 5. ]) Note that .linspace() includes the stop value. To obtain an array of common fractions, increase the number of items: np . linspace ( 0 , 5 , 21 ) array([0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. , 2.25, 2.5 , 2.75, 3. , 3.25, 3.5 , 3.75, 4. , 4.25, 4.5 , 4.75, 5. ]) eye \u00b6 Creates an identity matrix [ Numpy eye ] np . eye ( 4 ) array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) Random \u00b6 Numpy also has lots of ways to create random number arrays. Here we will go through some of the most used methods from random module 1. rand \u00b6 Creates an array of the given shape and populates it with random samples from a uniform distribution over [0, 1) . [ Random rand ] np . random . rand ( 2 ) array([0.69647666, 0.14395438]) np . random . rand ( 5 , 5 ) array([[0.9112553 , 0.75849901, 0.43392287, 0.4134459 , 0.10902179], [0.66881652, 0.21265267, 0.21783956, 0.08716564, 0.46147918], [0.16064897, 0.38241433, 0.50076915, 0.58926492, 0.69837196], [0.88502465, 0.2996012 , 0.49291933, 0.75316852, 0.29998398], [0.42345042, 0.57034504, 0.94797283, 0.70571464, 0.35788149]]) 2. randn \u00b6 Returns a sample (or samples) from the \u201cstandard normal\u201d distribution [\u03c3 = 1]. Unlike rand which is uniform, values closer to zero are more likely to appear. [ Radnom randn ] np . random . randn ( 2 ) array([-0.55673554, -0.08515858]) np . random . randn ( 5 , 5 ) array([[ 0.83041645, -1.22369138, 0.258011 , 0.90984287, -0.48702078], [-0.88539528, -0.54034218, -0.39928196, 0.85910869, -0.36305332], [ 0.132046 , -1.28709664, 0.49352402, 0.80293611, 0.2601146 ], [ 0.74912365, 0.16013944, 0.39345536, -0.52355146, 1.0536796 ], [ 0.00293273, -0.14715505, -1.22460234, -0.65347358, -0.31514422]]) 3. randint \u00b6 Returns random integers from low (inclusive) to high (exclusive). [ Random randint ] np . random . randint ( 1 , 100 ) 42 np . random . randint ( 1 , 100 , 10 ) array([33, 26, 51, 78, 89, 15, 42, 68, 14, 62]) 4. seed \u00b6 Can be used to set the random state, so that the same \u201crandom\u201d results can be reproduced. [ Numpy ndarray random ] np . random . seed ( 42 ) np . random . rand ( 4 ) array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) np . random . seed ( 42 ) np . random . rand ( 4 ) array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) Array Attributes and Methods \u00b6 Let\u2019s discuss some useful attributes and methods for an array: In particular, the reshape attribute and max,min,argmax, argmin, shape & the dytpe methods Let\u2019s first create two numpy arrays to experiment with :) arr = np . arange ( 25 ) ranarr = np . random . randint ( 0 , 50 , 10 ) arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]) ranarr array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) 1. Reshape \u00b6 Returns an array containing the same data with a new shape.[ Numpy ndarray reshape ] arr . reshape ( 5 , 5 ) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]) 2. max, min, argmax, argmin \u00b6 These are useful methods for finding max or min values. Or to find their index locations using argmin or argmax ranarr array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) ranarr . max () 39 ranarr . argmax () 7 ranarr . min () 2 ranarr . argmin () 9 3. Shape \u00b6 Shape is an attribute that arrays have (not a method):[ Numpy ndarray Shape ] # Vector arr . shape (25,) # Notice the two sets of brackets arr . reshape ( 1 , 25 ) array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) arr . reshape ( 1 , 25 ) . shape (1, 25) arr . reshape ( 25 , 1 ) array([[ 0], [ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]]) arr . reshape ( 25 , 1 ) . shape (25, 1) 4. dtype \u00b6 You can also grab the data type of the object in the array:[ Numpy ndarray dtype ] arr . dtype dtype('int64') arr2 = np . array ([ 1.2 , 3.4 , 5.6 ]) arr2 . dtype dtype('float64') Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"00 NumPy Arrays"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#crash-course-numpy","text":"by Jawad Haider","title":"Crash Course: Numpy"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#00-numpy-arrays","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy 1.1 Installation Instructions 1.2 Using NumPy 2 NumPy Arrays 2.1 Creating NumPy Arrays 2.1.1 1. From a Python List 2.1.2 2. Built-in Methods 2.1.3 arange 2.1.4 zeros and ones 2.1.5 linspace 2.1.6 eye 2.2 Random 2.2.1 1. rand 2.2.2 2. randn 2.2.3 3. randint 2.2.4 4. seed 2.3 Array Attributes and Methods 2.3.1 1. Reshape 2.3.2 2. max, min, argmax, argmin 2.3.3 3. Shape 2.3.4 4. dtype 3 Great Job! Thats the end of this part.","title":"00 - Numpy Arrays"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#numpy","text":"NumPy is a powerful linear algebra library for Python. What makes it so important is that almost all of the libraries in the PyData ecosystem (pandas, scipy, scikit-learn, etc.) rely on NumPy as one of their main building blocks. Plus we will use it to generate data for our analysis examples later on! NumPy is also incredibly fast, as it has bindings to C libraries. For more info on why you would want to use arrays instead of lists, check out this great StackOverflow post . We will only learn the basics of NumPy. To get started we need to install it!","title":"NumPy"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#installation-instructions","text":"NumPy is already included in your environment! You are good to go if you are using the course environment! For those not using the provided environment: It is highly recommended you install Python using the Anaconda distribution to make sure all underlying dependencies (such as Linear Algebra libraries) all sync up with the use of a conda install. If you have Anaconda, install NumPy by going to your terminal or command prompt and typing: conda install numpy If you do not have Anaconda and can not install it, please refer to Numpy\u2019s official documentation on various installation instructions.","title":"Installation Instructions"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#using-numpy","text":"Once you\u2019ve installed NumPy you can import it as a library: import numpy as np NumPy has many built-in functions and capabilities. We won\u2019t cover them all but instead we will focus on some of the most important aspects of NumPy: vectors, arrays, matrices and number generation. Let\u2019s start by discussing arrays.","title":"Using NumPy"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#numpy-arrays","text":"NumPy arrays are the main way we will use NumPy throughout the course. NumPy arrays essentially come in two flavors: vectors and matrices. Vectors are strictly 1-dimensional (1D) arrays and matrices are 2D (but you should note a matrix can still have only one row or one column). Let\u2019s begin our introduction by exploring how to create NumPy arrays.","title":"NumPy Arrays"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#creating-numpy-arrays","text":"","title":"Creating NumPy Arrays"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#1-from-a-python-list","text":"We can create an array by directly converting a list or list of lists: my_list = [ 1 , 2 , 3 ] my_list [1, 2, 3] np . array ( my_list ) array([1, 2, 3]) my_matrix = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] my_matrix [[1, 2, 3], [4, 5, 6], [7, 8, 9]] np . array ( my_matrix ) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])","title":"1. From a Python List"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#2-built-in-methods","text":"There are lots of built-in ways to generate arrays.","title":"2. Built-in Methods"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#arange","text":"Return evenly spaced values within a given interval.[ Numpy ndarray arange ] np . arange ( 0 , 10 ) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) np . arange ( 0 , 11 , 2 ) array([ 0, 2, 4, 6, 8, 10])","title":"arange"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#zeros-and-ones","text":"Generate arrays of zeros or ones.[ Numpy ndarray zeros ] np.zeros(3) np . zeros (( 5 , 5 )) array([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) np . ones ( 3 ) array([1., 1., 1.]) np . ones (( 3 , 3 )) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])","title":"zeros and ones"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#linspace","text":"Return evenly spaced numbers over a specified interval.[ Numpy ndarray linspace ] np . linspace ( 0 , 10 , 3 ) array([ 0., 5., 10.]) np . linspace ( 0 , 5 , 20 ) array([0. , 0.26315789, 0.52631579, 0.78947368, 1.05263158, 1.31578947, 1.57894737, 1.84210526, 2.10526316, 2.36842105, 2.63157895, 2.89473684, 3.15789474, 3.42105263, 3.68421053, 3.94736842, 4.21052632, 4.47368421, 4.73684211, 5. ]) Note that .linspace() includes the stop value. To obtain an array of common fractions, increase the number of items: np . linspace ( 0 , 5 , 21 ) array([0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. , 2.25, 2.5 , 2.75, 3. , 3.25, 3.5 , 3.75, 4. , 4.25, 4.5 , 4.75, 5. ])","title":"linspace"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#eye","text":"Creates an identity matrix [ Numpy eye ] np . eye ( 4 ) array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]])","title":"eye"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#random","text":"Numpy also has lots of ways to create random number arrays. Here we will go through some of the most used methods from random module","title":"Random"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#1-rand","text":"Creates an array of the given shape and populates it with random samples from a uniform distribution over [0, 1) . [ Random rand ] np . random . rand ( 2 ) array([0.69647666, 0.14395438]) np . random . rand ( 5 , 5 ) array([[0.9112553 , 0.75849901, 0.43392287, 0.4134459 , 0.10902179], [0.66881652, 0.21265267, 0.21783956, 0.08716564, 0.46147918], [0.16064897, 0.38241433, 0.50076915, 0.58926492, 0.69837196], [0.88502465, 0.2996012 , 0.49291933, 0.75316852, 0.29998398], [0.42345042, 0.57034504, 0.94797283, 0.70571464, 0.35788149]])","title":"1. rand"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#2-randn","text":"Returns a sample (or samples) from the \u201cstandard normal\u201d distribution [\u03c3 = 1]. Unlike rand which is uniform, values closer to zero are more likely to appear. [ Radnom randn ] np . random . randn ( 2 ) array([-0.55673554, -0.08515858]) np . random . randn ( 5 , 5 ) array([[ 0.83041645, -1.22369138, 0.258011 , 0.90984287, -0.48702078], [-0.88539528, -0.54034218, -0.39928196, 0.85910869, -0.36305332], [ 0.132046 , -1.28709664, 0.49352402, 0.80293611, 0.2601146 ], [ 0.74912365, 0.16013944, 0.39345536, -0.52355146, 1.0536796 ], [ 0.00293273, -0.14715505, -1.22460234, -0.65347358, -0.31514422]])","title":"2. randn"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#3-randint","text":"Returns random integers from low (inclusive) to high (exclusive). [ Random randint ] np . random . randint ( 1 , 100 ) 42 np . random . randint ( 1 , 100 , 10 ) array([33, 26, 51, 78, 89, 15, 42, 68, 14, 62])","title":"3. randint"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#4-seed","text":"Can be used to set the random state, so that the same \u201crandom\u201d results can be reproduced. [ Numpy ndarray random ] np . random . seed ( 42 ) np . random . rand ( 4 ) array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) np . random . seed ( 42 ) np . random . rand ( 4 ) array([0.37454012, 0.95071431, 0.73199394, 0.59865848])","title":"4. seed"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#array-attributes-and-methods","text":"Let\u2019s discuss some useful attributes and methods for an array: In particular, the reshape attribute and max,min,argmax, argmin, shape & the dytpe methods Let\u2019s first create two numpy arrays to experiment with :) arr = np . arange ( 25 ) ranarr = np . random . randint ( 0 , 50 , 10 ) arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]) ranarr array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2])","title":"Array Attributes and Methods"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#1-reshape","text":"Returns an array containing the same data with a new shape.[ Numpy ndarray reshape ] arr . reshape ( 5 , 5 ) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]])","title":"1. Reshape"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#2-max-min-argmax-argmin","text":"These are useful methods for finding max or min values. Or to find their index locations using argmin or argmax ranarr array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) ranarr . max () 39 ranarr . argmax () 7 ranarr . min () 2 ranarr . argmin () 9","title":"2. max, min, argmax, argmin"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#3-shape","text":"Shape is an attribute that arrays have (not a method):[ Numpy ndarray Shape ] # Vector arr . shape (25,) # Notice the two sets of brackets arr . reshape ( 1 , 25 ) array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) arr . reshape ( 1 , 25 ) . shape (1, 25) arr . reshape ( 25 , 1 ) array([[ 0], [ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]]) arr . reshape ( 25 , 1 ) . shape (25, 1)","title":"3. Shape"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#4-dtype","text":"You can also grab the data type of the object in the array:[ Numpy ndarray dtype ] arr . dtype dtype('int64') arr2 = np . array ([ 1.2 , 3.4 , 5.6 ]) arr2 . dtype dtype('float64')","title":"4. dtype"},{"location":"bootcampsnotes/numpy/00-NumPy-Arrays/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/","text":"Crash Course: Numpy \u00b6 by Jawad Haider 01 - Numpy Indexing and Selection \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Indexing and Selection 1.1 Bracket Indexing and Selection 1.2 Broadcasting 1.3 Indexing a 2D array (matrices) 1.4 More Indexing Help 1.5 Conditional Selection 2 Great Job! Thats the end of this part. NumPy Indexing and Selection \u00b6 In this lecture we will discuss how to select elements or groups of elements from an array. import numpy as np #Creating sample array arr = np . arange ( 0 , 11 ) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) Bracket Indexing and Selection \u00b6 The simplest way to pick one or some elements of an array looks very similar to python lists: #Get a value at an index arr [ 8 ] 8 #Get values in a range arr [ 1 : 5 ] array([1, 2, 3, 4]) #Get values in a range arr [ 0 : 5 ] array([0, 1, 2, 3, 4]) Broadcasting \u00b6 NumPy arrays differ from normal Python lists because of their ability to broadcast. With lists, you can only reassign parts of a list with new parts of the same size and shape. That is, if you wanted to replace the first 5 elements in a list with a new value, you would have to pass in a new 5 element list. With NumPy arrays, you can broadcast a single value across a larger set of values: #Setting a value with index range (Broadcasting) arr [ 0 : 5 ] = 100 #Show arr array([100, 100, 100, 100, 100, 5, 6, 7, 8, 9, 10]) # Reset array, we'll see why I had to reset in a moment arr = np . arange ( 0 , 11 ) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) #Important notes on Slices slice_of_arr = arr [ 0 : 6 ] #Show slice slice_of_arr array([0, 1, 2, 3, 4, 5]) #Change Slice slice_of_arr [:] = 99 #Show Slice again slice_of_arr array([99, 99, 99, 99, 99, 99]) Now note the changes also occur in our original array! arr array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) Data is not copied, it\u2019s a view of the original array! This avoids memory problems! #To get a copy, need to be explicit arr_copy = arr . copy () arr_copy array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) Indexing a 2D array (matrices) \u00b6 The general format is arr_2d[row][col] or arr_2d[row,col] . I recommend using the comma notation for clarity. arr_2d = np . array (([ 5 , 10 , 15 ],[ 20 , 25 , 30 ],[ 35 , 40 , 45 ])) #Show arr_2d array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]]) #Indexing row arr_2d [ 1 ] array([20, 25, 30]) # Format is arr_2d[row][col] or arr_2d[row,col] # Getting individual element value arr_2d [ 1 ][ 0 ] 20 # Getting individual element value arr_2d [ 1 , 0 ] 20 # 2D array slicing #Shape (2,2) from top right corner arr_2d [: 2 , 1 :] array([[10, 15], [25, 30]]) #Shape bottom row arr_2d [ 2 ] array([35, 40, 45]) #Shape bottom row arr_2d [ 2 ,:] array([35, 40, 45]) More Indexing Help \u00b6 Indexing a 2D matrix can be a bit confusing at first, especially when you start to add in step size. Try google image searching NumPy indexing to find useful images, like this one: Image source Conditional Selection \u00b6 This is a very fundamental concept that will directly translate to pandas later on, make sure you understand this part! Let\u2019s briefly go over how to use brackets for selection based off of comparison operators. arr = np . arange ( 1 , 11 ) arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) arr > 4 array([False, False, False, False, True, True, True, True, True, True]) bool_arr = arr > 4 bool_arr array([False, False, False, False, True, True, True, True, True, True]) arr [ bool_arr ] array([ 5, 6, 7, 8, 9, 10]) arr [ arr > 2 ] array([ 3, 4, 5, 6, 7, 8, 9, 10]) x = 2 arr [ arr > x ] array([ 3, 4, 5, 6, 7, 8, 9, 10]) Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"01 NumPy Indexing and Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#crash-course-numpy","text":"by Jawad Haider","title":"Crash Course: Numpy"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#01-numpy-indexing-and-selection","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Indexing and Selection 1.1 Bracket Indexing and Selection 1.2 Broadcasting 1.3 Indexing a 2D array (matrices) 1.4 More Indexing Help 1.5 Conditional Selection 2 Great Job! Thats the end of this part.","title":"01 - Numpy Indexing and Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#numpy-indexing-and-selection","text":"In this lecture we will discuss how to select elements or groups of elements from an array. import numpy as np #Creating sample array arr = np . arange ( 0 , 11 ) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])","title":"NumPy Indexing and Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#bracket-indexing-and-selection","text":"The simplest way to pick one or some elements of an array looks very similar to python lists: #Get a value at an index arr [ 8 ] 8 #Get values in a range arr [ 1 : 5 ] array([1, 2, 3, 4]) #Get values in a range arr [ 0 : 5 ] array([0, 1, 2, 3, 4])","title":"Bracket Indexing and Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#broadcasting","text":"NumPy arrays differ from normal Python lists because of their ability to broadcast. With lists, you can only reassign parts of a list with new parts of the same size and shape. That is, if you wanted to replace the first 5 elements in a list with a new value, you would have to pass in a new 5 element list. With NumPy arrays, you can broadcast a single value across a larger set of values: #Setting a value with index range (Broadcasting) arr [ 0 : 5 ] = 100 #Show arr array([100, 100, 100, 100, 100, 5, 6, 7, 8, 9, 10]) # Reset array, we'll see why I had to reset in a moment arr = np . arange ( 0 , 11 ) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) #Important notes on Slices slice_of_arr = arr [ 0 : 6 ] #Show slice slice_of_arr array([0, 1, 2, 3, 4, 5]) #Change Slice slice_of_arr [:] = 99 #Show Slice again slice_of_arr array([99, 99, 99, 99, 99, 99]) Now note the changes also occur in our original array! arr array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) Data is not copied, it\u2019s a view of the original array! This avoids memory problems! #To get a copy, need to be explicit arr_copy = arr . copy () arr_copy array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10])","title":"Broadcasting"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#indexing-a-2d-array-matrices","text":"The general format is arr_2d[row][col] or arr_2d[row,col] . I recommend using the comma notation for clarity. arr_2d = np . array (([ 5 , 10 , 15 ],[ 20 , 25 , 30 ],[ 35 , 40 , 45 ])) #Show arr_2d array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]]) #Indexing row arr_2d [ 1 ] array([20, 25, 30]) # Format is arr_2d[row][col] or arr_2d[row,col] # Getting individual element value arr_2d [ 1 ][ 0 ] 20 # Getting individual element value arr_2d [ 1 , 0 ] 20 # 2D array slicing #Shape (2,2) from top right corner arr_2d [: 2 , 1 :] array([[10, 15], [25, 30]]) #Shape bottom row arr_2d [ 2 ] array([35, 40, 45]) #Shape bottom row arr_2d [ 2 ,:] array([35, 40, 45])","title":"Indexing a 2D array (matrices)"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#more-indexing-help","text":"Indexing a 2D matrix can be a bit confusing at first, especially when you start to add in step size. Try google image searching NumPy indexing to find useful images, like this one: Image source","title":"More Indexing Help"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#conditional-selection","text":"This is a very fundamental concept that will directly translate to pandas later on, make sure you understand this part! Let\u2019s briefly go over how to use brackets for selection based off of comparison operators. arr = np . arange ( 1 , 11 ) arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) arr > 4 array([False, False, False, False, True, True, True, True, True, True]) bool_arr = arr > 4 bool_arr array([False, False, False, False, True, True, True, True, True, True]) arr [ bool_arr ] array([ 5, 6, 7, 8, 9, 10]) arr [ arr > 2 ] array([ 3, 4, 5, 6, 7, 8, 9, 10]) x = 2 arr [ arr > x ] array([ 3, 4, 5, 6, 7, 8, 9, 10])","title":"Conditional Selection"},{"location":"bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/","text":"Crash Course: Numpy \u00b6 by Jawad Haider 02 - Numpy Operations \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Operations 1.1 Arithmetic 1.2 Universal Array Functions 1.3 Summary Statistics on Arrays 1.4 Axis Logic 2 Great Job! Thats the end of this part. NumPy Operations \u00b6 Arithmetic \u00b6 You can easily perform array with array arithmetic, or scalar with array arithmetic. Let\u2019s see some examples: import numpy as np arr = np . arange ( 0 , 10 ) arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) arr + arr array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) arr * arr array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) arr - arr array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # This will raise a Warning on division by zero, but not an error! # It just fills the spot with nan arr / arr C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide This is separate from the ipykernel package so we can avoid doing imports until array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.]) # Also a warning (but not an error) relating to infinity 1 / arr C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide array([ inf, 1. , 0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]) arr ** 3 array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) Universal Array Functions \u00b6 NumPy comes with many universal array functions , or ufuncs , which are essentially just mathematical operations that can be applied across the array. Let\u2019s show some common ones: # Taking Square Roots np . sqrt ( arr ) array([0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) # Calculating exponential (e^) np . exp ( arr ) array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03]) # Trigonometric Functions like sine np . sin ( arr ) array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849]) # Taking the Natural Logarithm np . log ( arr ) C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458]) Summary Statistics on Arrays \u00b6 NumPy also offers common summary statistics like sum , mean and max . You would call these as methods on an array. arr = np . arange ( 0 , 10 ) arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) arr . sum () 45 arr . mean () 4.5 arr . max () 9 Other summary statistics include: arr.min() returns 0 minimum arr.var() returns 8.25 variance arr.std() returns 2.8722813232690143 standard deviation Axis Logic \u00b6 When working with 2-dimensional arrays (matrices) we have to consider rows and columns. This becomes very important when we get to the section on pandas. In array terms, axis 0 (zero) is the vertical axis (rows), and axis 1 is the horizonal axis (columns). These values (0,1) correspond to the order in which arr.shape values are returned. Let\u2019s see how this affects our summary statistic calculations from above. arr_2d = np . array ([[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ],[ 9 , 10 , 11 , 12 ]]) arr_2d array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) arr_2d . sum ( axis = 0 ) array([15, 18, 21, 24]) By passing in axis=0 , we\u2019re returning an array of sums along the vertical axis, essentially [(1+5+9), (2+6+10), (3+7+11), (4+8+12)] arr_2d . shape (3, 4) This tells us that arr_2d has 3 rows and 4 columns. In arr_2d.sum(axis=0) above, the first element in each row was summed, then the second element, and so forth. So what should arr_2d.sum(axis=1) return? # THINK ABOUT WHAT THIS WILL RETURN BEFORE RUNNING THE CELL! arr_2d . sum ( axis = 1 ) Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"02 NumPy Operations"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#crash-course-numpy","text":"by Jawad Haider","title":"Crash Course: Numpy"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#02-numpy-operations","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Operations 1.1 Arithmetic 1.2 Universal Array Functions 1.3 Summary Statistics on Arrays 1.4 Axis Logic 2 Great Job! Thats the end of this part.","title":"02 - Numpy Operations"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#numpy-operations","text":"","title":"NumPy Operations"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#arithmetic","text":"You can easily perform array with array arithmetic, or scalar with array arithmetic. Let\u2019s see some examples: import numpy as np arr = np . arange ( 0 , 10 ) arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) arr + arr array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) arr * arr array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) arr - arr array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # This will raise a Warning on division by zero, but not an error! # It just fills the spot with nan arr / arr C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide This is separate from the ipykernel package so we can avoid doing imports until array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.]) # Also a warning (but not an error) relating to infinity 1 / arr C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide array([ inf, 1. , 0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]) arr ** 3 array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32)","title":"Arithmetic"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#universal-array-functions","text":"NumPy comes with many universal array functions , or ufuncs , which are essentially just mathematical operations that can be applied across the array. Let\u2019s show some common ones: # Taking Square Roots np . sqrt ( arr ) array([0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) # Calculating exponential (e^) np . exp ( arr ) array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03]) # Trigonometric Functions like sine np . sin ( arr ) array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849]) # Taking the Natural Logarithm np . log ( arr ) C:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458])","title":"Universal Array Functions"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#summary-statistics-on-arrays","text":"NumPy also offers common summary statistics like sum , mean and max . You would call these as methods on an array. arr = np . arange ( 0 , 10 ) arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) arr . sum () 45 arr . mean () 4.5 arr . max () 9 Other summary statistics include: arr.min() returns 0 minimum arr.var() returns 8.25 variance arr.std() returns 2.8722813232690143 standard deviation","title":"Summary Statistics on Arrays"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#axis-logic","text":"When working with 2-dimensional arrays (matrices) we have to consider rows and columns. This becomes very important when we get to the section on pandas. In array terms, axis 0 (zero) is the vertical axis (rows), and axis 1 is the horizonal axis (columns). These values (0,1) correspond to the order in which arr.shape values are returned. Let\u2019s see how this affects our summary statistic calculations from above. arr_2d = np . array ([[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ],[ 9 , 10 , 11 , 12 ]]) arr_2d array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) arr_2d . sum ( axis = 0 ) array([15, 18, 21, 24]) By passing in axis=0 , we\u2019re returning an array of sums along the vertical axis, essentially [(1+5+9), (2+6+10), (3+7+11), (4+8+12)] arr_2d . shape (3, 4) This tells us that arr_2d has 3 rows and 4 columns. In arr_2d.sum(axis=0) above, the first element in each row was summed, then the second element, and so forth. So what should arr_2d.sum(axis=1) return? # THINK ABOUT WHAT THIS WILL RETURN BEFORE RUNNING THE CELL! arr_2d . sum ( axis = 1 )","title":"Axis Logic"},{"location":"bootcampsnotes/numpy/02-NumPy-Operations/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/","text":"Crash Course: Numpy \u00b6 by Jawad Haider 03 - NumPy Exercises \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Exercises 1.1 Numpy Indexing and Selection 1.2 NumPy Operations 1.3 Bonus Question 2 Great Job! Thats the end of this part. NumPy Exercises \u00b6 Now that we\u2019ve learned about NumPy let\u2019s test your knowledge. We\u2019ll start off with a few simple tasks and then you\u2019ll be asked some more complicated questions. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! 1. Import NumPy as np \u00b6 2. Create an array of 10 zeros \u00b6 # CODE HERE # DON'T WRITE HERE array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 3. Create an array of 10 ones \u00b6 # DON'T WRITE HERE array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) 4. Create an array of 10 fives \u00b6 # DON'T WRITE HERE array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]) 5. Create an array of the integers from 10 to 50 \u00b6 # DON'T WRITE HERE array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]) 6. Create an array of all the even integers from 10 to 50 \u00b6 # DON'T WRITE HERE array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]) 7. Create a 3x3 matrix with values ranging from 0 to 8 \u00b6 # DON'T WRITE HERE array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) 8. Create a 3x3 identity matrix \u00b6 # DON'T WRITE HERE array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) 9. Use NumPy to generate a random number between 0 and 1 \u2003NOTE: Your result\u2019s value should be different from the one shown below. \u00b6 # DON'T WRITE HERE array([0.65248055]) 10. Use NumPy to generate an array of 25 random numbers sampled from a standard normal distribution \u2003\u2002NOTE: Your result\u2019s values should be different from the ones shown below. \u00b6 # DON'T WRITE HERE array([ 1.80076712, -1.12375847, -0.98524305, 0.11673573, 1.96346762, 1.81378592, -0.33790771, 0.85012656, 0.0100703 , -0.91005957, 0.29064366, 0.69906357, 0.1774377 , -0.61958694, -0.45498611, -2.0804685 , -0.06778549, 1.06403819, 0.4311884 , -1.09853837, 1.11980469, -0.48751963, 1.32517611, -0.61775122, -0.00622865]) 11. Create the following matrix: \u00b6 # DON'T WRITE HERE array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ], [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ], [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ], [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ], [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ], [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ], [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ], [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ], [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ], [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]]) 12. Create an array of 20 linearly spaced points between 0 and 1: \u00b6 # DON'T WRITE HERE array([0. , 0.05263158, 0.10526316, 0.15789474, 0.21052632, 0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421, 0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211, 0.78947368, 0.84210526, 0.89473684, 0.94736842, 1. ]) Numpy Indexing and Selection \u00b6 Now you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs: # RUN THIS CELL - THIS IS OUR STARTING MATRIX mat = np . arange ( 1 , 26 ) . reshape ( 5 , 5 ) mat array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) 13. Write code that reproduces the output shown below. \u2003\u2002Be careful not to run the cell immediately above the output, otherwise you won\u2019t be able to see the output any more. \u00b6 # CODE HERE # DON'T WRITE HERE array([[12, 13, 14, 15], [17, 18, 19, 20], [22, 23, 24, 25]]) 14. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE 20 15. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE array([[ 2], [ 7], [12]]) 16. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE array([21, 22, 23, 24, 25]) 17. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE array([[16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) NumPy Operations \u00b6 18. Get the sum of all the values in mat \u00b6 # DON'T WRITE HERE 325 19. Get the standard deviation of the values in mat \u00b6 # DON'T WRITE HERE 7.211102550927978 20. Get the sum of all the columns in mat \u00b6 # DON'T WRITE HERE array([55, 60, 65, 70, 75]) Bonus Question \u00b6 We worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"03 NumPy Exercises"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#crash-course-numpy","text":"by Jawad Haider","title":"Crash Course: Numpy"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#03-numpy-exercises","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Exercises 1.1 Numpy Indexing and Selection 1.2 NumPy Operations 1.3 Bonus Question 2 Great Job! Thats the end of this part.","title":"03 - NumPy Exercises"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#numpy-exercises","text":"Now that we\u2019ve learned about NumPy let\u2019s test your knowledge. We\u2019ll start off with a few simple tasks and then you\u2019ll be asked some more complicated questions. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"NumPy Exercises"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#1-import-numpy-as-np","text":"","title":"1. Import NumPy as np"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#2-create-an-array-of-10-zeros","text":"# CODE HERE # DON'T WRITE HERE array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","title":"2. Create an array of 10 zeros"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#3-create-an-array-of-10-ones","text":"# DON'T WRITE HERE array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])","title":"3. Create an array of 10 ones"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#4-create-an-array-of-10-fives","text":"# DON'T WRITE HERE array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])","title":"4. Create an array of 10 fives"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#5-create-an-array-of-the-integers-from-10-to-50","text":"# DON'T WRITE HERE array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])","title":"5. Create an array of the integers from 10 to 50"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#6-create-an-array-of-all-the-even-integers-from-10-to-50","text":"# DON'T WRITE HERE array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50])","title":"6. Create an array of all the even integers from 10 to 50"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#7-create-a-3x3-matrix-with-values-ranging-from-0-to-8","text":"# DON'T WRITE HERE array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])","title":"7. Create a 3x3 matrix with values ranging from 0 to 8"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#8-create-a-3x3-identity-matrix","text":"# DON'T WRITE HERE array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])","title":"8. Create a 3x3 identity matrix"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#9-use-numpy-to-generate-a-random-number-between-0-and-1-note-your-results-value-should-be-different-from-the-one-shown-below","text":"# DON'T WRITE HERE array([0.65248055])","title":"9. Use NumPy to generate a random number between 0 and 1\u2003NOTE: Your result\u2019s value should be different from the one shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#10-use-numpy-to-generate-an-array-of-25-random-numbers-sampled-from-a-standard-normal-distribution-note-your-results-values-should-be-different-from-the-ones-shown-below","text":"# DON'T WRITE HERE array([ 1.80076712, -1.12375847, -0.98524305, 0.11673573, 1.96346762, 1.81378592, -0.33790771, 0.85012656, 0.0100703 , -0.91005957, 0.29064366, 0.69906357, 0.1774377 , -0.61958694, -0.45498611, -2.0804685 , -0.06778549, 1.06403819, 0.4311884 , -1.09853837, 1.11980469, -0.48751963, 1.32517611, -0.61775122, -0.00622865])","title":"10. Use NumPy to generate an array of 25 random numbers sampled from a standard normal distribution\u2003\u2002NOTE: Your result\u2019s values should be different from the ones shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#11-create-the-following-matrix","text":"# DON'T WRITE HERE array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ], [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ], [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ], [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ], [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ], [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ], [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ], [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ], [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ], [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]])","title":"11. Create the following matrix:"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#12-create-an-array-of-20-linearly-spaced-points-between-0-and-1","text":"# DON'T WRITE HERE array([0. , 0.05263158, 0.10526316, 0.15789474, 0.21052632, 0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421, 0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211, 0.78947368, 0.84210526, 0.89473684, 0.94736842, 1. ])","title":"12. Create an array of 20 linearly spaced points between 0 and 1:"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#numpy-indexing-and-selection","text":"Now you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs: # RUN THIS CELL - THIS IS OUR STARTING MATRIX mat = np . arange ( 1 , 26 ) . reshape ( 5 , 5 ) mat array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]])","title":"Numpy Indexing and Selection"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#13-write-code-that-reproduces-the-output-shown-below-be-careful-not-to-run-the-cell-immediately-above-the-output-otherwise-you-wont-be-able-to-see-the-output-any-more","text":"# CODE HERE # DON'T WRITE HERE array([[12, 13, 14, 15], [17, 18, 19, 20], [22, 23, 24, 25]])","title":"13. Write code that reproduces the output shown below.\u2003\u2002Be careful not to run the cell immediately above the output, otherwise you won\u2019t be able to see the output any more."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#14-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE 20","title":"14. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#15-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE array([[ 2], [ 7], [12]])","title":"15. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#16-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE array([21, 22, 23, 24, 25])","title":"16. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#17-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE array([[16, 17, 18, 19, 20], [21, 22, 23, 24, 25]])","title":"17. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#numpy-operations","text":"","title":"NumPy Operations"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#18-get-the-sum-of-all-the-values-in-mat","text":"# DON'T WRITE HERE 325","title":"18. Get the sum of all the values in mat"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#19-get-the-standard-deviation-of-the-values-in-mat","text":"# DON'T WRITE HERE 7.211102550927978","title":"19. Get the standard deviation of the values in mat"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#20-get-the-sum-of-all-the-columns-in-mat","text":"# DON'T WRITE HERE array([55, 60, 65, 70, 75])","title":"20. Get the sum of all the columns in mat"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#bonus-question","text":"We worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint","title":"Bonus Question"},{"location":"bootcampsnotes/numpy/03-NumPy-Exercises/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/","text":"Crash Course: Numpy \u00b6 by Jawad Haider 04 - NumPy Exercises - Solutions \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Exercises - Solutions 1.1 Numpy Indexing and Selection 1.2 NumPy Operations 1.3 Bonus Question 2 Great Job! Thats the end of this part. NumPy Exercises - Solutions \u00b6 Now that we\u2019ve learned about NumPy let\u2019s test your knowledge. We\u2019ll start off with a few simple tasks and then you\u2019ll be asked some more complicated questions. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! 1. Import NumPy as np \u00b6 import numpy as np 2. Create an array of 10 zeros \u00b6 # CODE HERE # DON'T WRITE HERE np . zeros ( 10 ) array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 3. Create an array of 10 ones \u00b6 # DON'T WRITE HERE np . ones ( 10 ) array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) 4. Create an array of 10 fives \u00b6 # DON'T WRITE HERE np . ones ( 10 ) * 5 array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]) 5. Create an array of the integers from 10 to 50 \u00b6 # DON'T WRITE HERE np . arange ( 10 , 51 ) array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]) 6. Create an array of all the even integers from 10 to 50 \u00b6 # DON'T WRITE HERE np . arange ( 10 , 51 , 2 ) array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]) 7. Create a 3x3 matrix with values ranging from 0 to 8 \u00b6 # DON'T WRITE HERE np . arange ( 9 ) . reshape ( 3 , 3 ) array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) 8. Create a 3x3 identity matrix \u00b6 # DON'T WRITE HERE np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) 9. Use NumPy to generate a random number between 0 and 1 \u2003NOTE: Your result\u2019s value should be different from the one shown below. \u00b6 # DON'T WRITE HERE np . random . rand ( 1 ) array([0.65248055]) 10. Use NumPy to generate an array of 25 random numbers sampled from a standard normal distribution \u2003\u2002NOTE: Your result\u2019s values should be different from the ones shown below. \u00b6 # DON'T WRITE HERE np . random . randn ( 25 ) array([ 1.80076712, -1.12375847, -0.98524305, 0.11673573, 1.96346762, 1.81378592, -0.33790771, 0.85012656, 0.0100703 , -0.91005957, 0.29064366, 0.69906357, 0.1774377 , -0.61958694, -0.45498611, -2.0804685 , -0.06778549, 1.06403819, 0.4311884 , -1.09853837, 1.11980469, -0.48751963, 1.32517611, -0.61775122, -0.00622865]) 11. Create the following matrix: \u00b6 # DON'T WRITE HERE np . arange ( 1 , 101 ) . reshape ( 10 , 10 ) / 100 array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ], [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ], [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ], [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ], [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ], [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ], [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ], [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ], [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ], [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]]) 12. Create an array of 20 linearly spaced points between 0 and 1: \u00b6 # DON'T WRITE HERE np . linspace ( 0 , 1 , 20 ) array([0. , 0.05263158, 0.10526316, 0.15789474, 0.21052632, 0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421, 0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211, 0.78947368, 0.84210526, 0.89473684, 0.94736842, 1. ]) Numpy Indexing and Selection \u00b6 Now you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs: # RUN THIS CELL - THIS IS OUR STARTING MATRIX mat = np . arange ( 1 , 26 ) . reshape ( 5 , 5 ) mat array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) 13. Write code that reproduces the output shown below. \u2003\u2002Be careful not to run the cell immediately above the output, otherwise you won\u2019t be able to see the output any more. \u00b6 # CODE HERE # DON'T WRITE HERE mat [ 2 :, 1 :] array([[12, 13, 14, 15], [17, 18, 19, 20], [22, 23, 24, 25]]) 14. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE mat [ 3 , 4 ] 20 15. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE mat [: 3 , 1 : 2 ] array([[ 2], [ 7], [12]]) 16. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE mat [ 4 ,:] array([21, 22, 23, 24, 25]) 17. Write code that reproduces the output shown below. \u00b6 # DON'T WRITE HERE mat [ 3 : 5 ,:] array([[16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) NumPy Operations \u00b6 18. Get the sum of all the values in mat \u00b6 # DON'T WRITE HERE mat . sum () 325 19. Get the standard deviation of the values in mat \u00b6 # DON'T WRITE HERE mat . std () 7.211102550927978 20. Get the sum of all the columns in mat \u00b6 # DON'T WRITE HERE mat . sum ( axis = 0 ) array([55, 60, 65, 70, 75]) Bonus Question \u00b6 We worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint np . random . seed ( 101 ) Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"04 NumPy Exercises Solutions"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#crash-course-numpy","text":"by Jawad Haider","title":"Crash Course: Numpy"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#04-numpy-exercises-solutions","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 NumPy Exercises - Solutions 1.1 Numpy Indexing and Selection 1.2 NumPy Operations 1.3 Bonus Question 2 Great Job! Thats the end of this part.","title":"04 - NumPy Exercises - Solutions"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#numpy-exercises-solutions","text":"Now that we\u2019ve learned about NumPy let\u2019s test your knowledge. We\u2019ll start off with a few simple tasks and then you\u2019ll be asked some more complicated questions. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"NumPy Exercises - Solutions"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#1-import-numpy-as-np","text":"import numpy as np","title":"1. Import NumPy as np"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#2-create-an-array-of-10-zeros","text":"# CODE HERE # DON'T WRITE HERE np . zeros ( 10 ) array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","title":"2. Create an array of 10 zeros"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#3-create-an-array-of-10-ones","text":"# DON'T WRITE HERE np . ones ( 10 ) array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])","title":"3. Create an array of 10 ones"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#4-create-an-array-of-10-fives","text":"# DON'T WRITE HERE np . ones ( 10 ) * 5 array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])","title":"4. Create an array of 10 fives"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#5-create-an-array-of-the-integers-from-10-to-50","text":"# DON'T WRITE HERE np . arange ( 10 , 51 ) array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])","title":"5. Create an array of the integers from 10 to 50"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#6-create-an-array-of-all-the-even-integers-from-10-to-50","text":"# DON'T WRITE HERE np . arange ( 10 , 51 , 2 ) array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50])","title":"6. Create an array of all the even integers from 10 to 50"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#7-create-a-3x3-matrix-with-values-ranging-from-0-to-8","text":"# DON'T WRITE HERE np . arange ( 9 ) . reshape ( 3 , 3 ) array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])","title":"7. Create a 3x3 matrix with values ranging from 0 to 8"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#8-create-a-3x3-identity-matrix","text":"# DON'T WRITE HERE np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])","title":"8. Create a 3x3 identity matrix"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#9-use-numpy-to-generate-a-random-number-between-0-and-1-note-your-results-value-should-be-different-from-the-one-shown-below","text":"# DON'T WRITE HERE np . random . rand ( 1 ) array([0.65248055])","title":"9. Use NumPy to generate a random number between 0 and 1\u2003NOTE: Your result\u2019s value should be different from the one shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#10-use-numpy-to-generate-an-array-of-25-random-numbers-sampled-from-a-standard-normal-distribution-note-your-results-values-should-be-different-from-the-ones-shown-below","text":"# DON'T WRITE HERE np . random . randn ( 25 ) array([ 1.80076712, -1.12375847, -0.98524305, 0.11673573, 1.96346762, 1.81378592, -0.33790771, 0.85012656, 0.0100703 , -0.91005957, 0.29064366, 0.69906357, 0.1774377 , -0.61958694, -0.45498611, -2.0804685 , -0.06778549, 1.06403819, 0.4311884 , -1.09853837, 1.11980469, -0.48751963, 1.32517611, -0.61775122, -0.00622865])","title":"10. Use NumPy to generate an array of 25 random numbers sampled from a standard normal distribution\u2003\u2002NOTE: Your result\u2019s values should be different from the ones shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#11-create-the-following-matrix","text":"# DON'T WRITE HERE np . arange ( 1 , 101 ) . reshape ( 10 , 10 ) / 100 array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ], [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ], [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ], [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ], [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ], [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ], [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ], [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ], [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ], [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]])","title":"11. Create the following matrix:"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#12-create-an-array-of-20-linearly-spaced-points-between-0-and-1","text":"# DON'T WRITE HERE np . linspace ( 0 , 1 , 20 ) array([0. , 0.05263158, 0.10526316, 0.15789474, 0.21052632, 0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421, 0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211, 0.78947368, 0.84210526, 0.89473684, 0.94736842, 1. ])","title":"12. Create an array of 20 linearly spaced points between 0 and 1:"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#numpy-indexing-and-selection","text":"Now you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs: # RUN THIS CELL - THIS IS OUR STARTING MATRIX mat = np . arange ( 1 , 26 ) . reshape ( 5 , 5 ) mat array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]])","title":"Numpy Indexing and Selection"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#13-write-code-that-reproduces-the-output-shown-below-be-careful-not-to-run-the-cell-immediately-above-the-output-otherwise-you-wont-be-able-to-see-the-output-any-more","text":"# CODE HERE # DON'T WRITE HERE mat [ 2 :, 1 :] array([[12, 13, 14, 15], [17, 18, 19, 20], [22, 23, 24, 25]])","title":"13. Write code that reproduces the output shown below.\u2003\u2002Be careful not to run the cell immediately above the output, otherwise you won\u2019t be able to see the output any more."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#14-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE mat [ 3 , 4 ] 20","title":"14. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#15-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE mat [: 3 , 1 : 2 ] array([[ 2], [ 7], [12]])","title":"15. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#16-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE mat [ 4 ,:] array([21, 22, 23, 24, 25])","title":"16. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#17-write-code-that-reproduces-the-output-shown-below","text":"# DON'T WRITE HERE mat [ 3 : 5 ,:] array([[16, 17, 18, 19, 20], [21, 22, 23, 24, 25]])","title":"17. Write code that reproduces the output shown below."},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#numpy-operations","text":"","title":"NumPy Operations"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#18-get-the-sum-of-all-the-values-in-mat","text":"# DON'T WRITE HERE mat . sum () 325","title":"18. Get the sum of all the values in mat"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#19-get-the-standard-deviation-of-the-values-in-mat","text":"# DON'T WRITE HERE mat . std () 7.211102550927978","title":"19. Get the standard deviation of the values in mat"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#20-get-the-sum-of-all-the-columns-in-mat","text":"# DON'T WRITE HERE mat . sum ( axis = 0 ) array([55, 60, 65, 70, 75])","title":"20. Get the sum of all the columns in mat"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#bonus-question","text":"We worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint np . random . seed ( 101 )","title":"Bonus Question"},{"location":"bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/00-Intro-to-Pandas/","text":"Crash Course: Pandas \u00b6 by Jawad Haider 00 - Introduction to Pandas \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Introduction to Pandas \u00b6 In this section of the course we will learn how to use pandas for data analysis. You can think of pandas as an extremely powerful version of Excel, with a lot more features. In this section of the course, you should go through the notebooks in this order: Introduction to Pandas Series DataFrames Missing Data GroupBy Merging, Joining and Concatenating Operations Data Input and Output Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"00 Intro to Pandas"},{"location":"bootcampsnotes/pandas/00-Intro-to-Pandas/#crash-course-pandas","text":"by Jawad Haider","title":"Crash Course: Pandas"},{"location":"bootcampsnotes/pandas/00-Intro-to-Pandas/#00-introduction-to-pandas","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"00 - Introduction to Pandas"},{"location":"bootcampsnotes/pandas/00-Intro-to-Pandas/#introduction-to-pandas","text":"In this section of the course we will learn how to use pandas for data analysis. You can think of pandas as an extremely powerful version of Excel, with a lot more features. In this section of the course, you should go through the notebooks in this order: Introduction to Pandas Series DataFrames Missing Data GroupBy Merging, Joining and Concatenating Operations Data Input and Output","title":"Introduction to Pandas"},{"location":"bootcampsnotes/pandas/00-Intro-to-Pandas/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/01-Series/","text":"Crash Course: Pandas \u00b6 by Jawad Haider 01 - Series \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Series 1.1 Creating a Series 1.1.1 Using Lists 1.1.2 Using NumPy Arrays 1.1.3 Using Dictionaries 1.1.4 Data in a Series 1.2 Using an Index 2 Great Job! Thats the end of this part. Series \u00b6 The first main data type we will learn about for pandas is the Series data type. Let\u2019s import Pandas and explore the Series object. A Series is very similar to a NumPy array (in fact it is built on top of the NumPy array object). What differentiates the NumPy array from a Series, is that a Series can have axis labels, meaning it can be indexed by a label, instead of just a number location. It also doesn\u2019t need to hold numeric data, it can hold any arbitrary Python Object. Let\u2019s explore this concept through some examples: import numpy as np import pandas as pd Creating a Series \u00b6 You can convert a list,numpy array, or dictionary to a Series: labels = [ 'a' , 'b' , 'c' ] my_list = [ 10 , 20 , 30 ] arr = np . array ([ 10 , 20 , 30 ]) d = { 'a' : 10 , 'b' : 20 , 'c' : 30 } Using Lists \u00b6 pd . Series ( data = my_list ) 0 10 1 20 2 30 dtype: int64 pd . Series ( data = my_list , index = labels ) a 10 b 20 c 30 dtype: int64 pd . Series ( my_list , labels ) a 10 b 20 c 30 dtype: int64 Using NumPy Arrays \u00b6 pd . Series ( arr ) 0 10 1 20 2 30 dtype: int64 pd . Series ( arr , labels ) a 10 b 20 c 30 dtype: int64 Using Dictionaries \u00b6 pd . Series ( d ) a 10 b 20 c 30 dtype: int64 Data in a Series \u00b6 A pandas Series can hold a variety of object types: pd . Series ( data = labels ) 0 a 1 b 2 c dtype: object # Even functions (although unlikely that you will use this) pd . Series ([ sum , print , len ]) 0 <built-in function sum> 1 <built-in function print> 2 <built-in function len> dtype: object Using an Index \u00b6 The key to using a Series is understanding its index. Pandas makes use of these index names or numbers by allowing for fast look ups of information (works like a hash table or dictionary). Let\u2019s see some examples of how to grab information from a Series. Let us create two sereis, ser1 and ser2: ser1 = pd . Series ([ 1 , 2 , 3 , 4 ], index = [ 'USA' , 'Germany' , 'USSR' , 'Japan' ]) ser1 USA 1 Germany 2 USSR 3 Japan 4 dtype: int64 ser2 = pd . Series ([ 1 , 2 , 5 , 4 ], index = [ 'USA' , 'Germany' , 'Italy' , 'Japan' ]) ser2 USA 1 Germany 2 Italy 5 Japan 4 dtype: int64 ser1 [ 'USA' ] 1 Operations are then also done based off of index: ser1 + ser2 Germany 4.0 Italy NaN Japan 8.0 USA 2.0 USSR NaN dtype: float64 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"01 Series"},{"location":"bootcampsnotes/pandas/01-Series/#crash-course-pandas","text":"by Jawad Haider","title":"Crash Course: Pandas"},{"location":"bootcampsnotes/pandas/01-Series/#01-series","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Series 1.1 Creating a Series 1.1.1 Using Lists 1.1.2 Using NumPy Arrays 1.1.3 Using Dictionaries 1.1.4 Data in a Series 1.2 Using an Index 2 Great Job! Thats the end of this part.","title":"01 - Series"},{"location":"bootcampsnotes/pandas/01-Series/#series","text":"The first main data type we will learn about for pandas is the Series data type. Let\u2019s import Pandas and explore the Series object. A Series is very similar to a NumPy array (in fact it is built on top of the NumPy array object). What differentiates the NumPy array from a Series, is that a Series can have axis labels, meaning it can be indexed by a label, instead of just a number location. It also doesn\u2019t need to hold numeric data, it can hold any arbitrary Python Object. Let\u2019s explore this concept through some examples: import numpy as np import pandas as pd","title":"Series"},{"location":"bootcampsnotes/pandas/01-Series/#creating-a-series","text":"You can convert a list,numpy array, or dictionary to a Series: labels = [ 'a' , 'b' , 'c' ] my_list = [ 10 , 20 , 30 ] arr = np . array ([ 10 , 20 , 30 ]) d = { 'a' : 10 , 'b' : 20 , 'c' : 30 }","title":"Creating a Series"},{"location":"bootcampsnotes/pandas/01-Series/#using-lists","text":"pd . Series ( data = my_list ) 0 10 1 20 2 30 dtype: int64 pd . Series ( data = my_list , index = labels ) a 10 b 20 c 30 dtype: int64 pd . Series ( my_list , labels ) a 10 b 20 c 30 dtype: int64","title":"Using Lists"},{"location":"bootcampsnotes/pandas/01-Series/#using-numpy-arrays","text":"pd . Series ( arr ) 0 10 1 20 2 30 dtype: int64 pd . Series ( arr , labels ) a 10 b 20 c 30 dtype: int64","title":"Using NumPy Arrays"},{"location":"bootcampsnotes/pandas/01-Series/#using-dictionaries","text":"pd . Series ( d ) a 10 b 20 c 30 dtype: int64","title":"Using Dictionaries"},{"location":"bootcampsnotes/pandas/01-Series/#data-in-a-series","text":"A pandas Series can hold a variety of object types: pd . Series ( data = labels ) 0 a 1 b 2 c dtype: object # Even functions (although unlikely that you will use this) pd . Series ([ sum , print , len ]) 0 <built-in function sum> 1 <built-in function print> 2 <built-in function len> dtype: object","title":"Data in a Series"},{"location":"bootcampsnotes/pandas/01-Series/#using-an-index","text":"The key to using a Series is understanding its index. Pandas makes use of these index names or numbers by allowing for fast look ups of information (works like a hash table or dictionary). Let\u2019s see some examples of how to grab information from a Series. Let us create two sereis, ser1 and ser2: ser1 = pd . Series ([ 1 , 2 , 3 , 4 ], index = [ 'USA' , 'Germany' , 'USSR' , 'Japan' ]) ser1 USA 1 Germany 2 USSR 3 Japan 4 dtype: int64 ser2 = pd . Series ([ 1 , 2 , 5 , 4 ], index = [ 'USA' , 'Germany' , 'Italy' , 'Japan' ]) ser2 USA 1 Germany 2 Italy 5 Japan 4 dtype: int64 ser1 [ 'USA' ] 1 Operations are then also done based off of index: ser1 + ser2 Germany 4.0 Italy NaN Japan 8.0 USA 2.0 USSR NaN dtype: float64","title":"Using an Index"},{"location":"bootcampsnotes/pandas/01-Series/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/02-DataFrames/","text":"Crash Course: Pandas \u00b6 by Jawad Haider 02 - DataFrames \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 DataFrames 1.1 Selection and Indexing 1.1.1 Creating a new column: 1.1.2 Removing Columns 1.1.3 Selecting Rows 1.1.4 Selecting subset of rows and columns 1.1.5 Conditional Selection 1.2 More Index Details 1.3 DataFrame Summaries 2 Great Job! Thats the end of this part. DataFrames \u00b6 DataFrames are the workhorse of pandas and are directly inspired by the R programming language. We can think of a DataFrame as a bunch of Series objects put together to share the same index. Let\u2019s use pandas to explore this topic! import pandas as pd import numpy as np from numpy.random import randn np . random . seed ( 101 ) df = pd . DataFrame ( randn ( 5 , 4 ), index = 'A B C D E' . split (), columns = 'W X Y Z' . split ()) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 Selection and Indexing \u00b6 Let\u2019s learn the various methods to grab data from a DataFrame df [ 'W' ] A 2.706850 B 0.651118 C -2.018168 D 0.188695 E 0.190794 Name: W, dtype: float64 # Pass a list of column names df [[ 'W' , 'Z' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W Z A 2.706850 0.503826 B 0.651118 0.605965 C -2.018168 -0.589001 D 0.188695 0.955057 E 0.190794 0.683509 # SQL Syntax (NOT RECOMMENDED!) df . W A 2.706850 B 0.651118 C -2.018168 D 0.188695 E 0.190794 Name: W, dtype: float64 DataFrame Columns are just Series type ( df [ 'W' ]) pandas.core.series.Series Creating a new column: \u00b6 df [ 'new' ] = df [ 'W' ] + df [ 'Y' ] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 2.706850 0.628133 0.907969 0.503826 3.614819 B 0.651118 -0.319318 -0.848077 0.605965 -0.196959 C -2.018168 0.740122 0.528813 -0.589001 -1.489355 D 0.188695 -0.758872 -0.933237 0.955057 -0.744542 E 0.190794 1.978757 2.605967 0.683509 2.796762 Removing Columns \u00b6 df . drop ( 'new' , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 # Not inplace unless specified! df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 2.706850 0.628133 0.907969 0.503826 3.614819 B 0.651118 -0.319318 -0.848077 0.605965 -0.196959 C -2.018168 0.740122 0.528813 -0.589001 -1.489355 D 0.188695 -0.758872 -0.933237 0.955057 -0.744542 E 0.190794 1.978757 2.605967 0.683509 2.796762 df . drop ( 'new' , axis = 1 , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 Can also drop rows this way: df . drop ( 'E' , axis = 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 Selecting Rows \u00b6 df . loc [ 'A' ] W 2.706850 X 0.628133 Y 0.907969 Z 0.503826 Name: A, dtype: float64 Or select based off of position instead of label df . iloc [ 2 ] W -2.018168 X 0.740122 Y 0.528813 Z -0.589001 Name: C, dtype: float64 Selecting subset of rows and columns \u00b6 df . loc [ 'B' , 'Y' ] -0.8480769834036315 df . loc [[ 'A' , 'B' ],[ 'W' , 'Y' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W Y A 2.706850 0.907969 B 0.651118 -0.848077 Conditional Selection \u00b6 An important feature of pandas is conditional selection using bracket notation, very similar to numpy: df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 df > 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A True True True True B True False False True C False True True False D True False False True E True True True True df [ df > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 NaN NaN 0.605965 C NaN 0.740122 0.528813 NaN D 0.188695 NaN NaN 0.955057 E 0.190794 1.978757 2.605967 0.683509 df [ df [ 'W' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 df [ df [ 'W' ] > 0 ][ 'Y' ] A 0.907969 B -0.848077 D -0.933237 E 2.605967 Name: Y, dtype: float64 df [ df [ 'W' ] > 0 ][[ 'Y' , 'X' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Y X A 0.907969 0.628133 B -0.848077 -0.319318 D -0.933237 -0.758872 E 2.605967 1.978757 For two conditions you can use | and & with parenthesis: df [( df [ 'W' ] > 0 ) & ( df [ 'Y' ] > 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 0.190794 1.978757 2.605967 0.683509 More Index Details \u00b6 Let\u2019s discuss some more features of indexing, including resetting the index or setting it something else. We\u2019ll also talk about index hierarchy! df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 # Reset to default 0,1...n index df . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index W X Y Z 0 A 2.706850 0.628133 0.907969 0.503826 1 B 0.651118 -0.319318 -0.848077 0.605965 2 C -2.018168 0.740122 0.528813 -0.589001 3 D 0.188695 -0.758872 -0.933237 0.955057 4 E 0.190794 1.978757 2.605967 0.683509 newind = 'CA NY WY OR CO' . split () df [ 'States' ] = newind df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States A 2.706850 0.628133 0.907969 0.503826 CA B 0.651118 -0.319318 -0.848077 0.605965 NY C -2.018168 0.740122 0.528813 -0.589001 WY D 0.188695 -0.758872 -0.933237 0.955057 OR E 0.190794 1.978757 2.605967 0.683509 CO df . set_index ( 'States' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States CA 2.706850 0.628133 0.907969 0.503826 NY 0.651118 -0.319318 -0.848077 0.605965 WY -2.018168 0.740122 0.528813 -0.589001 OR 0.188695 -0.758872 -0.933237 0.955057 CO 0.190794 1.978757 2.605967 0.683509 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States A 2.706850 0.628133 0.907969 0.503826 CA B 0.651118 -0.319318 -0.848077 0.605965 NY C -2.018168 0.740122 0.528813 -0.589001 WY D 0.188695 -0.758872 -0.933237 0.955057 OR E 0.190794 1.978757 2.605967 0.683509 CO df . set_index ( 'States' , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States CA 2.706850 0.628133 0.907969 0.503826 NY 0.651118 -0.319318 -0.848077 0.605965 WY -2.018168 0.740122 0.528813 -0.589001 OR 0.188695 -0.758872 -0.933237 0.955057 CO 0.190794 1.978757 2.605967 0.683509 DataFrame Summaries \u00b6 There are a couple of ways to obtain summary data on DataFrames. df.describe() provides summary statistics on all numerical columns. df.info and df.dtypes displays the data type of all columns. df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z count 5.000000 5.000000 5.000000 5.000000 mean 0.343858 0.453764 0.452287 0.431871 std 1.681131 1.061385 1.454516 0.594708 min -2.018168 -0.758872 -0.933237 -0.589001 25% 0.188695 -0.319318 -0.848077 0.503826 50% 0.190794 0.628133 0.528813 0.605965 75% 0.651118 0.740122 0.907969 0.683509 max 2.706850 1.978757 2.605967 0.955057 df . dtypes W float64 X float64 Y float64 Z float64 dtype: object df . info () <class 'pandas.core.frame.DataFrame'> Index: 5 entries, CA to CO Data columns (total 4 columns): W 5 non-null float64 X 5 non-null float64 Y 5 non-null float64 Z 5 non-null float64 dtypes: float64(4) memory usage: 200.0+ bytes Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"02 DataFrames"},{"location":"bootcampsnotes/pandas/02-DataFrames/#crash-course-pandas","text":"by Jawad Haider","title":"Crash Course: Pandas"},{"location":"bootcampsnotes/pandas/02-DataFrames/#02-dataframes","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 DataFrames 1.1 Selection and Indexing 1.1.1 Creating a new column: 1.1.2 Removing Columns 1.1.3 Selecting Rows 1.1.4 Selecting subset of rows and columns 1.1.5 Conditional Selection 1.2 More Index Details 1.3 DataFrame Summaries 2 Great Job! Thats the end of this part.","title":"02 - DataFrames"},{"location":"bootcampsnotes/pandas/02-DataFrames/#dataframes","text":"DataFrames are the workhorse of pandas and are directly inspired by the R programming language. We can think of a DataFrame as a bunch of Series objects put together to share the same index. Let\u2019s use pandas to explore this topic! import pandas as pd import numpy as np from numpy.random import randn np . random . seed ( 101 ) df = pd . DataFrame ( randn ( 5 , 4 ), index = 'A B C D E' . split (), columns = 'W X Y Z' . split ()) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509","title":"DataFrames"},{"location":"bootcampsnotes/pandas/02-DataFrames/#selection-and-indexing","text":"Let\u2019s learn the various methods to grab data from a DataFrame df [ 'W' ] A 2.706850 B 0.651118 C -2.018168 D 0.188695 E 0.190794 Name: W, dtype: float64 # Pass a list of column names df [[ 'W' , 'Z' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W Z A 2.706850 0.503826 B 0.651118 0.605965 C -2.018168 -0.589001 D 0.188695 0.955057 E 0.190794 0.683509 # SQL Syntax (NOT RECOMMENDED!) df . W A 2.706850 B 0.651118 C -2.018168 D 0.188695 E 0.190794 Name: W, dtype: float64 DataFrame Columns are just Series type ( df [ 'W' ]) pandas.core.series.Series","title":"Selection and Indexing"},{"location":"bootcampsnotes/pandas/02-DataFrames/#creating-a-new-column","text":"df [ 'new' ] = df [ 'W' ] + df [ 'Y' ] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 2.706850 0.628133 0.907969 0.503826 3.614819 B 0.651118 -0.319318 -0.848077 0.605965 -0.196959 C -2.018168 0.740122 0.528813 -0.589001 -1.489355 D 0.188695 -0.758872 -0.933237 0.955057 -0.744542 E 0.190794 1.978757 2.605967 0.683509 2.796762","title":"Creating a new column:"},{"location":"bootcampsnotes/pandas/02-DataFrames/#removing-columns","text":"df . drop ( 'new' , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 # Not inplace unless specified! df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 2.706850 0.628133 0.907969 0.503826 3.614819 B 0.651118 -0.319318 -0.848077 0.605965 -0.196959 C -2.018168 0.740122 0.528813 -0.589001 -1.489355 D 0.188695 -0.758872 -0.933237 0.955057 -0.744542 E 0.190794 1.978757 2.605967 0.683509 2.796762 df . drop ( 'new' , axis = 1 , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 Can also drop rows this way: df . drop ( 'E' , axis = 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057","title":"Removing Columns"},{"location":"bootcampsnotes/pandas/02-DataFrames/#selecting-rows","text":"df . loc [ 'A' ] W 2.706850 X 0.628133 Y 0.907969 Z 0.503826 Name: A, dtype: float64 Or select based off of position instead of label df . iloc [ 2 ] W -2.018168 X 0.740122 Y 0.528813 Z -0.589001 Name: C, dtype: float64","title":"Selecting Rows"},{"location":"bootcampsnotes/pandas/02-DataFrames/#selecting-subset-of-rows-and-columns","text":"df . loc [ 'B' , 'Y' ] -0.8480769834036315 df . loc [[ 'A' , 'B' ],[ 'W' , 'Y' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W Y A 2.706850 0.907969 B 0.651118 -0.848077","title":"Selecting subset of rows and columns"},{"location":"bootcampsnotes/pandas/02-DataFrames/#conditional-selection","text":"An important feature of pandas is conditional selection using bracket notation, very similar to numpy: df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 df > 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A True True True True B True False False True C False True True False D True False False True E True True True True df [ df > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 NaN NaN 0.605965 C NaN 0.740122 0.528813 NaN D 0.188695 NaN NaN 0.955057 E 0.190794 1.978757 2.605967 0.683509 df [ df [ 'W' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 df [ df [ 'W' ] > 0 ][ 'Y' ] A 0.907969 B -0.848077 D -0.933237 E 2.605967 Name: Y, dtype: float64 df [ df [ 'W' ] > 0 ][[ 'Y' , 'X' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Y X A 0.907969 0.628133 B -0.848077 -0.319318 D -0.933237 -0.758872 E 2.605967 1.978757 For two conditions you can use | and & with parenthesis: df [( df [ 'W' ] > 0 ) & ( df [ 'Y' ] > 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 0.190794 1.978757 2.605967 0.683509","title":"Conditional Selection"},{"location":"bootcampsnotes/pandas/02-DataFrames/#more-index-details","text":"Let\u2019s discuss some more features of indexing, including resetting the index or setting it something else. We\u2019ll also talk about index hierarchy! df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 # Reset to default 0,1...n index df . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index W X Y Z 0 A 2.706850 0.628133 0.907969 0.503826 1 B 0.651118 -0.319318 -0.848077 0.605965 2 C -2.018168 0.740122 0.528813 -0.589001 3 D 0.188695 -0.758872 -0.933237 0.955057 4 E 0.190794 1.978757 2.605967 0.683509 newind = 'CA NY WY OR CO' . split () df [ 'States' ] = newind df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States A 2.706850 0.628133 0.907969 0.503826 CA B 0.651118 -0.319318 -0.848077 0.605965 NY C -2.018168 0.740122 0.528813 -0.589001 WY D 0.188695 -0.758872 -0.933237 0.955057 OR E 0.190794 1.978757 2.605967 0.683509 CO df . set_index ( 'States' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States CA 2.706850 0.628133 0.907969 0.503826 NY 0.651118 -0.319318 -0.848077 0.605965 WY -2.018168 0.740122 0.528813 -0.589001 OR 0.188695 -0.758872 -0.933237 0.955057 CO 0.190794 1.978757 2.605967 0.683509 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States A 2.706850 0.628133 0.907969 0.503826 CA B 0.651118 -0.319318 -0.848077 0.605965 NY C -2.018168 0.740122 0.528813 -0.589001 WY D 0.188695 -0.758872 -0.933237 0.955057 OR E 0.190794 1.978757 2.605967 0.683509 CO df . set_index ( 'States' , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z States CA 2.706850 0.628133 0.907969 0.503826 NY 0.651118 -0.319318 -0.848077 0.605965 WY -2.018168 0.740122 0.528813 -0.589001 OR 0.188695 -0.758872 -0.933237 0.955057 CO 0.190794 1.978757 2.605967 0.683509","title":"More Index Details"},{"location":"bootcampsnotes/pandas/02-DataFrames/#dataframe-summaries","text":"There are a couple of ways to obtain summary data on DataFrames. df.describe() provides summary statistics on all numerical columns. df.info and df.dtypes displays the data type of all columns. df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z count 5.000000 5.000000 5.000000 5.000000 mean 0.343858 0.453764 0.452287 0.431871 std 1.681131 1.061385 1.454516 0.594708 min -2.018168 -0.758872 -0.933237 -0.589001 25% 0.188695 -0.319318 -0.848077 0.503826 50% 0.190794 0.628133 0.528813 0.605965 75% 0.651118 0.740122 0.907969 0.683509 max 2.706850 1.978757 2.605967 0.955057 df . dtypes W float64 X float64 Y float64 Z float64 dtype: object df . info () <class 'pandas.core.frame.DataFrame'> Index: 5 entries, CA to CO Data columns (total 4 columns): W 5 non-null float64 X 5 non-null float64 Y 5 non-null float64 Z 5 non-null float64 dtypes: float64(4) memory usage: 200.0+ bytes","title":"DataFrame Summaries"},{"location":"bootcampsnotes/pandas/02-DataFrames/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/03-Missing-Data/","text":"Crash Course: Pandas \u00b6 by Jawad Haider 03 - Missing Data \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Missing Data 2 Great Job! Thats the end of this part. Missing Data \u00b6 Let\u2019s show a few convenient methods to deal with Missing Data in pandas: import numpy as np import pandas as pd df = pd . DataFrame ({ 'A' :[ 1 , 2 , np . nan ], 'B' :[ 5 , np . nan , np . nan ], 'C' :[ 1 , 2 , 3 ]}) df A B C 0 1.0 5.0 1 1 2.0 NaN 2 2 NaN NaN 3 df . dropna () A B C 0 1.0 5.0 1 df . dropna ( axis = 1 ) C 0 1 1 2 2 3 df . dropna ( thresh = 2 ) A B C 0 1.0 5.0 1 1 2.0 NaN 2 df . fillna ( value = 'FILL VALUE' ) A B C 0 1 5 1 1 2 FILL VALUE 2 2 FILL VALUE FILL VALUE 3 df [ 'A' ] . fillna ( value = df [ 'A' ] . mean ()) 0 1.0 1 2.0 2 1.5 Name: A, dtype: float64 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"03 Missing Data"},{"location":"bootcampsnotes/pandas/03-Missing-Data/#crash-course-pandas","text":"by Jawad Haider","title":"Crash Course: Pandas"},{"location":"bootcampsnotes/pandas/03-Missing-Data/#03-missing-data","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Missing Data 2 Great Job! Thats the end of this part.","title":"03 - Missing Data"},{"location":"bootcampsnotes/pandas/03-Missing-Data/#missing-data","text":"Let\u2019s show a few convenient methods to deal with Missing Data in pandas: import numpy as np import pandas as pd df = pd . DataFrame ({ 'A' :[ 1 , 2 , np . nan ], 'B' :[ 5 , np . nan , np . nan ], 'C' :[ 1 , 2 , 3 ]}) df A B C 0 1.0 5.0 1 1 2.0 NaN 2 2 NaN NaN 3 df . dropna () A B C 0 1.0 5.0 1 df . dropna ( axis = 1 ) C 0 1 1 2 2 3 df . dropna ( thresh = 2 ) A B C 0 1.0 5.0 1 1 2.0 NaN 2 df . fillna ( value = 'FILL VALUE' ) A B C 0 1 5 1 1 2 FILL VALUE 2 2 FILL VALUE FILL VALUE 3 df [ 'A' ] . fillna ( value = df [ 'A' ] . mean ()) 0 1.0 1 2.0 2 1.5 Name: A, dtype: float64","title":"Missing Data"},{"location":"bootcampsnotes/pandas/03-Missing-Data/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/04-Groupby/","text":"Crash Course: Pandas \u00b6 by Jawad Haider 04 - Groupby \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Groupby 2 Great Job! Thats the end of this part. Groupby \u00b6 The groupby method allows you to group rows of data together and call aggregate functions import pandas as pd # Create dataframe data = { 'Company' :[ 'GOOG' , 'GOOG' , 'MSFT' , 'MSFT' , 'FB' , 'FB' ], 'Person' :[ 'Sam' , 'Charlie' , 'Amy' , 'Vanessa' , 'Carl' , 'Sarah' ], 'Sales' :[ 200 , 120 , 340 , 124 , 243 , 350 ]} df = pd . DataFrame ( data ) df Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 Now you can use the .groupby() method to group rows together based off of a column name. For instance let\u2019s group based off of Company. This will create a DataFrameGroupBy object: df . groupby ( 'Company' ) <pandas.core.groupby.DataFrameGroupBy object at 0x113014128> You can save this object as a new variable: by_comp = df . groupby ( \"Company\" ) And then call aggregate methods off the object: by_comp . mean () Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 df . groupby ( 'Company' ) . mean () Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 More examples of aggregate methods: by_comp . std () Sales Company FB 75.660426 GOOG 56.568542 MSFT 152.735065 by_comp . min () Person Sales Company FB Carl 243 GOOG Charlie 120 MSFT Amy 124 by_comp . max () Person Sales Company FB Sarah 350 GOOG Sam 200 MSFT Vanessa 340 by_comp . count () Person Sales Company FB 2 2 GOOG 2 2 MSFT 2 2 by_comp . describe () Sales Company FB count 2.000000 mean 296.500000 std 75.660426 min 243.000000 25% 269.750000 50% 296.500000 75% 323.250000 max 350.000000 GOOG count 2.000000 mean 160.000000 std 56.568542 min 120.000000 25% 140.000000 50% 160.000000 75% 180.000000 max 200.000000 MSFT count 2.000000 mean 232.000000 std 152.735065 min 124.000000 25% 178.000000 50% 232.000000 75% 286.000000 max 340.000000 by_comp . describe () . transpose () Company FB GOOG MSFT count mean std min 25% 50% 75% max count mean ... 75% max count mean std min 25% 50% 75% max Sales 2.0 296.5 75.660426 243.0 269.75 296.5 323.25 350.0 2.0 160.0 ... 180.0 200.0 2.0 232.0 152.735065 124.0 178.0 232.0 286.0 340.0 1 rows \u00d7 24 columns by_comp . describe () . transpose ()[ 'GOOG' ] count mean std min 25% 50% 75% max Sales 2.0 160.0 56.568542 120.0 140.0 160.0 180.0 200.0 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"04 Groupby"},{"location":"bootcampsnotes/pandas/04-Groupby/#crash-course-pandas","text":"by Jawad Haider","title":"Crash Course: Pandas"},{"location":"bootcampsnotes/pandas/04-Groupby/#04-groupby","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Groupby 2 Great Job! Thats the end of this part.","title":"04 - Groupby"},{"location":"bootcampsnotes/pandas/04-Groupby/#groupby","text":"The groupby method allows you to group rows of data together and call aggregate functions import pandas as pd # Create dataframe data = { 'Company' :[ 'GOOG' , 'GOOG' , 'MSFT' , 'MSFT' , 'FB' , 'FB' ], 'Person' :[ 'Sam' , 'Charlie' , 'Amy' , 'Vanessa' , 'Carl' , 'Sarah' ], 'Sales' :[ 200 , 120 , 340 , 124 , 243 , 350 ]} df = pd . DataFrame ( data ) df Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 Now you can use the .groupby() method to group rows together based off of a column name. For instance let\u2019s group based off of Company. This will create a DataFrameGroupBy object: df . groupby ( 'Company' ) <pandas.core.groupby.DataFrameGroupBy object at 0x113014128> You can save this object as a new variable: by_comp = df . groupby ( \"Company\" ) And then call aggregate methods off the object: by_comp . mean () Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 df . groupby ( 'Company' ) . mean () Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 More examples of aggregate methods: by_comp . std () Sales Company FB 75.660426 GOOG 56.568542 MSFT 152.735065 by_comp . min () Person Sales Company FB Carl 243 GOOG Charlie 120 MSFT Amy 124 by_comp . max () Person Sales Company FB Sarah 350 GOOG Sam 200 MSFT Vanessa 340 by_comp . count () Person Sales Company FB 2 2 GOOG 2 2 MSFT 2 2 by_comp . describe () Sales Company FB count 2.000000 mean 296.500000 std 75.660426 min 243.000000 25% 269.750000 50% 296.500000 75% 323.250000 max 350.000000 GOOG count 2.000000 mean 160.000000 std 56.568542 min 120.000000 25% 140.000000 50% 160.000000 75% 180.000000 max 200.000000 MSFT count 2.000000 mean 232.000000 std 152.735065 min 124.000000 25% 178.000000 50% 232.000000 75% 286.000000 max 340.000000 by_comp . describe () . transpose () Company FB GOOG MSFT count mean std min 25% 50% 75% max count mean ... 75% max count mean std min 25% 50% 75% max Sales 2.0 296.5 75.660426 243.0 269.75 296.5 323.25 350.0 2.0 160.0 ... 180.0 200.0 2.0 232.0 152.735065 124.0 178.0 232.0 286.0 340.0 1 rows \u00d7 24 columns by_comp . describe () . transpose ()[ 'GOOG' ] count mean std min 25% 50% 75% max Sales 2.0 160.0 56.568542 120.0 140.0 160.0 180.0 200.0","title":"Groupby"},{"location":"bootcampsnotes/pandas/04-Groupby/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/05-Operations/","text":"Crash Course: Pandas \u00b6 by Jawad Haider 05 - Operations \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Operations 1.0.1 Info on Unique Values 1.0.2 Selecting Data 1.0.3 Applying Functions 1.0.4 Permanently Removing a Column 1.0.5 Get column and index names: 1.0.6 Sorting and Ordering a DataFrame: 2 Great Job! Thats the end of this part. Operations \u00b6 There are lots of operations with pandas that will be really useful to you, but don\u2019t fall into any distinct category. Let\u2019s show them here in this lecture: import pandas as pd df = pd . DataFrame ({ 'col1' :[ 1 , 2 , 3 , 4 ], 'col2' :[ 444 , 555 , 666 , 444 ], 'col3' :[ 'abc' , 'def' , 'ghi' , 'xyz' ]}) df . head () col1 col2 col3 0 1 444 abc 1 2 555 def 2 3 666 ghi 3 4 444 xyz Info on Unique Values \u00b6 df [ 'col2' ] . unique () array([444, 555, 666]) df [ 'col2' ] . nunique () 3 df [ 'col2' ] . value_counts () 444 2 555 1 666 1 Name: col2, dtype: int64 Selecting Data \u00b6 #Select from DataFrame using criteria from multiple columns newdf = df [( df [ 'col1' ] > 2 ) & ( df [ 'col2' ] == 444 )] newdf col1 col2 col3 3 4 444 xyz Applying Functions \u00b6 def times2 ( x ): return x * 2 df [ 'col1' ] . apply ( times2 ) 0 2 1 4 2 6 3 8 Name: col1, dtype: int64 df [ 'col3' ] . apply ( len ) 0 3 1 3 2 3 3 3 Name: col3, dtype: int64 df [ 'col1' ] . sum () 10 Permanently Removing a Column \u00b6 del df [ 'col1' ] df col2 col3 0 444 abc 1 555 def 2 666 ghi 3 444 xyz Get column and index names: \u00b6 df . columns Index(['col2', 'col3'], dtype='object') df . index RangeIndex(start=0, stop=4, step=1) Sorting and Ordering a DataFrame: \u00b6 df col2 col3 0 444 abc 1 555 def 2 666 ghi 3 444 xyz df . sort_values ( by = 'col2' ) #inplace=False by default col2 col3 0 444 abc 3 444 xyz 1 555 def 2 666 ghi Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"05 Operations"},{"location":"bootcampsnotes/pandas/05-Operations/#crash-course-pandas","text":"by Jawad Haider","title":"Crash Course: Pandas"},{"location":"bootcampsnotes/pandas/05-Operations/#05-operations","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Operations 1.0.1 Info on Unique Values 1.0.2 Selecting Data 1.0.3 Applying Functions 1.0.4 Permanently Removing a Column 1.0.5 Get column and index names: 1.0.6 Sorting and Ordering a DataFrame: 2 Great Job! Thats the end of this part.","title":"05 - Operations"},{"location":"bootcampsnotes/pandas/05-Operations/#operations","text":"There are lots of operations with pandas that will be really useful to you, but don\u2019t fall into any distinct category. Let\u2019s show them here in this lecture: import pandas as pd df = pd . DataFrame ({ 'col1' :[ 1 , 2 , 3 , 4 ], 'col2' :[ 444 , 555 , 666 , 444 ], 'col3' :[ 'abc' , 'def' , 'ghi' , 'xyz' ]}) df . head () col1 col2 col3 0 1 444 abc 1 2 555 def 2 3 666 ghi 3 4 444 xyz","title":"Operations"},{"location":"bootcampsnotes/pandas/05-Operations/#info-on-unique-values","text":"df [ 'col2' ] . unique () array([444, 555, 666]) df [ 'col2' ] . nunique () 3 df [ 'col2' ] . value_counts () 444 2 555 1 666 1 Name: col2, dtype: int64","title":"Info on Unique Values"},{"location":"bootcampsnotes/pandas/05-Operations/#selecting-data","text":"#Select from DataFrame using criteria from multiple columns newdf = df [( df [ 'col1' ] > 2 ) & ( df [ 'col2' ] == 444 )] newdf col1 col2 col3 3 4 444 xyz","title":"Selecting Data"},{"location":"bootcampsnotes/pandas/05-Operations/#applying-functions","text":"def times2 ( x ): return x * 2 df [ 'col1' ] . apply ( times2 ) 0 2 1 4 2 6 3 8 Name: col1, dtype: int64 df [ 'col3' ] . apply ( len ) 0 3 1 3 2 3 3 3 Name: col3, dtype: int64 df [ 'col1' ] . sum () 10","title":"Applying Functions"},{"location":"bootcampsnotes/pandas/05-Operations/#permanently-removing-a-column","text":"del df [ 'col1' ] df col2 col3 0 444 abc 1 555 def 2 666 ghi 3 444 xyz","title":"Permanently Removing a Column"},{"location":"bootcampsnotes/pandas/05-Operations/#get-column-and-index-names","text":"df . columns Index(['col2', 'col3'], dtype='object') df . index RangeIndex(start=0, stop=4, step=1)","title":"Get column and index names:"},{"location":"bootcampsnotes/pandas/05-Operations/#sorting-and-ordering-a-dataframe","text":"df col2 col3 0 444 abc 1 555 def 2 666 ghi 3 444 xyz df . sort_values ( by = 'col2' ) #inplace=False by default col2 col3 0 444 abc 3 444 xyz 1 555 def 2 666 ghi","title":"Sorting and Ordering a DataFrame:"},{"location":"bootcampsnotes/pandas/05-Operations/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/","text":"Crash Course: Pandas \u00b6 by Jawad Haider 06 - Input Output \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Data Input and Output 1.1 CSV 1.1.1 CSV Input 1.1.2 CSV Output 1.2 Excel 1.2.1 Excel Input 1.2.2 Excel Output 1.3 HTML 1.3.1 HTML Input 2 Great Job! Thats the end of this part. NOTE: Typically we will just be either reading csv files directly or using pandas-datareader to pull data from the web. Consider this lecture just a quick overview of what is possible with pandas (we won\u2019t be working with SQL or Excel files in this course) Data Input and Output \u00b6 This notebook is the reference code for getting input and output, pandas can read a variety of file types using its pd.read_ methods. Let\u2019s take a look at the most common data types: import numpy as np import pandas as pd CSV \u00b6 Comma Separated Values files are text files that use commas as field delimeters. Unless you\u2019re running the virtual environment included with the course, you may need to install xlrd and openpyxl . In your terminal/command prompt run: conda install xlrd conda install openpyxl Then restart Jupyter Notebook. (or use pip install if you aren\u2019t using the Anaconda Distribution) CSV Input \u00b6 df = pd . read_csv ( 'example.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 CSV Output \u00b6 df . to_csv ( 'example.csv' , index = False ) Excel \u00b6 Pandas can read and write MS Excel files. However, this only imports data, not formulas or images. A file that contains images or macros may cause the .read_excel() method to crash. Excel Input \u00b6 pd . read_excel ( 'Excel_Sample.xlsx' , sheet_name = 'Sheet1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Excel Output \u00b6 df . to_excel ( 'Excel_Sample.xlsx' , sheet_name = 'Sheet1' ) HTML \u00b6 Pandas can read table tabs off of HTML. Unless you\u2019re running the virtual environment included with the course, you may need to install lxml , htmllib5 , and BeautifulSoup4 . In your terminal/command prompt run: conda install lxml conda install html5lib conda install beautifulsoup4 Then restart Jupyter Notebook. (or use pip install if you aren\u2019t using the Anaconda Distribution) HTML Input \u00b6 Pandas read_html function will read tables off of a webpage and return a list of DataFrame objects: df = pd . read_html ( 'http://www.fdic.gov/bank/individual/failed/banklist.html' ) df [ 0 ] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Bank Name City ST CERT Acquiring Institution Closing Date Updated Date 0 Washington Federal Bank for Savings Chicago IL 30570 Royal Savings Bank December 15, 2017 February 21, 2018 1 The Farmers and Merchants State Bank of Argonia Argonia KS 17719 Conway Bank October 13, 2017 February 21, 2018 2 Fayette County Bank Saint Elmo IL 1802 United Fidelity Bank, fsb May 26, 2017 July 26, 2017 3 Guaranty Bank, (d/b/a BestBank in Georgia & Mi... Milwaukee WI 30003 First-Citizens Bank & Trust Company May 5, 2017 March 22, 2018 4 First NBC Bank New Orleans LA 58302 Whitney Bank April 28, 2017 December 5, 2017 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"06 Data Input and Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#crash-course-pandas","text":"by Jawad Haider","title":"Crash Course: Pandas"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#06-input-output","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Data Input and Output 1.1 CSV 1.1.1 CSV Input 1.1.2 CSV Output 1.2 Excel 1.2.1 Excel Input 1.2.2 Excel Output 1.3 HTML 1.3.1 HTML Input 2 Great Job! Thats the end of this part. NOTE: Typically we will just be either reading csv files directly or using pandas-datareader to pull data from the web. Consider this lecture just a quick overview of what is possible with pandas (we won\u2019t be working with SQL or Excel files in this course)","title":"06 - Input Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#data-input-and-output","text":"This notebook is the reference code for getting input and output, pandas can read a variety of file types using its pd.read_ methods. Let\u2019s take a look at the most common data types: import numpy as np import pandas as pd","title":"Data Input and Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#csv","text":"Comma Separated Values files are text files that use commas as field delimeters. Unless you\u2019re running the virtual environment included with the course, you may need to install xlrd and openpyxl . In your terminal/command prompt run: conda install xlrd conda install openpyxl Then restart Jupyter Notebook. (or use pip install if you aren\u2019t using the Anaconda Distribution)","title":"CSV"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#csv-input","text":"df = pd . read_csv ( 'example.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15","title":"CSV Input"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#csv-output","text":"df . to_csv ( 'example.csv' , index = False )","title":"CSV Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#excel","text":"Pandas can read and write MS Excel files. However, this only imports data, not formulas or images. A file that contains images or macros may cause the .read_excel() method to crash.","title":"Excel"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#excel-input","text":"pd . read_excel ( 'Excel_Sample.xlsx' , sheet_name = 'Sheet1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15","title":"Excel Input"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#excel-output","text":"df . to_excel ( 'Excel_Sample.xlsx' , sheet_name = 'Sheet1' )","title":"Excel Output"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#html","text":"Pandas can read table tabs off of HTML. Unless you\u2019re running the virtual environment included with the course, you may need to install lxml , htmllib5 , and BeautifulSoup4 . In your terminal/command prompt run: conda install lxml conda install html5lib conda install beautifulsoup4 Then restart Jupyter Notebook. (or use pip install if you aren\u2019t using the Anaconda Distribution)","title":"HTML"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#html-input","text":"Pandas read_html function will read tables off of a webpage and return a list of DataFrame objects: df = pd . read_html ( 'http://www.fdic.gov/bank/individual/failed/banklist.html' ) df [ 0 ] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Bank Name City ST CERT Acquiring Institution Closing Date Updated Date 0 Washington Federal Bank for Savings Chicago IL 30570 Royal Savings Bank December 15, 2017 February 21, 2018 1 The Farmers and Merchants State Bank of Argonia Argonia KS 17719 Conway Bank October 13, 2017 February 21, 2018 2 Fayette County Bank Saint Elmo IL 1802 United Fidelity Bank, fsb May 26, 2017 July 26, 2017 3 Guaranty Bank, (d/b/a BestBank in Georgia & Mi... Milwaukee WI 30003 First-Citizens Bank & Trust Company May 5, 2017 March 22, 2018 4 First NBC Bank New Orleans LA 58302 Whitney Bank April 28, 2017 December 5, 2017","title":"HTML Input"},{"location":"bootcampsnotes/pandas/06-Data-Input-and-Output/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/07-Pandas-Exercises/","text":"Crash Course: Pandas \u00b6 by Jawad Haider 07 - Pandas Excercise \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Pandas Exercises 2 Great Job! Thats the end of this part. Pandas Exercises \u00b6 TASK: Import pandas # CODE HERE TASK: Read in the bank.csv file that is located under the 01-Crash-Course-Pandas folder. Pay close attention to where the .csv file is located! Please don\u2019t post to the QA forums if you can\u2019t figure this one out, instead, run our solutions notebook directly to see how its done. # CODE HERE TASK: Display the first 5 rows of the data set # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no TASK: What is the average (mean) age of the people in the dataset? # CODE HERE 41.17009511170095 TASK: What is the marital status of the youngest person in the dataset? HINT # CODE HERE 'single' TASK: How many unique job categories are there? # CODE HERE 12 TASK: How many people are there per job category? (Take a peek at the expected output) # CODE HERE management 969 blue-collar 946 technician 768 admin. 478 services 417 retired 230 self-employed 183 entrepreneur 168 unemployed 128 housemaid 112 student 84 unknown 38 Name: job, dtype: int64 **TASK: What percent of people in the dataset were married? ** #CODE HERE 61.86684361866843 TASK: There is a column labeled \u201cdefault\u201d. Use pandas\u2019 .map() method to create a new column called \u201cdefault code\u201d which contains a 0 if there was no default, or a 1 if there was a default. Then show the head of the dataframe with this new column. Helpful Hint Link One Helpful Hint Link Two # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 TASK: Using pandas .apply() method, create a new column called \u201cmarital code\u201d. This column will only contained a shortened code of the possible marital status first letter. (For example \u201cm\u201d for \u201cmarried\u201d , \u201cs\u201d for \u201csingle\u201d etc\u2026 See if you can do this with a lambda expression. Lots of ways to do this one! Hint Link # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code marital code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 m 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 m 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 s 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 m 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 m TASK: What was the longest lasting duration? # CODE HERE 3025 TASK: What is the most common education level for people who are unemployed? # CODE HERE secondary 68 tertiary 32 primary 26 unknown 2 Name: education, dtype: int64 TASK: What is the average (mean) age for being unemployed? # CODE HERE 40.90625 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"07 Pandas Exercises"},{"location":"bootcampsnotes/pandas/07-Pandas-Exercises/#crash-course-pandas","text":"by Jawad Haider","title":"Crash Course: Pandas"},{"location":"bootcampsnotes/pandas/07-Pandas-Exercises/#07-pandas-excercise","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Pandas Exercises 2 Great Job! Thats the end of this part.","title":"07 - Pandas Excercise"},{"location":"bootcampsnotes/pandas/07-Pandas-Exercises/#pandas-exercises","text":"TASK: Import pandas # CODE HERE TASK: Read in the bank.csv file that is located under the 01-Crash-Course-Pandas folder. Pay close attention to where the .csv file is located! Please don\u2019t post to the QA forums if you can\u2019t figure this one out, instead, run our solutions notebook directly to see how its done. # CODE HERE TASK: Display the first 5 rows of the data set # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no TASK: What is the average (mean) age of the people in the dataset? # CODE HERE 41.17009511170095 TASK: What is the marital status of the youngest person in the dataset? HINT # CODE HERE 'single' TASK: How many unique job categories are there? # CODE HERE 12 TASK: How many people are there per job category? (Take a peek at the expected output) # CODE HERE management 969 blue-collar 946 technician 768 admin. 478 services 417 retired 230 self-employed 183 entrepreneur 168 unemployed 128 housemaid 112 student 84 unknown 38 Name: job, dtype: int64 **TASK: What percent of people in the dataset were married? ** #CODE HERE 61.86684361866843 TASK: There is a column labeled \u201cdefault\u201d. Use pandas\u2019 .map() method to create a new column called \u201cdefault code\u201d which contains a 0 if there was no default, or a 1 if there was a default. Then show the head of the dataframe with this new column. Helpful Hint Link One Helpful Hint Link Two # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 TASK: Using pandas .apply() method, create a new column called \u201cmarital code\u201d. This column will only contained a shortened code of the possible marital status first letter. (For example \u201cm\u201d for \u201cmarried\u201d , \u201cs\u201d for \u201csingle\u201d etc\u2026 See if you can do this with a lambda expression. Lots of ways to do this one! Hint Link # CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code marital code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 m 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 m 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 s 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 m 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 m TASK: What was the longest lasting duration? # CODE HERE 3025 TASK: What is the most common education level for people who are unemployed? # CODE HERE secondary 68 tertiary 32 primary 26 unknown 2 Name: education, dtype: int64 TASK: What is the average (mean) age for being unemployed? # CODE HERE 40.90625","title":"Pandas Exercises"},{"location":"bootcampsnotes/pandas/07-Pandas-Exercises/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/","text":"Crash Course: Pandas \u00b6 by Jawad Haider 07 - Pandas Excercise Solutions \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Pandas Exercises - Solutions 2 Great Job! Thats the end of this part. Pandas Exercises - Solutions \u00b6 TASK: Import pandas # CODE HERE import pandas as pd TASK: Read in the bank.csv file that is located under the 01-Crash-Course-Pandas folder. Pay close attention to where the .csv file is located! Please don\u2019t post to the QA forums if you can\u2019t figure this one out, instead, run our solutions notebook directly to see how its done. df = pd . read_csv ( 'bank.csv' ) TASK: Display the first 5 rows of the data set # CODE HERE df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no TASK: What is the average (mean) age of the people in the dataset? # CODE HERE df [ 'age' ] . mean () 41.17009511170095 TASK: What is the marital status of the youngest person in the dataset? HINT # CODE HERE df [ 'age' ] . idxmin () 503 df . iloc [ 503 ][ 'marital' ] 'single' TASK: How many unique job categories are there? # CODE HERE df [ 'job' ] . nunique () 12 TASK: How many people are there per job category? (Take a peek at the expected output) # CODE HERE df [ 'job' ] . value_counts () management 969 blue-collar 946 technician 768 admin. 478 services 417 retired 230 self-employed 183 entrepreneur 168 unemployed 128 housemaid 112 student 84 unknown 38 Name: job, dtype: int64 **TASK: What percent of people in the dataset were married? ** #CODE HERE # Many, many ways to do this one! Here is just one way: 100 * df [ 'marital' ] . value_counts ()[ 'married' ] / len ( df ) # df['marital].value_counts() 61.86684361866843 TASK: There is a column labeled \u201cdefault\u201d. Use pandas\u2019 .map() method to create a new column called \u201cdefault code\u201d which contains a 0 if there was no default, or a 1 if there was a default. Then show the head of the dataframe with this new column. Helpful Hint Link One Helpful Hint Link Two df [ 'default code' ] = df [ 'default' ] . map ({ 'no' : 0 , 'yes' : 1 }) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 TASK: Using pandas .apply() method, create a new column called \u201cmarital code\u201d. This column will only contained a shortened code of the possible marital status first letter. (For example \u201cm\u201d for \u201cmarried\u201d , \u201cs\u201d for \u201csingle\u201d etc\u2026 See if you can do this with a lambda expression. Lots of ways to do this one! Hint Link # CODE HERE df [ 'marital code' ] = df [ 'marital' ] . apply ( lambda status : status [ 0 ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code marital code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 m 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 m 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 s 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 m 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 m TASK: What was the longest lasting duration? # CODE HERE df [ 'duration' ] . max () 3025 TASK: What is the most common education level for people who are unemployed? # CODE HERE df [ df [ 'job' ] == 'unemployed' ][ 'education' ] . value_counts () secondary 68 tertiary 32 primary 26 unknown 2 Name: education, dtype: int64 TASK: What is the average (mean) age for being unemployed? # CODE HERE df [ df [ 'job' ] == 'unemployed' ][ 'age' ] . mean () 40.90625 Great Job! Thats the end of this part. \u00b6 Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"08 Pandas Exercises Solutions"},{"location":"bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/#crash-course-pandas","text":"by Jawad Haider","title":"Crash Course: Pandas"},{"location":"bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/#07-pandas-excercise-solutions","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Pandas Exercises - Solutions 2 Great Job! Thats the end of this part.","title":"07 - Pandas Excercise Solutions"},{"location":"bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/#pandas-exercises-solutions","text":"TASK: Import pandas # CODE HERE import pandas as pd TASK: Read in the bank.csv file that is located under the 01-Crash-Course-Pandas folder. Pay close attention to where the .csv file is located! Please don\u2019t post to the QA forums if you can\u2019t figure this one out, instead, run our solutions notebook directly to see how its done. df = pd . read_csv ( 'bank.csv' ) TASK: Display the first 5 rows of the data set # CODE HERE df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no TASK: What is the average (mean) age of the people in the dataset? # CODE HERE df [ 'age' ] . mean () 41.17009511170095 TASK: What is the marital status of the youngest person in the dataset? HINT # CODE HERE df [ 'age' ] . idxmin () 503 df . iloc [ 503 ][ 'marital' ] 'single' TASK: How many unique job categories are there? # CODE HERE df [ 'job' ] . nunique () 12 TASK: How many people are there per job category? (Take a peek at the expected output) # CODE HERE df [ 'job' ] . value_counts () management 969 blue-collar 946 technician 768 admin. 478 services 417 retired 230 self-employed 183 entrepreneur 168 unemployed 128 housemaid 112 student 84 unknown 38 Name: job, dtype: int64 **TASK: What percent of people in the dataset were married? ** #CODE HERE # Many, many ways to do this one! Here is just one way: 100 * df [ 'marital' ] . value_counts ()[ 'married' ] / len ( df ) # df['marital].value_counts() 61.86684361866843 TASK: There is a column labeled \u201cdefault\u201d. Use pandas\u2019 .map() method to create a new column called \u201cdefault code\u201d which contains a 0 if there was no default, or a 1 if there was a default. Then show the head of the dataframe with this new column. Helpful Hint Link One Helpful Hint Link Two df [ 'default code' ] = df [ 'default' ] . map ({ 'no' : 0 , 'yes' : 1 }) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 TASK: Using pandas .apply() method, create a new column called \u201cmarital code\u201d. This column will only contained a shortened code of the possible marital status first letter. (For example \u201cm\u201d for \u201cmarried\u201d , \u201cs\u201d for \u201csingle\u201d etc\u2026 See if you can do this with a lambda expression. Lots of ways to do this one! Hint Link # CODE HERE df [ 'marital code' ] = df [ 'marital' ] . apply ( lambda status : status [ 0 ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y default code marital code 0 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 0 unknown no 0 m 1 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 4 failure no 0 m 2 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 1 failure no 0 s 3 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 0 unknown no 0 m 4 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 0 unknown no 0 m TASK: What was the longest lasting duration? # CODE HERE df [ 'duration' ] . max () 3025 TASK: What is the most common education level for people who are unemployed? # CODE HERE df [ df [ 'job' ] == 'unemployed' ][ 'education' ] . value_counts () secondary 68 tertiary 32 primary 26 unknown 2 Name: education, dtype: int64 TASK: What is the average (mean) age for being unemployed? # CODE HERE df [ df [ 'job' ] == 'unemployed' ][ 'age' ] . mean () 40.90625","title":"Pandas Exercises - Solutions"},{"location":"bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/#great-job-thats-the-end-of-this-part","text":"Don't forget to give a star on github and follow for more curated Computer Science, Machine Learning materials","title":"Great Job! Thats the end of this part."},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/","text":"PyTorch Basics \u00b6 by Jawad Haider 00 - Tensor Basics \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Tensor Basics 1.1 Perform standard imports 1.2 Converting NumPy arrays to PyTorch tensors 1.3 Copying vs. sharing 1.4 Class constructors 1.5 Creating tensors from scratch 1.5.1 Uninitialized tensors with .empty() 1.5.2 Initialized tensors with .zeros() and .ones() 1.5.3 Tensors from ranges 1.5.4 Tensors from data 1.5.5 Changing the dtype of existing tensors 1.5.6 Random number tensors 1.5.7 Random number tensors that follow the input size 1.5.8 Setting the random seed 1.6 Tensor attributes 1.6.1 Great job! Tensor Basics \u00b6 This section covers: * Converting NumPy arrays to PyTorch tensors * Creating tensors from scratch Perform standard imports \u00b6 import torch import numpy as np Confirm you\u2019re using PyTorch version 1.1.0 torch . __version__ '1.1.0' Converting NumPy arrays to PyTorch tensors \u00b6 A torch.Tensor is a multi-dimensional matrix containing elements of a single data type. Calculations between tensors can only happen if the tensors share the same dtype. In some cases tensors are used as a replacement for NumPy to use the power of GPUs (more on this later). arr = np . array ([ 1 , 2 , 3 , 4 , 5 ]) print ( arr ) print ( arr . dtype ) print ( type ( arr )) [1 2 3 4 5] int32 <class 'numpy.ndarray'> x = torch . from_numpy ( arr ) # Equivalent to x = torch.as_tensor(arr) print ( x ) tensor([1, 2, 3, 4, 5], dtype=torch.int32) # Print the type of data held by the tensor print ( x . dtype ) torch.int32 # Print the tensor object type print ( type ( x )) print ( x . type ()) # this is more specific! <class 'torch.Tensor'> torch.IntTensor arr2 = np . arange ( 0. , 12. ) . reshape ( 4 , 3 ) print ( arr2 ) [[ 0. 1. 2.] [ 3. 4. 5.] [ 6. 7. 8.] [ 9. 10. 11.]] x2 = torch . from_numpy ( arr2 ) print ( x2 ) print ( x2 . type ()) tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]], dtype=torch.float64) torch.DoubleTensor Here torch.DoubleTensor refers to 64-bit floating point data. Tensor Datatypes TYPE NAME EQUIVALENT TENSOR TYPE 32-bit integer (signed) torch.int32 torch.int IntTensor 64-bit integer (signed) torch.int64 torch.long LongTensor 16-bit integer (signed) torch.int16 torch.short ShortTensor 32-bit floating point torch.float32 torch.float FloatTensor 64-bit floating point torch.float64 torch.double DoubleTensor 16-bit floating point torch.float16 torch.half HalfTensor 8-bit integer (signed) torch.int8 CharTensor 8-bit integer (unsigned) torch.uint8 ByteTensor Copying vs. sharing \u00b6 torch.from_numpy() torch.as_tensor() torch.tensor() There are a number of different functions available for creating tensors . When using torch.from_numpy() and torch.as_tensor() , the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy. # Using torch.from_numpy() arr = np . arange ( 0 , 5 ) t = torch . from_numpy ( arr ) print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) arr [ 2 ] = 77 print ( t ) tensor([ 0, 1, 77, 3, 4], dtype=torch.int32) # Using torch.tensor() arr = np . arange ( 0 , 5 ) t = torch . tensor ( arr ) print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) arr [ 2 ] = 77 print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) Class constructors \u00b6 torch.Tensor() torch.FloatTensor() torch.LongTensor() , etc. There\u2019s a subtle difference between using the factory function torch.tensor(data) and the class constructor torch.Tensor(data) . The factory function determines the dtype from the incoming data, or from a passed-in dtype argument. The class constructor torch.Tensor() is simply an alias for torch.FloatTensor(data) . Consider the following: data = np . array ([ 1 , 2 , 3 ]) a = torch . Tensor ( data ) # Equivalent to cc = torch.FloatTensor(data) print ( a , a . type ()) tensor([1., 2., 3.]) torch.FloatTensor b = torch . tensor ( data ) print ( b , b . type ()) tensor([1, 2, 3], dtype=torch.int32) torch.IntTensor c = torch . tensor ( data , dtype = torch . long ) print ( c , c . type ()) tensor([1, 2, 3]) torch.LongTensor Creating tensors from scratch \u00b6 Uninitialized tensors with .empty() \u00b6 torch.empty() returns an uninitialized tensor. Essentially a block of memory is allocated according to the size of the tensor, and any values already sitting in the block are returned. This is similar to the behavior of numpy.empty() . x = torch . empty ( 4 , 3 ) print ( x ) tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) Initialized tensors with .zeros() and .ones() \u00b6 torch.zeros(size) torch.ones(size) It\u2019s a good idea to pass in the intended dtype. x = torch . zeros ( 4 , 3 , dtype = torch . int64 ) print ( x ) tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) Tensors from ranges \u00b6 torch.arange(start,end,step) torch.linspace(start,end,steps) Note that with .arange() , end is exclusive, while with linspace() , end is inclusive. x = torch . arange ( 0 , 18 , 2 ) . reshape ( 3 , 3 ) print ( x ) tensor([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]]) x = torch . linspace ( 0 , 18 , 12 ) . reshape ( 3 , 4 ) print ( x ) tensor([[ 0.0000, 1.6364, 3.2727, 4.9091], [ 6.5455, 8.1818, 9.8182, 11.4545], [13.0909, 14.7273, 16.3636, 18.0000]]) Tensors from data \u00b6 torch.tensor() will choose the dtype based on incoming data: x = torch . tensor ([ 1 , 2 , 3 , 4 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([1, 2, 3, 4]) torch.int64 torch.LongTensor Alternatively you can set the type by the tensor method used. For a list of tensor types visit https://pytorch.org/docs/stable/tensors.html x = torch . FloatTensor ([ 5 , 6 , 7 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([5., 6., 7.]) torch.float32 torch.FloatTensor You can also pass the dtype in as an argument. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype x = torch . tensor ([ 8 , 9 , - 3 ], dtype = torch . int ) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([ 8, 9, -3], dtype=torch.int32) torch.int32 torch.IntTensor Changing the dtype of existing tensors \u00b6 Don\u2019t be tempted to use x = torch.tensor(x, dtype=torch.type) as it will raise an error about improper use of tensor cloning. Instead, use the tensor .type() method. print ( 'Old:' , x . type ()) x = x . type ( torch . int64 ) print ( 'New:' , x . type ()) Old: torch.IntTensor New: torch.LongTensor Random number tensors \u00b6 torch.rand(size) returns random samples from a uniform distribution over [0, 1) torch.randn(size) returns samples from the \u201cstandard normal\u201d distribution [\u03c3 = 1] Unlike rand which is uniform, values closer to zero are more likely to appear. torch.randint(low,high,size) returns random integers from low (inclusive) to high (exclusive) x = torch . rand ( 4 , 3 ) print ( x ) tensor([[0.0211, 0.2336, 0.6775], [0.4790, 0.5132, 0.9878], [0.7552, 0.0789, 0.1860], [0.6712, 0.1564, 0.3753]]) x = torch . randn ( 4 , 3 ) print ( x ) tensor([[ 0.7164, -0.1538, -0.9980], [-1.8252, 1.1863, -0.1523], [ 1.4093, -0.0212, -1.5598], [ 0.1831, -0.6961, 1.3497]]) x = torch . randint ( 0 , 5 , ( 4 , 3 )) print ( x ) tensor([[0, 3, 0], [1, 3, 4], [1, 2, 3], [4, 4, 3]]) Random number tensors that follow the input size \u00b6 torch.rand_like(input) torch.randn_like(input) torch.randint_like(input,low,high) these return random number tensors with the same size as input x = torch . zeros ( 2 , 5 ) print ( x ) tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) x2 = torch . randn_like ( x ) print ( x2 ) tensor([[-0.5442, -0.3149, 0.0922, 1.1829, -0.7873], [ 0.3143, 0.9465, 0.4534, 0.4623, 2.2044]]) The same syntax can be used with torch.zeros_like(input) torch.ones_like(input) x3 = torch . ones_like ( x2 ) print ( x3 ) tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) Setting the random seed \u00b6 torch.manual_seed(int) is used to obtain reproducible results torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]]) torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]]) Tensor attributes \u00b6 Besides dtype , we can look at other tensor attributes like shape , device and layout x . shape torch.Size([2, 3]) x . size () # equivalent to x.shape torch.Size([2, 3]) x . device device(type='cpu') PyTorch supports use of multiple devices , harnessing the power of one or more GPUs in addition to the CPU. We won\u2019t explore that here, but you should know that operations between tensors can only happen for tensors installed on the same device. x . layout torch.strided PyTorch has a class to hold the memory layout option. The default setting of strided will suit our purposes throughout the course. Great job! \u00b6","title":"00 Tensor Basics"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#pytorch-basics","text":"by Jawad Haider","title":"PyTorch Basics"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#00-tensor-basics","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Tensor Basics 1.1 Perform standard imports 1.2 Converting NumPy arrays to PyTorch tensors 1.3 Copying vs. sharing 1.4 Class constructors 1.5 Creating tensors from scratch 1.5.1 Uninitialized tensors with .empty() 1.5.2 Initialized tensors with .zeros() and .ones() 1.5.3 Tensors from ranges 1.5.4 Tensors from data 1.5.5 Changing the dtype of existing tensors 1.5.6 Random number tensors 1.5.7 Random number tensors that follow the input size 1.5.8 Setting the random seed 1.6 Tensor attributes 1.6.1 Great job!","title":"00 - Tensor Basics"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#tensor-basics","text":"This section covers: * Converting NumPy arrays to PyTorch tensors * Creating tensors from scratch","title":"Tensor Basics"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#perform-standard-imports","text":"import torch import numpy as np Confirm you\u2019re using PyTorch version 1.1.0 torch . __version__ '1.1.0'","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#converting-numpy-arrays-to-pytorch-tensors","text":"A torch.Tensor is a multi-dimensional matrix containing elements of a single data type. Calculations between tensors can only happen if the tensors share the same dtype. In some cases tensors are used as a replacement for NumPy to use the power of GPUs (more on this later). arr = np . array ([ 1 , 2 , 3 , 4 , 5 ]) print ( arr ) print ( arr . dtype ) print ( type ( arr )) [1 2 3 4 5] int32 <class 'numpy.ndarray'> x = torch . from_numpy ( arr ) # Equivalent to x = torch.as_tensor(arr) print ( x ) tensor([1, 2, 3, 4, 5], dtype=torch.int32) # Print the type of data held by the tensor print ( x . dtype ) torch.int32 # Print the tensor object type print ( type ( x )) print ( x . type ()) # this is more specific! <class 'torch.Tensor'> torch.IntTensor arr2 = np . arange ( 0. , 12. ) . reshape ( 4 , 3 ) print ( arr2 ) [[ 0. 1. 2.] [ 3. 4. 5.] [ 6. 7. 8.] [ 9. 10. 11.]] x2 = torch . from_numpy ( arr2 ) print ( x2 ) print ( x2 . type ()) tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]], dtype=torch.float64) torch.DoubleTensor Here torch.DoubleTensor refers to 64-bit floating point data.","title":"Converting NumPy arrays to PyTorch tensors"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#copying-vs-sharing","text":"torch.from_numpy() torch.as_tensor() torch.tensor() There are a number of different functions available for creating tensors . When using torch.from_numpy() and torch.as_tensor() , the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy. # Using torch.from_numpy() arr = np . arange ( 0 , 5 ) t = torch . from_numpy ( arr ) print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) arr [ 2 ] = 77 print ( t ) tensor([ 0, 1, 77, 3, 4], dtype=torch.int32) # Using torch.tensor() arr = np . arange ( 0 , 5 ) t = torch . tensor ( arr ) print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32) arr [ 2 ] = 77 print ( t ) tensor([0, 1, 2, 3, 4], dtype=torch.int32)","title":"Copying vs.\u00a0sharing"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#class-constructors","text":"torch.Tensor() torch.FloatTensor() torch.LongTensor() , etc. There\u2019s a subtle difference between using the factory function torch.tensor(data) and the class constructor torch.Tensor(data) . The factory function determines the dtype from the incoming data, or from a passed-in dtype argument. The class constructor torch.Tensor() is simply an alias for torch.FloatTensor(data) . Consider the following: data = np . array ([ 1 , 2 , 3 ]) a = torch . Tensor ( data ) # Equivalent to cc = torch.FloatTensor(data) print ( a , a . type ()) tensor([1., 2., 3.]) torch.FloatTensor b = torch . tensor ( data ) print ( b , b . type ()) tensor([1, 2, 3], dtype=torch.int32) torch.IntTensor c = torch . tensor ( data , dtype = torch . long ) print ( c , c . type ()) tensor([1, 2, 3]) torch.LongTensor","title":"Class constructors"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#creating-tensors-from-scratch","text":"","title":"Creating tensors from scratch"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#uninitialized-tensors-with-empty","text":"torch.empty() returns an uninitialized tensor. Essentially a block of memory is allocated according to the size of the tensor, and any values already sitting in the block are returned. This is similar to the behavior of numpy.empty() . x = torch . empty ( 4 , 3 ) print ( x ) tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])","title":"Uninitialized tensors with .empty()"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#initialized-tensors-with-zeros-and-ones","text":"torch.zeros(size) torch.ones(size) It\u2019s a good idea to pass in the intended dtype. x = torch . zeros ( 4 , 3 , dtype = torch . int64 ) print ( x ) tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]])","title":"Initialized tensors with .zeros() and .ones()"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#tensors-from-ranges","text":"torch.arange(start,end,step) torch.linspace(start,end,steps) Note that with .arange() , end is exclusive, while with linspace() , end is inclusive. x = torch . arange ( 0 , 18 , 2 ) . reshape ( 3 , 3 ) print ( x ) tensor([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]]) x = torch . linspace ( 0 , 18 , 12 ) . reshape ( 3 , 4 ) print ( x ) tensor([[ 0.0000, 1.6364, 3.2727, 4.9091], [ 6.5455, 8.1818, 9.8182, 11.4545], [13.0909, 14.7273, 16.3636, 18.0000]])","title":"Tensors from ranges"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#tensors-from-data","text":"torch.tensor() will choose the dtype based on incoming data: x = torch . tensor ([ 1 , 2 , 3 , 4 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([1, 2, 3, 4]) torch.int64 torch.LongTensor Alternatively you can set the type by the tensor method used. For a list of tensor types visit https://pytorch.org/docs/stable/tensors.html x = torch . FloatTensor ([ 5 , 6 , 7 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([5., 6., 7.]) torch.float32 torch.FloatTensor You can also pass the dtype in as an argument. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype x = torch . tensor ([ 8 , 9 , - 3 ], dtype = torch . int ) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([ 8, 9, -3], dtype=torch.int32) torch.int32 torch.IntTensor","title":"Tensors from data"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#changing-the-dtype-of-existing-tensors","text":"Don\u2019t be tempted to use x = torch.tensor(x, dtype=torch.type) as it will raise an error about improper use of tensor cloning. Instead, use the tensor .type() method. print ( 'Old:' , x . type ()) x = x . type ( torch . int64 ) print ( 'New:' , x . type ()) Old: torch.IntTensor New: torch.LongTensor","title":"Changing the dtype of existing tensors"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#random-number-tensors","text":"torch.rand(size) returns random samples from a uniform distribution over [0, 1) torch.randn(size) returns samples from the \u201cstandard normal\u201d distribution [\u03c3 = 1] Unlike rand which is uniform, values closer to zero are more likely to appear. torch.randint(low,high,size) returns random integers from low (inclusive) to high (exclusive) x = torch . rand ( 4 , 3 ) print ( x ) tensor([[0.0211, 0.2336, 0.6775], [0.4790, 0.5132, 0.9878], [0.7552, 0.0789, 0.1860], [0.6712, 0.1564, 0.3753]]) x = torch . randn ( 4 , 3 ) print ( x ) tensor([[ 0.7164, -0.1538, -0.9980], [-1.8252, 1.1863, -0.1523], [ 1.4093, -0.0212, -1.5598], [ 0.1831, -0.6961, 1.3497]]) x = torch . randint ( 0 , 5 , ( 4 , 3 )) print ( x ) tensor([[0, 3, 0], [1, 3, 4], [1, 2, 3], [4, 4, 3]])","title":"Random number tensors"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#random-number-tensors-that-follow-the-input-size","text":"torch.rand_like(input) torch.randn_like(input) torch.randint_like(input,low,high) these return random number tensors with the same size as input x = torch . zeros ( 2 , 5 ) print ( x ) tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) x2 = torch . randn_like ( x ) print ( x2 ) tensor([[-0.5442, -0.3149, 0.0922, 1.1829, -0.7873], [ 0.3143, 0.9465, 0.4534, 0.4623, 2.2044]]) The same syntax can be used with torch.zeros_like(input) torch.ones_like(input) x3 = torch . ones_like ( x2 ) print ( x3 ) tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]])","title":"Random number tensors that follow the input size"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#setting-the-random-seed","text":"torch.manual_seed(int) is used to obtain reproducible results torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]]) torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]])","title":"Setting the random seed"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#tensor-attributes","text":"Besides dtype , we can look at other tensor attributes like shape , device and layout x . shape torch.Size([2, 3]) x . size () # equivalent to x.shape torch.Size([2, 3]) x . device device(type='cpu') PyTorch supports use of multiple devices , harnessing the power of one or more GPUs in addition to the CPU. We won\u2019t explore that here, but you should know that operations between tensors can only happen for tensors installed on the same device. x . layout torch.strided PyTorch has a class to hold the memory layout option. The default setting of strided will suit our purposes throughout the course.","title":"Tensor attributes"},{"location":"bootcampsnotes/pytorch/00-Tensor-Basics/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/","text":"PyTorch Basics \u00b6 by Jawad Haider 01 - Tensor Operations \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Tensor Operations 1.1 Perform standard imports 1.2 Indexing and slicing 1.3 Reshape tensors with .view() 1.3.1 Views reflect the most current data 1.3.2 Views can infer the correct size 1.3.3 Adopt another tensor\u2019s shape with .view_as() 1.4 Tensor Arithmetic 1.4.1 Basic Tensor Operations 1.5 Dot products 1.6 Matrix multiplication 1.6.1 Matrix multiplication with broadcasting 2 Advanced operations 2.1 L2 or Euclidian Norm 2.2 Number of elements 2.3 Great work! Tensor Operations \u00b6 This section covers: * Indexing and slicing * Reshaping tensors (tensor views) * Tensor arithmetic and basic operations * Dot products * Matrix multiplication * Additional, more advanced operations Perform standard imports \u00b6 import torch import numpy as np Indexing and slicing \u00b6 Extracting specific values from a tensor works just the same as with NumPy arrays Image source: http://www.scipy-lectures.org/ _images/numpy_indexing.png x = torch . arange ( 6 ) . reshape ( 3 , 2 ) print ( x ) tensor([[0, 1], [2, 3], [4, 5]]) # Grabbing the right hand column values x [:, 1 ] tensor([1, 3, 5]) # Grabbing the right hand column as a (3,1) slice x [:, 1 :] tensor([[1], [3], [5]]) Reshape tensors with .view() \u00b6 view() and reshape() do essentially the same thing by returning a reshaped tensor without changing the original tensor in place. There\u2019s a good discussion of the differences here . x = torch . arange ( 10 ) print ( x ) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) x . view ( 2 , 5 ) tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) x . view ( 5 , 2 ) tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) # x is unchanged x tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Views reflect the most current data \u00b6 z = x . view ( 2 , 5 ) x [ 0 ] = 234 print ( z ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) Views can infer the correct size \u00b6 By passing in -1 PyTorch will infer the correct value from the given tensor x . view ( 2 , - 1 ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) x . view ( - 1 , 5 ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) Adopt another tensor\u2019s shape with .view_as() \u00b6 view_as(input) only works with tensors that have the same number of elements. x . view_as ( z ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) Tensor Arithmetic \u00b6 Adding tensors can be performed a few different ways depending on the desired result. As a simple expression: a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a + b ) tensor([5., 7., 9.]) As arguments passed into a torch operation: print ( torch . add ( a , b )) tensor([5., 7., 9.]) With an output tensor passed in as an argument: result = torch . empty ( 3 ) torch . add ( a , b , out = result ) # equivalent to result=torch.add(a,b) print ( result ) tensor([5., 7., 9.]) Changing a tensor in-place a . add_ ( b ) # equivalent to a=torch.add(a,b) print ( a ) tensor([5., 7., 9.]) NOTE: Any operation that changes a tensor in-place is post-fixed with an underscore *. In the above example: a.add*(b) changed a . Basic Tensor Operations \u00b6 Arithmetic OPERATION FUNCTION DESCRIPTION a + b a.add(b) element wise addition a - b a.sub(b) subtraction a \\* b a.mul(b) multiplication a / b a.div(b) division a % b a.fmod(b) modulo (remainder after division) a b a.pow(b) power Monomial Operations OPERATION FUNCTION DESCRIPTION \\|a\\| torch.abs(a) absolute value 1/a torch.reciprocal(a) reciprocal $\\sqrt{a}$ torch.sqrt(a) square root log(a) torch.log(a) natural log e a torch.exp(a) exponential 12.34 ==\\> 12. torch.trunc(a) truncated integer 12.34 ==\\> 0.34 torch.frac(a) fractional component Trigonometry OPERATION FUNCTION DESCRIPTION sin(a) torch.sin(a) sine cos(a) torch.sin(a) cosine tan(a) torch.sin(a) tangent arcsin(a) torch.asin(a) arc sine arccos(a) torch.acos(a) arc cosine arctan(a) torch.atan(a) arc tangent sinh(a) torch.sinh(a) hyperbolic sine cosh(a) torch.cosh(a) hyperbolic cosine tanh(a) torch.tanh(a) hyperbolic tangent Summary Statistics OPERATION FUNCTION DESCRIPTION $\\sum a$ torch.sum(a) sum $\\bar a$ torch.mean(a) mean a max torch.max(a) maximum a min torch.min(a) minimum torch.max(a,b) returns a tensor of size a containing the element wise max between a and b NOTE: Most arithmetic operations require float values. Those that do work with integers return integer tensors. For example, torch.div(a,b) performs floor division (truncates the decimal) for integer types, and classic division for floats. Use the space below to experiment with different operations \u00b6 a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( torch . add ( a , b ) . sum ()) tensor(21.) Dot products \u00b6 A dot product is the sum of the products of the corresponding entries of two 1D tensors. If the tensors are both vectors, the dot product is given as: \\(\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d & e & f \\end{bmatrix} = ad + be + cf\\) If the tensors include a column vector, then the dot product is the sum of the result of the multiplied matrices. For example: \\(\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\\\ e \\\\ f \\end{bmatrix} = ad + be + cf\\) Dot products can be expressed as torch.dot(a,b) or a.dot(b) or b.dot(a) a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a . mul ( b )) # for reference print () print ( a . dot ( b )) tensor([ 4., 10., 18.]) tensor(32.) NOTE: There\u2019s a slight difference between torch.dot() and numpy.dot() . While torch.dot() only accepts 1D arguments and returns a dot product, numpy.dot() also accepts 2D arguments and performs matrix multiplication. We show matrix multiplication below. Matrix multiplication \u00b6 2D Matrix multiplication is possible when the number of columns in tensor A matches the number of rows in tensor B . In this case, the product of tensor A with size \\((x,y)\\) and tensor B with size \\((y,z)\\) results in a tensor of size \\((x,z)\\) $\\begin{bmatrix} a & b & c \\\\ d & e & f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m & n \\\\ p & q \\\\ r & s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) & (an+bq+cs) \\\\ (dm+ep+fr) & (dn+eq+fs) \\end{bmatrix}$ Image source: https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg Matrix multiplication can be computed using torch.mm(a,b) or a.mm(b) or a @ b a = torch . tensor ([[ 0 , 2 , 4 ],[ 1 , 3 , 5 ]], dtype = torch . float ) b = torch . tensor ([[ 6 , 7 ],[ 8 , 9 ],[ 10 , 11 ]], dtype = torch . float ) print ( 'a: ' , a . size ()) print ( 'b: ' , b . size ()) print ( 'a x b: ' , torch . mm ( a , b ) . size ()) a: torch.Size([2, 3]) b: torch.Size([3, 2]) a x b: torch.Size([2, 2]) print ( torch . mm ( a , b )) tensor([[56., 62.], [80., 89.]]) print ( a . mm ( b )) tensor([[56., 62.], [80., 89.]]) print ( a @ b ) tensor([[56., 62.], [80., 89.]]) Matrix multiplication with broadcasting \u00b6 Matrix multiplication that involves broadcasting can be computed using torch.matmul(a,b) or a.matmul(b) or a @ b t1 = torch . randn ( 2 , 3 , 4 ) t2 = torch . randn ( 4 , 5 ) print ( torch . matmul ( t1 , t2 ) . size ()) torch.Size([2, 3, 5]) However, the same operation raises a RuntimeError with torch.mm() : print ( torch . mm ( t1 , t2 ) . size ()) RuntimeError: matrices expected, got 3D, 2D tensors at ..\\aten\\src\\TH/generic/THTensorMath.cpp:956 Advanced operations \u00b6 L2 or Euclidian Norm \u00b6 See torch.norm() The Euclidian Norm gives the vector norm of \\(x\\) where \\(x=(x_1,x_2,...,x_n)\\) . It is calculated as \\({\\displaystyle \\left\\|{\\boldsymbol {x}}\\right\\|_{2}:={\\sqrt {x_{1}^{2}+\\cdots +x_{n}^{2}}}}\\) When applied to a matrix, torch.norm() returns the Frobenius norm by default. x = torch . tensor ([ 2. , 5. , 8. , 14. ]) x . norm () tensor(17.) Number of elements \u00b6 See torch.numel() Returns the number of elements in a tensor. x = torch . ones ( 3 , 7 ) x . numel () 21 This can be useful in certain calculations like Mean Squared Error: def mse(t1, t2): diff = t1 - t2 return torch.sum(diff * diff) / diff .numel() Great work! \u00b6","title":"01 Tensor Operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#pytorch-basics","text":"by Jawad Haider","title":"PyTorch Basics"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#01-tensor-operations","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 Tensor Operations 1.1 Perform standard imports 1.2 Indexing and slicing 1.3 Reshape tensors with .view() 1.3.1 Views reflect the most current data 1.3.2 Views can infer the correct size 1.3.3 Adopt another tensor\u2019s shape with .view_as() 1.4 Tensor Arithmetic 1.4.1 Basic Tensor Operations 1.5 Dot products 1.6 Matrix multiplication 1.6.1 Matrix multiplication with broadcasting 2 Advanced operations 2.1 L2 or Euclidian Norm 2.2 Number of elements 2.3 Great work!","title":"01 - Tensor Operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#tensor-operations","text":"This section covers: * Indexing and slicing * Reshaping tensors (tensor views) * Tensor arithmetic and basic operations * Dot products * Matrix multiplication * Additional, more advanced operations","title":"Tensor Operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#perform-standard-imports","text":"import torch import numpy as np","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#indexing-and-slicing","text":"Extracting specific values from a tensor works just the same as with NumPy arrays Image source: http://www.scipy-lectures.org/ _images/numpy_indexing.png x = torch . arange ( 6 ) . reshape ( 3 , 2 ) print ( x ) tensor([[0, 1], [2, 3], [4, 5]]) # Grabbing the right hand column values x [:, 1 ] tensor([1, 3, 5]) # Grabbing the right hand column as a (3,1) slice x [:, 1 :] tensor([[1], [3], [5]])","title":"Indexing and slicing"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#reshape-tensors-with-view","text":"view() and reshape() do essentially the same thing by returning a reshaped tensor without changing the original tensor in place. There\u2019s a good discussion of the differences here . x = torch . arange ( 10 ) print ( x ) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) x . view ( 2 , 5 ) tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) x . view ( 5 , 2 ) tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) # x is unchanged x tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])","title":"Reshape tensors with .view()"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#views-reflect-the-most-current-data","text":"z = x . view ( 2 , 5 ) x [ 0 ] = 234 print ( z ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]])","title":"Views reflect the most current data"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#views-can-infer-the-correct-size","text":"By passing in -1 PyTorch will infer the correct value from the given tensor x . view ( 2 , - 1 ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]]) x . view ( - 1 , 5 ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]])","title":"Views can infer the correct size"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#adopt-another-tensors-shape-with-view_as","text":"view_as(input) only works with tensors that have the same number of elements. x . view_as ( z ) tensor([[234, 1, 2, 3, 4], [ 5, 6, 7, 8, 9]])","title":"Adopt another tensor\u2019s shape with .view_as()"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#tensor-arithmetic","text":"Adding tensors can be performed a few different ways depending on the desired result. As a simple expression: a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a + b ) tensor([5., 7., 9.]) As arguments passed into a torch operation: print ( torch . add ( a , b )) tensor([5., 7., 9.]) With an output tensor passed in as an argument: result = torch . empty ( 3 ) torch . add ( a , b , out = result ) # equivalent to result=torch.add(a,b) print ( result ) tensor([5., 7., 9.]) Changing a tensor in-place a . add_ ( b ) # equivalent to a=torch.add(a,b) print ( a ) tensor([5., 7., 9.]) NOTE: Any operation that changes a tensor in-place is post-fixed with an underscore *. In the above example: a.add*(b) changed a .","title":"Tensor Arithmetic"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#basic-tensor-operations","text":"Arithmetic OPERATION FUNCTION DESCRIPTION a + b a.add(b) element wise addition a - b a.sub(b) subtraction a \\* b a.mul(b) multiplication a / b a.div(b) division a % b a.fmod(b) modulo (remainder after division) a b a.pow(b) power Monomial Operations OPERATION FUNCTION DESCRIPTION \\|a\\| torch.abs(a) absolute value 1/a torch.reciprocal(a) reciprocal $\\sqrt{a}$ torch.sqrt(a) square root log(a) torch.log(a) natural log e a torch.exp(a) exponential 12.34 ==\\> 12. torch.trunc(a) truncated integer 12.34 ==\\> 0.34 torch.frac(a) fractional component Trigonometry OPERATION FUNCTION DESCRIPTION sin(a) torch.sin(a) sine cos(a) torch.sin(a) cosine tan(a) torch.sin(a) tangent arcsin(a) torch.asin(a) arc sine arccos(a) torch.acos(a) arc cosine arctan(a) torch.atan(a) arc tangent sinh(a) torch.sinh(a) hyperbolic sine cosh(a) torch.cosh(a) hyperbolic cosine tanh(a) torch.tanh(a) hyperbolic tangent Summary Statistics OPERATION FUNCTION DESCRIPTION $\\sum a$ torch.sum(a) sum $\\bar a$ torch.mean(a) mean a max torch.max(a) maximum a min torch.min(a) minimum torch.max(a,b) returns a tensor of size a containing the element wise max between a and b NOTE: Most arithmetic operations require float values. Those that do work with integers return integer tensors. For example, torch.div(a,b) performs floor division (truncates the decimal) for integer types, and classic division for floats.","title":"Basic Tensor Operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#use-the-space-below-to-experiment-with-different-operations","text":"a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( torch . add ( a , b ) . sum ()) tensor(21.)","title":"Use the space below to experiment with different operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#dot-products","text":"A dot product is the sum of the products of the corresponding entries of two 1D tensors. If the tensors are both vectors, the dot product is given as: \\(\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d & e & f \\end{bmatrix} = ad + be + cf\\) If the tensors include a column vector, then the dot product is the sum of the result of the multiplied matrices. For example: \\(\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\\\ e \\\\ f \\end{bmatrix} = ad + be + cf\\) Dot products can be expressed as torch.dot(a,b) or a.dot(b) or b.dot(a) a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a . mul ( b )) # for reference print () print ( a . dot ( b )) tensor([ 4., 10., 18.]) tensor(32.) NOTE: There\u2019s a slight difference between torch.dot() and numpy.dot() . While torch.dot() only accepts 1D arguments and returns a dot product, numpy.dot() also accepts 2D arguments and performs matrix multiplication. We show matrix multiplication below.","title":"Dot products"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#matrix-multiplication","text":"2D Matrix multiplication is possible when the number of columns in tensor A matches the number of rows in tensor B . In this case, the product of tensor A with size \\((x,y)\\) and tensor B with size \\((y,z)\\) results in a tensor of size \\((x,z)\\) $\\begin{bmatrix} a & b & c \\\\ d & e & f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m & n \\\\ p & q \\\\ r & s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) & (an+bq+cs) \\\\ (dm+ep+fr) & (dn+eq+fs) \\end{bmatrix}$ Image source: https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg Matrix multiplication can be computed using torch.mm(a,b) or a.mm(b) or a @ b a = torch . tensor ([[ 0 , 2 , 4 ],[ 1 , 3 , 5 ]], dtype = torch . float ) b = torch . tensor ([[ 6 , 7 ],[ 8 , 9 ],[ 10 , 11 ]], dtype = torch . float ) print ( 'a: ' , a . size ()) print ( 'b: ' , b . size ()) print ( 'a x b: ' , torch . mm ( a , b ) . size ()) a: torch.Size([2, 3]) b: torch.Size([3, 2]) a x b: torch.Size([2, 2]) print ( torch . mm ( a , b )) tensor([[56., 62.], [80., 89.]]) print ( a . mm ( b )) tensor([[56., 62.], [80., 89.]]) print ( a @ b ) tensor([[56., 62.], [80., 89.]])","title":"Matrix multiplication"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#matrix-multiplication-with-broadcasting","text":"Matrix multiplication that involves broadcasting can be computed using torch.matmul(a,b) or a.matmul(b) or a @ b t1 = torch . randn ( 2 , 3 , 4 ) t2 = torch . randn ( 4 , 5 ) print ( torch . matmul ( t1 , t2 ) . size ()) torch.Size([2, 3, 5]) However, the same operation raises a RuntimeError with torch.mm() : print ( torch . mm ( t1 , t2 ) . size ()) RuntimeError: matrices expected, got 3D, 2D tensors at ..\\aten\\src\\TH/generic/THTensorMath.cpp:956","title":"Matrix multiplication with broadcasting"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#advanced-operations","text":"","title":"Advanced operations"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#l2-or-euclidian-norm","text":"See torch.norm() The Euclidian Norm gives the vector norm of \\(x\\) where \\(x=(x_1,x_2,...,x_n)\\) . It is calculated as \\({\\displaystyle \\left\\|{\\boldsymbol {x}}\\right\\|_{2}:={\\sqrt {x_{1}^{2}+\\cdots +x_{n}^{2}}}}\\) When applied to a matrix, torch.norm() returns the Frobenius norm by default. x = torch . tensor ([ 2. , 5. , 8. , 14. ]) x . norm () tensor(17.)","title":"L2 or Euclidian Norm"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#number-of-elements","text":"See torch.numel() Returns the number of elements in a tensor. x = torch . ones ( 3 , 7 ) x . numel () 21 This can be useful in certain calculations like Mean Squared Error: def mse(t1, t2): diff = t1 - t2 return torch.sum(diff * diff) / diff .numel()","title":"Number of elements"},{"location":"bootcampsnotes/pytorch/01-Tensor-Operations/#great-work","text":"","title":"Great work!"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/","text":"PyTorch Basics \u00b6 by Jawad Haider 02 - PyTorch Basics Exercises \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 PyTorch Basics Exercises 1.0.1 1. Perform standard imports 1.0.2 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d 1.0.3 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) 1.0.4 4. Create a tensor \u201cx\u201d from the array above 1.0.5 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 1.0.6 6. Reshape x into a 3x2 tensor 1.0.7 7. Return the right-hand column of tensor x 1.0.8 8. Without changing x, return a tensor of square values of x 1.0.9 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x 1.0.10 10. Find the matrix product of x and y 1.1 Great job! PyTorch Basics Exercises \u00b6 For these exercises we\u2019ll create a tensor and perform several operations on it. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! 1. Perform standard imports \u00b6 Import torch and NumPy # CODE HERE 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d \u00b6 This allows us to share the same \u201crandom\u201d results. # CODE HERE 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) \u00b6 # CODE HERE # DON'T WRITE HERE [3 4 2 4 4 1] 4. Create a tensor \u201cx\u201d from the array above \u00b6 # CODE HERE # DON'T WRITE HERE tensor([3, 4, 2, 4, 4, 1], dtype=torch.int32) 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 \u00b6 Note: \u2018int64\u2019 is also called \u2018LongTensor\u2019 # CODE HERE # DON'T WRITE HERE torch.LongTensor 6. Reshape x into a 3x2 tensor \u00b6 There are several ways to do this. # CODE HERE # DON'T WRITE HERE tensor([[3, 4], [2, 4], [4, 1]]) 7. Return the right-hand column of tensor x \u00b6 # CODE HERE # DON'T WRITE HERE tensor([[4], [4], [1]]) 8. Without changing x, return a tensor of square values of x \u00b6 There are several ways to do this. # CODE HERE # DON'T WRITE HERE tensor([[ 9, 16], [ 4, 16], [16, 1]]) 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x \u00b6 Use PyTorch directly (not NumPy) to create a tensor of random integers between 0 (inclusive) and 5 (exclusive). Think about what shape it should have to permit matrix multiplication. # CODE HERE # DON'T WRITE HERE tensor([[2, 2, 1], [4, 1, 0]]) 10. Find the matrix product of x and y \u00b6 # CODE HERE # DON'T WRITE HERE tensor([[22, 10, 3], [20, 8, 2], [12, 9, 4]]) Great job! \u00b6","title":"02 PyTorch Basics Exercises"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#pytorch-basics","text":"by Jawad Haider","title":"PyTorch Basics"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#02-pytorch-basics-exercises","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 PyTorch Basics Exercises 1.0.1 1. Perform standard imports 1.0.2 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d 1.0.3 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) 1.0.4 4. Create a tensor \u201cx\u201d from the array above 1.0.5 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 1.0.6 6. Reshape x into a 3x2 tensor 1.0.7 7. Return the right-hand column of tensor x 1.0.8 8. Without changing x, return a tensor of square values of x 1.0.9 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x 1.0.10 10. Find the matrix product of x and y 1.1 Great job!","title":"02 - PyTorch Basics Exercises"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#pytorch-basics-exercises","text":"For these exercises we\u2019ll create a tensor and perform several operations on it. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"PyTorch Basics Exercises"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#1-perform-standard-imports","text":"Import torch and NumPy # CODE HERE","title":"1. Perform standard imports"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#2-set-the-random-seed-for-numpy-and-pytorch-both-to-42","text":"This allows us to share the same \u201crandom\u201d results. # CODE HERE","title":"2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#3-create-a-numpy-array-called-arr-that-contains-6-random-integers-between-0-inclusive-and-5-exclusive","text":"# CODE HERE # DON'T WRITE HERE [3 4 2 4 4 1]","title":"3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive)"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#4-create-a-tensor-x-from-the-array-above","text":"# CODE HERE # DON'T WRITE HERE tensor([3, 4, 2, 4, 4, 1], dtype=torch.int32)","title":"4. Create a tensor \u201cx\u201d from the array above"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#5-change-the-dtype-of-x-from-int32-to-int64","text":"Note: \u2018int64\u2019 is also called \u2018LongTensor\u2019 # CODE HERE # DON'T WRITE HERE torch.LongTensor","title":"5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#6-reshape-x-into-a-3x2-tensor","text":"There are several ways to do this. # CODE HERE # DON'T WRITE HERE tensor([[3, 4], [2, 4], [4, 1]])","title":"6. Reshape x into a 3x2 tensor"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#7-return-the-right-hand-column-of-tensor-x","text":"# CODE HERE # DON'T WRITE HERE tensor([[4], [4], [1]])","title":"7. Return the right-hand column of tensor x"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#8-without-changing-x-return-a-tensor-of-square-values-of-x","text":"There are several ways to do this. # CODE HERE # DON'T WRITE HERE tensor([[ 9, 16], [ 4, 16], [16, 1]])","title":"8. Without changing x, return a tensor of square values of x"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#9-create-a-tensor-y-with-the-same-number-of-elements-as-x-that-can-be-matrix-multiplied-with-x","text":"Use PyTorch directly (not NumPy) to create a tensor of random integers between 0 (inclusive) and 5 (exclusive). Think about what shape it should have to permit matrix multiplication. # CODE HERE # DON'T WRITE HERE tensor([[2, 2, 1], [4, 1, 0]])","title":"9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#10-find-the-matrix-product-of-x-and-y","text":"# CODE HERE # DON'T WRITE HERE tensor([[22, 10, 3], [20, 8, 2], [12, 9, 4]])","title":"10. Find the matrix product of x and y"},{"location":"bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/","text":"PyTorch Basics \u00b6 by Jawad Haider 02 - PyTorch Basics Exercises SOLUTION \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 PyTorch Basics Exercises - SOLUTIONS 1.0.1 1. Perform standard imports 1.0.2 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d 1.0.3 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) 1.0.4 4. Create a tensor \u201cx\u201d from the array above 1.0.5 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 1.0.6 6. Reshape x into a 3x2 tensor 1.0.7 7. Return the right-hand column of tensor x 1.0.8 8. Without changing x, return a tensor of square values of x 1.0.9 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x 1.0.10 10. Find the matrix product of x and y 1.1 Great job! PyTorch Basics Exercises - SOLUTIONS \u00b6 For these exercises we\u2019ll create a tensor and perform several operations on it. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! 1. Perform standard imports \u00b6 Import torch and NumPy # CODE HERE import torch import numpy as np 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d \u00b6 This allows us to share the same \u201crandom\u201d results. # CODE HERE np . random . seed ( 42 ) torch . manual_seed ( 42 ); # the semicolon suppresses the jupyter output line 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) \u00b6 # CODE HERE # DON'T WRITE HERE arr = np . random . randint ( 0 , 5 , 6 ) print ( arr ) [3 4 2 4 4 1] 4. Create a tensor \u201cx\u201d from the array above \u00b6 # CODE HERE # DON'T WRITE HERE x = torch . from_numpy ( arr ) print ( x ) tensor([3, 4, 2, 4, 4, 1], dtype=torch.int32) 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 \u00b6 Note: \u2018int64\u2019 is also called \u2018LongTensor\u2019 # CODE HERE # DON'T WRITE HERE x = x . type ( torch . int64 ) # x = x.type(torch.LongTensor) print ( x . type ()) torch.LongTensor 6. Reshape x into a 3x2 tensor \u00b6 There are several ways to do this. # CODE HERE # DON'T WRITE HERE x = x . view ( 3 , 2 ) # x = x.reshape(3,2) # x.resize_(3,2) print ( x ) tensor([[3, 4], [2, 4], [4, 1]]) 7. Return the right-hand column of tensor x \u00b6 # CODE HERE # DON'T WRITE HERE print ( x [:, 1 :]) # print(x[:,1]) tensor([[4], [4], [1]]) 8. Without changing x, return a tensor of square values of x \u00b6 There are several ways to do this. # CODE HERE # DON'T WRITE HERE print ( x * x ) # print(x**2) # print(x.mul(x)) # print(x.pow(2)) # print(torch.mul(x,x)) tensor([[ 9, 16], [ 4, 16], [16, 1]]) 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x \u00b6 Use PyTorch directly (not NumPy) to create a tensor of random integers between 0 (inclusive) and 5 (exclusive). Think about what shape it should have to permit matrix multiplication. # CODE HERE # DON'T WRITE HERE y = torch . randint ( 0 , 5 ,( 2 , 3 )) print ( y ) tensor([[2, 2, 1], [4, 1, 0]]) 10. Find the matrix product of x and y \u00b6 # CODE HERE # DON'T WRITE HERE print ( x . mm ( y )) tensor([[22, 10, 3], [20, 8, 2], [12, 9, 4]]) Great job! \u00b6","title":"03 PyTorch Basics Exercises Solutions"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#pytorch-basics","text":"by Jawad Haider","title":"PyTorch Basics"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#02-pytorch-basics-exercises-solution","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ 1 PyTorch Basics Exercises - SOLUTIONS 1.0.1 1. Perform standard imports 1.0.2 2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d 1.0.3 3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive) 1.0.4 4. Create a tensor \u201cx\u201d from the array above 1.0.5 5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019 1.0.6 6. Reshape x into a 3x2 tensor 1.0.7 7. Return the right-hand column of tensor x 1.0.8 8. Without changing x, return a tensor of square values of x 1.0.9 9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x 1.0.10 10. Find the matrix product of x and y 1.1 Great job!","title":"02 - PyTorch Basics Exercises SOLUTION"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#pytorch-basics-exercises-solutions","text":"For these exercises we\u2019ll create a tensor and perform several operations on it. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"PyTorch Basics Exercises - SOLUTIONS"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#1-perform-standard-imports","text":"Import torch and NumPy # CODE HERE import torch import numpy as np","title":"1. Perform standard imports"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#2-set-the-random-seed-for-numpy-and-pytorch-both-to-42","text":"This allows us to share the same \u201crandom\u201d results. # CODE HERE np . random . seed ( 42 ) torch . manual_seed ( 42 ); # the semicolon suppresses the jupyter output line","title":"2. Set the random seed for NumPy and PyTorch both to \u201c42\u201d"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#3-create-a-numpy-array-called-arr-that-contains-6-random-integers-between-0-inclusive-and-5-exclusive","text":"# CODE HERE # DON'T WRITE HERE arr = np . random . randint ( 0 , 5 , 6 ) print ( arr ) [3 4 2 4 4 1]","title":"3. Create a NumPy array called \u201carr\u201d that contains 6 random integers between 0 (inclusive) and 5 (exclusive)"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#4-create-a-tensor-x-from-the-array-above","text":"# CODE HERE # DON'T WRITE HERE x = torch . from_numpy ( arr ) print ( x ) tensor([3, 4, 2, 4, 4, 1], dtype=torch.int32)","title":"4. Create a tensor \u201cx\u201d from the array above"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#5-change-the-dtype-of-x-from-int32-to-int64","text":"Note: \u2018int64\u2019 is also called \u2018LongTensor\u2019 # CODE HERE # DON'T WRITE HERE x = x . type ( torch . int64 ) # x = x.type(torch.LongTensor) print ( x . type ()) torch.LongTensor","title":"5. Change the dtype of x from \u2018int32\u2019 to \u2018int64\u2019"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#6-reshape-x-into-a-3x2-tensor","text":"There are several ways to do this. # CODE HERE # DON'T WRITE HERE x = x . view ( 3 , 2 ) # x = x.reshape(3,2) # x.resize_(3,2) print ( x ) tensor([[3, 4], [2, 4], [4, 1]])","title":"6. Reshape x into a 3x2 tensor"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#7-return-the-right-hand-column-of-tensor-x","text":"# CODE HERE # DON'T WRITE HERE print ( x [:, 1 :]) # print(x[:,1]) tensor([[4], [4], [1]])","title":"7. Return the right-hand column of tensor x"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#8-without-changing-x-return-a-tensor-of-square-values-of-x","text":"There are several ways to do this. # CODE HERE # DON'T WRITE HERE print ( x * x ) # print(x**2) # print(x.mul(x)) # print(x.pow(2)) # print(torch.mul(x,x)) tensor([[ 9, 16], [ 4, 16], [16, 1]])","title":"8. Without changing x, return a tensor of square values of x"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#9-create-a-tensor-y-with-the-same-number-of-elements-as-x-that-can-be-matrix-multiplied-with-x","text":"Use PyTorch directly (not NumPy) to create a tensor of random integers between 0 (inclusive) and 5 (exclusive). Think about what shape it should have to permit matrix multiplication. # CODE HERE # DON'T WRITE HERE y = torch . randint ( 0 , 5 ,( 2 , 3 )) print ( y ) tensor([[2, 2, 1], [4, 1, 0]])","title":"9. Create a tensor \u201cy\u201d with the same number of elements as x, that can be matrix-multiplied with x"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#10-find-the-matrix-product-of-x-and-y","text":"# CODE HERE # DON'T WRITE HERE print ( x . mm ( y )) tensor([[22, 10, 3], [20, 8, 2], [12, 9, 4]])","title":"10. Find the matrix product of x and y"},{"location":"bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/","text":"Pytorch for Deep Learning BootCamp \u00b6 by Jawad Haider 00 - PyTorch Gradients \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ PyTorch Gradients Autograd - Automatic Differentiation Back-propagation on one step Back-propagation on multiple steps Turn off tracking PyTorch Gradients \u00b6 This section covers the PyTorch autograd implementation of gradient descent. Tools include: * torch.autograd.backward() * torch.autograd.grad() Before continuing in this section, be sure to watch the theory lectures to understand the following concepts: * Error functions (step andsigmoid) * One-hot encoding * Maximum likelihood * Cross entropy(including multi-class cross entropy) * Back propagation (backprop) Additional Resources: PyTorch Notes: Autograd mechanics Autograd - Automatic Differentiation \u00b6 In previous sections we created tensors and performed a variety of operations on them, but we did nothing to store the sequence of operations, or to apply the derivative of a completed function. In this section we\u2019ll introduce the concept of the dynamic computational graph which is comprised of all the Tensor objects in the network, as well as the Functions used to create them. Note that only the input Tensors we create ourselves will not have associated Function objects. The PyTorch autograd package provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensors themselves. When a Tensor\u2019s .requires_grad attribute is set to True, it starts to track all operations on it. When an operation finishes you can call .backward() and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its .grad attribute. Let\u2019s see this in practice. Back-propagation on one step \u00b6 We\u2019ll start by applying a single polynomial function to tensor . Then we\u2019ll backprop and print the gradient . Step 1. Perform standard imports \u00b6 import torch Step 2. Create a tensor with requires_grad set to True \u00b6 This sets up computational tracking on the tensor. x = torch . tensor ( 2.0 , requires_grad = True ) Step 3. Define a function \u00b6 y = 2 * x ** 4 + x ** 3 + 3 * x ** 2 + 5 * x + 1 print ( y ) tensor(63., grad_fn=<AddBackward0>) Since was created as a result of an operation, it has an associated gradient function accessible as y.grad_fn The calculation of is done as: This is the value of when . Step 4. Backprop \u00b6 y . backward () Step 5. Display the resulting gradient \u00b6 print ( x . grad ) tensor(93.) Note that x.grad is an attribute of tensor , so we don\u2019t use parentheses. The computation is the result of This is the slope of the polynomial at the point . Back-propagation on multiple steps \u00b6 Now let\u2019s do something more complex, involving layers and between and our output layer . 1. Create \u00b6 a tensor x = torch . tensor ([[ 1. , 2 , 3 ],[ 3 , 2 , 1 ]], requires_grad = True ) print ( x ) tensor([[1., 2., 3.], [3., 2., 1.]], requires_grad=True) 2. Create the first layer with \u00b6 y = 3 * x + 2 print ( y ) tensor([[ 5., 8., 11.], [11., 8., 5.]], grad_fn=<AddBackward0>) 3. Create the second layer with \u00b6 z = 2 * y ** 2 print ( z ) tensor([[ 50., 128., 242.], [242., 128., 50.]], grad_fn=<MulBackward0>) 4. Set the output to be the matrix mean \u00b6 out = z . mean () print ( out ) tensor(140., grad_fn=<MeanBackward1>) 5. Now perform back-propagation to find the gradient of x w.r.t out \u00b6 (If you haven\u2019t seen it before, w.r.t. is an abbreviation of with respect to ) out . backward () print ( x . grad ) tensor([[10., 16., 22.], [22., 16., 10.]]) You should see a 2x3 matrix. If we call the final out tensor \u201c \u201d, we can calculate the partial derivative of with respect to as follows: To solve the derivative of we use the chain rule , where the derivative of In this case Therefore, Turn off tracking \u00b6 There may be times when we don\u2019t want or need to track the computational history. You can reset a tensor\u2019s requires_grad attribute in-place using .requires_grad_(True) (or False) as needed. When performing evaluations, it\u2019s often helpful to wrap a set of operations in with torch.no_grad(): A less-used method is to run .detach() on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor. A NOTE ABOUT TENSORS AND VARIABLES: Prior to PyTorch v0.4.0 (April 2018) Tensors ( torch.Tensor ) only held data, and tracking history was reserved for the Variable wrapper ( torch.autograd.Variable ). Since v0.4.0 tensors and variables have merged, and tracking functionality is now available through the requires_grad=True flag.","title":"00 PyTorch Gradients"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#pytorch-for-deep-learning-bootcamp","text":"by Jawad Haider","title":"Pytorch for Deep Learning BootCamp"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#00-pytorch-gradients","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ PyTorch Gradients Autograd - Automatic Differentiation Back-propagation on one step Back-propagation on multiple steps Turn off tracking","title":"00 - PyTorch Gradients"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#pytorch-gradients","text":"This section covers the PyTorch autograd implementation of gradient descent. Tools include: * torch.autograd.backward() * torch.autograd.grad() Before continuing in this section, be sure to watch the theory lectures to understand the following concepts: * Error functions (step andsigmoid) * One-hot encoding * Maximum likelihood * Cross entropy(including multi-class cross entropy) * Back propagation (backprop)","title":"PyTorch Gradients"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#autograd-automatic-differentiation","text":"In previous sections we created tensors and performed a variety of operations on them, but we did nothing to store the sequence of operations, or to apply the derivative of a completed function. In this section we\u2019ll introduce the concept of the dynamic computational graph which is comprised of all the Tensor objects in the network, as well as the Functions used to create them. Note that only the input Tensors we create ourselves will not have associated Function objects. The PyTorch autograd package provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensors themselves. When a Tensor\u2019s .requires_grad attribute is set to True, it starts to track all operations on it. When an operation finishes you can call .backward() and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its .grad attribute. Let\u2019s see this in practice.","title":"Autograd - Automatic Differentiation"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#back-propagation-on-one-step","text":"We\u2019ll start by applying a single polynomial function to tensor . Then we\u2019ll backprop and print the gradient .","title":"Back-propagation on one step"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#step-1-perform-standard-imports","text":"import torch","title":"Step 1. Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#step-2-create-a-tensor-with-requires_grad-set-to-true","text":"This sets up computational tracking on the tensor. x = torch . tensor ( 2.0 , requires_grad = True )","title":"Step 2. Create a tensor with requires_grad set to True"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#step-3-define-a-function","text":"y = 2 * x ** 4 + x ** 3 + 3 * x ** 2 + 5 * x + 1 print ( y ) tensor(63., grad_fn=<AddBackward0>) Since was created as a result of an operation, it has an associated gradient function accessible as y.grad_fn The calculation of is done as: This is the value of when .","title":"Step 3. Define a function"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#step-4-backprop","text":"y . backward ()","title":"Step 4. Backprop"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#step-5-display-the-resulting-gradient","text":"print ( x . grad ) tensor(93.) Note that x.grad is an attribute of tensor , so we don\u2019t use parentheses. The computation is the result of This is the slope of the polynomial at the point .","title":"Step 5. Display the resulting gradient"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#back-propagation-on-multiple-steps","text":"Now let\u2019s do something more complex, involving layers and between and our output layer .","title":"Back-propagation on multiple steps"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#1-create","text":"a tensor x = torch . tensor ([[ 1. , 2 , 3 ],[ 3 , 2 , 1 ]], requires_grad = True ) print ( x ) tensor([[1., 2., 3.], [3., 2., 1.]], requires_grad=True)","title":"1. Create"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#2-create-the-first-layer-with","text":"y = 3 * x + 2 print ( y ) tensor([[ 5., 8., 11.], [11., 8., 5.]], grad_fn=<AddBackward0>)","title":"2. Create the first layer with"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#3-create-the-second-layer-with","text":"z = 2 * y ** 2 print ( z ) tensor([[ 50., 128., 242.], [242., 128., 50.]], grad_fn=<MulBackward0>)","title":"3. Create the second layer with"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#4-set-the-output-to-be-the-matrix-mean","text":"out = z . mean () print ( out ) tensor(140., grad_fn=<MeanBackward1>)","title":"4. Set the output to be the matrix mean"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#5-now-perform-back-propagation-to-find-the-gradient-of-x-wrt-out","text":"(If you haven\u2019t seen it before, w.r.t. is an abbreviation of with respect to ) out . backward () print ( x . grad ) tensor([[10., 16., 22.], [22., 16., 10.]]) You should see a 2x3 matrix. If we call the final out tensor \u201c \u201d, we can calculate the partial derivative of with respect to as follows: To solve the derivative of we use the chain rule , where the derivative of In this case Therefore,","title":"5. Now perform back-propagation to find the gradient of x w.r.t out"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/00-PyTorch-Gradients/#turn-off-tracking","text":"There may be times when we don\u2019t want or need to track the computational history. You can reset a tensor\u2019s requires_grad attribute in-place using .requires_grad_(True) (or False) as needed. When performing evaluations, it\u2019s often helpful to wrap a set of operations in with torch.no_grad(): A less-used method is to run .detach() on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor. A NOTE ABOUT TENSORS AND VARIABLES: Prior to PyTorch v0.4.0 (April 2018) Tensors ( torch.Tensor ) only held data, and tracking history was reserved for the Variable wrapper ( torch.autograd.Variable ). Since v0.4.0 tensors and variables have merged, and tracking functionality is now available through the requires_grad=True flag.","title":"Turn off tracking"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/","text":"Pytorch for Deep Learning BootCamp \u00b6 by Jawad Haider 01 - Linear Regression with PyTorch \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Linear Regression with PyTorch Perform standard imports Create a column matrix of X values Create a \u201crandom\u201d array of error values Create a column matrix of y values Plot the results Simple linear model Model classes Plot the initial model Set the loss function Set the optimization Train the model Plot the loss values Plot the result Great job! Linear Regression with PyTorch \u00b6 In this section we\u2019ll use PyTorch\u2019s machine learning model to progressively develop a best-fit line for a given set of data points. Like most linear regression algorithms, we\u2019re seeking to minimize the error between our model and the actual data, using a loss function like mean-squared-error. Image source: https://commons.wikimedia.org/wiki/File:Residuals_for_Linear_Regression_Fit.png To start, we\u2019ll develop a collection of data points that appear random, but that fit a known linear equation Perform standard imports \u00b6 import torch import torch.nn as nn # we'll use this a lot going forward! import numpy as np import matplotlib.pyplot as plt % matplotlib inline Create a column matrix of X values \u00b6 We can create tensors right away rather than convert from NumPy arrays. X = torch . linspace ( 1 , 50 , 50 ) . reshape ( - 1 , 1 ) # Equivalent to # X = torch.unsqueeze(torch.linspace(1,50,50), dim=1) Create a \u201crandom\u201d array of error values \u00b6 We want 50 random integer values that collectively cancel each other out. torch . manual_seed ( 71 ) # to obtain reproducible results e = torch . randint ( - 8 , 9 ,( 50 , 1 ), dtype = torch . float ) print ( e . sum ()) tensor(0.) Create a column matrix of y values \u00b6 Here we\u2019ll set our own parameters of , plus the error amount. y will have the same shape as X and e y = 2 * X + 1 + e print ( y . shape ) torch.Size([50, 1]) Plot the results \u00b6 We have to convert tensors to NumPy arrays just for plotting. plt . scatter ( X . numpy (), y . numpy ()) plt . ylabel ( 'y' ) plt . xlabel ( 'x' ); Note that when we created tensor , we did not pass requires_grad=True . This means that doesn\u2019t have a gradient function, and y.backward() won\u2019t work. Since PyTorch is not tracking operations, it doesn\u2019t know the relationship between and . Simple linear model \u00b6 As a quick demonstration we\u2019ll show how the built-in nn.Linear() model preselects weight and bias values at random. torch . manual_seed ( 59 ) model = nn . Linear ( in_features = 1 , out_features = 1 ) print ( model . weight ) print ( model . bias ) Parameter containing: tensor([[0.1060]], requires_grad=True) Parameter containing: tensor([0.9638], requires_grad=True) Without seeing any data, the model sets a random weight of 0.1060 and a bias of 0.9638. Model classes \u00b6 PyTorch lets us define models as object classes that can store multiple model layers. In upcoming sections we\u2019ll set up several neural network layers, and determine how each layer should perform its forward pass to the next layer. For now, though, we only need a single linear layer. class Model ( nn . Module ): def __init__ ( self , in_features , out_features ): super () . __init__ () self . linear = nn . Linear ( in_features , out_features ) def forward ( self , x ): y_pred = self . linear ( x ) return y_pred NOTE: The \u201cLinear\u201d model layer used here doesn\u2019t really refer to linear regression. Instead, it describes the type of neural network layer employed. Linear layers are also called \u201cfully connected\u201d or \u201cdense\u201d layers. Going forward our models may contain linear layers, convolutional layers, and more. When Model is instantiated, we need to pass in the size (dimensions) of the incoming and outgoing features. For our purposes we\u2019ll use (1,1). As above, we can see the initial hyperparameters. torch . manual_seed ( 59 ) model = Model ( 1 , 1 ) print ( model ) print ( 'Weight:' , model . linear . weight . item ()) print ( 'Bias: ' , model . linear . bias . item ()) Model( (linear): Linear(in_features=1, out_features=1, bias=True) ) Weight: 0.10597813129425049 Bias: 0.9637961387634277 As models become more complex, it may be better to iterate over all the model parameters: for name , param in model . named_parameters (): print ( name , ' \\t ' , param . item ()) linear.weight 0.10597813129425049 linear.bias 0.9637961387634277 NOTE: In the above example we had our Model class accept arguments for the number of input and output features. For simplicity we can hardcode them into the Model: class Model(torch.nn.Module): def \\_\\_init\\_\\_(self): super().\\_\\_init\\_\\_() self.linear = Linear(1,1) model = Model() Alternatively we can use default arguments: class Model(torch.nn.Module): def \\_\\_init\\_\\_(self, in_dim=1, out_dim=1): super().\\_\\_init\\_\\_() self.linear = Linear(in_dim,out_dim) model = Model() \\# or model = Model(i,o) Now let\u2019s see the result when we pass a tensor into the model. x = torch . tensor ([ 2.0 ]) print ( model . forward ( x )) # equivalent to print(model(x)) tensor([1.1758], grad_fn=<AddBackward0>) which is confirmed with Plot the initial model \u00b6 We can plot the untrained model against our dataset to get an idea of our starting point. x1 = np . array ([ X . min (), X . max ()]) print ( x1 ) [ 1. 50.] w1 , b1 = model . linear . weight . item (), model . linear . bias . item () print ( f 'Initial weight: { w1 : .8f } , Initial bias: { b1 : .8f } ' ) print () y1 = x1 * w1 + b1 print ( y1 ) Initial weight: 0.10597813, Initial bias: 0.96379614 [1.0697743 6.2627025] plt . scatter ( X . numpy (), y . numpy ()) plt . plot ( x1 , y1 , 'r' ) plt . title ( 'Initial Model' ) plt . ylabel ( 'y' ) plt . xlabel ( 'x' ); Set the loss function \u00b6 We could write our own function to apply a Mean Squared Error (MSE) that follows Fortunately PyTorch has it built in. By convention, you\u2019ll see the variable name \u201ccriterion\u201d used, but feel free to use something like \u201clinear_loss_func\u201d if that\u2019s clearer. criterion = nn . MSELoss () Set the optimization \u00b6 Here we\u2019ll use Stochastic Gradient Descent (SGD) with an applied learning rate (lr) of 0.001. Recall that the learning rate tells the optimizer how much to adjust each parameter on the next round of calculations. Too large a step and we run the risk of overshooting the minimum, causing the algorithm to diverge. Too small and it will take a long time to converge. For more complicated (multivariate) data, you might also consider passing optional momentum and weight_decay arguments. Momentum allows the algorithm to \u201croll over\u201d small bumps to avoid local minima that can cause convergence too soon. Weight decay (also called an L2 penalty) applies to biases. For more information, see torch.optim optimizer = torch . optim . SGD ( model . parameters (), lr = 0.001 ) # You'll sometimes see this as # optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3) Train the model \u00b6 An epoch is a single pass through the entire dataset. We want to pick a sufficiently large number of epochs to reach a plateau close to our known parameters of Let\u2019s walk through the steps we\u2019re about to take: 1. Set a reasonably large number of passes epochs = 50 2. Create a list to store loss values. This will let us view our progress afterward. losses = \\[\\] for i in range(epochs): 3. Bump \u201ci\u201d so that the printed report starts at 1 i+=1 4. Create a prediction set by running \u201cX\u201d through the current model parameters y_pred = model.forward(X) 5. Calculate the loss loss = criterion(y_pred, y) 6. Add the loss value to our tracking list losses.append(loss) 7. Print the current line of results print(f\u2019epoch: {i:2} loss: {loss.item():10.8f}\u2019) 8. Gradients accumulate with every backprop. To prevent compounding we need to reset the stored gradient for each new epoch. optimizer.zero_grad() 9. Now we can backprop loss.backward() 10. Finally, we can update the hyperparameters of our model optimizer.step() epochs = 50 losses = [] for i in range ( epochs ): i += 1 y_pred = model . forward ( X ) loss = criterion ( y_pred , y ) losses . append ( loss ) print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } weight: { model . linear . weight . item () : 10.8f } \\ bias: { model . linear . bias . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () epoch: 1 loss: 3057.21679688 weight: 0.10597813 bias: 0.96379614 epoch: 2 loss: 1588.53100586 weight: 3.33490038 bias: 1.06046367 epoch: 3 loss: 830.30010986 weight: 1.01483274 bias: 0.99226278 epoch: 4 loss: 438.85241699 weight: 2.68179965 bias: 1.04252183 epoch: 5 loss: 236.76152039 weight: 1.48402119 bias: 1.00766504 epoch: 6 loss: 132.42912292 weight: 2.34460592 bias: 1.03396463 epoch: 7 loss: 78.56572723 weight: 1.72622538 bias: 1.01632178 epoch: 8 loss: 50.75775909 weight: 2.17050409 bias: 1.03025162 epoch: 9 loss: 36.40123367 weight: 1.85124576 bias: 1.02149546 epoch: 10 loss: 28.98922729 weight: 2.08060074 bias: 1.02903891 epoch: 11 loss: 25.16238213 weight: 1.91576838 bias: 1.02487016 epoch: 12 loss: 23.18647385 weight: 2.03416562 bias: 1.02911627 epoch: 13 loss: 22.16612816 weight: 1.94905841 bias: 1.02731562 epoch: 14 loss: 21.63911057 weight: 2.01017213 bias: 1.02985907 epoch: 15 loss: 21.36677170 weight: 1.96622372 bias: 1.02928054 epoch: 16 loss: 21.22591782 weight: 1.99776423 bias: 1.03094459 epoch: 17 loss: 21.15294647 weight: 1.97506487 bias: 1.03099668 epoch: 18 loss: 21.11501122 weight: 1.99133754 bias: 1.03220642 epoch: 19 loss: 21.09517670 weight: 1.97960854 bias: 1.03258383 epoch: 20 loss: 21.08468437 weight: 1.98799884 bias: 1.03355861 epoch: 21 loss: 21.07901382 weight: 1.98193336 bias: 1.03410351 epoch: 22 loss: 21.07583046 weight: 1.98625445 bias: 1.03495669 epoch: 23 loss: 21.07393837 weight: 1.98311269 bias: 1.03558779 epoch: 24 loss: 21.07269859 weight: 1.98533309 bias: 1.03637791 epoch: 25 loss: 21.07181931 weight: 1.98370099 bias: 1.03705311 epoch: 26 loss: 21.07110596 weight: 1.98483658 bias: 1.03781021 epoch: 27 loss: 21.07048416 weight: 1.98398376 bias: 1.03850794 epoch: 28 loss: 21.06991386 weight: 1.98455977 bias: 1.03924775 epoch: 29 loss: 21.06936646 weight: 1.98410904 bias: 1.03995669 epoch: 30 loss: 21.06883621 weight: 1.98439610 bias: 1.04068720 epoch: 31 loss: 21.06830788 weight: 1.98415291 bias: 1.04140162 epoch: 32 loss: 21.06778145 weight: 1.98429084 bias: 1.04212701 epoch: 33 loss: 21.06726265 weight: 1.98415494 bias: 1.04284394 epoch: 34 loss: 21.06674004 weight: 1.98421574 bias: 1.04356635 epoch: 35 loss: 21.06622314 weight: 1.98413551 bias: 1.04428422 epoch: 36 loss: 21.06570625 weight: 1.98415649 bias: 1.04500473 epoch: 37 loss: 21.06518936 weight: 1.98410451 bias: 1.04572272 epoch: 38 loss: 21.06466866 weight: 1.98410523 bias: 1.04644191 epoch: 39 loss: 21.06415749 weight: 1.98406804 bias: 1.04715967 epoch: 40 loss: 21.06363869 weight: 1.98405814 bias: 1.04787791 epoch: 41 loss: 21.06312370 weight: 1.98402870 bias: 1.04859519 epoch: 42 loss: 21.06260681 weight: 1.98401320 bias: 1.04931259 epoch: 43 loss: 21.06209564 weight: 1.98398757 bias: 1.05002928 epoch: 44 loss: 21.06157875 weight: 1.98396957 bias: 1.05074584 epoch: 45 loss: 21.06106949 weight: 1.98394585 bias: 1.05146194 epoch: 46 loss: 21.06055450 weight: 1.98392630 bias: 1.05217779 epoch: 47 loss: 21.06004143 weight: 1.98390377 bias: 1.05289316 epoch: 48 loss: 21.05953217 weight: 1.98388338 bias: 1.05360830 epoch: 49 loss: 21.05901527 weight: 1.98386145 bias: 1.05432308 epoch: 50 loss: 21.05850983 weight: 1.98384094 bias: 1.05503750 Plot the loss values \u00b6 Let\u2019s see how loss changed over time plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'epoch' ); Plot the result \u00b6 Now we\u2019ll derive y1 from the new model to plot the most recent best-fit line. w1 , b1 = model . linear . weight . item (), model . linear . bias . item () print ( f 'Current weight: { w1 : .8f } , Current bias: { b1 : .8f } ' ) print () y1 = x1 * w1 + b1 print ( x1 ) print ( y1 ) Current weight: 1.98381913, Current bias: 1.05575156 [ 1. 50.] [ 3.0395708 100.246704 ] plt . scatter ( X . numpy (), y . numpy ()) plt . plot ( x1 , y1 , 'r' ) plt . title ( 'Current Model' ) plt . ylabel ( 'y' ) plt . xlabel ( 'x' ); Great job! \u00b6","title":"01 Linear Regression with PyTorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#pytorch-for-deep-learning-bootcamp","text":"by Jawad Haider","title":"Pytorch for Deep Learning BootCamp"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#01-linear-regression-with-pytorch","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Linear Regression with PyTorch Perform standard imports Create a column matrix of X values Create a \u201crandom\u201d array of error values Create a column matrix of y values Plot the results Simple linear model Model classes Plot the initial model Set the loss function Set the optimization Train the model Plot the loss values Plot the result Great job!","title":"01 - Linear Regression with PyTorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#linear-regression-with-pytorch","text":"In this section we\u2019ll use PyTorch\u2019s machine learning model to progressively develop a best-fit line for a given set of data points. Like most linear regression algorithms, we\u2019re seeking to minimize the error between our model and the actual data, using a loss function like mean-squared-error. Image source: https://commons.wikimedia.org/wiki/File:Residuals_for_Linear_Regression_Fit.png To start, we\u2019ll develop a collection of data points that appear random, but that fit a known linear equation","title":"Linear Regression with PyTorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#perform-standard-imports","text":"import torch import torch.nn as nn # we'll use this a lot going forward! import numpy as np import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#create-a-column-matrix-of-x-values","text":"We can create tensors right away rather than convert from NumPy arrays. X = torch . linspace ( 1 , 50 , 50 ) . reshape ( - 1 , 1 ) # Equivalent to # X = torch.unsqueeze(torch.linspace(1,50,50), dim=1)","title":"Create a column matrix of X values"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#create-a-random-array-of-error-values","text":"We want 50 random integer values that collectively cancel each other out. torch . manual_seed ( 71 ) # to obtain reproducible results e = torch . randint ( - 8 , 9 ,( 50 , 1 ), dtype = torch . float ) print ( e . sum ()) tensor(0.)","title":"Create a \u201crandom\u201d array of error values"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#create-a-column-matrix-of-y-values","text":"Here we\u2019ll set our own parameters of , plus the error amount. y will have the same shape as X and e y = 2 * X + 1 + e print ( y . shape ) torch.Size([50, 1])","title":"Create a column matrix of y values"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#plot-the-results","text":"We have to convert tensors to NumPy arrays just for plotting. plt . scatter ( X . numpy (), y . numpy ()) plt . ylabel ( 'y' ) plt . xlabel ( 'x' ); Note that when we created tensor , we did not pass requires_grad=True . This means that doesn\u2019t have a gradient function, and y.backward() won\u2019t work. Since PyTorch is not tracking operations, it doesn\u2019t know the relationship between and .","title":"Plot the results"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#simple-linear-model","text":"As a quick demonstration we\u2019ll show how the built-in nn.Linear() model preselects weight and bias values at random. torch . manual_seed ( 59 ) model = nn . Linear ( in_features = 1 , out_features = 1 ) print ( model . weight ) print ( model . bias ) Parameter containing: tensor([[0.1060]], requires_grad=True) Parameter containing: tensor([0.9638], requires_grad=True) Without seeing any data, the model sets a random weight of 0.1060 and a bias of 0.9638.","title":"Simple linear model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#model-classes","text":"PyTorch lets us define models as object classes that can store multiple model layers. In upcoming sections we\u2019ll set up several neural network layers, and determine how each layer should perform its forward pass to the next layer. For now, though, we only need a single linear layer. class Model ( nn . Module ): def __init__ ( self , in_features , out_features ): super () . __init__ () self . linear = nn . Linear ( in_features , out_features ) def forward ( self , x ): y_pred = self . linear ( x ) return y_pred NOTE: The \u201cLinear\u201d model layer used here doesn\u2019t really refer to linear regression. Instead, it describes the type of neural network layer employed. Linear layers are also called \u201cfully connected\u201d or \u201cdense\u201d layers. Going forward our models may contain linear layers, convolutional layers, and more. When Model is instantiated, we need to pass in the size (dimensions) of the incoming and outgoing features. For our purposes we\u2019ll use (1,1). As above, we can see the initial hyperparameters. torch . manual_seed ( 59 ) model = Model ( 1 , 1 ) print ( model ) print ( 'Weight:' , model . linear . weight . item ()) print ( 'Bias: ' , model . linear . bias . item ()) Model( (linear): Linear(in_features=1, out_features=1, bias=True) ) Weight: 0.10597813129425049 Bias: 0.9637961387634277 As models become more complex, it may be better to iterate over all the model parameters: for name , param in model . named_parameters (): print ( name , ' \\t ' , param . item ()) linear.weight 0.10597813129425049 linear.bias 0.9637961387634277 NOTE: In the above example we had our Model class accept arguments for the number of input and output features. For simplicity we can hardcode them into the Model: class Model(torch.nn.Module): def \\_\\_init\\_\\_(self): super().\\_\\_init\\_\\_() self.linear = Linear(1,1) model = Model() Alternatively we can use default arguments: class Model(torch.nn.Module): def \\_\\_init\\_\\_(self, in_dim=1, out_dim=1): super().\\_\\_init\\_\\_() self.linear = Linear(in_dim,out_dim) model = Model() \\# or model = Model(i,o) Now let\u2019s see the result when we pass a tensor into the model. x = torch . tensor ([ 2.0 ]) print ( model . forward ( x )) # equivalent to print(model(x)) tensor([1.1758], grad_fn=<AddBackward0>) which is confirmed with","title":"Model classes"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#plot-the-initial-model","text":"We can plot the untrained model against our dataset to get an idea of our starting point. x1 = np . array ([ X . min (), X . max ()]) print ( x1 ) [ 1. 50.] w1 , b1 = model . linear . weight . item (), model . linear . bias . item () print ( f 'Initial weight: { w1 : .8f } , Initial bias: { b1 : .8f } ' ) print () y1 = x1 * w1 + b1 print ( y1 ) Initial weight: 0.10597813, Initial bias: 0.96379614 [1.0697743 6.2627025] plt . scatter ( X . numpy (), y . numpy ()) plt . plot ( x1 , y1 , 'r' ) plt . title ( 'Initial Model' ) plt . ylabel ( 'y' ) plt . xlabel ( 'x' );","title":"Plot the initial model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#set-the-loss-function","text":"We could write our own function to apply a Mean Squared Error (MSE) that follows Fortunately PyTorch has it built in. By convention, you\u2019ll see the variable name \u201ccriterion\u201d used, but feel free to use something like \u201clinear_loss_func\u201d if that\u2019s clearer. criterion = nn . MSELoss ()","title":"Set the loss function"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#set-the-optimization","text":"Here we\u2019ll use Stochastic Gradient Descent (SGD) with an applied learning rate (lr) of 0.001. Recall that the learning rate tells the optimizer how much to adjust each parameter on the next round of calculations. Too large a step and we run the risk of overshooting the minimum, causing the algorithm to diverge. Too small and it will take a long time to converge. For more complicated (multivariate) data, you might also consider passing optional momentum and weight_decay arguments. Momentum allows the algorithm to \u201croll over\u201d small bumps to avoid local minima that can cause convergence too soon. Weight decay (also called an L2 penalty) applies to biases. For more information, see torch.optim optimizer = torch . optim . SGD ( model . parameters (), lr = 0.001 ) # You'll sometimes see this as # optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)","title":"Set the optimization"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#train-the-model","text":"An epoch is a single pass through the entire dataset. We want to pick a sufficiently large number of epochs to reach a plateau close to our known parameters of Let\u2019s walk through the steps we\u2019re about to take: 1. Set a reasonably large number of passes epochs = 50 2. Create a list to store loss values. This will let us view our progress afterward. losses = \\[\\] for i in range(epochs): 3. Bump \u201ci\u201d so that the printed report starts at 1 i+=1 4. Create a prediction set by running \u201cX\u201d through the current model parameters y_pred = model.forward(X) 5. Calculate the loss loss = criterion(y_pred, y) 6. Add the loss value to our tracking list losses.append(loss) 7. Print the current line of results print(f\u2019epoch: {i:2} loss: {loss.item():10.8f}\u2019) 8. Gradients accumulate with every backprop. To prevent compounding we need to reset the stored gradient for each new epoch. optimizer.zero_grad() 9. Now we can backprop loss.backward() 10. Finally, we can update the hyperparameters of our model optimizer.step() epochs = 50 losses = [] for i in range ( epochs ): i += 1 y_pred = model . forward ( X ) loss = criterion ( y_pred , y ) losses . append ( loss ) print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } weight: { model . linear . weight . item () : 10.8f } \\ bias: { model . linear . bias . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () epoch: 1 loss: 3057.21679688 weight: 0.10597813 bias: 0.96379614 epoch: 2 loss: 1588.53100586 weight: 3.33490038 bias: 1.06046367 epoch: 3 loss: 830.30010986 weight: 1.01483274 bias: 0.99226278 epoch: 4 loss: 438.85241699 weight: 2.68179965 bias: 1.04252183 epoch: 5 loss: 236.76152039 weight: 1.48402119 bias: 1.00766504 epoch: 6 loss: 132.42912292 weight: 2.34460592 bias: 1.03396463 epoch: 7 loss: 78.56572723 weight: 1.72622538 bias: 1.01632178 epoch: 8 loss: 50.75775909 weight: 2.17050409 bias: 1.03025162 epoch: 9 loss: 36.40123367 weight: 1.85124576 bias: 1.02149546 epoch: 10 loss: 28.98922729 weight: 2.08060074 bias: 1.02903891 epoch: 11 loss: 25.16238213 weight: 1.91576838 bias: 1.02487016 epoch: 12 loss: 23.18647385 weight: 2.03416562 bias: 1.02911627 epoch: 13 loss: 22.16612816 weight: 1.94905841 bias: 1.02731562 epoch: 14 loss: 21.63911057 weight: 2.01017213 bias: 1.02985907 epoch: 15 loss: 21.36677170 weight: 1.96622372 bias: 1.02928054 epoch: 16 loss: 21.22591782 weight: 1.99776423 bias: 1.03094459 epoch: 17 loss: 21.15294647 weight: 1.97506487 bias: 1.03099668 epoch: 18 loss: 21.11501122 weight: 1.99133754 bias: 1.03220642 epoch: 19 loss: 21.09517670 weight: 1.97960854 bias: 1.03258383 epoch: 20 loss: 21.08468437 weight: 1.98799884 bias: 1.03355861 epoch: 21 loss: 21.07901382 weight: 1.98193336 bias: 1.03410351 epoch: 22 loss: 21.07583046 weight: 1.98625445 bias: 1.03495669 epoch: 23 loss: 21.07393837 weight: 1.98311269 bias: 1.03558779 epoch: 24 loss: 21.07269859 weight: 1.98533309 bias: 1.03637791 epoch: 25 loss: 21.07181931 weight: 1.98370099 bias: 1.03705311 epoch: 26 loss: 21.07110596 weight: 1.98483658 bias: 1.03781021 epoch: 27 loss: 21.07048416 weight: 1.98398376 bias: 1.03850794 epoch: 28 loss: 21.06991386 weight: 1.98455977 bias: 1.03924775 epoch: 29 loss: 21.06936646 weight: 1.98410904 bias: 1.03995669 epoch: 30 loss: 21.06883621 weight: 1.98439610 bias: 1.04068720 epoch: 31 loss: 21.06830788 weight: 1.98415291 bias: 1.04140162 epoch: 32 loss: 21.06778145 weight: 1.98429084 bias: 1.04212701 epoch: 33 loss: 21.06726265 weight: 1.98415494 bias: 1.04284394 epoch: 34 loss: 21.06674004 weight: 1.98421574 bias: 1.04356635 epoch: 35 loss: 21.06622314 weight: 1.98413551 bias: 1.04428422 epoch: 36 loss: 21.06570625 weight: 1.98415649 bias: 1.04500473 epoch: 37 loss: 21.06518936 weight: 1.98410451 bias: 1.04572272 epoch: 38 loss: 21.06466866 weight: 1.98410523 bias: 1.04644191 epoch: 39 loss: 21.06415749 weight: 1.98406804 bias: 1.04715967 epoch: 40 loss: 21.06363869 weight: 1.98405814 bias: 1.04787791 epoch: 41 loss: 21.06312370 weight: 1.98402870 bias: 1.04859519 epoch: 42 loss: 21.06260681 weight: 1.98401320 bias: 1.04931259 epoch: 43 loss: 21.06209564 weight: 1.98398757 bias: 1.05002928 epoch: 44 loss: 21.06157875 weight: 1.98396957 bias: 1.05074584 epoch: 45 loss: 21.06106949 weight: 1.98394585 bias: 1.05146194 epoch: 46 loss: 21.06055450 weight: 1.98392630 bias: 1.05217779 epoch: 47 loss: 21.06004143 weight: 1.98390377 bias: 1.05289316 epoch: 48 loss: 21.05953217 weight: 1.98388338 bias: 1.05360830 epoch: 49 loss: 21.05901527 weight: 1.98386145 bias: 1.05432308 epoch: 50 loss: 21.05850983 weight: 1.98384094 bias: 1.05503750","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#plot-the-loss-values","text":"Let\u2019s see how loss changed over time plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'epoch' );","title":"Plot the loss values"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#plot-the-result","text":"Now we\u2019ll derive y1 from the new model to plot the most recent best-fit line. w1 , b1 = model . linear . weight . item (), model . linear . bias . item () print ( f 'Current weight: { w1 : .8f } , Current bias: { b1 : .8f } ' ) print () y1 = x1 * w1 + b1 print ( x1 ) print ( y1 ) Current weight: 1.98381913, Current bias: 1.05575156 [ 1. 50.] [ 3.0395708 100.246704 ] plt . scatter ( X . numpy (), y . numpy ()) plt . plot ( x1 , y1 , 'r' ) plt . title ( 'Current Model' ) plt . ylabel ( 'y' ) plt . xlabel ( 'x' );","title":"Plot the result"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/01-Linear-Regression-with-PyTorch/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/02-DataSets-with-Pytorch/","text":"Pytorch for Deep Learning BootCamp \u00b6 by Jawad Haider Datasets with PyTorch Perform standard imports Loading data from files Plot the data The classic method for building train/test split tensors Using PyTorch\u2019s Dataset and DataLoader classes A Quick Note on Torchvision Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Datasets with PyTorch \u00b6 In this section we\u2019ll show how to: * load data from outside files * build random batches using PyTorch\u2019s data utilities At the end we\u2019ll briefly mention torchvision . Perform standard imports \u00b6 import torch import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Loading data from files \u00b6 We\u2019ve seen how to load NumPy arrays into PyTorch, and anyone familiar with pandas.read_csv() can use it to prepare data before forming tensors. Here we\u2019ll load the iris flower dataset saved as a .csv file. df = pd . read_csv ( '../Data/iris.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 df . shape (150, 5) Plot the data \u00b6 fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () The iris dataset consists of 50 samples each from three species of Iris ( Iris setosa , Iris virginica and Iris versicolor ), for 150 total samples. We have four features (sepal length & width, petal length & width) and three unique labels: 0. Iris setosa 1. Iris virginica 2. Iris versicolor The classic method for building train/test split tensors \u00b6 Before introducing PyTorch\u2019s Dataset and DataLoader classes, we\u2019ll take a quick look at the alternative. from sklearn.model_selection import train_test_split train_X , test_X , train_y , test_y = train_test_split ( df . drop ( 'target' , axis = 1 ) . values , df [ 'target' ] . values , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( train_X ) X_test = torch . FloatTensor ( test_X ) y_train = torch . LongTensor ( train_y ) . reshape ( - 1 , 1 ) y_test = torch . LongTensor ( test_y ) . reshape ( - 1 , 1 ) print ( f 'Training size: { len ( y_train ) } ' ) labels , counts = y_train . unique ( return_counts = True ) print ( f 'Labels: { labels } \\n Counts: { counts } ' ) Training size: 120 Labels: tensor([0, 1, 2]) Counts: tensor([42, 42, 36]) NOTE: The importance of a balanced training set is discussed in A systematic study of the class imbalance problem in convolutional neural networks by Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski (10/15/17, latest rev 10/13/18) https://arxiv.org/abs/1710.05381 For example, the authors show that oversampling a less common class so that it matches the more common classes is always the preferred choice. X_train . size () torch.Size([120, 4]) y_train . size () torch.Size([120, 1]) NOTE: It\u2019s up to us to remember which columns correspond to which features. Using PyTorch\u2019s Dataset and DataLoader classes \u00b6 A far better alternative is to leverage PyTorch\u2019s Dataset and DataLoader classes. Usually, to set up a Dataset specific to our investigation we would define our own custom class that inherits from torch.utils.data.Dataset (we\u2019ll do this in the CNN section). For now, we can use the built-in TensorDataset class. from torch.utils.data import TensorDataset , DataLoader data = df . drop ( 'target' , axis = 1 ) . values labels = df [ 'target' ] . values iris = TensorDataset ( torch . FloatTensor ( data ), torch . LongTensor ( labels )) len ( iris ) 150 type ( iris ) torch.utils.data.dataset.TensorDataset for i in iris : print ( i ) (tensor([5.1000, 3.5000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.0000, 1.4000, 0.2000]), tensor(0)) (tensor([4.7000, 3.2000, 1.3000, 0.2000]), tensor(0)) (tensor([4.6000, 3.1000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.6000, 1.4000, 0.2000]), tensor(0)) (tensor([5.4000, 3.9000, 1.7000, 0.4000]), tensor(0)) (tensor([4.6000, 3.4000, 1.4000, 0.3000]), tensor(0)) (tensor([5.0000, 3.4000, 1.5000, 0.2000]), tensor(0)) (tensor([4.4000, 2.9000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.4000, 3.7000, 1.5000, 0.2000]), tensor(0)) (tensor([4.8000, 3.4000, 1.6000, 0.2000]), tensor(0)) (tensor([4.8000, 3.0000, 1.4000, 0.1000]), tensor(0)) (tensor([4.3000, 3.0000, 1.1000, 0.1000]), tensor(0)) (tensor([5.8000, 4.0000, 1.2000, 0.2000]), tensor(0)) (tensor([5.7000, 4.4000, 1.5000, 0.4000]), tensor(0)) (tensor([5.4000, 3.9000, 1.3000, 0.4000]), tensor(0)) (tensor([5.1000, 3.5000, 1.4000, 0.3000]), tensor(0)) (tensor([5.7000, 3.8000, 1.7000, 0.3000]), tensor(0)) (tensor([5.1000, 3.8000, 1.5000, 0.3000]), tensor(0)) (tensor([5.4000, 3.4000, 1.7000, 0.2000]), tensor(0)) (tensor([5.1000, 3.7000, 1.5000, 0.4000]), tensor(0)) (tensor([4.6000, 3.6000, 1.0000, 0.2000]), tensor(0)) (tensor([5.1000, 3.3000, 1.7000, 0.5000]), tensor(0)) (tensor([4.8000, 3.4000, 1.9000, 0.2000]), tensor(0)) (tensor([5.0000, 3.0000, 1.6000, 0.2000]), tensor(0)) (tensor([5.0000, 3.4000, 1.6000, 0.4000]), tensor(0)) (tensor([5.2000, 3.5000, 1.5000, 0.2000]), tensor(0)) (tensor([5.2000, 3.4000, 1.4000, 0.2000]), tensor(0)) (tensor([4.7000, 3.2000, 1.6000, 0.2000]), tensor(0)) (tensor([4.8000, 3.1000, 1.6000, 0.2000]), tensor(0)) (tensor([5.4000, 3.4000, 1.5000, 0.4000]), tensor(0)) (tensor([5.2000, 4.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.5000, 4.2000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.0000, 3.2000, 1.2000, 0.2000]), tensor(0)) (tensor([5.5000, 3.5000, 1.3000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([4.4000, 3.0000, 1.3000, 0.2000]), tensor(0)) (tensor([5.1000, 3.4000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.5000, 1.3000, 0.3000]), tensor(0)) (tensor([4.5000, 2.3000, 1.3000, 0.3000]), tensor(0)) (tensor([4.4000, 3.2000, 1.3000, 0.2000]), tensor(0)) (tensor([5.0000, 3.5000, 1.6000, 0.6000]), tensor(0)) (tensor([5.1000, 3.8000, 1.9000, 0.4000]), tensor(0)) (tensor([4.8000, 3.0000, 1.4000, 0.3000]), tensor(0)) (tensor([5.1000, 3.8000, 1.6000, 0.2000]), tensor(0)) (tensor([4.6000, 3.2000, 1.4000, 0.2000]), tensor(0)) (tensor([5.3000, 3.7000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.3000, 1.4000, 0.2000]), tensor(0)) (tensor([7.0000, 3.2000, 4.7000, 1.4000]), tensor(1)) (tensor([6.4000, 3.2000, 4.5000, 1.5000]), tensor(1)) (tensor([6.9000, 3.1000, 4.9000, 1.5000]), tensor(1)) (tensor([5.5000, 2.3000, 4.0000, 1.3000]), tensor(1)) (tensor([6.5000, 2.8000, 4.6000, 1.5000]), tensor(1)) (tensor([5.7000, 2.8000, 4.5000, 1.3000]), tensor(1)) (tensor([6.3000, 3.3000, 4.7000, 1.6000]), tensor(1)) (tensor([4.9000, 2.4000, 3.3000, 1.0000]), tensor(1)) (tensor([6.6000, 2.9000, 4.6000, 1.3000]), tensor(1)) (tensor([5.2000, 2.7000, 3.9000, 1.4000]), tensor(1)) (tensor([5.0000, 2.0000, 3.5000, 1.0000]), tensor(1)) (tensor([5.9000, 3.0000, 4.2000, 1.5000]), tensor(1)) (tensor([6.0000, 2.2000, 4.0000, 1.0000]), tensor(1)) (tensor([6.1000, 2.9000, 4.7000, 1.4000]), tensor(1)) (tensor([5.6000, 2.9000, 3.6000, 1.3000]), tensor(1)) (tensor([6.7000, 3.1000, 4.4000, 1.4000]), tensor(1)) (tensor([5.6000, 3.0000, 4.5000, 1.5000]), tensor(1)) (tensor([5.8000, 2.7000, 4.1000, 1.0000]), tensor(1)) (tensor([6.2000, 2.2000, 4.5000, 1.5000]), tensor(1)) (tensor([5.6000, 2.5000, 3.9000, 1.1000]), tensor(1)) (tensor([5.9000, 3.2000, 4.8000, 1.8000]), tensor(1)) (tensor([6.1000, 2.8000, 4.0000, 1.3000]), tensor(1)) (tensor([6.3000, 2.5000, 4.9000, 1.5000]), tensor(1)) (tensor([6.1000, 2.8000, 4.7000, 1.2000]), tensor(1)) (tensor([6.4000, 2.9000, 4.3000, 1.3000]), tensor(1)) (tensor([6.6000, 3.0000, 4.4000, 1.4000]), tensor(1)) (tensor([6.8000, 2.8000, 4.8000, 1.4000]), tensor(1)) (tensor([6.7000, 3.0000, 5.0000, 1.7000]), tensor(1)) (tensor([6.0000, 2.9000, 4.5000, 1.5000]), tensor(1)) (tensor([5.7000, 2.6000, 3.5000, 1.0000]), tensor(1)) (tensor([5.5000, 2.4000, 3.8000, 1.1000]), tensor(1)) (tensor([5.5000, 2.4000, 3.7000, 1.0000]), tensor(1)) (tensor([5.8000, 2.7000, 3.9000, 1.2000]), tensor(1)) (tensor([6.0000, 2.7000, 5.1000, 1.6000]), tensor(1)) (tensor([5.4000, 3.0000, 4.5000, 1.5000]), tensor(1)) (tensor([6.0000, 3.4000, 4.5000, 1.6000]), tensor(1)) (tensor([6.7000, 3.1000, 4.7000, 1.5000]), tensor(1)) (tensor([6.3000, 2.3000, 4.4000, 1.3000]), tensor(1)) (tensor([5.6000, 3.0000, 4.1000, 1.3000]), tensor(1)) (tensor([5.5000, 2.5000, 4.0000, 1.3000]), tensor(1)) (tensor([5.5000, 2.6000, 4.4000, 1.2000]), tensor(1)) (tensor([6.1000, 3.0000, 4.6000, 1.4000]), tensor(1)) (tensor([5.8000, 2.6000, 4.0000, 1.2000]), tensor(1)) (tensor([5.0000, 2.3000, 3.3000, 1.0000]), tensor(1)) (tensor([5.6000, 2.7000, 4.2000, 1.3000]), tensor(1)) (tensor([5.7000, 3.0000, 4.2000, 1.2000]), tensor(1)) (tensor([5.7000, 2.9000, 4.2000, 1.3000]), tensor(1)) (tensor([6.2000, 2.9000, 4.3000, 1.3000]), tensor(1)) (tensor([5.1000, 2.5000, 3.0000, 1.1000]), tensor(1)) (tensor([5.7000, 2.8000, 4.1000, 1.3000]), tensor(1)) (tensor([6.3000, 3.3000, 6.0000, 2.5000]), tensor(2)) (tensor([5.8000, 2.7000, 5.1000, 1.9000]), tensor(2)) (tensor([7.1000, 3.0000, 5.9000, 2.1000]), tensor(2)) (tensor([6.3000, 2.9000, 5.6000, 1.8000]), tensor(2)) (tensor([6.5000, 3.0000, 5.8000, 2.2000]), tensor(2)) (tensor([7.6000, 3.0000, 6.6000, 2.1000]), tensor(2)) (tensor([4.9000, 2.5000, 4.5000, 1.7000]), tensor(2)) (tensor([7.3000, 2.9000, 6.3000, 1.8000]), tensor(2)) (tensor([6.7000, 2.5000, 5.8000, 1.8000]), tensor(2)) (tensor([7.2000, 3.6000, 6.1000, 2.5000]), tensor(2)) (tensor([6.5000, 3.2000, 5.1000, 2.0000]), tensor(2)) (tensor([6.4000, 2.7000, 5.3000, 1.9000]), tensor(2)) (tensor([6.8000, 3.0000, 5.5000, 2.1000]), tensor(2)) (tensor([5.7000, 2.5000, 5.0000, 2.0000]), tensor(2)) (tensor([5.8000, 2.8000, 5.1000, 2.4000]), tensor(2)) (tensor([6.4000, 3.2000, 5.3000, 2.3000]), tensor(2)) (tensor([6.5000, 3.0000, 5.5000, 1.8000]), tensor(2)) (tensor([7.7000, 3.8000, 6.7000, 2.2000]), tensor(2)) (tensor([7.7000, 2.6000, 6.9000, 2.3000]), tensor(2)) (tensor([6.0000, 2.2000, 5.0000, 1.5000]), tensor(2)) (tensor([6.9000, 3.2000, 5.7000, 2.3000]), tensor(2)) (tensor([5.6000, 2.8000, 4.9000, 2.0000]), tensor(2)) (tensor([7.7000, 2.8000, 6.7000, 2.0000]), tensor(2)) (tensor([6.3000, 2.7000, 4.9000, 1.8000]), tensor(2)) (tensor([6.7000, 3.3000, 5.7000, 2.1000]), tensor(2)) (tensor([7.2000, 3.2000, 6.0000, 1.8000]), tensor(2)) (tensor([6.2000, 2.8000, 4.8000, 1.8000]), tensor(2)) (tensor([6.1000, 3.0000, 4.9000, 1.8000]), tensor(2)) (tensor([6.4000, 2.8000, 5.6000, 2.1000]), tensor(2)) (tensor([7.2000, 3.0000, 5.8000, 1.6000]), tensor(2)) (tensor([7.4000, 2.8000, 6.1000, 1.9000]), tensor(2)) (tensor([7.9000, 3.8000, 6.4000, 2.0000]), tensor(2)) (tensor([6.4000, 2.8000, 5.6000, 2.2000]), tensor(2)) (tensor([6.3000, 2.8000, 5.1000, 1.5000]), tensor(2)) (tensor([6.1000, 2.6000, 5.6000, 1.4000]), tensor(2)) (tensor([7.7000, 3.0000, 6.1000, 2.3000]), tensor(2)) (tensor([6.3000, 3.4000, 5.6000, 2.4000]), tensor(2)) (tensor([6.4000, 3.1000, 5.5000, 1.8000]), tensor(2)) (tensor([6.0000, 3.0000, 4.8000, 1.8000]), tensor(2)) (tensor([6.9000, 3.1000, 5.4000, 2.1000]), tensor(2)) (tensor([6.7000, 3.1000, 5.6000, 2.4000]), tensor(2)) (tensor([6.9000, 3.1000, 5.1000, 2.3000]), tensor(2)) (tensor([5.8000, 2.7000, 5.1000, 1.9000]), tensor(2)) (tensor([6.8000, 3.2000, 5.9000, 2.3000]), tensor(2)) (tensor([6.7000, 3.3000, 5.7000, 2.5000]), tensor(2)) (tensor([6.7000, 3.0000, 5.2000, 2.3000]), tensor(2)) (tensor([6.3000, 2.5000, 5.0000, 1.9000]), tensor(2)) (tensor([6.5000, 3.0000, 5.2000, 2.0000]), tensor(2)) (tensor([6.2000, 3.4000, 5.4000, 2.3000]), tensor(2)) (tensor([5.9000, 3.0000, 5.1000, 1.8000]), tensor(2)) Once we have a dataset we can wrap it with a DataLoader. This gives us a powerful sampler that provides single- or multi-process iterators over the dataset. iris_loader = DataLoader ( iris , batch_size = 105 , shuffle = True ) for i_batch , sample_batched in enumerate ( iris_loader ): print ( i_batch , sample_batched ) 0 [tensor([[6.7000, 3.1000, 4.4000, 1.4000], [4.8000, 3.4000, 1.6000, 0.2000], [4.8000, 3.4000, 1.9000, 0.2000], [5.6000, 2.9000, 3.6000, 1.3000], [6.7000, 3.3000, 5.7000, 2.1000], [6.9000, 3.1000, 4.9000, 1.5000], [6.7000, 2.5000, 5.8000, 1.8000], [6.4000, 3.1000, 5.5000, 1.8000], [6.7000, 3.1000, 5.6000, 2.4000], [5.5000, 2.5000, 4.0000, 1.3000], [7.0000, 3.2000, 4.7000, 1.4000], [5.5000, 4.2000, 1.4000, 0.2000], [7.3000, 2.9000, 6.3000, 1.8000], [7.7000, 2.8000, 6.7000, 2.0000], [4.9000, 3.1000, 1.5000, 0.1000], [7.2000, 3.0000, 5.8000, 1.6000], [6.7000, 3.1000, 4.7000, 1.5000], [5.8000, 2.7000, 3.9000, 1.2000], [5.5000, 2.4000, 3.8000, 1.1000], [5.0000, 3.5000, 1.6000, 0.6000], [5.1000, 3.8000, 1.6000, 0.2000], [4.8000, 3.0000, 1.4000, 0.1000], [6.5000, 3.0000, 5.5000, 1.8000], [6.7000, 3.0000, 5.2000, 2.3000], [6.8000, 2.8000, 4.8000, 1.4000], [7.4000, 2.8000, 6.1000, 1.9000], [5.0000, 3.4000, 1.6000, 0.4000], [6.3000, 3.3000, 6.0000, 2.5000], [5.7000, 2.8000, 4.1000, 1.3000], [5.1000, 3.8000, 1.9000, 0.4000], [6.6000, 2.9000, 4.6000, 1.3000], [6.3000, 3.4000, 5.6000, 2.4000], [5.0000, 3.2000, 1.2000, 0.2000], [5.9000, 3.2000, 4.8000, 1.8000], [4.7000, 3.2000, 1.6000, 0.2000], [5.1000, 3.8000, 1.5000, 0.3000], [5.7000, 2.6000, 3.5000, 1.0000], [5.7000, 4.4000, 1.5000, 0.4000], [5.0000, 2.0000, 3.5000, 1.0000], [4.4000, 3.2000, 1.3000, 0.2000], [5.2000, 3.4000, 1.4000, 0.2000], [5.5000, 2.3000, 4.0000, 1.3000], [7.6000, 3.0000, 6.6000, 2.1000], [4.4000, 2.9000, 1.4000, 0.2000], [5.7000, 3.8000, 1.7000, 0.3000], [7.7000, 3.0000, 6.1000, 2.3000], [4.9000, 2.5000, 4.5000, 1.7000], [5.9000, 3.0000, 5.1000, 1.8000], [7.2000, 3.6000, 6.1000, 2.5000], [5.8000, 2.8000, 5.1000, 2.4000], [4.7000, 3.2000, 1.3000, 0.2000], [6.2000, 3.4000, 5.4000, 2.3000], [5.7000, 3.0000, 4.2000, 1.2000], [5.6000, 2.7000, 4.2000, 1.3000], [5.2000, 4.1000, 1.5000, 0.1000], [5.1000, 3.5000, 1.4000, 0.3000], [5.0000, 3.0000, 1.6000, 0.2000], [6.3000, 2.3000, 4.4000, 1.3000], [6.5000, 3.2000, 5.1000, 2.0000], [5.6000, 2.8000, 4.9000, 2.0000], [5.4000, 3.4000, 1.7000, 0.2000], [5.9000, 3.0000, 4.2000, 1.5000], [6.2000, 2.2000, 4.5000, 1.5000], [5.1000, 3.4000, 1.5000, 0.2000], [6.9000, 3.1000, 5.4000, 2.1000], [4.6000, 3.2000, 1.4000, 0.2000], [5.8000, 2.7000, 4.1000, 1.0000], [5.8000, 2.7000, 5.1000, 1.9000], [6.0000, 2.2000, 4.0000, 1.0000], [6.3000, 2.7000, 4.9000, 1.8000], [7.1000, 3.0000, 5.9000, 2.1000], [6.3000, 2.9000, 5.6000, 1.8000], [4.6000, 3.1000, 1.5000, 0.2000], [4.4000, 3.0000, 1.3000, 0.2000], [5.5000, 2.6000, 4.4000, 1.2000], [5.4000, 3.4000, 1.5000, 0.4000], [4.9000, 2.4000, 3.3000, 1.0000], [6.2000, 2.8000, 4.8000, 1.8000], [7.2000, 3.2000, 6.0000, 1.8000], [6.3000, 3.3000, 4.7000, 1.6000], [5.6000, 3.0000, 4.5000, 1.5000], [6.0000, 2.7000, 5.1000, 1.6000], [6.0000, 2.2000, 5.0000, 1.5000], [6.4000, 2.9000, 4.3000, 1.3000], [5.8000, 2.6000, 4.0000, 1.2000], [6.9000, 3.1000, 5.1000, 2.3000], [5.6000, 3.0000, 4.1000, 1.3000], [5.4000, 3.9000, 1.3000, 0.4000], [5.3000, 3.7000, 1.5000, 0.2000], [6.3000, 2.5000, 4.9000, 1.5000], [5.0000, 3.6000, 1.4000, 0.2000], [5.1000, 3.3000, 1.7000, 0.5000], [6.1000, 2.8000, 4.7000, 1.2000], [6.2000, 2.9000, 4.3000, 1.3000], [6.7000, 3.0000, 5.0000, 1.7000], [6.1000, 2.6000, 5.6000, 1.4000], [6.4000, 2.7000, 5.3000, 1.9000], [4.5000, 2.3000, 1.3000, 0.3000], [6.1000, 2.8000, 4.0000, 1.3000], [5.4000, 3.0000, 4.5000, 1.5000], [6.5000, 3.0000, 5.2000, 2.0000], [6.0000, 3.0000, 4.8000, 1.8000], [5.0000, 3.5000, 1.3000, 0.3000], [6.5000, 3.0000, 5.8000, 2.2000], [5.0000, 3.3000, 1.4000, 0.2000]]), tensor([1, 0, 0, 1, 2, 1, 2, 2, 2, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 0, 0, 0, 2, 2, 1, 2, 0, 2, 1, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 1, 1, 0, 0, 0, 1, 2, 2, 0, 1, 1, 0, 2, 0, 1, 2, 1, 2, 2, 2, 0, 0, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 1, 2, 2, 0, 1, 1, 2, 2, 0, 2, 0])] 1 [tensor([[5.5000, 2.4000, 3.7000, 1.0000], [6.4000, 3.2000, 4.5000, 1.5000], [5.6000, 2.5000, 3.9000, 1.1000], [5.1000, 3.7000, 1.5000, 0.4000], [5.0000, 2.3000, 3.3000, 1.0000], [5.0000, 3.4000, 1.5000, 0.2000], [5.7000, 2.9000, 4.2000, 1.3000], [6.1000, 3.0000, 4.9000, 1.8000], [5.1000, 2.5000, 3.0000, 1.1000], [6.0000, 2.9000, 4.5000, 1.5000], [4.3000, 3.0000, 1.1000, 0.1000], [6.4000, 2.8000, 5.6000, 2.1000], [5.1000, 3.5000, 1.4000, 0.2000], [5.7000, 2.8000, 4.5000, 1.3000], [4.9000, 3.1000, 1.5000, 0.1000], [7.7000, 2.6000, 6.9000, 2.3000], [6.6000, 3.0000, 4.4000, 1.4000], [4.8000, 3.0000, 1.4000, 0.3000], [6.4000, 2.8000, 5.6000, 2.2000], [6.8000, 3.2000, 5.9000, 2.3000], [5.4000, 3.7000, 1.5000, 0.2000], [4.9000, 3.1000, 1.5000, 0.1000], [6.9000, 3.2000, 5.7000, 2.3000], [5.8000, 2.7000, 5.1000, 1.9000], [6.1000, 2.9000, 4.7000, 1.4000], [4.6000, 3.6000, 1.0000, 0.2000], [5.2000, 3.5000, 1.5000, 0.2000], [6.8000, 3.0000, 5.5000, 2.1000], [7.7000, 3.8000, 6.7000, 2.2000], [6.0000, 3.4000, 4.5000, 1.6000], [5.7000, 2.5000, 5.0000, 2.0000], [6.5000, 2.8000, 4.6000, 1.5000], [4.6000, 3.4000, 1.4000, 0.3000], [5.2000, 2.7000, 3.9000, 1.4000], [5.5000, 3.5000, 1.3000, 0.2000], [4.9000, 3.0000, 1.4000, 0.2000], [6.3000, 2.5000, 5.0000, 1.9000], [6.1000, 3.0000, 4.6000, 1.4000], [6.4000, 3.2000, 5.3000, 2.3000], [5.8000, 4.0000, 1.2000, 0.2000], [6.3000, 2.8000, 5.1000, 1.5000], [4.8000, 3.1000, 1.6000, 0.2000], [6.7000, 3.3000, 5.7000, 2.5000], [5.4000, 3.9000, 1.7000, 0.4000], [7.9000, 3.8000, 6.4000, 2.0000]]), tensor([1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 2, 1, 0, 2, 2, 0, 0, 2, 2, 1, 0, 0, 2, 2, 1, 2, 1, 0, 1, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2])] list ( iris_loader )[ 0 ][ 1 ] . bincount () tensor([30, 36, 39]) next ( iter ( iris_loader )) [tensor([[5.4000, 3.7000, 1.5000, 0.2000], [4.7000, 3.2000, 1.3000, 0.2000], [6.1000, 3.0000, 4.6000, 1.4000], [4.3000, 3.0000, 1.1000, 0.1000], [5.0000, 3.5000, 1.3000, 0.3000], [7.2000, 3.2000, 6.0000, 1.8000], [4.8000, 3.4000, 1.9000, 0.2000], [6.4000, 3.1000, 5.5000, 1.8000], [6.6000, 3.0000, 4.4000, 1.4000], [6.8000, 3.2000, 5.9000, 2.3000], [6.4000, 3.2000, 4.5000, 1.5000], [5.0000, 2.3000, 3.3000, 1.0000], [6.0000, 2.2000, 4.0000, 1.0000], [6.7000, 3.1000, 5.6000, 2.4000], [6.0000, 2.7000, 5.1000, 1.6000], [6.2000, 2.8000, 4.8000, 1.8000], [5.4000, 3.4000, 1.7000, 0.2000], [5.4000, 3.9000, 1.7000, 0.4000], [4.6000, 3.2000, 1.4000, 0.2000], [5.2000, 2.7000, 3.9000, 1.4000], [6.0000, 3.0000, 4.8000, 1.8000], [5.7000, 2.8000, 4.5000, 1.3000], [7.7000, 2.6000, 6.9000, 2.3000], [5.2000, 3.5000, 1.5000, 0.2000], [6.4000, 2.8000, 5.6000, 2.1000], [5.7000, 3.8000, 1.7000, 0.3000], [6.3000, 2.7000, 4.9000, 1.8000], [6.8000, 2.8000, 4.8000, 1.4000], [6.5000, 3.2000, 5.1000, 2.0000], [6.9000, 3.2000, 5.7000, 2.3000], [7.6000, 3.0000, 6.6000, 2.1000], [6.5000, 2.8000, 4.6000, 1.5000], [5.4000, 3.9000, 1.3000, 0.4000], [5.6000, 3.0000, 4.5000, 1.5000], [6.3000, 2.5000, 4.9000, 1.5000], [5.2000, 4.1000, 1.5000, 0.1000], [5.6000, 3.0000, 4.1000, 1.3000], [4.9000, 3.1000, 1.5000, 0.1000], [6.2000, 2.2000, 4.5000, 1.5000], [5.9000, 3.0000, 4.2000, 1.5000], [6.7000, 2.5000, 5.8000, 1.8000], [5.1000, 3.5000, 1.4000, 0.2000], [6.5000, 3.0000, 5.2000, 2.0000], [7.2000, 3.0000, 5.8000, 1.6000], [5.9000, 3.2000, 4.8000, 1.8000], [6.7000, 3.3000, 5.7000, 2.1000], [5.5000, 2.5000, 4.0000, 1.3000], [6.6000, 2.9000, 4.6000, 1.3000], [5.0000, 3.2000, 1.2000, 0.2000], [5.3000, 3.7000, 1.5000, 0.2000], [6.2000, 3.4000, 5.4000, 2.3000], [6.3000, 3.4000, 5.6000, 2.4000], [5.8000, 2.7000, 5.1000, 1.9000], [7.0000, 3.2000, 4.7000, 1.4000], [4.6000, 3.1000, 1.5000, 0.2000], [7.3000, 2.9000, 6.3000, 1.8000], [5.5000, 2.4000, 3.8000, 1.1000], [5.6000, 2.8000, 4.9000, 2.0000], [6.2000, 2.9000, 4.3000, 1.3000], [5.0000, 3.0000, 1.6000, 0.2000], [6.9000, 3.1000, 5.1000, 2.3000], [6.4000, 2.9000, 4.3000, 1.3000], [5.6000, 2.5000, 3.9000, 1.1000], [6.7000, 3.1000, 4.4000, 1.4000], [7.7000, 3.8000, 6.7000, 2.2000], [5.8000, 4.0000, 1.2000, 0.2000], [6.7000, 3.3000, 5.7000, 2.5000], [5.5000, 2.6000, 4.4000, 1.2000], [4.4000, 3.0000, 1.3000, 0.2000], [6.5000, 3.0000, 5.8000, 2.2000], [7.7000, 2.8000, 6.7000, 2.0000], [6.1000, 2.6000, 5.6000, 1.4000], [4.9000, 2.4000, 3.3000, 1.0000], [6.7000, 3.1000, 4.7000, 1.5000], [6.0000, 3.4000, 4.5000, 1.6000], [4.5000, 2.3000, 1.3000, 0.3000], [5.1000, 3.7000, 1.5000, 0.4000], [7.2000, 3.6000, 6.1000, 2.5000], [5.6000, 2.9000, 3.6000, 1.3000], [5.8000, 2.7000, 3.9000, 1.2000], [7.1000, 3.0000, 5.9000, 2.1000], [6.0000, 2.2000, 5.0000, 1.5000], [5.0000, 3.4000, 1.5000, 0.2000], [6.9000, 3.1000, 4.9000, 1.5000], [5.7000, 2.8000, 4.1000, 1.3000], [6.3000, 2.5000, 5.0000, 1.9000], [6.4000, 2.7000, 5.3000, 1.9000], [5.0000, 2.0000, 3.5000, 1.0000], [6.3000, 3.3000, 6.0000, 2.5000], [6.3000, 2.8000, 5.1000, 1.5000], [5.5000, 2.3000, 4.0000, 1.3000], [5.8000, 2.6000, 4.0000, 1.2000], [5.7000, 2.9000, 4.2000, 1.3000], [6.7000, 3.0000, 5.2000, 2.3000], [7.4000, 2.8000, 6.1000, 1.9000], [5.7000, 2.6000, 3.5000, 1.0000], [4.9000, 2.5000, 4.5000, 1.7000], [5.8000, 2.7000, 5.1000, 1.9000], [5.7000, 3.0000, 4.2000, 1.2000], [4.9000, 3.1000, 1.5000, 0.1000], [6.1000, 2.9000, 4.7000, 1.4000], [4.8000, 3.0000, 1.4000, 0.3000], [4.4000, 3.2000, 1.3000, 0.2000], [5.1000, 3.8000, 1.9000, 0.4000], [5.7000, 4.4000, 1.5000, 0.4000]]), tensor([0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 2, 0, 0, 0, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 0, 1, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 1, 2, 1, 1, 0, 0, 2, 2, 2, 1, 0, 2, 1, 2, 1, 0, 2, 1, 1, 1, 2, 0, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 0, 2, 1, 1, 2, 2, 0, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 0, 1, 0, 0, 0, 0])] A Quick Note on Torchvision \u00b6 PyTorch offers another powerful dataset tool called torchvision , which is useful when working with image data. We\u2019ll go into a lot more detail in the Convolutional Neural Network (CNN) section. For now, just know that torchvision offers built-in image datasets like MNIST and CIFAR-10 , as well as tools for transforming images into tensors.","title":"02 DataSets with Pytorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/02-DataSets-with-Pytorch/#pytorch-for-deep-learning-bootcamp","text":"by Jawad Haider Datasets with PyTorch Perform standard imports Loading data from files Plot the data The classic method for building train/test split tensors Using PyTorch\u2019s Dataset and DataLoader classes A Quick Note on Torchvision Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/","title":"Pytorch for Deep Learning BootCamp"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/02-DataSets-with-Pytorch/#datasets-with-pytorch","text":"In this section we\u2019ll show how to: * load data from outside files * build random batches using PyTorch\u2019s data utilities At the end we\u2019ll briefly mention torchvision .","title":"Datasets with PyTorch"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/02-DataSets-with-Pytorch/#perform-standard-imports","text":"import torch import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/02-DataSets-with-Pytorch/#loading-data-from-files","text":"We\u2019ve seen how to load NumPy arrays into PyTorch, and anyone familiar with pandas.read_csv() can use it to prepare data before forming tensors. Here we\u2019ll load the iris flower dataset saved as a .csv file. df = pd . read_csv ( '../Data/iris.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 df . shape (150, 5)","title":"Loading data from files"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/02-DataSets-with-Pytorch/#plot-the-data","text":"fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () The iris dataset consists of 50 samples each from three species of Iris ( Iris setosa , Iris virginica and Iris versicolor ), for 150 total samples. We have four features (sepal length & width, petal length & width) and three unique labels: 0. Iris setosa 1. Iris virginica 2. Iris versicolor","title":"Plot the data"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/02-DataSets-with-Pytorch/#the-classic-method-for-building-traintest-split-tensors","text":"Before introducing PyTorch\u2019s Dataset and DataLoader classes, we\u2019ll take a quick look at the alternative. from sklearn.model_selection import train_test_split train_X , test_X , train_y , test_y = train_test_split ( df . drop ( 'target' , axis = 1 ) . values , df [ 'target' ] . values , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( train_X ) X_test = torch . FloatTensor ( test_X ) y_train = torch . LongTensor ( train_y ) . reshape ( - 1 , 1 ) y_test = torch . LongTensor ( test_y ) . reshape ( - 1 , 1 ) print ( f 'Training size: { len ( y_train ) } ' ) labels , counts = y_train . unique ( return_counts = True ) print ( f 'Labels: { labels } \\n Counts: { counts } ' ) Training size: 120 Labels: tensor([0, 1, 2]) Counts: tensor([42, 42, 36]) NOTE: The importance of a balanced training set is discussed in A systematic study of the class imbalance problem in convolutional neural networks by Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski (10/15/17, latest rev 10/13/18) https://arxiv.org/abs/1710.05381 For example, the authors show that oversampling a less common class so that it matches the more common classes is always the preferred choice. X_train . size () torch.Size([120, 4]) y_train . size () torch.Size([120, 1]) NOTE: It\u2019s up to us to remember which columns correspond to which features.","title":"The classic method for building train/test split tensors"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/02-DataSets-with-Pytorch/#using-pytorchs-dataset-and-dataloader-classes","text":"A far better alternative is to leverage PyTorch\u2019s Dataset and DataLoader classes. Usually, to set up a Dataset specific to our investigation we would define our own custom class that inherits from torch.utils.data.Dataset (we\u2019ll do this in the CNN section). For now, we can use the built-in TensorDataset class. from torch.utils.data import TensorDataset , DataLoader data = df . drop ( 'target' , axis = 1 ) . values labels = df [ 'target' ] . values iris = TensorDataset ( torch . FloatTensor ( data ), torch . LongTensor ( labels )) len ( iris ) 150 type ( iris ) torch.utils.data.dataset.TensorDataset for i in iris : print ( i ) (tensor([5.1000, 3.5000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.0000, 1.4000, 0.2000]), tensor(0)) (tensor([4.7000, 3.2000, 1.3000, 0.2000]), tensor(0)) (tensor([4.6000, 3.1000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.6000, 1.4000, 0.2000]), tensor(0)) (tensor([5.4000, 3.9000, 1.7000, 0.4000]), tensor(0)) (tensor([4.6000, 3.4000, 1.4000, 0.3000]), tensor(0)) (tensor([5.0000, 3.4000, 1.5000, 0.2000]), tensor(0)) (tensor([4.4000, 2.9000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.4000, 3.7000, 1.5000, 0.2000]), tensor(0)) (tensor([4.8000, 3.4000, 1.6000, 0.2000]), tensor(0)) (tensor([4.8000, 3.0000, 1.4000, 0.1000]), tensor(0)) (tensor([4.3000, 3.0000, 1.1000, 0.1000]), tensor(0)) (tensor([5.8000, 4.0000, 1.2000, 0.2000]), tensor(0)) (tensor([5.7000, 4.4000, 1.5000, 0.4000]), tensor(0)) (tensor([5.4000, 3.9000, 1.3000, 0.4000]), tensor(0)) (tensor([5.1000, 3.5000, 1.4000, 0.3000]), tensor(0)) (tensor([5.7000, 3.8000, 1.7000, 0.3000]), tensor(0)) (tensor([5.1000, 3.8000, 1.5000, 0.3000]), tensor(0)) (tensor([5.4000, 3.4000, 1.7000, 0.2000]), tensor(0)) (tensor([5.1000, 3.7000, 1.5000, 0.4000]), tensor(0)) (tensor([4.6000, 3.6000, 1.0000, 0.2000]), tensor(0)) (tensor([5.1000, 3.3000, 1.7000, 0.5000]), tensor(0)) (tensor([4.8000, 3.4000, 1.9000, 0.2000]), tensor(0)) (tensor([5.0000, 3.0000, 1.6000, 0.2000]), tensor(0)) (tensor([5.0000, 3.4000, 1.6000, 0.4000]), tensor(0)) (tensor([5.2000, 3.5000, 1.5000, 0.2000]), tensor(0)) (tensor([5.2000, 3.4000, 1.4000, 0.2000]), tensor(0)) (tensor([4.7000, 3.2000, 1.6000, 0.2000]), tensor(0)) (tensor([4.8000, 3.1000, 1.6000, 0.2000]), tensor(0)) (tensor([5.4000, 3.4000, 1.5000, 0.4000]), tensor(0)) (tensor([5.2000, 4.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.5000, 4.2000, 1.4000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([5.0000, 3.2000, 1.2000, 0.2000]), tensor(0)) (tensor([5.5000, 3.5000, 1.3000, 0.2000]), tensor(0)) (tensor([4.9000, 3.1000, 1.5000, 0.1000]), tensor(0)) (tensor([4.4000, 3.0000, 1.3000, 0.2000]), tensor(0)) (tensor([5.1000, 3.4000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.5000, 1.3000, 0.3000]), tensor(0)) (tensor([4.5000, 2.3000, 1.3000, 0.3000]), tensor(0)) (tensor([4.4000, 3.2000, 1.3000, 0.2000]), tensor(0)) (tensor([5.0000, 3.5000, 1.6000, 0.6000]), tensor(0)) (tensor([5.1000, 3.8000, 1.9000, 0.4000]), tensor(0)) (tensor([4.8000, 3.0000, 1.4000, 0.3000]), tensor(0)) (tensor([5.1000, 3.8000, 1.6000, 0.2000]), tensor(0)) (tensor([4.6000, 3.2000, 1.4000, 0.2000]), tensor(0)) (tensor([5.3000, 3.7000, 1.5000, 0.2000]), tensor(0)) (tensor([5.0000, 3.3000, 1.4000, 0.2000]), tensor(0)) (tensor([7.0000, 3.2000, 4.7000, 1.4000]), tensor(1)) (tensor([6.4000, 3.2000, 4.5000, 1.5000]), tensor(1)) (tensor([6.9000, 3.1000, 4.9000, 1.5000]), tensor(1)) (tensor([5.5000, 2.3000, 4.0000, 1.3000]), tensor(1)) (tensor([6.5000, 2.8000, 4.6000, 1.5000]), tensor(1)) (tensor([5.7000, 2.8000, 4.5000, 1.3000]), tensor(1)) (tensor([6.3000, 3.3000, 4.7000, 1.6000]), tensor(1)) (tensor([4.9000, 2.4000, 3.3000, 1.0000]), tensor(1)) (tensor([6.6000, 2.9000, 4.6000, 1.3000]), tensor(1)) (tensor([5.2000, 2.7000, 3.9000, 1.4000]), tensor(1)) (tensor([5.0000, 2.0000, 3.5000, 1.0000]), tensor(1)) (tensor([5.9000, 3.0000, 4.2000, 1.5000]), tensor(1)) (tensor([6.0000, 2.2000, 4.0000, 1.0000]), tensor(1)) (tensor([6.1000, 2.9000, 4.7000, 1.4000]), tensor(1)) (tensor([5.6000, 2.9000, 3.6000, 1.3000]), tensor(1)) (tensor([6.7000, 3.1000, 4.4000, 1.4000]), tensor(1)) (tensor([5.6000, 3.0000, 4.5000, 1.5000]), tensor(1)) (tensor([5.8000, 2.7000, 4.1000, 1.0000]), tensor(1)) (tensor([6.2000, 2.2000, 4.5000, 1.5000]), tensor(1)) (tensor([5.6000, 2.5000, 3.9000, 1.1000]), tensor(1)) (tensor([5.9000, 3.2000, 4.8000, 1.8000]), tensor(1)) (tensor([6.1000, 2.8000, 4.0000, 1.3000]), tensor(1)) (tensor([6.3000, 2.5000, 4.9000, 1.5000]), tensor(1)) (tensor([6.1000, 2.8000, 4.7000, 1.2000]), tensor(1)) (tensor([6.4000, 2.9000, 4.3000, 1.3000]), tensor(1)) (tensor([6.6000, 3.0000, 4.4000, 1.4000]), tensor(1)) (tensor([6.8000, 2.8000, 4.8000, 1.4000]), tensor(1)) (tensor([6.7000, 3.0000, 5.0000, 1.7000]), tensor(1)) (tensor([6.0000, 2.9000, 4.5000, 1.5000]), tensor(1)) (tensor([5.7000, 2.6000, 3.5000, 1.0000]), tensor(1)) (tensor([5.5000, 2.4000, 3.8000, 1.1000]), tensor(1)) (tensor([5.5000, 2.4000, 3.7000, 1.0000]), tensor(1)) (tensor([5.8000, 2.7000, 3.9000, 1.2000]), tensor(1)) (tensor([6.0000, 2.7000, 5.1000, 1.6000]), tensor(1)) (tensor([5.4000, 3.0000, 4.5000, 1.5000]), tensor(1)) (tensor([6.0000, 3.4000, 4.5000, 1.6000]), tensor(1)) (tensor([6.7000, 3.1000, 4.7000, 1.5000]), tensor(1)) (tensor([6.3000, 2.3000, 4.4000, 1.3000]), tensor(1)) (tensor([5.6000, 3.0000, 4.1000, 1.3000]), tensor(1)) (tensor([5.5000, 2.5000, 4.0000, 1.3000]), tensor(1)) (tensor([5.5000, 2.6000, 4.4000, 1.2000]), tensor(1)) (tensor([6.1000, 3.0000, 4.6000, 1.4000]), tensor(1)) (tensor([5.8000, 2.6000, 4.0000, 1.2000]), tensor(1)) (tensor([5.0000, 2.3000, 3.3000, 1.0000]), tensor(1)) (tensor([5.6000, 2.7000, 4.2000, 1.3000]), tensor(1)) (tensor([5.7000, 3.0000, 4.2000, 1.2000]), tensor(1)) (tensor([5.7000, 2.9000, 4.2000, 1.3000]), tensor(1)) (tensor([6.2000, 2.9000, 4.3000, 1.3000]), tensor(1)) (tensor([5.1000, 2.5000, 3.0000, 1.1000]), tensor(1)) (tensor([5.7000, 2.8000, 4.1000, 1.3000]), tensor(1)) (tensor([6.3000, 3.3000, 6.0000, 2.5000]), tensor(2)) (tensor([5.8000, 2.7000, 5.1000, 1.9000]), tensor(2)) (tensor([7.1000, 3.0000, 5.9000, 2.1000]), tensor(2)) (tensor([6.3000, 2.9000, 5.6000, 1.8000]), tensor(2)) (tensor([6.5000, 3.0000, 5.8000, 2.2000]), tensor(2)) (tensor([7.6000, 3.0000, 6.6000, 2.1000]), tensor(2)) (tensor([4.9000, 2.5000, 4.5000, 1.7000]), tensor(2)) (tensor([7.3000, 2.9000, 6.3000, 1.8000]), tensor(2)) (tensor([6.7000, 2.5000, 5.8000, 1.8000]), tensor(2)) (tensor([7.2000, 3.6000, 6.1000, 2.5000]), tensor(2)) (tensor([6.5000, 3.2000, 5.1000, 2.0000]), tensor(2)) (tensor([6.4000, 2.7000, 5.3000, 1.9000]), tensor(2)) (tensor([6.8000, 3.0000, 5.5000, 2.1000]), tensor(2)) (tensor([5.7000, 2.5000, 5.0000, 2.0000]), tensor(2)) (tensor([5.8000, 2.8000, 5.1000, 2.4000]), tensor(2)) (tensor([6.4000, 3.2000, 5.3000, 2.3000]), tensor(2)) (tensor([6.5000, 3.0000, 5.5000, 1.8000]), tensor(2)) (tensor([7.7000, 3.8000, 6.7000, 2.2000]), tensor(2)) (tensor([7.7000, 2.6000, 6.9000, 2.3000]), tensor(2)) (tensor([6.0000, 2.2000, 5.0000, 1.5000]), tensor(2)) (tensor([6.9000, 3.2000, 5.7000, 2.3000]), tensor(2)) (tensor([5.6000, 2.8000, 4.9000, 2.0000]), tensor(2)) (tensor([7.7000, 2.8000, 6.7000, 2.0000]), tensor(2)) (tensor([6.3000, 2.7000, 4.9000, 1.8000]), tensor(2)) (tensor([6.7000, 3.3000, 5.7000, 2.1000]), tensor(2)) (tensor([7.2000, 3.2000, 6.0000, 1.8000]), tensor(2)) (tensor([6.2000, 2.8000, 4.8000, 1.8000]), tensor(2)) (tensor([6.1000, 3.0000, 4.9000, 1.8000]), tensor(2)) (tensor([6.4000, 2.8000, 5.6000, 2.1000]), tensor(2)) (tensor([7.2000, 3.0000, 5.8000, 1.6000]), tensor(2)) (tensor([7.4000, 2.8000, 6.1000, 1.9000]), tensor(2)) (tensor([7.9000, 3.8000, 6.4000, 2.0000]), tensor(2)) (tensor([6.4000, 2.8000, 5.6000, 2.2000]), tensor(2)) (tensor([6.3000, 2.8000, 5.1000, 1.5000]), tensor(2)) (tensor([6.1000, 2.6000, 5.6000, 1.4000]), tensor(2)) (tensor([7.7000, 3.0000, 6.1000, 2.3000]), tensor(2)) (tensor([6.3000, 3.4000, 5.6000, 2.4000]), tensor(2)) (tensor([6.4000, 3.1000, 5.5000, 1.8000]), tensor(2)) (tensor([6.0000, 3.0000, 4.8000, 1.8000]), tensor(2)) (tensor([6.9000, 3.1000, 5.4000, 2.1000]), tensor(2)) (tensor([6.7000, 3.1000, 5.6000, 2.4000]), tensor(2)) (tensor([6.9000, 3.1000, 5.1000, 2.3000]), tensor(2)) (tensor([5.8000, 2.7000, 5.1000, 1.9000]), tensor(2)) (tensor([6.8000, 3.2000, 5.9000, 2.3000]), tensor(2)) (tensor([6.7000, 3.3000, 5.7000, 2.5000]), tensor(2)) (tensor([6.7000, 3.0000, 5.2000, 2.3000]), tensor(2)) (tensor([6.3000, 2.5000, 5.0000, 1.9000]), tensor(2)) (tensor([6.5000, 3.0000, 5.2000, 2.0000]), tensor(2)) (tensor([6.2000, 3.4000, 5.4000, 2.3000]), tensor(2)) (tensor([5.9000, 3.0000, 5.1000, 1.8000]), tensor(2)) Once we have a dataset we can wrap it with a DataLoader. This gives us a powerful sampler that provides single- or multi-process iterators over the dataset. iris_loader = DataLoader ( iris , batch_size = 105 , shuffle = True ) for i_batch , sample_batched in enumerate ( iris_loader ): print ( i_batch , sample_batched ) 0 [tensor([[6.7000, 3.1000, 4.4000, 1.4000], [4.8000, 3.4000, 1.6000, 0.2000], [4.8000, 3.4000, 1.9000, 0.2000], [5.6000, 2.9000, 3.6000, 1.3000], [6.7000, 3.3000, 5.7000, 2.1000], [6.9000, 3.1000, 4.9000, 1.5000], [6.7000, 2.5000, 5.8000, 1.8000], [6.4000, 3.1000, 5.5000, 1.8000], [6.7000, 3.1000, 5.6000, 2.4000], [5.5000, 2.5000, 4.0000, 1.3000], [7.0000, 3.2000, 4.7000, 1.4000], [5.5000, 4.2000, 1.4000, 0.2000], [7.3000, 2.9000, 6.3000, 1.8000], [7.7000, 2.8000, 6.7000, 2.0000], [4.9000, 3.1000, 1.5000, 0.1000], [7.2000, 3.0000, 5.8000, 1.6000], [6.7000, 3.1000, 4.7000, 1.5000], [5.8000, 2.7000, 3.9000, 1.2000], [5.5000, 2.4000, 3.8000, 1.1000], [5.0000, 3.5000, 1.6000, 0.6000], [5.1000, 3.8000, 1.6000, 0.2000], [4.8000, 3.0000, 1.4000, 0.1000], [6.5000, 3.0000, 5.5000, 1.8000], [6.7000, 3.0000, 5.2000, 2.3000], [6.8000, 2.8000, 4.8000, 1.4000], [7.4000, 2.8000, 6.1000, 1.9000], [5.0000, 3.4000, 1.6000, 0.4000], [6.3000, 3.3000, 6.0000, 2.5000], [5.7000, 2.8000, 4.1000, 1.3000], [5.1000, 3.8000, 1.9000, 0.4000], [6.6000, 2.9000, 4.6000, 1.3000], [6.3000, 3.4000, 5.6000, 2.4000], [5.0000, 3.2000, 1.2000, 0.2000], [5.9000, 3.2000, 4.8000, 1.8000], [4.7000, 3.2000, 1.6000, 0.2000], [5.1000, 3.8000, 1.5000, 0.3000], [5.7000, 2.6000, 3.5000, 1.0000], [5.7000, 4.4000, 1.5000, 0.4000], [5.0000, 2.0000, 3.5000, 1.0000], [4.4000, 3.2000, 1.3000, 0.2000], [5.2000, 3.4000, 1.4000, 0.2000], [5.5000, 2.3000, 4.0000, 1.3000], [7.6000, 3.0000, 6.6000, 2.1000], [4.4000, 2.9000, 1.4000, 0.2000], [5.7000, 3.8000, 1.7000, 0.3000], [7.7000, 3.0000, 6.1000, 2.3000], [4.9000, 2.5000, 4.5000, 1.7000], [5.9000, 3.0000, 5.1000, 1.8000], [7.2000, 3.6000, 6.1000, 2.5000], [5.8000, 2.8000, 5.1000, 2.4000], [4.7000, 3.2000, 1.3000, 0.2000], [6.2000, 3.4000, 5.4000, 2.3000], [5.7000, 3.0000, 4.2000, 1.2000], [5.6000, 2.7000, 4.2000, 1.3000], [5.2000, 4.1000, 1.5000, 0.1000], [5.1000, 3.5000, 1.4000, 0.3000], [5.0000, 3.0000, 1.6000, 0.2000], [6.3000, 2.3000, 4.4000, 1.3000], [6.5000, 3.2000, 5.1000, 2.0000], [5.6000, 2.8000, 4.9000, 2.0000], [5.4000, 3.4000, 1.7000, 0.2000], [5.9000, 3.0000, 4.2000, 1.5000], [6.2000, 2.2000, 4.5000, 1.5000], [5.1000, 3.4000, 1.5000, 0.2000], [6.9000, 3.1000, 5.4000, 2.1000], [4.6000, 3.2000, 1.4000, 0.2000], [5.8000, 2.7000, 4.1000, 1.0000], [5.8000, 2.7000, 5.1000, 1.9000], [6.0000, 2.2000, 4.0000, 1.0000], [6.3000, 2.7000, 4.9000, 1.8000], [7.1000, 3.0000, 5.9000, 2.1000], [6.3000, 2.9000, 5.6000, 1.8000], [4.6000, 3.1000, 1.5000, 0.2000], [4.4000, 3.0000, 1.3000, 0.2000], [5.5000, 2.6000, 4.4000, 1.2000], [5.4000, 3.4000, 1.5000, 0.4000], [4.9000, 2.4000, 3.3000, 1.0000], [6.2000, 2.8000, 4.8000, 1.8000], [7.2000, 3.2000, 6.0000, 1.8000], [6.3000, 3.3000, 4.7000, 1.6000], [5.6000, 3.0000, 4.5000, 1.5000], [6.0000, 2.7000, 5.1000, 1.6000], [6.0000, 2.2000, 5.0000, 1.5000], [6.4000, 2.9000, 4.3000, 1.3000], [5.8000, 2.6000, 4.0000, 1.2000], [6.9000, 3.1000, 5.1000, 2.3000], [5.6000, 3.0000, 4.1000, 1.3000], [5.4000, 3.9000, 1.3000, 0.4000], [5.3000, 3.7000, 1.5000, 0.2000], [6.3000, 2.5000, 4.9000, 1.5000], [5.0000, 3.6000, 1.4000, 0.2000], [5.1000, 3.3000, 1.7000, 0.5000], [6.1000, 2.8000, 4.7000, 1.2000], [6.2000, 2.9000, 4.3000, 1.3000], [6.7000, 3.0000, 5.0000, 1.7000], [6.1000, 2.6000, 5.6000, 1.4000], [6.4000, 2.7000, 5.3000, 1.9000], [4.5000, 2.3000, 1.3000, 0.3000], [6.1000, 2.8000, 4.0000, 1.3000], [5.4000, 3.0000, 4.5000, 1.5000], [6.5000, 3.0000, 5.2000, 2.0000], [6.0000, 3.0000, 4.8000, 1.8000], [5.0000, 3.5000, 1.3000, 0.3000], [6.5000, 3.0000, 5.8000, 2.2000], [5.0000, 3.3000, 1.4000, 0.2000]]), tensor([1, 0, 0, 1, 2, 1, 2, 2, 2, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 0, 0, 0, 2, 2, 1, 2, 0, 2, 1, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 1, 1, 0, 0, 0, 1, 2, 2, 0, 1, 1, 0, 2, 0, 1, 2, 1, 2, 2, 2, 0, 0, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 1, 2, 2, 0, 1, 1, 2, 2, 0, 2, 0])] 1 [tensor([[5.5000, 2.4000, 3.7000, 1.0000], [6.4000, 3.2000, 4.5000, 1.5000], [5.6000, 2.5000, 3.9000, 1.1000], [5.1000, 3.7000, 1.5000, 0.4000], [5.0000, 2.3000, 3.3000, 1.0000], [5.0000, 3.4000, 1.5000, 0.2000], [5.7000, 2.9000, 4.2000, 1.3000], [6.1000, 3.0000, 4.9000, 1.8000], [5.1000, 2.5000, 3.0000, 1.1000], [6.0000, 2.9000, 4.5000, 1.5000], [4.3000, 3.0000, 1.1000, 0.1000], [6.4000, 2.8000, 5.6000, 2.1000], [5.1000, 3.5000, 1.4000, 0.2000], [5.7000, 2.8000, 4.5000, 1.3000], [4.9000, 3.1000, 1.5000, 0.1000], [7.7000, 2.6000, 6.9000, 2.3000], [6.6000, 3.0000, 4.4000, 1.4000], [4.8000, 3.0000, 1.4000, 0.3000], [6.4000, 2.8000, 5.6000, 2.2000], [6.8000, 3.2000, 5.9000, 2.3000], [5.4000, 3.7000, 1.5000, 0.2000], [4.9000, 3.1000, 1.5000, 0.1000], [6.9000, 3.2000, 5.7000, 2.3000], [5.8000, 2.7000, 5.1000, 1.9000], [6.1000, 2.9000, 4.7000, 1.4000], [4.6000, 3.6000, 1.0000, 0.2000], [5.2000, 3.5000, 1.5000, 0.2000], [6.8000, 3.0000, 5.5000, 2.1000], [7.7000, 3.8000, 6.7000, 2.2000], [6.0000, 3.4000, 4.5000, 1.6000], [5.7000, 2.5000, 5.0000, 2.0000], [6.5000, 2.8000, 4.6000, 1.5000], [4.6000, 3.4000, 1.4000, 0.3000], [5.2000, 2.7000, 3.9000, 1.4000], [5.5000, 3.5000, 1.3000, 0.2000], [4.9000, 3.0000, 1.4000, 0.2000], [6.3000, 2.5000, 5.0000, 1.9000], [6.1000, 3.0000, 4.6000, 1.4000], [6.4000, 3.2000, 5.3000, 2.3000], [5.8000, 4.0000, 1.2000, 0.2000], [6.3000, 2.8000, 5.1000, 1.5000], [4.8000, 3.1000, 1.6000, 0.2000], [6.7000, 3.3000, 5.7000, 2.5000], [5.4000, 3.9000, 1.7000, 0.4000], [7.9000, 3.8000, 6.4000, 2.0000]]), tensor([1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 2, 1, 0, 2, 2, 0, 0, 2, 2, 1, 0, 0, 2, 2, 1, 2, 1, 0, 1, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2])] list ( iris_loader )[ 0 ][ 1 ] . bincount () tensor([30, 36, 39]) next ( iter ( iris_loader )) [tensor([[5.4000, 3.7000, 1.5000, 0.2000], [4.7000, 3.2000, 1.3000, 0.2000], [6.1000, 3.0000, 4.6000, 1.4000], [4.3000, 3.0000, 1.1000, 0.1000], [5.0000, 3.5000, 1.3000, 0.3000], [7.2000, 3.2000, 6.0000, 1.8000], [4.8000, 3.4000, 1.9000, 0.2000], [6.4000, 3.1000, 5.5000, 1.8000], [6.6000, 3.0000, 4.4000, 1.4000], [6.8000, 3.2000, 5.9000, 2.3000], [6.4000, 3.2000, 4.5000, 1.5000], [5.0000, 2.3000, 3.3000, 1.0000], [6.0000, 2.2000, 4.0000, 1.0000], [6.7000, 3.1000, 5.6000, 2.4000], [6.0000, 2.7000, 5.1000, 1.6000], [6.2000, 2.8000, 4.8000, 1.8000], [5.4000, 3.4000, 1.7000, 0.2000], [5.4000, 3.9000, 1.7000, 0.4000], [4.6000, 3.2000, 1.4000, 0.2000], [5.2000, 2.7000, 3.9000, 1.4000], [6.0000, 3.0000, 4.8000, 1.8000], [5.7000, 2.8000, 4.5000, 1.3000], [7.7000, 2.6000, 6.9000, 2.3000], [5.2000, 3.5000, 1.5000, 0.2000], [6.4000, 2.8000, 5.6000, 2.1000], [5.7000, 3.8000, 1.7000, 0.3000], [6.3000, 2.7000, 4.9000, 1.8000], [6.8000, 2.8000, 4.8000, 1.4000], [6.5000, 3.2000, 5.1000, 2.0000], [6.9000, 3.2000, 5.7000, 2.3000], [7.6000, 3.0000, 6.6000, 2.1000], [6.5000, 2.8000, 4.6000, 1.5000], [5.4000, 3.9000, 1.3000, 0.4000], [5.6000, 3.0000, 4.5000, 1.5000], [6.3000, 2.5000, 4.9000, 1.5000], [5.2000, 4.1000, 1.5000, 0.1000], [5.6000, 3.0000, 4.1000, 1.3000], [4.9000, 3.1000, 1.5000, 0.1000], [6.2000, 2.2000, 4.5000, 1.5000], [5.9000, 3.0000, 4.2000, 1.5000], [6.7000, 2.5000, 5.8000, 1.8000], [5.1000, 3.5000, 1.4000, 0.2000], [6.5000, 3.0000, 5.2000, 2.0000], [7.2000, 3.0000, 5.8000, 1.6000], [5.9000, 3.2000, 4.8000, 1.8000], [6.7000, 3.3000, 5.7000, 2.1000], [5.5000, 2.5000, 4.0000, 1.3000], [6.6000, 2.9000, 4.6000, 1.3000], [5.0000, 3.2000, 1.2000, 0.2000], [5.3000, 3.7000, 1.5000, 0.2000], [6.2000, 3.4000, 5.4000, 2.3000], [6.3000, 3.4000, 5.6000, 2.4000], [5.8000, 2.7000, 5.1000, 1.9000], [7.0000, 3.2000, 4.7000, 1.4000], [4.6000, 3.1000, 1.5000, 0.2000], [7.3000, 2.9000, 6.3000, 1.8000], [5.5000, 2.4000, 3.8000, 1.1000], [5.6000, 2.8000, 4.9000, 2.0000], [6.2000, 2.9000, 4.3000, 1.3000], [5.0000, 3.0000, 1.6000, 0.2000], [6.9000, 3.1000, 5.1000, 2.3000], [6.4000, 2.9000, 4.3000, 1.3000], [5.6000, 2.5000, 3.9000, 1.1000], [6.7000, 3.1000, 4.4000, 1.4000], [7.7000, 3.8000, 6.7000, 2.2000], [5.8000, 4.0000, 1.2000, 0.2000], [6.7000, 3.3000, 5.7000, 2.5000], [5.5000, 2.6000, 4.4000, 1.2000], [4.4000, 3.0000, 1.3000, 0.2000], [6.5000, 3.0000, 5.8000, 2.2000], [7.7000, 2.8000, 6.7000, 2.0000], [6.1000, 2.6000, 5.6000, 1.4000], [4.9000, 2.4000, 3.3000, 1.0000], [6.7000, 3.1000, 4.7000, 1.5000], [6.0000, 3.4000, 4.5000, 1.6000], [4.5000, 2.3000, 1.3000, 0.3000], [5.1000, 3.7000, 1.5000, 0.4000], [7.2000, 3.6000, 6.1000, 2.5000], [5.6000, 2.9000, 3.6000, 1.3000], [5.8000, 2.7000, 3.9000, 1.2000], [7.1000, 3.0000, 5.9000, 2.1000], [6.0000, 2.2000, 5.0000, 1.5000], [5.0000, 3.4000, 1.5000, 0.2000], [6.9000, 3.1000, 4.9000, 1.5000], [5.7000, 2.8000, 4.1000, 1.3000], [6.3000, 2.5000, 5.0000, 1.9000], [6.4000, 2.7000, 5.3000, 1.9000], [5.0000, 2.0000, 3.5000, 1.0000], [6.3000, 3.3000, 6.0000, 2.5000], [6.3000, 2.8000, 5.1000, 1.5000], [5.5000, 2.3000, 4.0000, 1.3000], [5.8000, 2.6000, 4.0000, 1.2000], [5.7000, 2.9000, 4.2000, 1.3000], [6.7000, 3.0000, 5.2000, 2.3000], [7.4000, 2.8000, 6.1000, 1.9000], [5.7000, 2.6000, 3.5000, 1.0000], [4.9000, 2.5000, 4.5000, 1.7000], [5.8000, 2.7000, 5.1000, 1.9000], [5.7000, 3.0000, 4.2000, 1.2000], [4.9000, 3.1000, 1.5000, 0.1000], [6.1000, 2.9000, 4.7000, 1.4000], [4.8000, 3.0000, 1.4000, 0.3000], [4.4000, 3.2000, 1.3000, 0.2000], [5.1000, 3.8000, 1.9000, 0.4000], [5.7000, 4.4000, 1.5000, 0.4000]]), tensor([0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 2, 0, 0, 0, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 0, 1, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 1, 2, 1, 1, 0, 0, 2, 2, 2, 1, 0, 2, 1, 2, 1, 0, 2, 1, 1, 1, 2, 0, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 0, 2, 1, 1, 2, 2, 0, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 0, 1, 0, 0, 0, 0])]","title":"Using PyTorch\u2019s Dataset and DataLoader classes"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/02-DataSets-with-Pytorch/#a-quick-note-on-torchvision","text":"PyTorch offers another powerful dataset tool called torchvision , which is useful when working with image data. We\u2019ll go into a lot more detail in the Convolutional Neural Network (CNN) section. For now, just know that torchvision offers built-in image datasets like MNIST and CIFAR-10 , as well as tools for transforming images into tensors.","title":"A Quick Note on Torchvision"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/","text":"Pytorch for Deep Learning BootCamp \u00b6 by Jawad Haider 03 - Basic PyTorch Neural Network \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Basic PyTorch Neural Network Perform standard imports Create a model class Load the iris dataset Plot the dataset Perform Train/Test/Split Prepare DataLoader Define loss equations and optimizations Train the model Plot the loss function Validate the model Save the trained model to a file Save the model Load a new model Apply the model to classify new, unseen data Great job! Basic PyTorch Neural Network \u00b6 Now it\u2019s time to put the pieces together. In this section we\u2019ll: * create a multi-layer deep learning model * load data * train and validate the model We\u2019ll also introduce a new step: * save and load a trained model Our goal is to develop a model capable of classifying an iris plant based on four features. This is a multi-class classification where each sample can belong to ONE of 3 classes ( Iris setosa , Iris virginica or Iris versicolor ). The network will have 4 input neurons (flower dimensions) and 3 output neurons (scores). Our loss function will compare the target label (ground truth) to the corresponding output score. NOTE: Multi-class classifications usually involve converting the target vector to a one_hot encoded matrix. That is, if 5 labels show up as tensor([0,2,1,0,1]) then we would encode them as: tensor([[1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0]]) This is easily accomplished with torch.nn.functional.one_hot() . However, our loss function torch.nn.CrossEntropyLoss() takes care of this for us. Perform standard imports \u00b6 import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader from sklearn.model_selection import train_test_split import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Create a model class \u00b6 For this exercise we\u2019re using the Iris dataset. Since a single straight line can\u2019t classify three flowers we should include at least one hidden layer in our model. In the forward section we\u2019ll use the rectified linear unit (ReLU) function as our activation function. This is available as a full module torch.nn.ReLU or as just a functional call torch.nn.functional.relu class Model ( nn . Module ): def __init__ ( self , in_features = 4 , h1 = 8 , h2 = 9 , out_features = 3 ): super () . __init__ () self . fc1 = nn . Linear ( in_features , h1 ) # input layer self . fc2 = nn . Linear ( h1 , h2 ) # hidden layer self . out = nn . Linear ( h2 , out_features ) # output layer def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . out ( x ) return x # Instantiate the Model class using parameter defaults: torch . manual_seed ( 32 ) model = Model () Load the iris dataset \u00b6 df = pd . read_csv ( '../Data/iris.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 Plot the dataset \u00b6 The iris dataset has 4 features. To get an idea how they correlate we can plot four different relationships among them. We\u2019ll use the index positions of the columns to grab their names in pairs with plots = [(0,1),(2,3),(0,2),(1,3)] . Here (0,1) sets \u201csepal length (cm)\u201d as x and \u201csepal width (cm)\u201d as y fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () Perform Train/Test/Split \u00b6 X = df . drop ( 'target' , axis = 1 ) . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( X_train ) X_test = torch . FloatTensor ( X_test ) # y_train = F.one_hot(torch.LongTensor(y_train)) # not needed with Cross Entropy Loss # y_test = F.one_hot(torch.LongTensor(y_test)) y_train = torch . LongTensor ( y_train ) y_test = torch . LongTensor ( y_test ) Prepare DataLoader \u00b6 For this analysis we don\u2019t need to create a Dataset object, but we should take advantage of PyTorch\u2019s DataLoader tool. Even though our dataset is small (120 training samples), we\u2019ll load it into our model in two batches. This technique becomes very helpful with large datasets. Note that scikit-learn already shuffled the source dataset before preparing train and test sets. We\u2019ll still benefit from the DataLoader shuffle utility for model training if we make multiple passes throught the dataset. trainloader = DataLoader ( X_train , batch_size = 60 , shuffle = True ) testloader = DataLoader ( X_test , batch_size = 60 , shuffle = False ) Define loss equations and optimizations \u00b6 As before, we\u2019ll utilize Cross Entropy with torch.nn.CrossEntropyLoss() For the optimizer, we\u2019ll use a variation of Stochastic Gradient Descent called Adam (short for Adaptive Moment Estimation), with torch.optim.Adam() # FOR REDO torch . manual_seed ( 4 ) model = Model () criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 ) Train the model \u00b6 epochs = 100 losses = [] for i in range ( epochs ): i += 1 y_pred = model . forward ( X_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 10 == 1 : print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () epoch: 1 loss: 1.09568226 epoch: 11 loss: 0.98190653 epoch: 21 loss: 0.75652307 epoch: 31 loss: 0.49447522 epoch: 41 loss: 0.34981874 epoch: 51 loss: 0.22807853 epoch: 61 loss: 0.13547322 epoch: 71 loss: 0.09162075 epoch: 81 loss: 0.07378192 epoch: 91 loss: 0.06546164 Plot the loss function \u00b6 plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'epoch' ); Validate the model \u00b6 Now we run the test set through the model to see if the loss calculation resembles the training data. # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( f ' { loss : .8f } ' ) 0.06246195 correct = 0 with torch . no_grad (): for i , data in enumerate ( X_test ): y_val = model . forward ( data ) print ( f ' { i + 1 : 2 } . { str ( y_val ) : 38 } { y_test [ i ] } ' ) if y_val . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { len ( y_test ) } = { 100 * correct / len ( y_test ) : .2f } % correct' ) 1. tensor([-0.3355, 7.3630, 1.3783]) 1 2. tensor([0.2775, 8.1554, 0.4269]) 1 3. tensor([ 11.9969, 6.1847, -19.1976]) 0 4. tensor([-2.0187, 7.9664, 4.2447]) 1 5. tensor([-6.1348, 7.9516, 11.0913]) 2 6. tensor([-10.2635, 8.3101, 17.9998]) 2 7. tensor([ 12.0542, 6.4321, -19.2909]) 0 8. tensor([ 12.9507, 6.4819, -20.7540]) 0 9. tensor([-5.7723, 8.2435, 10.5083]) 2 10. tensor([-7.8867, 8.6126, 14.0731]) 2 11. tensor([-8.7055, 8.6074, 15.4337]) 2 12. tensor([ 11.6358, 5.8167, -18.6220]) 0 13. tensor([-8.1009, 8.2331, 14.3888]) 2 14. tensor([-2.0791, 7.7752, 4.3188]) 1 15. tensor([-6.0828, 8.3916, 11.0586]) 2 16. tensor([0.1360, 7.8660, 0.6409]) 1 17. tensor([-4.0875, 7.7217, 7.6642]) 2 18. tensor([ 13.1522, 6.5911, -21.0798]) 0 19. tensor([-1.5644, 8.0222, 3.4754]) 1 20. tensor([-6.2859, 8.9728, 11.4248]) 2 21. tensor([ 12.3859, 6.2571, -19.8275]) 0 22. tensor([ 13.8200, 7.0859, -22.1528]) 0 23. tensor([-8.8470, 8.3180, 15.6476]) 2 24. tensor([ 12.1979, 6.1264, -19.5260]) 0 25. tensor([-5.8084, 7.5468, 10.5340]) 2 26. tensor([-4.4526, 7.7876, 8.2865]) 2 27. tensor([-1.4284, 7.7786, 3.2328]) 1 28. tensor([ 0.5356, 7.5360, -0.0492]) 1 29. tensor([-5.8230, 8.1573, 10.5975]) 2 30. tensor([-5.2569, 7.7476, 9.6105]) 2 29 out of 30 = 96.67% correct Here we can see that #17 was misclassified. Save the trained model to a file \u00b6 Right now model has been trained and validated, and seems to correctly classify an iris 97% of the time. Let\u2019s save this to disk. The tools we\u2019ll use are torch.save() and torch.load() There are two basic ways to save a model. The first saves/loads the state_dict (learned parameters) of the model, but not the model class. The syntax follows: Save: torch.save(model.state_dict(), PATH) Load: model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.eval() The second saves the entire model including its class and parameters as a pickle file. Care must be taken if you want to load this into another notebook to make sure all the target data is brought in properly. Save: torch.save(model, PATH) Load: model = torch.load(PATH)) model.eval() In either method, you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results. For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html Save the model \u00b6 torch . save ( model . state_dict (), 'IrisDatasetModel.pt' ) Load a new model \u00b6 We\u2019ll load a new model object and test it as we had before to make sure it worked. new_model = Model () new_model . load_state_dict ( torch . load ( 'IrisDatasetModel.pt' )) new_model . eval () Model( (fc1): Linear(in_features=4, out_features=8, bias=True) (fc2): Linear(in_features=8, out_features=9, bias=True) (out): Linear(in_features=9, out_features=3, bias=True) ) with torch . no_grad (): y_val = new_model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( f ' { loss : .8f } ' ) 0.06246195 Apply the model to classify new, unseen data \u00b6 mystery_iris = torch . tensor ([ 5.6 , 3.7 , 2.2 , 0.5 ]) Let\u2019s plot this new iris in yellow to see where it falls in relation to the others: fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' , 'Mystery iris' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) # Add a plot for our mystery iris: ax . scatter ( mystery_iris [ plots [ i ][ 0 ]], mystery_iris [ plots [ i ][ 1 ]], color = 'y' ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () Now run it through the model: with torch . no_grad (): print ( new_model ( mystery_iris )) print () print ( labels [ new_model ( mystery_iris ) . argmax ()]) tensor([ 12.2116, 7.1285, -19.5247]) Iris setosa Great job! \u00b6","title":"03 Basic PyTorch NN"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#pytorch-for-deep-learning-bootcamp","text":"by Jawad Haider","title":"Pytorch for Deep Learning BootCamp"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#03-basic-pytorch-neural-network","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Basic PyTorch Neural Network Perform standard imports Create a model class Load the iris dataset Plot the dataset Perform Train/Test/Split Prepare DataLoader Define loss equations and optimizations Train the model Plot the loss function Validate the model Save the trained model to a file Save the model Load a new model Apply the model to classify new, unseen data Great job!","title":"03 - Basic PyTorch Neural Network"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#basic-pytorch-neural-network","text":"Now it\u2019s time to put the pieces together. In this section we\u2019ll: * create a multi-layer deep learning model * load data * train and validate the model We\u2019ll also introduce a new step: * save and load a trained model Our goal is to develop a model capable of classifying an iris plant based on four features. This is a multi-class classification where each sample can belong to ONE of 3 classes ( Iris setosa , Iris virginica or Iris versicolor ). The network will have 4 input neurons (flower dimensions) and 3 output neurons (scores). Our loss function will compare the target label (ground truth) to the corresponding output score. NOTE: Multi-class classifications usually involve converting the target vector to a one_hot encoded matrix. That is, if 5 labels show up as tensor([0,2,1,0,1]) then we would encode them as: tensor([[1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0]]) This is easily accomplished with torch.nn.functional.one_hot() . However, our loss function torch.nn.CrossEntropyLoss() takes care of this for us.","title":"Basic PyTorch Neural Network"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#perform-standard-imports","text":"import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader from sklearn.model_selection import train_test_split import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#create-a-model-class","text":"For this exercise we\u2019re using the Iris dataset. Since a single straight line can\u2019t classify three flowers we should include at least one hidden layer in our model. In the forward section we\u2019ll use the rectified linear unit (ReLU) function as our activation function. This is available as a full module torch.nn.ReLU or as just a functional call torch.nn.functional.relu class Model ( nn . Module ): def __init__ ( self , in_features = 4 , h1 = 8 , h2 = 9 , out_features = 3 ): super () . __init__ () self . fc1 = nn . Linear ( in_features , h1 ) # input layer self . fc2 = nn . Linear ( h1 , h2 ) # hidden layer self . out = nn . Linear ( h2 , out_features ) # output layer def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . out ( x ) return x # Instantiate the Model class using parameter defaults: torch . manual_seed ( 32 ) model = Model ()","title":"Create a model class"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#load-the-iris-dataset","text":"df = pd . read_csv ( '../Data/iris.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0","title":"Load the iris dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#plot-the-dataset","text":"The iris dataset has 4 features. To get an idea how they correlate we can plot four different relationships among them. We\u2019ll use the index positions of the columns to grab their names in pairs with plots = [(0,1),(2,3),(0,2),(1,3)] . Here (0,1) sets \u201csepal length (cm)\u201d as x and \u201csepal width (cm)\u201d as y fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show ()","title":"Plot the dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#perform-traintestsplit","text":"X = df . drop ( 'target' , axis = 1 ) . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( X_train ) X_test = torch . FloatTensor ( X_test ) # y_train = F.one_hot(torch.LongTensor(y_train)) # not needed with Cross Entropy Loss # y_test = F.one_hot(torch.LongTensor(y_test)) y_train = torch . LongTensor ( y_train ) y_test = torch . LongTensor ( y_test )","title":"Perform Train/Test/Split"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#prepare-dataloader","text":"For this analysis we don\u2019t need to create a Dataset object, but we should take advantage of PyTorch\u2019s DataLoader tool. Even though our dataset is small (120 training samples), we\u2019ll load it into our model in two batches. This technique becomes very helpful with large datasets. Note that scikit-learn already shuffled the source dataset before preparing train and test sets. We\u2019ll still benefit from the DataLoader shuffle utility for model training if we make multiple passes throught the dataset. trainloader = DataLoader ( X_train , batch_size = 60 , shuffle = True ) testloader = DataLoader ( X_test , batch_size = 60 , shuffle = False )","title":"Prepare DataLoader"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#define-loss-equations-and-optimizations","text":"As before, we\u2019ll utilize Cross Entropy with torch.nn.CrossEntropyLoss() For the optimizer, we\u2019ll use a variation of Stochastic Gradient Descent called Adam (short for Adaptive Moment Estimation), with torch.optim.Adam() # FOR REDO torch . manual_seed ( 4 ) model = Model () criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 )","title":"Define loss equations and optimizations"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#train-the-model","text":"epochs = 100 losses = [] for i in range ( epochs ): i += 1 y_pred = model . forward ( X_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 10 == 1 : print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () epoch: 1 loss: 1.09568226 epoch: 11 loss: 0.98190653 epoch: 21 loss: 0.75652307 epoch: 31 loss: 0.49447522 epoch: 41 loss: 0.34981874 epoch: 51 loss: 0.22807853 epoch: 61 loss: 0.13547322 epoch: 71 loss: 0.09162075 epoch: 81 loss: 0.07378192 epoch: 91 loss: 0.06546164","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#plot-the-loss-function","text":"plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'epoch' );","title":"Plot the loss function"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#validate-the-model","text":"Now we run the test set through the model to see if the loss calculation resembles the training data. # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( f ' { loss : .8f } ' ) 0.06246195 correct = 0 with torch . no_grad (): for i , data in enumerate ( X_test ): y_val = model . forward ( data ) print ( f ' { i + 1 : 2 } . { str ( y_val ) : 38 } { y_test [ i ] } ' ) if y_val . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { len ( y_test ) } = { 100 * correct / len ( y_test ) : .2f } % correct' ) 1. tensor([-0.3355, 7.3630, 1.3783]) 1 2. tensor([0.2775, 8.1554, 0.4269]) 1 3. tensor([ 11.9969, 6.1847, -19.1976]) 0 4. tensor([-2.0187, 7.9664, 4.2447]) 1 5. tensor([-6.1348, 7.9516, 11.0913]) 2 6. tensor([-10.2635, 8.3101, 17.9998]) 2 7. tensor([ 12.0542, 6.4321, -19.2909]) 0 8. tensor([ 12.9507, 6.4819, -20.7540]) 0 9. tensor([-5.7723, 8.2435, 10.5083]) 2 10. tensor([-7.8867, 8.6126, 14.0731]) 2 11. tensor([-8.7055, 8.6074, 15.4337]) 2 12. tensor([ 11.6358, 5.8167, -18.6220]) 0 13. tensor([-8.1009, 8.2331, 14.3888]) 2 14. tensor([-2.0791, 7.7752, 4.3188]) 1 15. tensor([-6.0828, 8.3916, 11.0586]) 2 16. tensor([0.1360, 7.8660, 0.6409]) 1 17. tensor([-4.0875, 7.7217, 7.6642]) 2 18. tensor([ 13.1522, 6.5911, -21.0798]) 0 19. tensor([-1.5644, 8.0222, 3.4754]) 1 20. tensor([-6.2859, 8.9728, 11.4248]) 2 21. tensor([ 12.3859, 6.2571, -19.8275]) 0 22. tensor([ 13.8200, 7.0859, -22.1528]) 0 23. tensor([-8.8470, 8.3180, 15.6476]) 2 24. tensor([ 12.1979, 6.1264, -19.5260]) 0 25. tensor([-5.8084, 7.5468, 10.5340]) 2 26. tensor([-4.4526, 7.7876, 8.2865]) 2 27. tensor([-1.4284, 7.7786, 3.2328]) 1 28. tensor([ 0.5356, 7.5360, -0.0492]) 1 29. tensor([-5.8230, 8.1573, 10.5975]) 2 30. tensor([-5.2569, 7.7476, 9.6105]) 2 29 out of 30 = 96.67% correct Here we can see that #17 was misclassified.","title":"Validate the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#save-the-trained-model-to-a-file","text":"Right now model has been trained and validated, and seems to correctly classify an iris 97% of the time. Let\u2019s save this to disk. The tools we\u2019ll use are torch.save() and torch.load() There are two basic ways to save a model. The first saves/loads the state_dict (learned parameters) of the model, but not the model class. The syntax follows: Save: torch.save(model.state_dict(), PATH) Load: model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.eval() The second saves the entire model including its class and parameters as a pickle file. Care must be taken if you want to load this into another notebook to make sure all the target data is brought in properly. Save: torch.save(model, PATH) Load: model = torch.load(PATH)) model.eval() In either method, you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results. For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html","title":"Save the trained model to a file"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#save-the-model","text":"torch . save ( model . state_dict (), 'IrisDatasetModel.pt' )","title":"Save the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#load-a-new-model","text":"We\u2019ll load a new model object and test it as we had before to make sure it worked. new_model = Model () new_model . load_state_dict ( torch . load ( 'IrisDatasetModel.pt' )) new_model . eval () Model( (fc1): Linear(in_features=4, out_features=8, bias=True) (fc2): Linear(in_features=8, out_features=9, bias=True) (out): Linear(in_features=9, out_features=3, bias=True) ) with torch . no_grad (): y_val = new_model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( f ' { loss : .8f } ' ) 0.06246195","title":"Load a new model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#apply-the-model-to-classify-new-unseen-data","text":"mystery_iris = torch . tensor ([ 5.6 , 3.7 , 2.2 , 0.5 ]) Let\u2019s plot this new iris in yellow to see where it falls in relation to the others: fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 10 , 7 )) fig . tight_layout () plots = [( 0 , 1 ),( 2 , 3 ),( 0 , 2 ),( 1 , 3 )] colors = [ 'b' , 'r' , 'g' ] labels = [ 'Iris setosa' , 'Iris virginica' , 'Iris versicolor' , 'Mystery iris' ] for i , ax in enumerate ( axes . flat ): for j in range ( 3 ): x = df . columns [ plots [ i ][ 0 ]] y = df . columns [ plots [ i ][ 1 ]] ax . scatter ( df [ df [ 'target' ] == j ][ x ], df [ df [ 'target' ] == j ][ y ], color = colors [ j ]) ax . set ( xlabel = x , ylabel = y ) # Add a plot for our mystery iris: ax . scatter ( mystery_iris [ plots [ i ][ 0 ]], mystery_iris [ plots [ i ][ 1 ]], color = 'y' ) fig . legend ( labels = labels , loc = 3 , bbox_to_anchor = ( 1.0 , 0.85 )) plt . show () Now run it through the model: with torch . no_grad (): print ( new_model ( mystery_iris )) print () print ( labels [ new_model ( mystery_iris ) . argmax ()]) tensor([ 12.2116, 7.1285, -19.5247]) Iris setosa","title":"Apply the model to classify new, unseen data"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/03-Basic-PyTorch-NN/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/","text":"Pytorch for Deep Learning BootCamp \u00b6 by Jawad Haider 04 - Full Artificial Neural Network Code Along \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Full Artificial Neural Network Code Along Working with tabular data Perform standard imports Load the NYC Taxi Fares dataset Calculate the distance traveled Add a datetime column and derive useful statistics Separate categorical from continuous columns Categorify Convert numpy arrays to tensors Set an embedding size Define a TabularModel Define loss function & optimizer Perform train/test splits Train the model Plot the loss function Validate the model Save the model Loading a saved model (starting from scratch) Feed new data through the trained model Great job! Full Artificial Neural Network Code Along \u00b6 In the last section we took in four continuous variables (lengths) to perform a classification. In this section we\u2019ll combine continuous and categorical data to perform a regression. The goal is to estimate the cost of a New York City cab ride from several inputs. The inspiration behind this code along is a recent Kaggle competition . NOTE: In this notebook we\u2019ll perform a regression with one output value. In the next one we\u2019ll perform a binary classification with two output values. Working with tabular data \u00b6 Deep learning with neural networks is often associated with sophisticated image recognition, and in upcoming sections we\u2019ll train models based on properties like pixels patterns and colors. Here we\u2019re working with tabular data (spreadsheets, SQL tables, etc.) with columns of values that may or may not be relevant. As it happens, neural networks can learn to make connections we probably wouldn\u2019t have developed on our own. However, to do this we have to handle categorical values separately from continuous ones. Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * continuous vs. categorical values * embeddings * batch normalization * dropout layers Perform standard imports \u00b6 import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Load the NYC Taxi Fares dataset \u00b6 The Kaggle competition provides a dataset with about 55 million records. The data contains only the pickup date & time, the latitude & longitude (GPS coordinates) of the pickup and dropoff locations, and the number of passengers. It is up to the contest participant to extract any further information. For instance, does the time of day matter? The day of the week? How do we determine the distance traveled from pairs of GPS coordinates? For this exercise we\u2019ve whittled the dataset down to just 120,000 records from April 11 to April 24, 2010. The records are randomly sorted. We\u2019ll show how to calculate distance from GPS coordinates, and how to create a pandas datatime object from a text column. This will let us quickly get information like day of the week, am vs. pm, etc. Let\u2019s get started! df = pd . read_csv ( '../Data/NYCTaxiFares.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 df [ 'fare_amount' ] . describe () count 120000.000000 mean 10.040326 std 7.500134 min 2.500000 25% 5.700000 50% 7.700000 75% 11.300000 max 49.900000 Name: fare_amount, dtype: float64 From this we see that fares range from \\$2.50 to \\$49.90, with a mean of \\$10.04 and a median of \\$7.70 Calculate the distance traveled \u00b6 The haversine formula calculates the distance on a sphere between two sets of GPS coordinates. Here we assign latitude values with (phi) and longitude with (lambda). The distance formula works out to where def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): \"\"\" Calculates the haversine distance between 2 sets of GPS coordinates in df \"\"\" r = 6371 # average radius of Earth in kilometers phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) d = ( r * c ) # in kilometers return d df [ 'dist_km' ] = haversine_distance ( df , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 Add a datetime column and derive useful statistics \u00b6 By creating a datetime object, we can extract information like \u201cday of the week\u201d, \u201cam vs. pm\u201d etc. Note that the data was saved in UTC time. Our data falls in April of 2010 which occurred during Daylight Savings Time in New York. For that reason, we\u2019ll make an adjustment to EDT using UTC-4 (subtracting four hours). df [ 'EDTdate' ] = pd . to_datetime ( df [ 'pickup_datetime' ] . str [: 19 ]) - pd . Timedelta ( hours = 4 ) df [ 'Hour' ] = df [ 'EDTdate' ] . dt . hour df [ 'AMorPM' ] = np . where ( df [ 'Hour' ] < 12 , 'am' , 'pm' ) df [ 'Weekday' ] = df [ 'EDTdate' ] . dt . strftime ( \" %a \" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km EDTdate Hour AMorPM Weekday 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 2010-04-19 04:17:56 4 am Mon 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2010-04-17 11:43:53 11 am Sat 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 2010-04-17 07:23:26 7 am Sat 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 2010-04-11 17:25:03 17 pm Sun 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 2010-04-16 22:19:01 22 pm Fri df [ 'EDTdate' ] . min () Timestamp('2010-04-11 00:00:10') df [ 'EDTdate' ] . max () Timestamp('2010-04-24 23:59:42') Separate categorical from continuous columns \u00b6 df . columns Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km', 'EDTdate', 'Hour', 'AMorPM', 'Weekday'], dtype='object') cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] y_col = [ 'fare_amount' ] # this column contains the labels NOTE: If you plan to use all of the columns in the data table, there\u2019s a shortcut to grab the remaining continuous columns: cont_cols = [col for col in df.columns if col not in cat_cols + y_col] Here we entered the continuous columns explicitly because there are columns we\u2019re not running through the model (pickup_datetime and EDTdate) Categorify \u00b6 Pandas offers a category dtype for converting categorical values to numerical codes. A dataset containing months of the year will be assigned 12 codes, one for each month. These will usually be the integers 0 to 11. Pandas replaces the column values with codes, and retains an index list of category values. In the steps ahead we\u2019ll call the categorical values \u201cnames\u201d and the encodings \u201ccodes\u201d. # Convert our three categorical columns to category dtypes. for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . dtypes pickup_datetime object fare_amount float64 fare_class int64 pickup_longitude float64 pickup_latitude float64 dropoff_longitude float64 dropoff_latitude float64 passenger_count int64 dist_km float64 EDTdate datetime64[ns] Hour category AMorPM category Weekday category dtype: object We can see that df[\u2018Hour\u2019] is a categorical feature by displaying some of the rows: df [ 'Hour' ] . head () 0 4 1 11 2 7 3 17 4 22 Name: Hour, dtype: category Categories (24, int64): [0, 1, 2, 3, ..., 20, 21, 22, 23] Here our categorical names are the integers 0 through 23, for a total of 24 unique categories. These values also correspond to the codes assigned to each name. We can access the category names with Series.cat.categories or just the codes with Series.cat.codes . This will make more sense if we look at df[\u2018AMorPM\u2019] : df [ 'AMorPM' ] . head () 0 am 1 am 2 am 3 pm 4 pm Name: AMorPM, dtype: category Categories (2, object): [am, pm] df [ 'AMorPM' ] . cat . categories Index(['am', 'pm'], dtype='object') df [ 'AMorPM' ] . head () . cat . codes 0 0 1 0 2 0 3 1 4 1 dtype: int8 df [ 'Weekday' ] . cat . categories Index(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype='object') df [ 'Weekday' ] . head () . cat . codes 0 1 1 2 2 2 3 3 4 0 dtype: int8 NOTE: NaN values in categorical data are assigned a code of -1. We don\u2019t have any in this particular dataset. Now we want to combine the three categorical columns into one input array using numpy.stack We don\u2019t want the Series index, just the values. hr = df [ 'Hour' ] . cat . codes . values ampm = df [ 'AMorPM' ] . cat . codes . values wkdy = df [ 'Weekday' ] . cat . codes . values cats = np . stack ([ hr , ampm , wkdy ], 1 ) cats [: 5 ] array([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]], dtype=int8) NOTE: This can be done in one line of code using a list comprehension: cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1) Don\u2019t worry about the dtype for now, we can make it int64 when we convert it to a tensor. Convert numpy arrays to tensors \u00b6 # Convert categorical variables to a tensor cats = torch . tensor ( cats , dtype = torch . int64 ) # this syntax is ok, since the source data is an array, not an existing tensor cats [: 5 ] tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]]) We can feed all of our continuous variables into the model as a tensor. Note that we\u2019re not normalizing the values here; we\u2019ll let the model perform this step. NOTE: We have to store conts and y as Float (float32) tensors, not Double (float64) in order for batch normalization to work properly. # Convert continuous variables to a tensor conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts = torch . tensor ( conts , dtype = torch . float ) conts [: 5 ] tensor([[ 40.7305, -73.9924, 40.7447, -73.9755, 1.0000, 2.1263], [ 40.7406, -73.9901, 40.7441, -73.9742, 1.0000, 1.3923], [ 40.7511, -73.9941, 40.7662, -73.9601, 2.0000, 3.3268], [ 40.7564, -73.9905, 40.7482, -73.9712, 1.0000, 1.8641], [ 40.7342, -73.9910, 40.7431, -73.9060, 1.0000, 7.2313]]) conts . type () 'torch.FloatTensor' # Convert labels to a tensor y = torch . tensor ( df [ y_col ] . values , dtype = torch . float ) . reshape ( - 1 , 1 ) y [: 5 ] tensor([[ 6.5000], [ 6.9000], [10.1000], [ 8.9000], [19.7000]]) cats . shape torch.Size([120000, 3]) conts . shape torch.Size([120000, 6]) y . shape torch.Size([120000, 1]) Set an embedding size \u00b6 The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50. # This will set embedding sizes for Hours, AMvsPM and Weekdays cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(24, 12), (2, 1), (7, 4)] Define a TabularModel \u00b6 This somewhat follows the fast.ai library The goal is to define a model based on the number of continuous columns (given by conts.shape[1] ) plus the number of categorical columns and their embeddings (given by len(emb_szs) and emb_szs respectively). The output would either be a regression (a single float value), or a classification (a group of bins and their softmax values). For this exercise our output will be a single regression value. Note that we\u2019ll assume our data contains both categorical and continuous data. You can add boolean parameters to your own model class to handle a variety of datasets. Let\u2019s walk through the steps we\u2019re about to take. See below for more detailed illustrations of the steps. 1. Extend the base Module class, set up the following parameters: - emb_szs: list of tuples: each categorical variable size is paired with an embedding size - n_cont: int: number of continuous variables - out_sz: int: output size - layers: list of ints: layer sizes - p: float: dropout probability for each layer (for simplicity we\u2019ll use the same value throughout) class TabularModel(nn.Module): def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5): super().\\_\\_init\\_\\_() 2. Set up the embedded layers with torch.nn.ModuleList() and torch.nn.Embedding() Categorical data will be filtered through these Embeddings in the forward section. self.embeds = nn.ModuleList(\\[nn.Embedding(ni, nf) for ni,nf in emb_szs\\]) 3. Set up a dropout function for the embeddings with torch.nn.Dropout() The default p-value=0.5 self.emb_drop = nn.Dropout(emb_drop) 4. Set up a normalization function for the continuous variables with torch.nn.BatchNorm1d() self.bn_cont = nn.BatchNorm1d(n_cont) 5. Set up a sequence of neural network layers where each level includes a Linear function, an activation function (we\u2019ll use ReLU ), a normalization step, and a dropout layer. We\u2019ll combine the list of layers with torch.nn.Sequential() self.bn_cont = nn.BatchNorm1d(n_cont) layerlist = \\[\\] n_emb = sum((nf for ni,nf in emb_szs)) n_in = n_emb + n_cont for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) layerlist.append(nn.BatchNorm1d(i)) layerlist.append(nn.Dropout(p)) n_in = i layerlist.append(nn.Linear(layers\\[-1\\],out_sz)) self.layers = nn.Sequential(\\*layerlist) 6. Define the forward method. Preprocess the embeddings and normalize the continuous variables before passing them through the layers. Use torch.cat() to combine multiple tensors into one. def forward(self, x_cat, x_cont): embeddings = \\[\\] for i,e in enumerate(self.embeds): embeddings.append(e(x_cat\\[:,i\\])) x = torch.cat(embeddings, 1) x = self.emb_drop(x) x_cont = self.bn_cont(x_cont) x = torch.cat(\\[x, x_cont\\], 1) x = self.layers(x) return x Breaking down the embeddings steps (this code is for illustration purposes only.) # This is our source data catz = cats [: 4 ] catz tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3]]) # This is passed in when the model is instantiated emb_szs [(24, 12), (2, 1), (7, 4)] # This is assigned inside the __init__() method selfembeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) selfembeds ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) list ( enumerate ( selfembeds )) [(0, Embedding(24, 12)), (1, Embedding(2, 1)), (2, Embedding(7, 4))] # This happens inside the forward() method embeddingz = [] for i , e in enumerate ( selfembeds ): embeddingz . append ( e ( catz [:, i ])) embeddingz [tensor([[ 0.7031, 0.5132, -1.0168, 1.3594, 0.7909, -1.2648, 0.9565, 0.6827, 0.4922, 0.1282, -0.9863, 0.2622], [ 0.0402, -0.8209, 2.5186, 0.5037, 0.0925, 0.0058, 1.0398, 1.7655, 0.0216, -0.1349, -1.0968, 1.4534], [ 0.5162, -0.6815, 1.4828, 0.1271, 1.1672, 0.8569, 1.1472, 0.7672, 0.4720, 1.3629, 1.2446, 0.3470], [ 0.1047, 0.2153, -1.4652, -1.6907, -0.0670, 0.9863, 0.7836, -1.3762, 1.6670, 0.1276, 1.2241, 0.1415]], grad_fn=<EmbeddingBackward>), tensor([[-1.1156], [-1.1156], [-1.1156], [-0.7577]], grad_fn=<EmbeddingBackward>), tensor([[-0.8579, -0.4516, 0.3814, 0.9935], [ 0.6264, -1.1347, 1.9039, 1.1867], [ 0.6264, -1.1347, 1.9039, 1.1867], [ 0.1609, -0.1231, 0.5787, -0.3180]], grad_fn=<EmbeddingBackward>)] # We concatenate the embedding sections (12,1,4) into one (17) z = torch . cat ( embeddingz , 1 ) z tensor([[ 0.7031, 0.5132, -1.0168, 1.3594, 0.7909, -1.2648, 0.9565, 0.6827, 0.4922, 0.1282, -0.9863, 0.2622, -1.1156, -0.8579, -0.4516, 0.3814, 0.9935], [ 0.0402, -0.8209, 2.5186, 0.5037, 0.0925, 0.0058, 1.0398, 1.7655, 0.0216, -0.1349, -1.0968, 1.4534, -1.1156, 0.6264, -1.1347, 1.9039, 1.1867], [ 0.5162, -0.6815, 1.4828, 0.1271, 1.1672, 0.8569, 1.1472, 0.7672, 0.4720, 1.3629, 1.2446, 0.3470, -1.1156, 0.6264, -1.1347, 1.9039, 1.1867], [ 0.1047, 0.2153, -1.4652, -1.6907, -0.0670, 0.9863, 0.7836, -1.3762, 1.6670, 0.1276, 1.2241, 0.1415, -0.7577, 0.1609, -0.1231, 0.5787, -0.3180]], grad_fn=<CatBackward>) # This was assigned under the __init__() method selfembdrop = nn . Dropout ( .4 ) z = selfembdrop ( z ) z tensor([[ 0.0000, 0.8553, -1.6946, 0.0000, 1.3182, -2.1080, 1.5942, 1.1378, 0.8204, 0.2137, -1.6438, 0.0000, -0.0000, -1.4298, -0.0000, 0.6357, 0.0000], [ 0.0000, -1.3682, 0.0000, 0.8395, 0.0000, 0.0000, 0.0000, 2.9425, 0.0359, -0.0000, -1.8280, 2.4223, -0.0000, 0.0000, -1.8912, 3.1731, 0.0000], [ 0.0000, -1.1359, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2786, 0.7867, 2.2715, 0.0000, 0.5783, -0.0000, 1.0440, -1.8912, 3.1731, 0.0000], [ 0.0000, 0.3589, -2.4420, -0.0000, -0.1117, 1.6438, 1.3059, -0.0000, 2.7783, 0.2127, 2.0402, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, -0.5300]], grad_fn=<MulBackward0>) This is how the categorical embeddings are passed into the layers. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) x = self . layers ( x ) return x torch . manual_seed ( 33 ) model = TabularModel ( emb_szs , conts . shape [ 1 ], 1 , [ 200 , 100 ], p = 0.4 ) model TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=1, bias=True) ) ) Define loss function & optimizer \u00b6 PyTorch does not offer a built-in RMSE Loss function, and it would be nice to see this in place of MSE. For this reason, we\u2019ll simply apply the torch.sqrt() function to the output of MSELoss during training. criterion = nn . MSELoss () # we'll convert this to RMSE later optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) Perform train/test splits \u00b6 At this point our batch size is the entire dataset of 120,000 records. This will take a long time to train, so you might consider reducing this. We\u2019ll use 60,000. Recall that our tensors are already randomly shuffled. batch_size = 60000 test_size = int ( batch_size * .2 ) cat_train = cats [: batch_size - test_size ] cat_test = cats [ batch_size - test_size : batch_size ] con_train = conts [: batch_size - test_size ] con_test = conts [ batch_size - test_size : batch_size ] y_train = y [: batch_size - test_size ] y_test = y [ batch_size - test_size : batch_size ] len ( cat_train ) 48000 len ( cat_test ) 12000 Train the model \u00b6 Expect this to take 30 minutes or more! We\u2019ve added code to tell us the duration at the end. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = torch . sqrt ( criterion ( y_pred , y_train )) # RMSE losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 12.49953079 epoch: 26 loss: 11.52666759 epoch: 51 loss: 10.47162533 epoch: 76 loss: 9.53090382 epoch: 101 loss: 8.72838020 epoch: 126 loss: 7.81538534 epoch: 151 loss: 6.70782852 epoch: 176 loss: 5.48520994 epoch: 201 loss: 4.37029028 epoch: 226 loss: 3.70538783 epoch: 251 loss: 3.51656818 epoch: 276 loss: 3.44707990 epoch: 300 loss: 3.42467499 Duration: 730 seconds Plot the loss function \u00b6 plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'RMSE Loss' ) plt . xlabel ( 'epoch' ); Validate the model \u00b6 Here we want to run the entire test set through the model, and compare it to the known labels. For this step we don\u2019t want to update weights and biases, so we set torch.no_grad() # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = torch . sqrt ( criterion ( y_val , y_test )) print ( f 'RMSE: { loss : .8f } ' ) RMSE: 3.34590030 This means that on average, predicted values are within \u00b1$3.31 of the actual value. Now let\u2019s look at the first 50 predicted values: print ( f ' { \"PREDICTED\" : >12 } { \"ACTUAL\" : >8 } { \"DIFF\" : >8 } ' ) for i in range ( 50 ): diff = np . abs ( y_val [ i ] . item () - y_test [ i ] . item ()) print ( f ' { i + 1 : 2 } . { y_val [ i ] . item () : 8.4f } { y_test [ i ] . item () : 8.4f } { diff : 8.4f } ' ) PREDICTED ACTUAL DIFF 1. 2.5379 2.9000 0.3621 2. 25.1634 5.7000 19.4634 3. 6.3749 7.7000 1.3251 4. 13.4677 12.5000 0.9677 5. 4.4992 4.1000 0.3992 6. 4.8968 5.3000 0.4032 7. 3.1796 3.7000 0.5204 8. 17.7814 14.5000 3.2814 9. 6.1348 5.7000 0.4348 10. 12.0325 10.1000 1.9325 11. 6.1323 4.5000 1.6323 12. 6.9208 6.1000 0.8208 13. 5.9448 6.9000 0.9552 14. 13.4625 14.1000 0.6375 15. 5.9277 4.5000 1.4277 16. 27.5778 34.1000 6.5222 17. 3.2774 12.5000 9.2226 18. 5.7506 4.1000 1.6506 19. 8.1940 8.5000 0.3060 20. 6.2858 5.3000 0.9858 21. 13.6693 11.3000 2.3693 22. 9.6759 10.5000 0.8241 23. 16.0568 15.3000 0.7568 24. 19.3310 14.9000 4.4310 25. 48.6873 49.5700 0.8827 26. 6.3257 5.3000 1.0257 27. 6.0392 3.7000 2.3392 28. 7.1921 6.5000 0.6921 29. 14.9567 14.1000 0.8567 30. 6.7476 4.9000 1.8476 31. 4.3447 3.7000 0.6447 32. 35.6969 38.6700 2.9731 33. 13.9892 12.5000 1.4892 34. 12.8934 16.5000 3.6066 35. 6.3164 5.7000 0.6164 36. 5.9684 8.9000 2.9316 37. 16.1289 22.1000 5.9711 38. 7.6541 12.1000 4.4459 39. 8.6153 10.1000 1.4847 40. 4.0447 3.3000 0.7447 41. 10.2168 8.5000 1.7168 42. 8.8325 8.1000 0.7325 43. 15.2500 14.5000 0.7500 44. 6.3571 4.9000 1.4571 45. 9.7002 8.5000 1.2002 46. 12.1134 12.1000 0.0134 47. 24.3001 23.7000 0.6001 48. 2.8357 3.7000 0.8643 49. 6.8266 9.3000 2.4734 50. 8.2082 8.1000 0.1082 So while many predictions were off by a few cents, some were off by \\$19.00. Feel free to change the batch size, test size, and number of epochs to obtain a better model. Save the model \u00b6 We can save a trained model to a file in case we want to come back later and feed new data through it. The best practice is to save the state of the model (weights & biases) and not the full definition. Also, we want to ensure that only a trained model is saved, to prevent overwriting a previously saved model with an untrained one. For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html # Make sure to save the model only after the training has happened! if len ( losses ) == epochs : torch . save ( model . state_dict (), 'TaxiFareRegrModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' ) Loading a saved model (starting from scratch) \u00b6 We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. To demonstrate, restart the kernel before proceeding. import torch import torch.nn as nn import numpy as np import pandas as pd def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): r = 6371 phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) return r * c class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) return self . layers ( x ) Now define the model. Before we can load the saved settings, we need to instantiate our TabularModel with the parameters we used before (embedding sizes, number of continuous columns, output size, layer sizes, and dropout layer p-value). emb_szs = [( 24 , 12 ), ( 2 , 1 ), ( 7 , 4 )] model2 = TabularModel ( emb_szs , 6 , 1 , [ 200 , 100 ], p = 0.4 ) Once the model is set up, loading the saved settings is a snap. model2 . load_state_dict ( torch . load ( 'TaxiFareRegrModel.pt' )); model2 . eval () # be sure to run this step! TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=1, bias=True) ) ) Next we\u2019ll define a function that takes in new parameters from the user, performs all of the preprocessing steps above, and passes the new data through our trained model. def test_data ( mdl ): # pass in the name of the new model # INPUT NEW DATA plat = float ( input ( 'What is the pickup latitude? ' )) plong = float ( input ( 'What is the pickup longitude? ' )) dlat = float ( input ( 'What is the dropoff latitude? ' )) dlong = float ( input ( 'What is the dropoff longitude? ' )) psngr = int ( input ( 'How many passengers? ' )) dt = input ( 'What is the pickup date and time? \\n Format as YYYY-MM-DD HH:MM:SS ' ) # PREPROCESS THE DATA dfx_dict = { 'pickup_latitude' : plat , 'pickup_longitude' : plong , 'dropoff_latitude' : dlat , 'dropoff_longitude' : dlong , 'passenger_count' : psngr , 'EDTdate' : dt } dfx = pd . DataFrame ( dfx_dict , index = [ 0 ]) dfx [ 'dist_km' ] = haversine_distance ( dfx , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) dfx [ 'EDTdate' ] = pd . to_datetime ( dfx [ 'EDTdate' ]) # We can skip the .astype(category) step since our fields are small, # and encode them right away dfx [ 'Hour' ] = dfx [ 'EDTdate' ] . dt . hour dfx [ 'AMorPM' ] = np . where ( dfx [ 'Hour' ] < 12 , 0 , 1 ) dfx [ 'Weekday' ] = dfx [ 'EDTdate' ] . dt . strftime ( \" %a \" ) dfx [ 'Weekday' ] = dfx [ 'Weekday' ] . replace ([ 'Fri' , 'Mon' , 'Sat' , 'Sun' , 'Thu' , 'Tue' , 'Wed' ], [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ]) . astype ( 'int64' ) # CREATE CAT AND CONT TENSORS cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] xcats = np . stack ([ dfx [ col ] . values for col in cat_cols ], 1 ) xcats = torch . tensor ( xcats , dtype = torch . int64 ) xconts = np . stack ([ dfx [ col ] . values for col in cont_cols ], 1 ) xconts = torch . tensor ( xconts , dtype = torch . float ) # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( xcats , xconts ) print ( f ' \\n The predicted fare amount is $ { z . item () : .2f } ' ) Feed new data through the trained model \u00b6 For convenience, here are the max and min values for each of the variables: Column Minimum Maximum pickup_latitude 40 41 pickup_longitude -74.5 -73.3 dropoff_latitude 40 41 dropoff_longitude -74.5 -73.3 passenger_count 1 5 EDTdate 2010-04-11 00:00:00 2010-04-24 23:59:42 Use caution! The distance between 1 degree of latitude (from 40 to 41) is 111km (69mi) and between 1 degree of longitude (from -73 to -74) is 85km (53mi). The longest cab ride in the dataset spanned a difference of only 0.243 degrees latitude and 0.284 degrees longitude. The mean difference for both latitude and longitude was about 0.02. To get a fair prediction, use values that fall close to one another. z = test_data ( model2 ) What is the pickup latitude? 40.5 What is the pickup longitude? -73.9 What is the dropoff latitude? 40.52 What is the dropoff longitude? -73.92 How many passengers? 2 What is the pickup date and time? Format as YYYY-MM-DD HH:MM:SS 2010-04-15 16:00:00 The predicted fare amount is $13.86 ## Great job!","title":"04a Full ANN Code Along Regression"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#pytorch-for-deep-learning-bootcamp","text":"by Jawad Haider","title":"Pytorch for Deep Learning BootCamp"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#04-full-artificial-neural-network-code-along","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Full Artificial Neural Network Code Along Working with tabular data Perform standard imports Load the NYC Taxi Fares dataset Calculate the distance traveled Add a datetime column and derive useful statistics Separate categorical from continuous columns Categorify Convert numpy arrays to tensors Set an embedding size Define a TabularModel Define loss function & optimizer Perform train/test splits Train the model Plot the loss function Validate the model Save the model Loading a saved model (starting from scratch) Feed new data through the trained model Great job!","title":"04 - Full Artificial Neural Network Code Along"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#full-artificial-neural-network-code-along","text":"In the last section we took in four continuous variables (lengths) to perform a classification. In this section we\u2019ll combine continuous and categorical data to perform a regression. The goal is to estimate the cost of a New York City cab ride from several inputs. The inspiration behind this code along is a recent Kaggle competition . NOTE: In this notebook we\u2019ll perform a regression with one output value. In the next one we\u2019ll perform a binary classification with two output values.","title":"Full Artificial Neural Network Code Along"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#working-with-tabular-data","text":"Deep learning with neural networks is often associated with sophisticated image recognition, and in upcoming sections we\u2019ll train models based on properties like pixels patterns and colors. Here we\u2019re working with tabular data (spreadsheets, SQL tables, etc.) with columns of values that may or may not be relevant. As it happens, neural networks can learn to make connections we probably wouldn\u2019t have developed on our own. However, to do this we have to handle categorical values separately from continuous ones. Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * continuous vs. categorical values * embeddings * batch normalization * dropout layers","title":"Working with tabular data"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#perform-standard-imports","text":"import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#load-the-nyc-taxi-fares-dataset","text":"The Kaggle competition provides a dataset with about 55 million records. The data contains only the pickup date & time, the latitude & longitude (GPS coordinates) of the pickup and dropoff locations, and the number of passengers. It is up to the contest participant to extract any further information. For instance, does the time of day matter? The day of the week? How do we determine the distance traveled from pairs of GPS coordinates? For this exercise we\u2019ve whittled the dataset down to just 120,000 records from April 11 to April 24, 2010. The records are randomly sorted. We\u2019ll show how to calculate distance from GPS coordinates, and how to create a pandas datatime object from a text column. This will let us quickly get information like day of the week, am vs. pm, etc. Let\u2019s get started! df = pd . read_csv ( '../Data/NYCTaxiFares.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 df [ 'fare_amount' ] . describe () count 120000.000000 mean 10.040326 std 7.500134 min 2.500000 25% 5.700000 50% 7.700000 75% 11.300000 max 49.900000 Name: fare_amount, dtype: float64 From this we see that fares range from \\$2.50 to \\$49.90, with a mean of \\$10.04 and a median of \\$7.70","title":"Load the NYC Taxi Fares dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#calculate-the-distance-traveled","text":"The haversine formula calculates the distance on a sphere between two sets of GPS coordinates. Here we assign latitude values with (phi) and longitude with (lambda). The distance formula works out to where def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): \"\"\" Calculates the haversine distance between 2 sets of GPS coordinates in df \"\"\" r = 6371 # average radius of Earth in kilometers phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) d = ( r * c ) # in kilometers return d df [ 'dist_km' ] = haversine_distance ( df , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321","title":"Calculate the distance traveled"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#add-a-datetime-column-and-derive-useful-statistics","text":"By creating a datetime object, we can extract information like \u201cday of the week\u201d, \u201cam vs. pm\u201d etc. Note that the data was saved in UTC time. Our data falls in April of 2010 which occurred during Daylight Savings Time in New York. For that reason, we\u2019ll make an adjustment to EDT using UTC-4 (subtracting four hours). df [ 'EDTdate' ] = pd . to_datetime ( df [ 'pickup_datetime' ] . str [: 19 ]) - pd . Timedelta ( hours = 4 ) df [ 'Hour' ] = df [ 'EDTdate' ] . dt . hour df [ 'AMorPM' ] = np . where ( df [ 'Hour' ] < 12 , 'am' , 'pm' ) df [ 'Weekday' ] = df [ 'EDTdate' ] . dt . strftime ( \" %a \" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km EDTdate Hour AMorPM Weekday 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 2010-04-19 04:17:56 4 am Mon 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2010-04-17 11:43:53 11 am Sat 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 2010-04-17 07:23:26 7 am Sat 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 2010-04-11 17:25:03 17 pm Sun 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 2010-04-16 22:19:01 22 pm Fri df [ 'EDTdate' ] . min () Timestamp('2010-04-11 00:00:10') df [ 'EDTdate' ] . max () Timestamp('2010-04-24 23:59:42')","title":"Add a datetime column and derive useful statistics"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#separate-categorical-from-continuous-columns","text":"df . columns Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km', 'EDTdate', 'Hour', 'AMorPM', 'Weekday'], dtype='object') cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] y_col = [ 'fare_amount' ] # this column contains the labels NOTE: If you plan to use all of the columns in the data table, there\u2019s a shortcut to grab the remaining continuous columns: cont_cols = [col for col in df.columns if col not in cat_cols + y_col] Here we entered the continuous columns explicitly because there are columns we\u2019re not running through the model (pickup_datetime and EDTdate)","title":"Separate categorical from continuous columns"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#categorify","text":"Pandas offers a category dtype for converting categorical values to numerical codes. A dataset containing months of the year will be assigned 12 codes, one for each month. These will usually be the integers 0 to 11. Pandas replaces the column values with codes, and retains an index list of category values. In the steps ahead we\u2019ll call the categorical values \u201cnames\u201d and the encodings \u201ccodes\u201d. # Convert our three categorical columns to category dtypes. for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . dtypes pickup_datetime object fare_amount float64 fare_class int64 pickup_longitude float64 pickup_latitude float64 dropoff_longitude float64 dropoff_latitude float64 passenger_count int64 dist_km float64 EDTdate datetime64[ns] Hour category AMorPM category Weekday category dtype: object We can see that df[\u2018Hour\u2019] is a categorical feature by displaying some of the rows: df [ 'Hour' ] . head () 0 4 1 11 2 7 3 17 4 22 Name: Hour, dtype: category Categories (24, int64): [0, 1, 2, 3, ..., 20, 21, 22, 23] Here our categorical names are the integers 0 through 23, for a total of 24 unique categories. These values also correspond to the codes assigned to each name. We can access the category names with Series.cat.categories or just the codes with Series.cat.codes . This will make more sense if we look at df[\u2018AMorPM\u2019] : df [ 'AMorPM' ] . head () 0 am 1 am 2 am 3 pm 4 pm Name: AMorPM, dtype: category Categories (2, object): [am, pm] df [ 'AMorPM' ] . cat . categories Index(['am', 'pm'], dtype='object') df [ 'AMorPM' ] . head () . cat . codes 0 0 1 0 2 0 3 1 4 1 dtype: int8 df [ 'Weekday' ] . cat . categories Index(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype='object') df [ 'Weekday' ] . head () . cat . codes 0 1 1 2 2 2 3 3 4 0 dtype: int8 NOTE: NaN values in categorical data are assigned a code of -1. We don\u2019t have any in this particular dataset. Now we want to combine the three categorical columns into one input array using numpy.stack We don\u2019t want the Series index, just the values. hr = df [ 'Hour' ] . cat . codes . values ampm = df [ 'AMorPM' ] . cat . codes . values wkdy = df [ 'Weekday' ] . cat . codes . values cats = np . stack ([ hr , ampm , wkdy ], 1 ) cats [: 5 ] array([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]], dtype=int8) NOTE: This can be done in one line of code using a list comprehension: cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1) Don\u2019t worry about the dtype for now, we can make it int64 when we convert it to a tensor.","title":"Categorify"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#convert-numpy-arrays-to-tensors","text":"# Convert categorical variables to a tensor cats = torch . tensor ( cats , dtype = torch . int64 ) # this syntax is ok, since the source data is an array, not an existing tensor cats [: 5 ] tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]]) We can feed all of our continuous variables into the model as a tensor. Note that we\u2019re not normalizing the values here; we\u2019ll let the model perform this step. NOTE: We have to store conts and y as Float (float32) tensors, not Double (float64) in order for batch normalization to work properly. # Convert continuous variables to a tensor conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts = torch . tensor ( conts , dtype = torch . float ) conts [: 5 ] tensor([[ 40.7305, -73.9924, 40.7447, -73.9755, 1.0000, 2.1263], [ 40.7406, -73.9901, 40.7441, -73.9742, 1.0000, 1.3923], [ 40.7511, -73.9941, 40.7662, -73.9601, 2.0000, 3.3268], [ 40.7564, -73.9905, 40.7482, -73.9712, 1.0000, 1.8641], [ 40.7342, -73.9910, 40.7431, -73.9060, 1.0000, 7.2313]]) conts . type () 'torch.FloatTensor' # Convert labels to a tensor y = torch . tensor ( df [ y_col ] . values , dtype = torch . float ) . reshape ( - 1 , 1 ) y [: 5 ] tensor([[ 6.5000], [ 6.9000], [10.1000], [ 8.9000], [19.7000]]) cats . shape torch.Size([120000, 3]) conts . shape torch.Size([120000, 6]) y . shape torch.Size([120000, 1])","title":"Convert numpy arrays to tensors"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#set-an-embedding-size","text":"The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50. # This will set embedding sizes for Hours, AMvsPM and Weekdays cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(24, 12), (2, 1), (7, 4)]","title":"Set an embedding size"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#define-a-tabularmodel","text":"This somewhat follows the fast.ai library The goal is to define a model based on the number of continuous columns (given by conts.shape[1] ) plus the number of categorical columns and their embeddings (given by len(emb_szs) and emb_szs respectively). The output would either be a regression (a single float value), or a classification (a group of bins and their softmax values). For this exercise our output will be a single regression value. Note that we\u2019ll assume our data contains both categorical and continuous data. You can add boolean parameters to your own model class to handle a variety of datasets. Let\u2019s walk through the steps we\u2019re about to take. See below for more detailed illustrations of the steps. 1. Extend the base Module class, set up the following parameters: - emb_szs: list of tuples: each categorical variable size is paired with an embedding size - n_cont: int: number of continuous variables - out_sz: int: output size - layers: list of ints: layer sizes - p: float: dropout probability for each layer (for simplicity we\u2019ll use the same value throughout) class TabularModel(nn.Module): def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5): super().\\_\\_init\\_\\_() 2. Set up the embedded layers with torch.nn.ModuleList() and torch.nn.Embedding() Categorical data will be filtered through these Embeddings in the forward section. self.embeds = nn.ModuleList(\\[nn.Embedding(ni, nf) for ni,nf in emb_szs\\]) 3. Set up a dropout function for the embeddings with torch.nn.Dropout() The default p-value=0.5 self.emb_drop = nn.Dropout(emb_drop) 4. Set up a normalization function for the continuous variables with torch.nn.BatchNorm1d() self.bn_cont = nn.BatchNorm1d(n_cont) 5. Set up a sequence of neural network layers where each level includes a Linear function, an activation function (we\u2019ll use ReLU ), a normalization step, and a dropout layer. We\u2019ll combine the list of layers with torch.nn.Sequential() self.bn_cont = nn.BatchNorm1d(n_cont) layerlist = \\[\\] n_emb = sum((nf for ni,nf in emb_szs)) n_in = n_emb + n_cont for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) layerlist.append(nn.BatchNorm1d(i)) layerlist.append(nn.Dropout(p)) n_in = i layerlist.append(nn.Linear(layers\\[-1\\],out_sz)) self.layers = nn.Sequential(\\*layerlist) 6. Define the forward method. Preprocess the embeddings and normalize the continuous variables before passing them through the layers. Use torch.cat() to combine multiple tensors into one. def forward(self, x_cat, x_cont): embeddings = \\[\\] for i,e in enumerate(self.embeds): embeddings.append(e(x_cat\\[:,i\\])) x = torch.cat(embeddings, 1) x = self.emb_drop(x) x_cont = self.bn_cont(x_cont) x = torch.cat(\\[x, x_cont\\], 1) x = self.layers(x) return x Breaking down the embeddings steps (this code is for illustration purposes only.) # This is our source data catz = cats [: 4 ] catz tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3]]) # This is passed in when the model is instantiated emb_szs [(24, 12), (2, 1), (7, 4)] # This is assigned inside the __init__() method selfembeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) selfembeds ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) list ( enumerate ( selfembeds )) [(0, Embedding(24, 12)), (1, Embedding(2, 1)), (2, Embedding(7, 4))] # This happens inside the forward() method embeddingz = [] for i , e in enumerate ( selfembeds ): embeddingz . append ( e ( catz [:, i ])) embeddingz [tensor([[ 0.7031, 0.5132, -1.0168, 1.3594, 0.7909, -1.2648, 0.9565, 0.6827, 0.4922, 0.1282, -0.9863, 0.2622], [ 0.0402, -0.8209, 2.5186, 0.5037, 0.0925, 0.0058, 1.0398, 1.7655, 0.0216, -0.1349, -1.0968, 1.4534], [ 0.5162, -0.6815, 1.4828, 0.1271, 1.1672, 0.8569, 1.1472, 0.7672, 0.4720, 1.3629, 1.2446, 0.3470], [ 0.1047, 0.2153, -1.4652, -1.6907, -0.0670, 0.9863, 0.7836, -1.3762, 1.6670, 0.1276, 1.2241, 0.1415]], grad_fn=<EmbeddingBackward>), tensor([[-1.1156], [-1.1156], [-1.1156], [-0.7577]], grad_fn=<EmbeddingBackward>), tensor([[-0.8579, -0.4516, 0.3814, 0.9935], [ 0.6264, -1.1347, 1.9039, 1.1867], [ 0.6264, -1.1347, 1.9039, 1.1867], [ 0.1609, -0.1231, 0.5787, -0.3180]], grad_fn=<EmbeddingBackward>)] # We concatenate the embedding sections (12,1,4) into one (17) z = torch . cat ( embeddingz , 1 ) z tensor([[ 0.7031, 0.5132, -1.0168, 1.3594, 0.7909, -1.2648, 0.9565, 0.6827, 0.4922, 0.1282, -0.9863, 0.2622, -1.1156, -0.8579, -0.4516, 0.3814, 0.9935], [ 0.0402, -0.8209, 2.5186, 0.5037, 0.0925, 0.0058, 1.0398, 1.7655, 0.0216, -0.1349, -1.0968, 1.4534, -1.1156, 0.6264, -1.1347, 1.9039, 1.1867], [ 0.5162, -0.6815, 1.4828, 0.1271, 1.1672, 0.8569, 1.1472, 0.7672, 0.4720, 1.3629, 1.2446, 0.3470, -1.1156, 0.6264, -1.1347, 1.9039, 1.1867], [ 0.1047, 0.2153, -1.4652, -1.6907, -0.0670, 0.9863, 0.7836, -1.3762, 1.6670, 0.1276, 1.2241, 0.1415, -0.7577, 0.1609, -0.1231, 0.5787, -0.3180]], grad_fn=<CatBackward>) # This was assigned under the __init__() method selfembdrop = nn . Dropout ( .4 ) z = selfembdrop ( z ) z tensor([[ 0.0000, 0.8553, -1.6946, 0.0000, 1.3182, -2.1080, 1.5942, 1.1378, 0.8204, 0.2137, -1.6438, 0.0000, -0.0000, -1.4298, -0.0000, 0.6357, 0.0000], [ 0.0000, -1.3682, 0.0000, 0.8395, 0.0000, 0.0000, 0.0000, 2.9425, 0.0359, -0.0000, -1.8280, 2.4223, -0.0000, 0.0000, -1.8912, 3.1731, 0.0000], [ 0.0000, -1.1359, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2786, 0.7867, 2.2715, 0.0000, 0.5783, -0.0000, 1.0440, -1.8912, 3.1731, 0.0000], [ 0.0000, 0.3589, -2.4420, -0.0000, -0.1117, 1.6438, 1.3059, -0.0000, 2.7783, 0.2127, 2.0402, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, -0.5300]], grad_fn=<MulBackward0>) This is how the categorical embeddings are passed into the layers. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) x = self . layers ( x ) return x torch . manual_seed ( 33 ) model = TabularModel ( emb_szs , conts . shape [ 1 ], 1 , [ 200 , 100 ], p = 0.4 ) model TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=1, bias=True) ) )","title":"Define a TabularModel"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#define-loss-function-optimizer","text":"PyTorch does not offer a built-in RMSE Loss function, and it would be nice to see this in place of MSE. For this reason, we\u2019ll simply apply the torch.sqrt() function to the output of MSELoss during training. criterion = nn . MSELoss () # we'll convert this to RMSE later optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#perform-traintest-splits","text":"At this point our batch size is the entire dataset of 120,000 records. This will take a long time to train, so you might consider reducing this. We\u2019ll use 60,000. Recall that our tensors are already randomly shuffled. batch_size = 60000 test_size = int ( batch_size * .2 ) cat_train = cats [: batch_size - test_size ] cat_test = cats [ batch_size - test_size : batch_size ] con_train = conts [: batch_size - test_size ] con_test = conts [ batch_size - test_size : batch_size ] y_train = y [: batch_size - test_size ] y_test = y [ batch_size - test_size : batch_size ] len ( cat_train ) 48000 len ( cat_test ) 12000","title":"Perform train/test splits"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#train-the-model","text":"Expect this to take 30 minutes or more! We\u2019ve added code to tell us the duration at the end. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = torch . sqrt ( criterion ( y_pred , y_train )) # RMSE losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 12.49953079 epoch: 26 loss: 11.52666759 epoch: 51 loss: 10.47162533 epoch: 76 loss: 9.53090382 epoch: 101 loss: 8.72838020 epoch: 126 loss: 7.81538534 epoch: 151 loss: 6.70782852 epoch: 176 loss: 5.48520994 epoch: 201 loss: 4.37029028 epoch: 226 loss: 3.70538783 epoch: 251 loss: 3.51656818 epoch: 276 loss: 3.44707990 epoch: 300 loss: 3.42467499 Duration: 730 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#plot-the-loss-function","text":"plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'RMSE Loss' ) plt . xlabel ( 'epoch' );","title":"Plot the loss function"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#validate-the-model","text":"Here we want to run the entire test set through the model, and compare it to the known labels. For this step we don\u2019t want to update weights and biases, so we set torch.no_grad() # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = torch . sqrt ( criterion ( y_val , y_test )) print ( f 'RMSE: { loss : .8f } ' ) RMSE: 3.34590030 This means that on average, predicted values are within \u00b1$3.31 of the actual value. Now let\u2019s look at the first 50 predicted values: print ( f ' { \"PREDICTED\" : >12 } { \"ACTUAL\" : >8 } { \"DIFF\" : >8 } ' ) for i in range ( 50 ): diff = np . abs ( y_val [ i ] . item () - y_test [ i ] . item ()) print ( f ' { i + 1 : 2 } . { y_val [ i ] . item () : 8.4f } { y_test [ i ] . item () : 8.4f } { diff : 8.4f } ' ) PREDICTED ACTUAL DIFF 1. 2.5379 2.9000 0.3621 2. 25.1634 5.7000 19.4634 3. 6.3749 7.7000 1.3251 4. 13.4677 12.5000 0.9677 5. 4.4992 4.1000 0.3992 6. 4.8968 5.3000 0.4032 7. 3.1796 3.7000 0.5204 8. 17.7814 14.5000 3.2814 9. 6.1348 5.7000 0.4348 10. 12.0325 10.1000 1.9325 11. 6.1323 4.5000 1.6323 12. 6.9208 6.1000 0.8208 13. 5.9448 6.9000 0.9552 14. 13.4625 14.1000 0.6375 15. 5.9277 4.5000 1.4277 16. 27.5778 34.1000 6.5222 17. 3.2774 12.5000 9.2226 18. 5.7506 4.1000 1.6506 19. 8.1940 8.5000 0.3060 20. 6.2858 5.3000 0.9858 21. 13.6693 11.3000 2.3693 22. 9.6759 10.5000 0.8241 23. 16.0568 15.3000 0.7568 24. 19.3310 14.9000 4.4310 25. 48.6873 49.5700 0.8827 26. 6.3257 5.3000 1.0257 27. 6.0392 3.7000 2.3392 28. 7.1921 6.5000 0.6921 29. 14.9567 14.1000 0.8567 30. 6.7476 4.9000 1.8476 31. 4.3447 3.7000 0.6447 32. 35.6969 38.6700 2.9731 33. 13.9892 12.5000 1.4892 34. 12.8934 16.5000 3.6066 35. 6.3164 5.7000 0.6164 36. 5.9684 8.9000 2.9316 37. 16.1289 22.1000 5.9711 38. 7.6541 12.1000 4.4459 39. 8.6153 10.1000 1.4847 40. 4.0447 3.3000 0.7447 41. 10.2168 8.5000 1.7168 42. 8.8325 8.1000 0.7325 43. 15.2500 14.5000 0.7500 44. 6.3571 4.9000 1.4571 45. 9.7002 8.5000 1.2002 46. 12.1134 12.1000 0.0134 47. 24.3001 23.7000 0.6001 48. 2.8357 3.7000 0.8643 49. 6.8266 9.3000 2.4734 50. 8.2082 8.1000 0.1082 So while many predictions were off by a few cents, some were off by \\$19.00. Feel free to change the batch size, test size, and number of epochs to obtain a better model.","title":"Validate the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#save-the-model","text":"We can save a trained model to a file in case we want to come back later and feed new data through it. The best practice is to save the state of the model (weights & biases) and not the full definition. Also, we want to ensure that only a trained model is saved, to prevent overwriting a previously saved model with an untrained one. For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html # Make sure to save the model only after the training has happened! if len ( losses ) == epochs : torch . save ( model . state_dict (), 'TaxiFareRegrModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' )","title":"Save the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#loading-a-saved-model-starting-from-scratch","text":"We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. To demonstrate, restart the kernel before proceeding. import torch import torch.nn as nn import numpy as np import pandas as pd def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): r = 6371 phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) return r * c class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) return self . layers ( x ) Now define the model. Before we can load the saved settings, we need to instantiate our TabularModel with the parameters we used before (embedding sizes, number of continuous columns, output size, layer sizes, and dropout layer p-value). emb_szs = [( 24 , 12 ), ( 2 , 1 ), ( 7 , 4 )] model2 = TabularModel ( emb_szs , 6 , 1 , [ 200 , 100 ], p = 0.4 ) Once the model is set up, loading the saved settings is a snap. model2 . load_state_dict ( torch . load ( 'TaxiFareRegrModel.pt' )); model2 . eval () # be sure to run this step! TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=1, bias=True) ) ) Next we\u2019ll define a function that takes in new parameters from the user, performs all of the preprocessing steps above, and passes the new data through our trained model. def test_data ( mdl ): # pass in the name of the new model # INPUT NEW DATA plat = float ( input ( 'What is the pickup latitude? ' )) plong = float ( input ( 'What is the pickup longitude? ' )) dlat = float ( input ( 'What is the dropoff latitude? ' )) dlong = float ( input ( 'What is the dropoff longitude? ' )) psngr = int ( input ( 'How many passengers? ' )) dt = input ( 'What is the pickup date and time? \\n Format as YYYY-MM-DD HH:MM:SS ' ) # PREPROCESS THE DATA dfx_dict = { 'pickup_latitude' : plat , 'pickup_longitude' : plong , 'dropoff_latitude' : dlat , 'dropoff_longitude' : dlong , 'passenger_count' : psngr , 'EDTdate' : dt } dfx = pd . DataFrame ( dfx_dict , index = [ 0 ]) dfx [ 'dist_km' ] = haversine_distance ( dfx , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) dfx [ 'EDTdate' ] = pd . to_datetime ( dfx [ 'EDTdate' ]) # We can skip the .astype(category) step since our fields are small, # and encode them right away dfx [ 'Hour' ] = dfx [ 'EDTdate' ] . dt . hour dfx [ 'AMorPM' ] = np . where ( dfx [ 'Hour' ] < 12 , 0 , 1 ) dfx [ 'Weekday' ] = dfx [ 'EDTdate' ] . dt . strftime ( \" %a \" ) dfx [ 'Weekday' ] = dfx [ 'Weekday' ] . replace ([ 'Fri' , 'Mon' , 'Sat' , 'Sun' , 'Thu' , 'Tue' , 'Wed' ], [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ]) . astype ( 'int64' ) # CREATE CAT AND CONT TENSORS cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] xcats = np . stack ([ dfx [ col ] . values for col in cat_cols ], 1 ) xcats = torch . tensor ( xcats , dtype = torch . int64 ) xconts = np . stack ([ dfx [ col ] . values for col in cont_cols ], 1 ) xconts = torch . tensor ( xconts , dtype = torch . float ) # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( xcats , xconts ) print ( f ' \\n The predicted fare amount is $ { z . item () : .2f } ' )","title":"Loading a saved model (starting from scratch)"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04a-Full-ANN-Code-Along-Regression/#feed-new-data-through-the-trained-model","text":"For convenience, here are the max and min values for each of the variables: Column Minimum Maximum pickup_latitude 40 41 pickup_longitude -74.5 -73.3 dropoff_latitude 40 41 dropoff_longitude -74.5 -73.3 passenger_count 1 5 EDTdate 2010-04-11 00:00:00 2010-04-24 23:59:42 Use caution! The distance between 1 degree of latitude (from 40 to 41) is 111km (69mi) and between 1 degree of longitude (from -73 to -74) is 85km (53mi). The longest cab ride in the dataset spanned a difference of only 0.243 degrees latitude and 0.284 degrees longitude. The mean difference for both latitude and longitude was about 0.02. To get a fair prediction, use values that fall close to one another. z = test_data ( model2 ) What is the pickup latitude? 40.5 What is the pickup longitude? -73.9 What is the dropoff latitude? 40.52 What is the dropoff longitude? -73.92 How many passengers? 2 What is the pickup date and time? Format as YYYY-MM-DD HH:MM:SS 2010-04-15 16:00:00 The predicted fare amount is $13.86 ## Great job!","title":"Feed new data through the trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/","text":"Pytorch for Deep Learning BootCamp \u00b6 by Jawad Haider 04b - Full Artificial Neural Network Code Along - CLASSIFICATION \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Full Artificial Neural Network Code Along - CLASSIFICATION Working with tabular data Perform standard imports Load the NYC Taxi Fares dataset Calculate the distance traveled Add a datetime column and derive useful statistics Separate categorical from continuous columns Categorify Convert numpy arrays to tensors Set an embedding size Define a TabularModel Define loss function & optimizer Perform train/test splits Train the model Plot the loss function Validate the model Save the model Loading a saved model (starting from scratch) Feed new data through the trained model Full Artificial Neural Network Code Along - CLASSIFICATION \u00b6 In the last section we took in four continuous variables (lengths) to perform a classification. In this section we\u2019ll combine continuous and categorical data to perform a similar classification. The goal is to estimate the relative cost of a New York City cab ride from several inputs. The inspiration behind this code along is a recent Kaggle competition . NOTE: This notebook differs from the previous regression notebook in that it uses \u2018fare_class\u2019 for the y set, and the output contains two values instead of one. In this exercise we\u2019re training our model to perform a binary classification, and predict whether a fare is greater or less than \\$10.00. Working with tabular data \u00b6 Deep learning with neural networks is often associated with sophisticated image recognition, and in upcoming sections we\u2019ll train models based on properties like pixels patterns and colors. Here we\u2019re working with tabular data (spreadsheets, SQL tables, etc.) with columns of values that may or may not be relevant. As it happens, neural networks can learn to make connections we probably wouldn\u2019t have developed on our own. However, to do this we have to handle categorical values separately from continuous ones. Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * continuous vs. categorical values * embeddings * batch normalization * dropout layers Perform standard imports \u00b6 import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Load the NYC Taxi Fares dataset \u00b6 The Kaggle competition provides a dataset with about 55 million records. The data contains only the pickup date & time, the latitude & longitude (GPS coordinates) of the pickup and dropoff locations, and the number of passengers. It is up to the contest participant to extract any further information. For instance, does the time of day matter? The day of the week? How do we determine the distance traveled from pairs of GPS coordinates? For this exercise we\u2019ve whittled the dataset down to just 120,000 records from April 11 to April 24, 2010. The records are randomly sorted. We\u2019ll show how to calculate distance from GPS coordinates, and how to create a pandas datatime object from a text column. This will let us quickly get information like day of the week, am vs. pm, etc. Let\u2019s get started! df = pd . read_csv ( '../Data/NYCTaxiFares.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 df [ 'fare_class' ] . value_counts () 0 80000 1 40000 Name: fare_class, dtype: int64 Conveniently, \u2154 of the data have fares under \\$10, and \u2153 have fares \\$10 and above. Fare classes correspond to fare amounts as follows: Class Values 0 \\ < \\$10.00 1 > = \\$10.00 > > > Calculate the distance traveled \u00b6 The haversine formula calculates the distance on a sphere between two sets of GPS coordinates. Here we assign latitude values with (phi) and longitude with (lambda). The distance formula works out to where def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): \"\"\" Calculates the haversine distance between 2 sets of GPS coordinates in df \"\"\" r = 6371 # average radius of Earth in kilometers phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) d = ( r * c ) # in kilometers return d df [ 'dist_km' ] = haversine_distance ( df , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 Add a datetime column and derive useful statistics \u00b6 By creating a datetime object, we can extract information like \u201cday of the week\u201d, \u201cam vs. pm\u201d etc. Note that the data was saved in UTC time. Our data falls in April of 2010 which occurred during Daylight Savings Time in New York. For that reason, we\u2019ll make an adjustment to EDT using UTC-4 (subtracting four hours). df [ 'EDTdate' ] = pd . to_datetime ( df [ 'pickup_datetime' ] . str [: 19 ]) - pd . Timedelta ( hours = 4 ) df [ 'Hour' ] = df [ 'EDTdate' ] . dt . hour df [ 'AMorPM' ] = np . where ( df [ 'Hour' ] < 12 , 'am' , 'pm' ) df [ 'Weekday' ] = df [ 'EDTdate' ] . dt . strftime ( \" %a \" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km EDTdate Hour AMorPM Weekday 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 2010-04-19 04:17:56 4 am Mon 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2010-04-17 11:43:53 11 am Sat 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 2010-04-17 07:23:26 7 am Sat 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 2010-04-11 17:25:03 17 pm Sun 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 2010-04-16 22:19:01 22 pm Fri df [ 'EDTdate' ] . min () Timestamp('2010-04-11 00:00:10') df [ 'EDTdate' ] . max () Timestamp('2010-04-24 23:59:42') Separate categorical from continuous columns \u00b6 df . columns Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km', 'EDTdate', 'Hour', 'AMorPM', 'Weekday'], dtype='object') cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] y_col = [ 'fare_class' ] # this column contains the labels NOTE: If you plan to use all of the columns in the data table, there\u2019s a shortcut to grab the remaining continuous columns: cont_cols = [col for col in df.columns if col not in cat_cols + y_col] Here we entered the continuous columns explicitly because there are columns we\u2019re not running through the model (fare_amount and EDTdate) Categorify \u00b6 Pandas offers a category dtype for converting categorical values to numerical codes. A dataset containing months of the year will be assigned 12 codes, one for each month. These will usually be the integers 0 to 11. Pandas replaces the column values with codes, and retains an index list of category values. In the steps ahead we\u2019ll call the categorical values \u201cnames\u201d and the encodings \u201ccodes\u201d. # Convert our three categorical columns to category dtypes. for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . dtypes pickup_datetime object fare_amount float64 fare_class int64 pickup_longitude float64 pickup_latitude float64 dropoff_longitude float64 dropoff_latitude float64 passenger_count int64 dist_km float64 EDTdate datetime64[ns] Hour category AMorPM category Weekday category dtype: object We can see that df[\u2018Hour\u2019] is a categorical feature by displaying some of the rows: df [ 'Hour' ] . head () 0 4 1 11 2 7 3 17 4 22 Name: Hour, dtype: category Categories (24, int64): [0, 1, 2, 3, ..., 20, 21, 22, 23] Here our categorical names are the integers 0 through 23, for a total of 24 unique categories. These values also correspond to the codes assigned to each name. We can access the category names with Series.cat.categories or just the codes with Series.cat.codes . This will make more sense if we look at df[\u2018AMorPM\u2019] : df [ 'AMorPM' ] . head () 0 am 1 am 2 am 3 pm 4 pm Name: AMorPM, dtype: category Categories (2, object): [am, pm] df [ 'AMorPM' ] . cat . categories Index(['am', 'pm'], dtype='object') df [ 'AMorPM' ] . head () . cat . codes 0 0 1 0 2 0 3 1 4 1 dtype: int8 df [ 'Weekday' ] . cat . categories Index(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype='object') df [ 'Weekday' ] . head () . cat . codes 0 1 1 2 2 2 3 3 4 0 dtype: int8 NOTE: NaN values in categorical data are assigned a code of -1. We don\u2019t have any in this particular dataset. Now we want to combine the three categorical columns into one input array using numpy.stack We don\u2019t want the Series index, just the values. hr = df [ 'Hour' ] . cat . codes . values ampm = df [ 'AMorPM' ] . cat . codes . values wkdy = df [ 'Weekday' ] . cat . codes . values cats = np . stack ([ hr , ampm , wkdy ], 1 ) cats [: 5 ] array([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]], dtype=int8) NOTE: This can be done in one line of code using a list comprehension: cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1) Don\u2019t worry about the dtype for now, we can make it int64 when we convert it to a tensor. Convert numpy arrays to tensors \u00b6 # Convert categorical variables to a tensor cats = torch . tensor ( cats , dtype = torch . int64 ) # this syntax is ok, since the source data is an array, not an existing tensor cats [: 5 ] tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]]) We can feed all of our continuous variables into the model as a tensor. We\u2019re not normalizing the values here; we\u2019ll let the model perform this step. NOTE: We have to store conts and y as Float (float32) tensors, not Double (float64) in order for batch normalization to work properly. # Convert continuous variables to a tensor conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts = torch . tensor ( conts , dtype = torch . float ) conts [: 5 ] tensor([[ 40.7305, -73.9924, 40.7447, -73.9755, 1.0000, 2.1263], [ 40.7406, -73.9901, 40.7441, -73.9742, 1.0000, 1.3923], [ 40.7511, -73.9941, 40.7662, -73.9601, 2.0000, 3.3268], [ 40.7564, -73.9905, 40.7482, -73.9712, 1.0000, 1.8641], [ 40.7342, -73.9910, 40.7431, -73.9060, 1.0000, 7.2313]]) conts . type () 'torch.FloatTensor' Note: the CrossEntropyLoss function we\u2019ll use below expects a 1d y-tensor, so we\u2019ll replace .reshape(-1,1) with .flatten() this time. # Convert labels to a tensor y = torch . tensor ( df [ y_col ] . values ) . flatten () y [: 5 ] tensor([0, 0, 1, 0, 1]) cats . shape torch.Size([120000, 3]) conts . shape torch.Size([120000, 6]) y . shape torch.Size([120000]) Set an embedding size \u00b6 The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50. # This will set embedding sizes for Hours, AMvsPM and Weekdays cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(24, 12), (2, 1), (7, 4)] Define a TabularModel \u00b6 This somewhat follows the fast.ai library The goal is to define a model based on the number of continuous columns (given by conts.shape[1] ) plus the number of categorical columns and their embeddings (given by len(emb_szs) and emb_szs respectively). The output would either be a regression (a single float value), or a classification (a group of bins and their softmax values). For this exercise our output will be a single regression value. Note that we\u2019ll assume our data contains both categorical and continuous data. You can add boolean parameters to your own model class to handle a variety of datasets. Let\u2019s walk through the steps we\u2019re about to take. See below for more detailed illustrations of the steps. 1. Extend the base Module class, set up the following parameters: - emb_szs: list of tuples: each categorical variable size is paired with an embedding size - n_cont: int: number of continuous variables - out_sz: int: output size - layers: list of ints: layer sizes - p: float: dropout probability for each layer (for simplicity we\u2019ll use the same value throughout) class TabularModel(nn.Module): def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5): super().\\_\\_init\\_\\_() 2. Set up the embedded layers with torch.nn.ModuleList() and torch.nn.Embedding() Categorical data will be filtered through these Embeddings in the forward section. self.embeds = nn.ModuleList(\\[nn.Embedding(ni, nf) for ni,nf in emb_szs\\]) 3. Set up a dropout function for the embeddings with torch.nn.Dropout() The default p-value=0.5 self.emb_drop = nn.Dropout(emb_drop) 4. Set up a normalization function for the continuous variables with torch.nn.BatchNorm1d() self.bn_cont = nn.BatchNorm1d(n_cont) 5. Set up a sequence of neural network layers where each level includes a Linear function, an activation function (we\u2019ll use ReLU ), a normalization step, and a dropout layer. We\u2019ll combine the list of layers with torch.nn.Sequential() self.bn_cont = nn.BatchNorm1d(n_cont) layerlist = \\[\\] n_emb = sum((nf for ni,nf in emb_szs)) n_in = n_emb + n_cont for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) layerlist.append(nn.BatchNorm1d(i)) layerlist.append(nn.Dropout(p)) n_in = i layerlist.append(nn.Linear(layers\\[-1\\],out_sz)) self.layers = nn.Sequential(\\*layerlist) 6. Define the forward method. Preprocess the embeddings and normalize the continuous variables before passing them through the layers. Use torch.cat() to combine multiple tensors into one. def forward(self, x_cat, x_cont): embeddings = \\[\\] for i,e in enumerate(self.embeds): embeddings.append(e(x_cat\\[:,i\\])) x = torch.cat(embeddings, 1) x = self.emb_drop(x) x_cont = self.bn_cont(x_cont) x = torch.cat(\\[x, x_cont\\], 1) x = self.layers(x) return x Breaking down the embeddings steps (this code is for illustration purposes only.) # This is our source data catz = cats [: 4 ] catz tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3]]) # This is passed in when the model is instantiated emb_szs [(24, 12), (2, 1), (7, 4)] # This is assigned inside the __init__() method selfembeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) selfembeds ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) list ( enumerate ( selfembeds )) [(0, Embedding(24, 12)), (1, Embedding(2, 1)), (2, Embedding(7, 4))] # This happens inside the forward() method embeddingz = [] for i , e in enumerate ( selfembeds ): embeddingz . append ( e ( catz [:, i ])) embeddingz [tensor([[ 0.0347, 0.3536, -1.2988, 1.6375, -0.0542, -0.2099, 0.3044, -1.2855, 0.8831, -0.7109, -0.9646, -0.1356], [-0.5039, -0.9924, 1.2296, -0.6908, 0.4641, -1.0487, 0.5577, -1.1560, 0.8318, -0.0834, 1.2123, -0.6210], [ 0.3509, 0.2216, 0.3432, 1.4547, -0.8747, 1.6727, -0.6417, -1.0160, 0.8217, -1.0531, 0.8357, -0.0637], [ 0.7978, 0.4566, 1.0926, -0.4095, -0.3366, 1.0216, 0.3601, -0.2927, 0.3536, 0.2170, -1.4778, -1.1965]], grad_fn=<EmbeddingBackward>), tensor([[-0.9676], [-0.9676], [-0.9676], [-1.0656]], grad_fn=<EmbeddingBackward>), tensor([[-2.1762, 1.0210, 1.3557, -0.1804], [-1.0131, 0.9989, -0.4746, -0.1461], [-1.0131, 0.9989, -0.4746, -0.1461], [-0.3646, -3.2237, -0.9956, 0.2598]], grad_fn=<EmbeddingBackward>)] # We concatenate the embedding sections (12,1,4) into one (17) z = torch . cat ( embeddingz , 1 ) z tensor([[ 0.0347, 0.3536, -1.2988, 1.6375, -0.0542, -0.2099, 0.3044, -1.2855, 0.8831, -0.7109, -0.9646, -0.1356, -0.9676, -2.1762, 1.0210, 1.3557, -0.1804], [-0.5039, -0.9924, 1.2296, -0.6908, 0.4641, -1.0487, 0.5577, -1.1560, 0.8318, -0.0834, 1.2123, -0.6210, -0.9676, -1.0131, 0.9989, -0.4746, -0.1461], [ 0.3509, 0.2216, 0.3432, 1.4547, -0.8747, 1.6727, -0.6417, -1.0160, 0.8217, -1.0531, 0.8357, -0.0637, -0.9676, -1.0131, 0.9989, -0.4746, -0.1461], [ 0.7978, 0.4566, 1.0926, -0.4095, -0.3366, 1.0216, 0.3601, -0.2927, 0.3536, 0.2170, -1.4778, -1.1965, -1.0656, -0.3646, -3.2237, -0.9956, 0.2598]], grad_fn=<CatBackward>) # This was assigned under the __init__() method selfembdrop = nn . Dropout ( .4 ) z = selfembdrop ( z ) z tensor([[ 0.0000, 0.0000, -2.1647, 0.0000, -0.0000, -0.3498, 0.5073, -2.1424, 0.0000, -1.1848, -1.6076, -0.2259, -1.6127, -3.6271, 0.0000, 2.2594, -0.3007], [-0.8398, -0.0000, 0.0000, -0.0000, 0.7734, -1.7478, 0.0000, -1.9267, 0.0000, -0.1390, 0.0000, -1.0350, -0.0000, -0.0000, 1.6648, -0.0000, -0.2435], [ 0.0000, 0.3693, 0.5719, 0.0000, -1.4578, 0.0000, -1.0694, -1.6933, 0.0000, -1.7552, 1.3929, -0.1062, -1.6127, -1.6886, 1.6648, -0.0000, -0.0000], [ 1.3297, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000, -0.4879, 0.0000, 0.0000, -2.4631, -1.9941, -1.7760, -0.6077, -5.3728, -1.6593, 0.4330]], grad_fn=<MulBackward0>) This is how the categorical embeddings are passed into the layers. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) x = self . layers ( x ) return x torch . manual_seed ( 33 ) model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 200 , 100 ], p = 0.4 ) # out_sz = 2 model TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=2, bias=True) ) ) Define loss function & optimizer \u00b6 For our classification we\u2019ll replace the MSE loss function with torch.nn.CrossEntropyLoss() For the optimizer, we\u2019ll continue to use torch.optim.Adam() criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) Perform train/test splits \u00b6 At this point our batch size is the entire dataset of 120,000 records. To save time we\u2019ll use the first 60,000. Recall that our tensors are already randomly shuffled. batch_size = 60000 test_size = 12000 cat_train = cats [: batch_size - test_size ] cat_test = cats [ batch_size - test_size : batch_size ] con_train = conts [: batch_size - test_size ] con_test = conts [ batch_size - test_size : batch_size ] y_train = y [: batch_size - test_size ] y_test = y [ batch_size - test_size : batch_size ] len ( cat_train ) 48000 len ( cat_test ) 12000 Train the model \u00b6 Expect this to take 30 minutes or more! We\u2019ve added code to tell us the duration at the end. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.73441482 epoch: 26 loss: 0.45090991 epoch: 51 loss: 0.35915938 epoch: 76 loss: 0.31940848 epoch: 101 loss: 0.29913244 epoch: 126 loss: 0.28824982 epoch: 151 loss: 0.28091952 epoch: 176 loss: 0.27713534 epoch: 201 loss: 0.27236161 epoch: 226 loss: 0.27171907 epoch: 251 loss: 0.26830241 epoch: 276 loss: 0.26365638 epoch: 300 loss: 0.25949642 Duration: 709 seconds Plot the loss function \u00b6 plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Cross Entropy Loss' ) plt . xlabel ( 'epoch' ); Validate the model \u00b6 # TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = criterion ( y_val , y_test ) print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.25455481 Now let\u2019s look at the first 50 predicted values rows = 50 correct = 0 print ( f ' { \"MODEL OUTPUT\" : 26 } ARGMAX Y_TEST' ) for i in range ( rows ): print ( f ' { str ( y_val [ i ]) : 26 } { y_val [ i ] . argmax () : ^7 }{ y_test [ i ] : ^7 } ' ) if y_val [ i ] . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { rows } = { 100 * correct / rows : .2f } % correct' ) MODEL OUTPUT ARGMAX Y_TEST tensor([ 1.8140, -1.6443]) 0 0 tensor([-1.8268, 2.6373]) 1 0 tensor([ 1.4028, -1.9248]) 0 0 tensor([-1.9130, 1.4853]) 1 1 tensor([ 1.1757, -2.4964]) 0 0 tensor([ 2.0996, -2.2990]) 0 0 tensor([ 1.3226, -1.8349]) 0 0 tensor([-1.6211, 2.3889]) 1 1 tensor([ 2.2489, -2.4253]) 0 0 tensor([-0.4459, 1.1358]) 1 1 tensor([ 1.5145, -2.1619]) 0 0 tensor([ 0.7704, -1.9443]) 0 0 tensor([ 0.9637, -1.3796]) 0 0 tensor([-1.3527, 1.7322]) 1 1 tensor([ 1.4110, -2.4595]) 0 0 tensor([-1.4455, 2.6081]) 1 1 tensor([ 2.2798, -2.5864]) 0 1 tensor([ 1.4585, -2.7982]) 0 0 tensor([ 0.3342, -0.8995]) 0 0 tensor([ 2.0525, -1.9737]) 0 0 tensor([-1.3571, 2.1911]) 1 1 tensor([-0.4669, 0.2872]) 1 1 tensor([-2.0624, 2.2875]) 1 1 tensor([-2.1334, 2.6416]) 1 1 tensor([-3.1325, 5.1561]) 1 1 tensor([ 2.2128, -2.5172]) 0 0 tensor([ 1.0346, -1.7764]) 0 0 tensor([ 1.1221, -1.6717]) 0 0 tensor([-2.1322, 1.6714]) 1 1 tensor([ 1.5009, -1.6338]) 0 0 tensor([ 2.0387, -1.8475]) 0 0 tensor([-1.6346, 2.8899]) 1 1 tensor([-3.0129, 2.3519]) 1 1 tensor([-1.5746, 2.0000]) 1 1 tensor([ 1.3056, -2.2630]) 0 0 tensor([ 0.6631, -1.4797]) 0 0 tensor([-1.4585, 2.1836]) 1 1 tensor([ 1.0574, -1.5848]) 0 1 tensor([ 0.3376, -0.8050]) 0 1 tensor([ 1.9217, -1.9764]) 0 0 tensor([ 0.1011, -0.5529]) 0 0 tensor([ 0.6703, -0.5540]) 0 0 tensor([-0.6733, 0.8777]) 1 1 tensor([ 2.2017, -2.0445]) 0 0 tensor([-0.0442, -0.4276]) 0 0 tensor([-1.1204, 1.2558]) 1 1 tensor([-1.8170, 2.7124]) 1 1 tensor([ 1.7404, -2.0341]) 0 0 tensor([ 1.3266, -2.3039]) 0 0 tensor([-0.0671, 0.3291]) 1 0 45 out of 50 = 90.00% correct Save the model \u00b6 Save the trained model to a file in case you want to come back later and feed new data through it. # Make sure to save the model only after the training has happened! if len ( losses ) == epochs : torch . save ( model . state_dict (), 'TaxiFareClssModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' ) Loading a saved model (starting from scratch) \u00b6 We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. To demonstrate, restart the kernel before proceeding. import torch import torch.nn as nn import numpy as np import pandas as pd def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): r = 6371 phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) return r * c class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) return self . layers ( x ) Now define the model. Before we can load the saved settings, we need to instantiate our TabularModel with the parameters we used before (embedding sizes, number of continuous columns, output size, layer sizes, and dropout layer p-value). emb_szs = [( 24 , 12 ), ( 2 , 1 ), ( 7 , 4 )] model2 = TabularModel ( emb_szs , 6 , 2 , [ 200 , 100 ], p = 0.4 ) Once the model is set up, loading the saved settings is a snap. model2 . load_state_dict ( torch . load ( 'TaxiFareClssModel.pt' )); model2 . eval () # be sure to run this step! TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=2, bias=True) ) ) Next we\u2019ll define a function that takes in new parameters from the user, performs all of the preprocessing steps above, and passes the new data through our trained model. def test_data ( mdl ): # pass in the name of the new model # INPUT NEW DATA plat = float ( input ( 'What is the pickup latitude? ' )) plong = float ( input ( 'What is the pickup longitude? ' )) dlat = float ( input ( 'What is the dropoff latitude? ' )) dlong = float ( input ( 'What is the dropoff longitude? ' )) psngr = int ( input ( 'How many passengers? ' )) dt = input ( 'What is the pickup date and time? \\n Format as YYYY-MM-DD HH:MM:SS ' ) # PREPROCESS THE DATA dfx_dict = { 'pickup_latitude' : plat , 'pickup_longitude' : plong , 'dropoff_latitude' : dlat , 'dropoff_longitude' : dlong , 'passenger_count' : psngr , 'EDTdate' : dt } dfx = pd . DataFrame ( dfx_dict , index = [ 0 ]) dfx [ 'dist_km' ] = haversine_distance ( dfx , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) dfx [ 'EDTdate' ] = pd . to_datetime ( dfx [ 'EDTdate' ]) # We can skip the .astype(category) step since our fields are small, # and encode them right away dfx [ 'Hour' ] = dfx [ 'EDTdate' ] . dt . hour dfx [ 'AMorPM' ] = np . where ( dfx [ 'Hour' ] < 12 , 0 , 1 ) dfx [ 'Weekday' ] = dfx [ 'EDTdate' ] . dt . strftime ( \" %a \" ) dfx [ 'Weekday' ] = dfx [ 'Weekday' ] . replace ([ 'Fri' , 'Mon' , 'Sat' , 'Sun' , 'Thu' , 'Tue' , 'Wed' ], [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ]) . astype ( 'int64' ) # CREATE CAT AND CONT TENSORS cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] xcats = np . stack ([ dfx [ col ] . values for col in cat_cols ], 1 ) xcats = torch . tensor ( xcats , dtype = torch . int64 ) xconts = np . stack ([ dfx [ col ] . values for col in cont_cols ], 1 ) xconts = torch . tensor ( xconts , dtype = torch . float ) # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( xcats , xconts ) . argmax () . item () print ( f ' \\n The predicted fare class is { z } ' ) Feed new data through the trained model \u00b6 For convenience, here are the max and min values for each of the variables: Column Minimum Maximum pickup_latitude 40 41 pickup_longitude -74.5 -73.3 dropoff_latitude 40 41 dropoff_longitude -74.5 -73.3 passenger_count 1 5 EDTdate 2010-04-11 00:00:00 2010-04-24 23:59:42 Use caution! The distance between 1 degree of latitude (from 40 to 41) is 111km (69mi) and between 1 degree of longitude (from -73 to -74) is 85km (53mi). The longest cab ride in the dataset spanned a difference of only 0.243 degrees latitude and 0.284 degrees longitude. The mean difference for both latitude and longitude was about 0.02. To get a fair prediction, use values that fall close to one another. test_data ( model2 ) What is the pickup latitude? 40.5 What is the pickup longitude? -73.9 What is the dropoff latitude? 40.52 What is the dropoff longitude? -73.92 How many passengers? 2 What is the pickup date and time? Format as YYYY-MM-DD HH:MM:SS 2010-04-15 16:00:00 The predicted fare class is 1 Perfect! Where our regression predicted a fare value of \\~\\\\\\$14, our binary classification predicts a fare greater than \\$10. \\## Great job!","title":"04b Full ANN Code Along Classification"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#pytorch-for-deep-learning-bootcamp","text":"by Jawad Haider","title":"Pytorch for Deep Learning BootCamp"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#04b-full-artificial-neural-network-code-along-classification","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Full Artificial Neural Network Code Along - CLASSIFICATION Working with tabular data Perform standard imports Load the NYC Taxi Fares dataset Calculate the distance traveled Add a datetime column and derive useful statistics Separate categorical from continuous columns Categorify Convert numpy arrays to tensors Set an embedding size Define a TabularModel Define loss function & optimizer Perform train/test splits Train the model Plot the loss function Validate the model Save the model Loading a saved model (starting from scratch) Feed new data through the trained model","title":"04b - Full Artificial Neural Network Code Along - CLASSIFICATION"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#full-artificial-neural-network-code-along-classification","text":"In the last section we took in four continuous variables (lengths) to perform a classification. In this section we\u2019ll combine continuous and categorical data to perform a similar classification. The goal is to estimate the relative cost of a New York City cab ride from several inputs. The inspiration behind this code along is a recent Kaggle competition . NOTE: This notebook differs from the previous regression notebook in that it uses \u2018fare_class\u2019 for the y set, and the output contains two values instead of one. In this exercise we\u2019re training our model to perform a binary classification, and predict whether a fare is greater or less than \\$10.00.","title":"Full Artificial Neural Network Code Along - CLASSIFICATION"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#working-with-tabular-data","text":"Deep learning with neural networks is often associated with sophisticated image recognition, and in upcoming sections we\u2019ll train models based on properties like pixels patterns and colors. Here we\u2019re working with tabular data (spreadsheets, SQL tables, etc.) with columns of values that may or may not be relevant. As it happens, neural networks can learn to make connections we probably wouldn\u2019t have developed on our own. However, to do this we have to handle categorical values separately from continuous ones. Make sure to watch the theory lectures! You\u2019ll want to be comfortable with: * continuous vs. categorical values * embeddings * batch normalization * dropout layers","title":"Working with tabular data"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#perform-standard-imports","text":"import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#load-the-nyc-taxi-fares-dataset","text":"The Kaggle competition provides a dataset with about 55 million records. The data contains only the pickup date & time, the latitude & longitude (GPS coordinates) of the pickup and dropoff locations, and the number of passengers. It is up to the contest participant to extract any further information. For instance, does the time of day matter? The day of the week? How do we determine the distance traveled from pairs of GPS coordinates? For this exercise we\u2019ve whittled the dataset down to just 120,000 records from April 11 to April 24, 2010. The records are randomly sorted. We\u2019ll show how to calculate distance from GPS coordinates, and how to create a pandas datatime object from a text column. This will let us quickly get information like day of the week, am vs. pm, etc. Let\u2019s get started! df = pd . read_csv ( '../Data/NYCTaxiFares.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 df [ 'fare_class' ] . value_counts () 0 80000 1 40000 Name: fare_class, dtype: int64 Conveniently, \u2154 of the data have fares under \\$10, and \u2153 have fares \\$10 and above. Fare classes correspond to fare amounts as follows: Class Values 0 \\ < \\$10.00 1 > = \\$10.00 > > >","title":"Load the NYC Taxi Fares dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#calculate-the-distance-traveled","text":"The haversine formula calculates the distance on a sphere between two sets of GPS coordinates. Here we assign latitude values with (phi) and longitude with (lambda). The distance formula works out to where def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): \"\"\" Calculates the haversine distance between 2 sets of GPS coordinates in df \"\"\" r = 6371 # average radius of Earth in kilometers phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) d = ( r * c ) # in kilometers return d df [ 'dist_km' ] = haversine_distance ( df , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321","title":"Calculate the distance traveled"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#add-a-datetime-column-and-derive-useful-statistics","text":"By creating a datetime object, we can extract information like \u201cday of the week\u201d, \u201cam vs. pm\u201d etc. Note that the data was saved in UTC time. Our data falls in April of 2010 which occurred during Daylight Savings Time in New York. For that reason, we\u2019ll make an adjustment to EDT using UTC-4 (subtracting four hours). df [ 'EDTdate' ] = pd . to_datetime ( df [ 'pickup_datetime' ] . str [: 19 ]) - pd . Timedelta ( hours = 4 ) df [ 'Hour' ] = df [ 'EDTdate' ] . dt . hour df [ 'AMorPM' ] = np . where ( df [ 'Hour' ] < 12 , 'am' , 'pm' ) df [ 'Weekday' ] = df [ 'EDTdate' ] . dt . strftime ( \" %a \" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pickup_datetime fare_amount fare_class pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count dist_km EDTdate Hour AMorPM Weekday 0 2010-04-19 08:17:56 UTC 6.5 0 -73.992365 40.730521 -73.975499 40.744746 1 2.126312 2010-04-19 04:17:56 4 am Mon 1 2010-04-17 15:43:53 UTC 6.9 0 -73.990078 40.740558 -73.974232 40.744114 1 1.392307 2010-04-17 11:43:53 11 am Sat 2 2010-04-17 11:23:26 UTC 10.1 1 -73.994149 40.751118 -73.960064 40.766235 2 3.326763 2010-04-17 07:23:26 7 am Sat 3 2010-04-11 21:25:03 UTC 8.9 0 -73.990485 40.756422 -73.971205 40.748192 1 1.864129 2010-04-11 17:25:03 17 pm Sun 4 2010-04-17 02:19:01 UTC 19.7 1 -73.990976 40.734202 -73.905956 40.743115 1 7.231321 2010-04-16 22:19:01 22 pm Fri df [ 'EDTdate' ] . min () Timestamp('2010-04-11 00:00:10') df [ 'EDTdate' ] . max () Timestamp('2010-04-24 23:59:42')","title":"Add a datetime column and derive useful statistics"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#separate-categorical-from-continuous-columns","text":"df . columns Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km', 'EDTdate', 'Hour', 'AMorPM', 'Weekday'], dtype='object') cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] y_col = [ 'fare_class' ] # this column contains the labels NOTE: If you plan to use all of the columns in the data table, there\u2019s a shortcut to grab the remaining continuous columns: cont_cols = [col for col in df.columns if col not in cat_cols + y_col] Here we entered the continuous columns explicitly because there are columns we\u2019re not running through the model (fare_amount and EDTdate)","title":"Separate categorical from continuous columns"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#categorify","text":"Pandas offers a category dtype for converting categorical values to numerical codes. A dataset containing months of the year will be assigned 12 codes, one for each month. These will usually be the integers 0 to 11. Pandas replaces the column values with codes, and retains an index list of category values. In the steps ahead we\u2019ll call the categorical values \u201cnames\u201d and the encodings \u201ccodes\u201d. # Convert our three categorical columns to category dtypes. for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . dtypes pickup_datetime object fare_amount float64 fare_class int64 pickup_longitude float64 pickup_latitude float64 dropoff_longitude float64 dropoff_latitude float64 passenger_count int64 dist_km float64 EDTdate datetime64[ns] Hour category AMorPM category Weekday category dtype: object We can see that df[\u2018Hour\u2019] is a categorical feature by displaying some of the rows: df [ 'Hour' ] . head () 0 4 1 11 2 7 3 17 4 22 Name: Hour, dtype: category Categories (24, int64): [0, 1, 2, 3, ..., 20, 21, 22, 23] Here our categorical names are the integers 0 through 23, for a total of 24 unique categories. These values also correspond to the codes assigned to each name. We can access the category names with Series.cat.categories or just the codes with Series.cat.codes . This will make more sense if we look at df[\u2018AMorPM\u2019] : df [ 'AMorPM' ] . head () 0 am 1 am 2 am 3 pm 4 pm Name: AMorPM, dtype: category Categories (2, object): [am, pm] df [ 'AMorPM' ] . cat . categories Index(['am', 'pm'], dtype='object') df [ 'AMorPM' ] . head () . cat . codes 0 0 1 0 2 0 3 1 4 1 dtype: int8 df [ 'Weekday' ] . cat . categories Index(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype='object') df [ 'Weekday' ] . head () . cat . codes 0 1 1 2 2 2 3 3 4 0 dtype: int8 NOTE: NaN values in categorical data are assigned a code of -1. We don\u2019t have any in this particular dataset. Now we want to combine the three categorical columns into one input array using numpy.stack We don\u2019t want the Series index, just the values. hr = df [ 'Hour' ] . cat . codes . values ampm = df [ 'AMorPM' ] . cat . codes . values wkdy = df [ 'Weekday' ] . cat . codes . values cats = np . stack ([ hr , ampm , wkdy ], 1 ) cats [: 5 ] array([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]], dtype=int8) NOTE: This can be done in one line of code using a list comprehension: cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1) Don\u2019t worry about the dtype for now, we can make it int64 when we convert it to a tensor.","title":"Categorify"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#convert-numpy-arrays-to-tensors","text":"# Convert categorical variables to a tensor cats = torch . tensor ( cats , dtype = torch . int64 ) # this syntax is ok, since the source data is an array, not an existing tensor cats [: 5 ] tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3], [22, 1, 0]]) We can feed all of our continuous variables into the model as a tensor. We\u2019re not normalizing the values here; we\u2019ll let the model perform this step. NOTE: We have to store conts and y as Float (float32) tensors, not Double (float64) in order for batch normalization to work properly. # Convert continuous variables to a tensor conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts = torch . tensor ( conts , dtype = torch . float ) conts [: 5 ] tensor([[ 40.7305, -73.9924, 40.7447, -73.9755, 1.0000, 2.1263], [ 40.7406, -73.9901, 40.7441, -73.9742, 1.0000, 1.3923], [ 40.7511, -73.9941, 40.7662, -73.9601, 2.0000, 3.3268], [ 40.7564, -73.9905, 40.7482, -73.9712, 1.0000, 1.8641], [ 40.7342, -73.9910, 40.7431, -73.9060, 1.0000, 7.2313]]) conts . type () 'torch.FloatTensor' Note: the CrossEntropyLoss function we\u2019ll use below expects a 1d y-tensor, so we\u2019ll replace .reshape(-1,1) with .flatten() this time. # Convert labels to a tensor y = torch . tensor ( df [ y_col ] . values ) . flatten () y [: 5 ] tensor([0, 0, 1, 0, 1]) cats . shape torch.Size([120000, 3]) conts . shape torch.Size([120000, 6]) y . shape torch.Size([120000])","title":"Convert numpy arrays to tensors"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#set-an-embedding-size","text":"The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50. # This will set embedding sizes for Hours, AMvsPM and Weekdays cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(24, 12), (2, 1), (7, 4)]","title":"Set an embedding size"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#define-a-tabularmodel","text":"This somewhat follows the fast.ai library The goal is to define a model based on the number of continuous columns (given by conts.shape[1] ) plus the number of categorical columns and their embeddings (given by len(emb_szs) and emb_szs respectively). The output would either be a regression (a single float value), or a classification (a group of bins and their softmax values). For this exercise our output will be a single regression value. Note that we\u2019ll assume our data contains both categorical and continuous data. You can add boolean parameters to your own model class to handle a variety of datasets. Let\u2019s walk through the steps we\u2019re about to take. See below for more detailed illustrations of the steps. 1. Extend the base Module class, set up the following parameters: - emb_szs: list of tuples: each categorical variable size is paired with an embedding size - n_cont: int: number of continuous variables - out_sz: int: output size - layers: list of ints: layer sizes - p: float: dropout probability for each layer (for simplicity we\u2019ll use the same value throughout) class TabularModel(nn.Module): def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5): super().\\_\\_init\\_\\_() 2. Set up the embedded layers with torch.nn.ModuleList() and torch.nn.Embedding() Categorical data will be filtered through these Embeddings in the forward section. self.embeds = nn.ModuleList(\\[nn.Embedding(ni, nf) for ni,nf in emb_szs\\]) 3. Set up a dropout function for the embeddings with torch.nn.Dropout() The default p-value=0.5 self.emb_drop = nn.Dropout(emb_drop) 4. Set up a normalization function for the continuous variables with torch.nn.BatchNorm1d() self.bn_cont = nn.BatchNorm1d(n_cont) 5. Set up a sequence of neural network layers where each level includes a Linear function, an activation function (we\u2019ll use ReLU ), a normalization step, and a dropout layer. We\u2019ll combine the list of layers with torch.nn.Sequential() self.bn_cont = nn.BatchNorm1d(n_cont) layerlist = \\[\\] n_emb = sum((nf for ni,nf in emb_szs)) n_in = n_emb + n_cont for i in layers: layerlist.append(nn.Linear(n_in,i)) layerlist.append(nn.ReLU(inplace=True)) layerlist.append(nn.BatchNorm1d(i)) layerlist.append(nn.Dropout(p)) n_in = i layerlist.append(nn.Linear(layers\\[-1\\],out_sz)) self.layers = nn.Sequential(\\*layerlist) 6. Define the forward method. Preprocess the embeddings and normalize the continuous variables before passing them through the layers. Use torch.cat() to combine multiple tensors into one. def forward(self, x_cat, x_cont): embeddings = \\[\\] for i,e in enumerate(self.embeds): embeddings.append(e(x_cat\\[:,i\\])) x = torch.cat(embeddings, 1) x = self.emb_drop(x) x_cont = self.bn_cont(x_cont) x = torch.cat(\\[x, x_cont\\], 1) x = self.layers(x) return x Breaking down the embeddings steps (this code is for illustration purposes only.) # This is our source data catz = cats [: 4 ] catz tensor([[ 4, 0, 1], [11, 0, 2], [ 7, 0, 2], [17, 1, 3]]) # This is passed in when the model is instantiated emb_szs [(24, 12), (2, 1), (7, 4)] # This is assigned inside the __init__() method selfembeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) selfembeds ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) list ( enumerate ( selfembeds )) [(0, Embedding(24, 12)), (1, Embedding(2, 1)), (2, Embedding(7, 4))] # This happens inside the forward() method embeddingz = [] for i , e in enumerate ( selfembeds ): embeddingz . append ( e ( catz [:, i ])) embeddingz [tensor([[ 0.0347, 0.3536, -1.2988, 1.6375, -0.0542, -0.2099, 0.3044, -1.2855, 0.8831, -0.7109, -0.9646, -0.1356], [-0.5039, -0.9924, 1.2296, -0.6908, 0.4641, -1.0487, 0.5577, -1.1560, 0.8318, -0.0834, 1.2123, -0.6210], [ 0.3509, 0.2216, 0.3432, 1.4547, -0.8747, 1.6727, -0.6417, -1.0160, 0.8217, -1.0531, 0.8357, -0.0637], [ 0.7978, 0.4566, 1.0926, -0.4095, -0.3366, 1.0216, 0.3601, -0.2927, 0.3536, 0.2170, -1.4778, -1.1965]], grad_fn=<EmbeddingBackward>), tensor([[-0.9676], [-0.9676], [-0.9676], [-1.0656]], grad_fn=<EmbeddingBackward>), tensor([[-2.1762, 1.0210, 1.3557, -0.1804], [-1.0131, 0.9989, -0.4746, -0.1461], [-1.0131, 0.9989, -0.4746, -0.1461], [-0.3646, -3.2237, -0.9956, 0.2598]], grad_fn=<EmbeddingBackward>)] # We concatenate the embedding sections (12,1,4) into one (17) z = torch . cat ( embeddingz , 1 ) z tensor([[ 0.0347, 0.3536, -1.2988, 1.6375, -0.0542, -0.2099, 0.3044, -1.2855, 0.8831, -0.7109, -0.9646, -0.1356, -0.9676, -2.1762, 1.0210, 1.3557, -0.1804], [-0.5039, -0.9924, 1.2296, -0.6908, 0.4641, -1.0487, 0.5577, -1.1560, 0.8318, -0.0834, 1.2123, -0.6210, -0.9676, -1.0131, 0.9989, -0.4746, -0.1461], [ 0.3509, 0.2216, 0.3432, 1.4547, -0.8747, 1.6727, -0.6417, -1.0160, 0.8217, -1.0531, 0.8357, -0.0637, -0.9676, -1.0131, 0.9989, -0.4746, -0.1461], [ 0.7978, 0.4566, 1.0926, -0.4095, -0.3366, 1.0216, 0.3601, -0.2927, 0.3536, 0.2170, -1.4778, -1.1965, -1.0656, -0.3646, -3.2237, -0.9956, 0.2598]], grad_fn=<CatBackward>) # This was assigned under the __init__() method selfembdrop = nn . Dropout ( .4 ) z = selfembdrop ( z ) z tensor([[ 0.0000, 0.0000, -2.1647, 0.0000, -0.0000, -0.3498, 0.5073, -2.1424, 0.0000, -1.1848, -1.6076, -0.2259, -1.6127, -3.6271, 0.0000, 2.2594, -0.3007], [-0.8398, -0.0000, 0.0000, -0.0000, 0.7734, -1.7478, 0.0000, -1.9267, 0.0000, -0.1390, 0.0000, -1.0350, -0.0000, -0.0000, 1.6648, -0.0000, -0.2435], [ 0.0000, 0.3693, 0.5719, 0.0000, -1.4578, 0.0000, -1.0694, -1.6933, 0.0000, -1.7552, 1.3929, -0.1062, -1.6127, -1.6886, 1.6648, -0.0000, -0.0000], [ 1.3297, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000, -0.4879, 0.0000, 0.0000, -2.4631, -1.9941, -1.7760, -0.6077, -5.3728, -1.6593, 0.4330]], grad_fn=<MulBackward0>) This is how the categorical embeddings are passed into the layers. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) x = self . layers ( x ) return x torch . manual_seed ( 33 ) model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 200 , 100 ], p = 0.4 ) # out_sz = 2 model TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=2, bias=True) ) )","title":"Define a TabularModel"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#define-loss-function-optimizer","text":"For our classification we\u2019ll replace the MSE loss function with torch.nn.CrossEntropyLoss() For the optimizer, we\u2019ll continue to use torch.optim.Adam() criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"Define loss function &amp; optimizer"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#perform-traintest-splits","text":"At this point our batch size is the entire dataset of 120,000 records. To save time we\u2019ll use the first 60,000. Recall that our tensors are already randomly shuffled. batch_size = 60000 test_size = 12000 cat_train = cats [: batch_size - test_size ] cat_test = cats [ batch_size - test_size : batch_size ] con_train = conts [: batch_size - test_size ] con_test = conts [ batch_size - test_size : batch_size ] y_train = y [: batch_size - test_size ] y_test = y [ batch_size - test_size : batch_size ] len ( cat_train ) 48000 len ( cat_test ) 12000","title":"Perform train/test splits"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#train-the-model","text":"Expect this to take 30 minutes or more! We\u2019ve added code to tell us the duration at the end. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.73441482 epoch: 26 loss: 0.45090991 epoch: 51 loss: 0.35915938 epoch: 76 loss: 0.31940848 epoch: 101 loss: 0.29913244 epoch: 126 loss: 0.28824982 epoch: 151 loss: 0.28091952 epoch: 176 loss: 0.27713534 epoch: 201 loss: 0.27236161 epoch: 226 loss: 0.27171907 epoch: 251 loss: 0.26830241 epoch: 276 loss: 0.26365638 epoch: 300 loss: 0.25949642 Duration: 709 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#plot-the-loss-function","text":"plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Cross Entropy Loss' ) plt . xlabel ( 'epoch' );","title":"Plot the loss function"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#validate-the-model","text":"# TO EVALUATE THE ENTIRE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = criterion ( y_val , y_test ) print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.25455481 Now let\u2019s look at the first 50 predicted values rows = 50 correct = 0 print ( f ' { \"MODEL OUTPUT\" : 26 } ARGMAX Y_TEST' ) for i in range ( rows ): print ( f ' { str ( y_val [ i ]) : 26 } { y_val [ i ] . argmax () : ^7 }{ y_test [ i ] : ^7 } ' ) if y_val [ i ] . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { rows } = { 100 * correct / rows : .2f } % correct' ) MODEL OUTPUT ARGMAX Y_TEST tensor([ 1.8140, -1.6443]) 0 0 tensor([-1.8268, 2.6373]) 1 0 tensor([ 1.4028, -1.9248]) 0 0 tensor([-1.9130, 1.4853]) 1 1 tensor([ 1.1757, -2.4964]) 0 0 tensor([ 2.0996, -2.2990]) 0 0 tensor([ 1.3226, -1.8349]) 0 0 tensor([-1.6211, 2.3889]) 1 1 tensor([ 2.2489, -2.4253]) 0 0 tensor([-0.4459, 1.1358]) 1 1 tensor([ 1.5145, -2.1619]) 0 0 tensor([ 0.7704, -1.9443]) 0 0 tensor([ 0.9637, -1.3796]) 0 0 tensor([-1.3527, 1.7322]) 1 1 tensor([ 1.4110, -2.4595]) 0 0 tensor([-1.4455, 2.6081]) 1 1 tensor([ 2.2798, -2.5864]) 0 1 tensor([ 1.4585, -2.7982]) 0 0 tensor([ 0.3342, -0.8995]) 0 0 tensor([ 2.0525, -1.9737]) 0 0 tensor([-1.3571, 2.1911]) 1 1 tensor([-0.4669, 0.2872]) 1 1 tensor([-2.0624, 2.2875]) 1 1 tensor([-2.1334, 2.6416]) 1 1 tensor([-3.1325, 5.1561]) 1 1 tensor([ 2.2128, -2.5172]) 0 0 tensor([ 1.0346, -1.7764]) 0 0 tensor([ 1.1221, -1.6717]) 0 0 tensor([-2.1322, 1.6714]) 1 1 tensor([ 1.5009, -1.6338]) 0 0 tensor([ 2.0387, -1.8475]) 0 0 tensor([-1.6346, 2.8899]) 1 1 tensor([-3.0129, 2.3519]) 1 1 tensor([-1.5746, 2.0000]) 1 1 tensor([ 1.3056, -2.2630]) 0 0 tensor([ 0.6631, -1.4797]) 0 0 tensor([-1.4585, 2.1836]) 1 1 tensor([ 1.0574, -1.5848]) 0 1 tensor([ 0.3376, -0.8050]) 0 1 tensor([ 1.9217, -1.9764]) 0 0 tensor([ 0.1011, -0.5529]) 0 0 tensor([ 0.6703, -0.5540]) 0 0 tensor([-0.6733, 0.8777]) 1 1 tensor([ 2.2017, -2.0445]) 0 0 tensor([-0.0442, -0.4276]) 0 0 tensor([-1.1204, 1.2558]) 1 1 tensor([-1.8170, 2.7124]) 1 1 tensor([ 1.7404, -2.0341]) 0 0 tensor([ 1.3266, -2.3039]) 0 0 tensor([-0.0671, 0.3291]) 1 0 45 out of 50 = 90.00% correct","title":"Validate the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#save-the-model","text":"Save the trained model to a file in case you want to come back later and feed new data through it. # Make sure to save the model only after the training has happened! if len ( losses ) == epochs : torch . save ( model . state_dict (), 'TaxiFareClssModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' )","title":"Save the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#loading-a-saved-model-starting-from-scratch","text":"We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. To demonstrate, restart the kernel before proceeding. import torch import torch.nn as nn import numpy as np import pandas as pd def haversine_distance ( df , lat1 , long1 , lat2 , long2 ): r = 6371 phi1 = np . radians ( df [ lat1 ]) phi2 = np . radians ( df [ lat2 ]) delta_phi = np . radians ( df [ lat2 ] - df [ lat1 ]) delta_lambda = np . radians ( df [ long2 ] - df [ long1 ]) a = np . sin ( delta_phi / 2 ) ** 2 + np . cos ( phi1 ) * np . cos ( phi2 ) * np . sin ( delta_lambda / 2 ) ** 2 c = 2 * np . arctan2 ( np . sqrt ( a ), np . sqrt ( 1 - a )) return r * c class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): super () . __init__ () self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) layerlist = [] n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) x = self . emb_drop ( x ) x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) return self . layers ( x ) Now define the model. Before we can load the saved settings, we need to instantiate our TabularModel with the parameters we used before (embedding sizes, number of continuous columns, output size, layer sizes, and dropout layer p-value). emb_szs = [( 24 , 12 ), ( 2 , 1 ), ( 7 , 4 )] model2 = TabularModel ( emb_szs , 6 , 2 , [ 200 , 100 ], p = 0.4 ) Once the model is set up, loading the saved settings is a snap. model2 . load_state_dict ( torch . load ( 'TaxiFareClssModel.pt' )); model2 . eval () # be sure to run this step! TabularModel( (embeds): ModuleList( (0): Embedding(24, 12) (1): Embedding(2, 1) (2): Embedding(7, 4) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=23, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=200, out_features=100, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.4) (8): Linear(in_features=100, out_features=2, bias=True) ) ) Next we\u2019ll define a function that takes in new parameters from the user, performs all of the preprocessing steps above, and passes the new data through our trained model. def test_data ( mdl ): # pass in the name of the new model # INPUT NEW DATA plat = float ( input ( 'What is the pickup latitude? ' )) plong = float ( input ( 'What is the pickup longitude? ' )) dlat = float ( input ( 'What is the dropoff latitude? ' )) dlong = float ( input ( 'What is the dropoff longitude? ' )) psngr = int ( input ( 'How many passengers? ' )) dt = input ( 'What is the pickup date and time? \\n Format as YYYY-MM-DD HH:MM:SS ' ) # PREPROCESS THE DATA dfx_dict = { 'pickup_latitude' : plat , 'pickup_longitude' : plong , 'dropoff_latitude' : dlat , 'dropoff_longitude' : dlong , 'passenger_count' : psngr , 'EDTdate' : dt } dfx = pd . DataFrame ( dfx_dict , index = [ 0 ]) dfx [ 'dist_km' ] = haversine_distance ( dfx , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ) dfx [ 'EDTdate' ] = pd . to_datetime ( dfx [ 'EDTdate' ]) # We can skip the .astype(category) step since our fields are small, # and encode them right away dfx [ 'Hour' ] = dfx [ 'EDTdate' ] . dt . hour dfx [ 'AMorPM' ] = np . where ( dfx [ 'Hour' ] < 12 , 0 , 1 ) dfx [ 'Weekday' ] = dfx [ 'EDTdate' ] . dt . strftime ( \" %a \" ) dfx [ 'Weekday' ] = dfx [ 'Weekday' ] . replace ([ 'Fri' , 'Mon' , 'Sat' , 'Sun' , 'Thu' , 'Tue' , 'Wed' ], [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ]) . astype ( 'int64' ) # CREATE CAT AND CONT TENSORS cat_cols = [ 'Hour' , 'AMorPM' , 'Weekday' ] cont_cols = [ 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' , 'passenger_count' , 'dist_km' ] xcats = np . stack ([ dfx [ col ] . values for col in cat_cols ], 1 ) xcats = torch . tensor ( xcats , dtype = torch . int64 ) xconts = np . stack ([ dfx [ col ] . values for col in cont_cols ], 1 ) xconts = torch . tensor ( xconts , dtype = torch . float ) # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( xcats , xconts ) . argmax () . item () print ( f ' \\n The predicted fare class is { z } ' )","title":"Loading a saved model (starting from scratch)"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/04b-Full-ANN-Code-Along-Classification/#feed-new-data-through-the-trained-model","text":"For convenience, here are the max and min values for each of the variables: Column Minimum Maximum pickup_latitude 40 41 pickup_longitude -74.5 -73.3 dropoff_latitude 40 41 dropoff_longitude -74.5 -73.3 passenger_count 1 5 EDTdate 2010-04-11 00:00:00 2010-04-24 23:59:42 Use caution! The distance between 1 degree of latitude (from 40 to 41) is 111km (69mi) and between 1 degree of longitude (from -73 to -74) is 85km (53mi). The longest cab ride in the dataset spanned a difference of only 0.243 degrees latitude and 0.284 degrees longitude. The mean difference for both latitude and longitude was about 0.02. To get a fair prediction, use values that fall close to one another. test_data ( model2 ) What is the pickup latitude? 40.5 What is the pickup longitude? -73.9 What is the dropoff latitude? 40.52 What is the dropoff longitude? -73.92 How many passengers? 2 What is the pickup date and time? Format as YYYY-MM-DD HH:MM:SS 2010-04-15 16:00:00 The predicted fare class is 1 Perfect! Where our regression predicted a fare value of \\~\\\\\\$14, our binary classification predicts a fare greater than \\$10. \\## Great job!","title":"Feed new data through the trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/","text":"Pytorch for Deep Learning BootCamp \u00b6 by Jawad Haider 05 - Neural Network Exercises \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Neural Network Exercises Census Income Dataset Perform standard imports 1. Separate continuous, categorical and label column names 2. Convert categorical columns to category dtypes Optional: Shuffle the dataset 3. Set the embedding sizes 4. Create an array of categorical values 5. Convert \u201ccats\u201d to a tensor 6. Create an array of continuous values 7. Convert \u201cconts\u201d to a tensor 8. Create a label tensor 9. Create train and test sets from cats , conts , and y Define the model class 10. Set the random seed 11. Create a TabularModel instance 12. Define the loss and optimization functions Train the model 13. Plot the Cross Entropy Loss against epochs 14. Evaluate the test set 15. Calculate the overall percent accuracy BONUS: Feed new data through the trained model Great job! Neural Network Exercises \u00b6 For these exercises we\u2019ll perform a binary classification on the Census Income dataset available from the UC Irvine Machine Learning Repository The goal is to determine if an individual earns more than $50K based on a set of continuous and categorical variables. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! Census Income Dataset \u00b6 For this exercises we\u2019re using the Census Income dataset available from the UC Irvine Machine Learning Repository . The full dataset has 48,842 entries. For this exercise we have reduced the number of records, fields and field entries, and have removed entries with null values. The file income.csv has 30,000 entries Each entry contains the following information about an individual: * age : the age of an individual as an integer from 18 to 90 (continuous) * sex : Male or Female (categorical) * education : represents the highest level of education achieved by an individual (categorical) * education_num : represents education as an integer from 3 to 16 (categorical) 3 5th-6th 8 12th 13 Bachelors 4 7th-8th 9 HS-grad 14 Masters 5 9th 10 Some-college 15 Prof-school 6 10th 11 Assoc-voc 16 Doctorate 7 11th 12 Assoc-acdm marital-status : marital status of an individual (categorical) Married Divorced Married-spouse-absent Separated Widowed Never-married workclass : a general term to represent the employment status of an individual (categorical) Local-gov Private State-gov Self-emp Federal-gov occupation : the general type of occupation of an individual (categorical) Adm-clerical Handlers-cleaners Protective-serv Craft-repair Machine-op-inspct Sales Exec-managerial Other-service Tech-support Farming-fishing Prof-specialty Transport-moving hours-per-week : the hours an individual has reported to work per week as an integer from 20 to 90 (continuous) income : whether or not an individual makes more than \\$50,000 annually (label) label : income represented as an integer (0: \\<=\\$50K, 1: >\\$50K) (optional label) Perform standard imports \u00b6 Run the cell below to load the libraries needed for this exercise and the Census Income dataset. import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle % matplotlib inline df = pd . read_csv ( '../Data/income.csv' ) print ( len ( df )) df . head () 30000 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 27 Male HS-grad 9 Never-married Private Craft-repair 40 <=50K 0 1 47 Male Masters 14 Married Local-gov Exec-managerial 50 >50K 1 2 59 Male HS-grad 9 Divorced Self-emp Prof-specialty 20 <=50K 0 3 38 Female Prof-school 15 Never-married Federal-gov Prof-specialty 57 >50K 1 4 64 Female 11th 7 Widowed Private Farming-fishing 40 <=50K 0 df [ 'label' ] . value_counts () 0 21700 1 8300 Name: label, dtype: int64 df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 30000 entries, 0 to 29999 Data columns (total 10 columns): age 30000 non-null int64 sex 30000 non-null object education 30000 non-null object education-num 30000 non-null int64 marital-status 30000 non-null object workclass 30000 non-null object occupation 30000 non-null object hours-per-week 30000 non-null int64 income 30000 non-null object label 30000 non-null int64 dtypes: int64(4), object(6) memory usage: 2.3+ MB 1. Separate continuous, categorical and label column names \u00b6 You should find that there are 5 categorical columns, 2 continuous columns and 1 label. In the case of education and education-num it doesn\u2019t matter which column you use. For the label column, be sure to use label and not income . Assign the variable names \u201ccat_cols\u201d, \u201ccont_cols\u201d and \u201cy_col\u201d to the lists of names. df . columns Index(['age', 'sex', 'education', 'education-num', 'marital-status', 'workclass', 'occupation', 'hours-per-week', 'income', 'label'], dtype='object') # CODE HERE cat_cols = [ 'sex' , 'education' , 'marital-status' , 'workclass' , 'occupation' ] cont_cols = [ 'hours-per-week' , 'education-num' ] y_col = [ 'label' ] # RUN THIS CODE TO COMPARE RESULTS: print ( f 'cat_cols has { len ( cat_cols ) } columns' ) print ( f 'cont_cols has { len ( cont_cols ) } columns' ) print ( f 'y_col has { len ( y_col ) } column' ) cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column # DON'T WRITE HERE cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column 2. Convert categorical columns to category dtypes \u00b6 # CODE HERE for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 30000 entries, 0 to 29999 Data columns (total 10 columns): age 30000 non-null int64 sex 30000 non-null category education 30000 non-null category education-num 30000 non-null int64 marital-status 30000 non-null category workclass 30000 non-null category occupation 30000 non-null category hours-per-week 30000 non-null int64 income 30000 non-null object label 30000 non-null int64 dtypes: category(5), int64(4), object(1) memory usage: 1.3+ MB # DON'T WRITE HERE Optional: Shuffle the dataset \u00b6 The income.csv dataset is already shuffled. However, if you would like to try different configurations after completing the exercises, this is where you would want to shuffle the entire set. # THIS CELL IS OPTIONAL df = shuffle ( df , random_state = 101 ) df . reset_index ( drop = True , inplace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 23 Female HS-grad 9 Never-married Private Other-service 50 <=50K 0 1 37 Female Prof-school 15 Married State-gov Prof-specialty 39 >50K 1 2 34 Male Some-college 10 Divorced Private Adm-clerical 40 <=50K 0 3 31 Male HS-grad 9 Married Private Craft-repair 40 >50K 1 4 20 Female Some-college 10 Never-married Private Sales 25 <=50K 0 3. Set the embedding sizes \u00b6 Create a variable \u201ccat_szs\u201d to hold the number of categories in each variable. Then create a variable \u201cemb_szs\u201d to hold the list of (category size, embedding size) tuples. # CODE HERE cat_szs = [ len ( df [ cat ] . cat . categories ) for cat in cat_cols ] emb_szs = [( size , min ( 50 ,( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)] # DON'T WRITE HERE [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)] 4. Create an array of categorical values \u00b6 Create a NumPy array called \u201ccats\u201d that contains a stack of each categorical column .cat.codes.values Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE cats = np . stack ([ df [ cat ] . cat . codes . values for cat in cat_cols ], axis = 1 ) # RUN THIS CODE TO COMPARE RESULTS cats [: 5 ] array([[ 0, 10, 3, 2, 6], [ 0, 12, 1, 4, 7], [ 1, 13, 0, 2, 0], [ 1, 10, 1, 2, 1], [ 0, 13, 3, 2, 9]], dtype=int8) # DON'T WRITE HERE array([[ 1, 10, 3, 2, 1], [ 1, 11, 1, 1, 2], [ 1, 10, 0, 3, 7], [ 0, 12, 3, 0, 7], [ 0, 1, 5, 2, 3]], dtype=int8) 5. Convert \u201ccats\u201d to a tensor \u00b6 Convert the \u201ccats\u201d NumPy array to a tensor of dtype int64 # CODE HERE cats = torch . tensor ( cats , dtype = torch . int64 ) cats . dtype /home/jawad/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). torch.int64 # DON'T WRITE HERE 6. Create an array of continuous values \u00b6 Create a NumPy array called \u201cconts\u201d that contains a stack of each continuous column. Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE conts = np . stack ([ df [ cont ] . values for cont in cont_cols ], axis = 1 ) # RUN THIS CODE TO COMPARE RESULTS conts [: 5 ] array([[50, 9], [39, 15], [40, 10], [40, 9], [25, 10]]) # DON'T WRITE HERE array([[27, 40], [47, 50], [59, 20], [38, 57], [64, 40]], dtype=int64) 7. Convert \u201cconts\u201d to a tensor \u00b6 Convert the \u201cconts\u201d NumPy array to a tensor of dtype float32 # CODE HERE conts = torch . tensor ( conts , dtype = torch . float ) # RUN THIS CODE TO COMPARE RESULTS conts . dtype torch.float32 # DON'T WRITE HERE torch.float32 8. Create a label tensor \u00b6 Create a tensor called \u201cy\u201d from the values in the label column. Be sure to flatten the tensor so that it can be passed into the CE Loss function. # CODE HERE y = torch . tensor ( df [ y_col ] . values ) . flatten () # DON'T WRITE HERE 9. Create train and test sets from cats , conts , and y \u00b6 We use the entire batch of 30,000 records, but a smaller batch size will save time during training. We used a test size of 5,000 records, but you can choose another fixed value or a percentage of the batch size. Make sure that your test records remain separate from your training records, without overlap. To make coding slices easier, we recommend assigning batch and test sizes to simple variables like \u201cb\u201d and \u201ct\u201d. # CODE HERE b = 10000 # suggested batch size t = 2000 # suggested test size cat_train = cats [: b - t ] cat_test = cats [ b - t : b ] cont_train = conts [: b - t ] cont_test = conts [ b - t : b ] y_train = y [: b - t ] y_test = y [ b - t : b ] # DON'T WRITE HERE Define the model class \u00b6 Run the cell below to define the TabularModel model class we\u2019ve used before. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): # Call the parent __init__ super () . __init__ () # Set up the embedding, dropout, and batch normalization layer attributes self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) # Assign a variable to hold a list of layers layerlist = [] # Assign a variable to store the number of embedding and continuous layers n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont # Iterate through the passed-in \"layers\" parameter (ie, [200,100]) to build a list of layers for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) # Convert the list of layers into an attribute self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): # Extract embedding values from the incoming categorical data embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) # Perform an initial dropout on the embeddings x = self . emb_drop ( x ) # Normalize the incoming continuous data x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) # Set up model layers x = self . layers ( x ) return x 10. Set the random seed \u00b6 To obtain results that can be recreated, set a torch manual_seed (we used 33). # CODE HERE torch . manual_seed ( 33 ) <torch._C.Generator at 0x7f2830066510> # DON'T WRITE HERE <torch._C.Generator at 0x1e5e64e5e30> 11. Create a TabularModel instance \u00b6 Create an instance called \u201cmodel\u201d with one hidden layer containing 50 neurons and a dropout layer p-value of 0.4 # CODE HERE model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 50 ], 0.4 ) # RUN THIS CODE TO COMPARE RESULTS model TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) ) # DON'T WRITE HERE TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) ) 12. Define the loss and optimization functions \u00b6 Create a loss function called \u201ccriterion\u201d using CrossEntropyLoss Create an optimization function called \u201coptimizer\u201d using Adam, with a learning rate of 0.001 # CODE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) # DON'T WRITE HERE Train the model \u00b6 Run the cell below to train the model through 300 epochs. Remember, results may vary! After completing the exercises, feel free to come back to this section and experiment with different parameters. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , cont_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.33478802 epoch: 26 loss: 0.34096697 epoch: 51 loss: 0.33127704 epoch: 76 loss: 0.33429772 epoch: 101 loss: 0.32238889 epoch: 126 loss: 0.31584346 epoch: 151 loss: 0.31843153 epoch: 176 loss: 0.31027427 epoch: 201 loss: 0.31265819 epoch: 226 loss: 0.31138414 epoch: 251 loss: 0.31025240 epoch: 276 loss: 0.30936244 epoch: 300 loss: 0.30457661 Duration: 26 seconds 13. Plot the Cross Entropy Loss against epochs \u00b6 Results may vary. The shape of the plot is what matters. # CODE HERE plt . plot ( range ( epochs ), losses ) # DON'T WRITE HERE 14. Evaluate the test set \u00b6 With torch set to no_grad , pass cat_test and con_test through the trained model. Create a validation set called \u201cy_val\u201d. Compare the output to y_test using the loss function defined above. Results may vary. # CODE HERE with torch . no_grad (): y_val = model ( cat_test , cont_test ) loss = criterion ( y_val , y_test ) # RUN THIS CODE TO COMPARE RESULTS print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.31630206 # TO EVALUATE THE TEST SET CE Loss: 0.30774996 15. Calculate the overall percent accuracy \u00b6 Using a for loop, compare the argmax values of the y_val validation set to the y_test set. # CODE HERE correct = 0 for i in range ( 2000 ): if ( y_val [ i ] . argmax () == y_test [ i ] . argmax ()): correct += 1 print ( f \" { correct } out of { 2000 } = { ( correct / 2000 ) * 100 : .2f } % correct\" ) 1394 out of 2000 = 69.70 % correct # DON'T WRITE HERE 4255 out of 5000 = 85.10% correct BONUS: Feed new data through the trained model \u00b6 See if you can write a function that allows a user to input their own values, and generates a prediction. HINT : There\u2019s no need to build a DataFrame. You can use inputs to populate column variables, convert them to embeddings with a context dictionary, and pass the embedded values directly into the tensor constructors: mar = input(\"What is the person's marital status? \") mar_d = dict(Divorced=0, Married=1, Married-spouse-absent=2, Never-married=3, Separated=4, Widowed=5) mar = mar_d[mar] cats = torch.tensor([..., ..., mar, ..., ...], dtype=torch.int64).reshape(1,-1) Make sure that names are put in alphabetical order before assigning numbers. Also, be sure to run model.eval() before passing new date through. Good luck! # WRITE YOUR CODE HERE: # RUN YOUR CODE HERE: # DON'T WRITE HERE What is the person's age? (18-90) 22 What is the person's sex? (Male/Female) male What is the person's education level? (3-16) 12 What is the person's marital status? married What is the person's workclass? private What is the person's occupation? sales How many hours/week are worked? (20-90) 40 The predicted label is 0 Great job! \u00b6","title":"05 Neural Network Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#pytorch-for-deep-learning-bootcamp","text":"by Jawad Haider","title":"Pytorch for Deep Learning BootCamp"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#05-neural-network-exercises","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Neural Network Exercises Census Income Dataset Perform standard imports 1. Separate continuous, categorical and label column names 2. Convert categorical columns to category dtypes Optional: Shuffle the dataset 3. Set the embedding sizes 4. Create an array of categorical values 5. Convert \u201ccats\u201d to a tensor 6. Create an array of continuous values 7. Convert \u201cconts\u201d to a tensor 8. Create a label tensor 9. Create train and test sets from cats , conts , and y Define the model class 10. Set the random seed 11. Create a TabularModel instance 12. Define the loss and optimization functions Train the model 13. Plot the Cross Entropy Loss against epochs 14. Evaluate the test set 15. Calculate the overall percent accuracy BONUS: Feed new data through the trained model Great job!","title":"05 - Neural Network Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#neural-network-exercises","text":"For these exercises we\u2019ll perform a binary classification on the Census Income dataset available from the UC Irvine Machine Learning Repository The goal is to determine if an individual earns more than $50K based on a set of continuous and categorical variables. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"Neural Network Exercises"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#census-income-dataset","text":"For this exercises we\u2019re using the Census Income dataset available from the UC Irvine Machine Learning Repository . The full dataset has 48,842 entries. For this exercise we have reduced the number of records, fields and field entries, and have removed entries with null values. The file income.csv has 30,000 entries Each entry contains the following information about an individual: * age : the age of an individual as an integer from 18 to 90 (continuous) * sex : Male or Female (categorical) * education : represents the highest level of education achieved by an individual (categorical) * education_num : represents education as an integer from 3 to 16 (categorical) 3 5th-6th 8 12th 13 Bachelors 4 7th-8th 9 HS-grad 14 Masters 5 9th 10 Some-college 15 Prof-school 6 10th 11 Assoc-voc 16 Doctorate 7 11th 12 Assoc-acdm marital-status : marital status of an individual (categorical) Married Divorced Married-spouse-absent Separated Widowed Never-married workclass : a general term to represent the employment status of an individual (categorical) Local-gov Private State-gov Self-emp Federal-gov occupation : the general type of occupation of an individual (categorical) Adm-clerical Handlers-cleaners Protective-serv Craft-repair Machine-op-inspct Sales Exec-managerial Other-service Tech-support Farming-fishing Prof-specialty Transport-moving hours-per-week : the hours an individual has reported to work per week as an integer from 20 to 90 (continuous) income : whether or not an individual makes more than \\$50,000 annually (label) label : income represented as an integer (0: \\<=\\$50K, 1: >\\$50K) (optional label)","title":"Census Income Dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#perform-standard-imports","text":"Run the cell below to load the libraries needed for this exercise and the Census Income dataset. import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle % matplotlib inline df = pd . read_csv ( '../Data/income.csv' ) print ( len ( df )) df . head () 30000 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 27 Male HS-grad 9 Never-married Private Craft-repair 40 <=50K 0 1 47 Male Masters 14 Married Local-gov Exec-managerial 50 >50K 1 2 59 Male HS-grad 9 Divorced Self-emp Prof-specialty 20 <=50K 0 3 38 Female Prof-school 15 Never-married Federal-gov Prof-specialty 57 >50K 1 4 64 Female 11th 7 Widowed Private Farming-fishing 40 <=50K 0 df [ 'label' ] . value_counts () 0 21700 1 8300 Name: label, dtype: int64 df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 30000 entries, 0 to 29999 Data columns (total 10 columns): age 30000 non-null int64 sex 30000 non-null object education 30000 non-null object education-num 30000 non-null int64 marital-status 30000 non-null object workclass 30000 non-null object occupation 30000 non-null object hours-per-week 30000 non-null int64 income 30000 non-null object label 30000 non-null int64 dtypes: int64(4), object(6) memory usage: 2.3+ MB","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#1-separate-continuous-categorical-and-label-column-names","text":"You should find that there are 5 categorical columns, 2 continuous columns and 1 label. In the case of education and education-num it doesn\u2019t matter which column you use. For the label column, be sure to use label and not income . Assign the variable names \u201ccat_cols\u201d, \u201ccont_cols\u201d and \u201cy_col\u201d to the lists of names. df . columns Index(['age', 'sex', 'education', 'education-num', 'marital-status', 'workclass', 'occupation', 'hours-per-week', 'income', 'label'], dtype='object') # CODE HERE cat_cols = [ 'sex' , 'education' , 'marital-status' , 'workclass' , 'occupation' ] cont_cols = [ 'hours-per-week' , 'education-num' ] y_col = [ 'label' ] # RUN THIS CODE TO COMPARE RESULTS: print ( f 'cat_cols has { len ( cat_cols ) } columns' ) print ( f 'cont_cols has { len ( cont_cols ) } columns' ) print ( f 'y_col has { len ( y_col ) } column' ) cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column # DON'T WRITE HERE cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column","title":"1. Separate continuous, categorical and label column names"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#2-convert-categorical-columns-to-category-dtypes","text":"# CODE HERE for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 30000 entries, 0 to 29999 Data columns (total 10 columns): age 30000 non-null int64 sex 30000 non-null category education 30000 non-null category education-num 30000 non-null int64 marital-status 30000 non-null category workclass 30000 non-null category occupation 30000 non-null category hours-per-week 30000 non-null int64 income 30000 non-null object label 30000 non-null int64 dtypes: category(5), int64(4), object(1) memory usage: 1.3+ MB # DON'T WRITE HERE","title":"2. Convert categorical columns to category dtypes"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#optional-shuffle-the-dataset","text":"The income.csv dataset is already shuffled. However, if you would like to try different configurations after completing the exercises, this is where you would want to shuffle the entire set. # THIS CELL IS OPTIONAL df = shuffle ( df , random_state = 101 ) df . reset_index ( drop = True , inplace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 23 Female HS-grad 9 Never-married Private Other-service 50 <=50K 0 1 37 Female Prof-school 15 Married State-gov Prof-specialty 39 >50K 1 2 34 Male Some-college 10 Divorced Private Adm-clerical 40 <=50K 0 3 31 Male HS-grad 9 Married Private Craft-repair 40 >50K 1 4 20 Female Some-college 10 Never-married Private Sales 25 <=50K 0","title":"Optional: Shuffle the dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#3-set-the-embedding-sizes","text":"Create a variable \u201ccat_szs\u201d to hold the number of categories in each variable. Then create a variable \u201cemb_szs\u201d to hold the list of (category size, embedding size) tuples. # CODE HERE cat_szs = [ len ( df [ cat ] . cat . categories ) for cat in cat_cols ] emb_szs = [( size , min ( 50 ,( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)] # DON'T WRITE HERE [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)]","title":"3. Set the embedding sizes"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#4-create-an-array-of-categorical-values","text":"Create a NumPy array called \u201ccats\u201d that contains a stack of each categorical column .cat.codes.values Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE cats = np . stack ([ df [ cat ] . cat . codes . values for cat in cat_cols ], axis = 1 ) # RUN THIS CODE TO COMPARE RESULTS cats [: 5 ] array([[ 0, 10, 3, 2, 6], [ 0, 12, 1, 4, 7], [ 1, 13, 0, 2, 0], [ 1, 10, 1, 2, 1], [ 0, 13, 3, 2, 9]], dtype=int8) # DON'T WRITE HERE array([[ 1, 10, 3, 2, 1], [ 1, 11, 1, 1, 2], [ 1, 10, 0, 3, 7], [ 0, 12, 3, 0, 7], [ 0, 1, 5, 2, 3]], dtype=int8)","title":"4. Create an array of categorical values"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#5-convert-cats-to-a-tensor","text":"Convert the \u201ccats\u201d NumPy array to a tensor of dtype int64 # CODE HERE cats = torch . tensor ( cats , dtype = torch . int64 ) cats . dtype /home/jawad/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). torch.int64 # DON'T WRITE HERE","title":"5. Convert \u201ccats\u201d to a tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#6-create-an-array-of-continuous-values","text":"Create a NumPy array called \u201cconts\u201d that contains a stack of each continuous column. Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE conts = np . stack ([ df [ cont ] . values for cont in cont_cols ], axis = 1 ) # RUN THIS CODE TO COMPARE RESULTS conts [: 5 ] array([[50, 9], [39, 15], [40, 10], [40, 9], [25, 10]]) # DON'T WRITE HERE array([[27, 40], [47, 50], [59, 20], [38, 57], [64, 40]], dtype=int64)","title":"6. Create an array of continuous values"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#7-convert-conts-to-a-tensor","text":"Convert the \u201cconts\u201d NumPy array to a tensor of dtype float32 # CODE HERE conts = torch . tensor ( conts , dtype = torch . float ) # RUN THIS CODE TO COMPARE RESULTS conts . dtype torch.float32 # DON'T WRITE HERE torch.float32","title":"7. Convert \u201cconts\u201d to a tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#8-create-a-label-tensor","text":"Create a tensor called \u201cy\u201d from the values in the label column. Be sure to flatten the tensor so that it can be passed into the CE Loss function. # CODE HERE y = torch . tensor ( df [ y_col ] . values ) . flatten () # DON'T WRITE HERE","title":"8. Create a label tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#9-create-train-and-test-sets-from-cats-conts-and-y","text":"We use the entire batch of 30,000 records, but a smaller batch size will save time during training. We used a test size of 5,000 records, but you can choose another fixed value or a percentage of the batch size. Make sure that your test records remain separate from your training records, without overlap. To make coding slices easier, we recommend assigning batch and test sizes to simple variables like \u201cb\u201d and \u201ct\u201d. # CODE HERE b = 10000 # suggested batch size t = 2000 # suggested test size cat_train = cats [: b - t ] cat_test = cats [ b - t : b ] cont_train = conts [: b - t ] cont_test = conts [ b - t : b ] y_train = y [: b - t ] y_test = y [ b - t : b ] # DON'T WRITE HERE","title":"9. Create train and test sets from cats, conts, and y"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#define-the-model-class","text":"Run the cell below to define the TabularModel model class we\u2019ve used before. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): # Call the parent __init__ super () . __init__ () # Set up the embedding, dropout, and batch normalization layer attributes self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) # Assign a variable to hold a list of layers layerlist = [] # Assign a variable to store the number of embedding and continuous layers n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont # Iterate through the passed-in \"layers\" parameter (ie, [200,100]) to build a list of layers for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) # Convert the list of layers into an attribute self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): # Extract embedding values from the incoming categorical data embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) # Perform an initial dropout on the embeddings x = self . emb_drop ( x ) # Normalize the incoming continuous data x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) # Set up model layers x = self . layers ( x ) return x","title":"Define the model class"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#10-set-the-random-seed","text":"To obtain results that can be recreated, set a torch manual_seed (we used 33). # CODE HERE torch . manual_seed ( 33 ) <torch._C.Generator at 0x7f2830066510> # DON'T WRITE HERE <torch._C.Generator at 0x1e5e64e5e30>","title":"10. Set the random seed"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#11-create-a-tabularmodel-instance","text":"Create an instance called \u201cmodel\u201d with one hidden layer containing 50 neurons and a dropout layer p-value of 0.4 # CODE HERE model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 50 ], 0.4 ) # RUN THIS CODE TO COMPARE RESULTS model TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) ) # DON'T WRITE HERE TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) )","title":"11. Create a TabularModel instance"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#12-define-the-loss-and-optimization-functions","text":"Create a loss function called \u201ccriterion\u201d using CrossEntropyLoss Create an optimization function called \u201coptimizer\u201d using Adam, with a learning rate of 0.001 # CODE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) # DON'T WRITE HERE","title":"12. Define the loss and optimization functions"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#train-the-model","text":"Run the cell below to train the model through 300 epochs. Remember, results may vary! After completing the exercises, feel free to come back to this section and experiment with different parameters. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , cont_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.33478802 epoch: 26 loss: 0.34096697 epoch: 51 loss: 0.33127704 epoch: 76 loss: 0.33429772 epoch: 101 loss: 0.32238889 epoch: 126 loss: 0.31584346 epoch: 151 loss: 0.31843153 epoch: 176 loss: 0.31027427 epoch: 201 loss: 0.31265819 epoch: 226 loss: 0.31138414 epoch: 251 loss: 0.31025240 epoch: 276 loss: 0.30936244 epoch: 300 loss: 0.30457661 Duration: 26 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#13-plot-the-cross-entropy-loss-against-epochs","text":"Results may vary. The shape of the plot is what matters. # CODE HERE plt . plot ( range ( epochs ), losses ) # DON'T WRITE HERE","title":"13. Plot the Cross Entropy Loss against epochs"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#14-evaluate-the-test-set","text":"With torch set to no_grad , pass cat_test and con_test through the trained model. Create a validation set called \u201cy_val\u201d. Compare the output to y_test using the loss function defined above. Results may vary. # CODE HERE with torch . no_grad (): y_val = model ( cat_test , cont_test ) loss = criterion ( y_val , y_test ) # RUN THIS CODE TO COMPARE RESULTS print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.31630206 # TO EVALUATE THE TEST SET CE Loss: 0.30774996","title":"14. Evaluate the test set"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#15-calculate-the-overall-percent-accuracy","text":"Using a for loop, compare the argmax values of the y_val validation set to the y_test set. # CODE HERE correct = 0 for i in range ( 2000 ): if ( y_val [ i ] . argmax () == y_test [ i ] . argmax ()): correct += 1 print ( f \" { correct } out of { 2000 } = { ( correct / 2000 ) * 100 : .2f } % correct\" ) 1394 out of 2000 = 69.70 % correct # DON'T WRITE HERE 4255 out of 5000 = 85.10% correct","title":"15. Calculate the overall percent accuracy"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#bonus-feed-new-data-through-the-trained-model","text":"See if you can write a function that allows a user to input their own values, and generates a prediction. HINT : There\u2019s no need to build a DataFrame. You can use inputs to populate column variables, convert them to embeddings with a context dictionary, and pass the embedded values directly into the tensor constructors: mar = input(\"What is the person's marital status? \") mar_d = dict(Divorced=0, Married=1, Married-spouse-absent=2, Never-married=3, Separated=4, Widowed=5) mar = mar_d[mar] cats = torch.tensor([..., ..., mar, ..., ...], dtype=torch.int64).reshape(1,-1) Make sure that names are put in alphabetical order before assigning numbers. Also, be sure to run model.eval() before passing new date through. Good luck! # WRITE YOUR CODE HERE: # RUN YOUR CODE HERE: # DON'T WRITE HERE What is the person's age? (18-90) 22 What is the person's sex? (Male/Female) male What is the person's education level? (3-16) 12 What is the person's marital status? married What is the person's workclass? private What is the person's occupation? sales How many hours/week are worked? (20-90) 40 The predicted label is 0","title":"BONUS: Feed new data through the trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/05-Neural-Network-Exercises/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/","text":"Pytorch for Deep Learning BootCamp \u00b6 by Jawad Haider 06 - Neural Network Exercises - SOLUTIONS \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Neural Network Exercises - SOLUTIONS Census Income Dataset Perform standard imports 1. Separate continuous, categorical and label column names 2. Convert categorical columns to category dtypes Optional: Shuffle the dataset 3. Set the embedding sizes 4. Create an array of categorical values 5. Convert \u201ccats\u201d to a tensor 6. Create an array of continuous values 7. Convert \u201cconts\u201d to a tensor 8. Create a label tensor 9. Create train and test sets from cats , conts , and y Define the model class 10. Set the random seed 11. Create a TabularModel instance 12. Define the loss and optimization functions Train the model 13. Plot the Cross Entropy Loss against epochs 14. Evaluate the test set 15. Calculate the overall percent accuracy BONUS: Feed new data through the trained model Great job! Neural Network Exercises - SOLUTIONS \u00b6 For these exercises we\u2019ll perform a binary classification on the Census Income dataset available from the UC Irvine Machine Learning Repository The goal is to determine if an individual earns more than $50K based on a set of continuous and categorical variables. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output! Census Income Dataset \u00b6 For this exercises we\u2019re using the Census Income dataset available from the UC Irvine Machine Learning Repository . The full dataset has 48,842 entries. For this exercise we have reduced the number of records, fields and field entries, and have removed entries with null values. The file income.csv has 30,000 entries Each entry contains the following information about an individual: * age : the age of an individual as an integer from 18 to 90 (continuous) * sex : Male or Female (categorical) * education : represents the highest level of education achieved by an individual (categorical) * education_num : represents education as an integer from 3 to 16 (categorical) 3 5th-6th 8 12th 13 Bachelors 4 7th-8th 9 HS-grad 14 Masters 5 9th 10 Some-college 15 Prof-school 6 10th 11 Assoc-voc 16 Doctorate 7 11th 12 Assoc-acdm marital-status : marital status of an individual (categorical) Married Divorced Married-spouse-absent Separated Widowed Never-married workclass : a general term to represent the employment status of an individual (categorical) Local-gov Private State-gov Self-emp Federal-gov occupation : the general type of occupation of an individual (categorical) Adm-clerical Handlers-cleaners Protective-serv Craft-repair Machine-op-inspct Sales Exec-managerial Other-service Tech-support Farming-fishing Prof-specialty Transport-moving hours-per-week : the hours an individual has reported to work per week as an integer from 20 to 90 (continuous) income : whether or not an individual makes more than \\$50,000 annually (label) label : income represented as an integer (0: \\<=\\$50K, 1: >\\$50K) (optional label) Perform standard imports \u00b6 Run the cell below to load the libraries needed for this exercise and the Census Income dataset. import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle % matplotlib inline df = pd . read_csv ( '../Data/income.csv' ) print ( len ( df )) df . head () 30000 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 27 Male HS-grad 9 Never-married Private Craft-repair 40 <=50K 0 1 47 Male Masters 14 Married Local-gov Exec-managerial 50 >50K 1 2 59 Male HS-grad 9 Divorced Self-emp Prof-specialty 20 <=50K 0 3 38 Female Prof-school 15 Never-married Federal-gov Prof-specialty 57 >50K 1 4 64 Female 11th 7 Widowed Private Farming-fishing 40 <=50K 0 df [ 'label' ] . value_counts () 0 21700 1 8300 Name: label, dtype: int64 1. Separate continuous, categorical and label column names \u00b6 You should find that there are 5 categorical columns, 2 continuous columns and 1 label. In the case of education and education-num it doesn\u2019t matter which column you use. For the label column, be sure to use label and not income . Assign the variable names \u201ccat_cols\u201d, \u201ccont_cols\u201d and \u201cy_col\u201d to the lists of names. df . columns # CODE HERE # RUN THIS CODE TO COMPARE RESULTS: print ( f 'cat_cols has { len ( cat_cols ) } columns' ) print ( f 'cont_cols has { len ( cont_cols ) } columns' ) print ( f 'y_col has { len ( y_col ) } column' ) # DON'T WRITE HERE cat_cols = [ 'sex' , 'education' , 'marital-status' , 'workclass' , 'occupation' ] cont_cols = [ 'age' , 'hours-per-week' ] y_col = [ 'label' ] print ( f 'cat_cols has { len ( cat_cols ) } columns' ) # 5 print ( f 'cont_cols has { len ( cont_cols ) } columns' ) # 2 print ( f 'y_col has { len ( y_col ) } column' ) # 1 cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column 2. Convert categorical columns to category dtypes \u00b6 # CODE HERE # DON'T WRITE HERE for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' ) Optional: Shuffle the dataset \u00b6 The income.csv dataset is already shuffled. However, if you would like to try different configurations after completing the exercises, this is where you would want to shuffle the entire set. # THIS CELL IS OPTIONAL df = shuffle ( df , random_state = 101 ) df . reset_index ( drop = True , inplace = True ) df . head () 3. Set the embedding sizes \u00b6 Create a variable \u201ccat_szs\u201d to hold the number of categories in each variable. Then create a variable \u201cemb_szs\u201d to hold the list of (category size, embedding size) tuples. # CODE HERE # DON'T WRITE HERE cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)] 4. Create an array of categorical values \u00b6 Create a NumPy array called \u201ccats\u201d that contains a stack of each categorical column .cat.codes.values Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS cats [: 5 ] # DON'T WRITE HERE sx = df [ 'sex' ] . cat . codes . values ed = df [ 'education' ] . cat . codes . values ms = df [ 'marital-status' ] . cat . codes . values wc = df [ 'workclass' ] . cat . codes . values oc = df [ 'occupation' ] . cat . codes . values cats = np . stack ([ sx , ed , ms , wc , oc ], 1 ) cats [: 5 ] array([[ 1, 10, 3, 2, 1], [ 1, 11, 1, 1, 2], [ 1, 10, 0, 3, 7], [ 0, 12, 3, 0, 7], [ 0, 1, 5, 2, 3]], dtype=int8) 5. Convert \u201ccats\u201d to a tensor \u00b6 Convert the \u201ccats\u201d NumPy array to a tensor of dtype int64 # CODE HERE # DON'T WRITE HERE cats = torch . tensor ( cats , dtype = torch . int64 ) 6. Create an array of continuous values \u00b6 Create a NumPy array called \u201cconts\u201d that contains a stack of each continuous column. Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS conts [: 5 ] # DON'T WRITE HERE conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts [: 5 ] array([[27, 40], [47, 50], [59, 20], [38, 57], [64, 40]], dtype=int64) 7. Convert \u201cconts\u201d to a tensor \u00b6 Convert the \u201cconts\u201d NumPy array to a tensor of dtype float32 # CODE HERE # RUN THIS CODE TO COMPARE RESULTS conts . dtype # DON'T WRITE HERE conts = torch . tensor ( conts , dtype = torch . float ) conts . dtype torch.float32 8. Create a label tensor \u00b6 Create a tensor called \u201cy\u201d from the values in the label column. Be sure to flatten the tensor so that it can be passed into the CE Loss function. # CODE HERE # DON'T WRITE HERE y = torch . tensor ( df [ y_col ] . values ) . flatten () 9. Create train and test sets from cats , conts , and y \u00b6 We use the entire batch of 30,000 records, but a smaller batch size will save time during training. We used a test size of 5,000 records, but you can choose another fixed value or a percentage of the batch size. Make sure that your test records remain separate from your training records, without overlap. To make coding slices easier, we recommend assigning batch and test sizes to simple variables like \u201cb\u201d and \u201ct\u201d. # CODE HERE b = 30000 # suggested batch size t = 5000 # suggested test size # DON'T WRITE HERE b = 30000 # suggested batch size t = 5000 # suggested test size cat_train = cats [: b - t ] cat_test = cats [ b - t : b ] con_train = conts [: b - t ] con_test = conts [ b - t : b ] y_train = y [: b - t ] y_test = y [ b - t : b ] Define the model class \u00b6 Run the cell below to define the TabularModel model class we\u2019ve used before. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): # Call the parent __init__ super () . __init__ () # Set up the embedding, dropout, and batch normalization layer attributes self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) # Assign a variable to hold a list of layers layerlist = [] # Assign a variable to store the number of embedding and continuous layers n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont # Iterate through the passed-in \"layers\" parameter (ie, [200,100]) to build a list of layers for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) # Convert the list of layers into an attribute self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): # Extract embedding values from the incoming categorical data embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) # Perform an initial dropout on the embeddings x = self . emb_drop ( x ) # Normalize the incoming continuous data x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) # Set up model layers x = self . layers ( x ) return x 10. Set the random seed \u00b6 To obtain results that can be recreated, set a torch manual_seed (we used 33). # CODE HERE # DON'T WRITE HERE torch . manual_seed ( 33 ) <torch._C.Generator at 0x1e5e64e5e30> 11. Create a TabularModel instance \u00b6 Create an instance called \u201cmodel\u201d with one hidden layer containing 50 neurons and a dropout layer p-value of 0.4 # CODE HERE # RUN THIS CODE TO COMPARE RESULTS model # DON'T WRITE HERE model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 50 ], p = 0.4 ) model TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) ) 12. Define the loss and optimization functions \u00b6 Create a loss function called \u201ccriterion\u201d using CrossEntropyLoss Create an optimization function called \u201coptimizer\u201d using Adam, with a learning rate of 0.001 # CODE HERE # DON'T WRITE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 ) Train the model \u00b6 Run the cell below to train the model through 300 epochs. Remember, results may vary! After completing the exercises, feel free to come back to this section and experiment with different parameters. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.65308946 epoch: 26 loss: 0.54059124 epoch: 51 loss: 0.46917316 epoch: 76 loss: 0.41288978 epoch: 101 loss: 0.37744597 epoch: 126 loss: 0.35649022 epoch: 151 loss: 0.34338138 epoch: 176 loss: 0.33378774 epoch: 201 loss: 0.32601979 epoch: 226 loss: 0.32018784 epoch: 251 loss: 0.31548899 epoch: 276 loss: 0.30901730 epoch: 300 loss: 0.30690485 Duration: 170 seconds 13. Plot the Cross Entropy Loss against epochs \u00b6 Results may vary. The shape of the plot is what matters. # CODE HERE # DON'T WRITE HERE plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Cross Entropy Loss' ) plt . xlabel ( 'epoch' ); 14. Evaluate the test set \u00b6 With torch set to no_grad , pass cat_test and con_test through the trained model. Create a validation set called \u201cy_val\u201d. Compare the output to y_test using the loss function defined above. Results may vary. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS print ( f 'CE Loss: { loss : .8f } ' ) # TO EVALUATE THE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = criterion ( y_val , y_test ) print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.30774996 15. Calculate the overall percent accuracy \u00b6 Using a for loop, compare the argmax values of the y_val validation set to the y_test set. # CODE HERE # DON'T WRITE HERE rows = len ( y_test ) correct = 0 # print(f'{\"MODEL OUTPUT\":26} ARGMAX Y_TEST') for i in range ( rows ): # print(f'{str(y_val[i]):26} {y_val[i].argmax().item():^7}{y_test[i]:^7}') if y_val [ i ] . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { rows } = { 100 * correct / rows : .2f } % correct' ) 4255 out of 5000 = 85.10% correct BONUS: Feed new data through the trained model \u00b6 See if you can write a function that allows a user to input their own values, and generates a prediction. HINT : There\u2019s no need to build a DataFrame. You can use inputs to populate column variables, convert them to embeddings with a context dictionary, and pass the embedded values directly into the tensor constructors: mar = input(\"What is the person's marital status? \") mar_d = dict(Divorced=0, Married=1, Married-spouse-absent=2, Never-married=3, Separated=4, Widowed=5) mar = mar_d[mar] cats = torch.tensor([..., ..., mar, ..., ...], dtype=torch.int64).reshape(1,-1) Make sure that names are put in alphabetical order before assigning numbers. Also, be sure to run model.eval() before passing new date through. Good luck! # WRITE YOUR CODE HERE: # RUN YOUR CODE HERE: # DON'T WRITE HERE def test_data ( mdl ): # pass in the name of the model # INPUT NEW DATA age = float ( input ( \"What is the person's age? (18-90) \" )) sex = input ( \"What is the person's sex? (Male/Female) \" ) . capitalize () edn = int ( input ( \"What is the person's education level? (3-16) \" )) mar = input ( \"What is the person's marital status? \" ) . capitalize () wrk = input ( \"What is the person's workclass? \" ) . capitalize () occ = input ( \"What is the person's occupation? \" ) . capitalize () hrs = float ( input ( \"How many hours/week are worked? (20-90) \" )) # PREPROCESS THE DATA sex_d = { 'Female' : 0 , 'Male' : 1 } mar_d = { 'Divorced' : 0 , 'Married' : 1 , 'Married-spouse-absent' : 2 , 'Never-married' : 3 , 'Separated' : 4 , 'Widowed' : 5 } wrk_d = { 'Federal-gov' : 0 , 'Local-gov' : 1 , 'Private' : 2 , 'Self-emp' : 3 , 'State-gov' : 4 } occ_d = { 'Adm-clerical' : 0 , 'Craft-repair' : 1 , 'Exec-managerial' : 2 , 'Farming-fishing' : 3 , 'Handlers-cleaners' : 4 , 'Machine-op-inspct' : 5 , 'Other-service' : 6 , 'Prof-specialty' : 7 , 'Protective-serv' : 8 , 'Sales' : 9 , 'Tech-support' : 10 , 'Transport-moving' : 11 } sex = sex_d [ sex ] mar = mar_d [ mar ] wrk = wrk_d [ wrk ] occ = occ_d [ occ ] # CREATE CAT AND CONT TENSORS cats = torch . tensor ([ sex , edn , mar , wrk , occ ], dtype = torch . int64 ) . reshape ( 1 , - 1 ) conts = torch . tensor ([ age , hrs ], dtype = torch . float ) . reshape ( 1 , - 1 ) # SET MODEL TO EVAL (in case this hasn't been done) mdl . eval () # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( cats , conts ) . argmax () . item () print ( f ' \\n The predicted label is { z } ' ) test_data ( model ) What is the person's age? (18-90) 22 What is the person's sex? (Male/Female) male What is the person's education level? (3-16) 12 What is the person's marital status? married What is the person's workclass? private What is the person's occupation? sales How many hours/week are worked? (20-90) 40 The predicted label is 0 Great job! \u00b6","title":"06 Neural Network Exercises Solutions"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#pytorch-for-deep-learning-bootcamp","text":"by Jawad Haider","title":"Pytorch for Deep Learning BootCamp"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#06-neural-network-exercises-solutions","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Neural Network Exercises - SOLUTIONS Census Income Dataset Perform standard imports 1. Separate continuous, categorical and label column names 2. Convert categorical columns to category dtypes Optional: Shuffle the dataset 3. Set the embedding sizes 4. Create an array of categorical values 5. Convert \u201ccats\u201d to a tensor 6. Create an array of continuous values 7. Convert \u201cconts\u201d to a tensor 8. Create a label tensor 9. Create train and test sets from cats , conts , and y Define the model class 10. Set the random seed 11. Create a TabularModel instance 12. Define the loss and optimization functions Train the model 13. Plot the Cross Entropy Loss against epochs 14. Evaluate the test set 15. Calculate the overall percent accuracy BONUS: Feed new data through the trained model Great job!","title":"06 - Neural Network Exercises - SOLUTIONS"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#neural-network-exercises-solutions","text":"For these exercises we\u2019ll perform a binary classification on the Census Income dataset available from the UC Irvine Machine Learning Repository The goal is to determine if an individual earns more than $50K based on a set of continuous and categorical variables. IMPORTANT NOTE! Make sure you don\u2019t run the cells directly above the example output shown, otherwise you will end up writing over the example output!","title":"Neural Network Exercises - SOLUTIONS"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#census-income-dataset","text":"For this exercises we\u2019re using the Census Income dataset available from the UC Irvine Machine Learning Repository . The full dataset has 48,842 entries. For this exercise we have reduced the number of records, fields and field entries, and have removed entries with null values. The file income.csv has 30,000 entries Each entry contains the following information about an individual: * age : the age of an individual as an integer from 18 to 90 (continuous) * sex : Male or Female (categorical) * education : represents the highest level of education achieved by an individual (categorical) * education_num : represents education as an integer from 3 to 16 (categorical) 3 5th-6th 8 12th 13 Bachelors 4 7th-8th 9 HS-grad 14 Masters 5 9th 10 Some-college 15 Prof-school 6 10th 11 Assoc-voc 16 Doctorate 7 11th 12 Assoc-acdm marital-status : marital status of an individual (categorical) Married Divorced Married-spouse-absent Separated Widowed Never-married workclass : a general term to represent the employment status of an individual (categorical) Local-gov Private State-gov Self-emp Federal-gov occupation : the general type of occupation of an individual (categorical) Adm-clerical Handlers-cleaners Protective-serv Craft-repair Machine-op-inspct Sales Exec-managerial Other-service Tech-support Farming-fishing Prof-specialty Transport-moving hours-per-week : the hours an individual has reported to work per week as an integer from 20 to 90 (continuous) income : whether or not an individual makes more than \\$50,000 annually (label) label : income represented as an integer (0: \\<=\\$50K, 1: >\\$50K) (optional label)","title":"Census Income Dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#perform-standard-imports","text":"Run the cell below to load the libraries needed for this exercise and the Census Income dataset. import torch import torch.nn as nn import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle % matplotlib inline df = pd . read_csv ( '../Data/income.csv' ) print ( len ( df )) df . head () 30000 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age sex education education-num marital-status workclass occupation hours-per-week income label 0 27 Male HS-grad 9 Never-married Private Craft-repair 40 <=50K 0 1 47 Male Masters 14 Married Local-gov Exec-managerial 50 >50K 1 2 59 Male HS-grad 9 Divorced Self-emp Prof-specialty 20 <=50K 0 3 38 Female Prof-school 15 Never-married Federal-gov Prof-specialty 57 >50K 1 4 64 Female 11th 7 Widowed Private Farming-fishing 40 <=50K 0 df [ 'label' ] . value_counts () 0 21700 1 8300 Name: label, dtype: int64","title":"Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#1-separate-continuous-categorical-and-label-column-names","text":"You should find that there are 5 categorical columns, 2 continuous columns and 1 label. In the case of education and education-num it doesn\u2019t matter which column you use. For the label column, be sure to use label and not income . Assign the variable names \u201ccat_cols\u201d, \u201ccont_cols\u201d and \u201cy_col\u201d to the lists of names. df . columns # CODE HERE # RUN THIS CODE TO COMPARE RESULTS: print ( f 'cat_cols has { len ( cat_cols ) } columns' ) print ( f 'cont_cols has { len ( cont_cols ) } columns' ) print ( f 'y_col has { len ( y_col ) } column' ) # DON'T WRITE HERE cat_cols = [ 'sex' , 'education' , 'marital-status' , 'workclass' , 'occupation' ] cont_cols = [ 'age' , 'hours-per-week' ] y_col = [ 'label' ] print ( f 'cat_cols has { len ( cat_cols ) } columns' ) # 5 print ( f 'cont_cols has { len ( cont_cols ) } columns' ) # 2 print ( f 'y_col has { len ( y_col ) } column' ) # 1 cat_cols has 5 columns cont_cols has 2 columns y_col has 1 column","title":"1. Separate continuous, categorical and label column names"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#2-convert-categorical-columns-to-category-dtypes","text":"# CODE HERE # DON'T WRITE HERE for cat in cat_cols : df [ cat ] = df [ cat ] . astype ( 'category' )","title":"2. Convert categorical columns to category dtypes"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#optional-shuffle-the-dataset","text":"The income.csv dataset is already shuffled. However, if you would like to try different configurations after completing the exercises, this is where you would want to shuffle the entire set. # THIS CELL IS OPTIONAL df = shuffle ( df , random_state = 101 ) df . reset_index ( drop = True , inplace = True ) df . head ()","title":"Optional: Shuffle the dataset"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#3-set-the-embedding-sizes","text":"Create a variable \u201ccat_szs\u201d to hold the number of categories in each variable. Then create a variable \u201cemb_szs\u201d to hold the list of (category size, embedding size) tuples. # CODE HERE # DON'T WRITE HERE cat_szs = [ len ( df [ col ] . cat . categories ) for col in cat_cols ] emb_szs = [( size , min ( 50 , ( size + 1 ) // 2 )) for size in cat_szs ] emb_szs [(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)]","title":"3. Set the embedding sizes"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#4-create-an-array-of-categorical-values","text":"Create a NumPy array called \u201ccats\u201d that contains a stack of each categorical column .cat.codes.values Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS cats [: 5 ] # DON'T WRITE HERE sx = df [ 'sex' ] . cat . codes . values ed = df [ 'education' ] . cat . codes . values ms = df [ 'marital-status' ] . cat . codes . values wc = df [ 'workclass' ] . cat . codes . values oc = df [ 'occupation' ] . cat . codes . values cats = np . stack ([ sx , ed , ms , wc , oc ], 1 ) cats [: 5 ] array([[ 1, 10, 3, 2, 1], [ 1, 11, 1, 1, 2], [ 1, 10, 0, 3, 7], [ 0, 12, 3, 0, 7], [ 0, 1, 5, 2, 3]], dtype=int8)","title":"4. Create an array of categorical values"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#5-convert-cats-to-a-tensor","text":"Convert the \u201ccats\u201d NumPy array to a tensor of dtype int64 # CODE HERE # DON'T WRITE HERE cats = torch . tensor ( cats , dtype = torch . int64 )","title":"5. Convert \u201ccats\u201d to a tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#6-create-an-array-of-continuous-values","text":"Create a NumPy array called \u201cconts\u201d that contains a stack of each continuous column. Note: your output may contain different values. Ours came after performing the shuffle step shown above. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS conts [: 5 ] # DON'T WRITE HERE conts = np . stack ([ df [ col ] . values for col in cont_cols ], 1 ) conts [: 5 ] array([[27, 40], [47, 50], [59, 20], [38, 57], [64, 40]], dtype=int64)","title":"6. Create an array of continuous values"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#7-convert-conts-to-a-tensor","text":"Convert the \u201cconts\u201d NumPy array to a tensor of dtype float32 # CODE HERE # RUN THIS CODE TO COMPARE RESULTS conts . dtype # DON'T WRITE HERE conts = torch . tensor ( conts , dtype = torch . float ) conts . dtype torch.float32","title":"7. Convert \u201cconts\u201d to a tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#8-create-a-label-tensor","text":"Create a tensor called \u201cy\u201d from the values in the label column. Be sure to flatten the tensor so that it can be passed into the CE Loss function. # CODE HERE # DON'T WRITE HERE y = torch . tensor ( df [ y_col ] . values ) . flatten ()","title":"8. Create a label tensor"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#9-create-train-and-test-sets-from-cats-conts-and-y","text":"We use the entire batch of 30,000 records, but a smaller batch size will save time during training. We used a test size of 5,000 records, but you can choose another fixed value or a percentage of the batch size. Make sure that your test records remain separate from your training records, without overlap. To make coding slices easier, we recommend assigning batch and test sizes to simple variables like \u201cb\u201d and \u201ct\u201d. # CODE HERE b = 30000 # suggested batch size t = 5000 # suggested test size # DON'T WRITE HERE b = 30000 # suggested batch size t = 5000 # suggested test size cat_train = cats [: b - t ] cat_test = cats [ b - t : b ] con_train = conts [: b - t ] con_test = conts [ b - t : b ] y_train = y [: b - t ] y_test = y [ b - t : b ]","title":"9. Create train and test sets from cats, conts, and y"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#define-the-model-class","text":"Run the cell below to define the TabularModel model class we\u2019ve used before. class TabularModel ( nn . Module ): def __init__ ( self , emb_szs , n_cont , out_sz , layers , p = 0.5 ): # Call the parent __init__ super () . __init__ () # Set up the embedding, dropout, and batch normalization layer attributes self . embeds = nn . ModuleList ([ nn . Embedding ( ni , nf ) for ni , nf in emb_szs ]) self . emb_drop = nn . Dropout ( p ) self . bn_cont = nn . BatchNorm1d ( n_cont ) # Assign a variable to hold a list of layers layerlist = [] # Assign a variable to store the number of embedding and continuous layers n_emb = sum (( nf for ni , nf in emb_szs )) n_in = n_emb + n_cont # Iterate through the passed-in \"layers\" parameter (ie, [200,100]) to build a list of layers for i in layers : layerlist . append ( nn . Linear ( n_in , i )) layerlist . append ( nn . ReLU ( inplace = True )) layerlist . append ( nn . BatchNorm1d ( i )) layerlist . append ( nn . Dropout ( p )) n_in = i layerlist . append ( nn . Linear ( layers [ - 1 ], out_sz )) # Convert the list of layers into an attribute self . layers = nn . Sequential ( * layerlist ) def forward ( self , x_cat , x_cont ): # Extract embedding values from the incoming categorical data embeddings = [] for i , e in enumerate ( self . embeds ): embeddings . append ( e ( x_cat [:, i ])) x = torch . cat ( embeddings , 1 ) # Perform an initial dropout on the embeddings x = self . emb_drop ( x ) # Normalize the incoming continuous data x_cont = self . bn_cont ( x_cont ) x = torch . cat ([ x , x_cont ], 1 ) # Set up model layers x = self . layers ( x ) return x","title":"Define the model class"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#10-set-the-random-seed","text":"To obtain results that can be recreated, set a torch manual_seed (we used 33). # CODE HERE # DON'T WRITE HERE torch . manual_seed ( 33 ) <torch._C.Generator at 0x1e5e64e5e30>","title":"10. Set the random seed"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#11-create-a-tabularmodel-instance","text":"Create an instance called \u201cmodel\u201d with one hidden layer containing 50 neurons and a dropout layer p-value of 0.4 # CODE HERE # RUN THIS CODE TO COMPARE RESULTS model # DON'T WRITE HERE model = TabularModel ( emb_szs , conts . shape [ 1 ], 2 , [ 50 ], p = 0.4 ) model TabularModel( (embeds): ModuleList( (0): Embedding(2, 1) (1): Embedding(14, 7) (2): Embedding(6, 3) (3): Embedding(5, 3) (4): Embedding(12, 6) ) (emb_drop): Dropout(p=0.4) (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=22, out_features=50, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.4) (4): Linear(in_features=50, out_features=2, bias=True) ) )","title":"11. Create a TabularModel instance"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#12-define-the-loss-and-optimization-functions","text":"Create a loss function called \u201ccriterion\u201d using CrossEntropyLoss Create an optimization function called \u201coptimizer\u201d using Adam, with a learning rate of 0.001 # CODE HERE # DON'T WRITE HERE criterion = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 )","title":"12. Define the loss and optimization functions"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#train-the-model","text":"Run the cell below to train the model through 300 epochs. Remember, results may vary! After completing the exercises, feel free to come back to this section and experiment with different parameters. import time start_time = time . time () epochs = 300 losses = [] for i in range ( epochs ): i += 1 y_pred = model ( cat_train , con_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) # a neat trick to save screen space: if i % 25 == 1 : print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) optimizer . zero_grad () loss . backward () optimizer . step () print ( f 'epoch: { i : 3 } loss: { loss . item () : 10.8f } ' ) # print the last line print ( f ' \\n Duration: { time . time () - start_time : .0f } seconds' ) # print the time elapsed epoch: 1 loss: 0.65308946 epoch: 26 loss: 0.54059124 epoch: 51 loss: 0.46917316 epoch: 76 loss: 0.41288978 epoch: 101 loss: 0.37744597 epoch: 126 loss: 0.35649022 epoch: 151 loss: 0.34338138 epoch: 176 loss: 0.33378774 epoch: 201 loss: 0.32601979 epoch: 226 loss: 0.32018784 epoch: 251 loss: 0.31548899 epoch: 276 loss: 0.30901730 epoch: 300 loss: 0.30690485 Duration: 170 seconds","title":"Train the model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#13-plot-the-cross-entropy-loss-against-epochs","text":"Results may vary. The shape of the plot is what matters. # CODE HERE # DON'T WRITE HERE plt . plot ( range ( epochs ), losses ) plt . ylabel ( 'Cross Entropy Loss' ) plt . xlabel ( 'epoch' );","title":"13. Plot the Cross Entropy Loss against epochs"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#14-evaluate-the-test-set","text":"With torch set to no_grad , pass cat_test and con_test through the trained model. Create a validation set called \u201cy_val\u201d. Compare the output to y_test using the loss function defined above. Results may vary. # CODE HERE # RUN THIS CODE TO COMPARE RESULTS print ( f 'CE Loss: { loss : .8f } ' ) # TO EVALUATE THE TEST SET with torch . no_grad (): y_val = model ( cat_test , con_test ) loss = criterion ( y_val , y_test ) print ( f 'CE Loss: { loss : .8f } ' ) CE Loss: 0.30774996","title":"14. Evaluate the test set"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#15-calculate-the-overall-percent-accuracy","text":"Using a for loop, compare the argmax values of the y_val validation set to the y_test set. # CODE HERE # DON'T WRITE HERE rows = len ( y_test ) correct = 0 # print(f'{\"MODEL OUTPUT\":26} ARGMAX Y_TEST') for i in range ( rows ): # print(f'{str(y_val[i]):26} {y_val[i].argmax().item():^7}{y_test[i]:^7}') if y_val [ i ] . argmax () . item () == y_test [ i ]: correct += 1 print ( f ' \\n { correct } out of { rows } = { 100 * correct / rows : .2f } % correct' ) 4255 out of 5000 = 85.10% correct","title":"15. Calculate the overall percent accuracy"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#bonus-feed-new-data-through-the-trained-model","text":"See if you can write a function that allows a user to input their own values, and generates a prediction. HINT : There\u2019s no need to build a DataFrame. You can use inputs to populate column variables, convert them to embeddings with a context dictionary, and pass the embedded values directly into the tensor constructors: mar = input(\"What is the person's marital status? \") mar_d = dict(Divorced=0, Married=1, Married-spouse-absent=2, Never-married=3, Separated=4, Widowed=5) mar = mar_d[mar] cats = torch.tensor([..., ..., mar, ..., ...], dtype=torch.int64).reshape(1,-1) Make sure that names are put in alphabetical order before assigning numbers. Also, be sure to run model.eval() before passing new date through. Good luck! # WRITE YOUR CODE HERE: # RUN YOUR CODE HERE: # DON'T WRITE HERE def test_data ( mdl ): # pass in the name of the model # INPUT NEW DATA age = float ( input ( \"What is the person's age? (18-90) \" )) sex = input ( \"What is the person's sex? (Male/Female) \" ) . capitalize () edn = int ( input ( \"What is the person's education level? (3-16) \" )) mar = input ( \"What is the person's marital status? \" ) . capitalize () wrk = input ( \"What is the person's workclass? \" ) . capitalize () occ = input ( \"What is the person's occupation? \" ) . capitalize () hrs = float ( input ( \"How many hours/week are worked? (20-90) \" )) # PREPROCESS THE DATA sex_d = { 'Female' : 0 , 'Male' : 1 } mar_d = { 'Divorced' : 0 , 'Married' : 1 , 'Married-spouse-absent' : 2 , 'Never-married' : 3 , 'Separated' : 4 , 'Widowed' : 5 } wrk_d = { 'Federal-gov' : 0 , 'Local-gov' : 1 , 'Private' : 2 , 'Self-emp' : 3 , 'State-gov' : 4 } occ_d = { 'Adm-clerical' : 0 , 'Craft-repair' : 1 , 'Exec-managerial' : 2 , 'Farming-fishing' : 3 , 'Handlers-cleaners' : 4 , 'Machine-op-inspct' : 5 , 'Other-service' : 6 , 'Prof-specialty' : 7 , 'Protective-serv' : 8 , 'Sales' : 9 , 'Tech-support' : 10 , 'Transport-moving' : 11 } sex = sex_d [ sex ] mar = mar_d [ mar ] wrk = wrk_d [ wrk ] occ = occ_d [ occ ] # CREATE CAT AND CONT TENSORS cats = torch . tensor ([ sex , edn , mar , wrk , occ ], dtype = torch . int64 ) . reshape ( 1 , - 1 ) conts = torch . tensor ([ age , hrs ], dtype = torch . float ) . reshape ( 1 , - 1 ) # SET MODEL TO EVAL (in case this hasn't been done) mdl . eval () # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP with torch . no_grad (): z = mdl ( cats , conts ) . argmax () . item () print ( f ' \\n The predicted label is { z } ' ) test_data ( model ) What is the person's age? (18-90) 22 What is the person's sex? (Male/Female) male What is the person's education level? (3-16) 12 What is the person's marital status? married What is the person's workclass? private What is the person's occupation? sales How many hours/week are worked? (20-90) 40 The predicted label is 0","title":"BONUS: Feed new data through the trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/06-Neural-Network-Exercises-Solutions/#great-job","text":"","title":"Great job!"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/","text":"Pytorch for Deep Learning BootCamp \u00b6 by Jawad Haider 07 - Saving and Loading Trained Models \u00b6 Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Saving and Loading Trained Models Saving a trained model Loading a saved model (starting from scratch) 1. Perform standard imports 2. Run the model definition 3. Instantiate the model, load parameters That\u2019s it! Saving and Loading Trained Models \u00b6 Refer back to this notebook as a refresher on saving and loading models. Saving a trained model \u00b6 Save a trained model to a file in case you want to come back later and feed new data through it. To save a trained model called \u201cmodel\u201d to a file called \u201cMyModel.pt\u201d: torch . save ( model . state_dict (), 'MyModel.pt' ) To ensure the model has been trained before saving (assumes the variables \u201closses\u201d and \u201cepochs\u201d have been defined): if len ( losses ) == epochs : torch . save ( model . state_dict (), 'MyModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' ) Loading a saved model (starting from scratch) \u00b6 We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions. 1. Perform standard imports \u00b6 These will depend on the scope of the model, chosen displays, metrics, etc. # Perform standard imports import torch import torch.nn as nn import numpy as np import pandas as pd 2. Run the model definition \u00b6 We\u2019ll introduce the model shown below in the next section. class MultilayerPerceptron ( nn . Module ): def __init__ ( self , in_sz = 784 , out_sz = 10 , layers = [ 120 , 84 ]): super () . __init__ () self . fc1 = nn . Linear ( in_sz , layers [ 0 ]) self . fc2 = nn . Linear ( layers [ 0 ], layers [ 1 ]) self . fc3 = nn . Linear ( layers [ 1 ], out_sz ) def forward ( self , X ): X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 ) 3. Instantiate the model, load parameters \u00b6 First we instantiate the model, then we load the pre-trained weights & biases, and finally we set the model to \u201ceval\u201d mode to prevent any further backprops. model2 = MultilayerPerceptron () model2 . load_state_dict ( torch . load ( 'MyModel.pt' )); model2 . eval () # be sure to run this step! That\u2019s it! \u00b6 Toward the end of the CNN section we\u2019ll show how to import a trained model and adapt it to a new set of image data.","title":"07 Recap Saving and Loading Trained Models"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/#pytorch-for-deep-learning-bootcamp","text":"by Jawad Haider","title":"Pytorch for Deep Learning BootCamp"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/#07-saving-and-loading-trained-models","text":"Copyright Qalmaqihir For more information, visit us at www.github.com/qalmaqihir/ Saving and Loading Trained Models Saving a trained model Loading a saved model (starting from scratch) 1. Perform standard imports 2. Run the model definition 3. Instantiate the model, load parameters That\u2019s it!","title":"07 - Saving and Loading Trained Models"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/#saving-and-loading-trained-models","text":"Refer back to this notebook as a refresher on saving and loading models.","title":"Saving and Loading Trained Models"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/#saving-a-trained-model","text":"Save a trained model to a file in case you want to come back later and feed new data through it. To save a trained model called \u201cmodel\u201d to a file called \u201cMyModel.pt\u201d: torch . save ( model . state_dict (), 'MyModel.pt' ) To ensure the model has been trained before saving (assumes the variables \u201closses\u201d and \u201cepochs\u201d have been defined): if len ( losses ) == epochs : torch . save ( model . state_dict (), 'MyModel.pt' ) else : print ( 'Model has not been trained. Consider loading a trained model instead.' )","title":"Saving a trained model"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/#loading-a-saved-model-starting-from-scratch","text":"We can load the trained weights and biases from a saved model. If we\u2019ve just opened the notebook, we\u2019ll have to run standard imports and function definitions.","title":"Loading a saved model (starting from scratch)"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/#1-perform-standard-imports","text":"These will depend on the scope of the model, chosen displays, metrics, etc. # Perform standard imports import torch import torch.nn as nn import numpy as np import pandas as pd","title":"1. Perform standard imports"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/#2-run-the-model-definition","text":"We\u2019ll introduce the model shown below in the next section. class MultilayerPerceptron ( nn . Module ): def __init__ ( self , in_sz = 784 , out_sz = 10 , layers = [ 120 , 84 ]): super () . __init__ () self . fc1 = nn . Linear ( in_sz , layers [ 0 ]) self . fc2 = nn . Linear ( layers [ 0 ], layers [ 1 ]) self . fc3 = nn . Linear ( layers [ 1 ], out_sz ) def forward ( self , X ): X = F . relu ( self . fc1 ( X )) X = F . relu ( self . fc2 ( X )) X = self . fc3 ( X ) return F . log_softmax ( X , dim = 1 )","title":"2. Run the model definition"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/#3-instantiate-the-model-load-parameters","text":"First we instantiate the model, then we load the pre-trained weights & biases, and finally we set the model to \u201ceval\u201d mode to prevent any further backprops. model2 = MultilayerPerceptron () model2 . load_state_dict ( torch . load ( 'MyModel.pt' )); model2 . eval () # be sure to run this step!","title":"3. Instantiate the model, load parameters"},{"location":"bootcampsnotes/pytorchDLbootcamp/anns/07-Recap-Saving-and-Loading-Trained-Models/#thats-it","text":"Toward the end of the CNN section we\u2019ll show how to import a trained model and adapt it to a new set of image data.","title":"That\u2019s it!"},{"location":"competitiveprogramming/","text":"Competitive Programming \u00b6 Here you will find Competitive coding problems from LeetCode, CodeForces, Exercisms and many more Checkout the problems and thier solution with concise explainations: \u00b6 I will add the explaination soon in the blogs section: For now I am just mentioning my repos for the solutions and problems Solutions in Python \u00b6 50 Leetcode Problems Challenge Data Structure Algorithms Solutions in Java \u00b6 Algorithmic Problems In Java Most Common Competitive Problems Competitive Programming Java Java Practices How To Program Practice Your contribution is wellcome \u00b6","title":"Competitive Programming :watch:  :knot:"},{"location":"competitiveprogramming/#competitive-programming","text":"Here you will find Competitive coding problems from LeetCode, CodeForces, Exercisms and many more","title":"Competitive Programming"},{"location":"competitiveprogramming/#checkout-the-problems-and-thier-solution-with-concise-explainations","text":"I will add the explaination soon in the blogs section: For now I am just mentioning my repos for the solutions and problems","title":"Checkout the problems and thier solution with concise explainations:"},{"location":"competitiveprogramming/#solutions-in-python","text":"50 Leetcode Problems Challenge Data Structure Algorithms","title":"Solutions in Python"},{"location":"competitiveprogramming/#solutions-in-java","text":"Algorithmic Problems In Java Most Common Competitive Problems Competitive Programming Java Java Practices How To Program Practice","title":"Solutions in Java"},{"location":"competitiveprogramming/#your-contribution-is-wellcome","text":"","title":"Your contribution is wellcome"},{"location":"corecs/","text":"Core Computer Science Courses \u00b6 Here you will find notes related to core cs courses I m Planning to add notes for the following courses: \u00b6 Foundamentals of Programming with Python Introduction to Computer Science Data Structure & Algorithms (in Python & Java) Object Oriented Programming (Using Java & Python) R Programming MATLAB I will add blogs about each course and the importance for your future career as well \u00b6 Check out the blogs frequectly !","title":"Core Computer Science Courses :computer:"},{"location":"corecs/#core-computer-science-courses","text":"Here you will find notes related to core cs courses","title":"Core Computer Science Courses"},{"location":"corecs/#i-m-planning-to-add-notes-for-the-following-courses","text":"Foundamentals of Programming with Python Introduction to Computer Science Data Structure & Algorithms (in Python & Java) Object Oriented Programming (Using Java & Python) R Programming MATLAB","title":"I m Planning to add notes for the following courses:"},{"location":"corecs/#i-will-add-blogs-about-each-course-and-the-importance-for-your-future-career-as-well","text":"Check out the blogs frequectly !","title":"I will add blogs about each course and the importance for your future career as well"},{"location":"myresearch/","text":"Publications \u00b6 Here you will find my research publications, conference papers and much more Journal Publications \u00b6 \u201cAn Effective Classification Methodology for Brain MRI Classification Based on Statistical Features, DWT, and Blended ANN\u201d. IEEE Access Dec 2021 Doi \u201cA Smart Approximation Algorithm for Minimum Vertex Cover Problem Based on Min-to-Min Strategy\u201d. International Journal of Advance Computer Science and Applications (IJACSA) Vol. 11 No. 12, 2020 Doi Conference proceedings: \u00b6 15 th International Conference on Machine Vision A Novel and Efficient Methodology Based on Blended Machine Learning Techniques for Brain MRI Classification To be held on 18-22 Nov 2022, in Rome Italy","title":"Publications :receipt:"},{"location":"myresearch/#publications","text":"Here you will find my research publications, conference papers and much more","title":"Publications"},{"location":"myresearch/#journal-publications","text":"\u201cAn Effective Classification Methodology for Brain MRI Classification Based on Statistical Features, DWT, and Blended ANN\u201d. IEEE Access Dec 2021 Doi \u201cA Smart Approximation Algorithm for Minimum Vertex Cover Problem Based on Min-to-Min Strategy\u201d. International Journal of Advance Computer Science and Applications (IJACSA) Vol. 11 No. 12, 2020 Doi","title":"Journal Publications"},{"location":"myresearch/#conference-proceedings","text":"15 th International Conference on Machine Vision A Novel and Efficient Methodology Based on Blended Machine Learning Techniques for Brain MRI Classification To be held on 18-22 Nov 2022, in Rome Italy","title":"Conference proceedings:"}]}