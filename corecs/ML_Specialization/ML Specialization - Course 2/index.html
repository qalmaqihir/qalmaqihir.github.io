
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="All of my Computer Science & AI/ML/DL/ Book notes, BootCamp notes & Useful materials for anyone who  wants to learn; Knowledge should be free for those who need it.">
      
      
        <meta name="author" content="Jawad Haider">
      
      
        <link rel="canonical" href="https://qalmaqihir.github.io/corecs/ML_Specialization/ML%20Specialization%20-%20Course%202/">
      
      <link rel="icon" href="../../../assets/logo.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-8.5.3">
    
    
      
        <title>ML Specialization - Course 2 - CS Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.80dcb947.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ml-specialization-course-2" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
Follow us on
<a href="https://twitter.com/qalmaqihir" target="_blank" rel="noopener">
    <span class="twemoji twitter" style="color: #1DA1F2">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
            <path
                d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z">
            </path>
        </svg>
    </span>
    <strong>Twitter</strong>
</a> and

<a href="https://www.youtube.com/channel/UCB-D3NBU6UZ5N7IGKOJxtqQ?sub_confirmation=1" target="_blank" rel="noopener">
    <span class="twemoji youtube" style="color: #FF0000;">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512">
            <path
                d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z">
            </path>
        </svg>
    </span>
    <strong>YouTube</strong>
</a>  and stay <strong>Updated !</strong> for New Content

<!-- We use the twemoji project for the pancake emoji symbol
and for other emoji and icon purposes.
Check them out here: https://github.com/twitter/twemoji
-->

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="CS Notes" class="md-header__button md-logo" aria-label="CS Notes" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CS Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ML Specialization - Course 2
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/qalmaqihir/qalmaqihir.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    qalmaqihir/qalmaqihir.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        Home
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../booksnotes/" class="md-tabs__link">
        Books Notes
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../bootcampsnotes/" class="md-tabs__link">
        Bootcamps Notes
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../competitiveprogramming/" class="md-tabs__link">
        Competitive Programming
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../" class="md-tabs__link md-tabs__link--active">
        Core Computer Science
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../myresearch/" class="md-tabs__link">
        My Research
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../about/" class="md-tabs__link">
        About
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../blogs/" class="md-tabs__link">
        Blog
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../contact/" class="md-tabs__link">
        Contact
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="CS Notes" class="md-nav__button md-logo" aria-label="CS Notes" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    CS Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/qalmaqihir/qalmaqihir.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    qalmaqihir/qalmaqihir.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
        
          
            
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../..">Home</a>
          
            <label for="__nav_1">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Home" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Home
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/" class="md-nav__link">
        Books Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/" class="md-nav__link">
        Bootcamps Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../competitiveprogramming/" class="md-nav__link">
        Competitive Programming
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        Core Computer Science
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../myresearch/" class="md-nav__link">
        My Research
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../blogs/" class="md-nav__link">
        Blogs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../contact/" class="md-nav__link">
        Contact
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
        
          
            
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../booksnotes/">Books Notes</a>
          
            <label for="__nav_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Books Notes" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Books Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
        
          
            
          
        
          
            
          
        
          
        
          
        
          
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../booksnotes/pythonDataScienceHandBook/">Data Science Handbook</a>
          
            <label for="__nav_2_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Data Science Handbook" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          Data Science Handbook
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_3" type="checkbox" id="__nav_2_2_3" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2_3">
          Chapter 2 - Introduction to Numpy
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Chapter 2 - Introduction to Numpy" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_3">
          <span class="md-nav__icon md-icon"></span>
          Chapter 2 - Introduction to Numpy
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/" class="md-nav__link">
        00 Understanding Data Types in Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/" class="md-nav__link">
        01 basics of numpy arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/" class="md-nav__link">
        02 Computation on NumPy Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/" class="md-nav__link">
        03 Aggregation Min Max
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/" class="md-nav__link">
        04 Computation on Arrays Broadcasting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/" class="md-nav__link">
        05 Comparisons  Masks and Boolean Logic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/" class="md-nav__link">
        06 Fany Indexing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/" class="md-nav__link">
        07 Sorted Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/" class="md-nav__link">
        08  Structured Data
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_4" type="checkbox" id="__nav_2_2_4" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2_4">
          Chapter 3 - Data Manipulation with Pandas
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Chapter 3 - Data Manipulation with Pandas" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_4">
          <span class="md-nav__icon md-icon"></span>
          Chapter 3 - Data Manipulation with Pandas
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/" class="md-nav__link">
        01 Introduction to Pandas Objects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/" class="md-nav__link">
        02 Data Indexing and Selection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/" class="md-nav__link">
        03 Operating on Data in Pandas
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/" class="md-nav__link">
        04 Handling Missing Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/" class="md-nav__link">
        05 Hierarchical Indexing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/" class="md-nav__link">
        06 Combine dataset Concat and Append
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/" class="md-nav__link">
        07 Combining Dataset Merge and Join
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/" class="md-nav__link">
        08 Aggregation and Grouping
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/" class="md-nav__link">
        09 Pivot Tables
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/" class="md-nav__link">
        10 Vectoried String Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/" class="md-nav__link">
        11 Working with Time Series
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/" class="md-nav__link">
        12 Frequency Offset
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/" class="md-nav__link">
        13 Resampling Shifting and Windowing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/" class="md-nav__link">
        Example Visualizing Seattle Bicycle Counts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/" class="md-nav__link">
        Example  Birthrate Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/" class="md-nav__link">
        Example  US States Data (Merge and Join)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/" class="md-nav__link">
        Example Recipe Database
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_5" type="checkbox" id="__nav_2_2_5" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2_5">
          Chapter 4 - Visualization with Matplotlib
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Chapter 4 - Visualization with Matplotlib" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_5">
          <span class="md-nav__icon md-icon"></span>
          Chapter 4 - Visualization with Matplotlib
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/" class="md-nav__link">
        01general Matplotlib tips
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/" class="md-nav__link">
        02simple lineplots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/" class="md-nav__link">
        03simple scatter plots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/" class="md-nav__link">
        04visualizing errors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/" class="md-nav__link">
        05density and contour plots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/" class="md-nav__link">
        06Histograms Binnings and Density
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/" class="md-nav__link">
        07customized plot legends
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/" class="md-nav__link">
        08customizing colorbar
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/" class="md-nav__link">
        09multiple subplots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/" class="md-nav__link">
        10text and annotation Example
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/" class="md-nav__link">
        11customizing ticks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/" class="md-nav__link">
        12customizing matplotlib configuration and stylesheets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/" class="md-nav__link">
        13threedimensional plotting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/" class="md-nav__link">
        14 geographic data with basemap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/" class="md-nav__link">
        15visualiztion with seaborn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/" class="md-nav__link">
        example California cities
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/" class="md-nav__link">
        Example Exploring Marathon Finishing times
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/" class="md-nav__link">
        Example Handwritten Digits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/" class="md-nav__link">
        Example surface temperature data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/" class="md-nav__link">
        Example Visualizing a Mobius Strip
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
      
        
          
            
          
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../booksnotes/pythonDataScienceHandBook/">Machine Learning Handbook</a>
          
            <label for="__nav_2_3">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Machine Learning Handbook" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning Handbook
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../bootcampsnotes/">Bootcamps Notes</a>
          
            <label for="__nav_3">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Bootcamps Notes" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Bootcamps Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_2">
          Numpy Crash Course
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Numpy Crash Course" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          Numpy Crash Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2_1" type="checkbox" id="__nav_3_2_1" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_2_1">
          Numpy Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Numpy Topics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_2_1">
          <span class="md-nav__icon md-icon"></span>
          Numpy Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/00-NumPy-Arrays/" class="md-nav__link">
        00 NumPy Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/" class="md-nav__link">
        01 NumPy Indexing and Selection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/02-NumPy-Operations/" class="md-nav__link">
        02 NumPy Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/03-NumPy-Exercises/" class="md-nav__link">
        03 NumPy Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/" class="md-nav__link">
        04 NumPy Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_3" type="checkbox" id="__nav_3_3" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_3">
          Pandas Crash Course
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Pandas Crash Course" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          Pandas Crash Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_3_1" type="checkbox" id="__nav_3_3_1" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_3_1">
          Pandas Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Pandas Topics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_3_1">
          <span class="md-nav__icon md-icon"></span>
          Pandas Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/00-Intro-to-Pandas/" class="md-nav__link">
        00 Intro to Pandas
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/01-Series/" class="md-nav__link">
        01 Series
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/02-DataFrames/" class="md-nav__link">
        02 DataFrames
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/03-Missing-Data/" class="md-nav__link">
        03 Missing Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/04-Groupby/" class="md-nav__link">
        04 Groupby
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/05-Operations/" class="md-nav__link">
        05 Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/06-Data-Input-and-Output/" class="md-nav__link">
        06 Data Input and Output
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/07-Pandas-Exercises/" class="md-nav__link">
        07 Pandas Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/" class="md-nav__link">
        08 Pandas Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_4" type="checkbox" id="__nav_3_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_4">
          PyTorch Baics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="PyTorch Baics" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          PyTorch Baics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_4_1" type="checkbox" id="__nav_3_4_1" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_4_1">
          PyTorch Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="PyTorch Topics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_4_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/00-Tensor-Basics/" class="md-nav__link">
        00 Tensor Basics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/01-Tensor-Operations/" class="md-nav__link">
        01 Tensor Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/" class="md-nav__link">
        02 PyTorch Basics Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/" class="md-nav__link">
        03 PyTorch Basics Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5" type="checkbox" id="__nav_3_5" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5">
          PyTorch for Deeplearning Bootcamp
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="PyTorch for Deeplearning Bootcamp" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          PyTorch for Deeplearning Bootcamp
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_1" type="checkbox" id="__nav_3_5_1" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_1">
          ANN - Artificial Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ANN - Artificial Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_1">
          <span class="md-nav__icon md-icon"></span>
          ANN - Artificial Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/" class="md-nav__link">
        00 PyTorch Gradients
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/" class="md-nav__link">
        01 Linear Regression with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/" class="md-nav__link">
        02 DataSets with Pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/" class="md-nav__link">
        03 Basic PyTorch NN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/" class="md-nav__link">
        04a Full ANN Code Along Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/" class="md-nav__link">
        04b Full ANN Code Along Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/" class="md-nav__link">
        05 Neural Network Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/" class="md-nav__link">
        06 Neural Network Exercises Solutions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/" class="md-nav__link">
        07 Recap Saving and Loading Trained Models
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_2" type="checkbox" id="__nav_3_5_2" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_2">
          CNN - Convolutional Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CNN - Convolutional Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_2">
          <span class="md-nav__icon md-icon"></span>
          CNN - Convolutional Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/" class="md-nav__link">
        00 MNIST ANN Code Along
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/" class="md-nav__link">
        01 MNIST with CNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/" class="md-nav__link">
        02 CIFAR CNN Code Along
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/" class="md-nav__link">
        03 Loading Real Image Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/" class="md-nav__link">
        04 CNN on Custom Images
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/" class="md-nav__link">
        05 CNN Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/" class="md-nav__link">
        06 CNN Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_3" type="checkbox" id="__nav_3_5_3" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_3">
          RNN - Recurrent Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="RNN - Recurrent Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_3">
          <span class="md-nav__icon md-icon"></span>
          RNN - Recurrent Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/" class="md-nav__link">
        00 Basic RNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/" class="md-nav__link">
        01 RNN on a Time Series
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/" class="md-nav__link">
        02 RNN Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/" class="md-nav__link">
        03 RNN Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_4" type="checkbox" id="__nav_3_5_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_4">
          Using GPU
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Using GPU" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_4">
          <span class="md-nav__icon md-icon"></span>
          Using GPU
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/" class="md-nav__link">
        00 Using GPU and CUDA
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_5" type="checkbox" id="__nav_3_5_5" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_5">
          NLP with PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP with PyTorch" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_5">
          <span class="md-nav__icon md-icon"></span>
          NLP with PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/" class="md-nav__link">
        00 RNN for Text Generation 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6" type="checkbox" id="__nav_3_6" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6">
          Tensorflow for Deeplearning Bootcamp
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tensorflow for Deeplearning Bootcamp" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          Tensorflow for Deeplearning Bootcamp
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_1" type="checkbox" id="__nav_3_6_1" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_1">
          Colab Basics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Colab Basics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_1">
          <span class="md-nav__icon md-icon"></span>
          Colab Basics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Demo/" class="md-nav__link">
        TF2 0 Demo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Installing_Tensorflow/" class="md-nav__link">
        TF2 0 Installing Tensorflow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/" class="md-nav__link">
        TF2 0 Loading Data
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_2" type="checkbox" id="__nav_3_6_2" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_2">
          Machine Learning Basics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Machine Learning Basics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_2">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning Basics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Classification/" class="md-nav__link">
        TF2 0 Linear Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Regression/" class="md-nav__link">
        TF2 0 Linear Regression
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_3" type="checkbox" id="__nav_3_6_3" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_3">
          ANN - Artificial Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ANN - Artificial Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_3">
          <span class="md-nav__icon md-icon"></span>
          ANN - Artificial Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_MNIST/" class="md-nav__link">
        TF2 0 ANN MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_Regression/" class="md-nav__link">
        TF2 0 ANN Regression
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_4" type="checkbox" id="__nav_3_6_4" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_4">
          CNN - Convolutional Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CNN - Convolutional Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_4">
          <span class="md-nav__icon md-icon"></span>
          CNN - Convolutional Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_Fashion_MNIST/" class="md-nav__link">
        TF2 0 Fashion MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR/" class="md-nav__link">
        TF2 0 CIFAR
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR_Improved/" class="md-nav__link">
        TF2 0 CIFAR Improved
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_5" type="checkbox" id="__nav_3_6_5" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_5">
          RNN - Recurrent Neural Network
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="RNN - Recurrent Neural Network" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_5">
          <span class="md-nav__icon md-icon"></span>
          RNN - Recurrent Neural Network
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Autoregressive_Model/" class="md-nav__link">
        TF2 0 Autoregressive Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_SimpleRNN_Sine/" class="md-nav__link">
        TF2 0 SimpleRNN Sine
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_Shapes/" class="md-nav__link">
        TF2 0 RNN Shapes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_LSTM_Nonlinear/" class="md-nav__link">
        TF2 0 LSTM Nonlinear
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Long_Distance/" class="md-nav__link">
        TF2 0 Long Distance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_MNIST/" class="md-nav__link">
        TF2 0 RNN MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Stock_Returns/" class="md-nav__link">
        TF2 0 Stock Returns
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_6" type="checkbox" id="__nav_3_6_6" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_6">
          NLP - Natural Language Processing
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP - Natural Language Processing" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_6">
          <span class="md-nav__icon md-icon"></span>
          NLP - Natural Language Processing
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Text_Preprocessing/" class="md-nav__link">
        TF2 0 Text Preprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_CNN/" class="md-nav__link">
        TF2 0 Spam Detection CNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_RNN/" class="md-nav__link">
        TF2 0 Spam Detection RNN
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_7" type="checkbox" id="__nav_3_6_7" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_7">
          Recommendar Systems
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Recommendar Systems" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_7">
          <span class="md-nav__icon md-icon"></span>
          Recommendar Systems
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RecommenderSystem/TF2_0_Recommender_System/" class="md-nav__link">
        TF2 0 Recommender System
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_8" type="checkbox" id="__nav_3_6_8" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_8">
          Transfer Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Transfer Learning" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_8">
          <span class="md-nav__icon md-icon"></span>
          Transfer Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning/" class="md-nav__link">
        TF2 0 Transfer Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning_with_Data_Augmentation/" class="md-nav__link">
        TF2 0 Transfer Learning with Data Augmentation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_9" type="checkbox" id="__nav_3_6_9" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_9">
          GANs - Generative Adversarial Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="GANs - Generative Adversarial Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_9">
          <span class="md-nav__icon md-icon"></span>
          GANs - Generative Adversarial Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/GANs/TF2_0_GAN/" class="md-nav__link">
        TF2 0 GAN
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_10" type="checkbox" id="__nav_3_6_10" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_10">
          Advance Tensorflow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Advance Tensorflow" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_10">
          <span class="md-nav__icon md-icon"></span>
          Advance Tensorflow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Serving/" class="md-nav__link">
        TF2 0 Serving
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Mirrored_Strategy/" class="md-nav__link">
        TF2 0 Mirrored Strategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_TFLite/" class="md-nav__link">
        TF2 0 TFLite
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TPU/" class="md-nav__link">
        TPU
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_11" type="checkbox" id="__nav_3_6_11" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_11">
          Low-level Transorflow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Low-level Transorflow" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_11">
          <span class="md-nav__icon md-icon"></span>
          Low-level Transorflow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Basic_Computation/" class="md-nav__link">
        TF2 0 Basic Computation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Variables_and_Gradient_Tape/" class="md-nav__link">
        TF2 0 Variables and Gradient Tape
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Build_Your_Own_Model/" class="md-nav__link">
        TF2 0 Build Your Own Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../competitiveprogramming/">Competitive Programming</a>
          
        </div>
      
      <nav class="md-nav" aria-label="Competitive Programming" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Competitive Programming
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../">Core Computer Science</a>
          
            <label for="__nav_5">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Core Computer Science" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Core Computer Science
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_2">
          ML Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ML Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          ML Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ML%20Specialization%20-%20Course%201/" class="md-nav__link">
        ML Specialization - Course 1
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        ML Specialization - Course 2
      </a>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ML%20Specialization%20-%20Course%203/" class="md-nav__link">
        ML Specialization - Course 3
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3" type="checkbox" id="__nav_5_3" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3">
          NLP Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          NLP Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%201/" class="md-nav__link">
        NLP Specialization - Course 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%202/" class="md-nav__link">
        NLP Specialization - Course 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%203/" class="md-nav__link">
        NLP Specialization - Course 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%204/" class="md-nav__link">
        NLP Specialization - Course 4
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_4" type="checkbox" id="__nav_5_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_4">
          IBM Data Science Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="IBM Data Science Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_4">
          <span class="md-nav__icon md-icon"></span>
          IBM Data Science Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Coming Soon" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_5" type="checkbox" id="__nav_5_5" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_5">
          NLP Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_5">
          <span class="md-nav__icon md-icon"></span>
          NLP Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Coming Soon" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../myresearch/">My Research</a>
          
        </div>
      
      <nav class="md-nav" aria-label="My Research" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          My Research
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7">
          About
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="About" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          About
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      
      
        
          
            
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../blogs/">Blog</a>
          
            <label for="__nav_8">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Blog" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Blog
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_2" type="checkbox" id="__nav_8_2" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_2">
          2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="2022" data-md-level="2">
        <label class="md-nav__title" for="__nav_8_2">
          <span class="md-nav__icon md-icon"></span>
          2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../blogs/2022/How-to-use-these-resources/" class="md-nav__link">
        How-to-use-these-resources
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../blogs/2022/How-to-contribute/" class="md-nav__link">
        How to contribute
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_9" type="checkbox" id="__nav_9" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9">
          Contact
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Contact" data-md-level="1">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          Contact
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../contact/" class="md-nav__link">
        Contact
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/qalmaqihir/qalmaqihir.github.io/edit/master/docs/corecs/ML_Specialization/ML Specialization - Course 2.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


<h1 id="ml-specialization-course-2">ML Specialization - Course 2<a class="headerlink" href="#ml-specialization-course-2" title="Permanent link">&para;</a></h1>
<h1 id="advance-algorithms">Advance Algorithms<a class="headerlink" href="#advance-algorithms" title="Permanent link">&para;</a></h1>
<p>Note 2024-01-15T12.01.27</p>
<p>========================</p>
<h2 id="week-1">Week 1<a class="headerlink" href="#week-1" title="Permanent link">&para;</a></h2>
<h3 id="neurons-and-the-brain">Neurons and the Brain<a class="headerlink" href="#neurons-and-the-brain" title="Permanent link">&para;</a></h3>
<p><strong>Neurons and the Brain - Key Points:</strong></p>
<ol>
<li><strong>Historical Perspective:</strong></li>
<li>Neural networks were initially developed to mimic the learning and thinking processes of the human brain.</li>
<li>
<p>Over the decades, they have evolved into powerful tools, although their functioning differs significantly from biological brains.</p>
</li>
<li>
<p><strong>Neural Network Evolution:</strong></p>
</li>
<li>Neural network research started in the 1950s, experienced phases of popularity, and gained significant traction in the 2000s with the rebranding of deep learning.</li>
<li>
<p>The resurgence was marked by breakthroughs in applications like speech and image recognition.</p>
</li>
<li>
<p><strong>Brain and Neurons:</strong></p>
</li>
<li>The biological brain consists of neurons that transmit electrical impulses, forming connections with other neurons.</li>
<li>
<p>Artificial neurons in neural networks are simplified mathematical models inspired by biological neurons.</p>
</li>
<li>
<p><strong>Simplified Neuron Model:</strong></p>
</li>
<li>In an artificial neural network, a neuron receives inputs, performs computations, and outputs a value.</li>
<li>
<p>Multiple neurons collectively process inputs, enabling complex learning tasks.</p>
</li>
<li>
<p><strong>Caveat on Biological Analogies:</strong></p>
</li>
<li>While inspired by the brain, modern neural networks deviate significantly from how the biological brain operates.</li>
<li>
<p>Current neuroscience understanding is limited, and blindly mimicking the brain may not lead to advanced intelligence.</p>
</li>
<li>
<p><strong>Data and Performance:</strong></p>
</li>
<li>The recent success of neural networks can be attributed to the abundance of digital data.</li>
<li>
<p>Traditional algorithms struggled to scale with increasing data, but neural networks demonstrated the ability to exploit large datasets effectively.</p>
</li>
<li>
<p><strong>Data and Neural Network Performance:</strong></p>
</li>
<li>Neural network performance scales positively with the amount of data, especially for large networks.</li>
<li>
<p>Faster computer processors, including GPUs, played a crucial role in the success of deep learning algorithms.</p>
</li>
<li>
<p><strong>Acceleration of Deep Learning:</strong></p>
</li>
<li>
<p>The combination of abundant data, powerful hardware, and scalable neural network architectures led to the rapid acceleration of deep learning in various applications.</p>
</li>
<li>
<p><strong>Looking Beyond Biological Motivation:</strong></p>
</li>
<li>While neural networks originated from a biological motivation, current research focuses more on engineering principles for algorithm effectiveness.</li>
<li>The term "deep learning" has become dominant, overshadowing the initial biological motivation.</li>
</ol>
<p>Understanding these key points provides a foundation for delving deeper into the mechanics and applications of neural networks.</p>
<h3 id="demand-prediction">Demand Prediction<a class="headerlink" href="#demand-prediction" title="Permanent link">&para;</a></h3>
<p><strong>Demand Prediction with Neural Networks - Key Takeaways:</strong></p>
<ol>
<li><strong>Example Context:</strong></li>
<li><strong>Problem:</strong> Demand prediction for T-shirts.</li>
<li><strong>Input Feature (x):</strong> Price of the T-shirt.</li>
<li>
<p><strong>Goal:</strong> Predict whether a T-shirt will be a top seller.</p>
</li>
<li>
<p><strong>Logistic Regression Basis:</strong></p>
</li>
<li>Logistic regression is applied to model the probability of a T-shirt being a top seller based on its price.</li>
<li>
<p>The logistic regression formula:  <br />
<span class="arithmatex">\(<span class="arithmatex">\(\(1 / (1 + e^{-(wx + b)})\), where \(w\) and \(b\) are parameters.\)</span>\)</span>   </p>
</li>
<li>
<p><strong>Artificial Neuron Concept:</strong></p>
</li>
<li>Neurons in neural networks are simplified models inspired by biological neurons.</li>
<li>
<p>An artificial neuron takes inputs, performs computations, and outputs a value (activation).</p>
</li>
<li>
<p><strong>Single Neuron Model:</strong></p>
</li>
<li>Affordability prediction: Neuron takes inputs of price and shipping costs.</li>
<li>
<p>Activation (<span class="arithmatex">\(a\)</span>): Probability of the T-shirt being affordable.</p>
</li>
<li>
<p><strong>Extension to Multiple Features:</strong></p>
</li>
<li>A more complex model involves considering multiple features: price, shipping costs, marketing, and material quality.</li>
<li>
<p>Neurons estimate affordability, awareness, and perceived quality.</p>
</li>
<li>
<p><strong>Hidden Layer and Activation:</strong></p>
</li>
<li>Neurons estimating features form a layer (hidden layer).</li>
<li>
<p>Activation values (<span class="arithmatex">\(a\)</span>): Outputs of neurons, represent the degree of perceived affordability, awareness, and quality.</p>
</li>
<li>
<p><strong>Output Layer:</strong></p>
</li>
<li>
<p>The final output layer combines the activation values to predict the probability of a T-shirt being a top seller.</p>
</li>
<li>
<p><strong>Neural Network Layers:</strong></p>
</li>
<li>Input Layer: Takes a vector of features.</li>
<li>Hidden Layer: Estimates intermediate features (affordability, awareness, quality).</li>
<li>
<p>Output Layer: Produces the final prediction (probability of a top-selling T-shirt).</p>
</li>
<li>
<p><strong>Learned Features and Intuition:</strong></p>
</li>
<li>Neural networks can learn their own features, eliminating the need for manual feature engineering.</li>
<li>
<p>The power of neural networks lies in their ability to discover relevant features during training.</p>
</li>
<li>
<p><strong>Architecture Decisions:</strong></p>
</li>
<li>Design choices include the number of hidden layers and neurons per layer.</li>
<li>
<p>A well-chosen architecture contributes to the performance of the learning algorithm</p>
</li>
<li>
<p><strong>Multilayer Perceptron (MLP):</strong></p>
</li>
<li>
<p>Neural networks with multiple hidden layers are often referred to as multilayer perceptrons (MLPs).</p>
</li>
<li>
<p><strong>Future Applications:</strong>  </p>
</li>
<li>Neural networks extend beyond demand prediction.</li>
<li>Next, the video will explore an application in computer vision: face recognition.</li>
</ol>
<p>Understanding the architecture, concepts, and flexibility of neural networks lays the groundwork for their application in diverse domains.</p>
<h3 id="example-recognizing-images">Example: Recognizing Images<a class="headerlink" href="#example-recognizing-images" title="Permanent link">&para;</a></h3>
<p><strong>Neural Networks in Computer Vision - Key Points:</strong></p>
<ol>
<li><strong>Image Representation:</strong></li>
<li>Images are represented as grids or matrices of pixel intensity values.</li>
<li>
<p>Example: A 1,000 by 1,000 image results in a vector of one million pixel values.</p>
</li>
<li>
<p><strong>Face Recognition Task:</strong></p>
</li>
<li>Objective: Train a neural network to recognize faces.</li>
<li>Input: Feature vector with a million pixel brightness values.</li>
<li>
<p>Output: Identity of the person in the picture.</p>
</li>
<li>
<p><strong>Neural Network Architecture:</strong></p>
</li>
<li>Input Layer: Takes the pixel intensity values.</li>
<li>Hidden Layers: Extract features in a hierarchical manner.</li>
<li>
<p>Output Layer: Produces the final prediction (e.g., identity probability).</p>
</li>
<li>
<p><strong>Feature Detection in Hidden Layers:</strong></p>
</li>
<li><strong>First Hidden Layer:</strong><ul>
<li>Neurons may detect simple features like vertical or oriented lines.</li>
</ul>
</li>
<li><strong>Second Hidden Layer:</strong><ul>
<li>Neurons group short lines to detect parts of faces (e.g., eyes, nose).</li>
</ul>
</li>
<li>
<p><strong>Subsequent Hidden Layers:</strong></p>
<ul>
<li>Hierarchical grouping to detect larger, coarser face shapes.</li>
</ul>
</li>
<li>
<p><strong>Automatic Feature Learning:</strong></p>
</li>
<li>The neural network learns to detect features automatically from the data.</li>
<li>
<p>No manual specification of what features to look for in each layer.</p>
</li>
<li>
<p><strong>Visualization of Neurons:</strong></p>
</li>
<li>Visualizing neurons in hidden layers reveals what each neuron is trying to detect.</li>
<li>
<p>Neurons in early layers focus on short edges; later layers combine features for face recognition.</p>
</li>
<li>
<p><strong>Adaptability to Different Datasets:</strong></p>
</li>
<li>Training the same neural network on a dataset of cars leads to automatic adaptation.</li>
<li>
<p>The network learns to detect features specific to cars.</p>
</li>
<li>
<p><strong>Implementation for Computer Vision:</strong></p>
</li>
<li>In computer vision applications, neural networks learn hierarchical features for pattern recognition.</li>
<li>
<p>Neural networks can be applied to various tasks, such as image recognition and object detection.</p>
</li>
<li>
<p><strong>Upcoming Application: Handwritten Digit Recognition:</strong></p>
</li>
<li>
<p>Future videos will demonstrate how to build and apply a neural network for recognizing handwritten digits.</p>
</li>
<li>
<p><strong>Next Steps: Concrete Mathematics and Implementation:</strong></p>
<ul>
<li>The upcoming video will delve into the mathematical details and practical implementation of neural networks.</li>
</ul>
</li>
</ol>
<p>Understanding the adaptive nature of neural networks, their ability to learn features from data, and their versatility in recognizing patterns sets the stage for exploring the mechanics of building and implementing neural networks.</p>
<h3 id="neural-network-layer">Neural network layer<a class="headerlink" href="#neural-network-layer" title="Permanent link">&para;</a></h3>
<p><strong>Key Points: Constructing a Neural Network Layer</strong></p>
<ol>
<li><strong>Neural Network Layer Basics:</strong></li>
<li>A layer of neurons is a fundamental building block in neural networks.</li>
<li>
<p>Understanding how to construct a layer enables the creation of larger neural networks.</p>
</li>
<li>
<p><strong>Example from Demand Prediction:</strong></p>
</li>
<li>
<p>Previous example: Input layer with four features, a hidden layer with three neurons, and an output layer with one neuron.</p>
</li>
<li>
<p><strong>Hidden Layer Computations:</strong></p>
</li>
<li>Each neuron in the hidden layer operates like a logistic regression unit.</li>
<li>Parameters for the first neuron: <span class="arithmatex">\(w_1, b_1\)</span>, activation <span class="arithmatex">\(a_1 = g(w_1 \cdot x + b_1)\)</span>.</li>
<li>
<p>Similar computations for other neurons in the hidden layer.</p>
</li>
<li>
<p><strong>Layer Indexing:</strong></p>
</li>
<li>By convention, the hidden layer is termed "Layer 1," and the output layer is termed "Layer 2."</li>
<li>
<p>Superscripts in square brackets (e.g., <span class="arithmatex">\(w^{[1]}, a^{[1]}\)</span>) distinguish quantities associated with different layers.</p>
</li>
<li>
<p><strong>Output of Layer 1:</strong></p>
</li>
<li>
<p>The output of Layer 1, denoted as <span class="arithmatex">\(a^{[1]}\)</span>, is a vector comprising activation values of neurons in Layer 1.</p>
</li>
<li>
<p><strong>Layer 2 (Output Layer) Computations:</strong></p>
</li>
<li>The input to Layer 2 is the output of Layer 1 <span class="arithmatex">\((\(a^{[1]}\))\)</span>.</li>
<li>
<p>The output of the single neuron in Layer 2: <span class="arithmatex">\(a^{[2]} = g(w_1^{[2]} \cdot a^{[1]} + b^{[2]})\)</span>.</p>
</li>
<li>
<p><strong>Layer 2 Parameters:</strong></p>
</li>
<li>
<p>Parameters of Layer 2 are denoted with superscripts in square brackets (e.g., <span class="arithmatex">\(w^{[2]}, b^{[2]}\)</span>).</p>
</li>
<li>
<p><strong>Binary Prediction:</strong></p>
</li>
<li>Optionally, a binary prediction (<span class="arithmatex">\(y_{\text{hat}}\)</span>) can be obtained by thresholding <span class="arithmatex">\(a^{[2]}_1\)</span> at 0.5.</li>
<li>
<p>If <span class="arithmatex">\(a^{[2]}_1 \geq 0.5\)</span>, predict <span class="arithmatex">\(y_{\text{hat}} = 1\)</span>; otherwise, predict <span class="arithmatex">\(y_{\text{hat}} = 0\)</span>.</p>
</li>
<li>
<p><strong>Generalizing to More Layers:</strong></p>
</li>
<li>The principles apply to neural networks with more layers (e.g., Layer 3, Layer 4, and so on).</li>
<li>
<p>Superscripts in square brackets help distinguish parameters and activation values for different layers.</p>
</li>
<li>
<p><strong>Complex Neural Networks:</strong></p>
<ul>
<li>Larger neural networks involve stacking multiple layers.</li>
<li>Further examples will enhance the understanding of layer composition and network construction.</li>
</ul>
</li>
</ol>
<p>Understanding how to compute and propagate information through different layers forms the basis for building more intricate neural network architectures. Further examples will elaborate on constructing and utilizing larger neural networks.</p>
<h3 id="more-complex-neural-networks">More complex neural networks<a class="headerlink" href="#more-complex-neural-networks" title="Permanent link">&para;</a></h3>
<p><strong>Key Points: Constructing More Complex Neural Networks</strong></p>
<ol>
<li><strong>Network Architecture:</strong></li>
<li>
<p>Example neural network with four layers: Layers 0 (input), 1, 2, and 3 (hidden layers), and Layer 4 (output).</p>
</li>
<li>
<p><strong>Layer 3 Computations:</strong></p>
</li>
<li>Focus on computations in Layer 3, the third hidden layer.</li>
<li>
<p>Takes input <span class="arithmatex">\(a^{[2]}\)</span> and produces output <span class="arithmatex">\(a^{[3]}\)</span> with three neurons.</p>
</li>
<li>
<p><strong>Parameters of Layer 3:</strong></p>
</li>
<li>
<p>Three neurons in Layer 3 have parameters <span class="arithmatex">\(w_1, b_1, w_2, b_2, w_3, b_3\)</span>.</p>
</li>
<li>
<p><strong>Computational Process:</strong></p>
</li>
<li>Each neuron applies the sigmoid function to <span class="arithmatex">\(w \cdot a^{[2]} + b\)</span> to produce <span class="arithmatex">\(a_1, a_2, a_3\)</span>.</li>
<li>
<p>Output vector <span class="arithmatex">\(a^{[3]}\)</span> is formed by these activation values.</p>
</li>
<li>
<p><strong>Notation Conventions:</strong></p>
</li>
<li>
<p>Superscripts in square brackets (e.g., <span class="arithmatex">\(w^{[3]}, a^{[3]}\)</span>) specify quantities associated with Layer 3.</p>
</li>
<li>
<p><strong>Understanding Superscripts and Subscripts:</strong></p>
</li>
<li>Test understanding by hiding notation and determining correct superscripts and subscripts.</li>
<li>
<p>Correctly identified: <span class="arithmatex">\(w_2^{[3]}, a_2^{[2]}, b_3^{[3]}\)</span>.</p>
</li>
<li>
<p><strong>General Form of Activation Equation:</strong></p>
</li>
<li>Activation  <span class="arithmatex">\(\(a_{\text{lj}}^{[l]}\) for layer \(l\) and unit \(j\):\)</span>
     <span class="arithmatex">\(\[a_{\text{lj}}^{[l]} = g(w_{\text{lj}}^{[l]} \cdot a_{\text{(l-1)}}^{[l-1]} + b_{\text{lj}}^{[l]})\]\)</span></li>
<li>
<p><span class="arithmatex">\(g\)</span> is the activation function (e.g., sigmoid).</p>
</li>
<li>
<p><strong>Activation Function (g):</strong></p>
</li>
<li>In the context of a neural network, <span class="arithmatex">\(g\)</span> is referred to as the activation function.</li>
<li>
<p>Sigmoid function is a common activation function.</p>
</li>
<li>
<p><strong>Input Vector Notation:</strong></p>
</li>
<li>Input vector <span class="arithmatex">\(X\)</span> is also denoted as <span class="arithmatex">\(a_0\)</span>, ensuring consistency with layer notation.</li>
<li>
<p>Allows the activation equation to be applicable to the first layer as well.</p>
</li>
<li>
<p><strong>Computing Activations:</strong></p>
<ul>
<li>Equipped to compute activation values for any layer given parameters and activations of the previous layer.</li>
</ul>
</li>
</ol>
<p>Understanding the computations within each layer, along with consistent notation, forms the foundation for constructing and comprehending more complex neural network architectures. The activation equation provides a versatile tool for understanding how information is processed through different layers in a neural network.</p>
<h3 id="inference-making-predictions-forward-propagation">Inference: making predictions (forward propagation)<a class="headerlink" href="#inference-making-predictions-forward-propagation" title="Permanent link">&para;</a></h3>
<p><strong>Key Points: Forward Propagation for Neural Network Inference</strong></p>
<ol>
<li><strong>Motivating Example: Handwritten Digit Recognition:</strong></li>
<li>Binary classification: Distinguishing between handwritten digits 0 and 1.</li>
<li>Input: 8x8 image (64 pixel intensity values).</li>
<li>
<p>Neural network architecture: Two hidden layers (25 neurons in the first, 15 in the second), output layer.</p>
</li>
<li>
<p><strong>Sequence of Computations:</strong></p>
</li>
<li>
<p><strong>Step 1 (Input to Hidden Layer 1):</strong>
     <span class="arithmatex">\(<span class="arithmatex">\(\[a^{[1]} = g(w^{[1]} \cdot a^{[0]} + b^{[1]})\]\)</span>\)</span>    </p>
<ul>
<li>
<div class="arithmatex">\[\(a^{[0]} = X\), input features.\]</div>
</li>
<li><span class="arithmatex">\(<span class="arithmatex">\(\(a^{[1]}\): Activation of hidden layer 1 (25 values)\)</span>\)</span>.  </li>
<li>
<div class="arithmatex">\[\(g\): Activation function (sigmoid).\]</div>
</li>
</ul>
</li>
<li>
<p><strong>Step 2 (Hidden Layer 1 to Hidden Layer 2):</strong>
     <span class="arithmatex">\(\[a^{[2]} = g(w^{[2]} \cdot a^{[1]} + b^{[2]})\]\)</span></p>
<ul>
<li><span class="arithmatex">\(\(a^{[2]}\): Activation of hidden layer 2 (15 values).\)</span></li>
<li><span class="arithmatex">\(\(w^{[2]}, b^{[2]}\): Parameters of hidden layer 2.\)</span></li>
</ul>
</li>
<li>
<p><strong>Step 3 (Hidden Layer 2 to Output Layer):</strong>
     <span class="arithmatex">\(\[a^{[3]} = g(w^{[3]} \cdot a^{[2]} + b^{[3]})\]\)</span></p>
<ul>
<li><span class="arithmatex">\(\(a^{[3]}\): Output layer activation (1 value).\)</span></li>
<li><span class="arithmatex">\(\(w^{[3]}, b^{[3]}\): Parameters of output layer.\)</span></li>
</ul>
</li>
<li>
<p><strong>Step 4 (Optional: Thresholding for Binary Classification):</strong>
     <span class="arithmatex">\(\[y_{\text{hat}} = \begin{cases} 1 &amp; \text{if } a^{[3]} \geq 0.5 \\ 0 &amp; \text{otherwise} \end{cases}\]\)</span></p>
<ul>
<li><span class="arithmatex">\(\(y_{\text{hat}}\): Binary classification prediction.\)</span></li>
</ul>
</li>
<li>
<p><strong>Forward Propagation:</strong></p>
</li>
<li>Forward propagation involves making computations from input (<span class="arithmatex">\(X\)</span>) to output (<span class="arithmatex">\(a^{[3]}\)</span>).</li>
<li>
<p>The process of propagating activations from left to right is known as forward propagation.</p>
</li>
<li>
<p><strong>TensorFlow Implementation:</strong></p>
</li>
<li>TensorFlow provides tools for implementing neural networks.</li>
<li>
<p>In the next video, there will be a demonstration of how to implement the forward propagation algorithm in TensorFlow.</p>
</li>
<li>
<p><strong>Backward Propagation:</strong></p>
</li>
<li>Mentioned as a future topic (covered in the next week's material).</li>
<li>Contrasts with forward propagation and is used for learning.</li>
</ol>
<p>Understanding the sequence of computations in forward propagation is crucial for making predictions using a neural network. The activation function (e.g., sigmoid) is applied at each layer, and the output can be thresholded for binary classification. TensorFlow provides a practical framework for implementing such algorithms.</p>
<h3 id="inference-in-code">Inference in Code<a class="headerlink" href="#inference-in-code" title="Permanent link">&para;</a></h3>
<p><strong>Key Points: Implementing Inference Code in TensorFlow</strong></p>
<ol>
<li><strong>TensorFlow for Deep Learning:</strong></li>
<li>TensorFlow is a leading framework for implementing deep learning algorithms.</li>
<li>
<p>Commonly used in building neural networks for various applications.</p>
</li>
<li>
<p><strong>Example: Coffee Bean Roasting Optimization:</strong></p>
</li>
<li>Task: Optimize the quality of coffee beans based on temperature and duration parameters.</li>
<li>
<p>Dataset includes different temperature/duration combinations labeled as good or bad coffee.</p>
</li>
<li>
<p><strong>Neural Network Architecture:</strong></p>
</li>
<li>Input Features (<span class="arithmatex">\(x\)</span>): Temperature and duration (e.g., 200 degrees Celsius for 17 minutes).</li>
<li>Layer 1: Dense layer with 3 units and sigmoid activation function.</li>
<li>Layer 2: Dense layer with 1 unit and sigmoid activation function.</li>
<li>
<p>Thresholding optional for binary classification.</p>
</li>
<li>
<p><strong>Inference Steps:</strong></p>
</li>
<li>
<p><strong>Step 1 (Layer 1):</strong>
     <span class="arithmatex">\(\[a^{[1]} = \text{sigmoid}(\text{dense}(\text{units}=3, \text{activation}=\text{sigmoid})(x))\]\)</span></p>
</li>
<li>
<p><strong>Step 2 (Layer 2):</strong>
    <span class="arithmatex">\(\[a^{[2]} = \text{sigmoid}(\text{dense}(\text{units}=1, \text{activation}=\text{sigmoid})(a^{[1]}))\]\)</span></p>
</li>
<li>
<p><strong>Step 3 (Optional Thresholding):</strong>
     <span class="arithmatex">\(\[y_{\text{hat}} = \begin{cases} 1 &amp; \text{if } a^{[2]} \geq 0.5 \\ 0 &amp; \text{otherwise} \end{cases}\]\)</span></p>
</li>
<li>
<p><strong>Numpy Arrays in TensorFlow:</strong></p>
</li>
<li>Pay attention to the structure of numpy arrays used in TensorFlow.</li>
<li>
<p>Proper handling of data structure is essential.</p>
</li>
<li>
<p><strong>Additional Details in Lab:</strong></p>
</li>
<li>Loading the TensorFlow library and neural network parameters (w, b).</li>
<li>
<p>Lab exercises will cover these details.</p>
</li>
<li>
<p><strong>Handwritten Digit Classification Example:</strong></p>
</li>
<li>Input (<span class="arithmatex">\(x\)</span>): List of pixel intensity values.</li>
<li>Layer 1: Dense layer with 25 units and sigmoid activation function.</li>
<li>Layer 2: Dense layer with unspecified units and activation function.</li>
<li>
<p>Layer 3: Output layer.</p>
</li>
<li>
<p><strong>Syntax for Inference in TensorFlow:</strong></p>
</li>
<li>TensorFlow provides a clear syntax for building and executing neural network inference.</li>
<li>Understanding the steps of forward propagation is crucial for implementing inference.</li>
</ol>
<p>The provided examples illustrate how to structure and implement inference code using TensorFlow, emphasizing the importance of proper data handling and following the syntax for building neural network layers. The lab exercises will provide hands-on experience with TensorFlow implementation.</p>
<h3 id="data-in-tensorflow">Data in TensorFlow<a class="headerlink" href="#data-in-tensorflow" title="Permanent link">&para;</a></h3>
<p><strong>Key Points: Data Representation in NumPy and TensorFlow</strong></p>
<ol>
<li><strong>History of NumPy and TensorFlow:</strong></li>
<li>NumPy was created as a standard library for linear algebra in Python many years ago.</li>
<li>
<p>TensorFlow, developed by the Google Brain team, was created later for deep learning.</p>
</li>
<li>
<p><strong>Inconsistencies in Data Representation:</strong></p>
</li>
<li>Due to historical reasons, there are some inconsistencies between NumPy and TensorFlow in data representation.</li>
<li>
<p>Understanding these conventions is crucial for correct implementation.</p>
</li>
<li>
<p><strong>Matrix Representation in NumPy:</strong></p>
</li>
<li>Matrices are represented using the <code>np.array</code> function.</li>
<li>Example: $ <span class="arithmatex">\( \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix} \)</span> is a 2 x 3 matrix.$</li>
<li>
<p>Square brackets denote rows, and double square brackets group rows.</p>
</li>
<li>
<p><strong>TensorFlow Data Representation:</strong></p>
</li>
<li>TensorFlow prefers matrices over 1D arrays for efficiency.</li>
<li>A matrix with one row: <code>x = np.array([[200, 17]])</code> (1 x 2 matrix).</li>
<li>A matrix with one column: <code>x = np.array([[200], [17]])</code> (2 x 1 matrix).</li>
<li>
<p>A 1D array (vector): <code>x = np.array([200, 17])</code>.</p>
</li>
<li>
<p><strong>TensorFlow Tensors:</strong></p>
</li>
<li>TensorFlow represents matrices using tensors.</li>
<li>A tensor is a data type designed for efficient matrix computations.</li>
<li>
<p>The shape of the tensor represents the matrix dimensions.</p>
</li>
<li>
<p><strong>TensorFlow vs. NumPy Representation:</strong></p>
</li>
<li>Conversion from TensorFlow tensor to NumPy array: <code>tensor.numpy()</code>.</li>
<li>
<p>TensorFlow internally converts NumPy arrays to tensors for efficiency.</p>
</li>
<li>
<p><strong>Data Conversion:</strong></p>
</li>
<li>TensorFlow tensors and NumPy arrays can be converted back and forth.</li>
<li>
<p>Be aware of the conversion when working with both libraries.</p>
</li>
<li>
<p><strong>TensorFlow Data Types:</strong></p>
</li>
<li>TensorFlow tensors are often of type <code>float32</code>.</li>
<li>Tensors efficiently handle large datasets for deep learning computations.</li>
</ol>
<p>Understanding these conventions helps in correctly representing data when working with both NumPy and TensorFlow. While the historical differences can be seen as a challenge, awareness of data representation and conversion methods is essential for smooth integration.</p>
<h3 id="building-a-neural-network">Building a neural network<a class="headerlink" href="#building-a-neural-network" title="Permanent link">&para;</a></h3>
<p><strong>Key Points: Building a Neural Network in TensorFlow</strong></p>
<ol>
<li><strong>Sequential Model in TensorFlow:</strong></li>
<li>TensorFlow provides a convenient way to build neural networks using the <code>Sequential</code> model.</li>
<li>
<p>The <code>Sequential</code> model allows you to sequentially stack layers to create a neural network.</p>
</li>
<li>
<p><strong>Simplified Code with Sequential Model:</strong></p>
</li>
<li>Instead of explicitly creating and connecting layers one by one, you can use the <code>Sequential</code> model to string them together.</li>
<li>
<p>The code becomes more concise and resembles the architecture of the neural network.</p>
</li>
<li>
<p><strong>Training and Inference in TensorFlow:</strong></p>
</li>
<li>To train the neural network, you need to call <code>model.compile</code> and <code>model.fit</code> functions.</li>
<li>
<p>For inference or making predictions, use <code>model.predict</code> on new data.</p>
</li>
<li>
<p><strong>Coding Convention in TensorFlow:</strong></p>
</li>
<li>By convention, you often see more concise code without explicit assignments for each layer.</li>
<li>
<p>Example: <code>model = Sequential([Dense(3, activation='sigmoid'), Dense(1, activation='sigmoid')])</code></p>
</li>
<li>
<p><strong>Applying Sequential Model to Different Examples:</strong></p>
</li>
<li>The same sequential model approach can be applied to different examples, such as coffee bean roasting or handwritten digit classification.</li>
<li>
<p>Model architecture is defined using layers, and data is fed to the model for training and inference.</p>
</li>
<li>
<p><strong>Understanding Code Implementation:</strong></p>
</li>
<li>While concise code is powerful, it's essential to understand the underlying mechanisms.</li>
<li>
<p>Next, you'll learn how to implement forward propagation from scratch in Python to deepen your understanding.</p>
</li>
<li>
<p><strong>Deeper Understanding of Algorithms:</strong></p>
</li>
<li>Although most machine learning engineers use high-level libraries like TensorFlow, understanding the fundamentals allows you to troubleshoot and optimize effectively.</li>
</ol>
<p>In the next video, you'll dive into implementing forward propagation from scratch in Python. This hands-on approach will enhance your understanding of the neural network's inner workings.</p>
<h3 id="forward-prop-in-a-single-layer">Forward prop in a single layer<a class="headerlink" href="#forward-prop-in-a-single-layer" title="Permanent link">&para;</a></h3>
<p><strong>Forward Propagation in a Single Layer - Python Implementation</strong></p>
<p>If you were to implement forward propagation yourself from scratch in Python, here's how you might go about it. This example uses a 1D array to represent vectors and parameters, and it continues with the coffee roasting model. The goal is to compute the output <code>a2</code> given an input feature vector <code>x</code>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="c1"># Define sigmoid function</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="c1"># Coffee Roasting Model - Parameters</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="n">w1_1</span><span class="p">,</span> <span class="n">b1_1</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="n">w1_2</span><span class="p">,</span> <span class="n">b1_2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="n">w1_3</span><span class="p">,</span> <span class="n">b1_3</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.1</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="n">w2_1</span><span class="p">,</span> <span class="n">b2_1</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="c1"># Input feature vector x</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">200</span><span class="p">,</span> <span class="mi">17</span><span class="p">])</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span class="c1"># Compute a1_1</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a><span class="n">z1_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1_1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1_1</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="n">a1_1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z1_1</span><span class="p">)</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="c1"># Compute a1_2</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a><span class="n">z1_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1_2</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1_2</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a><span class="n">a1_2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z1_2</span><span class="p">)</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a><span class="c1"># Compute a1_3</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a><span class="n">z1_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1_3</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1_3</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a><span class="n">a1_3</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z1_3</span><span class="p">)</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a><span class="c1"># Group a1 values into an array</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a><span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a1_1</span><span class="p">,</span> <span class="n">a1_2</span><span class="p">,</span> <span class="n">a1_3</span><span class="p">])</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a><span class="c1"># Compute a2_1 (output)</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a><span class="n">z2_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2_1</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2_1</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a><span class="n">a2_1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z2_1</span><span class="p">)</span>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a><span class="c1"># Output of the neural network</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a><span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a2_1</span><span class="p">])</span>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output of the neural network (a2):&quot;</span><span class="p">,</span> <span class="n">a2</span><span class="p">)</span>
</code></pre></div>
<p><strong>Key Points:</strong></p>
<ol>
<li><strong>Sigmoid Function:</strong></li>
<li>
<p>The sigmoid function (<code>sigmoid(z)</code>) is used to introduce non-linearity.</p>
</li>
<li>
<p><strong>Parameters:</strong></p>
</li>
<li>
<p>Parameters (<code>w</code> and <code>b</code>) for each neuron in the layer are predefined.</p>
</li>
<li>
<p><strong>Computing Activations (<code>a1_i</code>):</strong></p>
</li>
<li>
<p>For each neuron in the first layer, compute the weighted sum (<code>z1_i</code>) and apply the sigmoid function to get the activation (<code>a1_i</code>).</p>
</li>
<li>
<p><strong>Grouping Activations (<code>a1</code>):</strong></p>
</li>
<li>
<p>Group the individual activations (<code>a1_i</code>) into an array (<code>a1</code>), representing the output of the first layer.</p>
</li>
<li>
<p><strong>Computing Output (<code>a2_1</code>):</strong></p>
</li>
<li>
<p>For the second layer, compute the weighted sum (<code>z2_1</code>) using the activations from the first layer and apply the sigmoid function to get the final output (<code>a2_1</code>).</p>
</li>
<li>
<p><strong>Output of Neural Network (<code>a2</code>):</strong></p>
</li>
<li>The output of the neural network is represented by the array <code>a2</code>.</li>
</ol>
<p>In the next video, you'll explore how to simplify and generalize this code to implement forward propagation for a more complex neural network.</p>
<h3 id="general-implementation-of-forward-propagation">General implementation of forward propagation<a class="headerlink" href="#general-implementation-of-forward-propagation" title="Permanent link">&para;</a></h3>
<p><strong>General Implementation of Forward Propagation - Python</strong></p>
<p>In this video, you'll learn about a more general implementation of forward propagation in Python. The goal is to create a function, <code>dense</code>, that implements a single layer of a neural network. The function takes as input the activation from the previous layer (<code>a_prev</code>), the weights (<code>w</code>), and biases (<code>b</code>) for the neurons in the current layer and outputs the activations for the current layer.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="c1"># Define sigmoid function</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="c1"># General implementation of forward propagation for a single layer</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="k">def</span> <span class="nf">dense</span><span class="p">(</span><span class="n">a_prev</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    <span class="n">units</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Number of units in the current layer</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">units</span><span class="p">)</span>  <span class="c1"># Initialize activations array</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">units</span><span class="p">):</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>        <span class="n">w_j</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span>  <span class="c1"># Extract jth column of weights</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w_j</span><span class="p">,</span> <span class="n">a_prev</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>  <span class="c1"># Compute weighted sum</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>        <span class="n">a</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>  <span class="c1"># Apply sigmoid activation</span>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>    <span class="k">return</span> <span class="n">a</span>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a><span class="c1"># Example usage: Stringing together dense layers</span>
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a><span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>    <span class="c1"># Parameters for layer 1</span>
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a>    <span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a>    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>
<a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a>    <span class="c1"># Parameters for layer 2</span>
<a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a>    <span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a>    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a>
<a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a>    <span class="c1"># Compute activations for each layer</span>
<a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a>    <span class="n">a1</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
<a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a>    <span class="n">a2</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
<a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a>
<a id="__codelineno-1-33" name="__codelineno-1-33" href="#__codelineno-1-33"></a>    <span class="k">return</span> <span class="n">a2</span>
<a id="__codelineno-1-34" name="__codelineno-1-34" href="#__codelineno-1-34"></a>
<a id="__codelineno-1-35" name="__codelineno-1-35" href="#__codelineno-1-35"></a><span class="c1"># Example input features</span>
<a id="__codelineno-1-36" name="__codelineno-1-36" href="#__codelineno-1-36"></a><span class="n">x_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">200</span><span class="p">,</span> <span class="mi">17</span><span class="p">])</span>
<a id="__codelineno-1-37" name="__codelineno-1-37" href="#__codelineno-1-37"></a>
<a id="__codelineno-1-38" name="__codelineno-1-38" href="#__codelineno-1-38"></a><span class="c1"># Perform forward propagation</span>
<a id="__codelineno-1-39" name="__codelineno-1-39" href="#__codelineno-1-39"></a><span class="n">output</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span>
<a id="__codelineno-1-40" name="__codelineno-1-40" href="#__codelineno-1-40"></a>
<a id="__codelineno-1-41" name="__codelineno-1-41" href="#__codelineno-1-41"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output of the neural network:&quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</code></pre></div>
<p><strong>Key Points:</strong></p>
<ol>
<li><strong><code>dense</code> Function:</strong></li>
<li>
<p>The <code>dense</code> function takes the activations from the previous layer (<code>a_prev</code>), weights (<code>w</code>), and biases (<code>b</code>) as input and computes the activations for the current layer.</p>
</li>
<li>
<p><strong>Stringing Layers Together:</strong></p>
</li>
<li>
<p>The <code>forward_propagation</code> function demonstrates how to string together multiple layers sequentially by calling the <code>dense</code> function for each layer.</p>
</li>
<li>
<p><strong>Sigmoid Activation:</strong></p>
</li>
<li>
<p>The sigmoid activation function is applied to introduce non-linearity in each layer.</p>
</li>
<li>
<p><strong>Output of Neural Network:</strong></p>
</li>
<li>The final output of the neural network is computed by calling <code>forward_propagation</code> on the input features.</li>
</ol>
<p>Understanding this general implementation is crucial for gaining insights into how neural networks work under the hood. It also provides a foundation for debugging and troubleshooting when using machine learning libraries like TensorFlow.</p>
<h3 id="is-there-a-path-to-agi">Is there a path to AGI?<a class="headerlink" href="#is-there-a-path-to-agi" title="Permanent link">&para;</a></h3>
<p><strong>Is There a Path to AGI?</strong></p>
<p>In this video, the instructor discusses the concept of Artificial General Intelligence (AGI) and shares thoughts on whether there is a clear path to achieving human-level intelligence in AI systems.</p>
<p><strong>Key Points:</strong></p>
<ol>
<li><strong>Inspiring Dream of AGI:</strong></li>
<li>
<p>Since the early days of exploring neural networks, the dream of building an AI system as intelligent as a human has been inspiring. The instructor still holds onto this dream today.</p>
</li>
<li>
<p><strong>AGI vs. ANI:</strong></p>
</li>
<li>AI encompasses two different concepts - ANI (Artificial Narrow Intelligence) and AGI (Artificial General Intelligence).</li>
<li>ANI refers to AI systems designed for specific tasks, such as smart speakers, self-driving cars, or web search. It excels in one narrow task.</li>
<li>
<p>AGI represents the vision of building AI systems capable of performing any task a typical human can do.</p>
</li>
<li>
<p><strong>Progress in ANI vs. AGI:</strong></p>
</li>
<li>
<p>There has been tremendous progress in ANI over the last several years, leading to significant value creation. However, progress in ANI does not necessarily imply progress toward AGI.</p>
</li>
<li>
<p><strong>Simulating Neurons and Brain Complexity:</strong></p>
</li>
<li>
<p>The initial hope was that simulating a large number of neurons would lead to intelligent systems. However, the simplicity of artificial neural networks, such as logistic regression units, contrasts sharply with the complexity of biological neurons.</p>
</li>
<li>
<p><strong>Unknowns in Brain Functionality:</strong></p>
</li>
<li>
<p>Our understanding of how the human brain works is limited, and fundamental questions about how neurons map inputs to outputs remain unanswered.</p>
</li>
<li>
<p><strong>Challenges in Simulating the Brain:</strong></p>
</li>
<li>
<p>Simulating the human brain as a path to AGI is considered an incredibly difficult task due to the vast differences between artificial neural networks and the intricacies of the brain's functionality.</p>
</li>
<li>
<p><strong>One Learning Algorithm Hypothesis:</strong></p>
</li>
<li>
<p>Experiments on animals have shown that a single piece of biological brain tissue can adapt to a wide range of tasks, suggesting the existence of one or a small set of learning algorithms. Discovering and implementing these algorithms could lead to AGI.</p>
</li>
<li>
<p><strong>Adaptability of Brain Tissue:</strong></p>
</li>
<li>
<p>Experiments where brain tissue was rewired to process different types of sensory input (e.g., auditory cortex learning to see) indicate the adaptability of the brain to various tasks.</p>
</li>
<li>
<p><strong>Hope for Breakthroughs:</strong></p>
</li>
<li>
<p>Despite the challenges, there is hope for breakthroughs in AGI, especially considering the brain's adaptability and the possibility of identifying fundamental learning algorithms.</p>
</li>
<li>
<p><strong>Fascination with AGI Research:</strong></p>
<ul>
<li>Pursuing AGI remains one of the most fascinating science and engineering problems. The hope is that with hard work and dedication, progress may be made in understanding and approximating the algorithms responsible for human intelligence.</li>
</ul>
</li>
<li>
<p><strong>Short-Term Impact of Neural Networks:</strong></p>
<ul>
<li>While AGI remains a long-term goal, neural networks and machine learning are already powerful tools with applications in various domains. The short-term impact of these technologies is significant.</li>
</ul>
</li>
<li>
<p><strong>Optional Videos on Efficient Implementations:</strong></p>
<ul>
<li>The next set of optional videos will dive into efficient implementations of neural networks, focusing on vectorized implementations.</li>
</ul>
</li>
</ol>
<p>Congratulations on completing the required videos for this week. The optional videos will provide additional insights into optimizing neural network implementations.</p>
<h3 id="optional">Optional<a class="headerlink" href="#optional" title="Permanent link">&para;</a></h3>
<h3 id="how-neural-networks-are-implemented-efficiently">How neural networks are implemented efficiently<a class="headerlink" href="#how-neural-networks-are-implemented-efficiently" title="Permanent link">&para;</a></h3>
<p><strong>Efficient Implementation of Neural Networks</strong></p>
<p>In this video, the instructor discusses the efficiency of implementing neural networks through vectorization, utilizing matrix multiplications. Vectorized implementations play a crucial role in scaling up neural networks and achieving success in deep learning.</p>
<p><strong>Key Points:</strong></p>
<ol>
<li><strong>Vectorized Implementations:</strong></li>
<li>Deep learning researchers have been able to scale up neural networks by leveraging vectorized implementations.</li>
<li>
<p>Vectorized implementations are efficient and utilize matrix multiplications.</p>
</li>
<li>
<p><strong>Parallel Computing and Hardware:</strong></p>
</li>
<li>Parallel computing hardware, including GPUs and certain CPU functions, excels at performing large matrix multiplications.</li>
<li>
<p>This capability has contributed significantly to the success and scalability of deep learning.</p>
</li>
<li>
<p><strong>Previous Code for Forward Propagation:</strong></p>
</li>
<li>The left side shows the code for implementing forward propagation in a single layer, which was previously introduced.</li>
<li>
<p>Input <code>X</code>, weights <code>W</code> for three neurons, and parameters <code>B</code> are used to compute the output.</p>
</li>
<li>
<p><strong>Vectorized Implementation:</strong></p>
</li>
<li>The same computation can be implemented using vectorization.</li>
<li>Define <code>X</code> as a 2D array, <code>W</code> remains the same, and <code>B</code> is also a 1 by 3 2D array.</li>
<li>
<p>The for loop inside the previous implementation can be replaced with a concise vectorized code.</p>
</li>
<li>
<p><strong>NumPy <code>matmul</code> Function:</strong></p>
</li>
<li>NumPy's <code>matmul</code> function is utilized for matrix multiplication.</li>
<li><code>Z</code> is computed as the matrix product of <code>X</code> and <code>W</code>, followed by adding the matrix <code>B</code>.</li>
<li><code>A_out</code> is obtained by applying the sigmoid function element-wise to the matrix <code>Z</code>.</li>
<li>
<p>The result is a vectorized implementation of forward propagation through a dense layer in a neural network.</p>
</li>
<li>
<p><strong>Efficiency and 2D Arrays:</strong></p>
</li>
<li>All quantities involved, including <code>X</code>, <code>W</code>, <code>B</code>, <code>Z</code>, and <code>A_out</code>, are now represented as 2D arrays (matrices).</li>
<li>
<p>This vectorized approach is highly efficient for implementing one step of forward propagation.</p>
</li>
<li>
<p><strong>Understanding Matrix Multiplication:</strong></p>
</li>
<li>The video suggests optional videos on matrix multiplication for those unfamiliar with linear algebra concepts.</li>
<li>
<p>The next two optional videos cover matrix multiplication and are followed by a detailed explanation of how <code>matmul</code> achieves a vectorized implementation.</p>
</li>
<li>
<p><strong>Optional Videos on Matrix Multiplication:</strong></p>
</li>
<li>These videos provide insights into linear algebra concepts, including vectors, matrices, transposes, and matrix multiplications.</li>
<li>If already familiar with these concepts, viewers can skip to the last optional video for a detailed explanation of the vectorized implementation using <code>matmul</code>.</li>
</ol>
<p>Understanding the efficiency of vectorized implementations, particularly through matrix multiplications, is crucial for scaling up neural networks. The next videos offer optional content for a deeper understanding of these mathematical concepts.</p>
<h3 id="matrix-multiplication">Matrix multiplication<a class="headerlink" href="#matrix-multiplication" title="Permanent link">&para;</a></h3>
<p><strong>Matrix Multiplication</strong></p>
<p>In this video, the instructor introduces the concept of matrix multiplication, building up from the dot product of vectors. The process is explained using examples and visualizations to help understand the fundamentals of multiplying matrices and its applications in neural network implementations.</p>
<p><strong>Key Points:</strong></p>
<ol>
<li><strong>Dot Product of Vectors:</strong></li>
<li>The dot product between two vectors, such as [1, 2] and [3, 4], is computed by multiplying corresponding elements and summing the results.</li>
<li>
<p>The general formula for the dot product is <span class="arithmatex">\(z = a_1 \cdot w_1 + a_2 \cdot w_2 + \ldots\)</span>.</p>
</li>
<li>
<p><strong>Equivalent Form:</strong></p>
</li>
<li>The dot product <span class="arithmatex">\(z\)</span> between vectors <span class="arithmatex">\(a\)</span> and <span class="arithmatex">\(w\)</span> is equivalent to <span class="arithmatex">\(z = a^T \cdot w\)</span>, where <span class="arithmatex">\(a^T\)</span> is the transpose of vector <span class="arithmatex">\(a\)</span>.</li>
<li>
<p>Transposing a vector involves turning it from a column vector to a row vector, and vice versa.</p>
</li>
<li>
<p><strong>Vector-Matrix Multiplication:</strong></p>
</li>
<li>Vector-matrix multiplication involves multiplying a vector by a matrix.</li>
<li>
<p>If <span class="arithmatex">\(a\)</span> is a column vector, <span class="arithmatex">\(a^T\)</span> is a row vector, and <span class="arithmatex">\(w\)</span> is a matrix, then <span class="arithmatex">\(z = a^T \cdot w\)</span> involves multiplying each element of <span class="arithmatex">\(a^T\)</span> with the corresponding column of <span class="arithmatex">\(w\)</span> and summing the results.</p>
</li>
<li>
<p><strong>Matrix Transposition:</strong></p>
</li>
<li>To compute <span class="arithmatex">\(a^T\)</span> for a matrix <span class="arithmatex">\(a\)</span>, transpose each column of <span class="arithmatex">\(a\)</span> to form the rows of <span class="arithmatex">\(a^T\)</span>.</li>
<li>
<p>Matrix transposition involves swapping rows and columns.</p>
</li>
<li>
<p><strong>Matrix-Matrix Multiplication:</strong></p>
</li>
<li>To multiply two matrices, <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(W\)</span>, consider grouping the columns of <span class="arithmatex">\(A\)</span> and the rows of <span class="arithmatex">\(W\)</span> together.</li>
<li>
<p>The general approach is to take the dot product of each row of <span class="arithmatex">\(A\)</span> with each column of <span class="arithmatex">\(W\)</span> to form the resulting matrix.</p>
</li>
<li>
<p><strong>Example Matrix-Matrix Multiplication:</strong></p>
</li>
<li>Given matrices <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(W\)</span>, compute <span class="arithmatex">\(A^T \cdot W\)</span> by taking dot products of rows of <span class="arithmatex">\(A^T\)</span> with columns of <span class="arithmatex">\(W\)</span>.</li>
<li>
<p>Break down the process by considering individual rows/columns, perform dot products, and construct the resulting matrix.</p>
</li>
<li>
<p><strong>General Form of Matrix Multiplication:</strong></p>
</li>
<li>Matrix-matrix multiplication involves systematic dot products to form each element of the resulting matrix.</li>
<li>The general formula is <span class="arithmatex">\(Z_{ij} = \sum_k A_{ik} \cdot W_{kj}\)</span>, where <span class="arithmatex">\(Z\)</span> is the resulting matrix.</li>
</ol>
<p>Understanding matrix multiplication is essential for comprehending the vectorized implementation of neural network operations. The next video explores the general form of matrix multiplication, providing further clarity on this fundamental mathematical concept.</p>
<h3 id="matrix-multiplication-rules">Matrix multiplication rules<a class="headerlink" href="#matrix-multiplication-rules" title="Permanent link">&para;</a></h3>
<p><strong>Matrix Multiplication Rules</strong></p>
<p>In this video, the instructor delves into the general form of matrix multiplication, providing a detailed explanation of how to multiply two matrices together. The tutorial emphasizes the importance of understanding this process for the vectorized implementation of neural networks.</p>
<p><strong>Key Points:</strong></p>
<ol>
<li><strong>Matrix Dimensions:</strong></li>
<li>Matrix <span class="arithmatex">\(A\)</span> is a 2 by 3 matrix, meaning it has two rows and three columns.</li>
<li>
<p>Matrix <span class="arithmatex">\(W\)</span> is introduced with factors <span class="arithmatex">\(w_1, w_2, w_3, w_4\)</span>, stacked together.</p>
</li>
<li>
<p><strong>Matrix Transposition:</strong></p>
</li>
<li><span class="arithmatex">\(A^T\)</span> (transpose of <span class="arithmatex">\(A\)</span>) is obtained by laying the columns of <span class="arithmatex">\(A\)</span> on the side, forming rows <span class="arithmatex">\(A_1^T, A_2^T, A_3^T\)</span>.</li>
<li>
<p><span class="arithmatex">\(W\)</span> is represented as factors <span class="arithmatex">\(w_1, w_2, w_3, w_4\)</span>.</p>
</li>
<li>
<p><strong>Matrix Multiplication:</strong></p>
</li>
<li>To compute <span class="arithmatex">\(A^T \cdot W\)</span>, consider different shades of orange for columns of <span class="arithmatex">\(A\)</span> and shades of blue for columns of <span class="arithmatex">\(W\)</span>.</li>
<li>
<p>The resulting matrix <span class="arithmatex">\(Z\)</span> will be a 3 by 4 matrix (3 rows, 4 columns).</p>
</li>
<li>
<p><strong>Computing Elements of Z:</strong></p>
</li>
<li>To compute an element in <span class="arithmatex">\(Z\)</span>, consider the corresponding row of <span class="arithmatex">\(A^T\)</span> and column of <span class="arithmatex">\(W\)</span>.</li>
<li>
<p>Example: <span class="arithmatex">\(Z_{ij} = A_i^T \cdot W_j\)</span>, where <span class="arithmatex">\(i\)</span> is the row index and <span class="arithmatex">\(j\)</span> is the column index.</p>
</li>
<li>
<p><strong>Example Computations:</strong></p>
</li>
<li>Example 1: <span class="arithmatex">\(Z_{11} = A_1^T \cdot W_1 = (1 \cdot 3) + (2 \cdot 4) = 11\)</span>.</li>
<li>Example 2: <span class="arithmatex">\(Z_{32} = A_3^T \cdot W_2 = (0.1 \cdot 5) + (0.2 \cdot 6) = 1.7\)</span>.</li>
<li>
<p>Example 3: <span class="arithmatex">\(Z_{23} = A_2^T \cdot W_3 = (-1 \cdot 7) + (-2 \cdot 8) = -23\)</span>.</p>
</li>
<li>
<p><strong>Matrix Multiplication Requirements:</strong></p>
</li>
<li>In order to multiply two matrices, the number of columns in the first matrix must be equal to the number of rows in the second matrix.</li>
<li>
<p>In the example, <span class="arithmatex">\(A^T\)</span> is a 3 by 2 matrix, and <span class="arithmatex">\(W\)</span> is a 2 by 4 matrix, fulfilling the requirement.</p>
</li>
<li>
<p><strong>Output Matrix Z:</strong></p>
</li>
<li>The resulting matrix <span class="arithmatex">\(Z\)</span> will have the same number of rows as <span class="arithmatex">\(A^T\)</span> and the same number of columns as <span class="arithmatex">\(W\)</span>.</li>
</ol>
<p>Understanding matrix multiplication is crucial for the vectorized implementation of neural networks, as it enables efficient computation of forward and backward propagation steps. The next video is expected to apply these matrix multiplication concepts to the vectorized implementation of a neural network, highlighting the efficiency gained through this approach.</p>
<h3 id="matrix-multiplication-code">Matrix multiplication code<a class="headerlink" href="#matrix-multiplication-code" title="Permanent link">&para;</a></h3>
<p><strong>Matrix Multiplication Code: Vectorized Implementation of Neural Network</strong></p>
<p>In this segment, the instructor provides a code walkthrough for the vectorized implementation of a neural network's forward propagation. The code involves matrix multiplication using NumPy, which significantly enhances computational efficiency.</p>
<p><strong>Code Walkthrough:</strong></p>
<ol>
<li><strong>Matrix Definitions:</strong></li>
<li>Define matrix <span class="arithmatex">\(A\)</span> as the input feature values (1 by 2 matrix).</li>
<li>Initialize matrix <span class="arithmatex">\(W\)</span> with parameters <span class="arithmatex">\(w_1, w_2, w_3\)</span> (2 by 3 matrix).</li>
<li>
<p>Create matrix <span class="arithmatex">\(B\)</span> with bias terms <span class="arithmatex">\(b_1, b_2, b_3\)</span> (1 by 3 matrix).</p>
</li>
<li>
<p><strong>Matrix Transposition:</strong></p>
</li>
<li>
<p>Compute <span class="arithmatex">\(A^T\)</span> (transpose of <span class="arithmatex">\(A\)</span>) using either <span class="arithmatex">\(AT = A.T\)</span> or <span class="arithmatex">\(AT = np.transpose(A)\)</span>.</p>
</li>
<li>
<p><strong>Matrix Multiplication:</strong></p>
</li>
<li>Calculate <span class="arithmatex">\(Z\)</span> using <span class="arithmatex">\(Z = \text{np.matmul}(A^T, W) + B\)</span> or <span class="arithmatex">\(Z = A^T \cdot W + B\)</span>.</li>
<li>
<p>Alternative notation: <span class="arithmatex">\(Z = A^T @ W + B\)</span>.</p>
</li>
<li>
<p><strong>Activation Function (Sigmoid):</strong></p>
</li>
<li>Apply the sigmoid function element-wise to the matrix <span class="arithmatex">\(Z\)</span>.</li>
<li>
<p><span class="arithmatex">\(A_{\text{out}} = g(Z)\)</span>, where <span class="arithmatex">\(g\)</span> is the sigmoid function.</p>
</li>
<li>
<p><strong>Implementation in Code:</strong></p>
</li>
<li>Define <span class="arithmatex">\(A^T\)</span>, <span class="arithmatex">\(W\)</span>, and <span class="arithmatex">\(B\)</span> using NumPy arrays.</li>
<li>Implement forward propagation using <span class="arithmatex">\(Z = \text{np.matmul}(A^T, W) + B\)</span>.</li>
<li>Apply the sigmoid function to <span class="arithmatex">\(Z\)</span> to get the output <span class="arithmatex">\(A_{\text{out}}\)</span>.</li>
<li>
<p>Return <span class="arithmatex">\(A_{\text{out}}\)</span>.</p>
</li>
<li>
<p><strong>Implementation Considerations:</strong></p>
</li>
<li>In TensorFlow convention, <span class="arithmatex">\(A^T\)</span> is often represented as <span class="arithmatex">\(A_{\text{in}}\)</span>.</li>
<li>
<p>The layout of individual examples may be in rows rather than columns.</p>
</li>
<li>
<p><strong>Efficiency Gain:</strong></p>
</li>
<li>The vectorized implementation leverages efficient matrix multiplication capabilities.</li>
<li>
<p>Modern computers are optimized for matrix operations, making this approach computationally advantageous.</p>
</li>
<li>
<p><strong>Congratulations and Next Steps:</strong></p>
</li>
<li>Understanding this implementation allows for efficient inference and forward propagation in neural networks.</li>
<li>Encouragement to explore quizzes, practice labs, and optional labs for further reinforcement.</li>
</ol>
<p>This video concludes the optional series on matrix multiplication and the vectorized implementation of neural network forward propagation. The next week's content will focus on training a neural network.</p>
<h2 id="week-2">Week 2<a class="headerlink" href="#week-2" title="Permanent link">&para;</a></h2>
<h3 id="tensorflow-implementation">TensorFlow Implementation<a class="headerlink" href="#tensorflow-implementation" title="Permanent link">&para;</a></h3>
<p><strong>TensorFlow Implementation for Training a Neural Network</strong></p>
<p>In this segment, the instructor introduces the TensorFlow code for training a neural network. The code involves defining the model architecture, specifying the loss function, and then fitting the model to the training data. The goal is to provide a high-level overview of the process before delving into detailed explanations in subsequent videos.</p>
<p><strong>TensorFlow Code for Training a Neural Network:</strong></p>
<ol>
<li><strong>Model Definition:</strong></li>
<li>Sequentially define the layers of the neural network.</li>
<li>Example architecture: Input layer, hidden layers (25 units and 15 units), and output layer.</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="p">])</span>
</code></pre></div>
<ol>
<li><strong>Model Compilation:</strong></li>
<li>Specify the loss function to be used during training.</li>
<li>In this example, binary crossentropy is chosen.</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">)</span>
</code></pre></div>
<ol>
<li><strong>Training the Model:</strong></li>
<li>Use the <code>fit</code> function to train the model on the dataset (X, Y).</li>
<li>Specify the number of epochs (iterations) for training.</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>
<p>The <code>fit</code> function updates the model parameters based on the chosen loss function and the training data.</p>
</li>
<li>
<p><strong>Understanding the Steps:</strong></p>
</li>
<li>Specifying the model architecture (Step 1).</li>
<li>Compiling the model with a specific loss function (Step 2).</li>
<li>Training the model using the <code>fit</code> function (Step 3).</li>
</ul>
<p><strong>Important Note:</strong>
- Understanding the code is emphasized to avoid blindly using it without comprehension.
- The conceptual understanding helps in debugging and optimizing the learning algorithm.</p>
<p><strong>Next Steps:</strong>
- The subsequent videos will delve into detailed explanations of each step in the TensorFlow implementation.</p>
<p>This introduction sets the stage for exploring the intricacies of training a neural network using TensorFlow in the following videos.</p>
<h3 id="training-details">Training Details<a class="headerlink" href="#training-details" title="Permanent link">&para;</a></h3>
<p><strong>Training Details in TensorFlow</strong></p>
<p>In this segment, the instructor explains the details of training a neural network using TensorFlow. The process is broken down into three steps, drawing parallels with the training of logistic regression models.</p>
<ol>
<li><strong>Specify Output Computation:</strong></li>
<li>In logistic regression, the first step involves specifying how to compute the output given input features (x) and parameters (w, b).</li>
<li>This is similar for neural networks, where the architecture is defined using TensorFlow. The code snippet specifies the layers, units, and activation functions.</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="p">])</span>
</code></pre></div>
<ol>
<li><strong>Loss Function and Cost:</strong></li>
<li>For logistic regression, the loss function measures how well the model is performing on a single training example.</li>
<li>In neural networks, the binary cross-entropy loss function is commonly used for binary classification problems (e.g., recognizing handwritten digits as zero or one).</li>
<li>TensorFlow is then instructed to compile the model using this loss function, and the cost function is automatically derived.</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">)</span>
</code></pre></div>
<ol>
<li><strong>Minimize Cost with Gradient Descent:</strong></li>
<li>Gradient descent is used to minimize the cost function with respect to the parameters (weights and biases) of the neural network.</li>
<li>TensorFlow's <code>fit</code> function is employed to perform the training. It uses backpropagation to compute partial derivatives and updates the parameters iteratively.</li>
</ol>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>The number of epochs determines how many iterations of the learning algorithm (e.g., gradient descent) to run.</li>
</ul>
<p><strong>Choosing Loss Functions:</strong>
- For classification problems: Binary cross-entropy loss.
- For regression problems: Mean squared error loss.</p>
<p><strong>Library Evolution and Usage:</strong>
- The discussion includes a note on the evolution of technology, where mature libraries like TensorFlow are now widely used for neural network implementations.
- The instructor emphasizes the importance of understanding the workings of these libraries for effective debugging.</p>
<p><strong>Next Steps:</strong>
- The upcoming videos will explore further enhancements to neural networks, such as different activation functions, to improve their performance.</p>
<p>This explanation provides a comprehensive overview of the training process in TensorFlow, making the audience familiar with the key steps involved.</p>
<h3 id="alternatives-to-the-sigmoid-activation">Alternatives to the sigmoid activation<a class="headerlink" href="#alternatives-to-the-sigmoid-activation" title="Permanent link">&para;</a></h3>
<p><strong>Alternatives to Sigmoid Activation in Neural Networks</strong></p>
<p>In this segment, the instructor discusses alternatives to the sigmoid activation function in neural networks, introducing the Rectified Linear Unit (ReLU) and briefly mentioning the linear activation function.</p>
<ol>
<li><strong>Motivation for Alternatives:</strong></li>
<li>
<p>Sigmoid activation has been used extensively in previous examples as it was derived from logistic regression. However, other activation functions can make neural networks more powerful.</p>
</li>
<li>
<p><strong>Example: Degree of Awareness Prediction:</strong></p>
</li>
<li>
<p>Consider an example where awareness is not binary (aware or not aware) but can have various degrees. Instead of modeling awareness as a binary number, it could be a non-negative value, allowing for a range of awareness levels.</p>
</li>
<li>
<p><strong>ReLU Activation Function:</strong></p>
</li>
<li>The ReLU activation function (Rectified Linear Unit) is introduced as a common alternative.</li>
<li>It is defined as g(z) = max(0, z), resulting in a piecewise linear function that outputs z for positive z values and 0 for negative z values.</li>
<li>
<p>ReLU allows neurons to take on larger positive values, providing flexibility in representing different degrees of awareness.</p>
</li>
<li>
<p><strong>Linear Activation Function:</strong></p>
</li>
<li>The linear activation function, g(z) = z, is mentioned. If used, some might refer to it as having "no activation function" since it essentially outputs the input.</li>
<li>
<p>In this course, the term "linear activation function" is preferred.</p>
</li>
<li>
<p><strong>Commonly Used Activation Functions:</strong></p>
</li>
<li>Sigmoid Activation: g(z) = 1 / (1 + e^(-z))</li>
<li>ReLU (Rectified Linear Unit): g(z) = max(0, z)</li>
<li>
<p>Linear Activation: g(z) = z</p>
</li>
<li>
<p><strong>Choosing Activation Functions:</strong></p>
</li>
<li>The choice of activation function depends on the problem and desired properties.</li>
<li>Sigmoid is useful for binary classification problems.</li>
<li>ReLU is often a default choice for hidden layers due to its simplicity and effectiveness in practice.</li>
<li>
<p>Linear activation can be suitable for regression problems.</p>
</li>
<li>
<p><strong>Upcoming Topics:</strong></p>
</li>
<li>The next video will delve into how to choose between these activation functions for each neuron in a neural network.</li>
</ol>
<p>This explanation provides an introduction to alternative activation functions, highlighting their applications and considerations in the context of neural network design. The audience is prepared for the upcoming discussion on choosing activation functions for different scenarios.</p>
<h3 id="choosing-activation-functions">Choosing activation functions<a class="headerlink" href="#choosing-activation-functions" title="Permanent link">&para;</a></h3>
<p><strong>Choosing Activation Functions in Neural Networks</strong></p>
<p>In this segment, the instructor provides guidance on choosing activation functions for different neurons in a neural network, focusing on both the output layer and hidden layers.</p>
<ol>
<li><strong>Activation Function for the Output Layer:</strong></li>
<li>For binary classification problems (y is either 0 or 1), the sigmoid activation function is recommended. This allows the neural network to predict the probability that y is equal to 1, similar to logistic regression.</li>
<li>For regression problems where y can take positive or negative values, the linear activation function is suggested. This is suitable for predicting numerical values.</li>
<li>
<p>If y can only take non-negative values (e.g., predicting house prices), the ReLU activation function is recommended, as it outputs only non-negative values.</p>
</li>
<li>
<p><strong>Activation Function for Hidden Layers:</strong></p>
</li>
<li>The ReLU activation function is the most common choice for hidden layers in modern neural networks.</li>
<li>ReLU is preferred over sigmoid due to computational efficiency (faster to compute) and its property of going flat in only one part of the graph, which aids in faster learning during gradient descent.</li>
<li>
<p>The historical use of sigmoid in hidden layers has evolved, and ReLU has become the default choice for practitioners.</p>
</li>
<li>
<p><strong>Summary of Recommendations:</strong></p>
</li>
<li>Output Layer:<ul>
<li>Binary Classification: Sigmoid activation.</li>
<li>Regression (Positive/Negative Values): Linear activation.</li>
<li>Non-negative Values: ReLU activation.</li>
</ul>
</li>
<li>
<p>Hidden Layers: ReLU activation as the default choice.</p>
</li>
<li>
<p><strong>Implementation in TensorFlow:</strong></p>
</li>
<li>
<p>The instructor demonstrates how to implement activation functions in TensorFlow. For hidden layers, ReLU is used, and for the output layer, the activation function can be chosen based on the problem.</p>
</li>
<li>
<p><strong>Other Activation Functions:</strong></p>
</li>
<li>
<p>The instructor mentions that there are alternative activation functions, such as tanh, LeakyReLU, and swish, which are used in specific cases. However, the recommended choices in the video are considered sufficient for most applications.</p>
</li>
<li>
<p><strong>Importance of Activation Functions:</strong></p>
</li>
<li>The video concludes by raising the question of why activation functions are needed at all. The next video will explore the importance of activation functions in neural networks and why using linear activation or no activation does not work effectively.</li>
</ol>
<p>This segment provides practical guidance on selecting activation functions based on the nature of the problem for both the output and hidden layers of a neural network. It also emphasizes the prevalence of ReLU in modern neural network architectures.</p>
<h3 id="why-do-we-need-activation-functions">Why do we need activation functions?<a class="headerlink" href="#why-do-we-need-activation-functions" title="Permanent link">&para;</a></h3>
<p><strong>Importance of Activation Functions in Neural Networks</strong></p>
<p>In this segment, the instructor explains why neural networks need activation functions and why using a linear activation function in every neuron would defeat the purpose of using a neural network.</p>
<ol>
<li><strong>Linear Activation Function Leads to Linear Regression:</strong></li>
<li>If a linear activation function (g(z) = z) is used for all nodes in a neural network, the network essentially becomes a linear regression model.</li>
<li>
<p>The instructor uses a simple example with one input node, one hidden unit, and one output unit to illustrate that the output of the neural network becomes a linear function of the input.</p>
</li>
<li>
<p><strong>Mathematical Representation:</strong></p>
</li>
<li>The output of the neural network, a2, can be expressed as a linear function of the input x: a2 = wx + b, where w and b are learned parameters.</li>
<li>
<p>This result holds true for a neural network with multiple layers if linear activation functions are used throughout.</p>
</li>
<li>
<p><strong>Equivalent to Linear Regression or Logistic Regression:</strong></p>
</li>
<li>If linear activation functions are used in all layers, the neural network is equivalent to linear regression.</li>
<li>
<p>If a logistic activation function is used in the output layer, the neural network is equivalent to logistic regression.</p>
</li>
<li>
<p><strong>Conclusion:</strong></p>
</li>
<li>Using linear activation functions in hidden layers limits the expressive power of the neural network, making it no more complex than linear regression.</li>
<li>
<p>The instructor recommends against using the linear activation function in hidden layers and suggests using the ReLU activation function as a common and effective alternative.</p>
</li>
<li>
<p><strong>Generalization for Classification Problems:</strong></p>
</li>
<li>The instructor hints at a generalization for classification problems where y can take on multiple categorical values (more than two). This will be covered in the next video.</li>
</ol>
<p>This segment highlights the fundamental role of activation functions in introducing non-linearity to neural networks, enabling them to learn complex patterns beyond the capabilities of linear models. The recommendation is to avoid linear activation functions in hidden layers to harness the full potential of neural networks.</p>
<h3 id="multiclass">Multiclass<a class="headerlink" href="#multiclass" title="Permanent link">&para;</a></h3>
<p><strong>Multiclass Classification</strong></p>
<p>In this segment, the instructor introduces the concept of multiclass classification, which involves problems where there are more than two possible output labels. Unlike binary classification, where the output is either 0 or 1, multiclass classification problems deal with multiple discrete categories. The instructor provides examples to illustrate this concept.</p>
<ol>
<li><strong>Examples of Multiclass Classification Problems:</strong></li>
<li>Handwritten Digit Recognition: Instead of distinguishing between just 0 and 1, you might have 10 possible digits (0 to 9).</li>
<li>Medical Diagnosis: Classifying patients into multiple disease categories (e.g., three or five different diseases).</li>
<li>
<p>Visual Defect Inspection: Identifying defects in manufactured parts, where there are multiple types of defects (e.g., scratch, discoloration, chip).</p>
</li>
<li>
<p><strong>Data Representation for Multiclass Classification:</strong></p>
</li>
<li>For binary classification, the dataset might have features x1 and x2, and logistic regression estimates the probability of y being 1.</li>
<li>
<p>In multiclass classification, the dataset involves multiple classes, and the goal is to estimate the probability of y being each class (e.g., 1, 2, 3, or 4).</p>
</li>
<li>
<p><strong>Decision Boundary for Multiclass Classification:</strong></p>
</li>
<li>The algorithm for multiclass classification can learn a decision boundary that separates the feature space into multiple categories.</li>
<li>
<p>The decision boundary is extended to accommodate more than two classes.</p>
</li>
<li>
<p><strong>Introduction to Softmax Regression:</strong></p>
</li>
<li>The next video will cover the softmax regression algorithm, which is a generalization of logistic regression for multiclass classification.</li>
<li>
<p>Softmax regression allows the estimation of probabilities for each class.</p>
</li>
<li>
<p><strong>Neural Networks for Multiclass Classification:</strong></p>
</li>
<li>Following softmax regression, the instructor mentions that the algorithm will be incorporated into a neural network to enable training for multiclass classification problems.</li>
</ol>
<p>This segment sets the stage for understanding and tackling multiclass classification problems, expanding the scope beyond binary classification. The upcoming video will delve into the softmax regression algorithm, offering a solution for handling multiple classes in a systematic way.</p>
<h3 id="softmax">Softmax<a class="headerlink" href="#softmax" title="Permanent link">&para;</a></h3>
<p><strong>Softmax Regression</strong></p>
<p>In this segment, the instructor introduces softmax regression, a generalization of logistic regression to handle multiclass classification problems. The softmax regression algorithm estimates probabilities for each possible output class and can be used in scenarios where there are more than two discrete categories.</p>
<ol>
<li><strong>Recap of Logistic Regression:</strong></li>
<li>Logistic regression is a binary classification algorithm where the output, y, can take on two values (0 or 1).</li>
<li>
<p>It computes the output by first calculating z as the weighted sum of input features, followed by applying a sigmoid function (g of z) to estimate the probability of y being 1.</p>
</li>
<li>
<p><strong>Extension to Multiclass Classification:</strong></p>
</li>
<li>Logistic regression can be seen as computing two numbers: a_1 (probability of y = 1) and a_2 (probability of y = 0).</li>
<li>
<p>Softmax regression generalizes this idea to multiple classes, allowing for more than two possible output values.</p>
</li>
<li>
<p><strong>Softmax Regression for Four Possible Outputs:</strong></p>
</li>
<li>When there are four possible outputs (y can be 1, 2, 3, or 4), softmax regression computes z for each class.</li>
<li>
<p>The formula for softmax regression is introduced, where a_j is calculated using the exponential function.</p>
</li>
<li>
<p><strong>General Case for Softmax Regression:</strong></p>
</li>
<li>For n possible outputs (y can be 1, 2, ..., n), softmax regression computes z_j for each class.</li>
<li>
<p>The formula for a_j is given by e^(z_j) divided by the sum of exponentials over all classes.</p>
</li>
<li>
<p><strong>Interpretation of Output Probabilities:</strong></p>
</li>
<li>a_j is interpreted as the algorithm's estimate of the chance that y is equal to j given the input features x.</li>
<li>
<p>The probabilities a_j always add up to 1.</p>
</li>
<li>
<p><strong>Quiz on Probability Calculation:</strong></p>
</li>
<li>
<p>The audience is presented with a quiz to calculate a_4 given probabilities a_1, a_2, and a_3. The correct answer is determined by subtracting the sum from 1.</p>
</li>
<li>
<p><strong>Softmax Regression as a Generalization of Logistic Regression:</strong></p>
</li>
<li>
<p>Softmax regression generalizes logistic regression. When n equals 2, softmax regression reduces to logistic regression with slightly different parameters.</p>
</li>
<li>
<p><strong>Cost Function for Softmax Regression:</strong></p>
</li>
<li>The loss function for softmax regression is introduced, where the loss for each class is defined as the negative log of the predicted probability.</li>
<li>
<p>The cost function is the average loss over the entire training set.</p>
</li>
<li>
<p><strong>Visualizing Loss Function:</strong></p>
</li>
<li>
<p>Negative log(a_j) is visualized as a curve, and the loss incentivizes the algorithm to assign higher probabilities to the correct class.</p>
</li>
<li>
<p><strong>Softmax Regression for Multiclass Classification:</strong></p>
<ul>
<li>Softmax regression provides a way to model and train for multiclass classification problems.</li>
</ul>
</li>
</ol>
<h3 id="neural-network-with-softmax-output">Neural Network with Softmax output<a class="headerlink" href="#neural-network-with-softmax-output" title="Permanent link">&para;</a></h3>
<p><strong>Neural Network with Softmax Output</strong></p>
<p>In this segment, the instructor explains how to modify a neural network for multiclass classification by incorporating a Softmax regression model into the output layer. The Softmax regression model is used to estimate probabilities for each class, allowing the neural network to handle scenarios with more than two discrete categories.</p>
<ol>
<li><strong>Neural Network Architecture for Multiclass Classification:</strong></li>
<li>For binary classification, a neural network with two output units and a sigmoid activation function is used.</li>
<li>For multiclass classification (e.g., 10 classes for handwritten digits), the output layer is modified to have 10 output units, each associated with a possible class.</li>
<li>
<p>The new output layer is a Softmax output layer.</p>
</li>
<li>
<p><strong>Forward Propagation in the Neural Network:</strong></p>
</li>
<li>Given an input X, the activations A1 and A2 for the first and second hidden layers are computed as before.</li>
<li>The activations A3 for the Softmax output layer are computed using the Softmax regression model.</li>
<li>
<p>Z1 through Z10 are calculated, and A1 through A10 are obtained by applying the Softmax activation function.</p>
</li>
<li>
<p><strong>Softmax Activation Function:</strong></p>
</li>
<li>The Softmax activation function is different from previous activation functions.</li>
<li>
<p>Each activation value (A1 to A10) depends on all values of Z1 to Z10 simultaneously.</p>
</li>
<li>
<p><strong>TensorFlow Implementation:</strong></p>
</li>
<li>TensorFlow code is provided to implement the neural network with Softmax output.</li>
<li>Three layers are sequentially strung together: 25 units with ReLU activation, 15 units with ReLU activation, and 10 units with Softmax activation.</li>
<li>
<p>The cost function used is <code>SparseCategoricalCrossentropy</code> to handle multiclass classification.</p>
</li>
<li>
<p><strong>Note on TensorFlow Code:</strong></p>
</li>
<li>The instructor mentions that while the provided code works, there's a better version that will be introduced in a later video for improved accuracy.</li>
<li>
<p>The provided code is not recommended for use due to this upcoming improvement.</p>
</li>
<li>
<p><strong>Next Steps:</strong></p>
</li>
<li>The audience is informed that the next video will introduce the recommended version of the TensorFlow code for training a Softmax neural network.</li>
</ol>
<p>This segment provides a clear understanding of how to modify a neural network for multiclass classification using a Softmax output layer. The focus on architecture, forward propagation, and TensorFlow implementation lays the groundwork for more advanced multiclass classification tasks. The anticipation of a better version of the code adds an element of curiosity for the audience.</p>
<h3 id="improved-implementation-of-softmax">Improved implementation of softmax<a class="headerlink" href="#improved-implementation-of-softmax" title="Permanent link">&para;</a></h3>
<p><strong>Improved Implementation of Softmax</strong></p>
<p>In this segment, the instructor discusses an improved implementation of softmax that reduces numerical round-off errors, leading to more accurate computations within TensorFlow. The illustration uses logistic regression to provide insight into numerical stability and how TensorFlow can rearrange terms to enhance accuracy.</p>
<ol>
<li><strong>Illustration with Logistic Regression:</strong></li>
<li>Two options are presented to compute the same quantity (x) in a computer.</li>
<li>Option 1: Set x = 2/10,000.</li>
<li>Option 2: Set x = 1 + 1/10,000 - (1 - 1/10,000).</li>
<li>
<p>Numerical round-off errors are evident, and it's demonstrated that both options result in the same value.</p>
</li>
<li>
<p><strong>Numerical Stability in TensorFlow:</strong></p>
</li>
<li>The importance of numerical stability is highlighted, especially when dealing with very small or very large numbers.</li>
<li>
<p>TensorFlow can rearrange terms for more accurate computations if given flexibility.</p>
</li>
<li>
<p><strong>Implementation for Logistic Regression:</strong></p>
</li>
<li>The instructor shows how rearranging terms in the loss function can lead to a more numerically accurate computation.</li>
<li>Instead of explicitly computing the activation (a), the loss function is specified directly, allowing TensorFlow to optimize the computation.</li>
<li>
<p>The code example is provided to demonstrate this improvement.</p>
</li>
<li>
<p><strong>Extension to Softmax Regression:</strong></p>
</li>
<li>The same idea is applied to softmax regression for multiclass classification.</li>
<li>The code is modified to specify the loss function directly, allowing TensorFlow to rearrange terms for improved accuracy.</li>
<li>
<p>The recommended version uses a linear activation function in the output layer and <code>from_logits=True</code> in the loss function.</p>
</li>
<li>
<p><strong>Numerical Stability in Code:</strong></p>
</li>
<li>While the recommended version is more numerically accurate, it may be less legible.</li>
<li>
<p>The trade-off between numerical accuracy and code readability is acknowledged.</p>
</li>
<li>
<p><strong>Final Implementation Details:</strong></p>
</li>
<li>The linear activation function is used in the output layer instead of softmax.</li>
<li>The <code>from_logits=True</code> parameter is crucial for numerical stability in TensorFlow.</li>
<li>
<p>The instructor emphasizes that the recommended code is conceptually equivalent to the original version but is more numerically accurate.</p>
</li>
<li>
<p><strong>Wrapping up Multiclass Classification:</strong></p>
</li>
<li>
<p>The audience is informed that the discussion has covered multiclass classification with a softmax output layer in a numerically stable way.</p>
</li>
<li>
<p><strong>Next Topic: Multi-label Classification:</strong></p>
</li>
<li>The instructor hints at the upcoming topic of multi-label classification, introducing a new type of classification problem.</li>
</ol>
<p>This segment provides valuable insights into the nuances of numerical stability in softmax regression and how to achieve more accurate computations in TensorFlow. The emphasis on implementation details and the trade-off between accuracy and code readability enhances the audience's understanding of the topic. The mention of multi-label classification sets the stage for the next segment.</p>
<h3 id="classification-with-multiple-outputs-optional">Classification with multiple outputs (Optional)<a class="headerlink" href="#classification-with-multiple-outputs-optional" title="Permanent link">&para;</a></h3>
<p><strong>Classification with Multiple Outputs</strong></p>
<p>In this segment, the instructor introduces the concept of multi-label classification, which is distinct from multi-class classification. In multi-label classification, each input can be associated with multiple labels simultaneously. The example used is that of a self-driving car system identifying whether there are cars, buses, and pedestrians in an image, with each label having a binary output (presence or absence).</p>
<p>Key points covered:</p>
<ol>
<li><strong>Multi-Label Classification:</strong></li>
<li>In multi-label classification, an input can be associated with multiple labels.</li>
<li>
<p>The example involves identifying the presence of cars, buses, and pedestrians in an image, resulting in a vector of three binary values.</p>
</li>
<li>
<p><strong>Neural Network Approach:</strong></p>
</li>
<li>One approach is to treat each label as a separate binary classification problem.</li>
<li>
<p>Alternatively, a single neural network can be trained to simultaneously detect all labels using a vectorized output.</p>
</li>
<li>
<p><strong>Neural Network Architecture:</strong></p>
</li>
<li>The architecture involves an input layer (X), hidden layers (a^1, a^2), and an output layer (a^3) with three nodes.</li>
<li>
<p>Sigmoid activation functions are used for each output node, providing binary outputs for car, bus, and pedestrian detection.</p>
</li>
<li>
<p><strong>Multi-Class vs. Multi-Label:</strong></p>
</li>
<li>Multi-class classification involves predicting a single label from multiple classes (e.g., digit classification).</li>
<li>
<p>Multi-label classification deals with associating multiple labels with an input, often binary decisions (presence or absence).</p>
</li>
<li>
<p><strong>Clarification on Multi-Label Classification:</strong></p>
</li>
<li>The instructor emphasizes the distinction between multi-class and multi-label classification.</li>
<li>
<p>The choice between them depends on the specific requirements of the application.</p>
</li>
<li>
<p><strong>Conclusion of Multi-Class and Multi-Label Classification:</strong></p>
</li>
<li>
<p>The section on multi-class and multi-label classification concludes, clarifying the definitions and use cases for each.</p>
</li>
<li>
<p><strong>Next Topic: Advanced Neural Network Concepts:</strong></p>
</li>
<li>The upcoming videos will explore advanced neural network concepts, including an optimization algorithm better than gradient descent.</li>
<li>The promise is that this algorithm will enable faster learning in neural networks.</li>
</ol>
<p>This segment provides a clear understanding of multi-label classification, its application in scenarios like object detection, and the architectural considerations when implementing neural networks for such tasks. The anticipation of advanced concepts adds excitement and hints at further exploration of optimization techniques.</p>
<h3 id="advanced-optimization">Advanced Optimization<a class="headerlink" href="#advanced-optimization" title="Permanent link">&para;</a></h3>
<p><strong>Advanced Optimization: Adam Algorithm</strong></p>
<p>In this video, the instructor introduces an optimization algorithm called the Adam algorithm, which is an enhancement over gradient descent. Adam stands for Adaptive Moment Estimation, and it's designed to automatically adjust the learning rate for each parameter of a neural network, potentially leading to faster convergence.</p>
<p>Key points covered:</p>
<ol>
<li><strong>Gradient Descent Recap:</strong></li>
<li>Reminder of the gradient descent update rule: <span class="arithmatex">\(w_j := w_j - \alpha \frac{\partial J}{\partial w_j}\)</span>.</li>
<li>
<p>Illustration of how gradient descent may take small steps, and the learning rate (<span class="arithmatex">\(\alpha\)</span>) influences the size of these steps.</p>
</li>
<li>
<p><strong>Need for Adaptive Learning Rates:</strong></p>
</li>
<li>If the learning rate is too small, convergence can be slow.</li>
<li>If the learning rate is too large, oscillations or divergence may occur.</li>
<li>
<p>Desire for an algorithm that can automatically adjust the learning rate.</p>
</li>
<li>
<p><strong>Introduction to Adam Algorithm:</strong></p>
</li>
<li>Adam is an optimization algorithm that adapts the learning rate based on the behavior of each parameter.</li>
<li>
<p>It uses different learning rates for each parameter rather than a single global learning rate.</p>
</li>
<li>
<p><strong>Adaptive Learning Rates:</strong></p>
</li>
<li>If a parameter consistently moves in the same direction, increase its learning rate.</li>
<li>If a parameter oscillates, decrease its learning rate.</li>
<li>
<p>The goal is to have faster convergence for stable directions and slower convergence for oscillating directions.</p>
</li>
<li>
<p><strong>Implementation in TensorFlow:</strong></p>
</li>
<li>In TensorFlow, using Adam involves specifying the optimizer as <code>tf.keras.optimizers.Adam</code>.</li>
<li>
<p>The initial learning rate (<span class="arithmatex">\(\alpha\)</span>) is set as a parameter, and practitioners may experiment with different values.</p>
</li>
<li>
<p><strong>Robustness of Adam:</strong></p>
</li>
<li>Adam is more robust to the choice of the initial learning rate compared to traditional gradient descent.</li>
<li>
<p>It adapts the learning rates during training, reducing the need for fine-tuning.</p>
</li>
<li>
<p><strong>Practical Usage:</strong></p>
</li>
<li>Adam has become a widely used optimization algorithm in training neural networks.</li>
<li>
<p>Practitioners often choose Adam over traditional gradient descent due to its adaptability and efficiency.</p>
</li>
<li>
<p><strong>Next Steps: Advanced Concepts:</strong></p>
</li>
<li>The upcoming videos will cover more advanced concepts in neural networks, starting with alternative layer types.</li>
</ol>
<p>This video provides a clear introduction to the Adam optimization algorithm, explaining its adaptive learning rate mechanism and how it contributes to faster convergence in neural network training. The practical implementation in TensorFlow is also demonstrated, making it accessible for practitioners looking to enhance their training algorithms.</p>
<h3 id="additional-layer-types">Additional Layer Types<a class="headerlink" href="#additional-layer-types" title="Permanent link">&para;</a></h3>
<p><strong>Summary: Additional Layer Types - Convolutional Layers</strong></p>
<p>In this video, the instructor introduces convolutional layers, a type of neural network layer that differs from the dense layers discussed previously. Key points covered include:</p>
<ol>
<li><strong>Introduction to Dense Layers:</strong></li>
<li>
<p>Recap: Dense layers connect every neuron to all activations from the previous layer.</p>
</li>
<li>
<p><strong>Motivation for Convolutional Layers:</strong></p>
</li>
<li>Convolutional layers provide an alternative where neurons focus on specific regions rather than the entire input.</li>
<li>
<p>An example is given with an image of a handwritten digit (nine) to illustrate the concept.</p>
</li>
<li>
<p><strong>Benefits of Convolutional Layers:</strong></p>
</li>
<li>Speeds up computation by focusing on specific regions.</li>
<li>
<p>Requires less training data and is less prone to overfitting.</p>
</li>
<li>
<p><strong>Explanation of Convolutional Layers:</strong></p>
</li>
<li>Neurons in a convolutional layer look at limited windows or regions of the input.</li>
<li>
<p>Each neuron processes a subset of input values, creating a hierarchical representation.</p>
</li>
<li>
<p><strong>Example with EKG Signals:</strong></p>
</li>
<li>Illustration of a convolutional neural network for classifying EKG signals.</li>
<li>Neurons in the first hidden layer focus on different windows of the input signal.</li>
<li>
<p>Subsequent hidden layers may also be convolutional.</p>
</li>
<li>
<p><strong>Architecture Choices in Convolutional Layers:</strong></p>
</li>
<li>Parameters such as the size of the input window and the number of neurons are design choices.</li>
<li>
<p>Effective architectural choices can lead to more powerful neural networks.</p>
</li>
<li>
<p><strong>Application to Modern Architectures:</strong></p>
</li>
<li>Mention of cutting-edge architectures like transformers, LSTMs, and attention models.</li>
<li>
<p>Researchers often explore inventing new layer types to enhance neural network capabilities.</p>
</li>
<li>
<p><strong>Notable Points:</strong></p>
</li>
<li>Convolutional layers are not required for the course's homework, but the knowledge provides additional intuition.</li>
<li>
<p>Neural networks can incorporate various layer types as building blocks for complexity and power.</p>
</li>
<li>
<p><strong>Conclusion:</strong></p>
</li>
<li>The video concludes the required content for the week, expressing appreciation for the learners' engagement.</li>
<li>Next week's content will cover practical advice for building machine learning systems.</li>
</ol>
<p>The video provides a fundamental understanding of convolutional layers and their role in neural network architectures. While not essential for the current coursework, the knowledge offers insights into diverse layer types and their applications.</p>
<h3 id="what-is-a-derivative-optional">What is a derivative? (Optional)<a class="headerlink" href="#what-is-a-derivative-optional" title="Permanent link">&para;</a></h3>
<p><strong>Summary: What is a Derivative? (Optional)</strong></p>
<p>In this optional video, the instructor introduces the concept of derivatives, emphasizing their importance in the backpropagation algorithm for training neural networks. Key points covered include:</p>
<ol>
<li><strong>Backpropagation and Derivatives:</strong></li>
<li>Backpropagation involves computing derivatives of the cost function with respect to the parameters of the neural network.</li>
<li>
<p>Derivatives guide the gradient descent or Adam optimization algorithms during training.</p>
</li>
<li>
<p><strong>Simplified Cost Function:</strong></p>
</li>
<li>The instructor uses a simplified cost function, <span class="arithmatex">\( J(w) = w^2 \)</span>, for illustration.</li>
<li>
<p>When <span class="arithmatex">\( w = 3 \)</span>, <span class="arithmatex">\( J(w) = 9 \)</span>.</p>
</li>
<li>
<p><strong>Understanding Derivatives:</strong></p>
</li>
<li>The instructor introduces the concept of derivatives by increasing <span class="arithmatex">\( w \)</span> by a tiny amount (<span class="arithmatex">\( \varepsilon \)</span>) and observing how <span class="arithmatex">\( J(w) \)</span> changes.</li>
<li>
<p>The ratio <span class="arithmatex">\( \frac{\Delta J(w)}{\Delta w} \)</span> is approximately constant.</p>
</li>
<li>
<p><strong>Informal Definition of Derivative:</strong></p>
</li>
<li>
<p>If <span class="arithmatex">\( w \)</span> increases by a tiny amount <span class="arithmatex">\( \varepsilon \)</span> and <span class="arithmatex">\( J(w) \)</span> increases by <span class="arithmatex">\( k \times \varepsilon \)</span>, the derivative <span class="arithmatex">\( \frac{dJ(w)}{dw} = k \)</span>.</p>
</li>
<li>
<p><strong>Examples and Calculations:</strong></p>
</li>
<li>Examples are provided with different values of <span class="arithmatex">\( \varepsilon \)</span> and <span class="arithmatex">\( w \)</span>.</li>
<li>
<p>Derivatives for functions <span class="arithmatex">\( w^3 \)</span>, <span class="arithmatex">\( w \)</span>, and <span class="arithmatex">\( \frac{1}{w} \)</span> are calculated.</p>
</li>
<li>
<p><strong>Using SymPy for Derivatives:</strong></p>
</li>
<li>SymPy, a Python library for symbolic mathematics, is introduced for calculating derivatives.</li>
<li>
<p>Derivatives for different functions are computed and verified.</p>
</li>
<li>
<p><strong>Notation for Derivatives:</strong></p>
</li>
<li>Traditional calculus notation for derivatives, such as <span class="arithmatex">\( \frac{dJ(w)}{dw} \)</span>, is briefly discussed.</li>
<li>
<p>The instructor mentions a preference for a simpler notation used throughout the course.</p>
</li>
<li>
<p><strong>Conclusion:</strong></p>
</li>
<li>Derivatives represent the rate of change of a function with respect to its parameters.</li>
<li>Derivatives are crucial in optimizing neural network parameters during training.</li>
</ol>
<p>This video provides an optional exploration of derivatives and their significance in the context of neural network training. It covers basic concepts and introduces SymPy for symbolic computation of derivatives. The instructor encourages learners to pause the video and perform calculations to reinforce understanding.</p>
<h3 id="computation-graph-optional">Computation graph (Optional)<a class="headerlink" href="#computation-graph-optional" title="Permanent link">&para;</a></h3>
<h3 id="larger-neural-network-example-optional">Larger neural network example (Optional)<a class="headerlink" href="#larger-neural-network-example-optional" title="Permanent link">&para;</a></h3>
<h2 id="week-3">Week 3<a class="headerlink" href="#week-3" title="Permanent link">&para;</a></h2>
<h3 id="deciding-what-to-try-next">Deciding what to try next<a class="headerlink" href="#deciding-what-to-try-next" title="Permanent link">&para;</a></h3>
<p><strong>Week Overview:</strong>
This week delves into the nuances of effective decision-making in machine learning projects, leveraging a repertoire of algorithms like linear and logistic regression, neural networks, and anticipating insights from upcoming topics on decision trees.</p>
<p><strong>Introduction:</strong>
The significance of adeptly using machine learning tools and the pivotal role of strategic decision-making in project development set the stage for this week's exploration.</p>
<p><strong>Example Scenario: Regularized Linear Regression:</strong>
Embarking on the implementation of regularized linear regression for housing price prediction reveals an initial challenge—large prediction errors. The scenario prompts an exploration of potential strategies to enhance model performance.</p>
<p><strong>Decision-Making Strategies:</strong>
1. <strong>Increase Training Examples:</strong>
   - <strong>Pros:</strong> Potential for enhanced generalization.
   - <strong>Cons:</strong> Diminishing returns, resource-intensive.</p>
<ol>
<li><strong>Feature Adjustment:</strong></li>
<li>a. <strong>Reduce Features:</strong><ul>
<li><strong>Pros:</strong> Simplification of the model.</li>
<li><strong>Cons:</strong> Loss of crucial information.</li>
</ul>
</li>
<li>
<p>b. <strong>Add Features:</strong></p>
<ul>
<li><strong>Pros:</strong> Opportunity for improved prediction accuracy.</li>
<li><strong>Cons:</strong> Augmented model complexity.</li>
</ul>
</li>
<li>
<p><strong>Polynomial Features:</strong></p>
</li>
<li>
<p>a. <strong>Add Polynomial Features:</strong></p>
<ul>
<li><strong>Pros:</strong> Capability to capture nonlinear relationships.</li>
<li><strong>Cons:</strong> Elevated model complexity, risk of overfitting.</li>
</ul>
</li>
<li>
<p><strong>Regularization Parameter (Lambda) Adjustment:</strong></p>
</li>
<li>a. <strong>Decrease Lambda:</strong><ul>
<li><strong>Pros:</strong> Mitigation of regularization, increased model flexibility.</li>
<li><strong>Cons:</strong> Proneness to overfitting.</li>
</ul>
</li>
<li>b. <strong>Increase Lambda:</strong><ul>
<li><strong>Pros:</strong> Intensified regularization, potential feature simplification.</li>
<li><strong>Cons:</strong> Potential loss of significant features.</li>
</ul>
</li>
</ol>
<p><strong>Key Principles:</strong>
- <strong>Data Collection:</strong>
  - Evaluating the utility of collecting more data through diagnostics.
  - Emphasizing the need to avoid prolonged data collection without evident benefits.</p>
<ul>
<li><strong>Diagnostics:</strong></li>
<li><strong>Definition:</strong> Rigorous tests offering profound insights into algorithmic performance.</li>
<li><strong>Purpose:</strong> Identification of issues, strategic guidance for enhancements, and prevention of superfluous efforts.</li>
<li><strong>Implementation Time:</strong> Acknowledging the time investment in diagnostic setup for its eventual efficiency gains.</li>
</ul>
<p><strong>Performance Evaluation:</strong>
- <strong>Importance:</strong> Paramount for assessing the effectiveness of the machine learning algorithm.
- <strong>Diagnostics Focus:</strong>
  - <strong>Training Set Performance:</strong> Scrutinizing the algorithm's learning dynamics.
  - <strong>Cross-Validation Set Performance:</strong> Evaluating the model's generalization capacity.
  - <strong>Test Set Performance:</strong> Validating the algorithm's robustness on entirely new data.</p>
<h3 id="evaluating-a-model">Evaluating a model<a class="headerlink" href="#evaluating-a-model" title="Permanent link">&para;</a></h3>
<p><strong>Model Evaluation:</strong></p>
<p><strong>Scenario: Predicting Housing Prices</strong>
- <strong>Model Complexity Issue:</strong>
  - Polynomial model (4<sup>th</sup> order) fitting training data remarkably well.
  - Concerns about its generalization to new, unseen data.
  - Complexity challenges in visualizing higher-dimensional models.</p>
<p><strong>Evaluation Technique: Train-Test Split</strong>
- <strong>Data Division:</strong>
  - 70% training set, 30% test set.
  - Training set: <span class="arithmatex">\(x_{1}, y_{1}, ..., x_{m_{\text{train}}}, y_{m_{\text{train}}}\)</span>
  - Test set: <span class="arithmatex">\(x_{1_{\text{test}}}, y_{1_{\text{test}}}, ..., x_{m_{\text{test}}_{\text{test}}}, y_{m_{\text{test}}_{\text{test}}}\)</span></p>
<p><strong>Linear Regression Evaluation (Squared Error):</strong>
- <strong>Model Training:</strong>
  - Minimizing <span class="arithmatex">\(J(w, b) = \frac{1}{2m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} (h_{w, b}(x_{i}) - y_{i})^2 + \frac{\lambda}{2m_{\text{train}}} \sum_{j=1}^{n} w_{j}^2\)</span>
- <strong>Evaluation Metrics:</strong>
  - <strong>Training Error:</strong>
    - <span class="arithmatex">\(J_{\text{train}}(w, b) = \frac{1}{2m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} (h_{w, b}(x_{i}) - y_{i})^2\)</span>
  - <strong>Test Error:</strong>
    - <span class="arithmatex">\(J_{\text{test}}(w, b) = \frac{1}{2m_{\text{test}}} \sum_{i=1}^{m_{\text{test}}} (h_{w, b}(x_{\text{test}_i}) - y_{\text{test}_i})^2\)</span></p>
<p><strong>Classification Problem Evaluation (Logistic Regression):</strong>
- <strong>Model Training:</strong>
  - Minimizing <span class="arithmatex">\(J(w, b) = -\frac{1}{m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} [y_{i} \log(h_{w, b}(x_{i})) + (1 - y_{i}) \log(1 - h_{w, b}(x_{i}))] + \frac{\lambda}{2m_{\text{train}}} \sum_{j=1}^{n} w_{j}^2\)</span>
- <strong>Evaluation Metrics:</strong>
  - <strong>Classification Error (Alternate):</strong>
    - <span class="arithmatex">\(J_{\text{train}} = \frac{1}{m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} \text{error}(h_{w, b}(x_{i}), y_{i})\)</span>
    - <span class="arithmatex">\(J_{\text{test}} = \frac{1}{m_{\text{test}}} \sum_{i=1}^{m_{\text{test}}} \text{error}(h_{w, b}(x_{\text{test}_i}), y_{\text{test}_i})\)</span>
  - <strong>Binary Classification Error:</strong>
    - <span class="arithmatex">\(J_{\text{train}} = \frac{1}{m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} \text{misclassified}(h_{w, b}(x_{i}), y_{i})\)</span>
    - <span class="arithmatex">\(J_{\text{test}} = \frac{1}{m_{\text{test}}} \sum_{i=1}^{m_{\text{test}}} \text{misclassified}(h_{w, b}(x_{\text{test}_i}), y_{\text{test}_i})\)</span></p>
<p><strong>Evaluation Insights:</strong>
- <strong>Overfitting Indicators:</strong>
  - Low <span class="arithmatex">\(J_{\text{train}}\)</span> but high <span class="arithmatex">\(J_{\text{test}}\)</span> suggests overfitting and poor generalization.
- <strong>Automatic Model Selection:</strong>
  - Ongoing refinement for automated model selection based on evaluation metrics.
  - Enables informed decisions on model complexity for various applications.</p>
<h3 id="model-selection-and-trainingcross-validationtest-sets">Model selection and training/cross validation/test sets<a class="headerlink" href="#model-selection-and-trainingcross-validationtest-sets" title="Permanent link">&para;</a></h3>
<p><strong>Automatic Model Selection with Cross-Validation:</strong></p>
<p><strong>Model Evaluation Refinement:</strong>
- <strong>Issue with Test Set Error:</strong>
  - Test set error may provide an optimistic estimate of generalization error.
  - The danger lies in selecting the model based on the test set error.</p>
<p><strong>Introduction of Cross-Validation Set:</strong>
- <strong>Data Splitting:</strong>
  - Training set (60%), Cross-validation set (20%), Test set (20%).
  - <span class="arithmatex">\(M_{\text{train}} = 6\)</span>, <span class="arithmatex">\(M_{\text{cv}} = 2\)</span>, <span class="arithmatex">\(M_{\text{test}} = 2\)</span>.</p>
<p><strong>Model Selection Procedure:</strong>
- <strong>Models to Evaluate:</strong>
  - Consider polynomial models of degrees 1 to 10.
- <strong>Parameters and Evaluation:</strong>
  - Fit parameters <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(b\)</span> for each model using the training set.
  - Evaluate on the cross-validation set to get <span class="arithmatex">\(J_{\text{cv}}(w, b)\)</span>.
- <strong>Model Choice:</strong>
  - Select the model with the lowest cross-validation error, e.g., <span class="arithmatex">\(J_{\text{cv}}\)</span> for the 4<sup>th</sup>-degree polynomial.
- <strong>Generalization Estimate:</strong>
  - Report the generalization error on the test set, e.g., <span class="arithmatex">\(J_{\text{test}}(w, b)\)</span>.</p>
<p><strong>Preventing Test Set Contamination:</strong>
- <strong>Best Practice:</strong>
  - Avoid decisions based on the test set (e.g., model selection).
  - Utilize the test set only after finalizing the model to estimate true generalization error.</p>
<p><strong>Application to Neural Networks:</strong>
- <strong>Model Choices:</strong>
  - Explore various neural network architectures.
  - Train multiple models and evaluate on the cross-validation set.
- <strong>Parameters and Evaluation:</strong>
  - Obtain parameters <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(b\)</span> for each architecture.
  - Choose the architecture with the lowest cross-validation error.
- <strong>Final Evaluation:</strong>
  - Use the test set for estimating the generalization error.</p>
<p><strong>Model Selection Best Practices:</strong>
- <strong>Decision Scope:</strong>
  - Make decisions (fitting parameters, model architecture) based on training and cross-validation sets.
  - Delay test set evaluation until the final model choice.</p>
<p><strong>Summary:</strong>
- <strong>Automatic Model Selection:</strong>
  - Cross-validation introduces a dedicated set for model evaluation.
  - Avoids contamination of test set during decision-making processes.
- <strong>Widespread Practice:</strong>
  - Widely used for choosing models in machine learning applications.
- <strong>Future Focus:</strong>
  - Delving into diagnostics, with a focus on bias and variance analysis.</p>
<h3 id="diagnosing-bias-and-variance">Diagnosing bias and variance<a class="headerlink" href="#diagnosing-bias-and-variance" title="Permanent link">&para;</a></h3>
<p><strong>Iterative Model Development:</strong>
- <strong>Continuous Improvement:</strong>
  - The development of a machine learning system is an iterative process.
  - Initial models often fall short of desired performance.</p>
<p><strong>Key Diagnostic Tool: Bias and Variance:</strong>
- <strong>Bias:</strong>
  - <em>Definition:</em> A measure of how well the model fits the training data.
  - <em>High Bias (Underfitting):</em> Indicates that the model is too simple and cannot capture the underlying patterns in the data.
  - <em>Math:</em> <span class="arithmatex">\(J_{\text{train}}(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2\)</span>
- <strong>Variance:</strong>
  - <em>Definition:</em> A measure of how much the model's predictions vary on different training sets.
  - <em>High Variance (Overfitting):</em> Indicates that the model is too complex and captures noise in the training data.
  - <em>Math:</em> <span class="arithmatex">\(J_{\text{cv}}(\theta) = \frac{1}{2m_{\text{cv}}}\sum_{i=1}^{m_{\text{cv}}}(h_{\theta}(x_{\text{cv}}^{(i)}) - y_{\text{cv}}^{(i)})^2\)</span></p>
<p><strong>Example: Polynomial Regression:</strong>
- <strong>Underfitting (High Bias):</strong>
  - <em>Characteristics:</em> Poor fit to both training set and cross-validation set.
  - <em>Indicators:</em> <span class="arithmatex">\(J_{\text{train}}\)</span> is high.
- <strong>Overfitting (High Variance):</strong>
  - <em>Characteristics:</em> Excellent fit to training set but poor generalization to new data.
  - <em>Indicators:</em> <span class="arithmatex">\(J_{\text{train}}\)</span> is low, <span class="arithmatex">\(J_{\text{cv}}\)</span> significantly higher.</p>
<p><strong>Systematic Diagnosis:</strong>
- <strong>Performance Metrics:</strong>
  - <em>Evaluation:</em> Use metrics like <span class="arithmatex">\(J_{\text{train}}\)</span> and <span class="arithmatex">\(J_{\text{cv}}\)</span> to diagnose bias and variance.
- <strong>High Bias:</strong>
  - <em>Indicators:</em> <span class="arithmatex">\(J_{\text{train}}\)</span> is high, suggesting the model doesn't fit the training data well.
- <strong>High Variance:</strong>
  - <em>Indicators:</em> <span class="arithmatex">\(J_{\text{cv}}\)</span> much greater than <span class="arithmatex">\(J_{\text{train}}\)</span>, signaling overfitting.</p>
<p><strong>Graphical Representation:</strong>
- <strong>Degree of Polynomial vs. Errors:</strong>
  - <em>Trend:</em> <span class="arithmatex">\(J_{\text{train}}\)</span> tends to decrease as the degree of the polynomial increases.
  - <em>Optimal Degree:</em> There's a sweet spot for <span class="arithmatex">\(J_{\text{cv}}\)</span>; too low or too high leads to higher error.
  - <em>Math:</em> <span class="arithmatex">\(J_{\text{train}}(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2\)</span>
  - <em>Math:</em> <span class="arithmatex">\(J_{\text{cv}}(\theta) = \frac{1}{2m_{\text{cv}}}\sum_{i=1}^{m_{\text{cv}}}(h_{\theta}(x_{\text{cv}}^{(i)}) - y_{\text{cv}}^{(i)})^2\)</span></p>
<p><strong>Simultaneous High Bias and High Variance:</strong>
- <strong>Rare in Linear Regression:</strong>
  - <em>Common Scenario:</em> Linear models usually exhibit either high bias or high variance.
- <strong>Neural Networks Exception:</strong>
  - <em>Occurrence:</em> Some neural network applications may experience both high bias and high variance.
- <strong>Indicator:</strong>
  - <em>Observation:</em> Poor performance on the training set (high bias) and significantly worse on the cross-validation set.
  - <em>Math:</em> <span class="arithmatex">\(J_{\text{train}}\)</span> high, <span class="arithmatex">\(J_{\text{cv}}\)</span> much greater than <span class="arithmatex">\(J_{\text{train}}\)</span>.</p>
<p><strong>Key Takeaways:</strong>
- <strong>High Bias:</strong>
  - <em>Issue:</em> Poor fit to training data.
  - <em>Indicator:</em> <span class="arithmatex">\(J_{\text{train}}\)</span> is high.
- <strong>High Variance:</strong>
  - <em>Issue:</em> Overfitting to training data.
  - <em>Indicator:</em> <span class="arithmatex">\(J_{\text{cv}}\)</span> much greater than <span class="arithmatex">\(J_{\text{train}}\)</span>.
- <strong>Diagnosis Tool:</strong>
  - <em>Approach:</em> Evaluate performance on both training and cross-validation sets.</p>
<p><strong>Next Steps:</strong>
- <strong>Performance Improvement:</strong>
  - <em>Guidance:</em> Understanding bias and variance provides insights into improving model performance.
- <strong>Upcoming:</strong>
  - <em>Focus:</em> Exploration of regularization effects on bias and variance.
  - <em>Strategies:</em> Introduction to strategies for enhancing machine learning model performance.</p>
<h3 id="regularization-and-biasvariance">Regularization and bias/variance<a class="headerlink" href="#regularization-and-biasvariance" title="Permanent link">&para;</a></h3>
<p><strong>Regularization and Bias-Variance Tradeoff:</strong>
- <strong>Introduction:</strong>
  - <em>Objective:</em> Understand how regularization, specifically the regularization parameter <span class="arithmatex">\(\lambda\)</span>, influences bias and variance.
  - <em>Context:</em> Using a fourth-order polynomial example with regularization.</p>
<p><strong>Effect of <span class="arithmatex">\(\lambda\)</span> on Model:</strong>
- <strong>High <span class="arithmatex">\(\lambda\)</span> (e.g., 10,000):</strong>
  - <em>Outcome:</em> Model simplification, parameters <span class="arithmatex">\(w\)</span> driven close to zero.
  - <em>Result:</em> High bias, underfitting, poor performance on training set (<span class="arithmatex">\(J_{\text{train}}\)</span> large).
- <strong>Low <span class="arithmatex">\(\lambda\)</span> (e.g., 0):</strong>
  - <em>Outcome:</em> No regularization, overfitting.
  - <em>Result:</em> High variance, fits training data well (<span class="arithmatex">\(J_{\text{train}}\)</span> small) but poor generalization (<span class="arithmatex">\(J_{\text{cv}}\)</span> much larger than <span class="arithmatex">\(J_{\text{train}}\)</span>).
- <strong>Intermediate <span class="arithmatex">\(\lambda\)</span>:</strong>
  - <em>Target:</em> Find a balance, moderate regularization.
  - <em>Result:</em> Balanced model, good fit to data, small <span class="arithmatex">\(J_{\text{train}}\)</span> and <span class="arithmatex">\(J_{\text{cv}}\)</span>.</p>
<p><strong>Choosing <span class="arithmatex">\(\lambda\)</span> with Cross-Validation:</strong>
- <strong>Procedure:</strong>
  - <em>Step 1:</em> Try various <span class="arithmatex">\(\lambda\)</span> values (e.g., 0, 0.01, 0.02, ..., 10).
  - <em>Step 2:</em> Minimize cost function for each <span class="arithmatex">\(\lambda\)</span> and compute <span class="arithmatex">\(J_{\text{cv}}\)</span> for evaluation.
  - <em>Step 3:</em> Identify <span class="arithmatex">\(\lambda\)</span> with lowest <span class="arithmatex">\(J_{\text{cv}}\)</span> as optimal.
  - <em>Example:</em> Choose <span class="arithmatex">\(\lambda\)</span> where <span class="arithmatex">\(J_{\text{cv}}\)</span> is lowest (e.g., <span class="arithmatex">\(\lambda = 0.02\)</span>).
  - <em>Result:</em> Obtained parameters <span class="arithmatex">\(w_{\text{chosen}}\)</span>.</p>
<p><strong>Generalization Error Estimation:</strong>
- <em>Test Set Error:</em> Evaluate on a separate test set to estimate generalization error.
  - <span class="arithmatex">\(J_{\text{test}}(w_{\text{chosen}})\)</span>.</p>
<p><strong>Visualization of Bias and Variance:</strong>
- <strong>Graphical Representation:</strong>
  - <em>X-Axis:</em> Annotated with <span class="arithmatex">\(\lambda\)</span> values.
  - <em>Left Extreme (Small <span class="arithmatex">\(\lambda\)</span>):</em> High variance, overfitting (small <span class="arithmatex">\(J_{\text{train}}\)</span>, large <span class="arithmatex">\(J_{\text{cv}}\)</span>).
  - <em>Right Extreme (Large <span class="arithmatex">\(\lambda\)</span>):</em> High bias, underfitting (large <span class="arithmatex">\(J_{\text{train}}\)</span>, large <span class="arithmatex">\(J_{\text{cv}}\)</span>).
  - <em>Intermediate <span class="arithmatex">\(\lambda\)</span>:</em> Balanced model, minimized <span class="arithmatex">\(J_{\text{cv}}\)</span>.
  - <em>Trend:</em> <span class="arithmatex">\(J_{\text{train}}\)</span> increases with <span class="arithmatex">\(\lambda\)</span> due to the regularization term.</p>
<p><strong>Quantifying "High" or "Much Higher":</strong>
- <strong>Baseline Performance:</strong>
  - <em>Definition:</em> Establish a baseline to gauge performance.
  - <em>Refinement:</em> Helps in comparing <span class="arithmatex">\(J_{\text{train}}\)</span> and <span class="arithmatex">\(J_{\text{cv}}\)</span>.
- <strong>Baseline Examples:</strong>
  - <em>Example 1:</em> If <span class="arithmatex">\(J_{\text{train}} = 1\)</span> and <span class="arithmatex">\(J_{\text{cv}} = 10\)</span>, <span class="arithmatex">\(J_{\text{cv}}\)</span> is much higher.
  - <em>Example 2:</em> If <span class="arithmatex">\(J_{\text{train}} = 10\)</span> and <span class="arithmatex">\(J_{\text{cv}} = 12\)</span>, <span class="arithmatex">\(J_{\text{cv}}\)</span> is moderately higher.</p>
<p><strong>Next Steps:</strong>
- <strong>Baseline Approach:</strong>
  - <em>Advantage:</em> Provides a quantitative measure for assessing bias and variance.
- <strong>Further Refinement:</strong>
  - <em>Upcoming:</em> Exploration of additional refinements in evaluating model performance.
  - <em>Clarity:</em> Understanding the significance of <span class="arithmatex">\(J_{\text{train}}\)</span> and <span class="arithmatex">\(J_{\text{cv}}\)</span> values in practical terms.</p>
<h3 id="establishing-a-baseline-level-of-performance">Establishing a baseline level of performance<a class="headerlink" href="#establishing-a-baseline-level-of-performance" title="Permanent link">&para;</a></h3>
<p><strong>Establishing a Baseline Level of Performance:</strong></p>
<p><strong>Context:</strong>
- Example: Speech recognition system (applied multiple times).
- Objective: Understand how to assess bias and variance by comparing errors to a baseline level of performance.</p>
<p><strong>Speech Recognition Example:</strong>
- <em>Application:</em> Web search on a mobile phone using speech recognition.
- <em>Typical Queries:</em> "What is today's weather?" or "Coffee shops near me."
- <em>Algorithm Output:</em> Transcripts for audio queries.</p>
<p><strong>Training and Cross-Validation Errors:</strong>
- <em>Training Error (J_{train}):</em> 10.8% (Algorithm's error on training set).
- <em>Cross-Validation Error (J_{cv}):</em> 14.8% (Algorithm's error on cross-validation set).</p>
<p><strong>Benchmarking Against Human Level Performance:</strong>
- <em>Human Level Error:</em> 10.6% (Error in transcribing audio by fluent speakers).
- <em>Analysis:</em> Algorithm performs 0.2% worse than human level.</p>
<p><strong>Significance of Human Level Performance:</strong>
- <em>Benchmark:</em> Human level performance often used as a baseline.
- <em>Comparison:</em> Training error compared to the desired human level of performance.</p>
<p><strong>Judging Bias and Variance:</strong>
- <em>High Bias:</em> Training error significantly higher than baseline (0.2% difference).
- <em>High Variance:</em> Large gap between training error and cross-validation error (4% difference).</p>
<p><strong>Quantifying "High" or "Much Higher":</strong>
- <em>Baseline Approach:</em> Establishing a baseline level of performance is crucial.
- <em>Examples:</em> 
  - High variance: 0.2% (difference to baseline), 4% (gap with cross-validation error).
  - High bias: 4.4% (difference to baseline), 4.7% (gap with cross-validation error).</p>
<p><strong>Use of Baseline in Different Applications:</strong>
- <em>Perfect Performance:</em> Baseline could be zero percent for perfect performance.
- <em>Noisy Data Example:</em> Baseline may be higher than zero (e.g., speech recognition).
- <em>Balanced Analysis:</em> Considers the goal and feasibility in each application.</p>
<p><strong>Assessing Bias and Variance in Practice:</strong>
- <em>Combination:</em> Algorithms can have both high bias and high variance.
- <em>Practical Approach:</em> Consider both training error, baseline, and cross-validation error.
- <em>Refinement:</em> Gives a more accurate assessment of algorithm performance.</p>
<p><strong>Summary:</strong>
- <strong>Bias Assessment:</strong> Evaluate if the training error is significantly higher than the baseline.
- <strong>Variance Assessment:</strong> Examine the gap between training error and cross-validation error.
- <strong>Practical Judgment:</strong> Consider the context and feasibility of achieving zero error.</p>
<h3 id="learning-curves">Learning curves<a class="headerlink" href="#learning-curves" title="Permanent link">&para;</a></h3>
<p><strong>Learning Curves: Understanding Algorithm Performance</strong></p>
<p><strong>Context:</strong>
- Learning curves provide insights into how a learning algorithm performs with varying amounts of experience (training examples).
- Example: Plotting learning curves for a second-order polynomial quadratic function.</p>
<p><strong>Components of Learning Curves:</strong>
1. <strong>Horizontal Axis (m_train):</strong>
   - Represents the training set size or the number of examples.</p>
<ol>
<li><strong>Vertical Axis (Error - J):</strong></li>
<li>Represents the error, either J_cv (cross-validation error) or J_train (training error).</li>
</ol>
<p><strong>Learning Curve for Cross-Validation Error (J_cv):</strong>
- As m_train increases, J_cv tends to decrease.
- Larger training sets lead to better models, reducing cross-validation error.</p>
<p><strong>Learning Curve for Training Error (J_train):</strong>
- Surprisingly, as m_train increases, J_train might increase.
- Explanation: With a small training set, fitting a quadratic function perfectly is easy. As the set size grows, fitting all examples perfectly becomes harder, causing the error to increase.</p>
<p><strong>High Bias Scenario (Underfitting):</strong>
- Example: Fitting a linear function.
- Both J_train and J_cv tend to flatten out after a certain point.
- Plateau Effect: Limited improvement with more data, indicating a high bias problem.
- Baseline (Human-Level) Performance: Indicates a significant gap.</p>
<p><strong>High Variance Scenario (Overfitting):</strong>
- Example: Fitting a high-degree polynomial.
- J_train increases with m_train, but J_cv remains much higher.
- Large gap between J_cv and J_train signals overfitting.
- Baseline Performance: J_train may even be lower than human-level performance.</p>
<p><strong>Insights:</strong>
- <strong>High Bias:</strong> Increasing training data alone won't help much; the model is too simple.
- <strong>High Variance:</strong> Increasing training data is likely to help; the model can improve with more examples.
- <strong>Conclusion:</strong> Different responses based on whether the algorithm has high bias or high variance.</p>
<p><strong>Practical Considerations:</strong>
- <strong>Learning Curve Visualization:</strong> Plot J_train and J_cv for different-sized subsets.
- <strong>Computational Cost:</strong> Training many models with varying subsets is computationally expensive.
- <strong>Mental Visualization:</strong> Having a mental picture of learning curves aids in understanding bias and variance dynamics.</p>
<p><strong>Application to Housing Price Prediction:</strong>
- Revisiting the housing price prediction example.
- Using insights from bias and variance to decide the next steps in model improvement.</p>
<p><strong>Next Steps:</strong>
- Understanding how bias and variance considerations guide decisions in refining a machine learning model.
- Applying these concepts to real-world scenarios for effective model improvement.</p>
<h3 id="deciding-what-to-try-next-revisited">Deciding what to try next revisited<a class="headerlink" href="#deciding-what-to-try-next-revisited" title="Permanent link">&para;</a></h3>
<p><strong>Deciding What to Try Next Revisited</strong></p>
<p><strong>Understanding Bias and Variance:</strong>
- Review of using training error (J_train) and cross-validation error (J_cv) to diagnose learning algorithm issues.
- High bias: Algorithm doesn't perform well on training set.
- High variance: Algorithm overfits the training set, fails to generalize.</p>
<p><strong>Strategies for High Bias:</strong>
1. <strong>Get more training examples:</strong>
   - Helps if algorithm is underfitting due to lack of data.
   - Primarily addresses high variance problems.</p>
<ol>
<li><strong>Try a smaller set of features:</strong></li>
<li>Reducing the number of features decreases model complexity.</li>
<li>
<p>Addresses high variance by preventing overfitting.</p>
</li>
<li>
<p><strong>Adding additional features:</strong></p>
</li>
<li>Provides more information to the algorithm.</li>
<li>
<p>Enhances model complexity, addressing high bias.</p>
</li>
<li>
<p><strong>Adding polynomial features:</strong></p>
</li>
<li>Increases feature complexity to capture more patterns.</li>
<li>
<p>Fixes high bias by enabling the model to learn more complex relationships.</p>
</li>
<li>
<p><strong>Decreasing Lambda (regularization parameter):</strong></p>
</li>
<li>Lowers the regularization term importance.</li>
<li>
<p>Allows the model to fit the training set more closely, addressing high bias.</p>
</li>
<li>
<p><strong>Increasing Lambda:</strong></p>
</li>
<li>Raises the regularization term importance.</li>
<li>Forces the model to be less complex, addressing high variance.</li>
</ol>
<p><strong>High Variance vs. High Bias:</strong>
- <strong>High Variance:</strong> Overfitting; algorithm too complex.
- <strong>High Bias:</strong> Underfitting; algorithm too simple.
- Strategies for one may exacerbate the other.</p>
<p><strong>Not a Fix for High Bias:</strong>
- Reducing the training set size is not an effective strategy for high bias.
- Shrinking the training set can make the model fit better but often worsens cross-validation performance.</p>
<p><strong>Application to Neural Networks:</strong>
- Bias and variance concepts apply to neural network training.
- Next video: Explore bias and variance in the context of training neural networks.</p>
<p><strong>Conclusion:</strong>
- Bias and variance are powerful concepts for algorithm development.
- Continual practice enhances mastery.
- Next: Apply bias and variance concepts to neural network training.</p>
<h3 id="biasvariance-and-neural-networks">Bias/variance and neural networks<a class="headerlink" href="#biasvariance-and-neural-networks" title="Permanent link">&para;</a></h3>
<p><strong>Bias/Variance and Neural Networks</strong></p>
<p><strong>Overview:</strong>
- Neural networks provide a way to address bias and variance simultaneously.
- Tradeoff between bias and variance addressed by training large neural networks.
- Recipe for using neural networks to reduce bias and variance.</p>
<p><strong>Bias and Variance Tradeoff:</strong>
- Traditional machine learning discussed bias-variance tradeoff.
- Balancing model complexity (degree of polynomial) with regularization (λ) to avoid high bias or high variance.
- Neural networks offer a different approach to this tradeoff.</p>
<p><strong>Large Neural Networks and Bias:</strong>
- Large neural networks trained on moderate-sized datasets are low bias machines.
- Bigger networks can fit training set well, reducing bias.
- Recipe: Train on training set, if high bias, use a larger network.</p>
<p><strong>Recipe for Reducing Bias:</strong>
1. Train on training set.
2. Check if J_train is high (compared to target performance).
3. If high bias, use a larger neural network.
4. Repeat until J_train is satisfactory.</p>
<p><strong>Checking Variance:</strong>
- After achieving low bias on training set, check if the model has high variance.
- If large gap between J_cv and J_train, indicates high variance.
- To address high variance, consider getting more data.</p>
<p><strong>Iterative Process:</strong>
- Bias and variance may change during algorithm development.
- Adjust based on current issue (bias or variance) and repeat the process.
- Neural networks allow iterating on model size without tradeoff concerns.</p>
<p><strong>Limitations and Considerations:</strong>
- Computational expense can increase with larger neural networks.
- Availability of more data may be limited.
- Rise of deep learning influenced by access to large datasets and powerful hardware.</p>
<p><strong>Regularization in Neural Networks:</strong>
- Regularization term similar to linear regression: λ/2m * ∑(w^2).
- TensorFlow implementation: <code>kernel_regularizer=tf.keras.regularizers.l2(0.01)</code>.</p>
<p><strong>Takeaways:</strong>
1. Larger neural networks, with appropriate regularization, almost never hurt.
2. Large neural networks are often low bias machines, suitable for complex tasks.
3. Computational expense and data availability are limitations.
4. Regularization term helps prevent overfitting.</p>
<p><strong>Next Steps:</strong>
- Understanding bias and variance in neural networks provides insights for model development.
- Applying these concepts to real-world machine learning systems in the next video.
- Practical advice for efficiently advancing in machine learning system development.</p>
<h3 id="iterative-loop-of-ml-development">Iterative loop of ML development<a class="headerlink" href="#iterative-loop-of-ml-development" title="Permanent link">&para;</a></h3>
<p><strong>Iterative Loop of Machine Learning Development</strong></p>
<p><strong>Overview:</strong>
- The process of developing a machine learning system involves an iterative loop.
- Steps include deciding on the system's architecture, implementing and training the model, and making adjustments based on diagnostics.
- A continuous loop is followed until the desired performance is achieved.</p>
<p><strong>Steps in the Iterative Loop:</strong>
1. <strong>Decide on Architecture:</strong>
   - Choose the machine learning model.
   - Decide on data to use.
   - Select hyperparameters.</p>
<ol>
<li><strong>Implement and Train Model:</strong></li>
<li>Implement the chosen architecture.</li>
<li>
<p>Train the model on the training set.</p>
</li>
<li>
<p><strong>Diagnostics:</strong></p>
</li>
<li>Evaluate the model's performance.</li>
<li>
<p>Use diagnostics like bias and variance analysis.</p>
</li>
<li>
<p><strong>Adjustments:</strong></p>
</li>
<li>Based on insights from diagnostics, make decisions.</li>
<li>
<p>Modify architecture, hyperparameters, or data.</p>
</li>
<li>
<p><strong>Repeat:</strong></p>
</li>
<li>Go through the loop again with the new choices.</li>
<li>Iterate until reaching the desired performance.</li>
</ol>
<p><strong>Example: Email Spam Classifier:</strong>
- Text classification problem: Spam vs. non-spam.
- Features (x): Top 10,000 words in the English language.
- Construct feature vector based on word presence or frequency.
- Train a classification algorithm (e.g., logistic regression, neural network) to predict y (spam or non-spam).</p>
<p><strong>Improvement Ideas:</strong>
- Collect more data.
- Develop more sophisticated features based on email routing.
- Extract features from the email body.
- Detect misspellings or deliberate misspellings.</p>
<p><strong>Choosing Promising Ideas:</strong>
- Diagnose whether the algorithm has high bias or high variance.
- High bias: Larger neural network or more complex model.
- High variance: Collect more data.
- Choosing promising ideas can significantly speed up the project.</p>
<p><strong>Next Steps:</strong>
- The iterative loop is a fundamental aspect of machine learning development.
- Diagnostics, such as bias and variance analysis, guide decision-making.
- The next video will introduce error analysis as another key component of gaining insights in machine learning development.</p>
<h3 id="error-analysis">Error analysis<a class="headerlink" href="#error-analysis" title="Permanent link">&para;</a></h3>
<p><strong>Error Analysis in Machine Learning</strong></p>
<p><strong>Importance of Diagnostics:</strong>
- Bias and variance analysis is crucial for understanding model performance.
- Error analysis is the second important idea for improving learning algorithm performance.</p>
<p><strong>Error Analysis Process:</strong>
1. <strong>Example Scenario:</strong>
   - Cross-validation set with 500 examples.
   - Algorithm misclassifies 100 examples.</p>
<ol>
<li><strong>Manual Examination:</strong></li>
<li>Manually inspect the 100 misclassified examples.</li>
<li>
<p>Group them based on common themes or traits.</p>
</li>
<li>
<p><strong>Categorization:</strong></p>
</li>
<li>Identify common traits, e.g., pharmaceutical spam, misspellings, phishing emails.</li>
<li>
<p>Count occurrences in each category.</p>
</li>
<li>
<p><strong>Analysis:</strong></p>
</li>
<li>Prioritize categories based on frequency and impact.</li>
<li>
<p>Understand which types of errors are more significant.</p>
</li>
<li>
<p><strong>Overlapping Categories:</strong></p>
</li>
<li>Categories may overlap; an email can fall into multiple categories.</li>
<li>
<p>E.g., pharmaceutical spam with unusual routing or phishing emails with deliberate misspellings.</p>
</li>
<li>
<p><strong>Handling Large Datasets:</strong></p>
</li>
<li>If dealing with a large cross-validation set (e.g., 5,000 examples), sample a subset for manual analysis (e.g., 100 examples).</li>
<li>This provides insights into common errors without exhaustive examination.</li>
</ol>
<p><strong>Decision-Making:</strong>
- Error analysis guides decisions on what changes to prioritize in the model or data.
- Helps identify which types of errors are more prevalent and impactful.</p>
<p><strong>Limitations of Error Analysis:</strong>
- Easier for problems where humans excel in judgment.
- More challenging for tasks even humans find difficult (e.g., predicting ad clicks).</p>
<p><strong>Example Scenario Insights:</strong>
- Deliberate misspellings had a smaller impact on misclassifications (3 out of 100).
- Pharmaceutical spam and phishing emails were significant problem areas.
- More data collection for pharmaceutical spam and phishing emails could be beneficial.</p>
<p><strong>Practical Implications:</strong>
- Error analysis saves time by focusing efforts on the most impactful changes.
- Understanding error patterns provides inspiration for addressing specific issues in the model.</p>
<p><strong>Next Steps:</strong>
- Error analysis complements bias and variance diagnostics in decision-making.
- The next video will delve into the topic of adding more data to improve learning algorithms efficiently.</p>
<h3 id="adding-more-data">Adding more Data<a class="headerlink" href="#adding-more-data" title="Permanent link">&para;</a></h3>
<p><strong>Tips for Adding Data in Machine Learning</strong></p>
<p><strong>1. Targeted Data Collection:</strong>
   - Instead of adding more data of all types, focus on specific areas that need improvement.
   - Example: If error analysis reveals issues with pharmaceutical spam, collect more data specifically related to pharma spam.</p>
<p><strong>2. Unlabeled Data and Manual Labeling:</strong>
   - Utilize unlabeled data by manually reviewing it for relevant examples.
   - Skim through unlabeled data to identify and add examples of specific categories.</p>
<p><strong>3. Data Augmentation:</strong>
   - Widely used for image and audio data.
   - Distort existing training examples to create new ones.
   - Examples include rotation, enlargement, shrinking, contrast changes, and mirroring.
   - For audio, adding background noise or simulating different environments.
   - Changes made should be representative of test set conditions.</p>
<p><strong>4. Data Synthesis:</strong>
   - Create entirely new examples from scratch.
   - Example: Photo OCR task, generating synthetic data using various fonts, colors, and contrasts.
   - Useful for tasks where obtaining real data is challenging.</p>
<p><strong>5. Representativeness in Augmentation:</strong>
   - Ensure that augmentations represent realistic variations observed in the test set.
   - Meaningful distortions improve model generalization.
   - Random, meaningless noise may not contribute effectively.</p>
<p><strong>6. Transfer Learning:</strong>
   - A technique for leveraging data from a different, often unrelated task to boost performance.
   - Especially valuable when data is limited for the target task.
   - Neural networks can be pre-trained on a related task, and the knowledge is transferred to the target task.</p>
<p><strong>7. Data-Centric Approach:</strong>
   - Shift focus from a model-centric to a data-centric approach.
   - Spend time engineering the data used by the algorithm.
   - Collecting more targeted data, using augmentation, and synthesizing data can be efficient ways to enhance performance.</p>
<p><strong>8. Efficient Use of Algorithms:</strong>
   - Many existing algorithms (e.g., linear regression, logistic regression, neural networks) are powerful and work well for various applications.
   - Efficiently leveraging data can be more fruitful than solely focusing on algorithm improvement.</p>
<p><strong>9. Data Centricity vs. Model Centricity:</strong>
   - Emphasizes the importance of prioritizing data-centric approaches in certain scenarios.
   - Tools discussed in the video offer practical methods for making data more effective.</p>
<p><strong>10. Special Cases:</strong>
   - Some applications may face challenges in acquiring sufficient data.
   - Transfer learning can be a powerful technique in such cases.</p>
<p><strong>11. Future Exploration:</strong>
   - Transfer learning will be discussed in the next video, exploring how it can significantly enhance performance in scenarios with limited data.</p>
<p><strong>Conclusion:</strong>
   - The presented techniques offer a toolbox for efficiently adding data and improving machine learning algorithm performance.
   - Consider the specific needs of your application and choose techniques accordingly.</p>
<h3 id="transfer-learning-using-data-from-a-different-task">Transfer learning: using data from a different task<a class="headerlink" href="#transfer-learning-using-data-from-a-different-task" title="Permanent link">&para;</a></h3>
<p><strong>Transfer Learning: Leveraging Data from a Different Task</strong></p>
<p><strong>Overview:</strong>
- Transfer learning is a powerful technique for applications with limited data.
- It involves using data from a different task to enhance performance in the target application.
- Two main steps: supervised pre-training on a large dataset, followed by fine-tuning on a smaller dataset for the specific task.</p>
<p><strong>How Transfer Learning Works:</strong>
1. <strong>Supervised Pre-Training:</strong>
   - Train a neural network on a large dataset (e.g., one million images) with multiple classes (e.g., cats, dogs, cars, people).
   - Learn parameters (weights and biases) for all layers, including the output layer with a thousand classes.
   - Parameters: <span class="arithmatex">\(W^1, b^1, W^2, b^2, W^3, b^3, W^4, b^4, W^5, b^5\)</span>.</p>
<ol>
<li><strong>Creating a New Model for the Target Task:</strong></li>
<li>Copy the pre-trained neural network.</li>
<li>Remove the output layer with a thousand classes.</li>
<li>Add a new output layer with just 10 classes (for digits 0-9).</li>
<li>
<p>New parameters: <span class="arithmatex">\(W^1, b^1, W^2, b^2, W^3, b^3, W^4, b^4, W^5, b^5\)</span> (reused) and new <span class="arithmatex">\(W^6, b^6\)</span> for the output layer.</p>
</li>
<li>
<p><strong>Fine-Tuning:</strong></p>
</li>
<li>Use the pre-trained parameters as a starting point.</li>
<li>Two options:<ul>
<li>Option 1: Only train the parameters of the new output layer (<span class="arithmatex">\(W^6, b^6\)</span>).</li>
<li>Option 2: Train all parameters, but initialize the first four layers with pre-trained values.</li>
</ul>
</li>
</ol>
<p><strong>Why Transfer Learning Works:</strong>
- Neural networks learn hierarchical features from simple to complex.
- Pre-training on diverse tasks helps capture generic features like edges, corners, and shapes.
- Fine-tuning on the target task refines the model for specific recognition (e.g., handwritten digits).</p>
<p><strong>Choosing Option 1 or Option 2:</strong>
- Option 1 (output layer only) might be better for very small datasets.
- Option 2 (all layers) can work better with larger datasets.</p>
<p><strong>Practical Considerations:</strong>
- Researchers often share pre-trained models online.
- Downloading pre-trained models accelerates the process.
- Two-step process: supervised pre-training, followed by fine-tuning.</p>
<p><strong>Benefits of Transfer Learning:</strong>
- Enables effective use of smaller datasets.
- Helps in cases where acquiring target task data is challenging.
- Community sharing of pre-trained models promotes collaborative progress in machine learning.</p>
<p><strong>Restrictions of Pre-Training:</strong>
- Input types must match between pre-training and fine-tuning.
- For computer vision tasks, pre-training requires an image input of specific dimensions.
- Different tasks (e.g., image recognition vs. speech recognition) require different pre-training datasets.</p>
<p><strong>Popular Examples:</strong>
- GPT-3, BERT, ImageNet are examples of pre-trained models used for various applications.
- These models are often fine-tuned for specific tasks.</p>
<p><strong>Conclusion:</strong>
- Transfer learning is a valuable technique for boosting performance when data is limited.
- Two-step process involves pre-training on a large dataset and fine-tuning on a smaller dataset.
- Open sharing of pre-trained models in the machine learning community contributes to collective progress.
- Next, the video will explore the full cycle of a machine learning project, covering all essential steps.</p>
<h3 id="full-cycle-of-a-machine-learning-project">Full cycle of a machine learning project<a class="headerlink" href="#full-cycle-of-a-machine-learning-project" title="Permanent link">&para;</a></h3>
<p><strong>Full Cycle of a Machine Learning Project</strong></p>
<p><strong>1. Scope the Project:</strong>
   - Define the project's objectives and goals.
   - Example: Speech recognition for voice search.</p>
<p><strong>2. Data Collection:</strong>
   - Identify and gather the necessary data for training and evaluation.
   - Obtain audio and transcripts for labeling.</p>
<p><strong>3. Model Training:</strong>
   - Train the machine learning model, e.g., a speech recognition system.
   - Conduct error analysis and iteratively improve the model.</p>
<p><strong>4. Iterative Loop:</strong>
   - Perform error analysis or bias-variance analysis.
   - Consider collecting more data based on analysis results.
   - Repeat the training loop until the model is deemed good enough.</p>
<p><strong>5. Deployment:</strong>
   - Deploy the trained model in a production environment.
   - Make it available for users to access.</p>
<p><strong>6. Monitoring and Maintenance:</strong>
   - Continuously monitor the performance of the deployed system.
   - Implement maintenance strategies to address performance issues promptly.</p>
<p><strong>7. Model Improvement:</strong>
   - If the deployed model does not meet expectations, go back to training.
   - Gather more data, iterate on the model, and potentially re-deploy.</p>
<p><strong>8. Data from Production:</strong>
   - If allowed, use data from the production deployment for further improvement.
   - Leverage user interactions to enhance model performance.</p>
<p><strong>Deployment in Production:</strong>
   - Implement the model in an inference server to make predictions.
   - Application (e.g., mobile app) communicates with the server via API calls.
   - Software engineering may be needed for efficient and reliable predictions.
   - Scale deployment based on the application's user base.</p>
<p><strong>MLOps (Machine Learning Operations):</strong>
   - A growing field focused on systematic deployment and maintenance of ML systems.
   - Involves practices for reliability, scaling, monitoring, and updates.
   - Considerations for optimizing computational cost, logging data, and system updates.</p>
<p><strong>Ethics in Machine Learning:</strong>
   - Ethical considerations are crucial in machine learning development.
   - Address issues related to bias, fairness, transparency, and user privacy.
   - MLOps includes practices for responsible AI deployment.</p>
<p><strong>Conclusion:</strong>
   - Building a machine learning system involves a comprehensive cycle.
   - From scoping to deployment, continuous monitoring, and improvement.
   - Consider MLOps practices for systematic and ethical ML development.</p>
<p>The next video will delve into the ethical aspects of building machine learning systems, addressing the responsibility of developers in ensuring fairness and transparency.</p>
<h3 id="fairness-bias-and-ethics">Fairness, bias, and ethics<a class="headerlink" href="#fairness-bias-and-ethics" title="Permanent link">&para;</a></h3>
<p><strong>Fairness, Bias, and Ethics in Machine Learning</strong></p>
<p>Machine learning algorithms have a significant impact on billions of people, making it crucial to consider fairness, bias, and ethics when building systems. Several issues highlight the importance of ethical considerations in machine learning:</p>
<ol>
<li><strong>Unacceptable Bias Examples:</strong></li>
<li>Discrimination in hiring tools against women.</li>
<li>Face recognition systems biased against dark-skinned individuals.</li>
<li>Biased bank loan approvals that discriminate against subgroups.</li>
<li>
<p>Algorithms reinforcing negative stereotypes.</p>
</li>
<li>
<p><strong>Negative Use Cases:</strong></p>
</li>
<li>Deepfake videos created without consent or disclosure.</li>
<li>Social media algorithms spreading toxic or incendiary content.</li>
<li>Bots generating fake content for commercial or political purposes.</li>
<li>
<p>Misuse of machine learning for harmful products or fraudulent activities.</p>
</li>
<li>
<p><strong>Ethical Decision-Making:</strong></p>
</li>
<li>Avoid building machine learning systems with negative societal impacts.</li>
<li>Consider the ethical implications of applications and projects.</li>
<li>
<p>If faced with an unethical project, consider walking away.</p>
</li>
<li>
<p><strong>Diverse Team and Brainstorming:</strong></p>
</li>
<li>Assemble a diverse team to brainstorm potential issues.</li>
<li>Diversity across gender, ethnicity, culture, and other dimensions.</li>
<li>
<p>Increase the likelihood of recognizing and addressing problems before deployment.</p>
</li>
<li>
<p><strong>Literature Search and Standards:</strong></p>
</li>
<li>Conduct a literature search on industry or application-specific standards.</li>
<li>Standards emerging in various sectors may inform ethical considerations.</li>
<li>
<p>Guidelines for fairness and bias in decision-making systems.</p>
</li>
<li>
<p><strong>Audit Against Identified Dimensions:</strong></p>
</li>
<li>Audit the system against identified dimensions of potential harm.</li>
<li>Measure performance to identify bias against specific subgroups.</li>
<li>
<p>Identify and fix any problems prior to deployment.</p>
</li>
<li>
<p><strong>Mitigation Plan:</strong></p>
</li>
<li>Develop a mitigation plan in case issues arise after deployment.</li>
<li>Consider rolling back to a previous system if needed.</li>
<li>
<p>Continuously monitor for potential harm and act quickly if problems occur.</p>
</li>
<li>
<p><strong>Taking Ethics Seriously:</strong></p>
</li>
<li>Addressing ethical considerations is not to be taken lightly.</li>
<li>Some projects have more serious ethical implications than others.</li>
<li>
<p>Collective efforts to improve ethical standards in machine learning are crucial.</p>
</li>
<li>
<p><strong>Ongoing Improvement:</strong></p>
</li>
<li>Constantly work towards getting better at addressing ethical issues.</li>
<li>Spot problems, fix them proactively, and learn from mistakes.</li>
<li>Ethical considerations matter as machine learning systems can impact many lives.</li>
</ol>
<p><strong>Conclusion:</strong>
   - Considerations of fairness, bias, and ethics are integral to machine learning.
   - Ethical decision-making, diverse teams, and proactive measures are essential.
   - Continuous improvement and responsible development practices are crucial for the field.</p>
<p>In the next optional video, the focus will be on addressing skewed datasets, particularly those where the ratio of positive to negative examples is significantly imbalanced. This is an important aspect of machine learning applications that requires special techniques for effective handling.</p>
<h3 id="error-metrics-for-skewed-datasets">Error metrics for skewed datasets<a class="headerlink" href="#error-metrics-for-skewed-datasets" title="Permanent link">&para;</a></h3>
<p><strong>Error Metrics for Skewed Datasets: Precision and Recall</strong></p>
<p>In machine learning applications with highly skewed datasets, where the ratio of positive to negative examples is far from 50-50, traditional error metrics like accuracy may not provide meaningful insights. Instead, precision and recall become crucial metrics for evaluating model performance.</p>
<p><strong>Example Scenario:</strong>
Consider a binary classifier aiming to detect a rare disease based on patient data. Let <span class="arithmatex">\( y = 1 \)</span> indicate the presence of the disease, and <span class="arithmatex">\( y = 0 \)</span> indicate its absence. Suppose the classifier achieves 1% error on the test set, seemingly a good outcome. However, if only 0.5% of patients have the disease, a simplistic algorithm that always predicts <span class="arithmatex">\( y = 0 \)</span> could achieve 99.5% accuracy, outperforming the learning algorithm with 1% error.</p>
<p><strong>Confusion Matrix:</strong>
To assess performance in such scenarios, a confusion matrix is constructed. It is a 2x2 table representing the outcomes of predictions:</p>
<div class="arithmatex">\[
\begin{matrix}
\text{Actual Class} &amp; 1 &amp; 0 \\
\text{Predicted Class} &amp; &amp; \\
1 &amp; \text{True Positive (TP)} &amp; \text{False Positive (FP)} \\
0 &amp; \text{False Negative (FN)} &amp; \text{True Negative (TN)} \\
\end{matrix}
\]</div>
<p><strong>Precision and Recall:</strong>
- <strong>Precision (<span class="arithmatex">\( \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} \)</span>):</strong>
  Precision measures, of all predicted positive instances, the fraction that is true positives. It quantifies the accuracy of positive predictions. A high precision indicates that when the classifier predicts positive, it is likely correct.</p>
<ul>
<li><strong>Recall (<span class="arithmatex">\( \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} \)</span>):</strong>
  Recall measures, of all actual positive instances, the fraction that the classifier correctly predicted as positive. It quantifies the ability to capture all positive instances. A high recall suggests effective identification of positive cases.</li>
</ul>
<p><strong>Example Computation:</strong>
Consider a confusion matrix with TP = 15, FP = 5, FN = 10, TN = 70. The precision would be <span class="arithmatex">\( \frac{15}{15 + 5} = 0.75 \)</span> (75%), and the recall would be <span class="arithmatex">\( \frac{15}{15 + 10} = 0.6 \)</span> (60%).</p>
<p><strong>Interpretation:</strong>
- Precision: Of all predicted positive cases, the classifier is correct 75% of the time.
- Recall: Of all actual positive cases, the classifier identifies 60%.</p>
<p><strong>Usefulness of Precision and Recall:</strong>
- Helps evaluate algorithms in cases of skewed datasets.
- Identifies trade-offs between accuracy and the ability to capture rare positive instances.
- Useful for scenarios where certain outcomes are more critical than others.</p>
<p>In the next video, we'll explore how to balance precision and recall, optimizing the performance of a learning algorithm.</p>
<h3 id="trading-off-precision-and-recall">Trading off precision and recall<a class="headerlink" href="#trading-off-precision-and-recall" title="Permanent link">&para;</a></h3>
<p><strong>Trade-off Between Precision and Recall</strong></p>
<p>In machine learning, precision and recall are two important metrics used to evaluate the performance of a classification algorithm, especially in binary classification problems. Precision measures the accuracy of the positive predictions, while recall measures the ability of the model to capture all the positive instances. In an ideal scenario, we would want both high precision and high recall, but there's often a trade-off between the two.</p>
<h3 id="precision-and-recall-recap">Precision and Recall Recap:<a class="headerlink" href="#precision-and-recall-recap" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Precision:</strong></li>
<li>Precision = <span class="arithmatex">\(\frac{\text{True Positives}}{\text{Total Predicted Positives}}\)</span></li>
<li>
<p>Measures the accuracy of positive predictions.</p>
</li>
<li>
<p><strong>Recall:</strong></p>
</li>
<li>Recall = <span class="arithmatex">\(\frac{\text{True Positives}}{\text{Total Actual Positives}}\)</span></li>
<li>Measures the ability to capture all positive instances.</li>
</ul>
<h3 id="thresholding-in-logistic-regression">Thresholding in Logistic Regression:<a class="headerlink" href="#thresholding-in-logistic-regression" title="Permanent link">&para;</a></h3>
<ul>
<li>When using logistic regression, predictions are made based on a threshold (usually 0.5).</li>
<li>Adjusting the threshold allows for different trade-offs between precision and recall.</li>
</ul>
<h3 id="precision-recall-trade-off">Precision-Recall Trade-off:<a class="headerlink" href="#precision-recall-trade-off" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Higher Threshold (e.g., 0.7):</strong></li>
<li>Predict <span class="arithmatex">\(y = 1\)</span> only if <span class="arithmatex">\(f(x) \geq 0.7\)</span>.</li>
<li>Results in higher precision but lower recall.</li>
<li>
<p>More confident predictions.</p>
</li>
<li>
<p><strong>Lower Threshold (e.g., 0.3):</strong></p>
</li>
<li>Predict <span class="arithmatex">\(y = 1\)</span> if <span class="arithmatex">\(f(x) \geq 0.3\)</span>.</li>
<li>Results in lower precision but higher recall.</li>
<li>
<p>More liberal predictions.</p>
</li>
<li>
<p><strong>Flexibility in Threshold:</strong></p>
</li>
<li>By adjusting the threshold, you can make trade-offs between precision and recall.</li>
</ul>
<h3 id="f1-score">F1 Score:<a class="headerlink" href="#f1-score" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Motivation:</strong></li>
<li>Combining precision and recall into a single score.</li>
<li>
<p>Gives more emphasis to the lower value between precision and recall.</p>
</li>
<li>
<p><strong>Formula:</strong>
  [ F1 = \frac{2}{\frac{1}{\text{Precision}} + \frac{1}{\text{Recall}}} ]</p>
</li>
<li>
<p><strong>Alternative Computation:</strong>
  [ F1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} ]</p>
</li>
<li>
<p><strong>F1 Score Characteristics:</strong></p>
</li>
<li>Emphasizes the lower value between precision and recall.</li>
<li>A way to find a balance between precision and recall.</li>
</ul>
<h3 id="choosing-the-best-threshold">Choosing the Best Threshold:<a class="headerlink" href="#choosing-the-best-threshold" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Manual Threshold Setting:</strong></li>
<li>Often done based on the application's requirements.</li>
<li>
<p>Depends on the cost of false positives and false negatives.</p>
</li>
<li>
<p><strong>Automatic Threshold Selection:</strong></p>
</li>
<li>F1 score is one way to automatically find a trade-off.</li>
<li>Helps combine precision and recall into a single metric.</li>
</ul>
<h3 id="conclusion">Conclusion:<a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h3>
<ul>
<li>There is often a trade-off between precision and recall.</li>
<li>Adjusting the threshold in logistic regression allows for different trade-offs.</li>
<li>The F1 score is a metric that combines precision and recall, giving more weight to the lower value.</li>
</ul>
<h3 id="next-week">Next Week:<a class="headerlink" href="#next-week" title="Permanent link">&para;</a></h3>
<ul>
<li>The next week will cover decision trees, another powerful machine learning technique widely used in various applications.</li>
<li>Decision trees provide a different perspective and approach to solving machine learning problems.</li>
</ul>
<p>Congratulations on completing this week's material, and looking forward to the next week!</p>
<h2 id="week-4">Week 4<a class="headerlink" href="#week-4" title="Permanent link">&para;</a></h2>
<h3 id="decision-tree-model">Decision tree model<a class="headerlink" href="#decision-tree-model" title="Permanent link">&para;</a></h3>
<p><strong>Understanding Decision Trees</strong></p>
<p>Welcome to the final week of the course on Advanced Learning Algorithms! In this week, we will explore decision trees and tree ensembles, powerful learning algorithms widely used in various applications.</p>
<h3 id="decision-trees-overview">Decision Trees Overview:<a class="headerlink" href="#decision-trees-overview" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Powerful and Widely Used:</strong></li>
<li>Decision trees and tree ensembles are powerful and widely used in practice.</li>
<li>
<p>Despite their success, decision trees haven't received as much attention in academia.</p>
</li>
<li>
<p><strong>Running Example: Cat Classification:</strong></p>
</li>
<li>To explain how decision trees work, a cat classification example is used.</li>
<li>
<p>The goal is to train a classifier to determine whether an animal is a cat based on features like ear shape, face shape, and whiskers.</p>
</li>
<li>
<p><strong>Dataset:</strong></p>
</li>
<li>10 training examples with features (ear shape, face shape, whiskers) and ground truth labels (cat or not).</li>
<li>Binary classification task with categorical features.</li>
</ul>
<h3 id="decision-tree-structure">Decision Tree Structure:<a class="headerlink" href="#decision-tree-structure" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Tree Structure:</strong></li>
<li>A decision tree is represented as a tree structure.</li>
<li>
<p>Nodes: Ovals or rectangles.</p>
<ul>
<li>Root Node: Topmost node.</li>
<li>Decision Nodes: Nodes that decide based on a feature.</li>
<li>Leaf Nodes: Nodes that make predictions.</li>
</ul>
</li>
<li>
<p><strong>Example Decision Tree:</strong></p>
</li>
<li>The example decision tree is shown, where decisions are made based on ear shape, face shape, and whiskers.</li>
<li>Starting at the root node, decisions are made by traversing down the tree based on feature values.</li>
</ul>
<h3 id="terminology">Terminology:<a class="headerlink" href="#terminology" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Root Node:</strong></li>
<li>
<p>Topmost node in the decision tree.</p>
</li>
<li>
<p><strong>Decision Nodes:</strong></p>
</li>
<li>
<p>Nodes that decide based on a feature value.</p>
</li>
<li>
<p><strong>Leaf Nodes:</strong></p>
</li>
<li>Nodes that make predictions.</li>
</ul>
<h3 id="decision-tree-examples">Decision Tree Examples:<a class="headerlink" href="#decision-tree-examples" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Multiple Trees:</strong></li>
<li>Different decision trees can be generated for the same classification task.</li>
<li>
<p>Each tree may perform differently on the training and test sets.</p>
</li>
<li>
<p><strong>Tree Variations:</strong></p>
</li>
<li>
<p>Multiple examples of decision trees are shown, each with different structures.</p>
</li>
<li>
<p><strong>Algorithm's Job:</strong></p>
</li>
<li>The decision tree learning algorithm's job is to select a tree that performs well on the training set and generalizes to new data.</li>
</ul>
<h3 id="next-steps">Next Steps:<a class="headerlink" href="#next-steps" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Learning a Decision Tree:</strong></li>
<li>How does the algorithm learn a specific decision tree from a training set?</li>
</ul>
<p>In the next video, we'll dive into the process of how decision tree learning algorithms work and how they learn specific trees from training data. Let's continue our exploration of decision trees!</p>
<h3 id="learning-process">Learning Process<a class="headerlink" href="#learning-process" title="Permanent link">&para;</a></h3>
<p><strong>Building Decision Trees: Key Steps</strong></p>
<p>In this video, we'll explore the overall process of building a decision tree, focusing on the steps involved and the key decisions made during the learning process.</p>
<h3 id="steps-in-building-a-decision-tree">Steps in Building a Decision Tree:<a class="headerlink" href="#steps-in-building-a-decision-tree" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Choose the Root Node Feature:</strong></li>
<li>Start with a training set of examples (cats and dogs).</li>
<li>Decide what feature to use at the root node (topmost node).</li>
<li>
<p>For example, select the "ear shape" feature.</p>
</li>
<li>
<p><strong>Split Based on Root Node Feature:</strong></p>
</li>
<li>Split the training examples based on the chosen feature.</li>
<li>
<p>In the example, split based on "pointy ears" and "floppy ears."</p>
</li>
<li>
<p><strong>Decide Features for Subsequent Nodes:</strong></p>
</li>
<li>Focus on each branch (left and right) separately.</li>
<li>Decide what features to use for further splitting.</li>
<li>
<p>In the left branch, choose the "face shape" feature.</p>
</li>
<li>
<p><strong>Repeat the Process:</strong></p>
</li>
<li>Continue the process, splitting and deciding features until reaching leaf nodes.</li>
<li>Create leaf nodes for making predictions (e.g., "cat" or "not cat").</li>
</ol>
<h3 id="key-decisions-in-building-decision-trees">Key Decisions in Building Decision Trees:<a class="headerlink" href="#key-decisions-in-building-decision-trees" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Choosing Splitting Features:</strong></li>
<li>Decision: Which feature to use for splitting at each node?</li>
<li>
<p>Objective: Maximize purity in subsets to get close to all cats or all dogs.</p>
</li>
<li>
<p><strong>Deciding When to Stop Splitting:</strong></p>
</li>
<li>Decision: When to stop splitting and create leaf nodes?</li>
<li>Criteria:<ul>
<li>Pure subsets (100% cats or dogs).</li>
<li>Maximum depth reached.</li>
<li>Minimum improvement in purity score.</li>
<li>Number of examples below a certain threshold.</li>
</ul>
</li>
</ol>
<h3 id="challenges-and-considerations">Challenges and Considerations:<a class="headerlink" href="#challenges-and-considerations" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Complexity of Decision Tree Learning:</strong></li>
<li>Decision trees involve several key decisions and steps.</li>
<li>
<p>Over the years, researchers proposed different refinements, resulting in a multifaceted algorithm.</p>
</li>
<li>
<p><strong>Algorithm Evolution:</strong></p>
</li>
<li>Researchers introduced modifications and criteria for splitting and stopping.</li>
<li>Despite its complexity, decision trees are effective.</li>
</ul>
<h3 id="next-steps_1">Next Steps:<a class="headerlink" href="#next-steps_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Entropy and Impurity:</strong></li>
<li>Dive deeper into the concept of entropy as a measure of impurity in a node.</li>
<li>
<p>Understand how entropy helps in making decisions during the splitting process.</p>
</li>
<li>
<p><strong>Guidance on Usage:</strong></p>
</li>
<li>Gain insights into using open source packages for decision tree implementation.</li>
<li>Receive guidance on making effective decisions in the learning process.</li>
</ul>
<h3 id="closing-note">Closing Note:<a class="headerlink" href="#closing-note" title="Permanent link">&para;</a></h3>
<ul>
<li>Decision trees might seem complicated due to various pieces, but they work well.</li>
<li>The upcoming video will delve into the concept of entropy and its role in decision tree learning.</li>
</ul>
<p>In the next video, we'll explore entropy as a measure of impurity and understand how it guides the decision-making process during the creation of decision trees. Let's continue our journey into decision tree learning!</p>
<h3 id="measuring-purity">Measuring purity<a class="headerlink" href="#measuring-purity" title="Permanent link">&para;</a></h3>
<p><strong>Measuring Purity with Entropy</strong></p>
<p>In this video, we'll explore the concept of entropy as a measure of impurity for a set of examples in the context of building decision trees.</p>
<h3 id="entropy-definition">Entropy Definition:<a class="headerlink" href="#entropy-definition" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Entropy Function (H):</strong></li>
<li><strong>Formula:</strong> <span class="arithmatex">\( H(p_1) = -p_1 \cdot \log_2(p_1) - p_0 \cdot \log_2(p_0) \)</span><ul>
<li><span class="arithmatex">\( p_1 \)</span>: Fraction of positive examples (cats).</li>
<li><span class="arithmatex">\( p_0 \)</span>: Fraction of negative examples (not cats).</li>
</ul>
</li>
<li>
<p><strong>Convention:</strong> Use base-2 logarithms for consistency.</p>
</li>
<li>
<p><strong>Graphical Representation:</strong></p>
</li>
<li><strong>Axis:</strong> Horizontal axis represents <span class="arithmatex">\( p_1 \)</span> (fraction of cats).</li>
<li><strong>Curve:</strong> Entropy curve is highest at 50-50 mix (impurity = 1).</li>
<li><strong>Purity:</strong> Entropy is 0 for all cats or all not cats (perfect purity).</li>
</ul>
<h3 id="examples">Examples:<a class="headerlink" href="#examples" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Balanced Set (3 Cats, 3 Dogs):</strong></li>
<li><span class="arithmatex">\( p_1 = \frac{3}{6} \)</span> (50% cats).</li>
<li>
<p><span class="arithmatex">\( H(p_1) = 1 \)</span> (maximal impurity at 50-50 mix).</p>
</li>
<li>
<p><strong>Mostly Cats (5 Cats, 1 Dog):</strong></p>
</li>
<li><span class="arithmatex">\( p_1 = \frac{5}{6} \)</span> (83% cats).</li>
<li>
<p><span class="arithmatex">\( H(p_1) \approx 0.65 \)</span> (lower impurity, higher purity).</p>
</li>
<li>
<p><strong>All Cats (6 Cats, 0 Dogs):</strong></p>
</li>
<li><span class="arithmatex">\( p_1 = 1 \)</span> (100% cats).</li>
<li>
<p><span class="arithmatex">\( H(p_1) = 0 \)</span> (perfect purity).</p>
</li>
<li>
<p><strong>Imbalanced Set (2 Cats, 4 Dogs):</strong></p>
</li>
<li><span class="arithmatex">\( p_1 = \frac{2}{6} \)</span> (33% cats).</li>
<li>
<p><span class="arithmatex">\( H(p_1) \approx 0.92 \)</span> (higher impurity).</p>
</li>
<li>
<p><strong>All Dogs (0 Cats, 6 Dogs):</strong></p>
</li>
<li><span class="arithmatex">\( p_1 = 0 \)</span> (0% cats).</li>
<li><span class="arithmatex">\( H(p_1) = 0 \)</span> (perfect purity).</li>
</ol>
<h3 id="entropy-function-details">Entropy Function Details:<a class="headerlink" href="#entropy-function-details" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Computational Note:</strong></li>
<li><span class="arithmatex">\( 0 \cdot \log_2(0) \)</span> conventionally considered as 0.</li>
<li>
<p>If <span class="arithmatex">\( p_1 = 0 \)</span> or <span class="arithmatex">\( p_1 = 1 \)</span>, <span class="arithmatex">\( \log_2(0) = 0 \)</span> is assumed.</p>
</li>
<li>
<p><strong>Scaling Factor:</strong></p>
</li>
<li>Use <span class="arithmatex">\( \log_2 \)</span> for simplicity.</li>
<li>
<p>Scaling factor ensures the peak of the curve is 1 (maximal impurity).</p>
</li>
<li>
<p><strong>Comparison to Logistic Loss:</strong></p>
</li>
<li>Resembles the logistic loss formula but with a different rationale.</li>
<li>Mathematical details not covered; focus on application in decision trees.</li>
</ul>
<h3 id="summary">Summary:<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Entropy Function:</strong></li>
<li>Measures impurity in a set of examples.</li>
<li>Peaks at 50-50 mix (maximal impurity).</li>
<li>
<p>Decreases to 0 for all cats or all not cats (perfect purity).</p>
</li>
<li>
<p><strong>Decision Tree Context:</strong></p>
</li>
<li>Entropy guides decisions on what features to split on during tree building.</li>
<li>Simplicity and effectiveness make it a commonly used impurity measure.</li>
</ul>
<p>In the next video, we'll delve into how entropy is used to make decisions about feature selection during the creation of decision trees. Understanding this process will enhance our grasp of decision tree learning. Let's proceed to the next step in our exploration!</p>
<h3 id="choosing-a-split-information-gain">Choosing a split: Information Gain<a class="headerlink" href="#choosing-a-split-information-gain" title="Permanent link">&para;</a></h3>
<p><strong>Choosing a Split: Information Gain</strong></p>
<p>In the process of building a decision tree, the choice of which feature to split on at a node is based on reducing entropy, a measure of impurity. This reduction in entropy is termed <strong>information gain</strong>. In this video, we'll delve into how to calculate information gain and make decisions on feature selection.</p>
<h3 id="example-splitting-on-features">Example: Splitting on Features<a class="headerlink" href="#example-splitting-on-features" title="Permanent link">&para;</a></h3>
<p>Let's revisit the decision tree example for distinguishing cats from not cats. If we consider three potential features—ear shape, face shape, and whiskers—we can evaluate the information gain associated with each.</p>
<ol>
<li><strong>Ear Shape Split:</strong></li>
<li>Left Sub-Branch: <span class="arithmatex">\( P_1 = \frac{4}{5} \)</span> (4 cats, 1 not cat).</li>
<li>Right Sub-Branch: <span class="arithmatex">\( P_1 = \frac{1}{5} \)</span> (1 cat, 4 not cats).</li>
<li>Compute entropies for both sub-branches.</li>
<li>
<p>Calculate information gain.</p>
</li>
<li>
<p><strong>Face Shape Split:</strong></p>
</li>
<li>Left Sub-Branch: <span class="arithmatex">\( P_1 = \frac{4}{7} \)</span> (4 cats, 3 not cats).</li>
<li>Right Sub-Branch: <span class="arithmatex">\( P_1 = \frac{1}{3} \)</span> (1 cat, 2 not cats).</li>
<li>Compute entropies for both sub-branches.</li>
<li>
<p>Calculate information gain.</p>
</li>
<li>
<p><strong>Whiskers Split:</strong></p>
</li>
<li>Left Sub-Branch: <span class="arithmatex">\( P_1 = \frac{3}{4} \)</span> (3 cats, 1 not cat).</li>
<li>Right Sub-Branch: <span class="arithmatex">\( P_1 = \frac{2}{6} \)</span> (1 cat, 5 not cats).</li>
<li>Compute entropies for both sub-branches.</li>
<li>Calculate information gain.</li>
</ol>
<h3 id="weighted-average-for-decision-making">Weighted Average for Decision Making<a class="headerlink" href="#weighted-average-for-decision-making" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Importance of Weighting:</strong></li>
<li>Weighted average is crucial to consider the significance of impurity reduction based on the number of examples in each sub-branch.</li>
<li>
<p>Larger subsets are more influential in decision-making.</p>
</li>
<li>
<p><strong>Decision Criterion:</strong></p>
</li>
<li>
<p>Choose the feature that yields the lowest weighted average entropy (highest information gain).</p>
</li>
<li>
<p><strong>Mathematical Formulation:</strong></p>
</li>
<li>Use the entropy at the root node minus the weighted average entropy for each potential split.</li>
<li>Decision based on maximizing information gain.</li>
</ul>
<h3 id="formal-definition-of-information-gain">Formal Definition of Information Gain<a class="headerlink" href="#formal-definition-of-information-gain" title="Permanent link">&para;</a></h3>
<p>For the example of ear shape split:</p>
<ul>
<li><span class="arithmatex">\( p_1^{\text{left}} = \frac{4}{5} \)</span>, <span class="arithmatex">\( w^{\text{left}} = \frac{5}{10} \)</span>.</li>
<li><span class="arithmatex">\( p_1^{\text{right}} = \frac{1}{5} \)</span>, <span class="arithmatex">\( w^{\text{right}} = \frac{5}{10} \)</span>.</li>
<li><span class="arithmatex">\( p_1^{\text{root}} = \frac{5}{10} \)</span> (entropy at the root node is 0.5).</li>
</ul>
<p><strong>Information Gain Formula:</strong>
[ \text{Information Gain} = \text{Entropy}(\text{Root}) - \left( w^{\text{left}} \cdot \text{Entropy}(p_1^{\text{left}}) + w^{\text{right}} \cdot \text{Entropy}(p_1^{\text{right}}) \right) ]</p>
<h3 id="decision-tree-building-algorithm">Decision Tree Building Algorithm<a class="headerlink" href="#decision-tree-building-algorithm" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Feature Selection:</strong></li>
<li>Evaluate information gain for all possible features.</li>
<li>
<p>Choose the feature with the highest information gain.</p>
</li>
<li>
<p><strong>Sub-Branch Creation:</strong></p>
</li>
<li>Split the data based on the selected feature.</li>
<li>
<p>Recursively apply the decision tree algorithm to each sub-branch.</p>
</li>
<li>
<p><strong>Stopping Criteria:</strong></p>
</li>
<li>
<p>Determine when to stop splitting (e.g., small information gain, maximum depth reached, minimum examples in a node).</p>
</li>
<li>
<p><strong>Leaf Node Prediction:</strong></p>
</li>
<li>Assign a class label (e.g., cat or not cat) to each leaf node.</li>
</ol>
<p>By following these steps, a decision tree is built to classify examples based on features, maximizing the separation between classes.</p>
<p>Understanding how to calculate information gain is fundamental to the decision tree learning process. In the next video, we'll bring together all the components discussed to outline the complete decision tree building algorithm. Let's proceed to the next step in our exploration!</p>
<h3 id="putting-it-together">Putting it together<a class="headerlink" href="#putting-it-together" title="Permanent link">&para;</a></h3>
<p><strong>Putting It Together: Decision Tree Building Process</strong></p>
<p>Building a decision tree involves a systematic process to determine the optimal features for splitting nodes and creating sub-branches. The overall algorithm is as follows:</p>
<h3 id="decision-tree-building-process">Decision Tree Building Process:<a class="headerlink" href="#decision-tree-building-process" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Root Node:</strong></li>
<li>Start with all training examples at the root node.</li>
<li>Calculate information gain for all features.</li>
<li>
<p>Choose the feature with the highest information gain for the initial split.</p>
</li>
<li>
<p><strong>Initial Split:</strong></p>
</li>
<li>Split the dataset into two subsets based on the selected feature.</li>
<li>Create left and right branches of the tree.</li>
<li>
<p>Send training examples to the appropriate sub-branches.</p>
</li>
<li>
<p><strong>Recursive Splitting:</strong></p>
</li>
<li>Repeat the splitting process for the left and right sub-branches.</li>
<li>
<p>Continue until stopping criteria are met.</p>
</li>
<li>
<p><strong>Stopping Criteria:</strong></p>
</li>
<li>Stopping criteria may include:<ul>
<li>Node is 100% of a single class.</li>
<li>Entropy is zero (maximum purity).</li>
<li>Maximum tree depth is reached.</li>
<li>Information gain from additional splits is below a threshold.</li>
<li>Number of examples in a node is below a threshold.</li>
</ul>
</li>
</ol>
<h3 id="illustration">Illustration:<a class="headerlink" href="#illustration" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Example Decision Tree:</strong></li>
<li>Start with all examples at the root.</li>
<li>Choose the feature (e.g., ear shape) with the highest information gain for the initial split.</li>
<li>Create left and right sub-branches based on the ear shape.</li>
<li>
<p>Repeat the process for each sub-branch until stopping criteria are met.</p>
</li>
<li>
<p><strong>Stopping at Leaf Nodes:</strong></p>
</li>
<li>If stopping criteria are met (e.g., all examples in a node belong to a single class), create a leaf node with a prediction.</li>
</ul>
<h3 id="recursive-algorithm">Recursive Algorithm:<a class="headerlink" href="#recursive-algorithm" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Recursion in Decision Trees:</strong></li>
<li>Building the decision tree involves recursively applying the decision tree algorithm to smaller subsets of examples.</li>
<li>
<p>Recursive algorithms involve calling the algorithm on subsets of the data.</p>
</li>
<li>
<p><strong>Example:</strong></p>
</li>
<li>Build a decision tree on the left sub-branch using a subset of five examples.</li>
<li>Build a decision tree on the right sub-branch using a subset of five examples.</li>
</ul>
<h3 id="parameter-choices">Parameter Choices:<a class="headerlink" href="#parameter-choices" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Maximum Depth Parameter:</strong></li>
<li>Larger maximum depth allows for a more complex decision tree.</li>
<li>Similar to adjusting the complexity of models (e.g., polynomial degree, neural network size).</li>
<li>Default choices may be available in open-source libraries.</li>
<li>
<p>Cross-validation can be used to fine-tune this parameter.</p>
</li>
<li>
<p><strong>Information Gain Threshold:</strong></p>
</li>
<li>Decide to stop splitting when information gain falls below a certain threshold.</li>
<li>Balances model complexity and overfitting risk.</li>
</ul>
<h3 id="making-predictions">Making Predictions:<a class="headerlink" href="#making-predictions" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Prediction Process:</strong></li>
<li>To make predictions, follow the decision path from the root to a leaf node.</li>
<li>Leaf nodes provide class predictions.</li>
</ul>
<h3 id="further-refinements">Further Refinements:<a class="headerlink" href="#further-refinements" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Cross-Validation:</strong></li>
<li>Cross-validation can be used to fine-tune parameters (e.g., maximum depth).</li>
<li>
<p>Libraries may provide automated methods for parameter selection.</p>
</li>
<li>
<p><strong>Handling Categorical Features:</strong></p>
</li>
<li>Decision trees can be extended to handle categorical features with more than two values.</li>
<li>Explore handling such cases in upcoming videos.</li>
</ul>
<p>Understanding the decision tree building process and its recursive nature is key to implementing or using decision tree algorithms. In the next videos, further refinements and extensions to decision trees will be explored</p>
<h3 id="using-one-hot-encoding-of-categorical-features">Using one-hot encoding of categorical features<a class="headerlink" href="#using-one-hot-encoding-of-categorical-features" title="Permanent link">&para;</a></h3>
<h3 id="continuous-valued-features">Continuous valued features<a class="headerlink" href="#continuous-valued-features" title="Permanent link">&para;</a></h3>
<p><strong>Using One-Hot Encoding for Categorical Features</strong></p>
<p>In machine learning, when dealing with categorical features that can take on more than two discrete values, one-hot encoding is a common technique to represent these features in a format suitable for algorithms like decision trees. This approach is particularly useful when a feature can have multiple categorical values.</p>
<h3 id="example-ear-shape-feature">Example: Ear Shape Feature<a class="headerlink" href="#example-ear-shape-feature" title="Permanent link">&para;</a></h3>
<p>Consider a new training set for a pet adoption center application where the ear shape feature can now take on three possible values: pointy, floppy, and oval.</p>
<ul>
<li><strong>Original Categorical Feature:</strong></li>
<li>Ear shape: pointy, floppy, oval.</li>
</ul>
<h3 id="one-hot-encoding">One-Hot Encoding:<a class="headerlink" href="#one-hot-encoding" title="Permanent link">&para;</a></h3>
<p>Instead of representing the ear shape as a single feature with three values, one-hot encoding creates three new binary features:</p>
<ol>
<li><strong>Pointy Ear Feature:</strong></li>
<li>
<p>1 if the animal has pointy ears, 0 otherwise.</p>
</li>
<li>
<p><strong>Floppy Ear Feature:</strong></p>
</li>
<li>
<p>1 if the animal has floppy ears, 0 otherwise.</p>
</li>
<li>
<p><strong>Oval Ear Feature:</strong></p>
</li>
<li>1 if the animal has oval ears, 0 otherwise.</li>
</ol>
<h3 id="transformation">Transformation:<a class="headerlink" href="#transformation" title="Permanent link">&para;</a></h3>
<p>For each example in the dataset, these new features are populated based on the original ear shape values:</p>
<ul>
<li>If the ear shape is <strong>pointy</strong>, the Pointy Ear Feature is set to 1, and Floppy and Oval Ear Features are set to 0.</li>
<li>If the ear shape is <strong>floppy</strong>, the Floppy Ear Feature is set to 1, and Pointy and Oval Ear Features are set to 0.</li>
<li>If the ear shape is <strong>oval</strong>, the Oval Ear Feature is set to 1, and Pointy and Floppy Ear Features are set to 0.</li>
</ul>
<h3 id="one-hot-encoding-details">One-Hot Encoding Details:<a class="headerlink" href="#one-hot-encoding-details" title="Permanent link">&para;</a></h3>
<ul>
<li>For a categorical feature with <strong>k possible values</strong>, create <strong>k binary features</strong>.</li>
<li>Each binary feature can only take on values of <strong>0 or 1</strong>.</li>
<li>In one-hot encoding, exactly <strong>one binary feature is set to 1</strong> for each example (the "hot" feature).</li>
</ul>
<h3 id="decision-tree-application">Decision Tree Application:<a class="headerlink" href="#decision-tree-application" title="Permanent link">&para;</a></h3>
<p>Once the one-hot encoding is applied, the dataset is transformed into a format suitable for a decision tree. The decision tree learning algorithm, as discussed previously, can then be applied to this data with no further modifications.</p>
<h3 id="generalization-to-neural-networks">Generalization to Neural Networks:<a class="headerlink" href="#generalization-to-neural-networks" title="Permanent link">&para;</a></h3>
<ul>
<li>One-hot encoding is not limited to decision trees. It can also be used to encode categorical features for neural networks.</li>
<li>For example, if the face shape feature is categorical (round or not round), it can be encoded as 1 or 0 using one-hot encoding.</li>
</ul>
<h3 id="conclusion_1">Conclusion:<a class="headerlink" href="#conclusion_1" title="Permanent link">&para;</a></h3>
<ul>
<li>One-hot encoding is a versatile technique for handling categorical features with more than two values.</li>
<li>It allows for the representation of categorical features in a format compatible with various machine learning algorithms, including decision trees and neural networks.</li>
</ul>
<p>In the next video, the focus will shift to handling continuous value features, exploring how decision trees can accommodate features that can take on any numerical value.</p>
<h3 id="handling-continuous-valued-features-in-decision-trees">Handling Continuous Valued Features in Decision Trees<a class="headerlink" href="#handling-continuous-valued-features-in-decision-trees" title="Permanent link">&para;</a></h3>
<p><strong>Handling Continuous Valued Features in Decision Trees</strong></p>
<p>In machine learning, decision trees are versatile algorithms that can handle both discrete and continuous valued features. When dealing with features that take on any numerical value (continuous features), modifications to the decision tree algorithm are necessary.</p>
<h3 id="example-weight-feature">Example: Weight Feature<a class="headerlink" href="#example-weight-feature" title="Permanent link">&para;</a></h3>
<p>Consider a modification to the cat adoption center dataset by adding a new feature: the weight of the animal in pounds. The weight is a continuous valued feature that can be any number.</p>
<h3 id="decision-tree-learning-algorithm-modification">Decision Tree Learning Algorithm Modification:<a class="headerlink" href="#decision-tree-learning-algorithm-modification" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Splitting Criteria:</strong></li>
<li>Instead of splitting only on discrete features (ear shape, face shape, whiskers), now consider splitting on continuous features as well (e.g., weight).</li>
<li>
<p>The decision tree learning algorithm should evaluate which feature (either discrete or continuous) provides the best information gain.</p>
</li>
<li>
<p><strong>Choosing Thresholds:</strong></p>
</li>
<li>For continuous features like weight, introduce thresholds to determine how to split the data.</li>
<li>Consider multiple threshold values and evaluate information gain for each.</li>
</ol>
<h3 id="threshold-selection">Threshold Selection:<a class="headerlink" href="#threshold-selection" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Example Thresholds for Weight:</strong></li>
<li>Weight ≤ 8</li>
<li>Weight ≤ 9</li>
<li>
<p>Weight ≤ 13</p>
</li>
<li>
<p><strong>Information Gain Calculation:</strong></p>
</li>
<li>For each threshold, calculate information gain using the standard formula.</li>
<li>Information Gain = Entropy at Root - Weighted Sum of Entropy in Subsets</li>
</ul>
<h3 id="decision-making">Decision Making:<a class="headerlink" href="#decision-making" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Select the Best Split:</strong></li>
<li>Choose the feature and threshold that result in the highest information gain.</li>
<li>
<p>Information gain helps measure the reduction in entropy, indicating the effectiveness of the split.</p>
</li>
<li>
<p><strong>Splitting the Node:</strong></p>
</li>
<li>Once the best feature and threshold are determined, split the data accordingly.</li>
<li>For example, if weight ≤ 9 provides the highest information gain, split the data into subsets based on this condition.</li>
</ul>
<h3 id="handling-multiple-thresholds">Handling Multiple Thresholds:<a class="headerlink" href="#handling-multiple-thresholds" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Iterative Testing:</strong></li>
<li>Try various thresholds along the continuous feature's range.</li>
<li>
<p>A common approach is to use midpoints between sorted values as thresholds.</p>
</li>
<li>
<p><strong>Selecting the Optimal Threshold:</strong></p>
</li>
<li>Choose the threshold that maximizes information gain.</li>
</ul>
<h3 id="building-the-tree">Building the Tree:<a class="headerlink" href="#building-the-tree" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Recursion:</strong></li>
<li>After a split, apply the same process recursively to build subtrees.</li>
<li>Continue the process for each subset of the data.</li>
</ul>
<h3 id="conclusion_2">Conclusion:<a class="headerlink" href="#conclusion_2" title="Permanent link">&para;</a></h3>
<ul>
<li>Decision trees can handle continuous valued features by introducing thresholds for splitting.</li>
<li>The decision to split on a continuous feature is based on information gain.</li>
<li>Thresholds are tested iteratively, and the one yielding the highest information gain is selected.</li>
</ul>
<p>In the next video, the focus will shift to a generalization of decision trees for regression problems, where the goal is to predict numerical values rather than discrete categories.</p>
<h3 id="regression-trees-optional">Regression Trees (optional)<a class="headerlink" href="#regression-trees-optional" title="Permanent link">&para;</a></h3>
<p><strong>Regression Trees: Predicting Numerical Values</strong></p>
<p>In the optional video, regression trees are introduced as a generalization of decision trees for predicting numerical values. Unlike classification problems where the goal is to predict discrete categories, regression problems involve predicting continuous numerical values. The example used in this video is predicting the weight of an animal based on features like ear shape and face shape.</p>
<h3 id="structure-of-a-regression-tree">Structure of a Regression Tree:<a class="headerlink" href="#structure-of-a-regression-tree" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Target Output (Y):</strong></li>
<li>
<p>The target output is a numerical value (e.g., weight) that we want to predict.</p>
</li>
<li>
<p><strong>Leaf Node Prediction:</strong></p>
</li>
<li>The prediction at a leaf node is made by averaging the target values of the training examples that reach that leaf.</li>
</ol>
<h3 id="example-regression-tree">Example Regression Tree:<a class="headerlink" href="#example-regression-tree" title="Permanent link">&para;</a></h3>
<p><img alt="Regression Tree Example" src="image_link" /><br />
<em>Illustration of a regression tree predicting animal weight based on ear shape and face shape.</em></p>
<h3 id="decision-making-process">Decision-Making Process:<a class="headerlink" href="#decision-making-process" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Choosing Splitting Feature:</strong></li>
<li>Instead of reducing entropy, regression trees aim to reduce the variance of the target values.</li>
<li>
<p>The decision to split is based on the feature that results in the most significant reduction in variance.</p>
</li>
<li>
<p><strong>Variance Calculation:</strong></p>
</li>
<li>Variance measures how much the values in a set vary from the mean.</li>
<li>
<p>The weighted average variance after a split is computed for both subsets of the data.</p>
</li>
<li>
<p><strong>Reduction in Variance:</strong></p>
</li>
<li>
<p>Similar to information gain in classification, reduction in variance is computed by measuring how much the variance decreases after a split.</p>
</li>
<li>
<p><strong>Selecting the Best Split:</strong></p>
</li>
<li>The feature and threshold that provide the largest reduction in variance are chosen for splitting.</li>
</ol>
<h3 id="illustrative-example">Illustrative Example:<a class="headerlink" href="#illustrative-example" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Split on Ear Shape:</strong></li>
<li>Calculate the variance for each subset after the split.</li>
<li>Compute the reduction in variance.</li>
<li>
<p>Choose ear shape as the splitting feature due to the largest reduction in variance.</p>
</li>
<li>
<p><strong>Recursion:</strong></p>
</li>
<li>Repeat the process for each subset of data, creating a recursive tree structure.</li>
<li>Continue splitting until further splitting does not significantly reduce variance or other stopping criteria are met.</li>
</ol>
<h3 id="importance-of-reducing-variance">Importance of Reducing Variance:<a class="headerlink" href="#importance-of-reducing-variance" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong></li>
<li>The goal is to minimize the variability of predicted values at each leaf node.</li>
<li>Reducing variance leads to more accurate predictions in regression problems.</li>
</ul>
<h3 id="summary_1">Summary:<a class="headerlink" href="#summary_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Regression trees predict numerical values by averaging target values at leaf nodes.</li>
<li>Splitting decisions are based on minimizing the variance of target values.</li>
<li>The decision tree is constructed recursively, optimizing splits for variance reduction.</li>
</ul>
<p>In the next video, the concept of ensembles of decision trees, known as random forests, will be explored. Ensembles provide enhanced predictive performance compared to individual decision trees.</p>
<h3 id="using-multiple-decision-trees">Using multiple decision trees<a class="headerlink" href="#using-multiple-decision-trees" title="Permanent link">&para;</a></h3>
<p><strong>Using Multiple Decision Trees: Tree Ensembles</strong></p>
<p>The video introduces the concept of tree ensembles as a solution to the sensitivity of a single decision tree to small changes in the data. Instead of relying on a single decision tree, an ensemble of multiple trees is built to improve robustness and predictive accuracy.</p>
<h3 id="weakness-of-single-decision-trees">Weakness of Single Decision Trees:<a class="headerlink" href="#weakness-of-single-decision-trees" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Sensitivity to Data Changes:</strong></li>
<li>A single decision tree can be highly sensitive to small changes in the training data.</li>
<li>
<p>Changing just one example in the dataset can result in a different tree structure.</p>
</li>
<li>
<p><strong>Lack of Robustness:</strong></p>
</li>
<li>The lack of robustness makes the algorithm less reliable for making predictions on new, unseen data.</li>
</ol>
<h3 id="tree-ensemble-solution">Tree Ensemble Solution:<a class="headerlink" href="#tree-ensemble-solution" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Tree Ensemble Definition:</strong></li>
<li>
<p>A tree ensemble is a collection of multiple decision trees.</p>
</li>
<li>
<p><strong>Voting Mechanism:</strong></p>
</li>
<li>Each tree in the ensemble independently makes predictions.</li>
<li>The final prediction is determined by a voting mechanism (e.g., majority vote).</li>
</ol>
<h3 id="example-tree-ensemble">Example Tree Ensemble:<a class="headerlink" href="#example-tree-ensemble" title="Permanent link">&para;</a></h3>
<p><img alt="Tree Ensemble" src="image_link" /><br />
<em>Illustration of a tree ensemble with three decision trees.</em></p>
<h3 id="voting-process">Voting Process:<a class="headerlink" href="#voting-process" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Individual Tree Predictions:</strong></li>
<li>
<p>Each tree in the ensemble independently predicts the class or value.</p>
</li>
<li>
<p><strong>Voting Mechanism:</strong></p>
</li>
<li>The final prediction is based on the majority vote of all trees.</li>
</ol>
<h3 id="robustness-improvement">Robustness Improvement:<a class="headerlink" href="#robustness-improvement" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Reduced Sensitivity:</strong></li>
<li>The ensemble approach reduces sensitivity to individual variations in the training data.</li>
<li>Changes in a single training example have a smaller impact on the overall prediction.</li>
</ol>
<h3 id="key-technique-sampling-with-replacement">Key Technique: Sampling with Replacement:<a class="headerlink" href="#key-technique-sampling-with-replacement" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Sampling with Replacement:</strong></li>
<li>A technique from statistics used to create multiple variations of the training data.</li>
<li>
<p>Each tree in the ensemble is trained on a different version of the dataset.</p>
</li>
<li>
<p><strong>Randomness in Training:</strong></p>
</li>
<li>The randomness introduced by sampling with replacement leads to diverse trees in the ensemble.</li>
</ol>
<h3 id="summary_2">Summary:<a class="headerlink" href="#summary_2" title="Permanent link">&para;</a></h3>
<ul>
<li>Tree ensembles are used to mitigate the sensitivity of individual decision trees.</li>
<li>Multiple trees independently make predictions, and a voting mechanism combines their outputs.</li>
<li>Sampling with replacement is a key technique to create diverse trees in the ensemble.</li>
</ul>
<p>In the next video, the technique of sampling with replacement will be explored further as a means to create diverse training datasets for each tree in the ensemble.</p>
<h3 id="sampling-with-replacement">Sampling with replacement<a class="headerlink" href="#sampling-with-replacement" title="Permanent link">&para;</a></h3>
<p><strong>Sampling with Replacement for Tree Ensembles</strong></p>
<p>The video introduces the concept of sampling with replacement and demonstrates the process using colored tokens. This technique is crucial for building an ensemble of trees, where diverse training sets are created to enhance the robustness and diversity of individual trees.</p>
<h3 id="sampling-with-replacement-demonstration">Sampling with Replacement Demonstration:<a class="headerlink" href="#sampling-with-replacement-demonstration" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Colored Tokens:</strong></li>
<li>Four colored tokens (red, yellow, green, blue) are used for demonstration.</li>
<li>
<p>Tokens are placed in a black velvet bag.</p>
</li>
<li>
<p><strong>Sampling Process:</strong></p>
</li>
<li>Tokens are sampled randomly with replacement.</li>
<li>
<p>After each sample, the token is placed back into the bag.</p>
</li>
<li>
<p><strong>Example Sequence:</strong></p>
</li>
<li>Example sequence: Green, yellow, blue, blue.</li>
<li>
<p>The sequence may contain repeated tokens due to replacement.</p>
</li>
<li>
<p><strong>Relevance to Tree Ensemble:</strong></p>
</li>
<li>The demonstration illustrates the concept of creating diverse samples with replacement.</li>
</ol>
<h3 id="application-to-tree-ensemble">Application to Tree Ensemble:<a class="headerlink" href="#application-to-tree-ensemble" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Theoretical Bag:</strong></li>
<li>
<p>The training examples (cats and dogs) are considered in a theoretical bag.</p>
</li>
<li>
<p><strong>Creating a New Training Set:</strong></p>
</li>
<li>Randomly pick training examples from the bag with replacement.</li>
<li>
<p>Examples are put back into the bag after each pick.</p>
</li>
<li>
<p><strong>Repeats and Differences:</strong></p>
</li>
<li>The resulting training set may contain repeated examples.</li>
<li>
<p>Not all original examples may be included in the new set.</p>
</li>
<li>
<p><strong>Diversity in Training Sets:</strong></p>
</li>
<li>The process creates multiple random training sets, each slightly different.</li>
<li>This diversity is crucial for building an ensemble of trees.</li>
</ol>
<h3 id="importance-of-sampling-with-replacement">Importance of Sampling with Replacement:<a class="headerlink" href="#importance-of-sampling-with-replacement" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Robustness and Diversity:</strong></li>
<li>Sampling with replacement ensures that individual trees in the ensemble see different variations of the data.</li>
<li>Robustness and diversity lead to more reliable and accurate predictions.</li>
</ol>
<h3 id="next-steps_2">Next Steps:<a class="headerlink" href="#next-steps_2" title="Permanent link">&para;</a></h3>
<ul>
<li>The video sets the stage for using sampling with replacement to build an ensemble of trees.</li>
<li>The following video is expected to cover how these diverse training sets contribute to the construction of an ensemble of decision trees.</li>
</ul>
<p>The technique of sampling with replacement is a key element in creating diverse training datasets for individual trees in a tree ensemble, contributing to the overall robustness of the algorithm.</p>
<h3 id="random-forest-algorithm">Random forest algorithm<a class="headerlink" href="#random-forest-algorithm" title="Permanent link">&para;</a></h3>
<p><strong>Random Forest Algorithm</strong></p>
<p>The video introduces the random forest algorithm, a powerful tree ensemble method that outperforms single decision trees. The algorithm involves creating multiple decision trees by sampling with replacement from the original training set and training a decision tree on each sampled set.</p>
<h3 id="random-forest-algorithm_1">Random Forest Algorithm:<a class="headerlink" href="#random-forest-algorithm_1" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Sampling with Replacement:</strong></li>
<li>Given a training set of size M, repeat the following B times (B is the number of trees in the ensemble).</li>
<li>Sample with replacement to create a new training set of size M.</li>
<li>
<p>Train a decision tree on the new training set.</p>
</li>
<li>
<p><strong>Ensemble Building:</strong></p>
</li>
<li>
<p>After repeating the process B times, an ensemble of B decision trees is obtained.</p>
</li>
<li>
<p><strong>Voting for Predictions:</strong></p>
</li>
<li>When making predictions, each tree in the ensemble votes on the final prediction.</li>
<li>
<p>The majority vote or averaged prediction is considered the final prediction.</p>
</li>
<li>
<p><strong>Parameter B:</strong></p>
</li>
<li>The number of trees (B) is a parameter to be chosen.</li>
<li>
<p>Common values for B are around 64, 128, or 100.</p>
</li>
<li>
<p><strong>Bagged Decision Tree:</strong></p>
</li>
<li>This specific instance of tree ensemble creation is sometimes referred to as a bagged decision tree.</li>
<li>The term "bag" signifies the sampling with replacement procedure.</li>
</ol>
<h3 id="enhancements-randomization-of-feature-choice">Enhancements: Randomization of Feature Choice<a class="headerlink" href="#enhancements-randomization-of-feature-choice" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Randomization of Feature Choice:</strong></li>
<li>To further randomize the algorithm, a modification is introduced.</li>
<li>At each node, instead of choosing from all N features, a random subset of K features (K &lt; N) is selected.</li>
<li>
<p>The split is chosen from this subset of features.</p>
</li>
<li>
<p><strong>Parameter K:</strong></p>
</li>
<li>
<p>A typical choice for K is the square root of N, where N is the total number of features.</p>
</li>
<li>
<p><strong>Advantages:</strong></p>
</li>
<li>Randomizing feature choice at each node increases diversity among the trees in the ensemble.</li>
<li>This modification results in the Random Forest algorithm.</li>
</ol>
<h3 id="robustness-of-random-forest">Robustness of Random Forest:<a class="headerlink" href="#robustness-of-random-forest" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Robustness through Diversity:</strong></li>
<li>Sampling with replacement explores small changes to the data, and averaging over these changes increases robustness.</li>
<li>The algorithm becomes less sensitive to individual variations in the training set.</li>
</ol>
<h3 id="joke-closure">Joke Closure:<a class="headerlink" href="#joke-closure" title="Permanent link">&para;</a></h3>
<ul>
<li>A light-hearted joke about camping in a random forest is shared.</li>
</ul>
<h3 id="next-steps_3">Next Steps:<a class="headerlink" href="#next-steps_3" title="Permanent link">&para;</a></h3>
<ul>
<li>The video concludes by mentioning that while random forests are effective, boosted decision trees, specifically the XGBoost algorithm, can perform even better. The next video will cover XGBoost.</li>
</ul>
<p>The random forest algorithm is an ensemble method that leverages the strength of multiple decision trees to achieve improved accuracy and robustness compared to individual trees. The algorithm's randomization techniques contribute to its effectiveness in diverse machine learning applications.</p>
<h3 id="xgboost">XGBoost<a class="headerlink" href="#xgboost" title="Permanent link">&para;</a></h3>
<p><strong>XGBoost (Extreme Gradient Boosting)</strong></p>
<p>The video introduces the XGBoost algorithm, an extremely popular and powerful implementation of boosted decision trees. XGBoost is known for its efficiency, speed, and success in machine learning competitions. The algorithm uses deliberate practice, focusing on examples where the current ensemble of trees performs poorly, to iteratively improve the model.</p>
<h3 id="key-concepts">Key Concepts:<a class="headerlink" href="#key-concepts" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Boosting:</strong></li>
<li>Boosting involves building decision trees sequentially, focusing on examples where the current model performs poorly.</li>
<li>
<p>The goal is to iteratively correct errors made by the previous trees.</p>
</li>
<li>
<p><strong>Modification to Bagged Decision Tree Algorithm:</strong></p>
</li>
<li>Instead of equal probability sampling with replacement, increase the probability of selecting misclassified examples.</li>
<li>
<p>This modification is inspired by the concept of deliberate practice.</p>
</li>
<li>
<p><strong>Boosting Procedure:</strong></p>
</li>
<li>After building a decision tree, evaluate its performance on the original training set.</li>
<li>Assign higher probabilities to examples misclassified by the current ensemble.</li>
<li>
<p>Repeat this process for a total of B times (B is the number of trees).</p>
</li>
<li>
<p><strong>Mathematical Details:</strong></p>
</li>
<li>The mathematical details of adjusting probabilities are complex but handled internally by the boosting algorithm.</li>
<li>
<p>Practitioners using XGBoost do not need to delve into these details.</p>
</li>
<li>
<p><strong>XGBoost:</strong></p>
</li>
<li>XGBoost (Extreme Gradient Boosting) is a widely used and open-source implementation of boosted trees.</li>
<li>Known for its efficiency, speed, and success in machine learning competitions.</li>
<li>
<p>Handles regularization internally to prevent overfitting.</p>
</li>
<li>
<p><strong>Application in Competitions:</strong></p>
</li>
<li>XGBoost is often used in machine learning competitions, including platforms like Kaggle.</li>
<li>
<p>Competitively performs against other algorithms, and XGBoost and deep learning algorithms are frequent winners.</p>
</li>
<li>
<p><strong>Technical Note:</strong></p>
</li>
<li>Instead of sampling with replacement, XGBoost assigns different weights to training examples.</li>
<li>
<p>This weight assignment contributes to the efficiency of the algorithm.</p>
</li>
<li>
<p><strong>Implementation in Python:</strong></p>
</li>
<li>To use XGBoost in Python, import the library and initialize the model as an XGBoost classifier or regressor.</li>
<li>The model can be trained and used for predictions similarly to other machine learning models.</li>
</ol>
<h3 id="code-example-classification">Code Example (Classification):<a class="headerlink" href="#code-example-classification" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="c1"># Initialize XGBoost classifier</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">()</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a><span class="c1"># Train the model</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a><span class="c1"># Make predictions</span>
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="code-example-regression">Code Example (Regression):<a class="headerlink" href="#code-example-regression" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a><span class="c1"># Initialize XGBoost regressor</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBRegressor</span><span class="p">()</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a><span class="c1"># Train the model</span>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a>
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a><span class="c1"># Make predictions</span>
<a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="conclusion_3">Conclusion:<a class="headerlink" href="#conclusion_3" title="Permanent link">&para;</a></h3>
<ul>
<li>XGBoost is a versatile and effective algorithm for both classification and regression tasks.</li>
<li>It is known for its competitiveness in machine learning competitions.</li>
<li>The details of the boosting procedure and weight assignments are handled internally.</li>
</ul>
<h3 id="next-steps_4">Next Steps:<a class="headerlink" href="#next-steps_4" title="Permanent link">&para;</a></h3>
<ul>
<li>The final video in the course will wrap up the content and discuss when to choose a decision tree versus a neural network in different scenarios.</li>
</ul>
<h3 id="when-to-use-decision-trees">When to use decision trees<a class="headerlink" href="#when-to-use-decision-trees" title="Permanent link">&para;</a></h3>
<p><strong>When to Use Decision Trees vs. Neural Networks</strong></p>
<p><strong>Decision Trees and Tree Ensembles:</strong>
- <strong>Applicability:</strong> Well-suited for tabular or structured data, common in tasks involving categorical or continuous features.
  - Examples: Housing price prediction, classification, regression.
- <strong>Limitation:</strong> Not recommended for unstructured data (images, video, audio, text).
- <strong>Training Speed:</strong> Decision trees, including tree ensembles, are generally fast to train.
- <strong>Interpretability:</strong> Small decision trees can be human-interpretable, facilitating understanding of decision-making.
- <strong>Algorithm Choice:</strong> XGBoost is a popular choice for tree ensembles due to its efficiency and competitiveness.</p>
<p><strong>Neural Networks:</strong>
- <strong>Versatility:</strong> Effective on all types of data, including structured and unstructured data, and mixed data.
- <strong>Competitive Edge:</strong> Neural networks excel on unstructured data such as images, video, audio, and text.
- <strong>Training Time:</strong> Larger neural networks may be slower to train compared to decision trees.
- <strong>Transfer Learning:</strong> Neural networks support transfer learning, crucial for small datasets to leverage pre-training on larger datasets.
- <strong>System Integration:</strong> Easier to integrate and train multiple neural networks within a system compared to multiple decision trees.</p>
<p><strong>Decision Factors:</strong>
- <strong>Data Type:</strong> Choose decision trees for structured/tabular data and neural networks for unstructured or mixed data.
- <strong>Computational Budget:</strong> Decision trees may be preferred if computational resources are constrained.
- <strong>Interpretability:</strong> Consider using decision trees when interpretability is crucial, especially for smaller trees.
- <strong>System Integration:</strong> Neural networks might be preferred in systems involving multiple models due to easier training.</p>
<p><strong>Course Wrap-up:</strong>
- <strong>Completion:</strong> Congratulations on completing the Advanced Learning Algorithms course.
- <strong>Learning Highlights:</strong> Explored both neural networks and decision trees with practical tips.
- <strong>Next Steps:</strong> Consider the upcoming unsupervised learning course for further exploration.</p>
<p><strong>Closing Remark:</strong>
- <strong>Wish for Success:</strong> Best of luck with the practice labs.
- <strong>Star Wars Reference:</strong> "May the forest be with you" (a play on "May the Force be with you").</p>
<h1 id="-completed-">- - Completed - -<a class="headerlink" href="#-completed-" title="Permanent link">&para;</a></h1>


  




                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../ML%20Specialization%20-%20Course%201/" class="md-footer__link md-footer__link--prev" aria-label="Previous: ML Specialization - Course 1" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              ML Specialization - Course 1
            </div>
          </div>
        </a>
      
      
        
        <a href="../ML%20Specialization%20-%20Course%203/" class="md-footer__link md-footer__link--next" aria-label="Next: ML Specialization - Course 3" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              ML Specialization - Course 3
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2020-2024 – Jawad Haider  <a href="#__consent">Change cookie settings</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/qalmaqihir" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.linkedin.com/in/jawad-haider-uca/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.youtube.com/channel/UCB-D3NBU6UZ5N7IGKOJxtqQ" target="_blank" rel="noopener" title="www.youtube.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://t.me/Qalmaqihir" target="_blank" rel="noopener" title="t.me" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M248 8C111.033 8 0 119.033 0 256s111.033 248 248 248 248-111.033 248-248S384.967 8 248 8Zm114.952 168.66c-3.732 39.215-19.881 134.378-28.1 178.3-3.476 18.584-10.322 24.816-16.948 25.425-14.4 1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25 5.342-39.5 3.652-3.793 67.107-61.51 68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608 69.142-14.845 10.194-26.894 9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7 18.45-13.7 108.446-47.248 144.628-62.3c68.872-28.647 83.183-33.623 92.511-33.789 2.052-.034 6.639.474 9.61 2.885a10.452 10.452 0 0 1 3.53 6.716 43.765 43.765 0 0 1 .417 9.769Z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/qalmaqihir" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://instagram.com/qalmaqihir" target="_blank" rel="noopener" title="instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["search.share", "search.highlight", "search.sugguest", "content.code.annotate", "content.tooltips", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.e21f8df8.min.js"></script>
      
    
  </body>
</html>