
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="All of my Computer Science & AI/ML/DL/ Book notes, BootCamp notes & Useful materials for anyone who  wants to learn; Knowledge should be free for those who need it.">
      
      
        <meta name="author" content="Jawad Haider">
      
      
        <link rel="canonical" href="https://qalmaqihir.github.io/corecs/ML_Specialization/ML%20Specialization%20-%20Course%201/">
      
      <link rel="icon" href="../../../assets/logo.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-8.5.3">
    
    
      
        <title>ML Specialization - Course 1 - CS Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.80dcb947.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ml-specialization-course-1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
Follow us on
<a href="https://twitter.com/qalmaqihir" target="_blank" rel="noopener">
    <span class="twemoji twitter" style="color: #1DA1F2">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
            <path
                d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z">
            </path>
        </svg>
    </span>
    <strong>Twitter</strong>
</a> and

<a href="https://www.youtube.com/channel/UCB-D3NBU6UZ5N7IGKOJxtqQ?sub_confirmation=1" target="_blank" rel="noopener">
    <span class="twemoji youtube" style="color: #FF0000;">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512">
            <path
                d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z">
            </path>
        </svg>
    </span>
    <strong>YouTube</strong>
</a>  and stay <strong>Updated !</strong> for New Content

<!-- We use the twemoji project for the pancake emoji symbol
and for other emoji and icon purposes.
Check them out here: https://github.com/twitter/twemoji
-->

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="CS Notes" class="md-header__button md-logo" aria-label="CS Notes" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CS Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ML Specialization - Course 1
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/qalmaqihir/qalmaqihir.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    qalmaqihir/qalmaqihir.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        Home
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../booksnotes/" class="md-tabs__link">
        Books Notes
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../bootcampsnotes/" class="md-tabs__link">
        Bootcamps Notes
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../competitiveprogramming/" class="md-tabs__link">
        Competitive Programming
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../" class="md-tabs__link md-tabs__link--active">
        Core Computer Science
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../myresearch/" class="md-tabs__link">
        My Research
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../about/" class="md-tabs__link">
        About
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../blogs/" class="md-tabs__link">
        Blog
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../contact/" class="md-tabs__link">
        Contact
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="CS Notes" class="md-nav__button md-logo" aria-label="CS Notes" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    CS Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/qalmaqihir/qalmaqihir.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    qalmaqihir/qalmaqihir.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
        
          
            
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../..">Home</a>
          
            <label for="__nav_1">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Home" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Home
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/" class="md-nav__link">
        Books Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/" class="md-nav__link">
        Bootcamps Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../competitiveprogramming/" class="md-nav__link">
        Competitive Programming
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        Core Computer Science
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../myresearch/" class="md-nav__link">
        My Research
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../blogs/" class="md-nav__link">
        Blogs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../contact/" class="md-nav__link">
        Contact
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
        
          
            
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../booksnotes/">Books Notes</a>
          
            <label for="__nav_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Books Notes" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Books Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
        
          
            
          
        
          
            
          
        
          
        
          
        
          
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../booksnotes/pythonDataScienceHandBook/">Data Science Handbook</a>
          
            <label for="__nav_2_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Data Science Handbook" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          Data Science Handbook
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_3" type="checkbox" id="__nav_2_2_3" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2_3">
          Chapter 2 - Introduction to Numpy
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Chapter 2 - Introduction to Numpy" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_3">
          <span class="md-nav__icon md-icon"></span>
          Chapter 2 - Introduction to Numpy
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/" class="md-nav__link">
        00 Understanding Data Types in Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/" class="md-nav__link">
        01 basics of numpy arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/" class="md-nav__link">
        02 Computation on NumPy Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/" class="md-nav__link">
        03 Aggregation Min Max
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/" class="md-nav__link">
        04 Computation on Arrays Broadcasting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/" class="md-nav__link">
        05 Comparisons  Masks and Boolean Logic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/" class="md-nav__link">
        06 Fany Indexing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/" class="md-nav__link">
        07 Sorted Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/" class="md-nav__link">
        08  Structured Data
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_4" type="checkbox" id="__nav_2_2_4" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2_4">
          Chapter 3 - Data Manipulation with Pandas
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Chapter 3 - Data Manipulation with Pandas" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_4">
          <span class="md-nav__icon md-icon"></span>
          Chapter 3 - Data Manipulation with Pandas
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/" class="md-nav__link">
        01 Introduction to Pandas Objects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/" class="md-nav__link">
        02 Data Indexing and Selection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/" class="md-nav__link">
        03 Operating on Data in Pandas
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/" class="md-nav__link">
        04 Handling Missing Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/" class="md-nav__link">
        05 Hierarchical Indexing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/" class="md-nav__link">
        06 Combine dataset Concat and Append
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/" class="md-nav__link">
        07 Combining Dataset Merge and Join
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/" class="md-nav__link">
        08 Aggregation and Grouping
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/" class="md-nav__link">
        09 Pivot Tables
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/" class="md-nav__link">
        10 Vectoried String Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/" class="md-nav__link">
        11 Working with Time Series
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/" class="md-nav__link">
        12 Frequency Offset
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/" class="md-nav__link">
        13 Resampling Shifting and Windowing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/" class="md-nav__link">
        Example Visualizing Seattle Bicycle Counts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/" class="md-nav__link">
        Example  Birthrate Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/" class="md-nav__link">
        Example  US States Data (Merge and Join)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/" class="md-nav__link">
        Example Recipe Database
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_5" type="checkbox" id="__nav_2_2_5" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2_5">
          Chapter 4 - Visualization with Matplotlib
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Chapter 4 - Visualization with Matplotlib" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_5">
          <span class="md-nav__icon md-icon"></span>
          Chapter 4 - Visualization with Matplotlib
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/" class="md-nav__link">
        01general Matplotlib tips
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/" class="md-nav__link">
        02simple lineplots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/" class="md-nav__link">
        03simple scatter plots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/" class="md-nav__link">
        04visualizing errors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/" class="md-nav__link">
        05density and contour plots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/" class="md-nav__link">
        06Histograms Binnings and Density
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/" class="md-nav__link">
        07customized plot legends
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/" class="md-nav__link">
        08customizing colorbar
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/" class="md-nav__link">
        09multiple subplots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/" class="md-nav__link">
        10text and annotation Example
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/" class="md-nav__link">
        11customizing ticks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/" class="md-nav__link">
        12customizing matplotlib configuration and stylesheets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/" class="md-nav__link">
        13threedimensional plotting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/" class="md-nav__link">
        14 geographic data with basemap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/" class="md-nav__link">
        15visualiztion with seaborn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/" class="md-nav__link">
        example California cities
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/" class="md-nav__link">
        Example Exploring Marathon Finishing times
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/" class="md-nav__link">
        Example Handwritten Digits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/" class="md-nav__link">
        Example surface temperature data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/" class="md-nav__link">
        Example Visualizing a Mobius Strip
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
      
        
          
            
          
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../booksnotes/pythonDataScienceHandBook/">Machine Learning Handbook</a>
          
            <label for="__nav_2_3">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Machine Learning Handbook" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning Handbook
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../bootcampsnotes/">Bootcamps Notes</a>
          
            <label for="__nav_3">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Bootcamps Notes" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Bootcamps Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_2">
          Numpy Crash Course
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Numpy Crash Course" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          Numpy Crash Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2_1" type="checkbox" id="__nav_3_2_1" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_2_1">
          Numpy Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Numpy Topics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_2_1">
          <span class="md-nav__icon md-icon"></span>
          Numpy Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/00-NumPy-Arrays/" class="md-nav__link">
        00 NumPy Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/" class="md-nav__link">
        01 NumPy Indexing and Selection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/02-NumPy-Operations/" class="md-nav__link">
        02 NumPy Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/03-NumPy-Exercises/" class="md-nav__link">
        03 NumPy Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/" class="md-nav__link">
        04 NumPy Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_3" type="checkbox" id="__nav_3_3" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_3">
          Pandas Crash Course
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Pandas Crash Course" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          Pandas Crash Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_3_1" type="checkbox" id="__nav_3_3_1" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_3_1">
          Pandas Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Pandas Topics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_3_1">
          <span class="md-nav__icon md-icon"></span>
          Pandas Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/00-Intro-to-Pandas/" class="md-nav__link">
        00 Intro to Pandas
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/01-Series/" class="md-nav__link">
        01 Series
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/02-DataFrames/" class="md-nav__link">
        02 DataFrames
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/03-Missing-Data/" class="md-nav__link">
        03 Missing Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/04-Groupby/" class="md-nav__link">
        04 Groupby
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/05-Operations/" class="md-nav__link">
        05 Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/06-Data-Input-and-Output/" class="md-nav__link">
        06 Data Input and Output
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/07-Pandas-Exercises/" class="md-nav__link">
        07 Pandas Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/" class="md-nav__link">
        08 Pandas Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_4" type="checkbox" id="__nav_3_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_4">
          PyTorch Baics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="PyTorch Baics" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          PyTorch Baics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_4_1" type="checkbox" id="__nav_3_4_1" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_4_1">
          PyTorch Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="PyTorch Topics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_4_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/00-Tensor-Basics/" class="md-nav__link">
        00 Tensor Basics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/01-Tensor-Operations/" class="md-nav__link">
        01 Tensor Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/" class="md-nav__link">
        02 PyTorch Basics Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/" class="md-nav__link">
        03 PyTorch Basics Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5" type="checkbox" id="__nav_3_5" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5">
          PyTorch for Deeplearning Bootcamp
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="PyTorch for Deeplearning Bootcamp" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          PyTorch for Deeplearning Bootcamp
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_1" type="checkbox" id="__nav_3_5_1" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_1">
          ANN - Artificial Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ANN - Artificial Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_1">
          <span class="md-nav__icon md-icon"></span>
          ANN - Artificial Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/" class="md-nav__link">
        00 PyTorch Gradients
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/" class="md-nav__link">
        01 Linear Regression with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/" class="md-nav__link">
        02 DataSets with Pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/" class="md-nav__link">
        03 Basic PyTorch NN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/" class="md-nav__link">
        04a Full ANN Code Along Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/" class="md-nav__link">
        04b Full ANN Code Along Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/" class="md-nav__link">
        05 Neural Network Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/" class="md-nav__link">
        06 Neural Network Exercises Solutions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/" class="md-nav__link">
        07 Recap Saving and Loading Trained Models
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_2" type="checkbox" id="__nav_3_5_2" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_2">
          CNN - Convolutional Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CNN - Convolutional Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_2">
          <span class="md-nav__icon md-icon"></span>
          CNN - Convolutional Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/" class="md-nav__link">
        00 MNIST ANN Code Along
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/" class="md-nav__link">
        01 MNIST with CNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/" class="md-nav__link">
        02 CIFAR CNN Code Along
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/" class="md-nav__link">
        03 Loading Real Image Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/" class="md-nav__link">
        04 CNN on Custom Images
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/" class="md-nav__link">
        05 CNN Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/" class="md-nav__link">
        06 CNN Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_3" type="checkbox" id="__nav_3_5_3" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_3">
          RNN - Recurrent Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="RNN - Recurrent Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_3">
          <span class="md-nav__icon md-icon"></span>
          RNN - Recurrent Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/" class="md-nav__link">
        00 Basic RNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/" class="md-nav__link">
        01 RNN on a Time Series
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/" class="md-nav__link">
        02 RNN Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/" class="md-nav__link">
        03 RNN Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_4" type="checkbox" id="__nav_3_5_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_4">
          Using GPU
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Using GPU" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_4">
          <span class="md-nav__icon md-icon"></span>
          Using GPU
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/" class="md-nav__link">
        00 Using GPU and CUDA
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_5" type="checkbox" id="__nav_3_5_5" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_5">
          NLP with PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP with PyTorch" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_5">
          <span class="md-nav__icon md-icon"></span>
          NLP with PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/" class="md-nav__link">
        00 RNN for Text Generation 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6" type="checkbox" id="__nav_3_6" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6">
          Tensorflow for Deeplearning Bootcamp
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tensorflow for Deeplearning Bootcamp" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          Tensorflow for Deeplearning Bootcamp
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_1" type="checkbox" id="__nav_3_6_1" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_1">
          Colab Basics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Colab Basics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_1">
          <span class="md-nav__icon md-icon"></span>
          Colab Basics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Demo/" class="md-nav__link">
        TF2 0 Demo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Installing_Tensorflow/" class="md-nav__link">
        TF2 0 Installing Tensorflow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/" class="md-nav__link">
        TF2 0 Loading Data
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_2" type="checkbox" id="__nav_3_6_2" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_2">
          Machine Learning Basics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Machine Learning Basics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_2">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning Basics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Classification/" class="md-nav__link">
        TF2 0 Linear Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Regression/" class="md-nav__link">
        TF2 0 Linear Regression
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_3" type="checkbox" id="__nav_3_6_3" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_3">
          ANN - Artificial Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ANN - Artificial Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_3">
          <span class="md-nav__icon md-icon"></span>
          ANN - Artificial Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_MNIST/" class="md-nav__link">
        TF2 0 ANN MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_Regression/" class="md-nav__link">
        TF2 0 ANN Regression
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_4" type="checkbox" id="__nav_3_6_4" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_4">
          CNN - Convolutional Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CNN - Convolutional Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_4">
          <span class="md-nav__icon md-icon"></span>
          CNN - Convolutional Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_Fashion_MNIST/" class="md-nav__link">
        TF2 0 Fashion MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR/" class="md-nav__link">
        TF2 0 CIFAR
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR_Improved/" class="md-nav__link">
        TF2 0 CIFAR Improved
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_5" type="checkbox" id="__nav_3_6_5" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_5">
          RNN - Recurrent Neural Network
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="RNN - Recurrent Neural Network" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_5">
          <span class="md-nav__icon md-icon"></span>
          RNN - Recurrent Neural Network
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Autoregressive_Model/" class="md-nav__link">
        TF2 0 Autoregressive Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_SimpleRNN_Sine/" class="md-nav__link">
        TF2 0 SimpleRNN Sine
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_Shapes/" class="md-nav__link">
        TF2 0 RNN Shapes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_LSTM_Nonlinear/" class="md-nav__link">
        TF2 0 LSTM Nonlinear
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Long_Distance/" class="md-nav__link">
        TF2 0 Long Distance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_MNIST/" class="md-nav__link">
        TF2 0 RNN MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Stock_Returns/" class="md-nav__link">
        TF2 0 Stock Returns
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_6" type="checkbox" id="__nav_3_6_6" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_6">
          NLP - Natural Language Processing
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP - Natural Language Processing" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_6">
          <span class="md-nav__icon md-icon"></span>
          NLP - Natural Language Processing
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Text_Preprocessing/" class="md-nav__link">
        TF2 0 Text Preprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_CNN/" class="md-nav__link">
        TF2 0 Spam Detection CNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_RNN/" class="md-nav__link">
        TF2 0 Spam Detection RNN
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_7" type="checkbox" id="__nav_3_6_7" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_7">
          Recommendar Systems
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Recommendar Systems" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_7">
          <span class="md-nav__icon md-icon"></span>
          Recommendar Systems
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RecommenderSystem/TF2_0_Recommender_System/" class="md-nav__link">
        TF2 0 Recommender System
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_8" type="checkbox" id="__nav_3_6_8" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_8">
          Transfer Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Transfer Learning" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_8">
          <span class="md-nav__icon md-icon"></span>
          Transfer Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning/" class="md-nav__link">
        TF2 0 Transfer Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning_with_Data_Augmentation/" class="md-nav__link">
        TF2 0 Transfer Learning with Data Augmentation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_9" type="checkbox" id="__nav_3_6_9" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_9">
          GANs - Generative Adversarial Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="GANs - Generative Adversarial Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_9">
          <span class="md-nav__icon md-icon"></span>
          GANs - Generative Adversarial Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/GANs/TF2_0_GAN/" class="md-nav__link">
        TF2 0 GAN
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_10" type="checkbox" id="__nav_3_6_10" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_10">
          Advance Tensorflow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Advance Tensorflow" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_10">
          <span class="md-nav__icon md-icon"></span>
          Advance Tensorflow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Serving/" class="md-nav__link">
        TF2 0 Serving
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Mirrored_Strategy/" class="md-nav__link">
        TF2 0 Mirrored Strategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_TFLite/" class="md-nav__link">
        TF2 0 TFLite
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TPU/" class="md-nav__link">
        TPU
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_11" type="checkbox" id="__nav_3_6_11" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_11">
          Low-level Transorflow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Low-level Transorflow" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_11">
          <span class="md-nav__icon md-icon"></span>
          Low-level Transorflow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Basic_Computation/" class="md-nav__link">
        TF2 0 Basic Computation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Variables_and_Gradient_Tape/" class="md-nav__link">
        TF2 0 Variables and Gradient Tape
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Build_Your_Own_Model/" class="md-nav__link">
        TF2 0 Build Your Own Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../competitiveprogramming/">Competitive Programming</a>
          
        </div>
      
      <nav class="md-nav" aria-label="Competitive Programming" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Competitive Programming
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../">Core Computer Science</a>
          
            <label for="__nav_5">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Core Computer Science" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Core Computer Science
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_2">
          ML Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ML Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          ML Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        ML Specialization - Course 1
      </a>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ML%20Specialization%20-%20Course%202/" class="md-nav__link">
        ML Specialization - Course 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ML%20Specialization%20-%20Course%203/" class="md-nav__link">
        ML Specialization - Course 3
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3" type="checkbox" id="__nav_5_3" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3">
          NLP Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          NLP Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%201/" class="md-nav__link">
        NLP Specialization - Course 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%202/" class="md-nav__link">
        NLP Specialization - Course 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%203/" class="md-nav__link">
        NLP Specialization - Course 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%204/" class="md-nav__link">
        NLP Specialization - Course 4
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_4" type="checkbox" id="__nav_5_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_4">
          IBM Data Science Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="IBM Data Science Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_4">
          <span class="md-nav__icon md-icon"></span>
          IBM Data Science Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Coming Soon" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_5" type="checkbox" id="__nav_5_5" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_5">
          NLP Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_5">
          <span class="md-nav__icon md-icon"></span>
          NLP Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Coming Soon" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../myresearch/">My Research</a>
          
        </div>
      
      <nav class="md-nav" aria-label="My Research" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          My Research
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7">
          About
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="About" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          About
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      
      
        
          
            
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../blogs/">Blog</a>
          
            <label for="__nav_8">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Blog" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Blog
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_2" type="checkbox" id="__nav_8_2" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_2">
          2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="2022" data-md-level="2">
        <label class="md-nav__title" for="__nav_8_2">
          <span class="md-nav__icon md-icon"></span>
          2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../blogs/2022/How-to-use-these-resources/" class="md-nav__link">
        How-to-use-these-resources
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../blogs/2022/How-to-contribute/" class="md-nav__link">
        How to contribute
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_9" type="checkbox" id="__nav_9" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9">
          Contact
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Contact" data-md-level="1">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          Contact
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../contact/" class="md-nav__link">
        Contact
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/qalmaqihir/qalmaqihir.github.io/edit/master/docs/corecs/ML_Specialization/ML Specialization - Course 1.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


<h1 id="ml-specialization-course-1">ML Specialization - Course 1<a class="headerlink" href="#ml-specialization-course-1" title="Permanent link">&para;</a></h1>
<h1 id="supervised-machine-learning-regression-and-classification">Supervised Machine Learning: Regression and Classification<a class="headerlink" href="#supervised-machine-learning-regression-and-classification" title="Permanent link">&para;</a></h1>
<p>Note 2023-12-21T06.17.08</p>
<p>========================</p>
<h1 id="week-1">Week 1<a class="headerlink" href="#week-1" title="Permanent link">&para;</a></h1>
<h2 id="what-is-machine-learning">What is Machine Learning?<a class="headerlink" href="#what-is-machine-learning" title="Permanent link">&para;</a></h2>
<h3 id="definition-by-arthur-samuel">Definition by Arthur Samuel<a class="headerlink" href="#definition-by-arthur-samuel" title="Permanent link">&para;</a></h3>
<ul>
<li>Arthur Samuel's definition: "Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed."</li>
<li>Example: Checkers playing program developed by Samuel in the 1950s.</li>
<li>The program learned by playing tens of thousands of games against itself, improving over time.</li>
<li>Importance of giving learning algorithms more opportunities to learn.</li>
</ul>
<h3 id="key-takeaways">Key Takeaways<a class="headerlink" href="#key-takeaways" title="Permanent link">&para;</a></h3>
<ul>
<li>Machine learning enables computers to learn without explicit programming.</li>
<li>Learning algorithms improve through experience and exposure to data.</li>
<li>The effectiveness of learning algorithms increases with more learning opportunities.</li>
</ul>
<h2 id="supervised-learning-part-1">Supervised Learning Part 1<a class="headerlink" href="#supervised-learning-part-1" title="Permanent link">&para;</a></h2>
<h3 id="introduction-to-supervised-learning">Introduction to Supervised Learning<a class="headerlink" href="#introduction-to-supervised-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>Machine learning's economic value is primarily through supervised learning (about 99%).</li>
<li>Supervised learning involves algorithms learning mappings from input (x) to output (y).</li>
<li>The algorithm is trained using examples that include correct output labels (y).</li>
</ul>
<h3 id="applications-of-supervised-learning">Applications of Supervised Learning<a class="headerlink" href="#applications-of-supervised-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>Examples: Spam filter, speech recognition, machine translation, online advertising, self-driving cars, visual inspection in manufacturing.</li>
<li>Supervised learning involves training models with labeled data and predicting outputs for new, unseen inputs.</li>
</ul>
<h3 id="housing-price-prediction-example">Housing Price Prediction Example<a class="headerlink" href="#housing-price-prediction-example" title="Permanent link">&para;</a></h3>
<ul>
<li>Regression: Predicting a number from infinitely many possibilities (e.g., house prices).</li>
<li>Different algorithms (straight line, curve) for predicting house prices.</li>
<li>Supervised learning involves predicting outputs based on input features.</li>
</ul>
<h3 id="key-takeaways_1">Key Takeaways<a class="headerlink" href="#key-takeaways_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Supervised learning predicts output labels based on input features.</li>
<li>Two main types: Regression (predicting numbers) and Classification (predicting categories).</li>
<li>Applications range from spam filtering to self-driving cars.</li>
</ul>
<h2 id="supervised-learning-part-2">Supervised Learning Part 2<a class="headerlink" href="#supervised-learning-part-2" title="Permanent link">&para;</a></h2>
<h3 id="classification-in-supervised-learning">Classification in Supervised Learning<a class="headerlink" href="#classification-in-supervised-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>Classification predicts categories from a limited set of possible outputs.</li>
<li>Example: Breast cancer detection, where tumors are classified as benign (0) or malignant (1).</li>
</ul>
<h3 id="multi-class-classification">Multi-Class Classification<a class="headerlink" href="#multi-class-classification" title="Permanent link">&para;</a></h3>
<ul>
<li>More than two categories are possible in some cases (e.g., types of cancer).</li>
<li>Each category is assigned a distinct label (0, 1, 2).</li>
<li>Categories can also be non-numeric (e.g., cat vs. dog).</li>
</ul>
<h3 id="multi-feature-input">Multi-Feature Input<a class="headerlink" href="#multi-feature-input" title="Permanent link">&para;</a></h3>
<ul>
<li>Multiple input features can be used for predictions (e.g., tumor size and patient's age).</li>
<li>Learning algorithm determines a boundary to classify inputs based on features.</li>
</ul>
<h3 id="key-takeaways_2">Key Takeaways<a class="headerlink" href="#key-takeaways_2" title="Permanent link">&para;</a></h3>
<ul>
<li>Classification predicts categories from a small set of possible outputs.</li>
<li>Multi-class classification handles more than two categories.</li>
<li>Input features can include multiple dimensions for more accurate predictions.</li>
</ul>
<h2 id="unsupervised-learning-part-1">Unsupervised Learning Part 1<a class="headerlink" href="#unsupervised-learning-part-1" title="Permanent link">&para;</a></h2>
<h3 id="introduction-to-unsupervised-learning">Introduction to Unsupervised Learning<a class="headerlink" href="#introduction-to-unsupervised-learning" title="Permanent link">&para;</a></h3>
<h3 id="overview-of-unsupervised-learning">Overview of Unsupervised Learning<a class="headerlink" href="#overview-of-unsupervised-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>Widely used after supervised learning.</li>
<li>Data lacks output labels (y); the goal is to find patterns or structure.</li>
<li>Example: Clustering algorithm groups similar data points.</li>
</ul>
<h3 id="clustering-in-unsupervised-learning">Clustering in Unsupervised Learning<a class="headerlink" href="#clustering-in-unsupervised-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>Data points are grouped into clusters based on similarities.</li>
<li>Example applications: Google News grouping articles, genetic/DNA data clustering.</li>
</ul>
<h3 id="market-segmentation-example">Market Segmentation Example<a class="headerlink" href="#market-segmentation-example" title="Permanent link">&para;</a></h3>
<ul>
<li>Unsupervised learning for grouping customers into market segments.</li>
<li>Deep learning dot AI community example: segments based on motivation.</li>
</ul>
<h3 id="key-takeaways_3">Key Takeaways<a class="headerlink" href="#key-takeaways_3" title="Permanent link">&para;</a></h3>
<ul>
<li>Unsupervised learning works without output labels (y).</li>
<li>Clustering groups similar data points together.</li>
<li>Applications include market segmentation and community categorization.</li>
</ul>
<h2 id="unsupervised-learning-part-2">Unsupervised Learning Part 2<a class="headerlink" href="#unsupervised-learning-part-2" title="Permanent link">&para;</a></h2>
<h3 id="types-of-unsupervised-learning">Types of Unsupervised Learning<a class="headerlink" href="#types-of-unsupervised-learning" title="Permanent link">&para;</a></h3>
<h3 id="formal-definition-of-unsupervised-learning">Formal Definition of Unsupervised Learning<a class="headerlink" href="#formal-definition-of-unsupervised-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>Data includes only inputs (x), without output labels (y).</li>
<li>The algorithm's task is to find patterns, structures, or interesting features in the data.</li>
</ul>
<h3 id="other-types-of-unsupervised-learning">Other Types of Unsupervised Learning<a class="headerlink" href="#other-types-of-unsupervised-learning" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Anomaly Detection:</strong></li>
<li>Detects unusual events or patterns in data.</li>
<li>
<p>Critical for fraud detection and other applications.</p>
</li>
<li>
<p><strong>Dimensionality Reduction:</strong></p>
</li>
<li>Reduces the dimensionality of large datasets while preserving information.</li>
<li>Useful for compressing data and improving efficiency.</li>
</ol>
<h3 id="conclusion-on-unsupervised-learning">Conclusion on Unsupervised Learning<a class="headerlink" href="#conclusion-on-unsupervised-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>Unsupervised learning includes clustering, anomaly detection, and dimensionality reduction.</li>
<li>Jupyter Notebooks are essential tools for machine learning exploration.</li>
</ul>
<h3 id="key-takeaways_4">Key Takeaways<a class="headerlink" href="#key-takeaways_4" title="Permanent link">&para;</a></h3>
<ul>
<li>Unsupervised learning finds patterns without labeled output data.</li>
<li>Types include clustering, anomaly detection, and dimensionality reduction.</li>
<li>Jupyter Notebooks are valuable tools for machine learning exploration.    </li>
</ul>
<h2 id="linear-regression-model-part-1">Linear Regression Model Part 1<a class="headerlink" href="#linear-regression-model-part-1" title="Permanent link">&para;</a></h2>
<h3 id="introduction-to-supervised-learning_1">Introduction to Supervised Learning<a class="headerlink" href="#introduction-to-supervised-learning_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Supervised learning involves training a model using data with known answers.</li>
<li>Linear Regression is a widely used algorithm for predicting numerical values.</li>
</ul>
<h3 id="problem-scenario">Problem Scenario<a class="headerlink" href="#problem-scenario" title="Permanent link">&para;</a></h3>
<ul>
<li>Predicting house prices based on the size of the house.</li>
<li>Example dataset from Portland, USA, with house sizes and prices.</li>
</ul>
<h3 id="linear-regression-overview">Linear Regression Overview<a class="headerlink" href="#linear-regression-overview" title="Permanent link">&para;</a></h3>
<ul>
<li>Fit a straight line to the data for predictions.</li>
<li>Fundamental concepts applicable to various machine learning models.</li>
</ul>
<h3 id="regression-vs-classification">Regression vs Classification<a class="headerlink" href="#regression-vs-classification" title="Permanent link">&para;</a></h3>
<ul>
<li>Linear regression predicts numerical values (e.g., house prices).</li>
<li>Classification predicts discrete categories (e.g., cat or dog).</li>
</ul>
<h3 id="data-visualization">Data Visualization<a class="headerlink" href="#data-visualization" title="Permanent link">&para;</a></h3>
<ul>
<li>Plotting house sizes vs. prices on a graph.</li>
<li>Understanding data points and their significance.</li>
</ul>
<h3 id="data-representation">Data Representation<a class="headerlink" href="#data-representation" title="Permanent link">&para;</a></h3>
<ul>
<li>Dataset comprises input features (size) and output targets (price).</li>
<li>Each row corresponds to a house, forming a training set.</li>
</ul>
<h3 id="notation-introduction">Notation Introduction<a class="headerlink" href="#notation-introduction" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Training Set:</strong> Data used to train the model.</li>
<li><strong>Notation:</strong> </li>
<li>Input feature: <span class="arithmatex">\(\(x\)\)</span></li>
<li>Output variable (target): <span class="arithmatex">\(\(y\)\)</span></li>
<li>Training examples denoted as <span class="arithmatex">\(\((x^{(i)}, y^{(i)})\)\)</span></li>
</ul>
<h3 id="terminology-recap">Terminology Recap<a class="headerlink" href="#terminology-recap" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Supervised Learning:</strong> Training with labeled data.</li>
<li><strong>Regression Model:</strong> Predicts numerical values.</li>
<li><strong>Classification Model:</strong> Predicts discrete categories.</li>
</ul>
<h2 id="linear-regression-model-part-2">Linear Regression Model Part 2<a class="headerlink" href="#linear-regression-model-part-2" title="Permanent link">&para;</a></h2>
<h3 id="supervised-learning-process">Supervised Learning Process<a class="headerlink" href="#supervised-learning-process" title="Permanent link">&para;</a></h3>
<ul>
<li>A training set includes input features and output targets.</li>
<li>The goal is to train a model <span class="arithmatex">\((\(f\))\)</span> to make predictions.</li>
</ul>
<h3 id="model-function-f">Model Function (<span class="arithmatex">\(f\)</span>)<a class="headerlink" href="#model-function-f" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\(\(f\)\)</span> takes input <span class="arithmatex">\(\(x\)\)</span> and produces an estimate <span class="arithmatex">\((\(y\))\)</span>.</li>
<li><span class="arithmatex">\(\(y\)\)</span> is the model's prediction for the target variable.</li>
</ul>
<h3 id="linear-regression-function">Linear Regression Function<a class="headerlink" href="#linear-regression-function" title="Permanent link">&para;</a></h3>
<ul>
<li>Linear function:<span class="arithmatex">\(\(f_w, b(x) = wx + b\)\)</span></li>
<li><span class="arithmatex">\(\(w\)\)</span> and <span class="arithmatex">\(<span class="arithmatex">\(\(b\)\)</span>\)</span> are parameters determining the line.</li>
<li>Linear Regression predicts based on a straight-line function.</li>
</ul>
<h3 id="univariate-linear-regression">Univariate Linear Regression<a class="headerlink" href="#univariate-linear-regression" title="Permanent link">&para;</a></h3>
<ul>
<li>One variable (<span class="arithmatex">\(\(x\)\)</span>) in the model.</li>
<li>Simplicity and ease of use make linear regression foundational.</li>
</ul>
<h3 id="choice-of-linear-function">Choice of Linear Function<a class="headerlink" href="#choice-of-linear-function" title="Permanent link">&para;</a></h3>
<ul>
<li>Linear functions are simple, facilitating initial understanding.</li>
<li>Foundation for transitioning to more complex, non-linear models.</li>
</ul>
<h3 id="cost-function">Cost Function<a class="headerlink" href="#cost-function" title="Permanent link">&para;</a></h3>
<ul>
<li>Critical for making linear regression work.</li>
<li>Universal concept in machine learning.</li>
<li>Explored further in the next video.</li>
</ul>
<h3 id="optional-lab">Optional Lab<a class="headerlink" href="#optional-lab" title="Permanent link">&para;</a></h3>
<ul>
<li>Introduction to defining a straight-line function in Python.</li>
<li>Experimenting with values of <span class="arithmatex">\(\(w\)\)</span> and <span class="arithmatex">\(<span class="arithmatex">\(\(b\)\)</span>\)</span> to fit training data.</li>
</ul>
<p><strong>Key Takeaways:</strong>
- Supervised learning involves training models with labeled data.
- Linear Regression predicts numerical values using a straight-line function.
- Notation (<span class="arithmatex">\(x\)</span>, <span class="arithmatex">\(y\)</span>, <span class="arithmatex">\((x^{(i)}, y^{(i)})\)</span>) is crucial for describing training sets.
- Linear functions (<span class="arithmatex">\(f_w, b(x)\)</span>) form the basis of Linear Regression.
- Understanding cost functions is essential for model optimization. </p>
<h2 id="cost-function-formula">Cost Function Formula<a class="headerlink" href="#cost-function-formula" title="Permanent link">&para;</a></h2>
<h3 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h3>
<ul>
<li>Implementing linear regression requires defining a cost function.</li>
<li>The cost function evaluates how well the model fits the training data.</li>
</ul>
<h3 id="linear-model-and-parameters">Linear Model and Parameters<a class="headerlink" href="#linear-model-and-parameters" title="Permanent link">&para;</a></h3>
<ul>
<li>Linear function: <span class="arithmatex">\(<span class="arithmatex">\(\(f_w, b(x) = wx + b\)\)</span>\)</span></li>
<li><span class="arithmatex">\(\(w\)\)</span> and <span class="arithmatex">\(<span class="arithmatex">\(\(b\)\)</span>\)</span> are parameters (coefficients or weights) adjusted during training.</li>
</ul>
<h3 id="visualization-of-linear-functions">Visualization of Linear Functions<a class="headerlink" href="#visualization-of-linear-functions" title="Permanent link">&para;</a></h3>
<ul>
<li>Different values of <span class="arithmatex">\(\(w\)\)</span> and <span class="arithmatex">\(\(b\)\)</span> yield different lines on a graph.</li>
<li>Understanding the impact of parameters on the function <span class="arithmatex">\(f(x)\)</span>.</li>
</ul>
<h3 id="error-measurement">Error Measurement<a class="headerlink" href="#error-measurement" title="Permanent link">&para;</a></h3>
<ul>
<li>Error (<span class="arithmatex">\(y - y\)</span>) measures the difference between predicted and target values.</li>
<li>Squaring the error gives a positive value and emphasizes larger errors.</li>
</ul>
<h3 id="cost-function-construction">Cost Function Construction<a class="headerlink" href="#cost-function-construction" title="Permanent link">&para;</a></h3>
<ul>
<li>Squared Error Cost Function:
  [ J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (f_w(x^i) - y<sup>i)</sup>2 ]</li>
<li><span class="arithmatex">\(m\)</span> is the number of training examples.</li>
</ul>
<h3 id="intuition-behind-the-cost-function">Intuition Behind the Cost Function<a class="headerlink" href="#intuition-behind-the-cost-function" title="Permanent link">&para;</a></h3>
<ul>
<li>Minimizing the cost function means minimizing the squared errors.</li>
<li>The sum is taken over all training examples.</li>
<li>Division by <span class="arithmatex">\(2m\)</span> for convention and mathematical convenience.</li>
</ul>
<h3 id="why-squared-error">Why Squared Error?<a class="headerlink" href="#why-squared-error" title="Permanent link">&para;</a></h3>
<ul>
<li>Commonly used for linear regression and regression problems.</li>
<li>The cost function measures the average squared difference between predictions and true values.</li>
</ul>
<h3 id="cost-function-notation">Cost Function Notation<a class="headerlink" href="#cost-function-notation" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\( J(w, b) \)</span> is the cost function, also denoted as <span class="arithmatex">\( J_{wb} \)</span>.</li>
<li>Aims to find values of <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(\(b\)\)</span> that minimize the cost.</li>
</ul>
<p><strong>Key Takeaways:</strong>
- The cost function measures the fit between the linear model and the training data.
- Squared error is used to penalize larger errors and emphasize accuracy.
- Minimizing the cost function involves adjusting parameters <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(\(b\)\)</span>.
- Division by <span class="arithmatex">\(2m\)</span> is a convention for simplicity; cost function still works without it.</p>
<h2 id="intuition-behind-cost-function">Intuition Behind Cost Function<a class="headerlink" href="#intuition-behind-cost-function" title="Permanent link">&para;</a></h2>
<h3 id="visualizing-cost-function-for-simplified-linear-regression">Visualizing Cost Function for Simplified Linear Regression<a class="headerlink" href="#visualizing-cost-function-for-simplified-linear-regression" title="Permanent link">&para;</a></h3>
<h3 id="recap">Recap<a class="headerlink" href="#recap" title="Permanent link">&para;</a></h3>
<ul>
<li>Linear regression involves fitting a straight line to training data.</li>
<li>The model: <span class="arithmatex">\( f_w, b(x) = wx + b \)</span> with parameters <span class="arithmatex">\( w \)</span> and <span class="arithmatex">\( b \)</span>.</li>
<li>Cost function <span class="arithmatex">\( J \)</span> measures the difference between predictions and true values.</li>
</ul>
<h3 id="simplified-model">Simplified Model<a class="headerlink" href="#simplified-model" title="Permanent link">&para;</a></h3>
<ul>
<li>Using <span class="arithmatex">\( f_w(x) = wx \)</span> to simplify and visualize the cost function.</li>
<li>Goal: Minimize <span class="arithmatex">\( J(w) \)</span> by finding the best value for <span class="arithmatex">\( w \)</span>.</li>
</ul>
<h3 id="graphical-representation">Graphical Representation<a class="headerlink" href="#graphical-representation" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\( f_w(x) \)</span> on the left: <span class="arithmatex">\( x \)</span> vs <span class="arithmatex">\( y \)</span> for different <span class="arithmatex">\( w \)</span>.</li>
<li><span class="arithmatex">\( J(w) \)</span> on the right: <span class="arithmatex">\( w \)</span> vs <span class="arithmatex">\( J \)</span> to visualize the cost for different <span class="arithmatex">\( w \)</span> values.</li>
</ul>
<h3 id="example-w-1">Example: <span class="arithmatex">\( w = 1 \)</span><a class="headerlink" href="#example-w-1" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\( f_1(x) = x \)</span> results in a line passing through the origin.</li>
<li>Cost function <span class="arithmatex">\( J(1) = 0 \)</span> since <span class="arithmatex">\( f_1(x) \)</span> perfectly fits the training data.</li>
</ul>
<h3 id="example-w-05">Example: <span class="arithmatex">\( w = 0.5 \)</span><a class="headerlink" href="#example-w-05" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\( f_{0.5}(x) = 0.5x \)</span> results in a line with a smaller slope.</li>
<li>Calculating <span class="arithmatex">\( J(0.5) \)</span> involves computing squared errors for each data point.</li>
</ul>
<h3 id="example-w-0">Example: <span class="arithmatex">\( w = 0 \)</span><a class="headerlink" href="#example-w-0" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\( f_0(x) \)</span> is a horizontal line at <span class="arithmatex">\( y = 0 \)</span>.</li>
<li><span class="arithmatex">\( J(0) \)</span> is calculated based on squared errors, resulting in a non-zero cost.</li>
</ul>
<h3 id="visualization-of-cost-function">Visualization of Cost Function<a class="headerlink" href="#visualization-of-cost-function" title="Permanent link">&para;</a></h3>
<ul>
<li>Plotting <span class="arithmatex">\( J(w) \)</span> for different <span class="arithmatex">\( w \)</span> values.</li>
<li>Each <span class="arithmatex">\( w \)</span> corresponds to a point on <span class="arithmatex">\( J(w) \)</span>, representing the cost for that parameter.</li>
</ul>
<h3 id="choosing-w-to-minimize-j">Choosing <span class="arithmatex">\( w \)</span> to Minimize <span class="arithmatex">\( J \)</span><a class="headerlink" href="#choosing-w-to-minimize-j" title="Permanent link">&para;</a></h3>
<ul>
<li>The goal is to find <span class="arithmatex">\( w \)</span> that minimizes <span class="arithmatex">\( J(w) \)</span>.</li>
<li>Smaller <span class="arithmatex">\( J \)</span> indicates a better fit between the model and data.</li>
</ul>
<h3 id="generalization-to-linear-regression">Generalization to Linear Regression<a class="headerlink" href="#generalization-to-linear-regression" title="Permanent link">&para;</a></h3>
<ul>
<li>In linear regression with <span class="arithmatex">\( w \)</span> and <span class="arithmatex">\( b \)</span>, find values that minimize <span class="arithmatex">\( J(w, b) \)</span>.</li>
<li>The cost function helps identify parameters for the best-fitting line.</li>
</ul>
<h3 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h3>
<ul>
<li>Visualized how <span class="arithmatex">\( J(w) \)</span> changes for different <span class="arithmatex">\( w \)</span> in a simplified linear regression.</li>
<li>Objective: Minimize <span class="arithmatex">\( J(w) \)</span> to improve the model's fit to the data.</li>
<li>Next video: Explore the cost function in the full version of linear regression with <span class="arithmatex">\( w \)</span> and <span class="arithmatex">\( b \)</span>.</li>
</ul>
<h2 id="visualizing-the-cost-function-and-introduction-to-gradient-descent">Visualizing the Cost Function and Introduction to Gradient Descent<a class="headerlink" href="#visualizing-the-cost-function-and-introduction-to-gradient-descent" title="Permanent link">&para;</a></h2>
<h2 id="visualizing-the-cost-function">Visualizing the Cost Function<a class="headerlink" href="#visualizing-the-cost-function" title="Permanent link">&para;</a></h2>
<h3 id="model-overview">Model Overview<a class="headerlink" href="#model-overview" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Components:</strong> Model, parameters (w and b), cost function (J of w, b)</li>
<li><strong>Objective:</strong> Minimize the cost function J of w, b over parameters w and b.</li>
</ul>
<h3 id="visualization">Visualization<a class="headerlink" href="#visualization" title="Permanent link">&para;</a></h3>
<ul>
<li>Previous visualization with b set to zero.</li>
<li>Return to the original model with both parameters w and b.</li>
<li>Explore the model function f(x) and its relation to the cost function J of w, b.</li>
<li>Training set example: house sizes and prices.</li>
<li>Illustration of a suboptimal model with specific values for w and b.</li>
</ul>
<h3 id="3d-surface-plot">3D Surface Plot<a class="headerlink" href="#3d-surface-plot" title="Permanent link">&para;</a></h3>
<ul>
<li>Cost function J of w, b in three dimensions.</li>
<li>Resembles a soup bowl or a hammock.</li>
<li>Each point on the surface represents a specific choice of w and b.</li>
<li>3D surface plot provides a comprehensive view of the cost function.</li>
</ul>
<h3 id="contour-plot">Contour Plot<a class="headerlink" href="#contour-plot" title="Permanent link">&para;</a></h3>
<ul>
<li>An alternative visualization of the cost function J.</li>
<li>Horizontal slices of the 3D surface plot.</li>
<li>Ellipses or ovals represent points with the same value of J.</li>
<li>The minimum of the bowl is at the center of the smallest oval.</li>
<li>Contour plots offer a 2D representation of the 3D cost function.</li>
</ul>
<h2 id="gradient-descent">Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permanent link">&para;</a></h2>
<h3 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Purpose:</strong> Systematically find values of w and b that minimize the cost function J of w, b.</li>
<li><strong>Applicability:</strong> Widely used in machine learning, including advanced models like neural networks.</li>
</ul>
<h3 id="algorithm">Algorithm<a class="headerlink" href="#algorithm" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong> Minimize the cost function J of w, b for linear regression and more general functions.</li>
<li><strong>Initialization:</strong> Start with initial guesses for w and b (commonly set to 0).</li>
<li><strong>Update Rule for w:</strong> <span class="arithmatex">\(w \leftarrow w - \alpha \frac{d}{dw}J(w, b)\)</span></li>
<li><strong>Update Rule for b:</strong> <span class="arithmatex">\(b \leftarrow b - \alpha \frac{d}{db}J(w, b)\)</span></li>
<li><strong>Learning Rate (<span class="arithmatex">\(\alpha\)</span>):</strong> Controls the size of the steps in the descent process.</li>
<li><strong>Iterative Process:</strong> Repeat the update steps until convergence.</li>
</ul>
<h3 id="intuition">Intuition<a class="headerlink" href="#intuition" title="Permanent link">&para;</a></h3>
<ul>
<li>Gradient descent aims to move downhill efficiently in the cost function landscape.</li>
<li>Visual analogy of standing on a hill and taking steps in the steepest downhill direction.</li>
<li>Multiple steps of gradient descent illustrated in finding local minima.</li>
</ul>
<h3 id="implementation">Implementation<a class="headerlink" href="#implementation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Simultaneous Update:</strong> Update both parameters w and b simultaneously.</li>
<li><strong>Correct Implementation:</strong> Use temp variables to store updated values and then assign them.</li>
</ul>
<h3 id="key-takeaways_5">Key Takeaways<a class="headerlink" href="#key-takeaways_5" title="Permanent link">&para;</a></h3>
<ul>
<li>Gradient descent is a fundamental optimization algorithm in machine learning.</li>
<li>The learning rate (<span class="arithmatex">\(\alpha\)</span>) and simultaneous updates are critical for effective implementation.</li>
<li>Visualization aids understanding of cost function landscapes and descent process.</li>
</ul>
<h2 id="next-steps">Next Steps<a class="headerlink" href="#next-steps" title="Permanent link">&para;</a></h2>
<ul>
<li>Explore specific choices of w and b in linear regression models.</li>
<li>Dive into the mathematical expressions for implementing gradient descent. </li>
</ul>
<h2 id="gradient-descent-intuition">Gradient Descent Intuition<a class="headerlink" href="#gradient-descent-intuition" title="Permanent link">&para;</a></h2>
<h3 id="overview_1">Overview<a class="headerlink" href="#overview_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong> Understand the intuition behind gradient descent.</li>
<li><strong>Algorithm Recap:</strong> <span class="arithmatex">\(W \leftarrow W - \alpha \frac{d}{dW}J(W)\)</span></li>
<li><span class="arithmatex">\(W\)</span>: Model parameters.</li>
<li><span class="arithmatex">\(\alpha\)</span>: Learning rate.</li>
<li><span class="arithmatex">\(\frac{d}{dW}J(W)\)</span>: Derivative of cost function <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(W\)</span>.</li>
</ul>
<h3 id="derivative-and-tangent-lines">Derivative and Tangent Lines<a class="headerlink" href="#derivative-and-tangent-lines" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Derivative Term (<span class="arithmatex">\(\frac{d}{dW}J(W)\)</span>):</strong></li>
<li>Represents the slope of the cost function at a given point.</li>
<li>Positive slope (<span class="arithmatex">\(\frac{d}{dW}J(W) &gt; 0\)</span>) implies moving left (decreasing <span class="arithmatex">\(W\)</span>).</li>
<li>Negative slope (<span class="arithmatex">\(\frac{d}{dW}J(W) &lt; 0\)</span>) implies moving right (increasing <span class="arithmatex">\(W\)</span>).</li>
</ul>
<h3 id="examples">Examples<a class="headerlink" href="#examples" title="Permanent link">&para;</a></h3>
<h4 id="case-1-positive-slope">Case 1: Positive Slope<a class="headerlink" href="#case-1-positive-slope" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Initial Point:</strong> <span class="arithmatex">\(W\)</span> at a specific location.</li>
<li><strong>Derivative:</strong> Positive, indicating an upward slope.</li>
<li><strong>Update:</strong> <span class="arithmatex">\(W \leftarrow W - \alpha \cdot \text{Positive}\)</span></li>
<li><strong>Effect:</strong> Decrease <span class="arithmatex">\(W\)</span>, moving left on the graph.</li>
<li><strong>Explanation:</strong> When the derivative is positive, the algorithm takes a step in the direction that reduces the cost, aligning with the goal of reaching the minimum.</li>
</ul>
<h4 id="case-2-negative-slope">Case 2: Negative Slope<a class="headerlink" href="#case-2-negative-slope" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Initial Point:</strong> <span class="arithmatex">\(W\)</span> at a different location.</li>
<li><strong>Derivative:</strong> Negative, indicating a downward slope.</li>
<li><strong>Update:</strong> <span class="arithmatex">\(W \leftarrow W - \alpha \cdot \text{Negative}\)</span></li>
<li><strong>Effect:</strong> Increase <span class="arithmatex">\(W\)</span>, moving right on the graph.</li>
<li><strong>Explanation:</strong> When the derivative is negative, the algorithm adjusts <span class="arithmatex">\(W\)</span> in the opposite direction, facilitating movement toward the minimum.</li>
</ul>
<h3 id="learning-rate-alpha">Learning Rate (<span class="arithmatex">\(\alpha\)</span>)<a class="headerlink" href="#learning-rate-alpha" title="Permanent link">&para;</a></h3>
<h4 id="small-learning-rate">Small Learning Rate<a class="headerlink" href="#small-learning-rate" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Effect:</strong> Tiny steps in parameter space.</li>
<li><strong>Outcome:</strong> Slow convergence; many steps needed.</li>
<li><strong>Explanation:</strong> A small learning rate results in cautious adjustments, leading to gradual convergence. While it ensures stability, it requires more iterations to reach the minimum.</li>
</ul>
<h4 id="large-learning-rate">Large Learning Rate<a class="headerlink" href="#large-learning-rate" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Effect:</strong> Large steps in parameter space.</li>
<li><strong>Outcome:</strong> Risk of overshooting; may fail to converge.</li>
<li><strong>Explanation:</strong> A large learning rate accelerates convergence but poses a risk of overshooting the minimum. If too large, the algorithm may oscillate or diverge instead of converging.</li>
</ul>
<h3 id="handling-local-minima">Handling Local Minima<a class="headerlink" href="#handling-local-minima" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>At Local Minimum:</strong> Derivative (<span class="arithmatex">\(\frac{d}{dW}J(W)\)</span>) becomes zero.</li>
<li><strong>Effect:</strong> <span class="arithmatex">\(W\)</span> remains unchanged; no update.</li>
<li><strong>Ensures:</strong> Convergence at local minima.</li>
<li><strong>Explanation:</strong> When the derivative is zero, indicating a local minimum, the algorithm does not update <span class="arithmatex">\(W\)</span>, ensuring stability at the minimum.</li>
</ul>
<h3 id="learning-rate-selection">Learning Rate Selection<a class="headerlink" href="#learning-rate-selection" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Critical Decision:</strong> Choosing an appropriate <span class="arithmatex">\(\alpha\)</span>.</li>
<li><strong>Small <span class="arithmatex">\(\alpha\)</span>:</strong> Slow convergence.</li>
<li><strong>Large <span class="arithmatex">\(\alpha\)</span>:</strong> Risk of overshooting, potential non-convergence.</li>
<li><strong>Explanation:</strong> Selecting the right learning rate is crucial; a balance must be struck between convergence speed and stability. Trial and error, coupled with monitoring algorithm behavior, helps in finding the optimal <span class="arithmatex">\(\alpha\)</span>.</li>
</ul>
<h3 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Intuition:</strong> Derivative guides the direction of parameter updates based on the slope of the cost function.</li>
<li><strong>Learning Rate:</strong> Balancing act between speed and stability.</li>
<li><strong>Adaptability:</strong> Gradient descent naturally adjusts step size near minima to ensure convergence.</li>
</ul>
<h2 id="learning-rate-in-gradient-descent">Learning Rate in Gradient Descent<a class="headerlink" href="#learning-rate-in-gradient-descent" title="Permanent link">&para;</a></h2>
<h3 id="importance-of-learning-rate">Importance of Learning Rate<a class="headerlink" href="#importance-of-learning-rate" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Significance:</strong> Influences convergence speed and stability.</li>
<li><strong>Control Mechanism:</strong> Governs the step size in parameter space.</li>
<li><strong>Explanation:</strong> The learning rate plays a crucial role in determining how quickly the algorithm converges and whether it remains stable throughout the process.</li>
</ul>
<h3 id="small-learning-rate_1">Small Learning Rate<a class="headerlink" href="#small-learning-rate_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Effect:</strong> Tiny steps in parameter space.</li>
<li>
<p><strong>Outcome:</strong> Slow convergence.</p>
</li>
<li>
<p><strong>Challenge:</strong> Requires numerous steps to approach the minimum.</p>
</li>
<li><strong>Explanation:</strong> A small learning rate ensures cautious updates, minimizing the risk of overshooting but extending the time needed for convergence.</li>
</ul>
<h3 id="large-learning-rate_1">Large Learning Rate<a class="headerlink" href="#large-learning-rate_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Effect:</strong> Large steps in parameter space.</li>
<li><strong>Outcome:</strong> Risk of overshooting the minimum.</li>
<li><strong>Challenge:</strong> May lead to non-convergence; divergence from the minimum.</li>
<li><strong>Explanation:</strong> A large learning rate accelerates convergence but may lead to instability, causing the algorithm to overshoot the minimum or fail to converge.</li>
</ul>
<h3 id="finding-the-right-balance">Finding the Right Balance<a class="headerlink" href="#finding-the-right-balance" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Goal:</strong> Optimal learning rate for efficient convergence.</li>
<li><strong>Trial and Error:</strong> Iterative adjustment based on algorithm behavior.</li>
<li><strong>Monitoring Convergence:</strong> Observe cost function reduction and parameter changes.</li>
<li><strong>Explanation:</strong> Striking the right balance involves experimentation and continuous monitoring. Iterative adjustments are made to find an optimal learning rate that ensures both speed and stability.</li>
</ul>
<h3 id="impact-on-convergence">Impact on Convergence<a class="headerlink" href="#impact-on-convergence" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Appropriate <span class="arithmatex">\(\alpha\)</span>:</strong> Efficient convergence to the minimum.</li>
<li><strong>Small <span class="arithmatex">\(\alpha\)</span>:</strong> Caution: Slow progress.</li>
<li><strong>Large <span class="arithmatex">\(\alpha\)</span>:</strong> Caution: Risk of instability and non-convergence.</li>
<li><strong>Explanation:</strong> The choice of learning rate significantly impacts convergence. An appropriate <span class="arithmatex">\(\alpha\)</span> leads to efficient convergence, while extremes (too small or too large) pose challenges.</li>
</ul>
<h3 id="handling-local-minima_1">Handling Local Minima<a class="headerlink" href="#handling-local-minima_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Automatic Adjustment:</strong> As the algorithm approaches a local minimum.</li>
<li><strong>Derivative Impact:</strong> Decreasing slope results in smaller steps.</li>
<li><strong>Ensures Stability:</strong> Prevents overshooting and divergence.</li>
<li><strong>Explanation:</strong> Automatic adjustment of step size as the algorithm nears a local minimum ensures stability. Decreasing slope corresponds to smaller steps, preventing overshooting and promoting convergence.</li>
</ul>
<h3 id="conclusion_1">Conclusion<a class="headerlink" href="#conclusion_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Learning Rate Significance:</strong> Crucial in balancing speed and stability.</li>
<li><strong>Iterative Process:</strong> Fine-tuning through experimentation.</li>
<li><strong>Adaptive Nature:</strong> Automatic adjustments near minima contribute to robust convergence.</li>
</ul>
<h2 id="gradient-descent-for-linear-regression">Gradient Descent for Linear Regression<a class="headerlink" href="#gradient-descent-for-linear-regression" title="Permanent link">&para;</a></h2>
<h3 id="linear-regression-model">Linear Regression Model<a class="headerlink" href="#linear-regression-model" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Model:</strong> <span class="arithmatex">\(f(W, b) = WX + b\)</span></li>
<li><strong>Objective:</strong> Minimize the cost function <span class="arithmatex">\(J(W, b)\)</span>.</li>
</ul>
<h3 id="squared-error-cost-function">Squared Error Cost Function<a class="headerlink" href="#squared-error-cost-function" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Cost Function:</strong> <span class="arithmatex">\(J(W, b) = \frac{1}{2m} \sum_{i=1}^{m} (f(W, b) - Y^i)^2\)</span></li>
<li><span class="arithmatex">\(m\)</span>: Number of training examples.</li>
<li><span class="arithmatex">\(X^i\)</span>: Input feature for the <span class="arithmatex">\(i\)</span>-th example.</li>
<li><span class="arithmatex">\(Y^i\)</span>: Actual output for the <span class="arithmatex">\(i\)</span>-th example.</li>
</ul>
<h3 id="gradient-descent-algorithm">Gradient Descent Algorithm<a class="headerlink" href="#gradient-descent-algorithm" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Update Rule:</strong> </li>
<li><span class="arithmatex">\(W \leftarrow W - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)X^i\)</span></li>
<li><span class="arithmatex">\(b \leftarrow b - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)\)</span></li>
<li><strong>Learning Rate (<span class="arithmatex">\(\alpha\)</span>):</strong> Determines step size.</li>
</ul>
<h3 id="derivatives-calculation-optional">Derivatives Calculation (Optional)<a class="headerlink" href="#derivatives-calculation-optional" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Derivative of <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(W\)</span>:</strong></li>
<li><span class="arithmatex">\(\frac{dJ}{dW} = \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)X^i\)</span></li>
<li><strong>Derivative of <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(\(b\)\)</span>:</strong></li>
<li><span class="arithmatex">\(\frac{dJ}{db} = \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)\)</span></li>
<li><strong>Optional:</strong> Derivations involve applying calculus rules and simplifications. Skip if not interested.</li>
</ul>
<h3 id="convexity-of-the-cost-function">Convexity of the Cost Function<a class="headerlink" href="#convexity-of-the-cost-function" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Key Property:</strong> Squared error cost function for linear regression is convex.</li>
<li><strong>Convex Function:</strong> Bowl-shaped with a single global minimum.</li>
<li><strong>Implication:</strong> Gradient descent always converges to the global minimum.</li>
<li><strong>Visualization:</strong> Unlike some functions with multiple local minima, linear regression's convex cost function ensures convergence to the optimal solution.</li>
</ul>
<h3 id="conclusion_2">Conclusion<a class="headerlink" href="#conclusion_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Implementation:</strong> Derive and use update rules for <span class="arithmatex">\(W\)</span> and <span class="arithmatex">\(\(b\)\)</span> in gradient descent.</li>
<li><strong>Convexity:</strong> Linear regression's cost function guarantees a single global minimum.</li>
<li><strong>Convergence:</strong> Proper choice of learning rate (<span class="arithmatex">\(\alpha\)</span>) ensures convergence to the global minimum.</li>
<li><strong>Next Step:</strong> Visualize and apply gradient descent in the next video.## Gradient Descent for Linear Regression</li>
</ul>
<h3 id="linear-regression-model_1">Linear Regression Model<a class="headerlink" href="#linear-regression-model_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Model:</strong> <span class="arithmatex">\(f(W, b) = WX + b\)</span></li>
<li><strong>Objective:</strong> Minimize the cost function <span class="arithmatex">\(J(W, b)\)</span>.</li>
</ul>
<h3 id="squared-error-cost-function_1">Squared Error Cost Function<a class="headerlink" href="#squared-error-cost-function_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Cost Function:</strong> <span class="arithmatex">\(J(W, b) = \frac{1}{2m} \sum_{i=1}^{m} (f(W, b) - Y^i)^2\)</span></li>
<li><span class="arithmatex">\(m\)</span>: Number of training examples.</li>
<li><span class="arithmatex">\(X^i\)</span>: Input feature for the <span class="arithmatex">\(i\)</span>-th example.</li>
<li><span class="arithmatex">\(Y^i\)</span>: Actual output for the <span class="arithmatex">\(i\)</span>-th example.</li>
</ul>
<h3 id="gradient-descent-algorithm_1">Gradient Descent Algorithm<a class="headerlink" href="#gradient-descent-algorithm_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Update Rule:</strong> </li>
<li><span class="arithmatex">\(W \leftarrow W - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)X^i\)</span></li>
<li><span class="arithmatex">\(b \leftarrow b - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)\)</span></li>
<li><strong>Learning Rate (<span class="arithmatex">\(\alpha\)</span>):</strong> Determines step size.</li>
</ul>
<h3 id="derivatives-calculation-optional_1">Derivatives Calculation (Optional)<a class="headerlink" href="#derivatives-calculation-optional_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Derivative of <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(W\)</span>:</strong></li>
<li><span class="arithmatex">\(\frac{dJ}{dW} = \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)X^i\)</span></li>
<li><strong>Derivative of <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(\(b\)\)</span>:</strong></li>
<li><span class="arithmatex">\(\frac{dJ}{db} = \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)\)</span></li>
<li><strong>Optional:</strong> Derivations involve applying calculus rules and simplifications. Skip if not interested.</li>
</ul>
<h3 id="convexity-of-the-cost-function_1">Convexity of the Cost Function<a class="headerlink" href="#convexity-of-the-cost-function_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Key Property:</strong> Squared error cost function for linear regression is convex.</li>
<li><strong>Convex Function:</strong> Bowl-shaped with a single global minimum.</li>
<li><strong>Implication:</strong> Gradient descent always converges to the global minimum.</li>
<li><strong>Visualization:</strong> Unlike some functions with multiple local minima, linear regression's convex cost function ensures convergence to the optimal solution.</li>
</ul>
<h3 id="conclusion_3">Conclusion<a class="headerlink" href="#conclusion_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Implementation:</strong> Derive and use update rules for <span class="arithmatex">\(W\)</span> and <span class="arithmatex">\(\(b\)\)</span> in gradient descent.</li>
<li><strong>Convexity:</strong> Linear regression's cost function guarantees a single global minimum.</li>
<li><strong>Convergence:</strong> Proper choice of learning rate (<span class="arithmatex">\(\alpha\)</span>) ensures convergence to the global minimum.</li>
<li><strong>Next Step:</strong> Visualize and apply gradient descent in the next video.</li>
</ul>
<h2 id="running-gradient-descent-for-linear-regression">Running Gradient Descent for Linear Regression<a class="headerlink" href="#running-gradient-descent-for-linear-regression" title="Permanent link">&para;</a></h2>
<h3 id="algorithm-in-action">Algorithm in Action<a class="headerlink" href="#algorithm-in-action" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Model and Data Visualization:</strong></li>
<li>Upper left: Plot of the model and data.</li>
<li>Upper right: Contour plot of the cost function.</li>
<li>Bottom: Surface plot of the cost function.</li>
<li><strong>Initialization:</strong> <span class="arithmatex">\(w = -0.1, b = 900\)</span> (<span class="arithmatex">\(f(x) = -0.1x + 900\)</span>).</li>
<li><strong>Steps:</strong></li>
<li><strong>Step 1:</strong> Update parameters, move to a new point.</li>
<li><strong>Step 2:</strong> Repeat the process, updating parameters with each step.</li>
<li><strong>Convergence:</strong> The cost decreases, and the line fits data better.</li>
<li><strong>Global Minimum:</strong> Parameters converge to optimal values.</li>
<li><strong>Prediction:</strong> Use the trained model to make predictions.</li>
</ul>
<h3 id="batch-gradient-descent">Batch Gradient Descent<a class="headerlink" href="#batch-gradient-descent" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong> On each step, consider all training examples (the entire batch).</li>
<li><strong>Derivatives:</strong> Compute derivatives, summing over all training examples.</li>
<li><strong>Intuition:</strong> Updates parameters based on the overall performance of the model.</li>
<li><strong>Note:</strong> Other versions exist (e.g., stochastic gradient descent, mini-batch gradient descent), but we focus on batch gradient descent for linear regression.</li>
</ul>
<h3 id="celebration-and-optional-lab">Celebration and Optional Lab<a class="headerlink" href="#celebration-and-optional-lab" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Achievement:</strong> Completing the linear regression model.</li>
<li><strong>Next Step:</strong> Optional lab to review gradient descent, implement code, and visualize cost changes.</li>
<li><strong>Optional Lab Content:</strong></li>
<li>Gradient descent algorithm review.</li>
<li>Code implementation.</li>
<li>Plot of cost changes over iterations.</li>
<li>Contour plot visualization.</li>
</ul>
<h3 id="closing-remarks">Closing Remarks<a class="headerlink" href="#closing-remarks" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Congratulations:</strong> Completing the first machine learning model.</li>
<li><strong>Optional Lab:</strong> Opportunity to deepen understanding and gain practical coding experience.</li>
<li><strong>Future Topics:</strong> Linear regression with multiple features, handling nonlinear curves, and practical tips for real-world applications.</li>
<li><strong>Next Week:</strong> Exciting topics to enhance the power and versatility of linear regression.</li>
<li><strong>Appreciation:</strong> Thank you for joining the class, and see you next week!</li>
</ul>
<h1 id="week-2">Week 2:<a class="headerlink" href="#week-2" title="Permanent link">&para;</a></h1>
<h2 id="visualizing-the-cost-function-and-introduction-to-gradient-descent_1">Visualizing the Cost Function and Introduction to Gradient Descent<a class="headerlink" href="#visualizing-the-cost-function-and-introduction-to-gradient-descent_1" title="Permanent link">&para;</a></h2>
<h2 id="visualizing-the-cost-function_1">Visualizing the Cost Function<a class="headerlink" href="#visualizing-the-cost-function_1" title="Permanent link">&para;</a></h2>
<h3 id="model-overview_1">Model Overview<a class="headerlink" href="#model-overview_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Components:</strong> Model, parameters (w and b), cost function (J of w, b)</li>
<li><strong>Objective:</strong> Minimize the cost function J of w, b over parameters w and b.</li>
</ul>
<h3 id="visualization_1">Visualization<a class="headerlink" href="#visualization_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Previous visualization with b set to zero.</li>
<li>Return to the original model with both parameters w and b.</li>
<li>Explore the model function f(x) and its relation to the cost function J of w, b.</li>
<li>Training set example: house sizes and prices.</li>
<li>Illustration of a suboptimal model with specific values for w and b.</li>
</ul>
<h3 id="3d-surface-plot_1">3D Surface Plot<a class="headerlink" href="#3d-surface-plot_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Cost function J of w, b in three dimensions.</li>
<li>Resembles a soup bowl or a hammock.</li>
<li>Each point on the surface represents a specific choice of w and b.</li>
<li>3D surface plot provides a comprehensive view of the cost function.</li>
</ul>
<h3 id="contour-plot_1">Contour Plot<a class="headerlink" href="#contour-plot_1" title="Permanent link">&para;</a></h3>
<ul>
<li>An alternative visualization of the cost function J.</li>
<li>Horizontal slices of the 3D surface plot.</li>
<li>Ellipses or ovals represent points with the same value of J.</li>
<li>The minimum of the bowl is at the center of the smallest oval.</li>
<li>Contour plots offer a 2D representation of the 3D cost function.</li>
</ul>
<h2 id="gradient-descent_1">Gradient Descent<a class="headerlink" href="#gradient-descent_1" title="Permanent link">&para;</a></h2>
<h3 id="overview_2">Overview<a class="headerlink" href="#overview_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Purpose:</strong> Systematically find values of w and b that minimize the cost function J of w, b.</li>
<li><strong>Applicability:</strong> Widely used in machine learning, including advanced models like neural networks.</li>
</ul>
<h3 id="algorithm_1">Algorithm<a class="headerlink" href="#algorithm_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong> Minimize the cost function J of w, b for linear regression and more general functions.</li>
<li><strong>Initialization:</strong> Start with initial guesses for w and b (commonly set to 0).</li>
<li><strong>Update Rule for w:</strong>   <span class="arithmatex">\(\(w \leftarrow w - \alpha \frac{d}{dw}J(w, b)\)\)</span>  </li>
<li><strong>Update Rule for b:</strong>  <span class="arithmatex">\(b \leftarrow b - \alpha \frac{d}{db}J(w, b)\)</span></li>
<li><strong>Learning Rate (<span class="arithmatex">\(\alpha\)</span>):</strong> Controls the size of the steps in the descent process.</li>
<li><strong>Iterative Process:</strong> Repeat the update steps until convergence.</li>
</ul>
<h3 id="intuition_1">Intuition<a class="headerlink" href="#intuition_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Gradient descent aims to move downhill efficiently in the cost function landscape.</li>
<li>Visual analogy of standing on a hill and taking steps in the steepest downhill direction.</li>
<li>Multiple steps of gradient descent illustrated in finding local minima.</li>
</ul>
<h3 id="implementation_1">Implementation<a class="headerlink" href="#implementation_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Simultaneous Update:</strong> Update both parameters w and b simultaneously.</li>
<li><strong>Correct Implementation:</strong> Use temp variables to store updated values and then assign them.</li>
</ul>
<h3 id="key-takeaways_6">Key Takeaways<a class="headerlink" href="#key-takeaways_6" title="Permanent link">&para;</a></h3>
<ul>
<li>Gradient descent is a fundamental optimization algorithm in machine learning.</li>
<li>The learning rate (<span class="arithmatex">\(\alpha\)</span>) and simultaneous updates are critical for effective implementation.</li>
<li>Visualization aids understanding of cost function landscapes and descent process.</li>
</ul>
<h2 id="next-steps_1">Next Steps<a class="headerlink" href="#next-steps_1" title="Permanent link">&para;</a></h2>
<ul>
<li>Explore specific choices of w and b in linear regression models.</li>
<li>Dive into the mathematical expressions for implementing gradient descent. </li>
</ul>
<h2 id="gradient-descent-intuition_1">Gradient Descent Intuition<a class="headerlink" href="#gradient-descent-intuition_1" title="Permanent link">&para;</a></h2>
<h3 id="overview_3">Overview<a class="headerlink" href="#overview_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong> Understand the intuition behind gradient descent.</li>
<li><strong>Algorithm Recap:</strong> <span class="arithmatex">\(W \leftarrow W - \alpha \frac{d}{dW}J(W)\)</span></li>
<li><span class="arithmatex">\(W\)</span>: Model parameters.</li>
<li><span class="arithmatex">\(\alpha\)</span>: Learning rate.</li>
<li><span class="arithmatex">\(\frac{d}{dW}J(W)\)</span>: Derivative of cost function <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(W\)</span>.</li>
</ul>
<h3 id="derivative-and-tangent-lines_1">Derivative and Tangent Lines<a class="headerlink" href="#derivative-and-tangent-lines_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Derivative Term (<span class="arithmatex">\(\frac{d}{dW}J(W)\)</span>):</strong></li>
<li>Represents the slope of the cost function at a given point.</li>
<li>Positive slope (<span class="arithmatex">\(\frac{d}{dW}J(W) &gt; 0\)</span>) implies moving left (decreasing <span class="arithmatex">\(W\)</span>).</li>
<li>Negative slope (<span class="arithmatex">\(\frac{d}{dW}J(W) &lt; 0\)</span>) implies moving right (increasing <span class="arithmatex">\(W\)</span>).</li>
</ul>
<h3 id="examples_1">Examples<a class="headerlink" href="#examples_1" title="Permanent link">&para;</a></h3>
<h4 id="case-1-positive-slope_1">Case 1: Positive Slope<a class="headerlink" href="#case-1-positive-slope_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Initial Point:</strong> <span class="arithmatex">\(W\)</span> at a specific location.</li>
<li><strong>Derivative:</strong> Positive, indicating an upward slope.</li>
<li><strong>Update:</strong> <span class="arithmatex">\(W \leftarrow W - \alpha \cdot \text{Positive}\)</span></li>
<li><strong>Effect:</strong> Decrease <span class="arithmatex">\(W\)</span>, moving left on the graph.</li>
<li><strong>Explanation:</strong> When the derivative is positive, the algorithm takes a step in the direction that reduces the cost, aligning with the goal of reaching the minimum.</li>
</ul>
<h4 id="case-2-negative-slope_1">Case 2: Negative Slope<a class="headerlink" href="#case-2-negative-slope_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Initial Point:</strong> <span class="arithmatex">\(W\)</span> at a different location.</li>
<li><strong>Derivative:</strong> Negative, indicating a downward slope.</li>
<li><strong>Update:</strong> <span class="arithmatex">\(W \leftarrow W - \alpha \cdot \text{Negative}\)</span></li>
<li><strong>Effect:</strong> Increase <span class="arithmatex">\(W\)</span>, moving right on the graph.</li>
<li><strong>Explanation:</strong> When the derivative is negative, the algorithm adjusts <span class="arithmatex">\(W\)</span> in the opposite direction, facilitating movement toward the minimum.</li>
</ul>
<h3 id="learning-rate-alpha_1">Learning Rate (<span class="arithmatex">\(\alpha\)</span>)<a class="headerlink" href="#learning-rate-alpha_1" title="Permanent link">&para;</a></h3>
<h4 id="small-learning-rate_2">Small Learning Rate<a class="headerlink" href="#small-learning-rate_2" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Effect:</strong> Tiny steps in parameter space.</li>
<li><strong>Outcome:</strong> Slow convergence; many steps needed.</li>
<li><strong>Explanation:</strong> A small learning rate results in cautious adjustments, leading to gradual convergence. While it ensures stability, it requires more iterations to reach the minimum.</li>
</ul>
<h4 id="large-learning-rate_2">Large Learning Rate<a class="headerlink" href="#large-learning-rate_2" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Effect:</strong> Large steps in parameter space.</li>
<li><strong>Outcome:</strong> Risk of overshooting; may fail to converge.</li>
<li><strong>Explanation:</strong> A large learning rate accelerates convergence but poses a risk of overshooting the minimum. If too large, the algorithm may oscillate or diverge instead of converging.</li>
</ul>
<h3 id="handling-local-minima_2">Handling Local Minima<a class="headerlink" href="#handling-local-minima_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>At Local Minimum:</strong> Derivative (<span class="arithmatex">\(\frac{d}{dW}J(W)\)</span>) becomes zero.</li>
<li><strong>Effect:</strong> <span class="arithmatex">\(W\)</span> remains unchanged; no update.</li>
<li><strong>Ensures:</strong> Convergence at local minima.</li>
<li><strong>Explanation:</strong> When the derivative is zero, indicating a local minimum, the algorithm does not update <span class="arithmatex">\(W\)</span>, ensuring stability at the minimum.</li>
</ul>
<h3 id="learning-rate-selection_1">Learning Rate Selection<a class="headerlink" href="#learning-rate-selection_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Critical Decision:</strong> Choosing an appropriate <span class="arithmatex">\(\alpha\)</span>.</li>
<li><strong>Small <span class="arithmatex">\(\alpha\)</span>:</strong> Slow convergence.</li>
<li><strong>Large <span class="arithmatex">\(\alpha\)</span>:</strong> Risk of overshooting, potential non-convergence.</li>
<li><strong>Explanation:</strong> Selecting the right learning rate is crucial; a balance must be struck between convergence speed and stability. Trial and error, coupled with monitoring algorithm behavior, helps in finding the optimal <span class="arithmatex">\(\alpha\)</span>.</li>
</ul>
<h3 id="conclusion_4">Conclusion<a class="headerlink" href="#conclusion_4" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Intuition:</strong> Derivative guides the direction of parameter updates based on the slope of the cost function.</li>
<li><strong>Learning Rate:</strong> Balancing act between speed and stability.</li>
<li><strong>Adaptability:</strong> Gradient descent naturally adjusts step size near minima to ensure convergence.</li>
</ul>
<h2 id="learning-rate-in-gradient-descent_1">Learning Rate in Gradient Descent<a class="headerlink" href="#learning-rate-in-gradient-descent_1" title="Permanent link">&para;</a></h2>
<h3 id="importance-of-learning-rate_1">Importance of Learning Rate<a class="headerlink" href="#importance-of-learning-rate_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Significance:</strong> Influences convergence speed and stability.</li>
<li><strong>Control Mechanism:</strong> Governs the step size in parameter space.</li>
<li><strong>Explanation:</strong> The learning rate plays a crucial role in determining how quickly the algorithm converges and whether it remains stable throughout the process.</li>
</ul>
<h3 id="small-learning-rate_3">Small Learning Rate<a class="headerlink" href="#small-learning-rate_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Effect:</strong> Tiny steps in parameter space.</li>
<li>
<p><strong>Outcome:</strong> Slow convergence.</p>
</li>
<li>
<p><strong>Challenge:</strong> Requires numerous steps to approach the minimum.</p>
</li>
<li><strong>Explanation:</strong> A small learning rate ensures cautious updates, minimizing the risk of overshooting but extending the time needed for convergence.</li>
</ul>
<h3 id="large-learning-rate_3">Large Learning Rate<a class="headerlink" href="#large-learning-rate_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Effect:</strong> Large steps in parameter space.</li>
<li><strong>Outcome:</strong> Risk of overshooting the minimum.</li>
<li><strong>Challenge:</strong> May lead to non-convergence; divergence from the minimum.</li>
<li><strong>Explanation:</strong> A large learning rate accelerates convergence but may lead to instability, causing the algorithm to overshoot the minimum or fail to converge.</li>
</ul>
<h3 id="finding-the-right-balance_1">Finding the Right Balance<a class="headerlink" href="#finding-the-right-balance_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Goal:</strong> Optimal learning rate for efficient convergence.</li>
<li><strong>Trial and Error:</strong> Iterative adjustment based on algorithm behavior.</li>
<li><strong>Monitoring Convergence:</strong> Observe cost function reduction and parameter changes.</li>
<li><strong>Explanation:</strong> Striking the right balance involves experimentation and continuous monitoring. Iterative adjustments are made to find an optimal learning rate that ensures both speed and stability.</li>
</ul>
<h3 id="impact-on-convergence_1">Impact on Convergence<a class="headerlink" href="#impact-on-convergence_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Appropriate <span class="arithmatex">\(\alpha\)</span>:</strong> Efficient convergence to the minimum.</li>
<li><strong>Small <span class="arithmatex">\(\alpha\)</span>:</strong> Caution: Slow progress.</li>
<li><strong>Large <span class="arithmatex">\(\alpha\)</span>:</strong> Caution: Risk of instability and non-convergence.</li>
<li><strong>Explanation:</strong> The choice of learning rate significantly impacts convergence. An appropriate <span class="arithmatex">\(\alpha\)</span> leads to efficient convergence, while extremes (too small or too large) pose challenges.</li>
</ul>
<h3 id="handling-local-minima_3">Handling Local Minima<a class="headerlink" href="#handling-local-minima_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Automatic Adjustment:</strong> As the algorithm approaches a local minimum.</li>
<li><strong>Derivative Impact:</strong> Decreasing slope results in smaller steps.</li>
<li><strong>Ensures Stability:</strong> Prevents overshooting and divergence.</li>
<li><strong>Explanation:</strong> Automatic adjustment of step size as the algorithm nears a local minimum ensures stability. Decreasing slope corresponds to smaller steps, preventing overshooting and promoting convergence.</li>
</ul>
<h3 id="conclusion_5">Conclusion<a class="headerlink" href="#conclusion_5" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Learning Rate Significance:</strong> Crucial in balancing speed and stability.</li>
<li><strong>Iterative Process:</strong> Fine-tuning through experimentation.</li>
<li><strong>Adaptive Nature:</strong> Automatic adjustments near minima contribute to robust convergence.</li>
</ul>
<h2 id="gradient-descent-for-linear-regression_1">Gradient Descent for Linear Regression<a class="headerlink" href="#gradient-descent-for-linear-regression_1" title="Permanent link">&para;</a></h2>
<h3 id="linear-regression-model_2">Linear Regression Model<a class="headerlink" href="#linear-regression-model_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Model:</strong> <span class="arithmatex">\(f(W, b) = WX + b\)</span></li>
<li><strong>Objective:</strong> Minimize the cost function <span class="arithmatex">\(J(W, b)\)</span>.</li>
</ul>
<h3 id="squared-error-cost-function_2">Squared Error Cost Function<a class="headerlink" href="#squared-error-cost-function_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Cost Function:</strong> <span class="arithmatex">\(J(W, b) = \frac{1}{2m} \sum_{i=1}^{m} (f(W, b) - Y^i)^2\)</span></li>
<li><span class="arithmatex">\(m\)</span>: Number of training examples.</li>
<li><span class="arithmatex">\(X^i\)</span>: Input feature for the <span class="arithmatex">\(i\)</span>-th example.</li>
<li><span class="arithmatex">\(Y^i\)</span>: Actual output for the <span class="arithmatex">\(i\)</span>-th example.</li>
</ul>
<h3 id="gradient-descent-algorithm_2">Gradient Descent Algorithm<a class="headerlink" href="#gradient-descent-algorithm_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Update Rule:</strong> </li>
<li><span class="arithmatex">\(W \leftarrow W - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)X^i\)</span></li>
<li><span class="arithmatex">\(b \leftarrow b - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)\)</span></li>
<li><strong>Learning Rate (<span class="arithmatex">\(\alpha\)</span>):</strong> Determines step size.</li>
</ul>
<h3 id="derivatives-calculation-optional_2">Derivatives Calculation (Optional)<a class="headerlink" href="#derivatives-calculation-optional_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Derivative of <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(W\)</span>:</strong></li>
<li><span class="arithmatex">\(\frac{dJ}{dW} = \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)X^i\)</span></li>
<li><strong>Derivative of <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(\(b\)\)</span>:</strong></li>
<li><span class="arithmatex">\(\frac{dJ}{db} = \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)\)</span></li>
<li><strong>Optional:</strong> Derivations involve applying calculus rules and simplifications. Skip if not interested.</li>
</ul>
<h3 id="convexity-of-the-cost-function_2">Convexity of the Cost Function<a class="headerlink" href="#convexity-of-the-cost-function_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Key Property:</strong> Squared error cost function for linear regression is convex.</li>
<li><strong>Convex Function:</strong> Bowl-shaped with a single global minimum.</li>
<li><strong>Implication:</strong> Gradient descent always converges to the global minimum.</li>
<li><strong>Visualization:</strong> Unlike some functions with multiple local minima, linear regression's convex cost function ensures convergence to the optimal solution.</li>
</ul>
<h3 id="conclusion_6">Conclusion<a class="headerlink" href="#conclusion_6" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Implementation:</strong> Derive and use update rules for <span class="arithmatex">\(W\)</span> and <span class="arithmatex">\(\(b\)\)</span> in gradient descent.</li>
<li><strong>Convexity:</strong> Linear regression's cost function guarantees a single global minimum.</li>
<li><strong>Convergence:</strong> Proper choice of learning rate (<span class="arithmatex">\(\alpha\)</span>) ensures convergence to the global minimum.</li>
<li><strong>Next Step:</strong> Visualize and apply gradient descent in the next video.## Gradient Descent for Linear Regression</li>
</ul>
<h3 id="linear-regression-model_3">Linear Regression Model<a class="headerlink" href="#linear-regression-model_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Model:</strong> <span class="arithmatex">\(f(W, b) = WX + b\)</span></li>
<li><strong>Objective:</strong> Minimize the cost function <span class="arithmatex">\(J(W, b)\)</span>.</li>
</ul>
<h3 id="squared-error-cost-function_3">Squared Error Cost Function<a class="headerlink" href="#squared-error-cost-function_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Cost Function:</strong> <span class="arithmatex">\(J(W, b) = \frac{1}{2m} \sum_{i=1}^{m} (f(W, b) - Y^i)^2\)</span></li>
<li><span class="arithmatex">\(m\)</span>: Number of training examples.</li>
<li><span class="arithmatex">\(X^i\)</span>: Input feature for the <span class="arithmatex">\(i\)</span>-th example.</li>
<li><span class="arithmatex">\(Y^i\)</span>: Actual output for the <span class="arithmatex">\(i\)</span>-th example.</li>
</ul>
<h3 id="gradient-descent-algorithm_3">Gradient Descent Algorithm<a class="headerlink" href="#gradient-descent-algorithm_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Update Rule:</strong> </li>
<li><span class="arithmatex">\(W \leftarrow W - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)X^i\)</span></li>
<li><span class="arithmatex">\(b \leftarrow b - \alpha \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)\)</span></li>
<li><strong>Learning Rate (<span class="arithmatex">\(\alpha\)</span>):</strong> Determines step size.</li>
</ul>
<h3 id="derivatives-calculation-optional_3">Derivatives Calculation (Optional)<a class="headerlink" href="#derivatives-calculation-optional_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Derivative of <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(W\)</span>:</strong></li>
<li><span class="arithmatex">\(\frac{dJ}{dW} = \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)X^i\)</span></li>
<li><strong>Derivative of <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(\(b\)\)</span>:</strong></li>
<li><span class="arithmatex">\(\frac{dJ}{db} = \frac{1}{m} \sum_{i=1}^{m} (f(W, b) - Y^i)\)</span></li>
<li><strong>Optional:</strong> Derivations involve applying calculus rules and simplifications. Skip if not interested.</li>
</ul>
<h3 id="convexity-of-the-cost-function_3">Convexity of the Cost Function<a class="headerlink" href="#convexity-of-the-cost-function_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Key Property:</strong> Squared error cost function for linear regression is convex.</li>
<li><strong>Convex Function:</strong> Bowl-shaped with a single global minimum.</li>
<li><strong>Implication:</strong> Gradient descent always converges to the global minimum.</li>
<li><strong>Visualization:</strong> Unlike some functions with multiple local minima, linear regression's convex cost function ensures convergence to the optimal solution.</li>
</ul>
<h3 id="conclusion_7">Conclusion<a class="headerlink" href="#conclusion_7" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Implementation:</strong> Derive and use update rules for <span class="arithmatex">\(W\)</span> and <span class="arithmatex">\(\(b\)\)</span> in gradient descent.</li>
<li><strong>Convexity:</strong> Linear regression's cost function guarantees a single global minimum.</li>
<li><strong>Convergence:</strong> Proper choice of learning rate (<span class="arithmatex">\(\alpha\)</span>) ensures convergence to the global minimum.</li>
<li><strong>Next Step:</strong> Visualize and apply gradient descent in the next video.</li>
</ul>
<h2 id="running-gradient-descent-for-linear-regression_1">Running Gradient Descent for Linear Regression<a class="headerlink" href="#running-gradient-descent-for-linear-regression_1" title="Permanent link">&para;</a></h2>
<h3 id="algorithm-in-action_1">Algorithm in Action<a class="headerlink" href="#algorithm-in-action_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Model and Data Visualization:</strong></li>
<li>Upper left: Plot of the model and data.</li>
<li>Upper right: Contour plot of the cost function.</li>
<li>Bottom: Surface plot of the cost function.</li>
<li><strong>Initialization:</strong> <span class="arithmatex">\(w = -0.1, b = 900\)</span> (<span class="arithmatex">\(f(x) = -0.1x + 900\)</span>).</li>
<li><strong>Steps:</strong></li>
<li><strong>Step 1:</strong> Update parameters, move to a new point.</li>
<li><strong>Step 2:</strong> Repeat the process, updating parameters with each step.</li>
<li><strong>Convergence:</strong> The cost decreases, and the line fits data better.</li>
<li><strong>Global Minimum:</strong> Parameters converge to optimal values.</li>
<li><strong>Prediction:</strong> Use the trained model to make predictions.</li>
</ul>
<h3 id="batch-gradient-descent_1">Batch Gradient Descent<a class="headerlink" href="#batch-gradient-descent_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong> On each step, consider all training examples (the entire batch).</li>
<li><strong>Derivatives:</strong> Compute derivatives, summing over all training examples.</li>
<li><strong>Intuition:</strong> Updates parameters based on the overall performance of the model.</li>
<li><strong>Note:</strong> Other versions exist (e.g., stochastic gradient descent, mini-batch gradient descent), but we focus on batch gradient descent for linear regression.</li>
</ul>
<h3 id="celebration-and-optional-lab_1">Celebration and Optional Lab<a class="headerlink" href="#celebration-and-optional-lab_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Achievement:</strong> Completing the linear regression model.</li>
<li><strong>Next Step:</strong> Optional lab to review gradient descent, implement code, and visualize cost changes.</li>
<li><strong>Optional Lab Content:</strong></li>
<li>Gradient descent algorithm review.</li>
<li>Code implementation.</li>
<li>Plot of cost changes over iterations.</li>
<li>Contour plot visualization.</li>
</ul>
<h3 id="closing-remarks_1">Closing Remarks<a class="headerlink" href="#closing-remarks_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Congratulations:</strong> Completing the first machine learning model.</li>
<li><strong>Optional Lab:</strong> Opportunity to deepen understanding and gain practical coding experience.</li>
<li><strong>Future Topics:</strong> Linear regression with multiple features, handling nonlinear curves, and practical tips for real-world applications.</li>
<li><strong>Next Week:</strong> Exciting topics to enhance the power and versatility of linear regression.</li>
<li><strong>Appreciation:</strong> Thank you for joining the class, and see you next week!</li>
</ul>
<p>Week 2</p>
<h2 id="linear-regression-with-multiple-features">Linear Regression with Multiple Features<a class="headerlink" href="#linear-regression-with-multiple-features" title="Permanent link">&para;</a></h2>
<h3 id="introduction_1">Introduction<a class="headerlink" href="#introduction_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong> Enhance linear regression with multiple features.</li>
<li><strong>Features:</strong> Instead of a single feature (e.g., house size), consider multiple features (e.g., size, bedrooms, floors, age).</li>
<li><strong>Notation:</strong></li>
<li><span class="arithmatex">\(X_1, X_2, X_3, X_4\)</span> for the four features.</li>
<li><span class="arithmatex">\(X_j\)</span> or <span class="arithmatex">\(X^j\)</span> denotes the list of features.</li>
<li><span class="arithmatex">\(n\)</span> is the total number of features.</li>
</ul>
<h3 id="multiple-features-model">Multiple Features Model<a class="headerlink" href="#multiple-features-model" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Original Model:</strong> <span class="arithmatex">\(f_{wb}(x) = wx + b\)</span></li>
<li><strong>Multiple Features Model:</strong> <span class="arithmatex">\(f_{wb}(x) = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b\)</span></li>
<li><strong>Interpretation:</strong> More features provide richer information for prediction.</li>
</ul>
<h3 id="notation-simplification">Notation Simplification<a class="headerlink" href="#notation-simplification" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\(X^i\)</span> is a vector representing the features for the <span class="arithmatex">\(i\)</span>-th training example.</li>
<li><span class="arithmatex">\(X_j^i\)</span> is the <span class="arithmatex">\(j\)</span>-th feature in the <span class="arithmatex">\(i\)</span>-th training example.</li>
</ul>
<h3 id="model-interpretation">Model Interpretation<a class="headerlink" href="#model-interpretation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Example Model:</strong></li>
<li><span class="arithmatex">\(0.1 \times X_1 + 4 \times X_2 + 10 \times X_3 - 2 \times X_4 + 80\)</span></li>
<li><strong>Interpretation:</strong></li>
<li><span class="arithmatex">\(b = 80\)</span> is the base price.</li>
<li><span class="arithmatex">\(0.1\)</span> for each sq. ft. (<span class="arithmatex">\(X_1\)</span>) increase by $100.</li>
<li><span class="arithmatex">\(4\)</span> for each bedroom (<span class="arithmatex">\(X_2\)</span>) increase by $4,000.</li>
<li><span class="arithmatex">\(10\)</span> for each floor (<span class="arithmatex">\(X_3\)</span>) increase by $10,000.</li>
<li><span class="arithmatex">\(-2\)</span> for each year (<span class="arithmatex">\(X_4\)</span>) decrease by $2,000.</li>
</ul>
<h3 id="multiple-linear-regression">Multiple Linear Regression<a class="headerlink" href="#multiple-linear-regression" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong> Model with multiple input features.</li>
<li><span class="arithmatex">\(f_{wb}(x) = \mathbf{w} \cdot \mathbf{X} + b\)</span></li>
<li><span class="arithmatex">\(\mathbf{w}\)</span>: Vector of weights (<span class="arithmatex">\(w_1, w_2, w_3, w_4\)</span>).</li>
<li><span class="arithmatex">\(\mathbf{X}\)</span>: Vector of features (<span class="arithmatex">\(X_1, X_2, X_3, X_4\)</span>).</li>
</ul>
<h3 id="vectorization-part-1">Vectorization Part 1<a class="headerlink" href="#vectorization-part-1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Vectorization Benefits:</strong></li>
<li>Shorter code.</li>
<li>Efficient execution using modern libraries and hardware.</li>
<li><strong>Example:</strong></li>
<li><span class="arithmatex">\(f_{wb}(x) = \mathbf{w} \cdot \mathbf{X} + b\)</span></li>
<li>Code without vectorization (inefficient).</li>
<li>Code with vectorization (NumPy dot function).</li>
</ul>
<h3 id="vectorization-example">Vectorization Example<a class="headerlink" href="#vectorization-example" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Code Example:</strong></li>
<li>NumPy arrays for <span class="arithmatex">\(w\)</span>, <span class="arithmatex">\(x\)</span>, and <span class="arithmatex">\(\(b\)\)</span>.</li>
<li>Non-vectorized code using loops.</li>
<li>Vectorized code using NumPy dot function.</li>
<li><strong>Benefits of Vectorization:</strong></li>
<li>Shorter and more readable code.</li>
<li>Improved computational efficiency.</li>
</ul>
<h3 id="conclusion_8">Conclusion<a class="headerlink" href="#conclusion_8" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Key Takeaways:</strong></li>
<li>Multiple linear regression incorporates multiple features.</li>
<li>Vectorization enhances code readability and computational efficiency.</li>
<li>NumPy dot function allows parallelization for faster execution.</li>
<li><strong>Next Topic:</strong> Explore more about vectorization in the next video.</li>
</ul>
<h2 id="vectorization-part-2-understanding-the-magic">Vectorization Part 2: Understanding the Magic<a class="headerlink" href="#vectorization-part-2-understanding-the-magic" title="Permanent link">&para;</a></h2>
<h3 id="introduction_2">Introduction<a class="headerlink" href="#introduction_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Reflection:</strong> Initial fascination with vectorization's efficiency.</li>
<li><strong>Objective:</strong> Explore the inner workings of vectorization.</li>
<li><strong>Example:</strong> Compare a non-vectorized for loop with a vectorized NumPy implementation.</li>
</ul>
<h3 id="non-vectorized-for-loop">Non-Vectorized For Loop<a class="headerlink" href="#non-vectorized-for-loop" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Sequential Computation:</strong></li>
<li>For loop processes values one after another.</li>
<li>Time-steps (t0 to t15) represent sequential operations.</li>
<li>Index <span class="arithmatex">\(j\)</span> ranges from 0 to 15.</li>
</ul>
<h3 id="vectorized-numpy-implementation">Vectorized NumPy Implementation<a class="headerlink" href="#vectorized-numpy-implementation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Parallel Processing:</strong></li>
<li>NumPy vectorization enables parallel computation.</li>
<li>All values of vectors <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(x\)</span> processed simultaneously.</li>
<li>Specialized hardware performs parallel addition efficiently.</li>
</ul>
<h3 id="computational-efficiency">Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Code Comparison:</strong></li>
<li>Non-vectorized code with a for loop.</li>
<li>Vectorized code using NumPy dot function.</li>
<li><strong>Benefits of Vectorization:</strong></li>
<li>Simultaneous computation of all vector elements.</li>
<li>Utilizes parallel processing hardware.</li>
<li>Significant speed improvement with large datasets or models.</li>
</ul>
<h3 id="concrete-example-updating-parameters">Concrete Example: Updating Parameters<a class="headerlink" href="#concrete-example-updating-parameters" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Scenario:</strong> 16 features and parameters (<span class="arithmatex">\(w_1\)</span> to <span class="arithmatex">\(w_{16}\)</span>).</li>
<li><strong>Derivative Calculation:</strong></li>
<li>Derivative values stored in NumPy arrays <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(d\)</span>.</li>
<li><strong>Parameter Update:</strong></li>
<li>Without vectorization: Sequential update using a for loop.</li>
<li>With vectorization: Parallel computation of all updates.</li>
</ul>
<h3 id="practical-impact">Practical Impact<a class="headerlink" href="#practical-impact" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Vectorized Implementation:</strong></li>
<li>Code: <span class="arithmatex">\(w \text{ -= } 0.1 \times d\)</span></li>
<li>Efficient parallel computation of all 16 updates.</li>
<li><strong>Scaling to Large Datasets:</strong></li>
<li>Impact on runtime is more significant with numerous features or large datasets.</li>
<li>Vectorization can make the difference between minutes and hours.</li>
</ul>
<h3 id="optional-lab-introduction-to-numpy">Optional Lab: Introduction to NumPy<a class="headerlink" href="#optional-lab-introduction-to-numpy" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Content:</strong></li>
<li>Introduction to NumPy, a widely used Python library for numerical operations.</li>
<li>Creating NumPy arrays, dot product calculation using NumPy.</li>
<li>Timing and comparing vectorized and non-vectorized code.</li>
</ul>
<h3 id="conclusion_9">Conclusion<a class="headerlink" href="#conclusion_9" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Key Takeaways:</strong></li>
<li>Vectorization utilizes parallel processing for simultaneous computations.</li>
<li>Speed improvement is more noticeable with larger datasets or models.</li>
<li>NumPy is a crucial library for efficient numerical operations.</li>
<li><strong>Next Topic:</strong> Apply vectorization to gradient descent in multiple linear regression.</li>
</ul>
<h2 id="gradient-descent-for-multiple-linear-regression">Gradient Descent for Multiple Linear Regression<a class="headerlink" href="#gradient-descent-for-multiple-linear-regression" title="Permanent link">&para;</a></h2>
<h3 id="recap-multiple-linear-regression">Recap: Multiple Linear Regression<a class="headerlink" href="#recap-multiple-linear-regression" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Vector Notation:</strong></li>
<li>Parameters <span class="arithmatex">\(w_1\)</span> to <span class="arithmatex">\(w_n\)</span> collected into vector <span class="arithmatex">\(w\)</span>.</li>
<li>Model: <span class="arithmatex">\(f_w, b(x) = w \cdot x + b\)</span>.</li>
</ul>
<h3 id="gradient-descent-update-rule">Gradient Descent Update Rule<a class="headerlink" href="#gradient-descent-update-rule" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong> Minimize the cost function <span class="arithmatex">\(J(w, b)\)</span>.</li>
<li><strong>Update Rule:</strong> <span class="arithmatex">\(w_j \leftarrow w_j - \alpha \frac{\partial J}{\partial w_j}\)</span> for <span class="arithmatex">\(j = 1, 2, ..., n\)</span>.</li>
<li><strong>Derivative Term:</strong> Different from univariate regression, but similar structure.</li>
<li><strong>Vectorized Implementation:</strong> Simultaneous update of all parameters.</li>
</ul>
<h3 id="normal-equation-optional">Normal Equation (Optional)<a class="headerlink" href="#normal-equation-optional" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Alternative Method:</strong> Solves for <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(\(b\)\)</span> without iterative gradient descent.</li>
<li><strong>Limited Applicability:</strong> Works only for linear regression.</li>
<li><strong>Disadvantages:</strong></li>
<li>Not generalized to other algorithms.</li>
<li>Slow for large feature sets.</li>
<li><strong>Use in Libraries:</strong> Some machine learning libraries might use it internally.</li>
</ul>
<h3 id="implementation-lab-optional">Implementation Lab (Optional)<a class="headerlink" href="#implementation-lab-optional" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Topics Covered:</strong></li>
<li>Implementing multiple linear regression using NumPy.</li>
<li>Vectorized computation for efficiency.</li>
</ul>
<h3 id="conclusion_10">Conclusion<a class="headerlink" href="#conclusion_10" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Key Takeaways:</strong></li>
<li>Gradient descent for multiple linear regression involves updating parameters using derivatives.</li>
<li>Vectorization improves efficiency with simultaneous updates.</li>
<li>Normal equation is an alternative method with limitations.</li>
<li>Optional lab for hands-on implementation.</li>
</ul>
<h2 id="feature-scaling-part-1-understanding-the-problem">Feature Scaling Part 1: Understanding the Problem<a class="headerlink" href="#feature-scaling-part-1-understanding-the-problem" title="Permanent link">&para;</a></h2>
<h3 id="introduction_3">Introduction<a class="headerlink" href="#introduction_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong> Improve gradient descent performance using feature scaling.</li>
<li><strong>Motivation:</strong> Examining the impact of feature scale differences.</li>
</ul>
<h3 id="feature-scaling-concept">Feature Scaling Concept<a class="headerlink" href="#feature-scaling-concept" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Example Scenario:</strong> Predicting house prices with features <span class="arithmatex">\(x_1\)</span> (size) and <span class="arithmatex">\(x_2\)</span> (number of bedrooms).</li>
<li><strong>Observation:</strong> Features with different scales may lead to suboptimal parameter choices.</li>
<li><strong>Impact on Gradient Descent:</strong> Contours of cost function may be tall and skinny.</li>
</ul>
<h3 id="importance-of-feature-scaling">Importance of Feature Scaling<a class="headerlink" href="#importance-of-feature-scaling" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Illustration:</strong> Scatter plot of features with varying scales.</li>
<li><strong>Contour Plot:</strong> Contours of the cost function may become elongated.</li>
<li><strong>Gradient Descent Behavior:</strong> Bouncing back and forth, slow convergence.</li>
</ul>
<h3 id="scaling-solutions">Scaling Solutions<a class="headerlink" href="#scaling-solutions" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong> Make features comparable in scale.</li>
<li><strong>Benefits:</strong> Faster convergence, efficient gradient descent.</li>
<li><strong>Methods:</strong> Feature scaling, mean normalization, Z-score normalization.</li>
</ul>
<h2 id="feature-scaling-part-2-implementation">Feature Scaling Part 2: Implementation<a class="headerlink" href="#feature-scaling-part-2-implementation" title="Permanent link">&para;</a></h2>
<h3 id="scaling-methods">Scaling Methods<a class="headerlink" href="#scaling-methods" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Min-Max Scaling:</strong></li>
<li>Rescale by dividing each feature by its maximum value.</li>
<li>
<p>Range: 0 to 1.</p>
</li>
<li>
<p><strong>Mean Normalization:</strong></p>
</li>
<li>Center features around zero.</li>
<li>
<p>Subtract mean and divide by range.</p>
</li>
<li>
<p><strong>Z-Score Normalization:</strong></p>
</li>
<li>Scale based on standard deviation.</li>
<li>Subtract mean and divide by standard deviation.</li>
</ol>
<h3 id="application">Application<a class="headerlink" href="#application" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Example:</strong></li>
<li>Original feature ranges: <span class="arithmatex">\(x_1\)</span> (300-2000), <span class="arithmatex">\(x_2\)</span> (0-5).</li>
<li>Illustration of scaled features.</li>
</ul>
<h3 id="choosing-scale-values">Choosing Scale Values<a class="headerlink" href="#choosing-scale-values" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Rule of Thumb:</strong></li>
<li>Aim for ranges around -1 to 1.</li>
<li>Loose values like -3 to 3 or -0.3 to 0.3 are acceptable.</li>
<li>Consistency across features is crucial.</li>
</ul>
<h3 id="recap_1">Recap<a class="headerlink" href="#recap_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Feature Scaling Purpose:</strong> Enhance gradient descent efficiency.</li>
<li><strong>Methods:</strong> Min-Max Scaling, Mean Normalization, Z-Score Normalization.</li>
<li><strong>Rule:</strong> Aim for comparable feature ranges for effective scaling.</li>
</ul>
<h2 id="next-steps-convergence-check-and-learning-rate-selection">Next Steps: Convergence Check and Learning Rate Selection<a class="headerlink" href="#next-steps-convergence-check-and-learning-rate-selection" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Upcoming Topics:</strong></li>
<li>Recognizing convergence in gradient descent.</li>
<li>Choosing an appropriate learning rate for gradient descent.</li>
</ul>
<h2 id="conclusion_11">Conclusion<a class="headerlink" href="#conclusion_11" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Key Takeaways:</strong></li>
<li>Feature scaling enhances gradient descent performance.</li>
<li>Multiple methods available for scaling.</li>
<li>Consistency in feature scale is essential.</li>
<li>Upcoming topics: Convergence check and learning rate selection.</li>
</ul>
<h2 id="checking-gradient-descent-for-convergence">Checking Gradient Descent for Convergence<a class="headerlink" href="#checking-gradient-descent-for-convergence" title="Permanent link">&para;</a></h2>
<h3 id="learning-curve">Learning Curve<a class="headerlink" href="#learning-curve" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong> Ensure gradient descent is converging effectively.</li>
<li><strong>Graph Representation:</strong></li>
<li>Horizontal axis: Number of iterations.</li>
<li>Vertical axis: Cost function <span class="arithmatex">\(J\)</span> on the training set.</li>
<li>Learning curve provides insights into algorithm performance.</li>
</ul>
<h3 id="interpretation">Interpretation<a class="headerlink" href="#interpretation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Ideal Scenario:</strong> Cost <span class="arithmatex">\(J\)</span> decreases consistently after each iteration.</li>
<li><strong>Visual Analysis:</strong> Observe the curve to detect convergence.</li>
<li><strong>Automatic Convergence Test:</strong> Use a small threshold <span class="arithmatex">\(\epsilon\)</span>.</li>
</ul>
<h3 id="convergence-analysis">Convergence Analysis<a class="headerlink" href="#convergence-analysis" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Graph Inspection:</strong></li>
<li>Observe flattening of the curve.</li>
<li>Convergence when cost stabilizes.</li>
</ul>
<h3 id="iteration-variability">Iteration Variability<a class="headerlink" href="#iteration-variability" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Number of Iterations:</strong></li>
<li>Varies across applications.</li>
<li>No fixed rule; learning curve helps decide.</li>
</ul>
<h3 id="automatic-convergence-test">Automatic Convergence Test<a class="headerlink" href="#automatic-convergence-test" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Threshold <span class="arithmatex">\(\epsilon\)</span>:</strong></li>
<li>If <span class="arithmatex">\(\Delta J &lt; \epsilon\)</span>, declare convergence.</li>
<li>Choosing the right threshold can be challenging.</li>
</ul>
<h3 id="debugging-tip">Debugging Tip<a class="headerlink" href="#debugging-tip" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Debugging with <span class="arithmatex">\(\epsilon\)</span>:</strong></li>
<li>Set <span class="arithmatex">\(\epsilon\)</span> to a small value.</li>
<li>Ensure cost decreases consistently.</li>
<li>Useful for detecting potential issues.</li>
</ul>
<h2 id="choosing-the-learning-rate">Choosing the Learning Rate<a class="headerlink" href="#choosing-the-learning-rate" title="Permanent link">&para;</a></h2>
<h3 id="importance-of-learning-rate_2">Importance of Learning Rate<a class="headerlink" href="#importance-of-learning-rate_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Significance:</strong></li>
<li>Learning rate influences algorithm efficiency.</li>
<li>Too small: Slow convergence.</li>
<li>Too large: May not converge; oscillations in cost.</li>
</ul>
<h3 id="learning-rate-selection_2">Learning Rate Selection<a class="headerlink" href="#learning-rate-selection_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Learning Curve Observation:</strong></li>
<li>Costs fluctuating (increase-decrease) may indicate issues.</li>
<li>Overshooting global minimum due to large learning rate.</li>
</ul>
<h3 id="learning-rate-illustration">Learning Rate Illustration<a class="headerlink" href="#learning-rate-illustration" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Impact of Learning Rate Size:</strong></li>
<li>Overshooting: Large learning rate.</li>
<li>Consistent increase: Learning rate too large.</li>
<li>Correct update: Learning rate within appropriate range.</li>
</ul>
<h3 id="debugging-with-small-learning-rates">Debugging with Small Learning Rates<a class="headerlink" href="#debugging-with-small-learning-rates" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Debugging Step:</strong></li>
<li>Set <span class="arithmatex">\(\alpha\)</span> to a very small value.</li>
<li>Verify cost decreases on every iteration.</li>
<li>Useful for identifying bugs.</li>
</ul>
<h3 id="choosing-an-appropriate-learning-rate">Choosing an Appropriate Learning Rate<a class="headerlink" href="#choosing-an-appropriate-learning-rate" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Strategy:</strong></li>
<li>Try a range of <span class="arithmatex">\(\alpha\)</span> values.</li>
<li>Plot cost function <span class="arithmatex">\(J\)</span> vs. iterations for each <span class="arithmatex">\(\alpha\)</span>.</li>
<li>Select <span class="arithmatex">\(\alpha\)</span> with consistent and rapid cost decrease.</li>
</ul>
<h3 id="iterative-approach">Iterative Approach<a class="headerlink" href="#iterative-approach" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Exploration Strategy:</strong></li>
<li>Start with small <span class="arithmatex">\(\alpha\)</span> (e.g., 0.001).</li>
<li>Gradually increase <span class="arithmatex">\(\alpha\)</span> (e.g., 0.003, 0.01).</li>
<li>Observe curve behavior and select appropriate <span class="arithmatex">\(\alpha\)</span>.</li>
</ul>
<h3 id="importance-of-graph-exploration">Importance of Graph Exploration<a class="headerlink" href="#importance-of-graph-exploration" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Visual Inspection:</strong></li>
<li>Insights into algorithm behavior.</li>
<li>Facilitates learning rate selection.</li>
</ul>
<h3 id="next-steps-optional-lab">Next Steps: Optional Lab<a class="headerlink" href="#next-steps-optional-lab" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Optional Lab Content:</strong></li>
<li>Feature scaling implementation.</li>
<li>Exploring different <span class="arithmatex">\(\alpha\)</span> values.</li>
<li>Gaining practical insights.</li>
</ul>
<h2 id="conclusion_12">Conclusion<a class="headerlink" href="#conclusion_12" title="Permanent link">&para;</a></h2>
<h3 id="key-takeaways_7">Key Takeaways<a class="headerlink" href="#key-takeaways_7" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Learning Curve:</strong></li>
<li>Monitors cost function behavior over iterations.</li>
<li>Aids in detecting convergence.</li>
<li><strong>Learning Rate:</strong></li>
<li>Influences gradient descent efficiency.</li>
<li>Optimal choice crucial for effective training.</li>
<li><strong>Practical Approach:</strong></li>
<li>Iterative exploration for <span class="arithmatex">\(\alpha\)</span> selection.</li>
<li>Graphical analysis enhances decision-making.</li>
</ul>
<h3 id="next-topic-custom-features-in-multiple-linear-regression">Next Topic: Custom Features in Multiple Linear Regression<a class="headerlink" href="#next-topic-custom-features-in-multiple-linear-regression" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Upcoming Topic:</strong></li>
<li>Enhancing multiple linear regression with custom features.</li>
<li>Fitting curves to data beyond straight lines.</li>
</ul>
<h2 id="feature-engineering">Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permanent link">&para;</a></h2>
<h3 id="importance-of-feature-choice">Importance of Feature Choice<a class="headerlink" href="#importance-of-feature-choice" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Significance:</strong></li>
<li>Crucial for algorithm performance.</li>
<li>Influences predictive accuracy.</li>
</ul>
<h3 id="example-predicting-house-price">Example: Predicting House Price<a class="headerlink" href="#example-predicting-house-price" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Original Features:</strong></li>
<li><span class="arithmatex">\(X_1\)</span> (frontage), <span class="arithmatex">\(X_2\)</span> (depth).</li>
<li><strong>Model Option 1:</strong></li>
<li><span class="arithmatex">\(f(x) = w_1X_1 + w_2X_2 + b\)</span>.</li>
<li><strong>Alternative Feature Engineering:</strong></li>
<li><strong>Insight:</strong> Area (<span class="arithmatex">\(X_3 = X_1 \times X_2\)</span>) is more predictive.</li>
<li><strong>New Model:</strong> <span class="arithmatex">\(f_w, b(x) = w_1X_1 + w_2X_2 + w_3X_3 + b\)</span>.</li>
<li><strong>Parameter Selection:</strong> <span class="arithmatex">\(w_1, w_2, w_3\)</span> based on data insights.</li>
</ul>
<h3 id="feature-engineering-process">Feature Engineering Process<a class="headerlink" href="#feature-engineering-process" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong></li>
<li><strong>Transform or Combine Features:</strong><ul>
<li>Enhance algorithm's predictive capabilities.</li>
</ul>
</li>
<li><strong>Intuition-Based Design:</strong></li>
<li>Leverage domain knowledge.</li>
<li>Improve model accuracy.</li>
</ul>
<h3 id="polynomial-regression">Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Permanent link">&para;</a></h3>
<h4 id="introduction_4">Introduction<a class="headerlink" href="#introduction_4" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Objective:</strong></li>
<li>Fit curves/non-linear functions to data.</li>
<li><strong>Dataset Example:</strong></li>
<li>Housing data with size (<span class="arithmatex">\(x\)</span>) as a feature.</li>
</ul>
<h4 id="polynomial-functions">Polynomial Functions<a class="headerlink" href="#polynomial-functions" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Options:</strong></li>
<li>Quadratic, cubic, etc.</li>
<li><span class="arithmatex">\(f(x) = w_1x + w_2x^2 + b\)</span>.</li>
<li><span class="arithmatex">\(f(x) = w_1x + w_2x^2 + w_3x^3 + b\)</span>.</li>
<li><strong>Feature Scaling Importance:</strong></li>
<li>Power-based features may have different value ranges.</li>
<li>Apply feature scaling for gradient descent.</li>
</ul>
<h4 id="feature-choices">Feature Choices<a class="headerlink" href="#feature-choices" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Wide Range:</strong></li>
<li>Different powers of <span class="arithmatex">\(x\)</span>.</li>
<li>Consider sqrt(<span class="arithmatex">\(x\)</span>) as an alternative.</li>
<li><strong>Decision Criteria:</strong></li>
<li>Model fitting.</li>
<li>Data characteristics.</li>
<li>Iterative exploration.</li>
</ul>
<h3 id="conclusion_13">Conclusion<a class="headerlink" href="#conclusion_13" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Feature Engineering Overview:</strong></li>
<li><strong>Goal:</strong> Optimize feature selection.</li>
<li><strong>Process:</strong> Transform or combine features.</li>
<li><strong>Outcome:</strong> Improved algorithm performance.</li>
</ul>
<h3 id="next-topic-polynomial-regression-implementation">Next Topic: Polynomial Regression Implementation<a class="headerlink" href="#next-topic-polynomial-regression-implementation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Upcoming Video:</strong></li>
<li>Practical implementation of polynomial regression.</li>
<li>Code exploration with features like <span class="arithmatex">\(x\)</span>, <span class="arithmatex">\(x^2\)</span>, <span class="arithmatex">\(x^3\)</span>.</li>
</ul>
<h2 id="polynomial-regression_1">Polynomial Regression<a class="headerlink" href="#polynomial-regression_1" title="Permanent link">&para;</a></h2>
<h3 id="introduction_5">Introduction<a class="headerlink" href="#introduction_5" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong></li>
<li>Fit curves/non-linear functions.</li>
<li><strong>Dataset Example:</strong></li>
<li>Housing data with size (<span class="arithmatex">\(x\)</span>) as a feature.</li>
</ul>
<h3 id="polynomial-functions_1">Polynomial Functions<a class="headerlink" href="#polynomial-functions_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Options:</strong></li>
<li>Quadratic, cubic, etc.</li>
<li><span class="arithmatex">\(f(x) = w_1x + w_2x^2 + b\)</span>.</li>
<li><span class="arithmatex">\(f(x) = w_1x + w_2x^2 + w_3x^3 + b\)</span>.</li>
</ul>
<h3 id="model-selection">Model Selection<a class="headerlink" href="#model-selection" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Decision Criteria:</strong></li>
<li>Quadratic, cubic, etc.</li>
<li>Data-driven insights.</li>
<li>Continuous improvement.</li>
</ul>
<h3 id="feature-scaling">Feature Scaling<a class="headerlink" href="#feature-scaling" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Importance:</strong></li>
<li>Power-based features may vary in scale.</li>
<li>Ensure comparable ranges for efficient gradient descent.</li>
</ul>
<h3 id="feature-choices_1">Feature Choices<a class="headerlink" href="#feature-choices_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Options:</strong></li>
<li>Wide range: Different powers of <span class="arithmatex">\(x\)</span>.</li>
<li>Consider sqrt(<span class="arithmatex">\(x\)</span>) as an alternative.</li>
<li><strong>Decision Criteria:</strong></li>
<li>Model fitting.</li>
<li>Data characteristics.</li>
<li>Iterative exploration.</li>
</ul>
<h3 id="practical-considerations">Practical Considerations<a class="headerlink" href="#practical-considerations" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Code Implementation:</strong></li>
<li>See polynomial regression in action.</li>
<li>Explore features like <span class="arithmatex">\(x\)</span>, <span class="arithmatex">\(x^2\)</span>, <span class="arithmatex">\(x^3\)</span>.</li>
</ul>
<h3 id="next-steps-optional-labs">Next Steps: Optional Labs<a class="headerlink" href="#next-steps-optional-labs" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Opportunity:</strong></li>
<li>Implement polynomial regression using provided code.</li>
<li>Explore Scikit-learn, a widely used machine learning library.</li>
<li><strong>Practice:</strong></li>
<li>Reinforce learning through hands-on exercises.</li>
<li>Understand algorithm implementation nuances.</li>
</ul>
<h3 id="conclusion_14">Conclusion<a class="headerlink" href="#conclusion_14" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Key Takeaways:</strong></li>
<li>Feature engineering crucial for optimal algorithm performance.</li>
<li>Polynomial regression extends capabilities to fit non-linear data.</li>
<li><strong>Preparation for Next Week:</strong></li>
<li>Explore optional labs for practical insights.</li>
<li>Get ready for classification algorithms in the upcoming week.</li>
</ul>
<h1 id="week-3">week 3<a class="headerlink" href="#week-3" title="Permanent link">&para;</a></h1>
<h2 id="logistic-regression">Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permanent link">&para;</a></h2>
<h3 id="introduction_6">Introduction<a class="headerlink" href="#introduction_6" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Objective:</strong></li>
<li>Classification algorithm.</li>
</ul>
<h3 id="motivation-for-logistic-regression">Motivation for Logistic Regression<a class="headerlink" href="#motivation-for-logistic-regression" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Linear Regression Issues:</strong></li>
<li>Unsuitable for classification.</li>
<li>Demonstrated with tumor classification and email spam examples.</li>
</ul>
<h3 id="binary-classification">Binary Classification<a class="headerlink" href="#binary-classification" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong></li>
<li>Two possible output classes: 0 or 1.</li>
<li>Examples: Spam or not, Fraudulent or not, Malignant or benign.</li>
</ul>
<h3 id="logistic-regression-model">Logistic Regression Model<a class="headerlink" href="#logistic-regression-model" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Key Concept:</strong></li>
<li>Utilizes S-shaped curve (Sigmoid function).</li>
<li><strong>Sigmoid Function:</strong></li>
<li>Denoted as <span class="arithmatex">\(g(z)\)</span> or logistic function.</li>
<li>Formula: <span class="arithmatex">\(g(z) = \frac{1}{1 + e^{-z}}\)</span>.</li>
<li>Maps any real-valued number to the range <span class="arithmatex">\([0, 1]\)</span>.</li>
<li>Properties: <span class="arithmatex">\(g(z) \approx 0\)</span> for large negative <span class="arithmatex">\(z\)</span>, <span class="arithmatex">\(g(z) \approx 1\)</span> for large positive <span class="arithmatex">\(z\)</span>.</li>
<li><strong>Logistic Regression Equation:</strong></li>
<li><span class="arithmatex">\(f(x) = g(wx + b)\)</span>, where <span class="arithmatex">\(f(x)\)</span> is the predicted output.</li>
</ul>
<h3 id="decision-boundary">Decision Boundary<a class="headerlink" href="#decision-boundary" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong></li>
<li>Separates the space into regions associated with different class labels.</li>
<li><strong>Interpretation:</strong></li>
<li>Threshold (e.g., 0.5) determines class prediction.</li>
<li>If <span class="arithmatex">\(f(x) \geq 0.5\)</span>, predict class 1; else, predict class 0.</li>
</ul>
<h3 id="interpretation-of-logistic-regression-output">Interpretation of Logistic Regression Output<a class="headerlink" href="#interpretation-of-logistic-regression-output" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Probability Interpretation:</strong></li>
<li><span class="arithmatex">\(f(x)\)</span> represents the probability of <span class="arithmatex">\(y = 1\)</span>.</li>
<li><span class="arithmatex">\(1 - f(x)\)</span> represents the probability of <span class="arithmatex">\(y = 0\)</span>.</li>
<li>Probabilities add up to 1.</li>
</ul>
<h3 id="implementation_2">Implementation<a class="headerlink" href="#implementation_2" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Code Exploration:</strong></li>
<li>Optional lab to understand Sigmoid function implementation.</li>
<li>Visualize and compare with classification tasks.</li>
</ul>
<h3 id="application-in-advertising">Application in Advertising<a class="headerlink" href="#application-in-advertising" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Historical Context:</strong></li>
<li>Internet advertising driven by logistic regression variations.</li>
<li>Decision-making for displaying ads on websites.</li>
</ul>
<h2 id="cost-function-for-logistic-regression">Cost function for logistic regression<a class="headerlink" href="#cost-function-for-logistic-regression" title="Permanent link">&para;</a></h2>
<p>In logistic regression, the squared error cost function used in linear regression is not suitable. Instead, a different cost function is employed to ensure convexity and facilitate the use of gradient descent for optimization.</p>
<p>Consider a binary classification task with a training set containing m examples, each with n features denoted as X_1 through X_n. The logistic regression model is defined by the equation:</p>
<div class="arithmatex">\[ f(x) = \frac{1}{1 + e^{-(w \cdot x + b)}} \]</div>
<p>The goal is to choose parameters w and b that best fit the training data.</p>
<p>For logistic regression, a new cost function is introduced. The overall cost function is defined as the average loss over all training examples:</p>
<div class="arithmatex">\[ J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L(f(x^{(i)}), y^{(i)}) \]</div>
<p>Here, L is the loss function for a single training example, which measures how well the algorithm is performing on that example. The form of the loss function is chosen to ensure convexity:</p>
<div class="arithmatex">\[ L(f(x), y) = -y \log(f(x)) - (1 - y) \log(1 - f(x)) \]</div>
<ul>
<li>When y equals 1, the loss is <span class="arithmatex">\(-\log(f(x))\)</span>.</li>
<li>When y equals 0, the loss is <span class="arithmatex">\(-\log(1 - f(x))\)</span>.</li>
</ul>
<p>Let's examine the intuition behind this choice:</p>
<ol>
<li><strong>Case: <span class="arithmatex">\( y = 1 \)</span></strong></li>
<li>If the model predicts a probability close to 1 (high confidence) and the true label is 1, the loss is minimal.</li>
<li>If the prediction is around 0.5, the loss is higher but still moderate.</li>
<li>If the prediction is close to 0 (low confidence), the loss is significantly higher.</li>
</ol>
<p>This encourages the model to make accurate predictions, especially when the true label is 1.</p>
<ol>
<li><strong>Case: <span class="arithmatex">\( y = 0 \)</span></strong></li>
<li>If the model predicts a probability close to 0 (high confidence) and the true label is 0, the loss is minimal.</li>
<li>As the prediction moves towards 1, the loss increases, reaching infinity as the prediction approaches 1.</li>
</ol>
<p>This penalizes the model heavily when predicting a high probability of the event occurring (when <span class="arithmatex">\( y = 0 \)</span>) but the event does not occur.</p>
<p>The choice of this loss function ensures that the overall cost function is convex, facilitating the use of gradient descent for optimization. In the next video, the derivation of the gradient descent update rules for logistic regression will be explored.</p>
<h2 id="simplified-cost-function-for-logistic-regression">Simplified Cost Function for Logistic Regression<a class="headerlink" href="#simplified-cost-function-for-logistic-regression" title="Permanent link">&para;</a></h2>
<p>In the context of logistic regression, a simplified way to express the loss function is introduced, making the implementation more straightforward. Recall the original loss function for logistic regression:</p>
<div class="arithmatex">\[ L(f(x), y) = -y \log(f(x)) - (1 - y) \log(1 - f(x)) \]</div>
<p>To simplify, the loss function can be written as follows:</p>
<div class="arithmatex">\[ L(f(x), y) = -y \log(f) - (1 - y) \log(1 - f) \]</div>
<p>This single equation is equivalent to the original complex formula, and its simplicity becomes apparent when handling binary classification problems where y can only be 0 or 1.</p>
<p>Explaining the equivalence in the two cases (y=1 and y=0):</p>
<ol>
<li><strong>Case: <span class="arithmatex">\( y = 1 \)</span></strong></li>
<li><span class="arithmatex">\( -y \)</span> becomes <span class="arithmatex">\(-1\)</span> (since <span class="arithmatex">\(y = 1\)</span>), and <span class="arithmatex">\(1 - y\)</span> becomes <span class="arithmatex">\(0\)</span>.</li>
<li>
<p>The loss simplifies to <span class="arithmatex">\(-1 \log(f)\)</span>, which is the first term in the original expression.</p>
</li>
<li>
<p><strong>Case: <span class="arithmatex">\( y = 0 \)</span></strong></p>
</li>
<li><span class="arithmatex">\( -y \)</span> becomes <span class="arithmatex">\(0\)</span> (since <span class="arithmatex">\(y = 0\)</span>), and <span class="arithmatex">\(1 - y\)</span> becomes <span class="arithmatex">\(1\)</span>.</li>
<li>The loss simplifies to <span class="arithmatex">\(-(1 - y) \log(1 - f)\)</span>, equivalent to the second term in the original expression.</li>
</ol>
<p>Using this simplified loss function, the overall cost function <span class="arithmatex">\(J(w, b)\)</span> is derived, representing the average loss over the entire training set:</p>
<div class="arithmatex">\[ J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L(f(x^{(i)}), y^{(i)}) \]</div>
<p>This cost function is widely used for logistic regression and is derived from statistical principles, specifically maximum likelihood estimation. The key advantage is that it results in a convex cost function, allowing the use of gradient descent for optimization.</p>
<p>The upcoming optional lab will provide an opportunity to implement the logistic cost function in code and explore the impact of different parameter choices on the cost calculation. With the simplified cost function, the next step is to apply gradient descent to logistic regression, which will be covered in the following video.</p>
<h2 id="gradient-descent-implementation-for-logistic-regression">Gradient Descent Implementation for Logistic Regression<a class="headerlink" href="#gradient-descent-implementation-for-logistic-regression" title="Permanent link">&para;</a></h2>
<p>To fit the parameters (weights <span class="arithmatex">\(w\)</span> and bias <span class="arithmatex">\(\(b\)\)</span>) of a logistic regression model, the objective is to minimize the cost function <span class="arithmatex">\(J(w, b)\)</span>. Gradient descent is applied to achieve this goal. The gradient descent algorithm is as follows:</p>
<ol>
<li>Update$ <span class="arithmatex">\(w_j\)</span>$ for all <span class="arithmatex">\(j\)</span>: <span class="arithmatex">\(w_j := w_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (f^{(i)} - y^{(i)}) x_j^{(i)}\)</span></li>
<li>Update <span class="arithmatex">\(\(b\)\)</span>: <span class="arithmatex">\(b := b - \alpha \frac{1}{m} \sum_{i=1}^{m} (f^{(i)} - y^{(i)})\)</span></li>
</ol>
<p>Here, <span class="arithmatex">\(f^{(i)}\)</span> is the predicted output for training example <span class="arithmatex">\(i\)</span>, and <span class="arithmatex">\(y^{(i)}\)</span> is the true label. The learning rate <span class="arithmatex">\(\alpha\)</span> controls the step size in each iteration.</p>
<p>Derivatives of the cost function are involved in these updates:</p>
<ol>
<li>Derivative of <span class="arithmatex">\(J\)</span> with respect to$ <span class="arithmatex">\(w_j\)</span>$: <span class="arithmatex">\(\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} (f^{(i)} - y^{(i)}) x_j^{(i)}\)</span></li>
<li>Derivative of <span class="arithmatex">\(J\)</span> with respect to <span class="arithmatex">\(\(b\)\)</span>: <span class="arithmatex">\(\(\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (f^{(i)} - y^{(i)})\)\)</span></li>
</ol>
<p>These derivatives represent the average error in predictions and are used to update the parameters in the direction that minimizes the cost.</p>
<p>It's important to note that even though these equations resemble those of linear regression, the crucial difference lies in the definition of the function <span class="arithmatex">\(f(x)\)</span>. In linear regression, <span class="arithmatex">\(f(x) = wx + b\)</span>, while in logistic regression, <span class="arithmatex">\(f(x)\)</span> is the sigmoid function applied to <span class="arithmatex">\(wx + b\)</span>.</p>
<p>The use of feature scaling, as introduced in linear regression, remains applicable to logistic regression. Scaling features to similar ranges can help gradient descent converge faster.</p>
<p>For a more efficient implementation, vectorization is introduced, similar to what was discussed for linear regression. However, details of vectorized implementation are not covered in this video.</p>
<p>An optional lab following this video provides insights into calculating the gradient for logistic regression in code. The lab also includes animated plots showing the sigmoid function, contour plot of the cost, 3D surface plot of the cost, and the learning curve evolving as gradient descent runs. Another optional lab introduces the use of scikit-learn, a popular machine learning library, to train logistic regression models for classification tasks.</p>
<p>This video concludes the implementation of logistic regression using gradient descent, marking the completion of a powerful and widely used learning algorithm. The viewer is congratulated for acquiring the knowledge and skills to implement logistic regression independently.</p>
<h2 id="understanding-overfitting-and-addressing-it">Understanding Overfitting and Addressing it<a class="headerlink" href="#understanding-overfitting-and-addressing-it" title="Permanent link">&para;</a></h2>
<p>In this video, we explore the issues of overfitting and underfitting in machine learning models, using examples from linear regression and logistic regression. Overfitting occurs when a model fits the training data too closely, capturing noise and not generalizing well to new data. On the other hand, underfitting happens when the model is too simple to capture the underlying patterns in the data.</p>
<p><strong>Overfitting in Linear Regression:</strong>
- Example: Predicting housing prices based on house size.
- Underfitting: A linear model that doesn't capture the underlying pattern in the data.
- Overfitting: Using a high-order polynomial (e.g., fourth-degree) that fits the training data perfectly but doesn't generalize well.</p>
<p><strong>Overfitting in Logistic Regression:</strong>
- Example: Classifying tumors as malignant or benign based on tumor size and patient age.
- Underfitting: A simple linear decision boundary.
- Just Right: A model with quadratic features that provides a good fit and generalizes well.
- Overfitting: A very high-order polynomial that fits the training data too closely.</p>
<p><strong>Goldilocks Principle:</strong>
- The goal is to find a model that is "just right," neither underfitting nor overfitting.
- Achieving this involves choosing an appropriate set of features and balancing model complexity.</p>
<p><strong>Addressing Overfitting:</strong>
1. <strong>Collect More Data:</strong>
   - Increasing the size of the training set can help the algorithm generalize better.
   - Not always feasible but highly effective when possible.</p>
<ol>
<li><strong>Reduce the Number of Features:</strong></li>
<li>Selecting a subset of relevant features can simplify the model and reduce overfitting.</li>
<li>
<p>Feature selection can be done manually based on intuition or automatically using algorithms.</p>
</li>
<li>
<p><strong>Regularization:</strong></p>
</li>
<li>Regularization is a technique to prevent overfitting by penalizing large parameter values.</li>
<li>It encourages the algorithm to use smaller parameter values, reducing the impact of individual features.</li>
<li>Regularization helps find a balance between fitting the training data and preventing overfitting.</li>
</ol>
<p><strong>Regularization in Detail (Next Video):</strong>
- Regularization discourages overly large parameter values.
- It allows keeping all features but prevents them from having an overly large effect.
- A detailed explanation of regularization and its application to linear and logistic regression will be covered in the next video.</p>
<p><strong>Lab on Overfitting (Optional):</strong>
- The lab provides a hands-on experience with examples of overfitting.
- Users can interactively adjust parameters, add data points, and explore the impact on model fitting.
- It includes options to address overfitting by adding more data or selecting features.</p>
<p>In the next video, the focus will be on understanding regularization in depth and applying it to linear and logistic regression, providing a tool to effectively combat overfitting.</p>
<h2 id="cost-function-with-regularization">Cost Function with Regularization<a class="headerlink" href="#cost-function-with-regularization" title="Permanent link">&para;</a></h2>
<p>In this video, we delve into the concept of regularization and how it can be incorporated into the cost function of a learning algorithm. The primary goal of regularization is to prevent overfitting by penalizing large parameter values. The video uses the example of predicting housing prices with linear regression to illustrate the concept.</p>
<p><strong>Example: Overfitting in Linear Regression</strong>
- Quadratic function provides a good fit to the data.
- Very high-order polynomial leads to overfitting.</p>
<p><strong>Regularization Intuition:</strong>
- Modify the cost function to penalize large parameter values.
- Introduce a regularization term: <span class="arithmatex">\( \lambda \sum_{j=1}^{n} W_j^2 \)</span>.
- Large values of <span class="arithmatex">\( W_j \)</span> are penalized, encouraging smaller values.</p>
<p><strong>Regularization Implementation:</strong>
- Penalize all parameters <span class="arithmatex">\( W_j \)</span> by adding <span class="arithmatex">\( \lambda \sum_{j=1}^{n} W_j^2 \)</span> to the cost function.
- <span class="arithmatex">\(\lambda\)</span> is the regularization parameter, similar to the learning rate <span class="arithmatex">\(\alpha\)</span>.
- Convention: Scale the regularization term by <span class="arithmatex">\( \frac{\lambda}{2m} \)</span> for simplicity.</p>
<p><strong>Regularization for Multiple Features:</strong>
- If there are many features (e.g., 100), penalize all <span class="arithmatex">\( W_j \)</span> parameters.
- Introduce regularization term: <span class="arithmatex">\( \lambda \sum_{j=1}^{n} W_j^2 \)</span>.
- <span class="arithmatex">\( \lambda \)</span> must be chosen carefully to balance fitting the data and preventing overfitting.</p>
<p><strong>Regularization and Model Complexity:</strong>
- Regularization promotes smaller parameter values, akin to a simpler model.
- Helps in avoiding overfitting and obtaining a smoother, less complex function.</p>
<p><strong>Regularization Term in the Cost Function:</strong>
- Original cost function: Mean Squared Error (MSE).
- Modified cost function: MSE + Regularization Term.</p>
<p><strong>Trade-off with Regularization Parameter <span class="arithmatex">\( \lambda \)</span>:</strong>
- The choice of <span class="arithmatex">\( \lambda \)</span> determines the trade-off between fitting the data and preventing overfitting.
- Large <span class="arithmatex">\( \lambda \)</span>: Smaller parameter values, underfitting.
- Small <span class="arithmatex">\( \lambda \)</span>: Larger parameter values, overfitting.</p>
<p><strong>Regularization in Linear Regression:</strong>
- Balancing the goals of fitting the training data and keeping parameters small.
- Different values of <span class="arithmatex">\( \lambda \)</span> result in different model behaviors.</p>
<p><strong>Model Behavior with <span class="arithmatex">\( \lambda \)</span>:</strong>
- <span class="arithmatex">\( \lambda = 0 \)</span>: Overfitting, overly complex curve.
- Large <span class="arithmatex">\( \lambda \)</span>: Underfitting, horizontal straight line.
- Optimal <span class="arithmatex">\( \lambda \)</span>: Balanced, fits a higher-order polynomial while preventing overfitting.</p>
<p><strong>Model Selection and Choosing <span class="arithmatex">\( \lambda \)</span>:</strong>
- Later discussions will cover various methods for selecting an appropriate <span class="arithmatex">\( \lambda \)</span>.
- Choosing the right <span class="arithmatex">\( \lambda \)</span> is crucial for effective regularization.</p>
<p>In the upcoming videos, the focus will be on applying regularization to linear and logistic regression. The goal is to provide insights on how to effectively train models, avoid overfitting, and strike a balance between fitting the data and preventing complexity.</p>
<h2 id="regularized-linear-regression-with-gradient-descent">Regularized Linear Regression with Gradient Descent<a class="headerlink" href="#regularized-linear-regression-with-gradient-descent" title="Permanent link">&para;</a></h2>
<p>In this video, we explore how to adapt gradient descent for regularized linear regression. The cost function now includes a regularization term, and the goal is to find parameters <span class="arithmatex">\( w \)</span> and <span class="arithmatex">\( b \)</span> that minimize this regularized cost function. The update rules for <span class="arithmatex">\( w_j \)</span> and <span class="arithmatex">\( b \)</span> are derived, incorporating the regularization term.</p>
<p><strong>Cost Function for Regularized Linear Regression:</strong>
- Original squared error cost function.
- Additional regularization term with <span class="arithmatex">\( \lambda \)</span> as the regularization parameter.</p>
<p><strong>Gradient Descent Update Rules:</strong>
- Original gradient descent update for unregularized linear regression:
  [ w_j := w_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x_j^{(i)} ]
  [ b := b - \alpha \frac{1}{m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) ]
- Updated rules for regularized linear regression:
  [ w_j := w_j \left(1 - \alpha \frac{\lambda}{m}\right) - \alpha \frac{1}{m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x_j^{(i)} ]
  [ b := b - \alpha \frac{1}{m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) ]</p>
<p><strong>Optional Intuition:</strong>
- The first term <span class="arithmatex">\(1 - \alpha \frac{\lambda}{m}\)</span> acts as a shrinkage factor.
- Regularization introduces a small reduction (<span class="arithmatex">\(&lt;1\)</span>) to$ <span class="arithmatex">\(w_j\)</span>$ on each iteration.
- This leads to a gradual shrinkage of$ <span class="arithmatex">\(w_j\)</span>$, preventing overfitting.</p>
<p><strong>Derivation of Update Rules:</strong>
- Derivatives of the regularized cost function lead to modified update rules.
- <span class="arithmatex">\( \frac{\partial J}{\partial w_j} \)</span> includes an additional term: <span class="arithmatex">\( \frac{\lambda}{m} w_j \)</span>.
- Regularization term impacts$ <span class="arithmatex">\(w_j\)</span>$ updates, encouraging smaller values.</p>
<p><strong>Mathematical Intuition:</strong>
- The regularization term $ (<span class="arithmatex">\( \frac{\lambda}{m} w_j \)</span>)$ has a shrinking effect.
- On each iteration,$ <span class="arithmatex">\(w_j\)</span>$ is multiplied by <span class="arithmatex">\(1 - \alpha \frac{\lambda}{m}\)</span>.
- The balance between fitting the data and regularization is controlled by <span class="arithmatex">\( \lambda \)</span>.</p>
<p><strong>Summary:</strong>
- Regularized linear regression aims to prevent overfitting by introducing a regularization term.
- Gradient descent updates include a shrinkage factor to control parameter values.
- Balancing regularization and fitting data is crucial for effective model training.</p>
<p><strong>Optional Derivation Slide:</strong>
- Derivative calculation of <span class="arithmatex">\( \frac{\partial J}{\partial w_j} \)</span> is provided for those interested in the mathematical details.</p>
<p>This video equips you with the knowledge to implement regularized linear regression using gradient descent. Regularization proves particularly useful when dealing with a large number of features and a relatively small training set. In the next video, we'll extend this regularization concept to logistic regression to address overfitting in that context.</p>
<h2 id="regularized-logistic-regression">Regularized Logistic Regression<a class="headerlink" href="#regularized-logistic-regression" title="Permanent link">&para;</a></h2>
<p>In this video, we delve into the implementation of regularized logistic regression. Similar to regularized linear regression, the gradient descent update for regularized logistic regression bears resemblance to its unregularized counterpart. The goal is to address overfitting, especially when dealing with numerous features.</p>
<p><strong>Challenges with Logistic Regression:</strong>
- Logistic regression can overfit with high-order polynomial features, leading to complex decision boundaries.
- When training with many features, there's an increased risk of overfitting.</p>
<p><strong>Cost Function Modification for Regularization:</strong>
- Original logistic regression cost function.
- Additional regularization term: <span class="arithmatex">\( \frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2 \)</span>, where <span class="arithmatex">\( \lambda \)</span> is the regularization parameter.</p>
<p><strong>Gradient Descent Update Rules:</strong>
- Similar to regularized linear regression.
- Simultaneous updates for <span class="arithmatex">\( w_j \)</span> and <span class="arithmatex">\( b \)</span>.
- Derivative with respect to <span class="arithmatex">\( w_j \)</span> includes an additional term: <span class="arithmatex">\( \frac{\lambda}{m} w_j \)</span>.
- The logistic function is applied to <span class="arithmatex">\( z \)</span> in the definition of <span class="arithmatex">\( f \)</span>.</p>
<p><strong>Implementation of Regularized Logistic Regression:</strong>
- Apply gradient descent to minimize the cost function.
- Regularize only the parameters <span class="arithmatex">\( w_j \)</span> and not <span class="arithmatex">\( b \)</span>.</p>
<p><strong>Optional Lab and Practical Application:</strong>
- In an optional lab, you can experiment with regularization to combat overfitting.
- Engineers in the industry often use logistic regression and its regularization techniques to create valuable applications.</p>
<p><strong>Closing Remarks:</strong>
- Understanding linear and logistic regression, along with regularization, is powerful for practical applications.
- Congratulations on completing the course.
- Further learning in the next course includes neural networks, which build on the concepts covered so far.</p>
<p>This video provides practical insights into implementing regularized logistic regression, an essential skill in machine learning applications. The optional lab allows you to experiment with regularization, enhancing your understanding of its impact. Congratulations on your progress, and get ready for the exciting world of neural networks in the next course!</p>


  




                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../../" class="md-footer__link md-footer__link--prev" aria-label="Previous: Core Computer Science" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Core Computer Science
            </div>
          </div>
        </a>
      
      
        
        <a href="../ML%20Specialization%20-%20Course%202/" class="md-footer__link md-footer__link--next" aria-label="Next: ML Specialization - Course 2" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              ML Specialization - Course 2
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2020-2024  Jawad Haider  <a href="#__consent">Change cookie settings</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/qalmaqihir" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.linkedin.com/in/jawad-haider-uca/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.youtube.com/channel/UCB-D3NBU6UZ5N7IGKOJxtqQ" target="_blank" rel="noopener" title="www.youtube.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://t.me/Qalmaqihir" target="_blank" rel="noopener" title="t.me" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M248 8C111.033 8 0 119.033 0 256s111.033 248 248 248 248-111.033 248-248S384.967 8 248 8Zm114.952 168.66c-3.732 39.215-19.881 134.378-28.1 178.3-3.476 18.584-10.322 24.816-16.948 25.425-14.4 1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25 5.342-39.5 3.652-3.793 67.107-61.51 68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608 69.142-14.845 10.194-26.894 9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7 18.45-13.7 108.446-47.248 144.628-62.3c68.872-28.647 83.183-33.623 92.511-33.789 2.052-.034 6.639.474 9.61 2.885a10.452 10.452 0 0 1 3.53 6.716 43.765 43.765 0 0 1 .417 9.769Z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/qalmaqihir" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://instagram.com/qalmaqihir" target="_blank" rel="noopener" title="instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["search.share", "search.highlight", "search.sugguest", "content.code.annotate", "content.tooltips", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.e21f8df8.min.js"></script>
      
    
  </body>
</html>