
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="All of my Computer Science & AI/ML/DL/ Book notes, BootCamp notes & Useful materials for anyone who  wants to learn; Knowledge should be free for those who need it.">
      
      
        <meta name="author" content="Jawad Haider">
      
      
        <link rel="canonical" href="https://qalmaqihir.github.io/corecs/ML_Specialization/ML%20Specialization%20-%20Course%203/">
      
      <link rel="icon" href="../../../assets/logo.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-8.5.3">
    
    
      
        <title>ML Specialization - Course 3 - CS Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.80dcb947.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ml-specialization-course-3" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
Follow us on
<a href="https://twitter.com/qalmaqihir" target="_blank" rel="noopener">
    <span class="twemoji twitter" style="color: #1DA1F2">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
            <path
                d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z">
            </path>
        </svg>
    </span>
    <strong>Twitter</strong>
</a> and

<a href="https://www.youtube.com/channel/UCB-D3NBU6UZ5N7IGKOJxtqQ?sub_confirmation=1" target="_blank" rel="noopener">
    <span class="twemoji youtube" style="color: #FF0000;">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512">
            <path
                d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z">
            </path>
        </svg>
    </span>
    <strong>YouTube</strong>
</a>  and stay <strong>Updated !</strong> for New Content

<!-- We use the twemoji project for the pancake emoji symbol
and for other emoji and icon purposes.
Check them out here: https://github.com/twitter/twemoji
-->

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="CS Notes" class="md-header__button md-logo" aria-label="CS Notes" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CS Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ML Specialization - Course 3
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/qalmaqihir/qalmaqihir.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    qalmaqihir/qalmaqihir.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        Home
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../booksnotes/" class="md-tabs__link">
        Books Notes
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../bootcampsnotes/" class="md-tabs__link">
        Bootcamps Notes
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../competitiveprogramming/" class="md-tabs__link">
        Competitive Programming
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../" class="md-tabs__link md-tabs__link--active">
        Core Computer Science
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../myresearch/" class="md-tabs__link">
        My Research
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../about/" class="md-tabs__link">
        About
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../blogs/" class="md-tabs__link">
        Blog
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../contact/" class="md-tabs__link">
        Contact
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="CS Notes" class="md-nav__button md-logo" aria-label="CS Notes" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    CS Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/qalmaqihir/qalmaqihir.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    qalmaqihir/qalmaqihir.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
        
          
            
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../..">Home</a>
          
            <label for="__nav_1">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Home" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Home
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/" class="md-nav__link">
        Books Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/" class="md-nav__link">
        Bootcamps Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../competitiveprogramming/" class="md-nav__link">
        Competitive Programming
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        Core Computer Science
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../myresearch/" class="md-nav__link">
        My Research
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../blogs/" class="md-nav__link">
        Blogs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../contact/" class="md-nav__link">
        Contact
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
        
          
            
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../booksnotes/">Books Notes</a>
          
            <label for="__nav_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Books Notes" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Books Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
        
          
            
          
        
          
            
          
        
          
        
          
        
          
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../booksnotes/pythonDataScienceHandBook/">Data Science Handbook</a>
          
            <label for="__nav_2_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Data Science Handbook" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          Data Science Handbook
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_3" type="checkbox" id="__nav_2_2_3" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2_3">
          Chapter 2 - Introduction to Numpy
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Chapter 2 - Introduction to Numpy" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_3">
          <span class="md-nav__icon md-icon"></span>
          Chapter 2 - Introduction to Numpy
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/00_Understanding_Data_Types_in_Python/" class="md-nav__link">
        00 Understanding Data Types in Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/01_basics_of_numpy_arrays/" class="md-nav__link">
        01 basics of numpy arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/02_Computation_on_NumPy_Arrays/" class="md-nav__link">
        02 Computation on NumPy Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/03_Aggregation_Min_Max/" class="md-nav__link">
        03 Aggregation Min Max
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/04_Computation_on_Arrays_Broadcasting/" class="md-nav__link">
        04 Computation on Arrays Broadcasting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/05_Comparisons_%20Masks_and_Boolean_Logic/" class="md-nav__link">
        05 Comparisons  Masks and Boolean Logic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/06_Fany_Indexing/" class="md-nav__link">
        06 Fany Indexing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/07_Sorted_Arrays/" class="md-nav__link">
        07 Sorted Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt2_Introduction_to_NumPy/08_%20Structured_Data/" class="md-nav__link">
        08  Structured Data
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_4" type="checkbox" id="__nav_2_2_4" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2_4">
          Chapter 3 - Data Manipulation with Pandas
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Chapter 3 - Data Manipulation with Pandas" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_4">
          <span class="md-nav__icon md-icon"></span>
          Chapter 3 - Data Manipulation with Pandas
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/01_Introduction%20to%20Pandas%20Objects/" class="md-nav__link">
        01 Introduction to Pandas Objects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/02_Data%20Indexing%20and%20Selection/" class="md-nav__link">
        02 Data Indexing and Selection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/03_Operating%20on%20Data%20in%20Pandas/" class="md-nav__link">
        03 Operating on Data in Pandas
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/04_Handling%20Missing%20Data/" class="md-nav__link">
        04 Handling Missing Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/05_Hierarchical%20Indexing/" class="md-nav__link">
        05 Hierarchical Indexing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/06_Combine%20dataset%20Concat%20and%20Append/" class="md-nav__link">
        06 Combine dataset Concat and Append
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/07_Combining%20Dataset%20Merge%20and%20Join/" class="md-nav__link">
        07 Combining Dataset Merge and Join
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/08_Aggregation%20and%20Grouping/" class="md-nav__link">
        08 Aggregation and Grouping
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/09_Pivot%20Tables/" class="md-nav__link">
        09 Pivot Tables
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/10_Vectoried%20String%20Operations/" class="md-nav__link">
        10 Vectoried String Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/11_Working%20with%20Time%20Series/" class="md-nav__link">
        11 Working with Time Series
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/12-Frequency%20Offset/" class="md-nav__link">
        12 Frequency Offset
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/13_Resampling%20Shifting%20and%20Windowing/" class="md-nav__link">
        13 Resampling Shifting and Windowing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example%20Visualizing%20Seattle%20Bicycle%20Counts/" class="md-nav__link">
        Example Visualizing Seattle Bicycle Counts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20Birthrate%20Data/" class="md-nav__link">
        Example  Birthrate Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_%20US%20States%20Data%20%28Merge%20and%20Join%29/" class="md-nav__link">
        Example  US States Data (Merge and Join)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt3_Data_Manipulation_with_Pandas/Example_Recipe%20Database/" class="md-nav__link">
        Example Recipe Database
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2_5" type="checkbox" id="__nav_2_2_5" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2_2_5">
          Chapter 4 - Visualization with Matplotlib
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Chapter 4 - Visualization with Matplotlib" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_2_5">
          <span class="md-nav__icon md-icon"></span>
          Chapter 4 - Visualization with Matplotlib
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/01general%20Matplotlib%20tips/" class="md-nav__link">
        01general Matplotlib tips
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/02simple_lineplots/" class="md-nav__link">
        02simple lineplots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/03simple%20scatter%20plots/" class="md-nav__link">
        03simple scatter plots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/04visualizing%20errors/" class="md-nav__link">
        04visualizing errors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/05density%20and%20contour%20plots/" class="md-nav__link">
        05density and contour plots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/06Histograms%20Binnings%20and%20Density/" class="md-nav__link">
        06Histograms Binnings and Density
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/07customized%20plot%20legends/" class="md-nav__link">
        07customized plot legends
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/08customizing%20colorbar/" class="md-nav__link">
        08customizing colorbar
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/09multiple%20subplots/" class="md-nav__link">
        09multiple subplots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/10text%20and%20annotation%20Example/" class="md-nav__link">
        10text and annotation Example
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/11customizing%20ticks/" class="md-nav__link">
        11customizing ticks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/12customizing%20matplotlib%20configuration%20and%20stylesheets/" class="md-nav__link">
        12customizing matplotlib configuration and stylesheets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/13threedimensional%20plotting/" class="md-nav__link">
        13threedimensional plotting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/14_geographic%20data%20with%20basemap/" class="md-nav__link">
        14 geographic data with basemap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/15visualiztion%20with%20seaborn/" class="md-nav__link">
        15visualiztion with seaborn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20California%20cities/" class="md-nav__link">
        example California cities
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Exploring%20Marathon%20Finishing%20times/" class="md-nav__link">
        Example Exploring Marathon Finishing times
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Handwritten%20Digits/" class="md-nav__link">
        Example Handwritten Digits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/example%20surface%20temperature%20data/" class="md-nav__link">
        Example surface temperature data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../booksnotes/pythonDataScienceHandBook/chpt4_Visualization%20with%20Matplotlib/Example%20Visualizing%20a%20Mobius%20Strip/" class="md-nav__link">
        Example Visualizing a Mobius Strip
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
      
        
          
            
          
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../booksnotes/pythonDataScienceHandBook/">Machine Learning Handbook</a>
          
            <label for="__nav_2_3">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Machine Learning Handbook" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning Handbook
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../bootcampsnotes/">Bootcamps Notes</a>
          
            <label for="__nav_3">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Bootcamps Notes" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Bootcamps Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_2">
          Numpy Crash Course
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Numpy Crash Course" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          Numpy Crash Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2_1" type="checkbox" id="__nav_3_2_1" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_2_1">
          Numpy Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Numpy Topics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_2_1">
          <span class="md-nav__icon md-icon"></span>
          Numpy Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/00-NumPy-Arrays/" class="md-nav__link">
        00 NumPy Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/01-NumPy-Indexing-and-Selection/" class="md-nav__link">
        01 NumPy Indexing and Selection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/02-NumPy-Operations/" class="md-nav__link">
        02 NumPy Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/03-NumPy-Exercises/" class="md-nav__link">
        03 NumPy Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/numpy/04-NumPy-Exercises-Solutions/" class="md-nav__link">
        04 NumPy Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_3" type="checkbox" id="__nav_3_3" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_3">
          Pandas Crash Course
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Pandas Crash Course" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          Pandas Crash Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_3_1" type="checkbox" id="__nav_3_3_1" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_3_1">
          Pandas Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Pandas Topics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_3_1">
          <span class="md-nav__icon md-icon"></span>
          Pandas Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/00-Intro-to-Pandas/" class="md-nav__link">
        00 Intro to Pandas
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/01-Series/" class="md-nav__link">
        01 Series
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/02-DataFrames/" class="md-nav__link">
        02 DataFrames
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/03-Missing-Data/" class="md-nav__link">
        03 Missing Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/04-Groupby/" class="md-nav__link">
        04 Groupby
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/05-Operations/" class="md-nav__link">
        05 Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/06-Data-Input-and-Output/" class="md-nav__link">
        06 Data Input and Output
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/07-Pandas-Exercises/" class="md-nav__link">
        07 Pandas Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pandas/08-Pandas-Exercises-Solutions/" class="md-nav__link">
        08 Pandas Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_4" type="checkbox" id="__nav_3_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_4">
          PyTorch Baics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="PyTorch Baics" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          PyTorch Baics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_4_1" type="checkbox" id="__nav_3_4_1" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_4_1">
          PyTorch Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="PyTorch Topics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_4_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/00-Tensor-Basics/" class="md-nav__link">
        00 Tensor Basics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/01-Tensor-Operations/" class="md-nav__link">
        01 Tensor Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/02-PyTorch-Basics-Exercises/" class="md-nav__link">
        02 PyTorch Basics Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorch/03-PyTorch-Basics-Exercises-Solutions/" class="md-nav__link">
        03 PyTorch Basics Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5" type="checkbox" id="__nav_3_5" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5">
          PyTorch for Deeplearning Bootcamp
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="PyTorch for Deeplearning Bootcamp" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          PyTorch for Deeplearning Bootcamp
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_1" type="checkbox" id="__nav_3_5_1" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_1">
          ANN - Artificial Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ANN - Artificial Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_1">
          <span class="md-nav__icon md-icon"></span>
          ANN - Artificial Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/" class="md-nav__link">
        00 PyTorch Gradients
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/01-Linear-Regression-with-PyTorch/" class="md-nav__link">
        01 Linear Regression with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/02-DataSets-with-Pytorch/" class="md-nav__link">
        02 DataSets with Pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/03-Basic-PyTorch-NN/" class="md-nav__link">
        03 Basic PyTorch NN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04a-Full-ANN-Code-Along-Regression/" class="md-nav__link">
        04a Full ANN Code Along Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/04b-Full-ANN-Code-Along-Classification/" class="md-nav__link">
        04b Full ANN Code Along Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/05-Neural-Network-Exercises/" class="md-nav__link">
        05 Neural Network Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/06-Neural-Network-Exercises-Solutions/" class="md-nav__link">
        06 Neural Network Exercises Solutions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/07-Recap-Saving-and-Loading-Trained-Models/" class="md-nav__link">
        07 Recap Saving and Loading Trained Models
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_2" type="checkbox" id="__nav_3_5_2" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_2">
          CNN - Convolutional Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CNN - Convolutional Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_2">
          <span class="md-nav__icon md-icon"></span>
          CNN - Convolutional Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/00-MNIST-ANN-Code-Along/" class="md-nav__link">
        00 MNIST ANN Code Along
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/01-MNIST-with-CNN/" class="md-nav__link">
        01 MNIST with CNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/02-CIFAR-CNN-Code-Along/" class="md-nav__link">
        02 CIFAR CNN Code Along
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/03-Loading-Real-Image-Data/" class="md-nav__link">
        03 Loading Real Image Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/04-CNN-on-Custom-Images/" class="md-nav__link">
        04 CNN on Custom Images
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/05-CNN-Exercises/" class="md-nav__link">
        05 CNN Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/03-CNN-Convolutional-Neural-Networks/06-CNN-Exercises-Solutions/" class="md-nav__link">
        06 CNN Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_3" type="checkbox" id="__nav_3_5_3" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_3">
          RNN - Recurrent Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="RNN - Recurrent Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_3">
          <span class="md-nav__icon md-icon"></span>
          RNN - Recurrent Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/00-Basic-RNN/" class="md-nav__link">
        00 Basic RNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/01-RNN-on-a-Time-Series/" class="md-nav__link">
        01 RNN on a Time Series
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/02-RNN-Exercises/" class="md-nav__link">
        02 RNN Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/04-RNN-Recurrent-Neural-Networks/03-RNN-Exercises-Solutions/" class="md-nav__link">
        03 RNN Exercises Solutions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_4" type="checkbox" id="__nav_3_5_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_4">
          Using GPU
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Using GPU" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_4">
          <span class="md-nav__icon md-icon"></span>
          Using GPU
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/05-Using-GPU/00-Using-GPU-and-CUDA/" class="md-nav__link">
        00 Using GPU and CUDA
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_5_5" type="checkbox" id="__nav_3_5_5" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5_5">
          NLP with PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP with PyTorch" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_5_5">
          <span class="md-nav__icon md-icon"></span>
          NLP with PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/pytorchDLbootcamp/06-NLP-with-PyTorch/00-RNN-for-Text-Generation%20/" class="md-nav__link">
        00 RNN for Text Generation 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6" type="checkbox" id="__nav_3_6" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6">
          Tensorflow for Deeplearning Bootcamp
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tensorflow for Deeplearning Bootcamp" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          Tensorflow for Deeplearning Bootcamp
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_1" type="checkbox" id="__nav_3_6_1" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_1">
          Colab Basics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Colab Basics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_1">
          <span class="md-nav__icon md-icon"></span>
          Colab Basics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Demo/" class="md-nav__link">
        TF2 0 Demo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Installing_Tensorflow/" class="md-nav__link">
        TF2 0 Installing Tensorflow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ColabBasics/TF2_0_Loading_Data/" class="md-nav__link">
        TF2 0 Loading Data
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_2" type="checkbox" id="__nav_3_6_2" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_2">
          Machine Learning Basics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Machine Learning Basics" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_2">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning Basics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Classification/" class="md-nav__link">
        TF2 0 Linear Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/MachineLearningBasics/TF2_0_Linear_Regression/" class="md-nav__link">
        TF2 0 Linear Regression
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_3" type="checkbox" id="__nav_3_6_3" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_3">
          ANN - Artificial Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ANN - Artificial Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_3">
          <span class="md-nav__icon md-icon"></span>
          ANN - Artificial Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_MNIST/" class="md-nav__link">
        TF2 0 ANN MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/ANN/TF2_0_ANN_Regression/" class="md-nav__link">
        TF2 0 ANN Regression
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_4" type="checkbox" id="__nav_3_6_4" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_4">
          CNN - Convolutional Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CNN - Convolutional Neural Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_4">
          <span class="md-nav__icon md-icon"></span>
          CNN - Convolutional Neural Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_Fashion_MNIST/" class="md-nav__link">
        TF2 0 Fashion MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR/" class="md-nav__link">
        TF2 0 CIFAR
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/CNN/TF2_0_CIFAR_Improved/" class="md-nav__link">
        TF2 0 CIFAR Improved
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_5" type="checkbox" id="__nav_3_6_5" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_5">
          RNN - Recurrent Neural Network
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="RNN - Recurrent Neural Network" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_5">
          <span class="md-nav__icon md-icon"></span>
          RNN - Recurrent Neural Network
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Autoregressive_Model/" class="md-nav__link">
        TF2 0 Autoregressive Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_SimpleRNN_Sine/" class="md-nav__link">
        TF2 0 SimpleRNN Sine
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_Shapes/" class="md-nav__link">
        TF2 0 RNN Shapes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_LSTM_Nonlinear/" class="md-nav__link">
        TF2 0 LSTM Nonlinear
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Long_Distance/" class="md-nav__link">
        TF2 0 Long Distance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_RNN_MNIST/" class="md-nav__link">
        TF2 0 RNN MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RNN/TF2_0_Stock_Returns/" class="md-nav__link">
        TF2 0 Stock Returns
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_6" type="checkbox" id="__nav_3_6_6" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_6">
          NLP - Natural Language Processing
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP - Natural Language Processing" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_6">
          <span class="md-nav__icon md-icon"></span>
          NLP - Natural Language Processing
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Text_Preprocessing/" class="md-nav__link">
        TF2 0 Text Preprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_CNN/" class="md-nav__link">
        TF2 0 Spam Detection CNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/NaturalLanguageProcessing/TF2_0_Spam_Detection_RNN/" class="md-nav__link">
        TF2 0 Spam Detection RNN
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_7" type="checkbox" id="__nav_3_6_7" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_7">
          Recommendar Systems
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Recommendar Systems" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_7">
          <span class="md-nav__icon md-icon"></span>
          Recommendar Systems
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/RecommenderSystem/TF2_0_Recommender_System/" class="md-nav__link">
        TF2 0 Recommender System
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_8" type="checkbox" id="__nav_3_6_8" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_8">
          Transfer Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Transfer Learning" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_8">
          <span class="md-nav__icon md-icon"></span>
          Transfer Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning/" class="md-nav__link">
        TF2 0 Transfer Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/TransferLearning/TF2_0_Transfer_Learning_with_Data_Augmentation/" class="md-nav__link">
        TF2 0 Transfer Learning with Data Augmentation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_9" type="checkbox" id="__nav_3_6_9" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_9">
          GANs - Generative Adversarial Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="GANs - Generative Adversarial Networks" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_9">
          <span class="md-nav__icon md-icon"></span>
          GANs - Generative Adversarial Networks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/GANs/TF2_0_GAN/" class="md-nav__link">
        TF2 0 GAN
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_10" type="checkbox" id="__nav_3_6_10" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_10">
          Advance Tensorflow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Advance Tensorflow" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_10">
          <span class="md-nav__icon md-icon"></span>
          Advance Tensorflow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Serving/" class="md-nav__link">
        TF2 0 Serving
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_Mirrored_Strategy/" class="md-nav__link">
        TF2 0 Mirrored Strategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TF2_0_TFLite/" class="md-nav__link">
        TF2 0 TFLite
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/AdvanceTensorflowUage/TPU/" class="md-nav__link">
        TPU
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6_11" type="checkbox" id="__nav_3_6_11" >
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6_11">
          Low-level Transorflow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Low-level Transorflow" data-md-level="3">
        <label class="md-nav__title" for="__nav_3_6_11">
          <span class="md-nav__icon md-icon"></span>
          Low-level Transorflow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Basic_Computation/" class="md-nav__link">
        TF2 0 Basic Computation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Variables_and_Gradient_Tape/" class="md-nav__link">
        TF2 0 Variables and Gradient Tape
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../bootcampsnotes/TensorFlowDLBootCamp/LowLevelTensorflow/TF2_0_Build_Your_Own_Model/" class="md-nav__link">
        TF2 0 Build Your Own Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../competitiveprogramming/">Competitive Programming</a>
          
        </div>
      
      <nav class="md-nav" aria-label="Competitive Programming" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Competitive Programming
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../">Core Computer Science</a>
          
            <label for="__nav_5">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Core Computer Science" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Core Computer Science
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_2">
          ML Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ML Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          ML Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ML%20Specialization%20-%20Course%201/" class="md-nav__link">
        ML Specialization - Course 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ML%20Specialization%20-%20Course%202/" class="md-nav__link">
        ML Specialization - Course 2
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        ML Specialization - Course 3
      </a>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3" type="checkbox" id="__nav_5_3" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3">
          NLP Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          NLP Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%201/" class="md-nav__link">
        NLP Specialization - Course 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%202/" class="md-nav__link">
        NLP Specialization - Course 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%203/" class="md-nav__link">
        NLP Specialization - Course 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%204/" class="md-nav__link">
        NLP Specialization - Course 4
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_4" type="checkbox" id="__nav_5_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_4">
          IBM Data Science Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="IBM Data Science Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_4">
          <span class="md-nav__icon md-icon"></span>
          IBM Data Science Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Coming Soon" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_5" type="checkbox" id="__nav_5_5" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_5">
          NLP Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP Specialization" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_5">
          <span class="md-nav__icon md-icon"></span>
          NLP Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Coming Soon" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../myresearch/">My Research</a>
          
        </div>
      
      <nav class="md-nav" aria-label="My Research" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          My Research
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7">
          About
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="About" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          About
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      
      
        
          
            
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../blogs/">Blog</a>
          
            <label for="__nav_8">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Blog" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Blog
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_2" type="checkbox" id="__nav_8_2" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_2">
          2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="2022" data-md-level="2">
        <label class="md-nav__title" for="__nav_8_2">
          <span class="md-nav__icon md-icon"></span>
          2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../blogs/2022/How-to-use-these-resources/" class="md-nav__link">
        How-to-use-these-resources
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../blogs/2022/How-to-contribute/" class="md-nav__link">
        How to contribute
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_9" type="checkbox" id="__nav_9" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9">
          Contact
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Contact" data-md-level="1">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          Contact
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../contact/" class="md-nav__link">
        Contact
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/qalmaqihir/qalmaqihir.github.io/edit/master/docs/corecs/ML_Specialization/ML Specialization - Course 3.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


<h1 id="ml-specialization-course-3">ML Specialization - Course  3<a class="headerlink" href="#ml-specialization-course-3" title="Permanent link">&para;</a></h1>
<h1 id="unsupervised-learning-recommenders-reinforcement-learning">Unsupervised Learning, Recommenders, Reinforcement Learning<a class="headerlink" href="#unsupervised-learning-recommenders-reinforcement-learning" title="Permanent link">&para;</a></h1>
<p>Note 2024-01-27T14.30.54</p>
<p>========================</p>
<h2 id="week-1">Week 1<a class="headerlink" href="#week-1" title="Permanent link">&para;</a></h2>
<h3 id="welcome">Welcome!<a class="headerlink" href="#welcome" title="Permanent link">&para;</a></h3>
<p>Certainly! Here are the concise notes:</p>
<p><strong>Welcome to Unsupervised Learning, Recommender Systems, and Reinforcement Learning</strong></p>
<p>This course expands beyond supervised learning, offering powerful tools to enhance your machine learning expertise.</p>
<p><strong>Week 1: Clustering and Anomaly Detection</strong>
- <strong>Objective:</strong> Understand clustering algorithms and anomaly detection.
- Clustering groups data into clusters; anomaly detection identifies unusual instances.
- Widely used in commercial applications; gain hands-on experience.</p>
<p><strong>Week 2: Recommender Systems</strong>
- <strong>Objective:</strong> Explore the critical role of recommender systems in various applications.
- Learn how online platforms recommend products or movies.
- Gain practical skills to implement your own recommender system.</p>
<p><strong>Week 3: Reinforcement Learning</strong>
- <strong>Objective:</strong> Dive into reinforcement learning, a powerful but emerging technology.
- Reinforcement learning excels in tasks such as playing video games and controlling robots.
- Implement reinforcement learning by landing a simulated moon lander.</p>
<p>This course promises to equip you with advanced skills, pushing the boundaries of what machine learning can achieve in unsupervised learning, recommender systems, and reinforcement learning.</p>
<h3 id="what-is-clustering">What is clustering?<a class="headerlink" href="#what-is-clustering" title="Permanent link">&para;</a></h3>
<p><strong>Clustering: Unsupervised Learning Algorithm</strong></p>
<ul>
<li><strong>Definition:</strong> Clustering is an unsupervised learning algorithm that identifies related or similar data points within a dataset.</li>
<li><strong>Contrast with Supervised Learning:</strong></li>
<li><strong>Supervised Learning:</strong> Requires both input features (x) and target labels (y).</li>
<li><strong>Unsupervised Learning (Clustering):</strong> Utilizes only input features (x), lacking target labels (y).</li>
<li><strong>Objective of Unsupervised Learning:</strong></li>
<li>Find interesting structures within the data without predefined target labels.</li>
<li><strong>Clustering Algorithm:</strong></li>
<li>Looks for specific structures, particularly grouping data into clusters where points within a cluster are similar.</li>
<li>Example: Dataset may be grouped into clusters, indicating points with similarities.</li>
<li><strong>Applications of Clustering:</strong></li>
<li>News article grouping, market segmentation, and learner categorization at deeplearning.ai.</li>
<li>Analysis of DNA data to identify individuals with similar genetic traits.</li>
<li>Astronomy: Clustering aids in grouping celestial bodies for analysis in space exploration.</li>
<li><strong>Versatility of Clustering:</strong></li>
<li>Applied across diverse domains, including genetics, astronomy, and beyond.</li>
</ul>
<p>In the upcoming video, we'll explore the widely used k-means algorithm, a fundamental clustering algorithm, to understand its workings.</p>
<h3 id="k-means-intuition">K-means intuition<a class="headerlink" href="#k-means-intuition" title="Permanent link">&para;</a></h3>
<p><strong>K-means Intuition:</strong></p>
<ul>
<li><strong>Definition:</strong> K-means is a clustering algorithm used to find groups (clusters) within a dataset.</li>
<li><strong>Example:</strong> Consider a dataset with 30 unlabeled training examples; the goal is to run K-means on this data.</li>
</ul>
<p><strong>Algorithm Overview:</strong>
1. <strong>Initialization:</strong>
   - Randomly guess the initial locations of cluster centroids.
   - Example: Red and blue crosses as initial guesses.</p>
<ol>
<li><strong>Iterative Process:</strong></li>
<li>
<p><strong>Step 1 - Assign Points to Clusters:</strong></p>
<ul>
<li>For each point, determine if it is closer to the red or blue cluster centroid.</li>
<li>Assign points to the closest centroid.</li>
</ul>
</li>
<li>
<p><strong>Step 2 - Move Cluster Centroids:</strong></p>
<ul>
<li>Calculate the average location of points in each cluster.</li>
<li>Move the cluster centroids to the computed averages.</li>
</ul>
</li>
<li>
<p><strong>Iterate:</strong></p>
</li>
<li>Repeat steps 1 and 2.</li>
<li>Reassign points and move centroids until convergence.</li>
</ol>
<p><strong>Visualization:</strong>
- Points are colored based on proximity to the cluster centroids.
- Iterative process refines the centroid locations.</p>
<p><strong>Convergence:</strong>
- Algorithm converges when no further changes occur in point assignments or centroid locations.</p>
<p><strong>Outcome:</strong>
- In the example, K-means identifies two clusters: upper and lower points.</p>
<p><strong>Next Steps:</strong>
- The upcoming video will delve into the mathematical formulas behind these iterative steps.
- Formalizing the algorithm to understand the underlying computations.</p>
<p>K-means efficiently discovers patterns in data by iteratively refining cluster assignments and centroid locations until convergence.</p>
<h3 id="k-means-algorithm">K-means algorithm<a class="headerlink" href="#k-means-algorithm" title="Permanent link">&para;</a></h3>
<p><strong>K-means Algorithm:</strong></p>
<ul>
<li><strong>Definition:</strong> K-means is an unsupervised machine learning algorithm used for clustering data into groups or clusters.</li>
</ul>
<p><strong>Initialization:</strong>
1. <strong>Random Initialization:</strong>
   - Set K cluster centroids: Mu1, Mu2, ..., Muk.
   - Each Mu is a vector with dimensions equal to the features of training examples.</p>
<p><strong>Iterative Steps (Repeat Until Convergence):</strong></p>
<p><strong>Step 1 - Assign Points to Clusters:</strong>
- <strong>Objective:</strong> Associate each data point with the nearest cluster centroid.
- <strong>Mathematical Expression:</strong>
  - Assign each point xi to the cluster with the closest centroid Mu k.
  - <span class="arithmatex">\(\(c^{(i)} = \text{arg min}_k \|x^{(i)} - \mu_k\|^2\)\)</span>.  </p>
<p><strong>Step 2 - Move Cluster Centroids:</strong>
- <strong>Objective:</strong> Update cluster centroids to be the mean of points assigned to each cluster.
- <strong>Mathematical Expression:</strong>
  - Update each cluster centroid Mu k as the mean of points assigned to it.
  - <span class="arithmatex">\(\mu_k = \frac{1}{|C_k|} \sum_{i \in C_k} x^{(i)}\)</span>.</p>
<p><strong>Convergence:</strong>
- <strong>Objective:</strong> Repeat steps 1 and 2 until no further changes in point assignments or centroid locations.</p>
<p><strong>Handling Zero-Assigned Clusters:</strong>
- <strong>Action:</strong> If a cluster has zero training examples, eliminate it or randomly reinitialize its centroid.</p>
<p><strong>Application to Continuous Data:</strong>
- K-means is applicable to datasets with continuous variations, where clusters may not be well-separated.</p>
<p><strong>Cost Function Optimization:</strong>
- <strong>Objective:</strong> K-means aims to optimize a cost function.
- <strong>Next Steps:</strong> The following video will delve into the cost function and the convergence properties of K-means.</p>
<h3 id="optimization-objective">Optimization objective<a class="headerlink" href="#optimization-objective" title="Permanent link">&para;</a></h3>
<p><strong>Optimization Objective in K-means:</strong></p>
<p>In supervised learning, algorithms optimize a cost function to improve their predictive performance. Similarly, the K-means algorithm, introduced after supervised learning in this specialization, also follows an optimization process.</p>
<p><strong>K-means Cost Function:</strong></p>
<ul>
<li><strong>Notation:</strong> CI represents the cluster index assigned to training example XI, and MuCI denotes the location of the cluster centroid.</li>
<li><strong>Cost Function (Distortion):</strong></li>
<li><span class="arithmatex">\( J = \frac{1}{m} \sum_{i=1}^{m} \|x^{(i)} - \mu_{c^{(i)}}\|^2 \)</span></li>
<li>This cost function, often known as the distortion function, measures the average squared distance between each training example and the cluster centroid to which it's assigned.</li>
</ul>
<p><strong>Optimization Algorithm:</strong></p>
<ol>
<li><strong>Step 1 - Assign Points to Clusters:</strong></li>
<li>Choose CI to minimize J while keeping Mu1 to MuK fixed.</li>
<li>
<p>Each training example is assigned to the nearest cluster centroid, optimizing the assignments.</p>
</li>
<li>
<p><strong>Step 2 - Move Cluster Centroids:</strong></p>
</li>
<li>Choose Mu1 to MuK to minimize J while keeping C1 to CM fixed.</li>
<li>Cluster centroids are updated as the mean of the points assigned to them, optimizing their locations.</li>
</ol>
<p><strong>Convergence:</strong>
K-means iteratively updates assignments and centroids to minimize the distortion. Convergence is reached when further updates no longer significantly reduce the distortion.</p>
<p><strong>Handling Convergence:</strong>
A lack of change in distortion or a single iteration with no reduction indicates convergence. If the reduction rate becomes very slow, it might be considered converged in practice.</p>
<p><strong>Multiple Initialization:</strong>
- K-means benefits from multiple random initializations of cluster centroids.
- The algorithm is run for each initialization.
- The clustering with the lowest final distortion is chosen.</p>
<p><strong>Benefits of Cost Function:</strong>
The cost function serves multiple purposes:
- Validates convergence and allows for early stopping.
- Assists in detecting bugs in the algorithm.
- Facilitates the use of multiple initializations to find more optimal solutions.</p>
<p>Understanding the role of the cost function in K-means helps ensure the algorithm's correct implementation, aids in interpreting convergence behavior, and provides a mechanism for improving cluster assignments through multiple initializations.</p>
<h3 id="initializing-k-means">Initializing K-means:<a class="headerlink" href="#initializing-k-means" title="Permanent link">&para;</a></h3>
<p><strong>Initializing K-means:</strong></p>
<p><strong>Choosing Initial Centroids:</strong>
- Number of clusters, K, is typically set less than or equal to the number of training examples, m.
- Randomly select K training examples as initial guesses for centroids (Mu1 to MuK).
- Each centroid initializes a cluster.</p>
<p><strong>Random Initialization Variability:</strong>
- Different random initializations may lead to diverse clusters.
- The method ensures various starting points for the algorithm.</p>
<p><strong>Local Optima Challenges:</strong>
- Random initialization may result in local optima.
- Local optima are suboptimal solutions where K-means gets stuck during optimization.
- Different initializations can lead to different local optima.</p>
<p><strong>Multiple Random Initializations:</strong>
- Run K-means algorithm multiple times with different initializations.
- Aim: Increase the likelihood of finding a better local optimum.
- Each run results in a set of clusters and centroids.</p>
<p><strong>Cost Function for Selection:</strong>
- Compute the cost function J for each run.
- J measures the distortion, the average squared distance between training examples and assigned centroids.
- Choose the clustering with the lowest J as the final result.</p>
<p><strong>Algorithm for Multiple Initializations:</strong>
1. Choose the number of random initializations (e.g., 100 times).
2. For each initialization:
   - Randomly select K training examples.
   - Initialize centroids (Mu1 to MuK).
   - Run K-means to convergence.
   - Compute the distortion (J).
3. Select the set of clusters with the lowest J as the final result.</p>
<p><strong>Benefits of Multiple Initializations:</strong>
- Increases the likelihood of finding a global optimum.
- Mitigates the impact of poor initializations.
- Enhances the robustness and performance of K-means.</p>
<p><strong>Choosing the Number of Initializations:</strong>
- Common range: 50 to 1000 initializations.
- Beyond 1000 may yield diminishing returns.
- A balance between computational efficiency and effectiveness.</p>
<p><strong>Personal Practice:</strong>
- Commonly use more than one random initialization for better results.
- Improves the overall performance of K-means in finding optimal clusters.</p>
<p><strong>Conclusion:</strong>
- Multiple initializations contribute to K-means' ability to discover better local optima.
- It provides a more robust and reliable clustering solution.
- The final choice is based on the clustering configuration with the lowest distortion.</p>
<h3 id="choosing-the-number-of-clusters">Choosing the number of clusters<a class="headerlink" href="#choosing-the-number-of-clusters" title="Permanent link">&para;</a></h3>
<p><strong>Choosing the Number of Clusters:</strong></p>
<p><strong>Challenge in Determining K:</strong>
- <em>Selecting K Ambiguity:</em> Determining the number of clusters (K) in K-means is often challenging due to its subjective nature.
- <em>Absence of Clear Indicators:</em> The absence of specific labels in clustering makes it difficult to identify the correct number of clusters.</p>
<p><strong>Unsupervised Nature of Clustering:</strong>
- <em>Label-Free Clustering:</em> Clustering operates in an unsupervised manner, lacking predefined labels for guidance.
- <em>Indistinct Application Signals:</em> Some applications may not provide clear signals about the optimal number of clusters.</p>
<p><strong>Elbow Method:</strong>
- <em>Idea Behind Elbow Method:</em> The elbow method involves running K-means for various K values and plotting the cost function (J).
- <em>Identifying Elbow:</em> Look for an "elbow" point in the plot, indicating a point where increasing K offers diminishing returns.
- <em>Limitations of Elbow Method:</em> While useful, the method may not work well for all datasets, especially those with smooth cost function decreases.</p>
<p><strong>Limitations of the Elbow Method:</strong>
- <em>Smooth Decrease in Cost Function:</em> Many cost functions may exhibit a smooth decrease without a distinct elbow.
- <em>Not Universally Applicable:</em> The elbow method might not be universally applicable to all clustering scenarios.</p>
<p><strong>Avoiding Cost Function Minimization:</strong>
- <em>Unreliability of Minimizing J:</em> Choosing K to minimize the cost function (J) is unreliable as it may lead to selecting an unnecessarily large K.
- <em>Potential Pitfall:</em> Aiming to minimize J might encourage selecting the largest possible K.</p>
<p><strong>Practical Approach:</strong>
- <em>Downstream Objective Evaluation:</em> Evaluate K-means based on its performance in achieving downstream objectives.
- <em>Adapt to Application Needs:</em> Run K-means with different K values and assess performance based on the application's requirements.</p>
<p><strong>Example: T-shirt Sizing:</strong>
- <em>Trade-off Consideration:</em> Illustration using t-shirt sizes emphasizes the trade-off between better fit and additional costs.
- <em>Practical Decision-Making:</em> Deciding the value of K is context-dependent, balancing fit improvement against associated expenses.</p>
<p><strong>Adaptation to Specific Applications:</strong>
- <em>Tool for Objectives:</em> View K-means as a tool to achieve specific objectives rather than a one-size-fits-all solution.
- <em>Application-Centric Approach:</em> Tailor the choice of K to the unique needs and goals of the application.</p>
<p><strong>Programming Exercise Insight:</strong>
- <em>Image Compression Exercise:</em> Similar trade-off concept in image compression exercise between image quality and file size.
- <em>Manual Decision-Making:</em> Decide the best K manually based on the desired balance between compressed image quality and size.</p>
<p><strong>Conclusion:</strong>
- <em>Subjective Nature:</em> Selection of K remains subjective.
- <em>Elbow Method Consideration:</em> The elbow method is one approach but may not suit all scenarios.
- <em>Practical Considerations:</em> Choose K based on practical considerations, trade-offs, and application-specific requirements.
- <em>Adaptive Application:</em> Adapt K-means to the unique demands of each application for more meaningful outcomes.</p>
<h3 id="anomaly-detection">Anomaly Detection<a class="headerlink" href="#anomaly-detection" title="Permanent link">&para;</a></h3>
<h3 id="finding-unusual-events">Finding unusual events<a class="headerlink" href="#finding-unusual-events" title="Permanent link">&para;</a></h3>
<p><strong>Anomaly Detection: Detecting Unusual Events</strong></p>
<p><strong>Introduction to Anomaly Detection:</strong>
- Unsupervised learning algorithm.
- Focus: Identifying anomalous or unusual events in an unlabeled dataset.
- Example: Aircraft engine manufacturing  detecting potential problems.</p>
<p><strong>Features for Aircraft Engine Example:</strong>
- Features (x1, x2) representing properties like heat and vibration.
- Data from normal engines is collected.
- Anomaly detection aims to identify if a new engine's behavior is abnormal.</p>
<p><strong>Anomaly Detection Algorithm:</strong>
1. <strong>Density Estimation:</strong>
   - Build a model for the probability distribution of x.
   - Learn which values of features are likely or unlikely.</p>
<ol>
<li><strong>New Test Example (Xtest):</strong></li>
<li>Given a new test example, compute the probability of Xtest using the model.</li>
<li>
<p>Compare the probability to a threshold (epsilon).</p>
</li>
<li>
<p><strong>Decision:</strong></p>
</li>
<li>If p(Xtest) &lt; epsilon, raise a flag for anomaly.</li>
<li>If p(Xtest) &gt;= epsilon, consider it normal.</li>
</ol>
<p><strong>Density Estimation with Gaussian Distribution:</strong>
- Model the probability distribution using a Gaussian distribution (normal distribution).
- Estimate parameters (mean and variance) from the training set.
- Compute probability density function (pdf) for a given example.</p>
<p><strong>Application Areas of Anomaly Detection:</strong>
1. <strong>Aircraft Engine Manufacturing:</strong>
   - Identify anomalous behavior in newly manufactured engines.
   - Flag potential issues for further inspection.</p>
<ol>
<li><strong>Fraud Detection:</strong></li>
<li>Monitor user activities (login frequency, transaction patterns, etc.).</li>
<li>
<p>Flag anomalous behavior for closer inspection.</p>
</li>
<li>
<p><strong>Manufacturing:</strong></p>
</li>
<li>Detect anomalies in various production processes.</li>
<li>
<p>Identify potential defects in manufactured units.</p>
</li>
<li>
<p><strong>Computer Clusters and Data Centers:</strong></p>
</li>
<li>Monitor machine features (memory usage, disk accesses, CPU load, etc.).</li>
<li>Flag unusual behavior in specific computers for investigation.</li>
</ol>
<p><strong>Practical Usage:</strong>
- Commonly used for fraud detection in websites, financial transactions, and more.
- Applied in manufacturing to ensure the quality of produced goods.
- Used in monitoring systems to identify anomalies in diverse applications.</p>
<p><strong>Next Steps:</strong>
- Understanding Gaussian distributions for density estimation in anomaly detection.
- Learning how to apply and implement anomaly detection algorithms.
- Exploring real-world examples and use cases for enhanced understanding.</p>
<h3 id="gaussian-normal-distribution">Gaussian (normal) distribution<a class="headerlink" href="#gaussian-normal-distribution" title="Permanent link">&para;</a></h3>
<p><strong>Gaussian (Normal) Distribution: Overview</strong></p>
<p><strong>Introduction:</strong>
- Gaussian distribution, also known as the normal distribution.
- Represents a probability distribution of a random variable.
- Often referred to as a bell-shaped curve.</p>
<p><strong>Parameters of Gaussian Distribution:</strong>
1. <strong>Mean (Mu):</strong>
   - Represents the center or average of the distribution.
   - Denoted by Mu.</p>
<ol>
<li><strong>Standard Deviation (Sigma):</strong></li>
<li>Indicates the spread or width of the distribution.</li>
<li>Denoted by Sigma.</li>
</ol>
<p><strong>Probability Density Function (pdf):</strong>
- The probability of a random variable x following a Gaussian distribution.
- Formula: <span class="arithmatex">\( p(x) = \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}} \)</span></p>
<p><strong>Interpretation:</strong>
- The higher the probability density, the more likely the value of x.
- The bell-shaped curve illustrates the likelihood of different values.</p>
<p><strong>Examples of Gaussian Distributions:</strong>
1. <strong>Mu = 0, Sigma = 1:</strong>
   - Standard normal distribution, centered at 0 with unit standard deviation.</p>
<ol>
<li><strong>Mu = 0, Sigma = 0.5:</strong></li>
<li>
<p>A narrower distribution, still centered at 0 with a smaller standard deviation.</p>
</li>
<li>
<p><strong>Mu = 0, Sigma = 2:</strong></p>
</li>
<li>
<p>A wider distribution, centered at 0 with a larger standard deviation.</p>
</li>
<li>
<p><strong>Changing Mu with Sigma = 0.5:</strong></p>
</li>
<li>Shifts the center of the distribution while maintaining the same width.</li>
</ol>
<p><strong>Applying Gaussian Distribution to Anomaly Detection:</strong>
- Given a dataset, estimate Mu and Sigma to fit a Gaussian distribution.
- Parameters calculated as follows:
   - <span class="arithmatex">\( \mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)} \)</span> (mean)
   - <span class="arithmatex">\( \sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x^{(i)} - \mu)^2 \)</span> (variance)</p>
<p><strong>Use in Anomaly Detection:</strong>
- If <span class="arithmatex">\( p(x) &lt; \epsilon \)</span>, flag the example as an anomaly.
- <span class="arithmatex">\( \epsilon \)</span> is a threshold set based on the desired sensitivity.</p>
<p><strong>Handling Multiple Features:</strong>
- For multiple features, extend Gaussian distribution to handle n features.
- Calculate Mu and Sigma for each feature independently.</p>
<p><strong>Summary:</strong>
- Gaussian distribution provides a probabilistic model for anomaly detection.
- Parameters Mu and Sigma are estimated from the training set.
- Anomalies are identified based on low probability density.</p>
<p>In the next video, we will delve into applying Gaussian distribution to an anomaly detection algorithm, especially when dealing with multiple features.</p>
<h3 id="anomaly-detection-algorithm">Anomaly detection algorithm<a class="headerlink" href="#anomaly-detection-algorithm" title="Permanent link">&para;</a></h3>
<p><strong>Anomaly Detection Algorithm: Overview</strong></p>
<p><strong>Objective:</strong>
- Build an anomaly detection algorithm using the Gaussian distribution.</p>
<p><strong>Dataset:</strong>
- Training set <span class="arithmatex">\(\(x^{(1)}, x^{(2)}, \ldots, x^{(m)}\)\)</span>.
- Each example <span class="arithmatex">\(x\)</span> has <span class="arithmatex">\(n\)</span> features.</p>
<p><strong>Density Estimation:</strong>
- Model <span class="arithmatex">\(p(x)\)</span> as the product of individual feature probabilities.
- <span class="arithmatex">\(p(x) = \prod_{j=1}^{n} p(x_j; \mu_j, \sigma_j^2)\)</span></p>
<p><strong>Parameters for Each Feature:</strong>
- For each feature <span class="arithmatex">\(x_j\)</span>, estimate parameters <span class="arithmatex">\(\mu_j\)</span> (mean) and <span class="arithmatex">\(\sigma_j^2\)</span> (variance).
- Parameters calculated from the training set:
  - <span class="arithmatex">\(\mu_j = \frac{1}{m} \sum_{i=1}^{m} x_j^{(i)}\)</span> (mean)
  - <span class="arithmatex">\(\sigma_j^2 = \frac{1}{m} \sum_{i=1}^{m} (x_j^{(i)} - \mu_j)^2\)</span> (variance)</p>
<p><strong>Anomaly Detection Process:</strong>
1. <strong>Choose Features:</strong>
   - Select features <span class="arithmatex">\(x_j\)</span> that may indicate anomalous examples.</p>
<ol>
<li><strong>Estimate Parameters:</strong></li>
<li>
<p>Compute <span class="arithmatex">\(\mu_j\)</span> and <span class="arithmatex">\(\sigma_j^2\)</span> for each feature using the training set.</p>
</li>
<li>
<p><strong>Compute <span class="arithmatex">\(p(x)\)</span>:</strong></p>
</li>
<li>
<p>For a new example <span class="arithmatex">\(x\)</span>, calculate <span class="arithmatex">\(p(x)\)</span> using the Gaussian distribution.</p>
</li>
<li>
<p><strong>Flag Anomalies:</strong></p>
</li>
<li>If <span class="arithmatex">\(p(x) &lt; \epsilon\)</span>, where <span class="arithmatex">\(\epsilon\)</span> is a threshold, flag the example as an anomaly.</li>
</ol>
<p><strong>Handling Multiple Features:</strong>
- Parameters <span class="arithmatex">\(\mu_j\)</span> and <span class="arithmatex">\(\sigma_j^2\)</span> are calculated independently for each feature.</p>
<p><strong>Example:</strong>
- If <span class="arithmatex">\(x\)</span> has two features, <span class="arithmatex">\(x_1\)</span> and <span class="arithmatex">\(x_2\)</span>:
  - <span class="arithmatex">\(p(x) = p(x_1; \mu_1, \sigma_1^2) \cdot p(x_2; \mu_2, \sigma_2^2)\)</span></p>
<p><strong>Choosing <span class="arithmatex">\(\epsilon\)</span>:</strong>
- Set <span class="arithmatex">\(\epsilon\)</span> based on desired sensitivity.
- A small <span class="arithmatex">\(\epsilon\)</span> may flag more anomalies but may lead to false positives.</p>
<p><strong>Performance Evaluation:</strong>
- Requires labeled data (anomalies and non-anomalies).
- Use metrics like precision, recall, and F1 score.</p>
<p><strong>Conclusion:</strong>
- Anomaly detection quantifies the likelihood of features being unusually large or small.
- Systematically identifies anomalies based on calculated probabilities.</p>
<p>In the next video, we will explore how to choose the parameter <span class="arithmatex">\(\epsilon\)</span> and evaluate the performance of an anomaly detection system.</p>
<h3 id="developing-and-evaluating-an-anomaly-detection-system">Developing and evaluating an anomaly detection system<a class="headerlink" href="#developing-and-evaluating-an-anomaly-detection-system" title="Permanent link">&para;</a></h3>
<p><strong>Developing and Evaluating an Anomaly Detection System: Overview</strong></p>
<p><strong>Key Ideas:</strong>
- <strong>Continuous Evaluation:</strong>
  - Continuously evaluate the anomaly detection system during development.
  - Make decisions, change features or parameters, and improve the system based on evaluations.</p>
<ul>
<li><strong>Labeled Data for Evaluation:</strong></li>
<li>Assume some labeled data with anomalies (y=1) and normal examples (y=0).</li>
<li>Use this labeled data for cross-validation and testing.</li>
</ul>
<p><strong>Labeled Data Assumption:</strong>
- Associate labels <span class="arithmatex">\(y = 1\)</span> for known anomalies and <span class="arithmatex">\(y = 0\)</span> for normal examples.
- Train on the unlabeled set but use labeled data for evaluation.</p>
<p><strong>Cross-Validation and Test Sets:</strong>
- Create cross-validation and test sets with labeled anomalies and normal examples.
- Allows evaluation and tuning on labeled data.</p>
<p><strong>Example: Aircraft Engines:</strong>
- Assume data from 10,000 normal engines and 20 flawed engines.
- Split data into training, cross-validation, and test sets.</p>
<p><strong>Alternative 1: Separate Test Set:</strong>
- Train on 6,000 normal engines.
- Cross-validation: 2,000 normal, 10 anomalies.
- Test set: 2,000 normal, 10 anomalies.</p>
<p><strong>Alternative 2: Combined Cross-Validation and Test Set:</strong>
- Train on 6,000 normal engines.
- Cross-validation and test set: 4,000 normal, 20 anomalies.</p>
<p><strong>Choosing the Approach:</strong>
- Separate test set ideal for sufficient data.
- Combined set used when data is limited.
- Risk of overfitting decisions to the cross-validation set.</p>
<p><strong>Evaluation Metrics:</strong>
- Compute precision, recall, F1 score in highly skewed data distributions.
- Assess how well the algorithm finds anomalies and avoids false positives.</p>
<p><strong>Handling Skewed Data Distribution:</strong>
- Use alternative metrics when anomalies are much fewer than normal examples.
- True positive, false positive, false negative, true negative rates.
- Precision, recall, F1 score.</p>
<p><strong>Evaluation Process:</strong>
1. Fit the model <span class="arithmatex">\(p(x)\)</span> on the training set.
2. Compute <span class="arithmatex">\(p(x)\)</span> on cross-validation/test examples.
3. Predict <span class="arithmatex">\(y = 1\)</span> if <span class="arithmatex">\(p(x) &lt; \epsilon\)</span>, else <span class="arithmatex">\(y = 0\)</span>.
4. Compare predictions to actual labels <span class="arithmatex">\(y\)</span>.
5. Evaluate algorithm performance based on labeled data.</p>
<p><strong>Comparison with Supervised Learning:</strong>
- Use labeled data for evaluation, but still an unsupervised learning algorithm.
- Address the question of when to use anomaly detection versus supervised learning.</p>
<p>In the next video, we will explore a comparison between anomaly detection and supervised learning and discuss scenarios where one approach might be preferred over the other.</p>
<h3 id="anomaly-detection-vs-supervised-learning">Anomaly detection vs. supervised learning<a class="headerlink" href="#anomaly-detection-vs-supervised-learning" title="Permanent link">&para;</a></h3>
<p><strong>Anomaly Detection vs. Supervised Learning: Choosing Between Them</strong></p>
<p><strong>Decision Criteria:</strong>
- <strong>Small Number of Positive Examples (y=1):</strong>
  - Anomaly Detection: 0-20 positive examples.
- <strong>Large Number of Positive Examples (y=1):</strong>
  - Supervised Learning: Larger number of positive examples.</p>
<p><strong>Main Difference:</strong>
- Anomaly Detection:
  - Appropriate when there are many different types of anomalies.
  - Assumes anomalies may be diverse and cover new forms.
- Supervised Learning:
  - Appropriate when positive examples are assumed to be similar.
  - Assumes future positive examples are likely to resemble training set examples.</p>
<p><strong>Illustration with Examples:</strong>
1. <strong>Financial Fraud Detection:</strong>
   - Anomaly Detection: Diverse types, new forms may emerge.
   - Supervised Learning: Spam detection works well for recurring types.</p>
<ol>
<li><strong>Manufacturing Defect Detection:</strong></li>
<li>Anomaly Detection: Finds new, previously unseen defects.</li>
<li>
<p>Supervised Learning: Suitable for known and recurring defects.</p>
</li>
<li>
<p><strong>Security Monitoring:</strong></p>
</li>
<li>Anomaly Detection: Detects brand new ways of system compromise.</li>
<li>
<p>Supervised Learning: May not work well for new hacking techniques.</p>
</li>
<li>
<p><strong>Weather Prediction:</strong></p>
</li>
<li>Supervised Learning: Predicting common weather patterns.</li>
<li>
<p>Anomaly Detection: Not suitable for recurring weather types.</p>
</li>
<li>
<p><strong>Medical Diagnosis:</strong></p>
</li>
<li>Supervised Learning: Identifying known diseases from symptoms.</li>
<li>Anomaly Detection: May not work well for diverse and rare diseases.</li>
</ol>
<h3 id="choosing-between-anomaly-detection-and-supervised-learning">Choosing Between Anomaly Detection and Supervised Learning<a class="headerlink" href="#choosing-between-anomaly-detection-and-supervised-learning" title="Permanent link">&para;</a></h3>
<p><strong>Choosing Between Anomaly Detection and Supervised Learning:</strong>
- <strong>Anomaly Detection:</strong>
  - Use when there are diverse positive examples or new forms may emerge.
  - Suitable for security-related applications with evolving threats.</p>
<ul>
<li><strong>Supervised Learning:</strong></li>
<li>Use when positive examples are likely to resemble training set examples.</li>
<li>Suitable for tasks like spam detection, manufacturing defects, weather prediction.</li>
</ul>
<p><strong>Framework for Decision:</strong>
- <strong>Nature of Positive Examples:</strong>
  - Diverse, evolving anomalies: Anomaly Detection.
  - Recurring, similar positives: Supervised Learning.</p>
<p><strong>Conclusion:</strong>
- <strong>Small Set of Positive Examples:</strong>
  - Consider the nature of positive examples and choose accordingly.
  - Anomaly detection can handle diverse and evolving anomalies.
  - Supervised learning assumes recurring patterns in positive examples.</p>
<p><strong>Next Video: Tuning Features for Anomaly Detection.</strong></p>
<h3 id="choosing-what-features-to-use">Choosing what features to use<a class="headerlink" href="#choosing-what-features-to-use" title="Permanent link">&para;</a></h3>
<p><strong>Choosing Features for Anomaly Detection</strong></p>
<p><strong>Importance of Feature Selection:</strong>
- Crucial for anomaly detection.
- More critical than in supervised learning.
- Anomaly detection relies on unlabeled data and needs clear feature signals.</p>
<p><strong>Gaussian Features:</strong>
- Aim for Gaussian-like feature distributions.
- Gaussian features are often easier for anomaly detection.
- Histograms are useful for visualizing feature distributions.</p>
<p><strong>Transforming Features:</strong>
- Adjust feature distributions to be more Gaussian if needed.
- Example transformations:
  - Logarithmic transformation (e.g., log(X)).
  - Square root transformation (e.g., sqrt(X)).
  - Exponential transformation (e.g., X^0.5).
  - Custom transformations based on domain knowledge.</p>
<p><strong>Example Feature Transformation in Python:</strong>
- Plotting histograms in a Jupyter notebook.
- Trying different transformations in real-time.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="c1"># Example Python code for feature transformation</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="c1"># Original feature X</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="c1"># Plot histogram of original feature</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original Feature&#39;</span><span class="p">)</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="c1"># Try different transformations</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># Log transformation as an example</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Transformed Feature&#39;</span><span class="p">)</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><strong>Feature Transformation in Practice:</strong>
- Explore various transformations to find the one that makes the data more Gaussian.
- Apply the chosen transformation consistently across training, validation, and test sets.</p>
<p><strong>Error Analysis for Anomaly Detection:</strong>
- After training, examine cases where the algorithm fails to detect anomalies.
- Identify features that may distinguish missed anomalies.
- Create new features to capture unique aspects of missed anomalies.</p>
<p><strong>Illustrative Example:</strong>
- Detecting fraudulent behavior.
- Original feature (X1) is the number of transactions.
- Discovering a new feature (X2) related to typing speed.
- The combination of features X1 and X2 helps distinguish anomalies.</p>
<p><strong>Monitoring Computers in a Data Center:</strong>
- Choosing features related to computer behavior.
- Combining features (e.g., CPU load and network traffic) to capture anomalies.
- Experimenting with ratios or other combinations to enhance anomaly detection.</p>
<p><strong>Conclusion:</strong>
- Feature selection is critical for anomaly detection.
- Aim for Gaussian-like feature distributions.
- Experiment with transformations and combinations to enhance anomaly detection.</p>
<p><strong>Next Week: Recommender Systems</strong>
- Explore how recommender systems work.
- Understand the algorithms behind product or content recommendations.
- Practical insights into building recommender systems.</p>
<p><strong>Thank you for completing this week! Enjoy the labs, and see you next week!</strong></p>
<h2 id="week-2">Week 2<a class="headerlink" href="#week-2" title="Permanent link">&para;</a></h2>
<h3 id="making-recommendations">Making recommendations<a class="headerlink" href="#making-recommendations" title="Permanent link">&para;</a></h3>
<p><strong>Introduction to Recommender Systems</strong></p>
<p>Recommender systems, also known as recommendation systems, play a significant role in various online platforms, driving user engagement and sales. These systems provide personalized suggestions to users based on their preferences, behaviors, and interactions with items (movies, products, articles, etc.). In this context, recommender systems are widely used by companies like Amazon, Netflix, and food delivery apps.</p>
<p><strong>Example: Predicting Movie Ratings</strong>
Let's consider a running example of predicting movie ratings. Users rate movies on a scale of one to five stars, and the goal is to recommend movies to users based on their preferences.</p>
<p><strong>Key Elements:</strong>
1. <strong>Users:</strong> Represented by <span class="arithmatex">\( \text{nu} \)</span>, denoting the number of users.
   - Example users: Alice, Bob, Carol, Dave.</p>
<ol>
<li><strong>Items (Movies):</strong> Represented by <span class="arithmatex">\( \text{nm} \)</span>, denoting the number of items (movies).</li>
<li>
<p>Example movies: Love at last, Romance forever, Cute puppies of love, Nonstop car chases, Sword versus karate.</p>
</li>
<li>
<p><strong>User Ratings:</strong> Denoted by <span class="arithmatex">\( r(i, j) \)</span>, where <span class="arithmatex">\( r(i, j) = 1 \)</span> if user <span class="arithmatex">\( j \)</span> has rated movie <span class="arithmatex">\( i \)</span>, and <span class="arithmatex">\( r(i, j) = 0 \)</span> otherwise.</p>
</li>
<li>
<p>Example: <span class="arithmatex">\( r(1, 1) = 1 \)</span> indicates that Alice has rated Love at last.</p>
</li>
<li>
<p><strong>User Ratings (Numerical):</strong> Denoted by <span class="arithmatex">\( y(i, j) \)</span>, representing the rating given by user <span class="arithmatex">\( j \)</span> to movie <span class="arithmatex">\( i \)</span>.</p>
</li>
<li>Example: <span class="arithmatex">\( y(3, 2) = 4 \)</span> indicates that user 2 (Bob) rated Cute puppies of love as 4 stars.</li>
</ol>
<p><strong>Objective of Recommender Systems:</strong>
The primary goal is to predict how users would rate items they haven't rated yet. This allows the system to recommend items that users are likely to enjoy. The assumption is that there is some underlying set of features or information about the items (movies) that influence user preferences.</p>
<p><strong>Algorithm Development Approach:</strong>
1. Assume access to features or additional information about the items.
2. Predict user ratings based on these features.</p>
<p><strong>Next Steps:</strong>
In the following video, the development of an algorithm for predicting user ratings will be discussed, starting with the assumption of having access to features. Later, the discussion will address scenarios where features are not available, exploring alternative approaches.</p>
<h3 id="using-per-item-features">Using per-item features<a class="headerlink" href="#using-per-item-features" title="Permanent link">&para;</a></h3>
<p><strong>Recommender Systems with Item Features</strong></p>
<p>In recommender systems, having features for each item (movie, product, etc.) can enhance the ability to make accurate predictions. In this context, features are additional characteristics or information about the items that influence user preferences. Let's explore how to develop a recommender system when item features are available.</p>
<p><strong>Example: Movie Ratings with Features</strong>
Consider the same movie dataset with users rating movies. Additionally, each movie now has two features, denoted as <span class="arithmatex">\(X_1\)</span> and <span class="arithmatex">\(X_2\)</span>, representing the degree to which the movie is categorized as romantic or action.</p>
<ul>
<li>Example features:</li>
<li>Love at Last: <span class="arithmatex">\(X_1 = 0.9\)</span>, <span class="arithmatex">\(X_2 = 0\)</span></li>
<li>Nonstop Car Chases: <span class="arithmatex">\(X_1 = 0.1\)</span>, <span class="arithmatex">\(X_2 = 1.0\)</span></li>
</ul>
<p><strong>Prediction for User Ratings:</strong>
For a given user <span class="arithmatex">\(j\)</span>, the prediction for the rating of movie <span class="arithmatex">\(i\)</span> can be made using linear regression:</p>
<div class="arithmatex">\[ \text{Prediction: } w^{(j)} \cdot X^{(i)} + b^{(j)} \]</div>
<ul>
<li><span class="arithmatex">\(w^{(j)}\)</span>: Parameter vector for user <span class="arithmatex">\(j\)</span></li>
<li><span class="arithmatex">\(X^{(i)}\)</span>: Feature vector for movie <span class="arithmatex">\(i\)</span></li>
<li><span class="arithmatex">\(b^{(j)}\)</span>: Bias term for user <span class="arithmatex">\(j\)</span></li>
</ul>
<p><strong>Cost Function for Learning:</strong>
The cost function for learning the parameters <span class="arithmatex">\(w^{(j)}\)</span> and <span class="arithmatex">\(b^{(j)}\)</span> for a specific user <span class="arithmatex">\(j\)</span> is based on mean squared error:</p>
<div class="arithmatex">\[ J(w^{(j)}, b^{(j)}) = \frac{1}{2m^{(j)}} \sum_{i:r(i,j)=1} \left( w^{(j)} \cdot X^{(i)} + b^{(j)} - y^{(i,j)} \right)^2 \]</div>
<ul>
<li><span class="arithmatex">\(m^{(j)}\)</span>: Number of movies rated by user <span class="arithmatex">\(j\)</span></li>
<li><span class="arithmatex">\(y^{(i,j)}\)</span>: Actual rating given by user <span class="arithmatex">\(j\)</span> to movie <span class="arithmatex">\(i\)</span></li>
<li>The sum is over movies rated by user <span class="arithmatex">\(j\)</span> (<span class="arithmatex">\(r(i,j) = 1\)</span>)</li>
</ul>
<p><strong>Regularization Term:</strong>
To prevent overfitting, a regularization term is added:</p>
<div class="arithmatex">\[ + \frac{\lambda}{2m^{(j)}} \sum_{k=1}^{n} (w_k^{(j)})^2 \]</div>
<ul>
<li><span class="arithmatex">\(\lambda\)</span>: Regularization parameter</li>
<li><span class="arithmatex">\(n\)</span>: Number of features (in this case, 2)</li>
</ul>
<p><strong>Learning Parameters for All Users:</strong>
To learn parameters for all users, the cost function is summed over all users:</p>
<div class="arithmatex">\[ J(\{w^{(j)}, b^{(j)}\}) = \sum_{j=1}^{\text{nu}} J(w^{(j)}, b^{(j)}) \]</div>
<p>Optimizing this cost function using an optimization algorithm like gradient descent provides parameters for predicting movie ratings for all users.</p>
<p><strong>Modification for All Users:</strong>
The division by <span class="arithmatex">\(m^{(j)}\)</span> in the cost function is sometimes eliminated (as <span class="arithmatex">\(m^{(j)}\)</span> is constant), making the optimization more convenient.</p>
<p>In the next video, the discussion will extend to scenarios where item features are not available in advance, exploring how to make recommendations without detailed item features.</p>
<h3 id="collaborative-filtering-algorithm">Collaborative filtering algorithm<a class="headerlink" href="#collaborative-filtering-algorithm" title="Permanent link">&para;</a></h3>
<p><strong>Collaborative Filtering Algorithm</strong></p>
<p>In the collaborative filtering algorithm, the goal is to make recommendations based on user ratings. This approach is particularly useful when you don't have detailed features for each item and want to learn them from the data. Here's a step-by-step explanation of how collaborative filtering works:</p>
<ol>
<li><strong>Model Setup with Features <span class="arithmatex">\(x_1, x_2, \ldots\)</span>:</strong></li>
<li>Users are represented by parameters <span class="arithmatex">\(w^{(j)}\)</span> and <span class="arithmatex">\(b^{(j)}\)</span>.</li>
<li>Movies are represented by feature vectors <span class="arithmatex">\(x^{(i)}\)</span> (e.g., <span class="arithmatex">\(x_1, x_2\)</span>).</li>
<li>
<p>For a specific user <span class="arithmatex">\(j\)</span> and movie <span class="arithmatex">\(i\)</span>, the predicted rating is given by <span class="arithmatex">\(w^{(j)} \cdot x^{(i)} + b^{(j)}\)</span>.</p>
</li>
<li>
<p><strong>Learning Features Without Prior Information:</strong></p>
</li>
<li>If the features <span class="arithmatex">\(x^{(i)}\)</span> are unknown, they can be learned from the data.</li>
<li>Assume that parameters <span class="arithmatex">\(w^{(j)}\)</span> are already learned for users.</li>
<li>
<p>For each movie <span class="arithmatex">\(i\)</span>, guess initial features and adjust them to minimize prediction errors.</p>
</li>
<li>
<p><strong>Cost Function for Learning Features <span class="arithmatex">\(x^{(i)}\)</span>:</strong></p>
</li>
<li>Define a cost function for learning features for a specific movie <span class="arithmatex">\(i\)</span>:
     [ J(x^{(i)}) = \frac{1}{2} \sum_{j:r(i,j)=1} \left( w^{(j)} \cdot x^{(i)} - y^{(i,j)} \right)^2 + \frac{\lambda}{2} \sum_{k=1}^{n} (x_k<sup>{(i)})</sup>2 ]</li>
<li>
<p>Minimize this cost function to learn features <span class="arithmatex">\(x^{(i)}\)</span>.</p>
</li>
<li>
<p><strong>Overall Cost Function for Collaborative Filtering:</strong></p>
</li>
<li>Combine the cost functions for learning <span class="arithmatex">\(w, b\)</span> and learning <span class="arithmatex">\(x\)</span> into an overall cost function:
     [ J(w, b, x) = \sum_{i=1}^{n_m} \sum_{j=1}^{\text{nu}} \left( (w^{(j)} \cdot x^{(i)} - y<sup>{(i,j)})</sup>2 + \frac{\lambda}{2} \sum_{k=1}^{n} (w_k<sup>{(j)})</sup>2 + \frac{\lambda}{2} \sum_{k=1}^{n} (x_k<sup>{(i)})</sup>2 \right) ]</li>
<li>
<p>Minimize this cost function with respect to <span class="arithmatex">\(w, b, x\)</span> using gradient descent.</p>
</li>
<li>
<p><strong>Gradient Descent Updates:</strong></p>
</li>
<li>Update <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(b\)</span> using gradient descent as usual.</li>
<li>
<p>Update each feature <span class="arithmatex">\(x_k^{(i)}\)</span> for all movies <span class="arithmatex">\(i\)</span> using gradient descent.</p>
</li>
<li>
<p><strong>Learning Collaborative Filtering:</strong></p>
</li>
<li>Iterate the updates until convergence to learn <span class="arithmatex">\(w, b, x\)</span>.</li>
<li>The collaborative filtering algorithm is now ready to make predictions and recommend items.</li>
</ol>
<p><strong>Binary Labels in Recommender Systems:</strong>
The problem formulation has focused on movie ratings, but in many cases, binary labels are used to indicate whether a user likes or interacts with an item. The collaborative filtering algorithm can be generalized to handle binary labels as well.</p>
<p>In the next video, the model will be extended to accommodate binary labels, providing a more versatile approach for different types of recommender systems.</p>
<h3 id="binary-labels-favs-likes-and-clicks">Binary labels: favs, likes and clicks<a class="headerlink" href="#binary-labels-favs-likes-and-clicks" title="Permanent link">&para;</a></h3>
<p><strong>Binary Labels: Favs, Likes, and Clicks</strong></p>
<p>In many applications of recommender systems, binary labels are used to indicate whether a user likes, engages with, or interacts with an item. These binary labels could represent actions such as making a purchase, liking a post, clicking on an item, or spending a certain amount of time engaging with it. Let's explore how to generalize the collaborative filtering algorithm to handle binary labels.</p>
<h3 id="generalizing-the-algorithm">Generalizing the Algorithm:<a class="headerlink" href="#generalizing-the-algorithm" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Data Representation:</strong></li>
<li>
<p>Binary labels: 1 (like, engage), 0 (dislike, not engage), ? (not yet exposed or unknown).</p>
</li>
<li>
<p><strong>Examples of Binary Labels:</strong></p>
</li>
<li>In an online shopping context, 1 could mean the user purchased the item, 0 could mean no purchase, and ? could mean the user was not exposed to the item.</li>
<li>In social media, 1 might represent the user favoriting or liking a post, 0 could mean no interaction, and ? could denote not being shown the post.</li>
<li>
<p>In online advertising, 1 might represent clicking on an ad, 0 could mean no click, and ? could denote not being shown the ad.</p>
</li>
<li>
<p><strong>Prediction Model for Binary Labels:</strong></p>
</li>
<li>Previously, <span class="arithmatex">\(y_{ij} = w_j \cdot x_i + b_j\)</span> (linear regression).</li>
<li>For binary labels, predict the probability of <span class="arithmatex">\(y_{ij} = 1\)</span> using the logistic function:
     [ g(z) = \frac{1}{1 + e^{-z}} ]</li>
<li>
<p>New prediction model: <span class="arithmatex">\(f(x) = g(w_j \cdot x_i + b_j)\)</span>.</p>
</li>
<li>
<p><strong>Cost Function for Binary Labels:</strong></p>
</li>
<li>Modify the cost function to be appropriate for logistic regression:
     [ J(w, b, x) = -\sum_{i=1}^{n_m} \sum_{j=1}^{\text{nu}} \left( y_{ij} \log(f(x)) + (1 - y_{ij}) \log(1 - f(x)) \right) + \frac{\lambda}{2} \sum_{k=1}^{n} (w_k<sup>{(j)})</sup>2 + \frac{\lambda}{2} \sum_{k=1}^{n} (x_k<sup>{(i)})</sup>2 ]</li>
</ol>
<h3 id="binary-labels-cost-function">Binary Labels Cost Function:<a class="headerlink" href="#binary-labels-cost-function" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[ J(w, b, x) = -\sum_{i=1}^{n_m} \sum_{j=1}^{\text{nu}} \left( y_{ij} \log(g(w_j \cdot x_i + b_j)) + (1 - y_{ij}) \log(1 - g(w_j \cdot x_i + b_j)) \right) + \frac{\lambda}{2} \sum_{k=1}^{n} (w_k^{(j)})^2 + \frac{\lambda}{2} \sum_{k=1}^{n} (x_k^{(i)})^2 \]</div>
<p>This cost function is adapted for binary labels and uses the logistic function to model the probability of engagement.</p>
<h3 id="practical-tips">Practical Tips:<a class="headerlink" href="#practical-tips" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Choosing Labels:</strong> Define what 1, 0, and ? mean based on the application context.</li>
<li><strong>User Engagement:</strong> Define engagement based on user behavior, such as clicks, likes, or time spent.</li>
<li><strong>Cost Function:</strong> Use the binary cross-entropy cost function for logistic regression.</li>
</ul>
<p>In the next video, practical implementation details and optimizations for the collaborative filtering algorithm with binary labels will be discussed.</p>
<h3 id="mean-normalization">Mean normalization<a class="headerlink" href="#mean-normalization" title="Permanent link">&para;</a></h3>
<p><strong>Mean Normalization for Recommender Systems</strong></p>
<p><strong>Objective:</strong>
Mean normalization aims to improve the efficiency and performance of recommender systems, particularly when dealing with users who have rated very few or no movies.</p>
<p><strong>Explanation:</strong>
1. <strong>Initial Scenario:</strong>
   - Consider a dataset representing movie ratings by users.
   - Adding a new user, Eve, who hasn't rated any movies yet.
   - Traditional collaborative filtering may predict all movies as zero stars for Eve due to lack of data.</p>
<ol>
<li><strong>Mean Normalization Concept:</strong></li>
<li>Compute the average rating for each movie based on existing user ratings.</li>
<li>Subtract the mean rating of each movie from individual ratings.</li>
<li>
<p>New ratings reflect deviations from the average, providing a more normalized view.</p>
</li>
<li>
<p><strong>Implementation:</strong></p>
</li>
<li>Create a matrix of ratings, with users as rows and movies as columns.</li>
<li>Compute the mean rating for each movie (column-wise).</li>
<li>Subtract the mean rating of each movie from individual ratings in the corresponding column.</li>
<li>
<p>This process normalizes the ratings to have zero mean, making predictions more reasonable.</p>
</li>
<li>
<p><strong>Example:</strong></p>
</li>
<li>Predictions for a new user like Eve now rely on the average rating of movies rather than assuming zero ratings.</li>
<li>
<p>Predictions are adjusted by adding back the mean rating of each movie to ensure realistic values.</p>
</li>
<li>
<p><strong>Benefits:</strong></p>
</li>
<li><strong>Efficiency:</strong> Optimization algorithms run faster with mean normalization.</li>
<li><strong>Improved Predictions:</strong> More reasonable predictions, especially for users with limited or no rating history.</li>
<li>
<p><strong>Algorithm Stability:</strong> Ensures the algorithm behaves better across different user scenarios.</p>
</li>
<li>
<p><strong>Implementation Choices:</strong></p>
</li>
<li>Focus on normalizing rows (users) rather than columns (movies), prioritizing user-specific predictions.</li>
<li>
<p>While normalizing columns could handle unrated movies, it's often less critical in practical scenarios.</p>
</li>
<li>
<p><strong>Practical Application:</strong></p>
</li>
<li>Implement mean normalization in TensorFlow for building efficient and effective recommender systems.</li>
<li>Normalize user ratings to zero mean, enhancing performance and user experience.</li>
</ol>
<p><strong>Conclusion:</strong>
Mean normalization enhances the performance and efficiency of recommender systems, particularly in scenarios with limited user data. By adjusting ratings to have zero mean, the algorithm provides more accurate predictions, ensuring better user engagement and satisfaction.</p>
<h3 id="tensorflow-implementation-of-collaborative-filtering">TensorFlow implementation of collaborative filtering<a class="headerlink" href="#tensorflow-implementation-of-collaborative-filtering" title="Permanent link">&para;</a></h3>
<p><strong>TensorFlow Implementation of Collaborative Filtering</strong></p>
<p><strong>Objective:</strong>
Implement collaborative filtering algorithm using TensorFlow, leveraging its automatic differentiation feature to compute derivatives efficiently.</p>
<p><strong>Explanation:</strong>
1. <strong>TensorFlow for Learning Algorithms:</strong>
   - TensorFlow is commonly associated with neural networks but is versatile for various learning algorithms.
   - Its automatic differentiation feature simplifies gradient computation, crucial for optimization algorithms like gradient descent.</p>
<ol>
<li><strong>Gradient Descent in TensorFlow:</strong></li>
<li>Traditional gradient descent involves iteratively updating parameters based on derivative of the cost function.</li>
<li>
<p>TensorFlow's gradient tape records operations to compute derivatives automatically.</p>
</li>
<li>
<p><strong>Example with TensorFlow:</strong></p>
</li>
<li>Define parameters like <code>w</code> and initialize them.</li>
<li>Specify learning rate, number of iterations, and other hyperparameters.</li>
<li>Use TensorFlow's gradient tape to compute derivatives of the cost function.</li>
<li>
<p>Update parameters using gradients and optimization algorithms like Adam.</p>
</li>
<li>
<p><strong>Auto Diff in TensorFlow:</strong></p>
</li>
<li>Auto Diff (Automatic Differentiation) simplifies derivative computation in TensorFlow.</li>
<li>
<p>TensorFlow computes derivatives of the cost function without manual calculation, enhancing efficiency and accuracy.</p>
</li>
<li>
<p><strong>Implementation Choices:</strong></p>
</li>
<li>Implement the cost function <code>J</code> specifying input parameters like <code>x</code>, <code>w</code>, <code>b</code>, and regularization terms.</li>
<li>Use gradient tape to record operations and compute derivatives automatically.</li>
<li>
<p>Update parameters using optimization algorithms like gradient descent or Adam.</p>
</li>
<li>
<p><strong>Benefits of TensorFlow:</strong></p>
</li>
<li>TensorFlow's Auto Diff feature eliminates manual computation of derivatives, simplifying implementation.</li>
<li>
<p>Allows for efficient optimization using advanced algorithms like Adam.</p>
</li>
<li>
<p><strong>Real-world Application:</strong></p>
</li>
<li>Apply collaborative filtering algorithm to real datasets like the MovieLens dataset.</li>
<li>
<p>TensorFlow's capabilities enable effective analysis and recommendation based on actual user ratings.</p>
</li>
<li>
<p><strong>Recipe for Implementation:</strong></p>
</li>
<li>While TensorFlow's standard neural network layers may not directly fit collaborative filtering algorithms, custom implementations with Auto Diff provide effective solutions.</li>
<li>Leveraging TensorFlow's tools, even non-neural network algorithms can be efficiently implemented.</li>
</ol>
<p><strong>Conclusion:</strong>
Implementing collaborative filtering in TensorFlow involves defining the cost function, leveraging TensorFlow's Auto Diff feature to compute derivatives, and updating parameters using optimization algorithms. TensorFlow's versatility extends beyond neural networks, making it a powerful tool for various learning algorithms, including collaborative filtering. Enjoy experimenting with collaborative filtering in TensorFlow and exploring its applications in real-world datasets like MovieLens.</p>
<p>Below is the TensorFlow implementation of collaborative filtering using gradient descent:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="c1"># Define parameters</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>  <span class="c1"># Initialize parameter w</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="n">x</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Input feature</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span class="n">y</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># True label</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># Learning rate</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="n">iterations</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># Number of iterations</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span class="c1"># Gradient descent optimization loop</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>    <span class="c1"># Compute the cost function J and gradients</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>        <span class="n">f</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span>  <span class="c1"># Model prediction</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>        <span class="n">J</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">f</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Cost function</span>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>    <span class="n">dJ_dw</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>  <span class="c1"># Derivative of J w.r.t. w</span>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>    <span class="c1"># Update parameters using gradient descent</span>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>    <span class="n">w</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">dJ_dw</span><span class="p">)</span>  <span class="c1"># w = w - alpha * dJ_dw</span>
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a>
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>    <span class="c1"># Print progress</span>
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Iteration </span><span class="si">{</span><span class="nb">iter</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">: w = </span><span class="si">{</span><span class="n">w</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s1">, J = </span><span class="si">{</span><span class="n">J</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<p>This code demonstrates how to use TensorFlow to perform gradient descent for optimizing a simple linear regression model, where <code>w</code> is the weight parameter. The cost function <code>J</code> is defined as the squared difference between the predicted value <code>f</code> and the true label <code>y</code>. The <code>tf.GradientTape()</code> context records operations for computing gradients, and <code>tape.gradient()</code> calculates the derivative of the cost function with respect to the parameter <code>w</code>. Finally, the <code>assign_sub()</code> method updates the parameter <code>w</code> based on the computed gradient and the learning rate.</p>
<p>For collaborative filtering using TensorFlow with the MovieLens dataset and Adam optimizer, here's a sample code:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="c1"># Define parameters and hyperparameters</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="n">num_users</span> <span class="o">=</span> <span class="mi">1000</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="n">num_movies</span> <span class="o">=</span> <span class="mi">500</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span class="n">latent_features</span> <span class="o">=</span> <span class="mi">10</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span class="n">lambda_reg</span> <span class="o">=</span> <span class="mf">0.01</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span class="n">iterations</span> <span class="o">=</span> <span class="mi">200</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a><span class="c1"># Initialize variables, ratings matrix, and optimizer</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="n">num_users</span><span class="p">,</span> <span class="n">latent_features</span><span class="p">]))</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="n">num_movies</span><span class="p">,</span> <span class="n">latent_features</span><span class="p">]))</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a><span class="c1"># Gradient descent optimization loop</span>
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a><span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>    <span class="c1"># Compute the cost function J</span>
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a>    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a>        <span class="n">f</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>  <span class="c1"># Model prediction</span>
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a>        <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">f</span> <span class="o">-</span> <span class="n">ratings_norm</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span>  <span class="c1"># Difference between predicted and actual ratings</span>
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#__codelineno-2-23"></a>        <span class="n">J</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_users</span><span class="p">)</span>  <span class="c1"># Cost function</span>
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#__codelineno-2-24"></a>        <span class="c1"># Add regularization term</span>
<a id="__codelineno-2-25" name="__codelineno-2-25" href="#__codelineno-2-25"></a>        <span class="n">J</span> <span class="o">+=</span> <span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">w</span><span class="p">))</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">b</span><span class="p">)))</span>
<a id="__codelineno-2-26" name="__codelineno-2-26" href="#__codelineno-2-26"></a>
<a id="__codelineno-2-27" name="__codelineno-2-27" href="#__codelineno-2-27"></a>    <span class="c1"># Compute gradients</span>
<a id="__codelineno-2-28" name="__codelineno-2-28" href="#__codelineno-2-28"></a>    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
<a id="__codelineno-2-29" name="__codelineno-2-29" href="#__codelineno-2-29"></a>
<a id="__codelineno-2-30" name="__codelineno-2-30" href="#__codelineno-2-30"></a>    <span class="c1"># Update parameters using Adam optimizer</span>
<a id="__codelineno-2-31" name="__codelineno-2-31" href="#__codelineno-2-31"></a>    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">]))</span>
<a id="__codelineno-2-32" name="__codelineno-2-32" href="#__codelineno-2-32"></a>
<a id="__codelineno-2-33" name="__codelineno-2-33" href="#__codelineno-2-33"></a>    <span class="c1"># Print progress</span>
<a id="__codelineno-2-34" name="__codelineno-2-34" href="#__codelineno-2-34"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Iteration </span><span class="si">{</span><span class="nb">iter</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">: Loss = </span><span class="si">{</span><span class="n">J</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<a id="__codelineno-2-35" name="__codelineno-2-35" href="#__codelineno-2-35"></a>
<a id="__codelineno-2-36" name="__codelineno-2-36" href="#__codelineno-2-36"></a><span class="c1"># Predict ratings for new users/movies</span>
<a id="__codelineno-2-37" name="__codelineno-2-37" href="#__codelineno-2-37"></a><span class="n">predicted_ratings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
</code></pre></div>
<p>This code showcases how to implement collaborative filtering with TensorFlow using the MovieLens dataset. The cost function <code>J</code> incorporates the squared difference between predicted and actual ratings, along with a regularization term to prevent overfitting. The Adam optimizer is used to minimize the cost function by updating the latent feature matrices <code>w</code> and <code>b</code>. Finally, the trained model can be used to predict ratings for new users or movies.</p>
<h3 id="finding-related-items">Finding Related Items<a class="headerlink" href="#finding-related-items" title="Permanent link">&para;</a></h3>
<p>To find related items using collaborative filtering, you can follow these steps:</p>
<ol>
<li><strong>Calculate Similarity Score</strong>: Given the features <span class="arithmatex">\( x^{(i)} \)</span> of item <span class="arithmatex">\( i \)</span> and the features <span class="arithmatex">\( x^{(k)} \)</span> of all other items, compute the similarity score between item <span class="arithmatex">\( i \)</span> and each other item <span class="arithmatex">\( k \)</span> using the squared distance between their feature vectors.</li>
</ol>
<div class="arithmatex">\[
\text{Similarity Score} = \sum_{l=1}^{n} (x^{(k)}_l - x^{(i)}_l)^2
\]</div>
<ol>
<li><strong>Find Similar Items</strong>: Sort the items based on their similarity scores and select the <span class="arithmatex">\( k \)</span> items with the smallest scores. These <span class="arithmatex">\( k \)</span> items are considered to be the most similar or related to item <span class="arithmatex">\( i \)</span>.</li>
</ol>
<p>Here's how you can implement this in Python:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="k">def</span> <span class="nf">find_related_items</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">item_index</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="sd">    Find related items to the given item based on their features.</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="sd">    Parameters:</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="sd">        features (np.ndarray): Array of shape (num_items, num_features) containing the features of all items.</span>
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a><span class="sd">        item_index (int): Index of the item for which related items are to be found.</span>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a><span class="sd">        k (int): Number of related items to return.</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a><span class="sd">    Returns:</span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a><span class="sd">        List of indices of the k most related items.</span>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a>    <span class="n">item_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">item_index</span><span class="p">]</span>
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a>    <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">features</span> <span class="o">-</span> <span class="n">item_features</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Calculate squared distances</span>
<a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a>    <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>  <span class="c1"># Sort indices based on distances</span>
<a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a>    <span class="n">related_indices</span> <span class="o">=</span> <span class="n">sorted_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Exclude the item itself and select top k</span>
<a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a>    <span class="k">return</span> <span class="n">related_indices</span>
<a id="__codelineno-3-20" name="__codelineno-3-20" href="#__codelineno-3-20"></a>
<a id="__codelineno-3-21" name="__codelineno-3-21" href="#__codelineno-3-21"></a><span class="c1"># Example usage</span>
<a id="__codelineno-3-22" name="__codelineno-3-22" href="#__codelineno-3-22"></a><span class="c1"># Suppose you have a matrix &#39;features&#39; containing features of all items</span>
<a id="__codelineno-3-23" name="__codelineno-3-23" href="#__codelineno-3-23"></a><span class="c1"># and you want to find related items to item at index 0</span>
<a id="__codelineno-3-24" name="__codelineno-3-24" href="#__codelineno-3-24"></a><span class="n">related_indices</span> <span class="o">=</span> <span class="n">find_related_items</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">item_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<a id="__codelineno-3-25" name="__codelineno-3-25" href="#__codelineno-3-25"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Related Items:&quot;</span><span class="p">,</span> <span class="n">related_indices</span><span class="p">)</span>
</code></pre></div>
<p>In this implementation, <code>features</code> is a NumPy array of shape <code>(num_items, num_features)</code> containing the features of all items. The function <code>find_related_items</code> takes the features array, the index of the item for which related items are to be found, and the number of related items to return (<code>k</code>). It calculates the squared distances between the given item's features and the features of all other items, sorts the items based on these distances, and returns the indices of the top <span class="arithmatex">\( k \)</span> most related items.</p>
<p>By using this approach, you can efficiently find items that are similar to a given item based on their features, allowing you to recommend related products to users on an online shopping website or suggest similar movies to users on a streaming platform.</p>
<h3 id="collaborative-filtering-vs-content-based-filtering">Collaborative filtering vs Content-based filtering<a class="headerlink" href="#collaborative-filtering-vs-content-based-filtering" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Collaborative Filtering</strong>:</li>
<li>Recommends items based on ratings of users who gave similar ratings as you.</li>
<li>Uses the ratings given by users for various items to recommend new items.</li>
<li>Does not require explicit features of users or items.</li>
<li>Predicts the rating of a user for an item based on the ratings of similar users for similar items.</li>
<li>It does not explicitly model the characteristics of items or users but instead relies on patterns of user behavior.</li>
<li>
<p>Often suffers from the cold start problem for new users or items with limited ratings.</p>
</li>
<li>
<p><strong>Content-Based Filtering</strong>:</p>
</li>
<li>Recommends items based on the features of users and items to find a good match.</li>
<li>Requires having features of each user and each item.</li>
<li>Predicts the suitability of an item for a user based on the features of both the user and the item.</li>
<li>Uses features such as age, gender, country, past behaviors, genre, year of release, critic reviews, etc., to describe users and items.</li>
<li>Constructs feature vectors for users and items and learns to match them based on these features.</li>
<li>Does not suffer as much from the cold start problem since it can make recommendations based on item features even if there are few user ratings.</li>
</ol>
<p>Here's a Python function to compute the dot product between user and item feature vectors:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="k">def</span> <span class="nf">compute_rating</span><span class="p">(</span><span class="n">user_features</span><span class="p">,</span> <span class="n">item_features</span><span class="p">):</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a><span class="sd">    Compute the predicted rating based on user and item features.</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a><span class="sd">    Parameters:</span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a><span class="sd">        user_features (np.ndarray): Array of user features.</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a><span class="sd">        item_features (np.ndarray): Array of item features.</span>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a><span class="sd">    Returns:</span>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a><span class="sd">        Predicted rating as a scalar value.</span>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">user_features</span><span class="p">,</span> <span class="n">item_features</span><span class="p">)</span>
<a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a>
<a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a><span class="c1"># Example usage</span>
<a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a><span class="c1"># Suppose you have user_features and item_features as feature vectors</span>
<a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a><span class="n">user_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>  <span class="c1"># Example user feature vector</span>
<a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a><span class="n">item_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>  <span class="c1"># Example item feature vector</span>
<a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a><span class="n">predicted_rating</span> <span class="o">=</span> <span class="n">compute_rating</span><span class="p">(</span><span class="n">user_features</span><span class="p">,</span> <span class="n">item_features</span><span class="p">)</span>
<a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted Rating:&quot;</span><span class="p">,</span> <span class="n">predicted_rating</span><span class="p">)</span>
</code></pre></div>
<p>In content-based filtering, the task is to learn appropriate user and item feature vectors (<span class="arithmatex">\( v_u \)</span> and <span class="arithmatex">\( v_m \)</span>) that capture users' preferences and item characteristics, respectively. These feature vectors are then used to predict the suitability of items for users based on their dot product.</p>
<h3 id="deep-learning-for-content-based-filtering">Deep learning for content-based filtering<a class="headerlink" href="#deep-learning-for-content-based-filtering" title="Permanent link">&para;</a></h3>
<p>In the video, the approach to developing a content-based filtering algorithm using deep learning is outlined. Here's a summary:</p>
<ol>
<li><strong>Neural Networks for Feature Extraction</strong>:</li>
<li>Use neural networks to compute feature vectors (<span class="arithmatex">\( v_u \)</span> and <span class="arithmatex">\( v_m \)</span>) for users and items, respectively.</li>
<li>The user network takes user features (<span class="arithmatex">\( x_u \)</span>) as input and outputs <span class="arithmatex">\( v_u \)</span>.</li>
<li>Similarly, the movie network takes movie features (<span class="arithmatex">\( x_m \)</span>) as input and outputs <span class="arithmatex">\( v_m \)</span>.</li>
<li>
<p>The output layers of both networks have multiple units (e.g., 32 units) instead of a single unit.</p>
</li>
<li>
<p><strong>Prediction</strong>:</p>
</li>
<li>Predict the rating of a user (<span class="arithmatex">\( j \)</span>) for a movie (<span class="arithmatex">\( i \)</span>) using the dot product of <span class="arithmatex">\( v_u \)</span> and <span class="arithmatex">\( v_m \)</span>.</li>
<li>
<p>Alternatively, for binary labels, apply the sigmoid function to the dot product to predict the probability of liking the item.</p>
</li>
<li>
<p><strong>Training</strong>:</p>
</li>
<li>Define a cost function (<span class="arithmatex">\( J \)</span>) similar to collaborative filtering to minimize the squared error between predictions and actual ratings.</li>
<li>Train the parameters of both the user and movie networks using gradient descent or other optimization algorithms.</li>
<li>
<p>Regularize the model to prevent overfitting.</p>
</li>
<li>
<p><strong>Finding Similar Items</strong>:</p>
</li>
<li>Use the computed feature vectors (<span class="arithmatex">\( v_m \)</span>) to find similar items to a given item (<span class="arithmatex">\( i \)</span>).</li>
<li>
<p>Find other items (<span class="arithmatex">\( k \)</span>) with small squared distance from the vector describing movie <span class="arithmatex">\( i \)</span>.</p>
</li>
<li>
<p><strong>Pre-computation</strong>:</p>
</li>
<li>
<p>Pre-compute similar items for each movie ahead of time to improve efficiency when making recommendations.</p>
</li>
<li>
<p><strong>Scaling</strong>:</p>
</li>
<li>Consider practical issues such as computational complexity when dealing with a large catalog of items.</li>
<li>
<p>Modify the algorithm to make it scalable for large item catalogs.</p>
</li>
<li>
<p><strong>Feature Engineering</strong>:</p>
</li>
<li>
<p>Spend time engineering good features for the application to improve the performance of the algorithm.</p>
</li>
<li>
<p><strong>Architecture</strong>:</p>
</li>
<li>Combining user and movie networks demonstrates the ability to build complex architectures using neural networks.</li>
</ol>
<p>Here's a Python-like pseudo-code representation of the described approach:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="c1"># Define user and movie networks</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="n">user_network</span> <span class="o">=</span> <span class="n">create_neural_network</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">user_feature_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="n">movie_network</span> <span class="o">=</span> <span class="n">create_neural_network</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">movie_feature_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="c1"># Train the networks using gradient descent to minimize squared error</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a>    <span class="k">for</span> <span class="n">user_features</span><span class="p">,</span> <span class="n">movie_features</span><span class="p">,</span> <span class="n">rating</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>        <span class="n">user_embedding</span> <span class="o">=</span> <span class="n">user_network</span><span class="p">(</span><span class="n">user_features</span><span class="p">)</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>        <span class="n">movie_embedding</span> <span class="o">=</span> <span class="n">movie_network</span><span class="p">(</span><span class="n">movie_features</span><span class="p">)</span>
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a>        <span class="n">predicted_rating</span> <span class="o">=</span> <span class="n">dot_product</span><span class="p">(</span><span class="n">user_embedding</span><span class="p">,</span> <span class="n">movie_embedding</span><span class="p">)</span>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">squared_error</span><span class="p">(</span><span class="n">predicted_rating</span><span class="p">,</span> <span class="n">rating</span><span class="p">)</span>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a>        <span class="c1"># Backpropagation and parameter updates</span>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a>
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a><span class="c1"># Use the trained networks to make predictions</span>
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a><span class="k">def</span> <span class="nf">predict_rating</span><span class="p">(</span><span class="n">user_features</span><span class="p">,</span> <span class="n">movie_features</span><span class="p">):</span>
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a>    <span class="n">user_embedding</span> <span class="o">=</span> <span class="n">user_network</span><span class="p">(</span><span class="n">user_features</span><span class="p">)</span>
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a>    <span class="n">movie_embedding</span> <span class="o">=</span> <span class="n">movie_network</span><span class="p">(</span><span class="n">movie_features</span><span class="p">)</span>
<a id="__codelineno-5-18" name="__codelineno-5-18" href="#__codelineno-5-18"></a>    <span class="k">return</span> <span class="n">dot_product</span><span class="p">(</span><span class="n">user_embedding</span><span class="p">,</span> <span class="n">movie_embedding</span><span class="p">)</span>
<a id="__codelineno-5-19" name="__codelineno-5-19" href="#__codelineno-5-19"></a>
<a id="__codelineno-5-20" name="__codelineno-5-20" href="#__codelineno-5-20"></a><span class="c1"># Find similar items to a given item based on computed embeddings</span>
<a id="__codelineno-5-21" name="__codelineno-5-21" href="#__codelineno-5-21"></a><span class="k">def</span> <span class="nf">find_similar_items</span><span class="p">(</span><span class="n">movie_features</span><span class="p">,</span> <span class="n">all_movie_features</span><span class="p">):</span>
<a id="__codelineno-5-22" name="__codelineno-5-22" href="#__codelineno-5-22"></a>    <span class="n">movie_embedding</span> <span class="o">=</span> <span class="n">movie_network</span><span class="p">(</span><span class="n">movie_features</span><span class="p">)</span>
<a id="__codelineno-5-23" name="__codelineno-5-23" href="#__codelineno-5-23"></a>    <span class="n">distances</span> <span class="o">=</span> <span class="n">compute_distances</span><span class="p">(</span><span class="n">movie_embedding</span><span class="p">,</span> <span class="n">all_movie_features</span><span class="p">)</span>
<a id="__codelineno-5-24" name="__codelineno-5-24" href="#__codelineno-5-24"></a>    <span class="n">similar_items</span> <span class="o">=</span> <span class="n">find_top_k_similar_items</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
<a id="__codelineno-5-25" name="__codelineno-5-25" href="#__codelineno-5-25"></a>    <span class="k">return</span> <span class="n">similar_items</span>
</code></pre></div>
<p>This pseudo-code outlines the training process, prediction, and finding similar items using the computed embeddings. It's important to note that the actual implementation would involve using deep learning libraries like TensorFlow or PyTorch for neural network training and inference.</p>
<h3 id="recommending-from-a-large-catalogue">Recommending from a large catalogue<a class="headerlink" href="#recommending-from-a-large-catalogue" title="Permanent link">&para;</a></h3>
<p>In the video, the process of efficiently recommending items from a large catalog is discussed, typically implemented in two steps: retrieval and ranking. Here's a summary:</p>
<ol>
<li><strong>Retrieval Step</strong>:</li>
<li>Generate a large list of plausible item candidates.</li>
<li>Include items that are similar to the ones the user has interacted with recently.</li>
<li>Add items based on user preferences, such as top genres or popular items in the user's country.</li>
<li>This step aims to ensure broad coverage and may include some irrelevant items.</li>
<li>
<p>The retrieved items may number in the hundreds.</p>
</li>
<li>
<p><strong>Ranking Step</strong>:</p>
</li>
<li>Take the list of retrieved items and rank them using a learned model.</li>
<li>Compute predicted ratings for each user-item pair using a neural network.</li>
<li>Display the ranked list of items to the user, prioritizing those with the highest predicted ratings.</li>
<li>
<p>This step refines the list of items to present the user with the most relevant recommendations.</p>
</li>
<li>
<p><strong>Efficiency Optimization</strong>:</p>
</li>
<li>Pre-compute item similarities to speed up retrieval.</li>
<li>Perform neural network inference only once for the user's feature vector, then compute inner products with pre-computed item embeddings.</li>
<li>
<p>Decide on the number of items to retrieve during the retrieval step based on offline experiments and trade-offs between performance and speed.</p>
</li>
<li>
<p><strong>Ethical Considerations</strong>:</p>
</li>
<li>Recommender systems have significant ethical implications and potential for harm.</li>
<li>Developers should take an ethical approach and prioritize serving users and society rather than solely maximizing engagement or profits.</li>
<li>Awareness of ethical issues and responsible design choices are essential when building recommender systems.</li>
</ol>
<p>Here's a Python-like pseudo-code representation of the retrieval and ranking steps:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="c1"># Retrieval Step</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="k">def</span> <span class="nf">retrieval_step</span><span class="p">(</span><span class="n">user_features</span><span class="p">,</span> <span class="n">recent_items</span><span class="p">,</span> <span class="n">user_preferences</span><span class="p">,</span> <span class="n">country</span><span class="p">):</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>    <span class="n">similar_items</span> <span class="o">=</span> <span class="n">find_similar_items</span><span class="p">(</span><span class="n">recent_items</span><span class="p">)</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>    <span class="n">genre_based_items</span> <span class="o">=</span> <span class="n">get_top_genre_items</span><span class="p">(</span><span class="n">user_preferences</span><span class="p">)</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>    <span class="n">country_based_items</span> <span class="o">=</span> <span class="n">get_top_country_items</span><span class="p">(</span><span class="n">country</span><span class="p">)</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>    <span class="n">retrieved_items</span> <span class="o">=</span> <span class="n">merge_lists</span><span class="p">(</span><span class="n">similar_items</span><span class="p">,</span> <span class="n">genre_based_items</span><span class="p">,</span> <span class="n">country_based_items</span><span class="p">)</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>    <span class="k">return</span> <span class="n">retrieved_items</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a><span class="c1"># Ranking Step</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span class="k">def</span> <span class="nf">ranking_step</span><span class="p">(</span><span class="n">user_features</span><span class="p">,</span> <span class="n">retrieved_items</span><span class="p">):</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>    <span class="n">user_embedding</span> <span class="o">=</span> <span class="n">compute_user_embedding</span><span class="p">(</span><span class="n">user_features</span><span class="p">)</span>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a>    <span class="n">ranked_items</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a>    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">retrieved_items</span><span class="p">:</span>
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a>        <span class="n">item_embedding</span> <span class="o">=</span> <span class="n">compute_item_embedding</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a>        <span class="n">predicted_rating</span> <span class="o">=</span> <span class="n">dot_product</span><span class="p">(</span><span class="n">user_embedding</span><span class="p">,</span> <span class="n">item_embedding</span><span class="p">)</span>
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a>        <span class="n">ranked_items</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">item</span><span class="p">,</span> <span class="n">predicted_rating</span><span class="p">))</span>
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a>    <span class="n">ranked_items</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-6-18" name="__codelineno-6-18" href="#__codelineno-6-18"></a>    <span class="k">return</span> <span class="n">ranked_items</span>
<a id="__codelineno-6-19" name="__codelineno-6-19" href="#__codelineno-6-19"></a>
<a id="__codelineno-6-20" name="__codelineno-6-20" href="#__codelineno-6-20"></a><span class="c1"># Main Recommender System</span>
<a id="__codelineno-6-21" name="__codelineno-6-21" href="#__codelineno-6-21"></a><span class="k">def</span> <span class="nf">recommend</span><span class="p">(</span><span class="n">user_features</span><span class="p">,</span> <span class="n">recent_items</span><span class="p">,</span> <span class="n">user_preferences</span><span class="p">,</span> <span class="n">country</span><span class="p">):</span>
<a id="__codelineno-6-22" name="__codelineno-6-22" href="#__codelineno-6-22"></a>    <span class="n">retrieved_items</span> <span class="o">=</span> <span class="n">retrieval_step</span><span class="p">(</span><span class="n">user_features</span><span class="p">,</span> <span class="n">recent_items</span><span class="p">,</span> <span class="n">user_preferences</span><span class="p">,</span> <span class="n">country</span><span class="p">)</span>
<a id="__codelineno-6-23" name="__codelineno-6-23" href="#__codelineno-6-23"></a>    <span class="n">ranked_items</span> <span class="o">=</span> <span class="n">ranking_step</span><span class="p">(</span><span class="n">user_features</span><span class="p">,</span> <span class="n">retrieved_items</span><span class="p">)</span>
<a id="__codelineno-6-24" name="__codelineno-6-24" href="#__codelineno-6-24"></a>    <span class="k">return</span> <span class="n">ranked_items</span><span class="p">[:</span><span class="n">top_n_recommendations</span><span class="p">]</span>
</code></pre></div>
<p>This pseudo-code outlines the retrieval and ranking steps in a recommender system. It combines user features, recent interactions, user preferences, and country information to retrieve a list of candidate items. Then, it ranks these items based on predicted ratings computed using neural network embeddings. Finally, it returns the top-ranked recommendations to the user.</p>
<h3 id="ethical-use-of-recommender-systems">Ethical use of recommender systems<a class="headerlink" href="#ethical-use-of-recommender-systems" title="Permanent link">&para;</a></h3>
<p>The video emphasizes the ethical considerations associated with recommender systems and highlights some problematic use cases along with potential ameliorations to reduce harm and increase societal benefit. Here's a summary:</p>
<ol>
<li><strong>Setting Goals for Recommender Systems</strong>:</li>
<li>Recommender systems can be configured in various ways, such as recommending items most likely to be rated highly by the user or most likely to be purchased.</li>
<li>They can also be used in advertising to show ads most likely to be clicked on or to maximize profit.</li>
<li>
<p>Choices in setting goals and deciding what to recommend can have ethical implications.</p>
</li>
<li>
<p><strong>Problematic Use Cases</strong>:</p>
</li>
<li><strong>Maximizing Profit</strong>: Recommending items or ads based on profitability rather than user preference can lead to suboptimal user experiences.</li>
<li><strong>Maximizing Engagement</strong>: Recommender systems that prioritize maximizing user engagement may inadvertently promote harmful content like conspiracy theories or hate speech.</li>
<li>
<p><strong>Lack of Transparency</strong>: Users may not realize that recommendations are driven by profit motives rather than their best interests, leading to a lack of trust.</p>
</li>
<li>
<p><strong>Ameliorations</strong>:</p>
</li>
<li><strong>Filtering Out Harmful Content</strong>: Implementing filters to remove problematic content like hate speech or scams can mitigate negative impacts.</li>
<li>
<p><strong>Transparency</strong>: Being transparent with users about the criteria used for recommendations can build trust and empower users to make informed choices.</p>
</li>
<li>
<p><strong>Challenges and Solutions</strong>:</p>
</li>
<li>Defining what constitutes harmful content or exploitative businesses is challenging but essential.</li>
<li>Encouraging open discussion and diverse perspectives can lead to better design choices and mitigate potential harm.</li>
<li>
<p>Ultimately, the goal should be to create systems that make society better off, not just maximize profits or engagement.</p>
</li>
<li>
<p><strong>Call to Action</strong>:</p>
</li>
<li>Developers are urged to consider the societal implications of their recommender systems and strive to create solutions that prioritize the well-being of users and society.</li>
<li>Transparency, ethical design, and ongoing evaluation of impact are crucial in ensuring that recommender systems contribute positively to society.</li>
</ol>
<p>In conclusion, while recommender systems offer powerful capabilities, their ethical use requires careful consideration of their impact on individuals and society. By prioritizing transparency, accountability, and societal benefit, developers can help ensure that recommender systems serve users in responsible and ethical ways.</p>
<h3 id="tensorflow-implementation-of-content-based-filtering">TensorFlow implementation of content-based filtering<a class="headerlink" href="#tensorflow-implementation-of-content-based-filtering" title="Permanent link">&para;</a></h3>
<p>In the TensorFlow implementation of content-based filtering discussed in the video, several key concepts are highlighted:</p>
<ol>
<li><strong>User and Item Networks</strong>:</li>
<li>The implementation starts with defining separate neural networks for users and items (movies in this case).</li>
<li>
<p>Each network consists of dense layers with specified numbers of hidden units, followed by an output layer with 32 units.</p>
</li>
<li>
<p><strong>Sequential Model in TensorFlow</strong>:</p>
</li>
<li>TensorFlow's Keras API is used to define the neural networks as sequential models.</li>
<li>
<p>Sequential models allow stacking layers one after the other, making it easy to create feedforward neural networks.</p>
</li>
<li>
<p><strong>Input Features</strong>:</p>
</li>
<li>User features and item features are fed into their respective neural networks using TensorFlow's syntax for model input.</li>
<li>
<p>This involves extracting the input features and passing them to the defined neural network models.</p>
</li>
<li>
<p><strong>Normalization</strong>:</p>
</li>
<li>After computing the user and item vectors (vu and vm), they are normalized to have a length of one.</li>
<li>
<p>Normalizing the vectors helps improve the performance of the algorithm.</p>
</li>
<li>
<p><strong>Dot Product</strong>:</p>
</li>
<li>The dot product between the normalized user and item vectors is computed using a special Keras layer (<code>tf.keras.layers.Dot</code>).</li>
<li>
<p>This dot product serves as the final prediction of the model.</p>
</li>
<li>
<p><strong>Model Definition</strong>:</p>
</li>
<li>
<p>Finally, the inputs and outputs of the model are defined to inform TensorFlow about the structure of the model.</p>
</li>
<li>
<p><strong>Cost Function</strong>:</p>
</li>
<li>
<p>The mean squared error (MSE) cost function is used to train the model, measuring the average squared difference between the predicted ratings and the actual ratings.</p>
</li>
<li>
<p><strong>L2 Normalization</strong>:</p>
</li>
<li>An additional step involves normalizing the length of the vectors vu and vm using TensorFlow's <code>l2_normalize</code> function, which helps improve the algorithm's performance.</li>
</ol>
<p>By following these key code snippets and concepts, developers can implement content-based filtering in TensorFlow to build recommender systems. The remaining code details can be explored in the practice lab to understand how all these components fit together into a working implementation.</p>
<h3 id="reducing-the-number-of-features-optional">Reducing the number of features (optional)<a class="headerlink" href="#reducing-the-number-of-features-optional" title="Permanent link">&para;</a></h3>
<p>Principal Component Analysis (PCA) is a widely used unsupervised learning algorithm for dimensionality reduction, commonly employed for data visualization when dealing with datasets containing a large number of features. The goal of PCA is to reduce the number of features while retaining as much information as possible, enabling visualization in two or three dimensions.</p>
<p>Here's how PCA works, illustrated with examples:</p>
<ol>
<li><strong>Introduction to PCA</strong>:</li>
<li>Consider a dataset with multiple features, such as measurements of passenger cars, including features like length, width, height, and more.</li>
<li>
<p>Visualizing such high-dimensional data directly is challenging.</p>
</li>
<li>
<p><strong>Example 1: Length vs. Width</strong>:</p>
</li>
<li>If we have a dataset with car lengths and widths, and we observe that width varies relatively little compared to length across cars.</li>
<li>
<p>PCA would automatically suggest using only the length (x_1) as a representative feature.</p>
</li>
<li>
<p><strong>Example 2: Length vs. Wheel Diameter</strong>:</p>
</li>
<li>Similarly, if we have car lengths and wheel diameters, and we observe that wheel diameter varies but not significantly across cars.</li>
<li>
<p>PCA would suggest using only the length (x_1) again.</p>
</li>
<li>
<p><strong>Example 3: Length vs. Height</strong>:</p>
</li>
<li>If we have car lengths and heights, and we observe substantial variation in both features across cars.</li>
<li>
<p>PCA would suggest creating a new axis (z-axis) that captures both length and height information, reducing the dataset to a single feature.</p>
</li>
<li>
<p><strong>Complex Example: Three-Dimensional Data</strong>:</p>
</li>
<li>Visualizing three-dimensional data on a two-dimensional screen can be challenging.</li>
<li>
<p>PCA can project the data onto a two-dimensional plane (z_1, z_2), retaining the most significant information.</p>
</li>
<li>
<p><strong>Application to Country Data</strong>:</p>
</li>
<li>For example, consider data on countries' GDP, per capita GDP, and Human Development Index (HDI).</li>
<li>
<p>PCA can compress these 50 features into two features (z_1, z_2), making it easier to visualize and understand the relationships between countries.</p>
</li>
<li>
<p><strong>Visualization and Understanding</strong>:</p>
</li>
<li>PCA facilitates data visualization, allowing data scientists to better understand the structure and patterns within the dataset.</li>
<li>
<p>It helps identify trends, outliers, and potential issues in the data.</p>
</li>
<li>
<p><strong>Reducing Dimensionality</strong>:</p>
</li>
<li>PCA reduces high-dimensional data to a lower dimension (typically two or three dimensions) for easier visualization.</li>
<li>It retains as much variance in the data as possible while reducing the number of features.</li>
</ol>
<p>PCA is a valuable tool for exploratory data analysis and visualization, enabling data scientists to gain insights from complex datasets. In the next video, the mechanics of the PCA algorithm will be explored in detail.</p>
<h3 id="pca-algorithm-optional">PCA algorithm (optional)<a class="headerlink" href="#pca-algorithm-optional" title="Permanent link">&para;</a></h3>
<p>Principal Component Analysis (PCA) is a technique used for dimensionality reduction, particularly useful for visualizing high-dimensional data. Here's a step-by-step explanation of how PCA works:</p>
<ol>
<li><strong>Data Preprocessing</strong>:</li>
<li>
<p>Before applying PCA, it's essential to preprocess the data by normalizing it to have zero mean. This ensures that features with different scales do not dominate the analysis.</p>
</li>
<li>
<p><strong>Initial Data Representation</strong>:</p>
</li>
<li>
<p>Suppose we have a dataset with two features, <span class="arithmatex">\( x_1 \)</span> and <span class="arithmatex">\( x_2 \)</span>. Initially, the data is represented using these two features as axes.</p>
</li>
<li>
<p><strong>Choosing a New Axis (Principal Component)</strong>:</p>
</li>
<li>PCA aims to replace these two features with just one feature, denoted as the <span class="arithmatex">\( z \)</span>-axis. The goal is to choose a new axis that captures the essential information in the data.</li>
<li>
<p>The new axis should be such that when data points are projected onto it, the spread or variance of the data is maximized.</p>
</li>
<li>
<p><strong>Projection onto the New Axis</strong>:</p>
</li>
<li>Each data point is projected onto the new axis, resulting in a one-dimensional representation of the data.</li>
<li>
<p>The projection involves finding the dot product of the original data point with the unit vector representing the new axis.</p>
</li>
<li>
<p><strong>Choosing the Principal Component</strong>:</p>
</li>
<li>The principal component is the axis that maximizes the spread or variance of the projected data points.</li>
<li>
<p>It is the direction along which the data varies the most.</p>
</li>
<li>
<p><strong>Orthogonality of Axes</strong>:</p>
</li>
<li>If additional principal components are chosen (for higher-dimensional data), they are orthogonal or perpendicular to each other.</li>
<li>
<p>Each subsequent principal component captures the maximum variance orthogonal to the previous ones.</p>
</li>
<li>
<p><strong>Difference from Linear Regression</strong>:</p>
</li>
<li>PCA is fundamentally different from linear regression.</li>
<li>Linear regression aims to predict an output variable <span class="arithmatex">\( y \)</span> based on input variables <span class="arithmatex">\( x \)</span>, optimizing for the least squares error between predicted and actual values.</li>
<li>
<p>PCA, on the other hand, does not involve a target variable <span class="arithmatex">\( y \)</span> and treats all input features equally. Its goal is to find axes that maximize data variance.</p>
</li>
<li>
<p><strong>Reconstruction</strong>:</p>
</li>
<li>After projecting data onto the principal component(s), it's possible to reconstruct an approximation of the original data.</li>
<li>
<p>This involves multiplying the projected value by the unit vector representing the principal component and obtaining a point in the original feature space.</p>
</li>
<li>
<p><strong>Implementation</strong>:</p>
</li>
<li>PCA can be implemented using various libraries such as scikit-learn in Python.</li>
<li>These libraries provide functions to perform PCA efficiently and handle the computational aspects.</li>
</ol>
<p>In summary, PCA is a powerful technique for reducing the dimensionality of data while preserving its essential characteristics. By choosing appropriate principal components, it enables the visualization and analysis of high-dimensional datasets.</p>
<h3 id="implementing-pca-using-the-scikit-learn-library-involves-several-steps-heres-how-you-can-do-it-in-code">Implementing PCA using the scikit-learn library involves several steps. Here's how you can do it in code:<a class="headerlink" href="#implementing-pca-using-the-scikit-learn-library-involves-several-steps-heres-how-you-can-do-it-in-code" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Data Preprocessing</strong>:</li>
<li>
<p>If your features have different ranges, perform feature scaling to ensure they have comparable scales.</p>
</li>
<li>
<p><strong>Running PCA</strong>:</p>
</li>
<li>Use the <code>PCA</code> class from scikit-learn to fit the data and obtain the principal components.</li>
<li>
<p>Specify the number of components you want to retain. For example, if you want to reduce from two dimensions to one dimension, set <code>n_components=1</code>.</p>
</li>
<li>
<p><strong>Explained Variance Ratio</strong>:</p>
</li>
<li>After fitting the PCA model, check the explained variance ratio to understand how much variance each principal component captures.</li>
<li>
<p>This can be accessed using the <code>explained_variance_ratio_</code> attribute of the PCA object.</p>
</li>
<li>
<p><strong>Transforming Data</strong>:</p>
</li>
<li>Use the <code>transform</code> method to project the original data onto the new axes (principal components).</li>
<li>This results in a reduced-dimensional representation of the data.</li>
</ol>
<p>Here's an example of how to implement PCA in Python using scikit-learn:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="c1"># Example dataset with six examples</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a><span class="c1"># Initialize PCA with one principal component</span>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a><span class="n">pca_1</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a>
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a><span class="c1"># Fit PCA to the data</span>
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a><span class="n">pca_1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a>
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a><span class="c1"># Check explained variance ratio</span>
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a><span class="nb">print</span><span class="p">(</span><span class="n">pca_1</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>  <span class="c1"># Output: [0.992]</span>
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#__codelineno-7-15"></a>
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#__codelineno-7-16"></a><span class="c1"># Transform data to one dimension</span>
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#__codelineno-7-17"></a><span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca_1</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<a id="__codelineno-7-18" name="__codelineno-7-18" href="#__codelineno-7-18"></a><span class="nb">print</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
</code></pre></div>
<p>In this example:
- We initialize PCA with one principal component.
- We fit the PCA model to the data.
- The explained variance ratio indicates that the first principal component captures 99.2% of the variance.
- We transform the data to one dimension using the <code>transform</code> method, resulting in <code>X_pca</code>, which contains the projected data onto the principal component.</p>
<p>You can modify the <code>n_components</code> parameter to experiment with different dimensionalities and observe how PCA affects the data representation. Additionally, you can visualize the transformed data to gain insights into its structure.s</p>
<h3 id="what-is-reinforcement-learning">What is Reinforcement Learning?<a class="headerlink" href="#what-is-reinforcement-learning" title="Permanent link">&para;</a></h3>
<h3 id="reinforcement-learning">Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permanent link">&para;</a></h3>
<p>Reinforcement Learning (RL) is a fundamental concept in machine learning that holds significant promise despite its current limited commercial applications. It stands as one of the pillars of machine learning, continuously advancing through research endeavors.</p>
<p><strong>Introduction to RL</strong>
- RL involves an agent learning to interact with an environment to achieve a certain goal over time.
- An example of RL in action is the autonomous helicopter, equipped with sensors and instruments, tasked with learning to fly under various conditions.</p>
<p><strong>Learning by Trial and Error</strong>
- In RL, the agent learns by trial and error, receiving feedback in the form of rewards or penalties.
- The task is to find a policy that maps from states to actions, guiding the agent's behavior.</p>
<p><strong>Challenges with Supervised Learning</strong>
- Supervised learning, where a model learns from labeled data, faces challenges in domains like autonomous helicopter flying due to the ambiguity of optimal actions.
- It's difficult to obtain a dataset of states and ideal actions.</p>
<p><strong>The Role of Reward Function</strong>
- In RL, a key component is the reward function, which guides the agent's learning process by signaling desirable and undesirable behaviors.
- It serves a similar purpose to training a dog: reinforcing good behavior and discouraging bad behavior.</p>
<p><strong>Flexibility and Power of RL</strong>
- RL offers flexibility by focusing on what the agent should achieve rather than prescribing how it should achieve it.
- The reward function allows for nuanced incentivization, shaping the agent's behavior towards desired outcomes.</p>
<p><strong>Applications of RL</strong>
- RL has found success in diverse applications, including controlling robots, optimizing factory operations, financial trading, and playing games.
- Notable examples include landing a lunar lander, factory optimization, stock trading strategies, and playing various games.</p>
<p><strong>Conclusion</strong>
- Despite its relatively limited use compared to supervised learning, RL stands out for its ability to autonomously learn optimal behaviors.
- Rather than specifying correct outputs for every input, RL focuses on defining a reward function that guides the agent's learning process.
- This approach empowers the algorithm to automatically discover effective actions.</p>
<h4 id="key-takeaways">Key Takeaways<a class="headerlink" href="#key-takeaways" title="Permanent link">&para;</a></h4>
<ul>
<li>RL involves an agent learning through interaction with an environment, guided by a reward signal.</li>
<li>Unlike supervised learning, which relies on labeled data, RL learns from feedback provided by a reward function.</li>
<li>The reward function incentivizes desirable behaviors and discourages undesirable ones, shaping the agent's learning process.</li>
<li>RL has diverse applications, ranging from robotics and optimization to finance and gaming.</li>
</ul>
<h4 id="advanced-thoughts">Advanced Thoughts<a class="headerlink" href="#advanced-thoughts" title="Permanent link">&para;</a></h4>
<ul>
<li>RL algorithms vary in complexity, from simple policy-based methods to more sophisticated value-based approaches like deep Q-networks (DQN) and actor-critic methods.</li>
<li>Balancing exploration and exploitation is a critical challenge in RL, ensuring that the agent explores new strategies while exploiting known effective ones.</li>
<li>The scalability of RL algorithms and their ability to handle high-dimensional state and action spaces are areas of ongoing research and development.</li>
</ul>
<h3 id="mars-rover-example">Mars rover example<a class="headerlink" href="#mars-rover-example" title="Permanent link">&para;</a></h3>
<h3 id="mars-rover-example_1">Mars Rover Example<a class="headerlink" href="#mars-rover-example_1" title="Permanent link">&para;</a></h3>
<p>To further illustrate the concept of reinforcement learning (RL), we'll explore a simplified example inspired by the Mars rover. This example, adapted from the work of Stanford professor Emma Branskill and collaborator Jagriti Agrawal, demonstrates how RL can be applied to guide the actions of an autonomous agent.</p>
<p><strong>Problem Setup</strong>
- The Mars rover is in a simplified environment with six possible positions (states), denoted as state 1 through state 6.
- Each state represents a location on Mars where the rover can conduct various scientific missions.</p>
<p><strong>Rewards and Goals</strong>
- Some states are more valuable for scientific exploration than others.
- State 1 and state 6 are particularly interesting, with associated rewards of 100 and 40, respectively.
- The rover's goal is to maximize its cumulative reward over time by selecting actions that lead to high-reward states.</p>
<p><strong>Actions and Decisions</strong>
- At each step, the rover can choose one of two actions: move left or move right.
- The rover must decide which action to take from its current state to maximize its long-term reward.</p>
<p><strong>Example Paths</strong>
- Starting from state 4, the rover might decide to move left, eventually reaching state 1 and receiving a reward of 100.
- Alternatively, it could move right, reaching state 6 and receiving a reward of 40.
- The choice of actions affects the rover's cumulative reward and efficiency in achieving its goals.</p>
<p><strong>Terminal States</strong>
- States 1 and 6 are terminal states, indicating the end of the rover's exploration for the day.
- After reaching a terminal state, the rover cannot earn additional rewards until the next day of exploration.</p>
<p><strong>Core Elements of RL</strong>
- In RL, each action taken by the rover involves:
  - State (S): The current position of the rover.
  - Action: The decision to move left or right.
  - Reward (R): The immediate reward obtained from the chosen action.
  - Next State (S'): The resulting state after taking the action.</p>
<p><strong>Formalism of RL</strong>
- RL algorithms analyze the state-action-reward-next state sequence to make decisions.
- The reward associated with each state guides the rover's behavior, influencing its choices to maximize long-term rewards.</p>
<p><strong>Next Steps</strong>
- In the upcoming video, we'll delve into how RL algorithms specify the desired behavior of the rover, particularly focusing on an important concept called the return.</p>
<h4 id="key-takeaways_1">Key Takeaways<a class="headerlink" href="#key-takeaways_1" title="Permanent link">&para;</a></h4>
<ul>
<li>RL enables the Mars rover to autonomously navigate its environment and maximize its cumulative reward.</li>
<li>The rover's decisions are guided by the rewards associated with each state, balancing exploration and exploitation to achieve its goals.</li>
<li>Understanding the state-action-reward-next state sequence is crucial for developing effective RL algorithms.</li>
</ul>
<h4 id="advanced-thoughts_1">Advanced Thoughts<a class="headerlink" href="#advanced-thoughts_1" title="Permanent link">&para;</a></h4>
<ul>
<li>RL algorithms for the Mars rover example may include methods like Q-learning or policy gradient algorithms to optimize decision-making.</li>
<li>Balancing the trade-off between exploration of new states and exploitation of known high-reward states is a critical challenge in RL.</li>
</ul>
<h4 id="additional-considerations">Additional Considerations<a class="headerlink" href="#additional-considerations" title="Permanent link">&para;</a></h4>
<ul>
<li>RL applications in real-world scenarios, such as space exploration, require robust algorithms capable of handling uncertainty and dynamic environments.</li>
</ul>
<h3 id="the-return-in-reinforcement-learning">The Return in reinforcement learning<a class="headerlink" href="#the-return-in-reinforcement-learning" title="Permanent link">&para;</a></h3>
<h3 id="the-return-in-reinforcement-learning_1">The Return in Reinforcement Learning<a class="headerlink" href="#the-return-in-reinforcement-learning_1" title="Permanent link">&para;</a></h3>
<p>In reinforcement learning (RL), understanding the concept of the return is crucial for evaluating the desirability of different sequences of rewards. The return allows us to weigh immediate rewards against future rewards, considering the time it takes to obtain them.</p>
<p><strong>Analogous Example</strong>
- Imagine choosing between picking up a $5 bill nearby or walking half an hour to collect a $10 bill across town. The return captures the value of rewards relative to the effort or time required to obtain them.</p>
<p><strong>Definition of Return</strong>
- The return in RL is the sum of rewards obtained over time, discounted by a factor called the discount factor ().
- The discount factor, typically close to 1, emphasizes immediate rewards over future ones. A common choice is  = 0.9 or 0.99.</p>
<p><strong>Calculation of Return</strong>
- For each step, the reward is multiplied by  raised to the power of the time step.
- The general formula for the return is: 
  - Return = R + R + R + R + ...
- The discount factor makes RL algorithms somewhat impatient, favoring immediate rewards over delayed ones.</p>
<p><strong>Illustrative Example</strong>
- Using a Mars rover example, starting from different states and taking different actions yields different returns.
- Returns are calculated by summing the discounted rewards, with higher returns indicating more favorable outcomes.</p>
<p><strong>Effects of Discount Factor</strong>
- A higher discount factor values immediate rewards more heavily, whereas a lower discount factor values future rewards more equally.
- Negative rewards incentivize the system to postpone them into the future, a behavior beneficial in financial applications.</p>
<p><strong>Applications of Return</strong>
- RL algorithms use the return to evaluate the desirability of different actions, guiding the agent's decision-making process.
- Understanding the trade-off between immediate and future rewards is essential for designing effective RL algorithms.</p>
<p><strong>Conclusion</strong>
- The return in RL accounts for the cumulative value of rewards, factoring in the time it takes to obtain them.
- By discounting future rewards, RL algorithms prioritize actions that yield immediate benefits, balancing short-term gains with long-term objectives.</p>
<p><strong>Additional Insights</strong>
- Negative rewards, common in certain applications, influence the timing of actions, encouraging the system to delay undesirable outcomes.
- The choice of discount factor reflects the system's impatience for rewards and plays a crucial role in determining optimal strategies.</p>
<h4 id="key-takeaways_2">Key Takeaways<a class="headerlink" href="#key-takeaways_2" title="Permanent link">&para;</a></h4>
<ul>
<li>The return in RL captures the cumulative value of rewards over time, weighted by a discount factor.</li>
<li>Immediate rewards are favored over future rewards, influencing the agent's decision-making process.</li>
<li>Negative rewards incentivize delaying undesirable outcomes, beneficial in certain applications like finance.</li>
</ul>
<h4 id="advanced-thoughts_2">Advanced Thoughts<a class="headerlink" href="#advanced-thoughts_2" title="Permanent link">&para;</a></h4>
<ul>
<li>Tuning the discount factor is critical for balancing short-term gains with long-term objectives in RL algorithms.</li>
<li>Understanding the effects of the discount factor on the timing of actions is essential for designing effective RL strategies.</li>
</ul>
<h4 id="further-considerations">Further Considerations<a class="headerlink" href="#further-considerations" title="Permanent link">&para;</a></h4>
<ul>
<li>RL algorithms often involve complex trade-offs between immediate rewards and future gains, requiring careful consideration of the discount factor's impact.</li>
<li>Negative rewards introduce additional challenges, necessitating robust algorithms capable of handling diverse reward structures.</li>
</ul>
<h3 id="making-decisions-policies-in-reinforcement-learning">Making decisions: Policies in reinforcement learning<a class="headerlink" href="#making-decisions-policies-in-reinforcement-learning" title="Permanent link">&para;</a></h3>
<h3 id="making-decisions-policies-in-reinforcement-learning_1">Making Decisions: Policies in Reinforcement Learning<a class="headerlink" href="#making-decisions-policies-in-reinforcement-learning_1" title="Permanent link">&para;</a></h3>
<p>In reinforcement learning (RL), decisions are made based on policies, which dictate the actions taken in different states to maximize the return. Here's a breakdown of policies in RL:</p>
<p><strong>Exploring Different Action Strategies</strong>
- RL algorithms can adopt various strategies for selecting actions, such as choosing the nearest reward, opting for the larger or smaller reward, or considering proximity to rewards.</p>
<p><strong>Definition of Policy ()</strong>
- A policy in RL is a function (denoted as ) that takes a state (s) as input and maps it to an action (a) to be taken in that state.
- The goal of RL is to determine a policy that, for each state, prescribes the action yielding the highest return.</p>
<p><strong>Example of Policy</strong>
- For instance, a policy might instruct to go left in states 2, 3, and 4, but go right in state 5.
- (s) indicates the action recommended by the policy for a given state s.</p>
<p><strong>Terminology and Standardization</strong>
- While "policy" might not be the most intuitive term, it has become standard in RL literature.
- An alternative term like "controller" might convey the concept more directly, but "policy" remains prevalent.</p>
<p><strong>Review and Transition</strong>
- In preceding videos, we covered essential RL concepts, including states, actions, rewards, returns, and policies.
- A quick review of these concepts will precede further exploration of RL algorithms in subsequent videos.</p>
<h4 id="key-takeaways_3">Key Takeaways<a class="headerlink" href="#key-takeaways_3" title="Permanent link">&para;</a></h4>
<ul>
<li>Policies in RL dictate the actions to be taken in different states to maximize returns.</li>
<li>RL algorithms can employ diverse strategies for action selection, guided by the policy function .</li>
</ul>
<h4 id="advanced-thoughts_3">Advanced Thoughts<a class="headerlink" href="#advanced-thoughts_3" title="Permanent link">&para;</a></h4>
<ul>
<li>Designing effective policies requires understanding the trade-offs between exploration and exploitation, as well as considering the stochastic nature of environments.</li>
</ul>
<h4 id="further-considerations_1">Further Considerations<a class="headerlink" href="#further-considerations_1" title="Permanent link">&para;</a></h4>
<ul>
<li>Experimentation with different policy strategies and their impact on learning efficiency and performance can provide valuable insights.</li>
<li>Integrating domain knowledge into policy design can enhance the effectiveness of RL algorithms in real-world applications.</li>
</ul>
<p>In the next video, we'll review the foundational concepts of RL covered so far and delve into developing algorithms aimed at finding optimal policies.</p>
<h3 id="review-of-key-concepts">Review of key concepts<a class="headerlink" href="#review-of-key-concepts" title="Permanent link">&para;</a></h3>
<h3 id="review-of-key-concepts-in-reinforcement-learning">Review of Key Concepts in Reinforcement Learning<a class="headerlink" href="#review-of-key-concepts-in-reinforcement-learning" title="Permanent link">&para;</a></h3>
<p>In our exploration of reinforcement learning (RL) using the Mars rover example, we've covered several fundamental concepts. Here's a quick review of those concepts and how they can be applied to other scenarios:</p>
<p><strong>1. States (S):</strong>
- States represent the different situations or configurations that the agent (e.g., Mars rover) can find itself in.
- In the Mars rover example, there were six states (1-6), each corresponding to a different position.</p>
<p><strong>2. Actions (A):</strong>
- Actions are the possible moves or decisions that the agent can make in a given state.
- For example, the Mars rover could move left or right from its current position.</p>
<p><strong>3. Rewards (R):</strong>
- Rewards indicate the immediate benefit or penalty associated with taking a particular action in a specific state.
- In the Mars rover example, different states had different rewards, such as 100 for the leftmost state and 40 for the rightmost state.</p>
<p><strong>4. Discount Factor ():</strong>
- The discount factor determines the importance of future rewards relative to immediate rewards.
- It's a value between 0 and 1, where a higher value gives more weight to future rewards.
- In the Mars rover example, a discount factor of 0.5 was used.</p>
<p><strong>5. Return:</strong>
- The return is the total cumulative reward that the agent expects to receive over time.
- It's calculated by summing the rewards obtained in each time step, discounted by the discount factor.
- The return reflects the overall value of a particular action sequence or policy.</p>
<p><strong>6. Policy ():</strong>
- A policy is a strategy or function that maps states to actions, guiding the agent's decision-making process.
- The goal of RL is to find an optimal policy that maximizes the expected return.
- Different policies can lead to different action selections in various states.</p>
<p><strong>Applications Beyond Mars Rover:</strong>
- RL concepts can be applied to a wide range of problems beyond the Mars rover example.
- For instance, RL can be used to control autonomous helicopters, play games like chess, or manage financial portfolios.</p>
<p><strong>Markov Decision Process (MDP):</strong>
- The formalism used to describe RL problems is known as a Markov Decision Process (MDP).
- MDPs model sequential decision-making in stochastic environments, where the future state depends only on the current state and action.
- The MDP framework provides a structured approach to understanding and solving RL problems.</p>
<p><strong>Next Steps:</strong>
- In the upcoming videos, we'll delve into developing algorithms for RL, starting with the state-action value function.
- Understanding the state-action value function is crucial for designing effective learning algorithms.</p>
<h4 id="key-takeaways_4">Key Takeaways:<a class="headerlink" href="#key-takeaways_4" title="Permanent link">&para;</a></h4>
<ul>
<li>RL involves states, actions, rewards, discount factors, returns, and policies, all within the framework of Markov Decision Processes.</li>
<li>These concepts can be applied to various real-world problems, offering a versatile approach to decision-making and optimization.</li>
</ul>
<h4 id="advanced-thoughts_4">Advanced Thoughts:<a class="headerlink" href="#advanced-thoughts_4" title="Permanent link">&para;</a></h4>
<ul>
<li>Tailoring RL algorithms to specific applications requires careful consideration of the environment dynamics and desired objectives.</li>
</ul>
<h4 id="further-considerations_2">Further Considerations:<a class="headerlink" href="#further-considerations_2" title="Permanent link">&para;</a></h4>
<ul>
<li>Experimenting with different discount factors and policies can provide insights into their impact on learning efficiency and performance.</li>
<li>Incorporating domain knowledge into RL algorithms can enhance their effectiveness in solving real-world challenges.</li>
</ul>
<p>The next video will delve into the state-action value function, a key component in RL algorithms. Let's proceed to explore this further.</p>
<h3 id="state-action-value-function-definition">State-action value function definition<a class="headerlink" href="#state-action-value-function-definition" title="Permanent link">&para;</a></h3>
<h3 id="state-action-value-function-q-function-definition">State-Action Value Function (Q Function) Definition<a class="headerlink" href="#state-action-value-function-q-function-definition" title="Permanent link">&para;</a></h3>
<p>In reinforcement learning (RL), one of the key quantities that algorithms aim to compute is the state-action value function, often denoted by the letter Q. Let's delve into what this function represents:</p>
<p><strong>1. Definition of Q(s, a):</strong>
- The state-action value function Q(s, a) is a function that takes a state (s) and an action (a) as inputs and outputs the expected return if the agent starts in state s, takes action a once, and then behaves optimally afterward.
- In simpler terms, Q(s, a) tells us the value of taking action a in state s, considering the future rewards that can be obtained by following the optimal policy.</p>
<p><strong>2. Example Calculation:</strong>
- Suppose we have the Mars rover example with a discount factor of 0.5 and a policy that advises going left from states 2, 3, and 4, and going right from state 5.
- To calculate Q(s, a) for different states and actions, we compute the return based on taking action a in state s and then following the optimal policy.
- For instance, Q(2, right) equals 12.5, indicating that taking action right in state 2 leads to a return of 12.5.</p>
<p><strong>3. Intuition Behind Q Function:</strong>
- The Q function helps in determining the best action to take in a given state.
- By comparing the Q values for different actions in a state, the agent can choose the action that maximizes the expected return.
- Ultimately, the goal is to compute Q(s, a) for all states and actions, enabling the agent to make optimal decisions in any situation.</p>
<p><strong>4. Maximal Q Value:</strong>
- The maximal Q value for a state s represents the highest expected return achievable from that state.
- For example, if Q(4, left) equals 12.5 and Q(4, right) equals 10, the maximal Q value for state 4 is 12.5, indicating that taking action left yields the highest return.</p>
<p><strong>5. Computing the Optimal Policy:</strong>
- If the agent can compute Q(s, a) for every state and action, it can select the action with the highest Q value in each state to form the optimal policy.
- This approach guides the agent to choose actions that maximize the expected return in the long run.</p>
<p><strong>6. Notation:</strong>
- The state-action value function is sometimes denoted as Q* or the optimal Q function, highlighting its role in determining the optimal policy.
- Q* represents the Q function associated with the optimal policy.</p>
<p><strong>7. Next Steps:</strong>
- In subsequent videos, algorithms for computing the Q function will be discussed, addressing the circularity in its definition and providing practical methods for learning it.</p>
<p>Understanding the state-action value function is crucial for developing effective RL algorithms. It serves as a key component in decision-making and policy optimization. Let's proceed to examine specific examples of Q values in the next video.</p>
<h3 id="state-action-value-function-example">State-action value function example<a class="headerlink" href="#state-action-value-function-example" title="Permanent link">&para;</a></h3>
<p>~&lt; ---------------------------------------------------------------------------------------  &gt;~<br />
                                                                    ~&lt;    &gt;<sub><br />
</sub>&lt; ---------------------------------------------------------------------------------------  &gt;~</p>
<h3 id="bellman-equation">Bellman Equation:<a class="headerlink" href="#bellman-equation" title="Permanent link">&para;</a></h3>
<h3 id="bellman-equation-understanding-the-key-equation-in-reinforcement-learning">Bellman Equation: Understanding the Key Equation in Reinforcement Learning<a class="headerlink" href="#bellman-equation-understanding-the-key-equation-in-reinforcement-learning" title="Permanent link">&para;</a></h3>
<p>The Bellman equation is a fundamental concept in reinforcement learning that helps compute the state-action value function <span class="arithmatex">\( Q(s, a) \)</span>. Let's break down the equation and understand its significance:</p>
<p><strong>1. Notation:</strong>
- <span class="arithmatex">\( S \)</span>: Current state.
- <span class="arithmatex">\( R(s) \)</span>: Immediate reward for being in state <span class="arithmatex">\( s \)</span>.
- <span class="arithmatex">\( A \)</span>: Current action taken in state <span class="arithmatex">\( S \)</span>.
- <span class="arithmatex">\( S' \)</span>: Next state after taking action <span class="arithmatex">\( A \)</span>.
- <span class="arithmatex">\( A' \)</span>: Possible action in state <span class="arithmatex">\( S' \)</span>.</p>
<p><strong>2. The Bellman Equation:</strong>
The Bellman equation for <span class="arithmatex">\( Q(s, a) \)</span> is expressed as follows:</p>
<div class="arithmatex">\[ Q(s, a) = R(s) + \gamma \cdot \max_{a'}[Q(s', a')] \]</div>
<p><strong>3. Intuition:</strong>
- <span class="arithmatex">\( R(s) \)</span>: Represents the immediate reward obtained in the current state <span class="arithmatex">\( s \)</span>.
- <span class="arithmatex">\( \gamma \cdot \max_{a'}[Q(s', a')] \)</span>: Captures the future rewards discounted by the factor <span class="arithmatex">\( \gamma \)</span>.
  - <span class="arithmatex">\( \gamma \)</span>: Discount factor (0 &lt; <span class="arithmatex">\( \gamma \)</span> &lt; 1) accounts for the importance of future rewards relative to immediate rewards.
  - <span class="arithmatex">\( \max_{a'}[Q(s', a')] \)</span>: Determines the maximum expected return achievable from the next state <span class="arithmatex">\( s' \)</span> after taking action <span class="arithmatex">\( a \)</span>.</p>
<p><strong>4. Example Application:</strong>
- Suppose we want to compute <span class="arithmatex">\( Q(2, \text{right}) \)</span> using the Bellman equation.
  - If the current state is 2 and the action is to go right, then <span class="arithmatex">\( s' = 3 \)</span>.
  - <span class="arithmatex">\( Q(2, \text{right}) = R(2) + \gamma \cdot \max_{a'}[Q(3, a')] \)</span>.
  - The equation incorporates the immediate reward <span class="arithmatex">\( R(2) \)</span> and the maximum expected return from the next state <span class="arithmatex">\( s' = 3 \)</span>.</p>
<p><strong>5. Terminal State Handling:</strong>
- In terminal states, the Bellman equation simplifies to <span class="arithmatex">\( Q(s, a) = R(s) \)</span> because there's no subsequent state <span class="arithmatex">\( s' \)</span>.
- Terminal states have fixed rewards and no further actions, hence the Q value equals the immediate reward.</p>
<p><strong>6. Understanding the Equation:</strong>
- The Bellman equation decomposes the total return into the immediate reward and the discounted future rewards.
- It illustrates how to iteratively update the Q values based on current rewards and future expectations.
- The equation captures the essence of dynamic programming in reinforcement learning, breaking down complex decision-making into simpler steps.</p>
<p><strong>7. Importance:</strong>
- The Bellman equation is central to many reinforcement learning algorithms, providing a framework for value iteration and policy improvement.
- It guides the agent in evaluating actions and updating Q values iteratively to converge towards an optimal policy.</p>
<p><strong>8. Optional Video:</strong>
- The video also offers an optional exploration of stochastic Markov decision processes, providing insights into RL applications with uncertain actions.</p>
<p>Understanding the Bellman equation is crucial for developing effective reinforcement learning algorithms. It forms the backbone of many RL approaches, facilitating decision-making and policy optimization.</p>
<h3 id="random-stochastic-environment-optional">Random (stochastic) environment (Optional)<a class="headerlink" href="#random-stochastic-environment-optional" title="Permanent link">&para;</a></h3>
<h3 id="understanding-stochastic-environments-in-reinforcement-learning">Understanding Stochastic Environments in Reinforcement Learning<a class="headerlink" href="#understanding-stochastic-environments-in-reinforcement-learning" title="Permanent link">&para;</a></h3>
<p>In some real-world scenarios, the outcomes of actions are not always deterministic. There might be uncertainties or randomness involved, leading to stochastic environments in reinforcement learning. Let's delve into the implications and strategies for dealing with such environments:</p>
<p><strong>1. Examples of Stochastic Environments:</strong>
- Consider a Mars rover commanded to go left. Despite the command, external factors like slippery terrain or wind may cause deviations.
- Real-world robots may not always execute commands precisely due to various factors like sensor noise, mechanical limitations, or environmental conditions.</p>
<p><strong>2. Modeling Stochastic Environments:</strong>
- In a stochastic environment, actions have probabilities associated with their outcomes.
- For instance, if the rover is commanded to go left, there might be a 90% chance of success (ending up in state 3) and a 10% chance of failure (ending up in state 5).</p>
<p><strong>3. Reinforcement Learning in Stochastic Environments:</strong>
- When employing a policy in a stochastic environment, the sequence of states visited and rewards obtained can vary probabilistically.
- The goal shifts from maximizing the return in a single trajectory to maximizing the expected return averaged over multiple trajectories.
- Expected return (or average return) captures the overall performance of a policy considering the randomness of outcomes.</p>
<p><strong>4. Expected Return Calculation:</strong>
- The expected return (<span class="arithmatex">\( \text{E}[R] \)</span>) is computed as the average of discounted rewards over all possible trajectories.
- It considers the probabilities of different outcomes and their associated rewards.
- Reinforcement learning algorithms aim to find policies that maximize this expected return.</p>
<p><strong>5. Modification of Bellman Equation:</strong>
- In stochastic environments, the Bellman equation is adjusted to account for the randomness in state transitions.
- The total return from a state-action pair now includes the immediate reward plus the expected future returns.
- The Bellman equation incorporates the average or expected value operator to handle stochasticity.</p>
<p><strong>6. Practical Implications:</strong>
- Stochastic environments present challenges in decision-making due to uncertainties in outcomes.
- Optimal policies need to balance between exploration and exploitation, considering both deterministic and stochastic actions.
- Reinforcement learning algorithms must be robust to variations in outcomes and adapt to the stochastic nature of the environment.</p>
<p><strong>7. Exploration in Stochastic Environments:</strong>
- In stochastic environments, exploration becomes crucial to learn about the distribution of outcomes associated with different actions.
- Algorithms need to explore diverse action trajectories to estimate the expected returns accurately.</p>
<p><strong>8. Real-world Applications:</strong>
- Many real-world applications involve stochastic environments, such as robotics, finance, healthcare, and gaming.
- Reinforcement learning algorithms must be capable of handling uncertainties and making robust decisions in such domains.</p>
<p><strong>9. Experimentation and Analysis:</strong>
- Experimentation with stochastic environments allows practitioners to understand the impact of uncertainties on policy performance.
- Analyzing how different factors (e.g., misstep probabilities) affect expected returns helps in fine-tuning algorithms for specific applications.</p>
<p>In summary, understanding and addressing stochastic environments are essential aspects of reinforcement learning. By incorporating randomness into the learning process and optimizing policies based on expected returns, RL algorithms can effectively navigate uncertain real-world scenarios.</p>
<h3 id="example-of-continuous-state-space-applications">Example of continuous state space applications<a class="headerlink" href="#example-of-continuous-state-space-applications" title="Permanent link">&para;</a></h3>
<h3 id="understanding-continuous-state-spaces-in-reinforcement-learning">Understanding Continuous State Spaces in Reinforcement Learning<a class="headerlink" href="#understanding-continuous-state-spaces-in-reinforcement-learning" title="Permanent link">&para;</a></h3>
<p>In many robotic control applications, the state space is not limited to a discrete set of states but instead consists of continuous values. Let's explore what this means and how it generalizes the concepts we've discussed so far:</p>
<p><strong>1. Discrete vs. Continuous State Spaces:</strong>
- In the simplified Mars rover example, the rover's state was represented by a discrete set of positions, such as six possible locations.
- However, in real-world scenarios, robots like cars, trucks, or helicopters can occupy any of a vast number of continuous positions.</p>
<p><strong>2. Examples of Continuous State Spaces:</strong>
- <strong>Car/Truck Control:</strong> The state of a self-driving car or truck may include continuous variables like x and y positions, orientation (Theta), velocity in x and y directions (x dot, y dot), and angular velocity (Theta dot).
- <strong>Helicopter Control:</strong> For an autonomous helicopter, the state might comprise x, y, and z positions, along with roll (Phi), pitch (Theta), yaw (Omega), velocity components, and angular velocities.</p>
<p><strong>3. Representation of Continuous States:</strong>
- Unlike discrete state spaces where states are single values, continuous state spaces require a vector of numbers to represent the state.
- Each element in the vector can take on any value within its valid range, allowing for a high degree of precision in describing the state.</p>
<p><strong>4. Challenges and Opportunities:</strong>
- Continuous state spaces pose challenges due to the infinite number of possible states, requiring sophisticated algorithms for decision-making.
- However, they also offer opportunities for more nuanced control and finer-grained actions, leading to more precise and efficient robot behavior.</p>
<p><strong>5. Reinforcement Learning in Continuous State Spaces:</strong>
- In reinforcement learning, algorithms must learn policies that map continuous states to appropriate actions.
- This involves exploring the state-action space efficiently and optimizing policies to maximize rewards over time.</p>
<p><strong>6. Generalization of Concepts:</strong>
- Despite the shift to continuous state spaces, the fundamental concepts of reinforcement learning, such as the Bellman equation and policy optimization, remain applicable.
- However, algorithms and techniques need to be adapted to handle the complexities of continuous spaces efficiently.</p>
<p><strong>7. Real-world Applications:</strong>
- Continuous state spaces are prevalent in various robotics and control applications, including autonomous vehicles, drones, manipulator arms, and more.
- Reinforcement learning enables these systems to learn complex behaviors and adapt to dynamic environments effectively.</p>
<p><strong>8. Simulation and Practice:</strong>
- Simulation environments provide a safe and scalable platform for training reinforcement learning agents in continuous state spaces.
- Practitioners can experiment with different algorithms and policies to develop robust control strategies for real-world deployment.</p>
<p><strong>9. Future Directions:</strong>
- Advances in reinforcement learning algorithms, along with improvements in sensor technology and computational power, continue to drive progress in robot control in continuous state spaces.
- Future research may focus on tackling specific challenges such as sample efficiency, exploration-exploitation trade-offs, and safety considerations.</p>
<p>In summary, understanding and effectively navigating continuous state spaces are crucial for developing successful reinforcement learning applications in robotics and control systems. By leveraging advanced algorithms and simulation environments, practitioners can design intelligent robots capable of operating autonomously in complex real-world environments.</p>
<h3 id="lunar-landers">Lunar landers<a class="headerlink" href="#lunar-landers" title="Permanent link">&para;</a></h3>
<h3 id="lunar-lander-a-reinforcement-learning-application">Lunar Lander: A Reinforcement Learning Application<a class="headerlink" href="#lunar-lander-a-reinforcement-learning-application" title="Permanent link">&para;</a></h3>
<p>The lunar lander simulation is a classic reinforcement learning environment where the goal is to safely land a spacecraft on the moon's surface. Here's an overview of the lunar lander problem and its key components:</p>
<p><strong>1. Objective:</strong>
- The task is to control a lunar lander approaching the moon's surface and guide it to land safely on a designated landing pad.</p>
<p><strong>2. Actions:</strong>
- The lunar lander has four possible actions:
  - <strong>Nothing:</strong> Do nothing and let inertia and gravity pull the lander downwards.
  - <strong>Left Thruster:</strong> Fire the left thruster to move the lander to the right.
  - <strong>Main Engine:</strong> Fire the main engine to provide downward thrust.
  - <strong>Right Thruster:</strong> Fire the right thruster to move the lander to the left.</p>
<p><strong>3. State Space:</strong>
- The state space comprises several continuous and binary variables:
  - Position (X, Y): Coordinates indicating horizontal and vertical position.
  - Velocity (X dot, Y dot): Speed in horizontal and vertical directions.
  - Angle (Theta): Tilt angle of the lander.
  - Angular Velocity (Theta dot): Rate of change of the tilt angle.
  - Grounded Flags (l, r): Binary values indicating whether the left and right legs are grounded.</p>
<p><strong>4. Reward Function:</strong>
- The reward function encourages safe and efficient landing:
  - <strong>Successful Landing:</strong> Reward between 100 and 140 for landing on the pad.
  - <strong>Moving Toward/Away:</strong> Positive reward for moving closer to the landing pad and negative reward for moving away.
  - <strong>Crash:</strong> Large negative reward (-100) for crashing.
  - <strong>Soft Landing:</strong> Reward for grounding each leg (+10) to encourage stability.
  - <strong>Fuel Conservation:</strong> Penalty for fuel usage (-0.3 for main engine, -0.03 for side thrusters).</p>
<p><strong>5. Learning Objective:</strong>
- The goal is to learn a policy  that maps states to actions to maximize the sum of discounted rewards.
- A high value of the discount factor  (e.g., 0.985) encourages long-term planning and consideration of future rewards.</p>
<p><strong>6. Learning Algorithm:</strong>
- The learning algorithm aims to learn an optimal policy using reinforcement learning techniques.
- Deep reinforcement learning approaches, often utilizing neural networks, are commonly employed to handle the complexity of the lunar lander problem.</p>
<p><strong>7. Reward Design:</strong>
- Designing an effective reward function is crucial for guiding the learning process towards desired behavior.
- The reward function should incentivize safe landings, fuel-efficient maneuvers, and stable control of the lander.</p>
<p><strong>8. Challenges:</strong>
- The lunar lander problem presents challenges such as continuous state spaces, complex dynamics, and sparse rewards.
- Learning algorithms need to efficiently explore the state-action space and balance exploration with exploitation to discover effective policies.</p>
<p><strong>9. Applications:</strong>
- The lunar lander simulation serves as a benchmark problem for testing and evaluating reinforcement learning algorithms.
- It also provides insights into real-world control problems, such as spacecraft landing and autonomous navigation.</p>
<p>In summary, the lunar lander problem offers a rich environment for studying reinforcement learning techniques, reward design, and policy optimization. By developing effective learning algorithms, researchers can advance our understanding of autonomous control and decision-making in complex and dynamic environments.</p>
<h3 id="learning-the-state-value-function">Learning the state-value function<a class="headerlink" href="#learning-the-state-value-function" title="Permanent link">&para;</a></h3>
<h3 id="learning-the-state-value-function-for-lunar-lander">Learning the State-Value Function for Lunar Lander<a class="headerlink" href="#learning-the-state-value-function-for-lunar-lander" title="Permanent link">&para;</a></h3>
<p>In the lunar lander problem, we aim to learn a policy that guides the spacecraft to land safely on the moon's surface. One approach to solving this problem is by learning the state-value function <span class="arithmatex">\( Q(s, a) \)</span>, which estimates the expected return from taking action <span class="arithmatex">\( a \)</span> in state <span class="arithmatex">\( s \)</span>. Here's how we can use reinforcement learning to train a neural network to approximate this function:</p>
<h4 id="1-neural-network-architecture">1. <strong>Neural Network Architecture:</strong><a class="headerlink" href="#1-neural-network-architecture" title="Permanent link">&para;</a></h4>
<ul>
<li>We'll train a neural network to approximate <span class="arithmatex">\( Q(s, a) \)</span> using the current state <span class="arithmatex">\( s \)</span> and action <span class="arithmatex">\( a \)</span> as inputs.</li>
<li>The input to the neural network consists of the state (8 numbers) concatenated with a one-hot encoding of the action (4 numbers), resulting in a 12-dimensional input vector <span class="arithmatex">\( X \)</span>.</li>
<li>The neural network will have two hidden layers with 64 units each and a single output unit to predict <span class="arithmatex">\( Q(s, a) \)</span>.</li>
</ul>
<h4 id="2-training-data-generation">2. <strong>Training Data Generation:</strong><a class="headerlink" href="#2-training-data-generation" title="Permanent link">&para;</a></h4>
<ul>
<li>We generate training data by interacting with the lunar lander environment.</li>
<li>During each interaction, we observe the current state <span class="arithmatex">\( s \)</span>, take an action <span class="arithmatex">\( a \)</span>, receive a reward <span class="arithmatex">\( R(s) \)</span>, and transition to a new state <span class="arithmatex">\( s' \)</span>.</li>
<li>We store these experiences as tuples <span class="arithmatex">\( (s, a, R(s), s') \)</span> in a replay buffer.</li>
</ul>
<h4 id="3-training-procedure">3. <strong>Training Procedure:</strong><a class="headerlink" href="#3-training-procedure" title="Permanent link">&para;</a></h4>
<ul>
<li>We periodically sample batches of experiences from the replay buffer to train the neural network.</li>
<li>For each experience <span class="arithmatex">\( (s, a, R(s), s') \)</span>, we compute the target value <span class="arithmatex">\( Y \)</span> using the Bellman equation:
  [ Y = R(s) + \gamma \max_{a'} Q(s', a') ]</li>
<li>We use this target value <span class="arithmatex">\( Y \)</span> as the ground truth to train the neural network to approximate <span class="arithmatex">\( Q(s, a) \)</span>.</li>
<li>We minimize the mean squared error loss between the predicted <span class="arithmatex">\( Q(s, a) \)</span> and the target <span class="arithmatex">\( Y \)</span> during training.</li>
</ul>
<h4 id="4-iterative-improvement">4. <strong>Iterative Improvement:</strong><a class="headerlink" href="#4-iterative-improvement" title="Permanent link">&para;</a></h4>
<ul>
<li>We repeat this process iteratively, continuously updating the neural network parameters based on new experiences.</li>
<li>As the neural network learns, it provides better estimates of the state-action values, improving the decision-making process.</li>
</ul>
<h4 id="5-algorithm-refinement">5. <strong>Algorithm Refinement:</strong><a class="headerlink" href="#5-algorithm-refinement" title="Permanent link">&para;</a></h4>
<ul>
<li>The algorithm described is a basic version of Deep Q-Network (DQN) learning.</li>
<li>Further refinements and enhancements to the algorithm can improve its performance and convergence speed, which we'll explore in subsequent videos.</li>
</ul>
<h4 id="6-implementation">6. <strong>Implementation:</strong><a class="headerlink" href="#6-implementation" title="Permanent link">&para;</a></h4>
<ul>
<li>Implementing the DQN algorithm involves integrating the neural network architecture, training procedure, and interaction with the environment using a suitable reinforcement learning framework like TensorFlow or PyTorch.</li>
</ul>
<h4 id="conclusion">Conclusion:<a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h4>
<p>Learning the state-value function <span class="arithmatex">\( Q(s, a) \)</span> using deep reinforcement learning techniques enables us to develop an effective policy for the lunar lander problem. By iteratively improving our estimates of the state-action values, we can guide the spacecraft to land safely on the moon's surface. Refinements to the basic algorithm can further enhance its performance and scalability.</p>
<h3 id="algorithm-refinement-improved-neural-network-architecture">Algorithm refinement: Improved neural network architecture<a class="headerlink" href="#algorithm-refinement-improved-neural-network-architecture" title="Permanent link">&para;</a></h3>
<h3 id="improved-neural-network-architecture-for-dqn">Improved Neural Network Architecture for DQN<a class="headerlink" href="#improved-neural-network-architecture-for-dqn" title="Permanent link">&para;</a></h3>
<p>In the previous video, we discussed a neural network architecture for training the Deep Q-Network (DQN) algorithm. However, there's an improved architecture that enhances the efficiency of the algorithm, which is commonly used in most DQN implementations. Let's explore this improved neural network architecture:</p>
<h4 id="original-architecture">Original Architecture:<a class="headerlink" href="#original-architecture" title="Permanent link">&para;</a></h4>
<ul>
<li>Input: 12-dimensional vector consisting of the state (8 numbers) concatenated with a one-hot encoding of the action (4 numbers).</li>
<li>Output: A single output unit predicting <span class="arithmatex">\( Q(s, a) \)</span> for each of the four possible actions.</li>
</ul>
<h4 id="improved-architecture">Improved Architecture:<a class="headerlink" href="#improved-architecture" title="Permanent link">&para;</a></h4>
<ul>
<li>Input: 8-dimensional vector representing the state of the lunar lander.</li>
<li>Hidden Layers: Two hidden layers with 64 units each.</li>
<li>Output: Four output units, each predicting <span class="arithmatex">\( Q(s, a) \)</span> for one of the four possible actions (nothing, left, main, right).</li>
</ul>
<h4 id="efficiency-enhancement">Efficiency Enhancement:<a class="headerlink" href="#efficiency-enhancement" title="Permanent link">&para;</a></h4>
<ul>
<li>The modified architecture allows the neural network to simultaneously compute the Q values for all four possible actions given a state <span class="arithmatex">\( s \)</span>.</li>
<li>This eliminates the need to run inference multiple times for each action, making the algorithm more efficient.</li>
<li>During training, this architecture also facilitates the computation of the maximum Q value over all actions for the Bellman update step.</li>
</ul>
<h4 id="implementation">Implementation:<a class="headerlink" href="#implementation" title="Permanent link">&para;</a></h4>
<ul>
<li>The neural network is trained to output Q values for all possible actions in a single forward pass.</li>
<li>This enables faster decision-making during policy execution as the Q values for all actions are available at once.</li>
</ul>
<h4 id="conclusion_1">Conclusion:<a class="headerlink" href="#conclusion_1" title="Permanent link">&para;</a></h4>
<p>The improved neural network architecture for DQN enhances the efficiency of the algorithm by allowing simultaneous computation of Q values for all possible actions given a state. This architectural change simplifies the implementation and accelerates the learning process, making it a preferred choice for DQN-based reinforcement learning tasks like the lunar lander problem.</p>
<h3 id="algorithm-refinement-greedy-policy">Algorithm refinement: -greedy policy<a class="headerlink" href="#algorithm-refinement-greedy-policy" title="Permanent link">&para;</a></h3>
<h3 id="algorithm-refinement-epsilon-greedy-policy">Algorithm Refinement: Epsilon-Greedy Policy<a class="headerlink" href="#algorithm-refinement-epsilon-greedy-policy" title="Permanent link">&para;</a></h3>
<p>In reinforcement learning, especially during the early stages of learning when the agent's estimate of the Q-function <span class="arithmatex">\( Q(s, a) \)</span> is still rudimentary, it's essential to balance exploration and exploitation. One common strategy to achieve this balance is through an Epsilon-greedy policy. Let's delve into how this policy works and its significance in reinforcement learning:</p>
<h4 id="the-challenge">The Challenge:<a class="headerlink" href="#the-challenge" title="Permanent link">&para;</a></h4>
<ul>
<li>When the agent is still learning, it doesn't have accurate estimates of the Q-values for different state-action pairs.</li>
<li>Taking actions purely based on these initial estimates might lead to suboptimal behavior, as the agent might not explore alternative actions enough to learn their true value.</li>
</ul>
<h4 id="epsilon-greedy-policy">Epsilon-Greedy Policy:<a class="headerlink" href="#epsilon-greedy-policy" title="Permanent link">&para;</a></h4>
<ul>
<li>The Epsilon-greedy policy addresses this challenge by blending exploration and exploitation.</li>
<li>Most of the time (e.g., with a probability of 0.95), the agent selects the action that maximizes the current estimate of the Q-function <span class="arithmatex">\( Q(s, a) \)</span>.</li>
<li>However, with a small probability (e.g., 5%), the agent chooses a random action regardless of its Q-value estimates.</li>
<li>This random exploration allows the agent to discover new actions and states that it might have overlooked otherwise.</li>
</ul>
<h4 id="importance-of-exploration">Importance of Exploration:<a class="headerlink" href="#importance-of-exploration" title="Permanent link">&para;</a></h4>
<ul>
<li>Exploration is crucial because it prevents the agent from getting stuck in suboptimal policies due to initial biases or inaccuracies in the Q-value estimates.</li>
<li>By occasionally exploring random actions, the agent can gather valuable information about the environment and refine its Q-value estimates over time.</li>
</ul>
<h4 id="implementation_1">Implementation:<a class="headerlink" href="#implementation_1" title="Permanent link">&para;</a></h4>
<ul>
<li>During the training process, the agent gradually reduces the exploration rate (Epsilon) over time.</li>
<li>Initially, the agent explores more (e.g., Epsilon = 1.0), taking random actions frequently.</li>
<li>As training progresses and the Q-value estimates improve, the agent relies more on exploitation and less on exploration (e.g., Epsilon gradually decreases to 0.01).</li>
</ul>
<h4 id="conclusion_2">Conclusion:<a class="headerlink" href="#conclusion_2" title="Permanent link">&para;</a></h4>
<ul>
<li>The Epsilon-greedy policy is a fundamental technique in reinforcement learning for balancing exploration and exploitation.</li>
<li>By incorporating randomness into action selection, the agent can effectively explore the environment and learn optimal policies.</li>
<li>Although the name "Epsilon-greedy" might seem counterintuitive, it accurately reflects the policy's balance between greedy exploitation and random exploration.</li>
<li>Tuning the exploration rate (Epsilon) is crucial, and it often requires experimentation to find the optimal value for a given task.</li>
<li>Implementing an Epsilon-greedy policy enhances the learning efficiency and effectiveness of reinforcement learning algorithms, such as the DQN algorithm used in the lunar lander problem.</li>
</ul>
<h3 id="algorithm-refinement-mini-batch-and-soft-updates-optional">Algorithm refinement: Mini-batch and soft updates (optional)<a class="headerlink" href="#algorithm-refinement-mini-batch-and-soft-updates-optional" title="Permanent link">&para;</a></h3>
<h3 id="algorithm-refinement-mini-batch-and-soft-updates">Algorithm Refinement: Mini-Batch and Soft Updates<a class="headerlink" href="#algorithm-refinement-mini-batch-and-soft-updates" title="Permanent link">&para;</a></h3>
<p>In reinforcement learning (RL), two further refinements can significantly enhance the efficiency and effectiveness of the learning algorithm: <em>mini-batch training and soft updates</em>.</p>
<h4 id="mini-batch-training">Mini-Batch Training:<a class="headerlink" href="#mini-batch-training" title="Permanent link">&para;</a></h4>
<ul>
<li>Mini-batch training is a technique commonly used in both supervised learning and RL.</li>
<li>In supervised learning, when dealing with large datasets, computing gradients over the entire dataset for each iteration of gradient descent can be computationally expensive.</li>
<li>Instead, mini-batch gradient descent is employed, where only a subset (mini-batch) of the dataset is used for each iteration.</li>
<li>Similarly, in RL, when training the neural network to approximate the Q-function, using mini-batches from the replay buffer can accelerate the learning process.</li>
<li>By training on smaller subsets of the data, each iteration becomes computationally less expensive, leading to faster convergence.</li>
<li>While mini-batch training introduces noise due to the randomness in selecting mini-batches, it tends to average out over iterations and still drives the parameters towards optimal values.</li>
</ul>
<h4 id="soft-updates">Soft Updates:<a class="headerlink" href="#soft-updates" title="Permanent link">&para;</a></h4>
<ul>
<li>Soft updates address the issue of abrupt changes in the Q-function estimate when updating the neural network parameters.</li>
<li>In the original algorithm, updating the Q-function involves replacing the current parameters <span class="arithmatex">\( W \)</span> and <span class="arithmatex">\( B \)</span> with the new parameters <span class="arithmatex">\( W_{\text{new}} \)</span> and <span class="arithmatex">\( B_{\text{new}} \)</span> obtained from training the new neural network.</li>
<li>However, this direct replacement can lead to significant fluctuations in the Q-function estimate if the new neural network is suboptimal.</li>
<li>Soft updates offer a more gradual transition by blending the old and new parameter values.</li>
<li>Instead of directly setting <span class="arithmatex">\( W \)</span> and <span class="arithmatex">\( B \)</span> to <span class="arithmatex">\( W_{\text{new}} \)</span> and <span class="arithmatex">\( B_{\text{new}} \)</span>, soft updates compute a weighted average between the old and new parameter values.</li>
<li>For example, <span class="arithmatex">\( W \)</span> is updated as <span class="arithmatex">\( 0.01 \times W_{\text{new}} + 0.99 \times W \)</span>, where the weights <span class="arithmatex">\( 0.01 \)</span> and <span class="arithmatex">\( 0.99 \)</span> control the extent of the update.</li>
<li>This gradual change prevents abrupt fluctuations in the Q-function estimate and helps the RL algorithm converge more reliably.</li>
</ul>
<h4 id="application-to-reinforcement-learning">Application to Reinforcement Learning:<a class="headerlink" href="#application-to-reinforcement-learning" title="Permanent link">&para;</a></h4>
<ul>
<li>In RL, mini-batch training accelerates the learning process by training the neural network on smaller subsets of the replay buffer.</li>
<li>Soft updates ensure a smoother transition between neural network parameter updates, leading to more stable and reliable convergence.</li>
<li>By incorporating these refinements, the RL algorithm becomes more efficient, making it easier to train complex tasks such as the Lunar Lander problem.</li>
</ul>
<h4 id="conclusion_3">Conclusion:<a class="headerlink" href="#conclusion_3" title="Permanent link">&para;</a></h4>
<ul>
<li>Mini-batch training and soft updates are crucial refinements that improve the efficiency and stability of reinforcement learning algorithms.</li>
<li>These techniques are applicable not only to RL but also to supervised learning, where dealing with large datasets is common.</li>
<li>Incorporating mini-batch training and soft updates into the RL algorithm enhances its performance and reliability, facilitating the successful training of challenging tasks like the Lunar Lander problem.</li>
</ul>
<h3 id="the-state-of-reinforcement-learning">The state of reinforcement learning<a class="headerlink" href="#the-state-of-reinforcement-learning" title="Permanent link">&para;</a></h3>
<h3 id="the-state-of-reinforcement-learning-practical-perspective">The State of Reinforcement Learning: Practical Perspective<a class="headerlink" href="#the-state-of-reinforcement-learning-practical-perspective" title="Permanent link">&para;</a></h3>
<p>Reinforcement learning (RL) is undoubtedly a captivating field of study, and it holds immense potential for various applications. However, it's essential to approach RL with a practical perspective, acknowledging both its strengths and limitations.</p>
<h4 id="practical-insights">Practical Insights:<a class="headerlink" href="#practical-insights" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>Hype vs. Reality:</strong></li>
<li>While there's significant excitement and research momentum behind RL, it's crucial to recognize that there might be some hype surrounding it.</li>
<li>
<p>Many research publications focus on simulated environments, where RL algorithms often perform well. However, transferring these algorithms to real-world applications, especially with physical systems like robots, can be challenging.</p>
</li>
<li>
<p><strong>Simulated vs. Real Environments:</strong></p>
</li>
<li>RL algorithms tend to work well in simulated environments and video games due to the controlled nature of these settings.</li>
<li>
<p>However, deploying RL algorithms in real-world scenarios, such as robotics, poses additional challenges and complexities.</p>
</li>
<li>
<p><strong>Application Scope:</strong></p>
</li>
<li>Despite the media attention, RL currently has fewer practical applications compared to supervised and unsupervised learning.</li>
<li>
<p>For most practical applications, the likelihood of utilizing supervised or unsupervised learning methods is higher than that of using RL.</p>
</li>
<li>
<p><strong>Personal Experience:</strong></p>
</li>
<li>Even practitioners with experience in RL, like myself, often find themselves primarily utilizing supervised and unsupervised learning techniques in their day-to-day work.</li>
<li>RL has been particularly useful in robotic control applications, but its broader adoption in various domains is still limited.</li>
</ol>
<h4 id="future-outlook">Future Outlook:<a class="headerlink" href="#future-outlook" title="Permanent link">&para;</a></h4>
<ul>
<li>Despite its current limitations, the potential of RL for future applications remains significant.</li>
<li>RL continues to be a major pillar of machine learning, and ongoing research in the field holds promise for advancing its capabilities.</li>
<li>Incorporating RL frameworks into your machine learning toolkit can enhance your effectiveness in building robust and functional machine learning systems.</li>
</ul>
<h4 id="conclusion_4">Conclusion:<a class="headerlink" href="#conclusion_4" title="Permanent link">&para;</a></h4>
<ul>
<li>While RL offers exciting possibilities, it's essential to approach it pragmatically, considering its practical applicability and the challenges associated with deploying RL algorithms in real-world settings.</li>
<li>By understanding the strengths and limitations of RL, you can leverage it effectively alongside other machine learning techniques to tackle diverse challenges and develop innovative solutions.</li>
</ul>
<h3 id="wrapping-up">Wrapping Up:<a class="headerlink" href="#wrapping-up" title="Permanent link">&para;</a></h3>
<ul>
<li>I hope you've found this exploration of reinforcement learning insightful and enjoyable.</li>
<li>Whether you're delving into RL for research or practical applications, I encourage you to experiment with implementing RL algorithms and witness the satisfaction of seeing them succeed.</li>
<li>As we conclude this specialization, I wish you continued success in your machine learning journey, equipped with the knowledge and skills to tackle exciting challenges and drive innovation in the field.</li>
</ul>


  




                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../ML%20Specialization%20-%20Course%202/" class="md-footer__link md-footer__link--prev" aria-label="Previous: ML Specialization - Course 2" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              ML Specialization - Course 2
            </div>
          </div>
        </a>
      
      
        
        <a href="../../NLP_Specialization/NLP%20Specialization%20-%20Course%201/" class="md-footer__link md-footer__link--next" aria-label="Next: NLP Specialization - Course 1" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              NLP Specialization - Course 1
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2020-2024  Jawad Haider  <a href="#__consent">Change cookie settings</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/qalmaqihir" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.linkedin.com/in/jawad-haider-uca/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.youtube.com/channel/UCB-D3NBU6UZ5N7IGKOJxtqQ" target="_blank" rel="noopener" title="www.youtube.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://t.me/Qalmaqihir" target="_blank" rel="noopener" title="t.me" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M248 8C111.033 8 0 119.033 0 256s111.033 248 248 248 248-111.033 248-248S384.967 8 248 8Zm114.952 168.66c-3.732 39.215-19.881 134.378-28.1 178.3-3.476 18.584-10.322 24.816-16.948 25.425-14.4 1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25 5.342-39.5 3.652-3.793 67.107-61.51 68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608 69.142-14.845 10.194-26.894 9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7 18.45-13.7 108.446-47.248 144.628-62.3c68.872-28.647 83.183-33.623 92.511-33.789 2.052-.034 6.639.474 9.61 2.885a10.452 10.452 0 0 1 3.53 6.716 43.765 43.765 0 0 1 .417 9.769Z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/qalmaqihir" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://instagram.com/qalmaqihir" target="_blank" rel="noopener" title="instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["search.share", "search.highlight", "search.sugguest", "content.code.annotate", "content.tooltips", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.e21f8df8.min.js"></script>
      
    
  </body>
</html>